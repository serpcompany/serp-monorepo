[
  {
    "owner": "mudler",
    "repo": "localai",
    "content": "TITLE: Full LocalAI Model YAML Configuration Reference\nDESCRIPTION: This YAML code provides a comprehensive reference for all available configuration options in a LocalAI model definition file. It covers settings related to model identification, performance (precision, threading, GPU usage), behavior (roles, backends, templates, function calls, system prompts), resource management (caching, memory mapping, NUMA), advanced features (sampling, LoRA, quantization, multimodal, diffusers), and specific backend parameters.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# Main configuration of the model, template, and system features.\nname: \"\" # Model name, used to identify the model in API calls.\n\n# Precision settings for the model, reducing precision can enhance performance on some hardware.\nf16: null # Whether to use 16-bit floating-point precision.\n\nembeddings: true # Enable embeddings for the model.\n\n# Concurrency settings for the application.\nthreads: null # Number of threads to use for processing.\n\n# Roles define how different entities interact in a conversational model.\n# It can be used to map roles to specific parts of the conversation.\nroles: {} # Roles for entities like user, system, assistant, etc.\n\n# Backend to use for computation (like llama-cpp, diffusers, whisper).\nbackend: \"\" # Backend for AI computations.\n\n# Templates for various types of model interactions.\ntemplate:\n    chat: \"\" # Template for chat interactions. Uses golang templates with Sprig functions.\n    chat_message: \"\" # Template for individual chat messages.  Uses golang templates with Sprig functions.\n    completion: \"\" # Template for generating text completions. Uses golang templates with Sprig functions.\n    edit: \"\" # Template for edit operations. Uses golang templates with Sprig functions.\n    function: \"\" # Template for function calls. Uses golang templates with Sprig functions.\n    use_tokenizer_template: false # Whether to use a specific tokenizer template. (vLLM)\n    join_chat_messages_by_character: null # Character to join chat messages, if applicable. Defaults to newline.\n\n# Function-related settings to control behavior of specific function calls.\nfunction:\n    disable_no_action: false # Whether to disable the no-action behavior.\n    grammar:\n        parallel_calls: false # Allow to return parallel tools\n        disable_parallel_new_lines: false # Disable parallel processing for new lines in grammar checks.\n        mixed_mode: false # Allow mixed-mode grammar enforcing\n        no_mixed_free_string: false # Disallow free strings in mixed mode.\n        disable: false # Completely disable grammar enforcing functionality.\n        prefix: \"\" # Prefix to add before grammars rules.\n        expect_strings_after_json: false # Expect string after JSON data.\n    no_action_function_name: \"\" # Function name to call when no action is determined.\n    no_action_description_name: \"\" # Description name for no-action functions.\n    response_regex: [] # Regular expressions to match response from\n    argument_regex: [] # Named regular to extract function arguments from the response.\n    argument_regex_key_name: \"key\" # Name of the named regex capture to capture the key of the function arguments\n\t  argument_regex_value_name: \"value\" # Name of the named regex capture to capture the value of the function arguments\n    json_regex_match: [] # Regular expressions to match JSON data when in tool mode\n    replace_function_results: [] # Placeholder to replace function call results with arbitrary strings or patterns.\n    replace_llm_results: [] # Replace language model results with arbitrary strings or patterns.\n    capture_llm_results: [] # Capture language model results as text result, among JSON, in function calls. For instance, if a model returns a block for \"thinking\" and a block for \"response\", this will allow you to capture the thinking block.\n    function_name_key: \"name\"\n    function_arguments_key: \"arguments\"\n\n# Feature gating flags to enable experimental or optional features.\nfeature_flags: {}\n\n# System prompt to use by default.\nsystem_prompt: \"\"\n\n# Configuration for splitting tensors across GPUs.\ntensor_split: \"\"\n\n# Identifier for the main GPU used in multi-GPU setups.\nmain_gpu: \"\"\n\n# Small value added to the denominator in RMS normalization to prevent division by zero.\nrms_norm_eps: 0\n\n# Natural question generation model parameter.\nngqa: 0\n\n# Path where prompt cache is stored.\nprompt_cache_path: \"\"\n\n# Whether to cache all prompts.\nprompt_cache_all: false\n\n# Whether the prompt cache is read-only.\nprompt_cache_ro: false\n\n# Mirostat sampling settings.\nmirostat_eta: null\nmirostat_tau: null\nmirostat: null\n\n# GPU-specific layers configuration.\ngpu_layers: null\n\n# Memory mapping for efficient I/O operations.\nmmap: null\n\n# Memory locking to ensure data remains in RAM.\nmmlock: null\n\n# Mode to use minimal VRAM for GPU operations.\nlow_vram: null\n\n# Words or phrases that halts processing.\nstopwords: []\n\n# Strings to cut from responses to maintain context or relevance.\ncutstrings: []\n\n# Strings to trim from responses for cleaner outputs.\ntrimspace: []\ntrimsuffix: []\n\n# Default context size for the model's understanding of the conversation or text.\ncontext_size: null\n\n# Non-uniform memory access settings, useful for systems with multiple CPUs.\nnuma: false\n\n# Configuration for LoRA\nlora_adapter: \"\"\nlora_base: \"\"\nlora_scale: 0\n\n# Disable matrix multiplication queuing in GPU operations.\nno_mulmatq: false\n\n# Model for generating draft responses.\ndraft_model: \"\"\nn_draft: 0\n\n# Quantization settings for the model, impacting memory and processing speed.\nquantization: \"\"\n\n# Utilization percentage of GPU memory to allocate for the model. (vLLM)\ngpu_memory_utilization: 0\n\n# Whether to trust and execute remote code.\ntrust_remote_code: false\n\n# Force eager execution of TensorFlow operations if applicable. (vLLM)\nenforce_eager: false\n\n# Space allocated for swapping data in and out of memory. (vLLM)\nswap_space: 0\n\n# Maximum model length, possibly referring to the number of tokens or parameters. (vLLM)\nmax_model_len: 0\n\n# Size of the tensor parallelism in distributed computing environments. (vLLM)\ntensor_parallel_size: 0\n\n# vision model to use for multimodal\nmmproj: \"\"\n\n# Disables offloading of key/value pairs in transformer models to save memory.\nno_kv_offloading: false\n\n# Scaling factor for the rope penalty.\nrope_scaling: \"\"\n\n# Type of configuration, often related to the type of task or model architecture.\ntype: \"\"\n\n# YARN settings\nyarn_ext_factor: 0\nyarn_attn_factor: 0\nyarn_beta_fast: 0\nyarn_beta_slow: 0\n# configuration for diffusers model\ndiffusers:\n    cuda: false # Whether to use CUDA\n    pipeline_type: \"\" # Type of pipeline to use.\n    scheduler_type: \"\" # Type of scheduler for controlling operations.\n    enable_parameters: \"\" # Parameters to enable in the diffuser.\n    cfg_scale: 0 # Scale for CFG in the diffuser setup.\n    img2img: false # Whether image-to-image transformation is supported.\n    clip_skip: 0 # Number of steps to skip in CLIP operations.\n    clip_model: \"\" # Model to use for CLIP operations.\n    clip_subfolder: \"\" # Subfolder for storing CLIP-related data.\n    control_net: \"\" # Control net to use\n\n# Step count, usually for image processing models\nstep: 0\n```\n\n----------------------------------------\n\nTITLE: Running a Gallery Model via LocalAI CLI\nDESCRIPTION: Demonstrates how to start LocalAI and run a specific model (`hermes-2-theta-llama-3-8b`) directly from the model gallery using the `local-ai run` command. This command downloads and runs the specified model if it's not already available locally.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai run hermes-2-theta-llama-3-8b\n```\n\n----------------------------------------\n\nTITLE: Detailed Model Application via API (Bash)\nDESCRIPTION: Provides a comprehensive example of using the `/models/apply` endpoint. It shows how to specify a model definition via `url` (full URL, GitHub short-hand, or local file) or `id` (gallery reference), assign a custom `name`, provide additional `files` to download (with URI, hash, and filename), and include `overrides` for the model configuration.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/models/apply -H \"Content-Type: application/json\" -d '{\n     \"url\": \"<MODEL_DEFINITION_URL>\",\n     \"id\": \"<GALLERY>@<MODEL_NAME>\",\n     \"name\": \"<INSTALLED_MODEL_NAME>\",\n     \"files\": [\n        {\n            \"uri\": \"<additional_file>\",\n            \"sha256\": \"<additional_file_hash>\",\n            \"filename\": \"<additional_file_name>\"\n        }\n     ],\n      \"overrides\": { \"backend\": \"...\", \"f16\": true }\n   }'\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI with Docker Containers\nDESCRIPTION: These Docker commands demonstrate how to run LocalAI in different container configurations. Options include CPU-only, Nvidia GPU support (CUDA 12), a combined CPU/GPU image, and All-in-One (AIO) images pre-loaded with models. Each command starts a container named 'local-ai' and maps port 8080.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# CPU only image:\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-cpu\n\n# Nvidia GPU:\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12\n\n# CPU and GPU image (bigger size):\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest\n\n# AIO images (it will pre-download a set of models ready for use, see https://localai.io/basics/container/)\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalAI Function Calling Without Grammar in YAML\nDESCRIPTION: Illustrates an advanced YAML configuration for a LocalAI model to handle function calls without relying on built-in grammar generation. Setting 'function.no_grammar' to true disables grammar, and 'function.response_regex' specifies one or more regex patterns with named capture groups ('function' and 'arguments') to extract tool call details directly from the LLM's text response.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: model_name\nparameters:\n  # Model file name\n  model: model/name\n\nfunction:\n  # set to true to not use grammars\n  no_grammar: true\n  # set one or more regexes used to extract the function tool arguments from the LLM response\n  response_regex:\n  - \"(?P<function>\\w+)\\s*\\((?P<arguments>.*)\\)\"\n```\n\n----------------------------------------\n\nTITLE: Example YAML Configuration for LocalAI phi-2 Model\nDESCRIPTION: Shows a sample YAML configuration file for the `phi-2` model used by LocalAI. It defines parameters like `name`, `context_size`, hardware settings (`f16`, `threads`, `gpu_layers`, `mmap`), model source (`parameters.model` referencing Hugging Face), inference parameters (`temperature`, `top_k`, `top_p`), and the prompt structure (`template.chat`, `template.completion`). Users can modify the template section to customize prompts.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/customize-model.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: phi-2\ncontext_size: 2048\nf16: true\nthreads: 11\ngpu_layers: 90\nmmap: true\nparameters:\n  # Reference any HF model or a local file here\n  model: huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf\n  temperature: 0.2\n  top_k: 40\n  top_p: 0.95\ntemplate:\n  \n  chat: &template |\n    Instruct: {{.Input}}\n    Output:\n  # Modify the prompt template here ^^^ as per your requirements\n  completion: *template\n```\n\n----------------------------------------\n\nTITLE: Loading Models with LocalAI CLI\nDESCRIPTION: These examples show how to use the `local-ai run` command to load and start LocalAI with specific models. Models can be loaded from the official model gallery, directly from Hugging Face URLs, the Ollama OCI registry, a remote configuration file URL, or a standard OCI registry like Docker Hub. This command typically installs the model if not present and then starts the LocalAI server with that model loaded.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)\nlocal-ai run llama-3.2-1b-instruct:q4_k_m\n# Start LocalAI with the phi-2 model directly from huggingface\nlocal-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf\n# Install and run a model from the Ollama OCI registry\nlocal-ai run ollama://gemma:2b\n# Run a model from a configuration file\nlocal-ai run https://gist.githubusercontent.com/.../phi-2.yaml\n# Install and run a model from a standard OCI registry (e.g., Docker Hub)\nlocal-ai run oci://localai/phi-2:latest\n```\n\n----------------------------------------\n\nTITLE: Performing Text Generation with LocalAI API using cURL\nDESCRIPTION: Sends a POST request to the LocalAI `/v1/chat/completions` endpoint to generate a text response based on a user message. It uses the `gpt-4` model and specifies the user's input and a temperature setting. This mimics the OpenAI Chat Completions API.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/try-it-out.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{ \"model\": \"gpt-4\", \"messages\": [{\"role\": \"user\", \"content\": \"How are you doing?\", \"temperature\": 0.1}] }' \n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment Variables via File - Bash\nDESCRIPTION: This snippet runs LocalAI in a Docker container and specifies environment variables through an env file with the --env-file flag. Dependencies: Docker, LocalAI image, and a properly formatted .env file prepared. Key parameter: --env-file pointing to file. Input: shell command; output: LocalAI starts with all key-value pairs from the env file. Limitation: .env file must exist and be correctly formatted.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --env-file .env localai\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Layers for llama.cpp in LocalAI - YAML\nDESCRIPTION: This YAML snippet configures a LocalAI model workload using llama.cpp, specifying the number of GPU layers with the gpu_layers parameter. It enables float16 (f16) acceleration for optimal performance with GPU support, referencing a relative model file under the parameters.model entry. Users should place this file in the appropriate configuration directory and ensure the backend is built with cuBLAS for gpu_layers to take effect. Inputs include model file paths, context size, threading, use of f16, and GPU layer count; outputs depend on model workload execution.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: my-model-name\\n# Default model parameters\\nparameters:\\n  # Relative to the models path\\n  model: llama.cpp-model.ggmlv3.q5_K_M.bin\\n\\ncontext_size: 1024\\nthreads: 1\\n\\nf16: true # enable with GPU acceleration\\ngpu_layers: 22 # GPU Layers (only used when built with cublas)\\n\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI in Docker with Nvidia GPU Support\nDESCRIPTION: Docker command to run LocalAI with Nvidia GPU support, utilizing CUDA 12 for accelerated inferencing.\nSOURCE: https://github.com/mudler/localai/blob/master/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12\n```\n\n----------------------------------------\n\nTITLE: Generating Basic Image via LocalAI API using Bash\nDESCRIPTION: Sends a POST request using curl to the LocalAI `/v1/images/generations` endpoint to generate a 256x256 image based on the prompt \"A cute baby sea otter\". Requires the LocalAI server to be running at `http://localhost:8080`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# 512x512 is supported too\ncurl http://localhost:8080/v1/images/generations -H \"Content-Type: application/json\" -d '{\n  \"prompt\": \"A cute baby sea otter\",\n  \"size\": \"256x256\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI Docker Container with Intel SYCL (Highlighting --device)\nDESCRIPTION: This Bash command demonstrates running a LocalAI Docker container with Intel SYCL support, specifically highlighting the essential `--device /dev/dri` flag required to pass the Intel GPU into the container. It uses a SYCL-enabled image tag (with a version placeholder), maps the models directory, exposes the port, and sets environment variables for debugging and threading. Requires Docker and Intel GPU drivers installed on the host.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -ti --device /dev/dri -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:{{< version >}}-sycl-f16-ffmpeg-core\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI with ROCm (AMD) GPU Acceleration in Docker - Bash\nDESCRIPTION: This Bash command runs a LocalAI container with ROCm/hipblas GPU acceleration by passing necessary environment variables and device mounts to docker run. Variables (DEBUG, REBUILD, BUILD_TYPE, GPU_TARGETS) define build type and targeted architecture. Required devices (/dev/dri, /dev/kfd) enable ROCm offloading. Dependencies: Docker, ROCm drivers, compatible AMD GPU, correct image version. Inputs: environment variables and device paths. Outputs: running LocalAI instance on ROCm backend; limitations per hardware and image support.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n -e DEBUG=true \\\n -e REBUILD=true \\\n -e BUILD_TYPE=hipblas \\\n -e GPU_TARGETS=gfx906 \\\n --device /dev/dri \\\n --device /dev/kfd \\\n quay.io/go-skynet/local-ai:master-aio-gpu-hipblas\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Diffusers Backend with CUDA Acceleration in LocalAI - YAML\nDESCRIPTION: This YAML configuration prepares the stablediffusion model to run on LocalAI using the diffusers backend. It specifies CUDA acceleration (cuda: true), float16 support (f16), number of steps, pipeline type, and additional enabled parameters and scheduler. The required dependency is the diffusers backend with CUDA support, and the file should be placed as a configuration in the relevant models directory. Key parameters include model file, backend, steps, enable_parameters, and scheduler_type. Limitations may apply based on backend support and hardware compatibility.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: stablediffusion\\nparameters:\\n  model: toonyou_beta6.safetensors\\nbackend: diffusers\\nstep: 30\\nf16: true\\ndiffusers:\\n  pipeline_type: StableDiffusionPipeline\\n  cuda: true\\n  enable_parameters: \\\"negative_prompt,num_inference_steps,clip_skip\\\"\\n  scheduler_type: \\\"k_dpmpp_sde\\\"\\n\n```\n\n----------------------------------------\n\nTITLE: Launching LocalAI with Full GPU Access Using CUDA in Docker - Bash\nDESCRIPTION: This Bash command runs a LocalAI containerized instance using the --gpus all flag to grant full access to the host GPU(s). It sets several environment variables (DEBUG, MODELS_PATH, THREADS), maps models from the current working directory, exposes port 8080, and launches a cublas-enabled image for CUDA support. Dependencies: docker, CUDA-compatible GPU, proper image tag. Inputs: model directory, environment flags. Outputs: a running LocalAI instance utilizing GPU acceleration. Limitations: only works with supported NVIDIA GPUs and correct image/tag configuration.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -ti --gpus all -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:v1.40.0-cublas-cuda12\\n\n```\n\n----------------------------------------\n\nTITLE: Sending Audio Files for Transcription Using LocalAI API - Bash\nDESCRIPTION: This Bash code snippet demonstrates how to send audio files to the /v1/audio/transcriptions endpoint provided by LocalAI for automated speech-to-text conversion. It uses the cURL command-line tool to post multipart/form-data requests with a specified audio file and model name. To utilize this snippet, ensure LocalAI is running and the whisper model is installed. The file path (FILE_PATH) and model name (MODEL_NAME) must be replaced with actual values to process media; the endpoint accepts all ffmpeg-supported audio formats.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/audio-to-text.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/audio/transcriptions -H \"Content-Type: multipart/form-data\" -F file=\"@<FILE_PATH>\" -F model=\"<MODEL_NAME>\"\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI Manually with Docker Compose\nDESCRIPTION: Shows how to run LocalAI using Docker Compose. The steps include cloning the LocalAI repository, placing the model file (`your-model.gguf`) in the `models` directory, optionally editing the `.env` file for configuration, and starting the services using `docker compose up`. It also includes `curl` commands to test the `/v1/models` and `/v1/completions` endpoints.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Clone LocalAI\ngit clone https://github.com/go-skynet/LocalAI\n\ncd LocalAI\n\n# (Optional) Checkout a specific LocalAI tag\n# git checkout -b build <TAG>\n\n# Copy your models to the models directory\ncp your-model.gguf models/\n\n# (Optional) Edit the .env file to set parameters like context size and threads\n# vim .env\n\n# Start with Docker Compose\ndocker compose up -d --pull always\n# Or build the images with:\n# docker compose up -d --build\n\n# Now the API is accessible at localhost:8080\ncurl http://localhost:8080/v1/models\n# {\"object\":\"list\",\"data\":[{\"id\":\"your-model.gguf\",\"object\":\"model\"}]}\n\ncurl http://localhost:8080/v1/completions -H \"Content-Type: application/json\" -d '{\n     \"model\": \"your-model.gguf\",\n     \"prompt\": \"A long time ago in a galaxy far, far away\",\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Running Models Using Different URI Schemes via LocalAI CLI\nDESCRIPTION: Provides examples of using the `local-ai run` command with different URI schemes to download and run models. It covers Hugging Face (`huggingface://`), Ollama OCI registry (`ollama://`), remote configuration files (`https://`), and standard OCI registries (`oci://`).\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start LocalAI with the phi-2 model\nlocal-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf\n# Install and run a model from the Ollama OCI registry\nlocal-ai run ollama://gemma:2b\n# Run a model from a configuration file\nlocal-ai run https://gist.githubusercontent.com/.../phi-2.yaml\n# Install and run a model from a standard OCI registry (e.g., Docker Hub)\nlocal-ai run oci://localai/phi-2:latest\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI Docker with Remote Model Configuration (phi-2)\nDESCRIPTION: Provides a `docker run` command to start the LocalAI container (`localai/localai:{{< version >}}-ffmpeg-core`), mapping the host's port 8080 to the container's port 8080. It loads the `phi-2` model configuration directly from a specified GitHub Gist URL upon startup. Docker installation is a prerequisite.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/customize-model.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8080:8080 localai/localai:{{< version >}}-ffmpeg-core https://gist.githubusercontent.com/mudler/ad601a0488b497b69ec549150d9edd18/raw/a8a8869ef1bb7e3830bf5c0bae29a0cce991ff8d/phi-2.yaml\n```\n\n----------------------------------------\n\nTITLE: Basic Text-to-Speech Request using Bash\nDESCRIPTION: Demonstrates a basic POST request to the LocalAI `/tts` endpoint using `curl`. It sends the text \"Hello world\" and specifies the default model \"tts\" to generate speech. The server returns an `audio/wav` file by default.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{\n  \"input\": \"Hello world\",\n  \"model\": \"tts\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Example: Running Llama2 Model Manually with Docker\nDESCRIPTION: Provides a concrete example of running LocalAI with Docker. It demonstrates downloading the `luna-ai-llama2` model using `wget`, optionally copying a prompt template, running the Docker container, and testing the API by listing available models (`/v1/models`) and sending a request to the chat completions endpoint (`/v1/chat/completions`) using `curl`. Expected JSON outputs are shown as comments.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir models\n\n# Download luna-ai-llama2 to models/\nwget https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GGUF/resolve/main/luna-ai-llama2-uncensored.Q4_0.gguf -O models/luna-ai-llama2\n\n# Use a template from the examples, if needed\ncp -rf prompt-templates/getting_started.tmpl models/luna-ai-llama2.tmpl\n\ndocker run -p 8080:8080 -v $PWD/models:/models -ti --rm quay.io/go-skynet/local-ai:latest --models-path /models --context-size 700 --threads 4\n\n# Now the API is accessible at localhost:8080\ncurl http://localhost:8080/v1/models\n# {\"object\":\"list\",\"data\":[{\"id\":\"luna-ai-llama2\",\"object\":\"model\"}]}\n\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n     \"model\": \"luna-ai-llama2\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}],\n     \"temperature\": 0.9\n   }'\n# {\"model\":\"luna-ai-llama2\",\"choices\":[{\"message\":{\"role\":\"assistant\",\"content\":\"I'm doing well, thanks. How about you?\"}}]}\n```\n\n----------------------------------------\n\nTITLE: Installing Build Dependencies - Homebrew - Bash\nDESCRIPTION: Installs the required libraries and tools for building LocalAI on macOS using Homebrew. This includes abseil, cmake, Go, gRPC, protobuf, and other dependencies that may be needed depending on selected backends. Must be run with Homebrew and appropriate OS permissions. No direct output other than package installation results.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install abseil cmake go grpc protobuf protoc-gen-go protoc-gen-go-grpc python wget\n```\n\n----------------------------------------\n\nTITLE: Loading Different Model Types in LocalAI\nDESCRIPTION: Commands demonstrating various ways to load AI models in LocalAI, including from model gallery, HuggingFace, Ollama registry, configuration files, and OCI registries.\nSOURCE: https://github.com/mudler/localai/blob/master/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)\nlocal-ai run llama-3.2-1b-instruct:q4_k_m\n# Start LocalAI with the phi-2 model directly from huggingface\nlocal-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf\n# Install and run a model from the Ollama OCI registry\nlocal-ai run ollama://gemma:2b\n# Run a model from a configuration file\nlocal-ai run https://gist.githubusercontent.com/.../phi-2.yaml\n# Install and run a model from a standard OCI registry (e.g., Docker Hub)\nlocal-ai run oci://localai/phi-2:latest\n```\n\n----------------------------------------\n\nTITLE: Downloading and Running LocalAI Binary (Bash)\nDESCRIPTION: This one-liner command downloads the appropriate LocalAI binary for the current operating system and architecture using `curl` and `uname`, saves it as `local-ai`, makes the file executable using `chmod +x`, and then executes the downloaded binary. It requires `curl` and `chmod` commands to be available in the system's PATH. The `{{< version >}}` placeholder is intended to be replaced with a specific release version.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/reference/binaries.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -Lo local-ai \"https://github.com/mudler/LocalAI/releases/download/{{< version >}}/local-ai-$(uname -s)-$(uname -m)\" && chmod +x local-ai && ./local-ai\n```\n\n----------------------------------------\n\nTITLE: Generating Images with LocalAI API using cURL\nDESCRIPTION: Sends a POST request to the LocalAI `/v1/images/generations` endpoint to create an image based on a text prompt. It specifies the desired prompt ('A cute baby sea otter') and the output image size ('256x256'). This endpoint mimics the OpenAI Image Generation API.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/try-it-out.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/images/generations \\\n      -H \"Content-Type: application/json\" -d '{\n          \"prompt\": \"A cute baby sea otter\",\n          \"size\": \"256x256\"\n        }'\n```\n\n----------------------------------------\n\nTITLE: Starting LocalAI Worker without P2P (llama-cpp-rpc) (Bash)\nDESCRIPTION: Runs a LocalAI worker process using the `llama-cpp-rpc` backend. This command is used for manual setup without P2P discovery. It requires specifying the listening host (`-H`), port (`-p`), and allocated memory (`-m`) for the worker via the `--llama-cpp-args` flag.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/distributed_inferencing.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai worker llama-cpp-rpc --llama-cpp-args=\"-H <listening_address> -p <listening_port> -m <memory>\" \n```\n\n----------------------------------------\n\nTITLE: Configuring Diffusers for Image-to-Video (GPU) using YAML\nDESCRIPTION: YAML configuration for setting up an image-to-video pipeline with the `diffusers` backend in LocalAI. It uses the `stabilityai/stable-video-diffusion-img2vid` model, specifies `StableVideoDiffusionPipeline` as the `pipeline_type`, and configures it for GPU usage with CUDA and f16 precision.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nname: img2vid\nparameters:\n  model: stabilityai/stable-video-diffusion-img2vid\nbackend: diffusers\nstep: 25\n# Force CPU usage\nf16: true\ncuda: true\ndiffusers:\n  pipeline_type: StableVideoDiffusionPipeline\n```\n\n----------------------------------------\n\nTITLE: Defining a Generic Embedding Model Backend in LocalAI - YAML\nDESCRIPTION: This YAML configuration snippet demonstrates how to manually create a LocalAI model config for embedding generation. It specifies the model name, backend framework, file path, and enables embedding functionality. Prerequisites include placing this file within the models directory and defining the correct backend and model file path. The config is essential for registering new embedding models in LocalAI, and the 'embeddings' boolean must be set to true for embedding features.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/embeddings.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: text-embedding-ada-002 # The model name used in the API\\nparameters:\\n  model: <model_file>\\nbackend: \"<backend>\"\\nembeddings: true\\n# .. other parameters\\n\n```\n\n----------------------------------------\n\nTITLE: Loading LocalAI Model Configuration from URL via CLI\nDESCRIPTION: This shell command shows how to start the LocalAI service while loading a model configuration directly from a URL. It uses the 'github://' shorthand to fetch the 'phi-2.yaml' configuration file from the 'master' branch of the specified GitHub repository.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nlocal-ai github://mudler/LocalAI/examples/configurations/phi-2.yaml@master\n```\n\n----------------------------------------\n\nTITLE: Posting an Embedding Generation Request Using Curl - Bash\nDESCRIPTION: This Bash command sends a POST request to the LocalAI API endpoint for embeddings using curl and formats the JSON output with jq. The request payload includes the input text and the designated embedding model. Dependencies are curl and jq. Replace 'my-awesome-model' with the desired model's name as configured in LocalAI. The input is a plain text string for embedding generation; output is the JSON-encoded embedding result.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/embeddings.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/embeddings -X POST -H \"Content-Type: application/json\" -d '{\\n  \"input\": \"My text\",\\n  \"model\": \"my-awesome-model\"\\n}' | jq \".\"\\n\n```\n\n----------------------------------------\n\nTITLE: Build Only Specific Backend with GRPC_BACKENDS - Bash\nDESCRIPTION: Builds LocalAI with only a specified backend (e.g., 'llama-cpp') by setting the GRPC_BACKENDS environment variable in the make command. Only the targeted backend will be included in the final build. All other build options and prerequisites apply as normal.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nmake GRPC_BACKENDS=backend-assets/grpc/llama-cpp build\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalAI AIO Service with Docker Compose (YAML)\nDESCRIPTION: Provides a `docker-compose.yml` configuration file to run LocalAI as a service. It defines an `api` service using a LocalAI AIO image (CPU version shown, GPU versions commented out), maps port 8080, includes a health check, mounts a local `./models` directory to `/build/models` inside the container, and includes commented-out sections for GPU resource allocation and environment variables like `DEBUG`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/container-images.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"3.9\"\nservices:\n  api:\n    image: localai/localai:latest-aio-cpu\n    # For a specific version:\n    # image: localai/localai:{{< version >}}-aio-cpu\n    # For Nvidia GPUs decomment one of the following (cuda11 or cuda12):\n    # image: localai/localai:{{< version >}}-aio-gpu-nvidia-cuda-11\n    # image: localai/localai:{{< version >}}-aio-gpu-nvidia-cuda-12\n    # image: localai/localai:latest-aio-gpu-nvidia-cuda-11\n    # image: localai/localai:latest-aio-gpu-nvidia-cuda-12\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/readyz\"]\n      interval: 1m\n      timeout: 20m\n      retries: 5\n    ports:\n      - 8080:8080\n    environment:\n      - DEBUG=true\n      # ...\n    volumes:\n      - ./models:/build/models:cached\n    # decomment the following piece if running with Nvidia GPUs\n    # deploy:\n    #   resources:\n    #     reservations:\n    #       devices:\n    #         - driver: nvidia\n    #           count: 1\n    #           capabilities: [gpu]\n```\n\n----------------------------------------\n\nTITLE: Querying GPT Vision API with LocalAI via Curl (Bash)\nDESCRIPTION: This Bash snippet demonstrates how to use curl to send a POST request to the LocalAI OpenAI-compatible /v1/chat/completions endpoint with a JSON payload. The payload requests a model (llava) to analyze an image and answer the question, 'What is in the image?'. Dependencies include curl and a running LocalAI server. Key parameters are 'model', 'messages' (with mixed text and image_url types), and 'temperature'. Inputs include a prompt and an image URL, with the expected output being a vision-based text response describing the image. The snippet is limited to models compatible with the API and needs LocalAI set up on localhost:8080.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/gpt-vision.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n     \"model\": \"llava\",\n     \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"What is in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Vall-E-X for Voice Cloning using YAML\nDESCRIPTION: Provides a YAML configuration example for setting up a Vall-E-X model for voice cloning. It defines a model named `cloned-voice`, specifies the `vall-e-x` backend, and points to an audio sample (`audio-sample.wav`) located in the models directory for voice cloning.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nname: cloned-voice\nbackend: vall-e-x\nparameters:\n  model: \"cloned-voice\"\ntts:\n    vall-e:\n      # The path to the audio file to be cloned\n      # relative to the models directory\n      # Max 15s\n      audio_path: \"audio-sample.wav\"\n```\n\n----------------------------------------\n\nTITLE: Build with CuBLAS Acceleration (Nvidia CUDA) - Bash\nDESCRIPTION: Builds LocalAI with Nvidia CUDA toolkit (CuBLAS) support for GPU acceleration. Requires that the Nvidia CUDA toolkit is pre-installed. Use is experimental and should be validated on target hardware.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nmake BUILD_TYPE=cublas build\n```\n\n----------------------------------------\n\nTITLE: Constraining LocalAI Chat Output to 'yes'/'no' using Grammar (Bash)\nDESCRIPTION: This Bash example uses `curl` to send a request to the LocalAI `/v1/chat/completions` endpoint. It demonstrates how to use the `grammar` parameter with a simple Backus-Naur Form (BNF) rule (`root ::= (\"yes\" | \"no\")`) to force the language model's response to be strictly either \"yes\" or \"no\". Requires a running LocalAI instance at `http://localhost:8080` and a `llama.cpp` compatible model (aliased as \"gpt-4\" here).\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/constrained_grammars.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n  \"model\": \"gpt-4\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Do you like apples?\"}],\n  \"grammar\": \"root ::= (\\\"yes\\\" | \\\"no\\\")\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Verifying Model Loading and CUDA Acceleration Output - Console Log\nDESCRIPTION: This console log output verifies that LocalAI and llama.cpp correctly detect CUDA devices, load a model, and allocate GPU resources. The output lists details such as detected GPU model, loaded layers, VRAM usage, and acceleration status. No dependencies beyond running a CUDA-accelerated LocalAI instance; outputs confirm backend, model details, and VRAM allocation status. Example log is for post-launch diagnostics and should match actual system output for a successful setup.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n5:22PM DBG Loading model in memory from file: /models/open-llama-7b-q4_0.bin\\nggml_init_cublas: found 1 CUDA devices:\\n  Device 0: Tesla T4\\nllama.cpp: loading model from /models/open-llama-7b-q4_0.bin\\nllama_model_load_internal: format     = ggjt v3 (latest)\\nllama_model_load_internal: n_vocab    = 32000\\nllama_model_load_internal: n_ctx      = 1024\\nllama_model_load_internal: n_embd     = 4096\\nllama_model_load_internal: n_mult     = 256\\nllama_model_load_internal: n_head     = 32\\nllama_model_load_internal: n_layer    = 32\\nllama_model_load_internal: n_rot      = 128\\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\\nllama_model_load_internal: n_ff       = 11008\\nllama_model_load_internal: n_parts    = 1\\nllama_model_load_internal: model size = 7B\\nllama_model_load_internal: ggml ctx size =    0.07 MB\\nllama_model_load_internal: using CUDA for GPU acceleration\\nllama_model_load_internal: mem required  = 4321.77 MB (+ 1026.00 MB per state)\\nllama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\\nllama_model_load_internal: offloading 10 repeating layers to GPU\\nllama_model_load_internal: offloaded 10/35 layers to GPU\\nllama_model_load_internal: total VRAM used: 1598 MB\\n...................................................................................................\\nllama_init_from_file: kv self size  =  512.00 MB\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama.cpp Embeddings Backend - YAML\nDESCRIPTION: This LocalAI model configuration YAML sets up embeddings via the 'llama-cpp' backend using a named model file (e.g., ggml-file.bin). Prerequisites include having the llama-cpp binaries and the named model file present in the system. The 'embeddings' key must be set to true, and the configuration should be placed in the models directory for LocalAI to load it at runtime.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/embeddings.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: my-awesome-model\\nbackend: llama-cpp\\nembeddings: true\\nparameters:\\n  model: ggml-file.bin\\n# ...\\n\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI Docker Container with Intel SYCL (phi-2 Example)\nDESCRIPTION: This Bash command runs a LocalAI Docker container leveraging Intel SYCL for GPU acceleration. It uses a specific SYCL-enabled image (`master-sycl-f32-ffmpeg-core`), maps the models directory, exposes the port, mounts the Intel GPU device (`/dev/dri`), and starts the container to load the `phi-2` model. The `--privileged` flag might be necessary depending on the environment and driver interaction. Requires Docker and Intel GPU drivers installed on the host.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -e DEBUG=true --privileged -ti -v $PWD/models:/build/models -p 8080:8080  -v /dev/dri:/dev/dri --rm quay.io/go-skynet/local-ai:master-sycl-f32-ffmpeg-core phi-2\n```\n\n----------------------------------------\n\nTITLE: Performing Function Calling with LocalAI API using cURL\nDESCRIPTION: Sends a POST request to the LocalAI `/v1/chat/completions` endpoint demonstrating function calling. It uses the `gpt-4` model, provides a user message asking about weather, and includes a `tools` array defining an available function (`get_current_weather`) with its description and parameters. `tool_choice: \"auto\"` lets the model decide whether to call the function.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/try-it-out.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"What is the weather like in Boston?\"\n      }\n    ],\n    \"tools\": [\n      {\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"get_current_weather\",\n          \"description\": \"Get the current weather in a given location\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n              },\n              \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\"]\n              }\n            },\n            \"required\": [\"location\"]\n          }\n        }\n      }\n    ],\n    \"tool_choice\": \"auto\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Performing GPT Vision Analysis with LocalAI API using cURL\nDESCRIPTION: Sends a POST request to the LocalAI `/v1/chat/completions` endpoint to analyze an image. It uses the `gpt-4-vision-preview` model and provides both a text prompt and an image URL within the messages payload. The temperature parameter controls the randomness of the output.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/try-it-out.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{ \n        \"model\": \"gpt-4-vision-preview\", \n        \"messages\": [\n          {\n            \"role\": \"user\", \"content\": [\n              {\"type\":\"text\", \"text\": \"What is in the image?\"},\n              {\n                \"type\": \"image_url\", \n                \"image_url\": {\n                  \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" \n                }\n              }\n            ], \n          \"temperature\": 0.9\n          }\n        ]\n      }' \n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Accelerate using Bash\nDESCRIPTION: A shell command to configure the Hugging Face Accelerate library with its default settings. Accelerate simplifies running PyTorch training scripts across different hardware setups (like multi-GPU or TPU) and is used by Axolotl to manage the fine-tuning process.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Converting Merged Model to GGUF Format using llama.cpp in Bash\nDESCRIPTION: A sequence of shell commands to convert the fine-tuned and merged PyTorch model (output from the LoRA merge step) into the GGUF format required by LocalAI and llama.cpp. This involves cloning the llama.cpp repository, compiling it (optionally with CUDA support), running the `convert.py` script to create an intermediate FP16 GGUF model, and finally using the `quantize` executable to create a quantized Q4_0 GGUF model (`custom-model-q4_0.bin`).\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Convert to gguf\ngit clone https://github.com/ggerganov/llama.cpp.git\npushd llama.cpp && make GGML_CUDA=1 && popd\n\n# We need to convert the pytorch model into ggml for quantization\n# It crates 'ggml-model-f16.bin' in the 'merged' directory.\npushd llama.cpp && python convert.py --outtype f16 \\\n    ../qlora-out/merged/pytorch_model-00001-of-00002.bin && popd\n\n# Start off by making a basic q4_0 4-bit quantization.\n# It's important to have 'ggml' in the name of the quant for some\n# software to recognize it's file format.\npushd llama.cpp &&  ./quantize ../qlora-out/merged/ggml-model-f16.gguf \\\n    ../custom-model-q4_0.bin q4_0\n```\n\n----------------------------------------\n\nTITLE: Defining a LocalAI Model Alias with YAML\nDESCRIPTION: This YAML configuration defines a LocalAI model named 'gpt-3.5-turbo'. It acts as an alias for the 'luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin' model file, specifying default parameters like temperature, context size, threads, backend ('llama-stable'), prompt caching settings, stopwords, role definitions, and associated prompt templates ('completion' and 'chat').\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: gpt-3.5-turbo\n# Default model parameters\nparameters:\n  # Relative to the models path\n  model: luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin\n  # temperature\n  temperature: 0.3\n  # all the OpenAI request options here..\n\n# Default context size\ncontext_size: 512\nthreads: 10\n# Define a backend (optional). By default it will try to guess the backend the first time the model is interacted with.\nbackend: llama-stable # available: llama, stablelm, gpt2, gptj rwkv\n\n# Enable prompt caching\nprompt_cache_path: \"alpaca-cache\"\nprompt_cache_all: true\n\n# stopwords (if supported by the backend)\nstopwords:\n- \"HUMAN:\"\n- \"### Response:\"\n# define chat roles\nroles:\n  assistant: '### Response:'\n  system: '### System Instruction:'\n  user: '### Instruction:'\ntemplate:\n  # template file \".tmpl\" with the prompt template to use by default on the endpoint call. Note there is no extension in the files\n  completion: completion\n  chat: chat\n```\n\n----------------------------------------\n\nTITLE: Configuring Diffusers for Image-to-Image (GPU) using YAML\nDESCRIPTION: YAML configuration for setting up an image-to-image pipeline using the `diffusers` backend in LocalAI. It specifies the `nitrosocke/Ghibli-Diffusion` model, sets the `pipeline_type` to `StableDiffusionImg2ImgPipeline`, enables CUDA and f16 precision, and explicitly enables the `image` parameter alongside others.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nname: stablediffusion-edit\nparameters:\n  model: nitrosocke/Ghibli-Diffusion\nbackend: diffusers\nstep: 25\ncuda: true\nf16: true\ndiffusers:\n  pipeline_type: StableDiffusionImg2ImgPipeline\n  enable_parameters: \"negative_prompt,num_inference_steps,image\"\n```\n\n----------------------------------------\n\nTITLE: Applying Stable Diffusion Model at Runtime (Bash)\nDESCRIPTION: Uses the `/models/apply` endpoint to download and install the Stable Diffusion model configuration from the LocalAI models gallery on GitHub while the LocalAI API is running. Requires a running LocalAI instance (`$LOCALAI`) and `curl`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{         \n     \"url\": \"github:mudler/LocalAI/gallery/stablediffusion.yaml@master\"\n   }'\n```\n\n----------------------------------------\n\nTITLE: Installing a Model by Config URL - LocalAI - Bash/JSON\nDESCRIPTION: Shows multiple ways to install a model using a JSON payload: by direct config URL, by model ID, or by providing the config URL. These commands are sent via POST to LocalAI's /models/apply endpoint. The example enables flexible model installation from different sources and demonstrates the API's versatility.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI=http://localhost:8080\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"config_url\": \"<MODEL_CONFIG_FILE_URL>\"\n   }' \n# or if from a repository\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"id\": \"<GALLERY>@<MODEL_NAME>\"\n   }' \n# or from a gallery config\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"url\": \"<MODEL_CONFIG_FILE_URL>\"\n   }' \n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependency Requirements with PyTorch CUDA and Transformers\nDESCRIPTION: Specifies Python package dependencies for a machine learning project using PyTorch (with CUDA 11.8 support), Huggingface Transformers, and Accelerate. The snippet includes an extra index URL for fetching the correct hardware-specific wheel for PyTorch and pins the torch package to a specific version compatible with CUDA. Ensure pip is version 21 or above for the direct package URL and version pinning to work properly. The input is used in pip or similar Python environment tools, and outputs an environment with all dependencies installed for GPU-accelerated ML tasks.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/exllama2/requirements-cublas11.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cu118\ntorch==2.4.1+cu118\ntransformers\naccelerate\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI Docker Container on Nvidia ARM64 Devices - Bash\nDESCRIPTION: This Bash snippet runs the LocalAI Docker container on an Nvidia ARM64 device, exposing port 8080 and mounting /data/models into the container for serving model files. The container is named local-ai, uses the Nvidia runtime, all available GPUs, and is set to always restart. It requires the LocalAI image to be present locally and expects Docker and Nvidia's container runtime to be configured. Replace /data/models with the path to your desired models directory. The output is a running AI inference API service on port 8080.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/reference/nvidia-l4t.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -e DEBUG=true -p 8080:8080 -v /data/models:/build/models  -ti --restart=always --name local-ai --runtime nvidia --gpus all quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core\n```\n\n----------------------------------------\n\nTITLE: Enabling Prompt Caching for LocalAI Models - YAML\nDESCRIPTION: This YAML snippet configures prompt caching by specifying both the cache file path (relative to the models folder) and a boolean indicating whether to cache all prompts. Dependencies: LocalAI supporting prompt_cache_path and prompt_cache_all in model configuration YAML. Parameters: prompt_cache_path sets the cache file's path; prompt_cache_all (bool) enables or disables prompt caching. Inputs are YAML model configuration files; output is persistent prompt cache files, improving model load time for prompts.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nprompt_cache_path: \"cache\"\nprompt_cache_all: true\n```\n\n----------------------------------------\n\nTITLE: Generating Text-to-Image with Diffusers Backend using Bash\nDESCRIPTION: Sends a text-to-image generation request using curl to a LocalAI instance configured with the `diffusers` backend. It specifies the target model (`animagine-xl`), provides positive and negative prompts separated by `|`, sets the number of inference steps, and defines the output image size.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/images/generations \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"prompt\": \"<positive prompt>|<negative prompt>\", \n      \"model\": \"animagine-xl\", \n      \"step\": 51,\n      \"size\": \"1024x1024\" \n    }'\n```\n\n----------------------------------------\n\nTITLE: Installing Models via LocalAI API - Bash\nDESCRIPTION: This bash example demonstrates how to programmatically install a model using LocalAI API by sending a POST request to /models/apply. Dependencies are curl, a running LocalAI server, and an accessible model definition URL. Parameters include 'id' (the canonical model file identifier) and an optional 'name' (the alias for the model). Input is a JSON payload; output is the installation of the specified model in LocalAI. Authentication may be necessary in production environments.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'http://localhost:8080/models/apply' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"id\": \"TheBloke/Luna-AI-Llama2-Uncensored-GGML/luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin\",\n    \"name\": \"lunademo\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Negative Prompt via LocalAI API using Bash\nDESCRIPTION: Demonstrates using a negative prompt with the LocalAI image generation API. The positive and negative prompts are separated by a pipe symbol (`|`) within the `prompt` field in the JSON payload sent via curl. Requires the LocalAI server to be running.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/images/generations -H \"Content-Type: application/json\" -d '{\n  \"prompt\": \"floating hair, portrait, ((loli)), ((one girl)), cute face, hidden hands, asymmetrical bangs, beautiful detailed eyes, eye shadow, hair ornament, ribbons, bowties, buttons, pleated skirt, (((masterpiece))), ((best quality)), colorful|((part of the head)), ((((mutated hands and fingers)))), deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, Octane renderer, lowres, bad anatomy, bad hands, text\",\n  \"size\": \"256x256\"\n}'\n```\n\n----------------------------------------\n\nTITLE: TTS Request using Model Defined in Config File (Bash)\nDESCRIPTION: Demonstrates making a TTS request using a model configuration defined in a YAML file. The `curl` command specifies the model name `xtts_v2` (defined in the YAML config) and provides French input text. This leverages the parameters (backend, language, voice) set in the configuration file. Output is piped to `aplay`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl -L http://localhost:8080/tts \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n\"model\": \"xtts_v2\",\n\"input\": \"Bonjour, je suis Ana Florence. Comment puis-je vous aider?\"\n}' | aplay\n```\n\n----------------------------------------\n\nTITLE: Configuring a Reranker Model Using Rerankers Backend in YAML\nDESCRIPTION: This YAML configuration snippet defines a reranker model named 'jina-reranker-v1-base-en' in LocalAI using the 'rerankers' backend. It specifies 'cross-encoder' as the model parameter, with optional commented settings for changing the reranker type and pipeline language. This configuration is required for LocalAI to load the reranker and can be used as a model config file or installed through the UI. No additional dependencies are required beyond LocalAI and access to rerankers. The 'model' parameter defines which cross-encoder to use, and any additional parameters may be set as needed in uncommented form.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/reranker.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: jina-reranker-v1-base-en\\nbackend: rerankers\\nparameters:\\n  model: cross-encoder\\n\\n# optionally:\\n# type: flashrank\\n# diffusers:\\n#  pipeline_type: en # to specify the english language\n```\n\n----------------------------------------\n\nTITLE: Configuring a Whisper Model for Transcription in LocalAI - YAML\nDESCRIPTION: This YAML configuration snippet defines a model named 'whisper-1' for use with LocalAI, specifying 'whisper' as the backend and setting the model parameter to 'whisper-en'. To use this, place the YAML file in the appropriate configuration directory alongside your downloaded model files. The YAML file ensures that LocalAI references the correct backend and associated parameters, and is a prerequisite for successful invocation of the transcription API.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/audio-to-text.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: whisper-1\nbackend: whisper\nparameters:\n  model: whisper-en\n```\n\n----------------------------------------\n\nTITLE: Generating Image-to-Video with Diffusers Backend using Bash\nDESCRIPTION: Example curl command to generate a video from an input image URL using the LocalAI API. It sends the image URL as the `file` parameter, specifies the output size, and targets the model configured for image-to-video generation (`img2vid`).\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n(echo -n '{\"file\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png?download=true\",\"size\": \"512x512\",\"model\":\"img2vid\"}') |\ncurl -H \"Content-Type: application/json\" -X POST -d @- http://localhost:8080/v1/images/generations\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI Manually with Docker\nDESCRIPTION: Illustrates the steps to run LocalAI manually using Docker. This involves creating a `models` directory, copying the model file (`your-model.gguf`), running the `local-ai` Docker container while mounting the models directory, and setting parameters like `--models-path`, `--context-size`, and `--threads`. It also includes a `curl` command to test the `/v1/completions` endpoint with the manually placed model.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Prepare the models into the `models` directory\nmkdir models\n\n# Copy your models to the directory\ncp your-model.gguf models/\n\n# Run the LocalAI container\ndocker run -p 8080:8080 -v $PWD/models:/models -ti --rm quay.io/go-skynet/local-ai:latest --models-path /models --context-size 700 --threads 4\n\n# Expected output:\n# ┌───────────────────────────────────────────────────┐\n# │                   Fiber v2.42.0                   │\n# │               http://127.0.0.1:8080               │\n# │       (bound on host 0.0.0.0 and port 8080)       │\n# │                                                   │\n# │ Handlers ............. 1  Processes ........... 1 │\n# │ Prefork ....... Disabled  PID ................. 1 │\n# └───────────────────────────────────────────────────┘\n\n# Test the endpoint with curl\ncurl http://localhost:8080/v1/completions -H \"Content-Type: application/json\" -d '{\n     \"model\": \"your-model.gguf\",\n     \"prompt\": \"A long time ago in a galaxy far, far away\",\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Configuring a LocalAI Model in YAML\nDESCRIPTION: Defines a basic configuration for a LocalAI model named 'gpt-3.5-turbo'. It specifies the model file ('ggml-openllama.bin') and sets generation parameters like 'top_p', 'top_k', and 'temperature'. This YAML file should be placed in the LocalAI models directory.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: gpt-3.5-turbo\nparameters:\n  # Model file name\n  model: ggml-openllama.bin\n  top_p: 80\n  top_k: 0.9\n  temperature: 0.1\n```\n\n----------------------------------------\n\nTITLE: Performing Text-to-Speech with LocalAI API using cURL\nDESCRIPTION: Sends a POST request to the LocalAI `/v1/audio/speech` endpoint to generate audio from text. It uses the `tts-1` model, provides the input text, specifies a voice (`alloy`), and saves the resulting audio to a file named `speech.mp3` using the `--output` flag. This mimics the OpenAI Text-to-Speech API.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/try-it-out.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/audio/speech \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1\",\n    \"input\": \"The quick brown fox jumped over the lazy dog.\",\n    \"voice\": \"alloy\"\n  }' \\\n  --output speech.mp3\n```\n\n----------------------------------------\n\nTITLE: Installing LocalAI via Bash Script - Bash\nDESCRIPTION: This snippet fetches and runs the LocalAI installer script using curl piped to sh. It enables users to quickly install LocalAI with minimal effort by sourcing the script directly from localai.io. There are no parameters required by default; simply running this command will start a standard installation. Prerequisites: a Unix-like shell environment with curl installed. This will install the latest available version of LocalAI.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/installer.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localai.io/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Coqui XTTSv2 Model using YAML\nDESCRIPTION: Example YAML configuration file defining a TTS model named `xtts_v2`. It uses the `coqui` backend, specifies the `xtts_v2` model, sets the language to French (`fr`), and selects the `Ana Florence` voice.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nname: xtts_v2\nbackend: coqui\nparameters:\n  language: fr\n  model: tts_models/multilingual/multi-dataset/xtts_v2\n\ntts:\n  voice: Ana Florence\n```\n\n----------------------------------------\n\nTITLE: Calling LocalAI with Explicit JSON Schema Grammar via Curl\nDESCRIPTION: Demonstrates making a request to the LocalAI chat completions endpoint using curl, where function definitions are provided directly within the request payload using the 'grammar_json_functions' parameter. This parameter accepts a JSON schema object (using 'oneOf' to list possible functions like 'create_event' and 'search' with their respective arguments) to constrain the model's output for function calling.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n     \"model\": \"gpt-4\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}],\n     \"temperature\": 0.1,\n     \"grammar_json_functions\": {\n        \"oneOf\": [\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"function\": {\"const\": \"create_event\"},\n                    \"arguments\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"date\": {\"type\": \"string\"},\n                            \"time\": {\"type\": \"string\"}\n                        }\n                    }\n                }\n            },\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"function\": {\"const\": \"search\"},\n                    \"arguments\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"query\": {\"type\": \"string\"}\n                        }\n                    }\n                }\n            }\n        ]\n    }\n   }'\n```\n\n----------------------------------------\n\nTITLE: Running vllm Backend Manually - Shell\nDESCRIPTION: The command starts LocalAI with a named backend ('vllm'), explicitly referencing a shell script to launch the backend implementation. Dependencies: local-ai binary, the backend shell script (run.sh) present, working directory set correctly. Key parameters: backend name (vllm), backend script path (using $PWD for the full path). Input: shell command; output: LocalAI connects to the script-based backend for model serving. Proper file permissions and environment setup are assumed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n./local-ai --external-grpc-backends \"vllm:$PWD/backend/python/vllm/run.sh\"\n```\n\n----------------------------------------\n\nTITLE: Generating Image-to-Image with Diffusers Backend using Bash\nDESCRIPTION: Example bash command using curl to perform image-to-image generation via the LocalAI API. It base64 encodes a local image specified by `IMAGE_PATH`, includes it in the JSON payload under the `file` key, along with the prompt, size, and target model (`stablediffusion-edit`). Requires the `base64` utility.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nIMAGE_PATH=/path/to/your/image\n(echo -n '{\"file\": \"'; base64 $IMAGE_PATH; echo '\", \"prompt\": \"a sky background\",\"size\": \"512x512\",\"model\":\"stablediffusion-edit\"}') |\ncurl -H \"Content-Type: application/json\" -d @-  http://localhost:8080/v1/images/generations\n```\n\n----------------------------------------\n\nTITLE: Example LocalAI API Response with Tool Call\nDESCRIPTION: Shows a sample JSON response from the LocalAI chat completions endpoint when a function call is triggered. The response includes metadata, the model used, and choices. Crucially, the 'message' field within 'choices' indicates 'finish_reason': 'tool_calls' and contains the 'tool_calls' array detailing the function to be called ('get_current_weather') and its extracted arguments.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"created\": 1724210813,\n    \"object\": \"chat.completion\",\n    \"id\": \"16b57014-477c-4e6b-8d25-aad028a5625e\",\n    \"model\": \"gpt-4\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"finish_reason\": \"tool_calls\",\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"index\": 0,\n                        \"id\": \"16b57014-477c-4e6b-8d25-aad028a5625e\",\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": \"get_current_weather\",\n                            \"arguments\": \"{\\\"location\\\":\\\"Beijing\\\",\\\"unit\\\":\\\"celsius\\\"}\"\n                        }\n                    }\n                ]\n            }\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 221,\n        \"completion_tokens\": 26,\n        \"total_tokens\": 247\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: TTS Request using Vall-E-X Backend in Bash\nDESCRIPTION: Illustrates a TTS request using the Vall-E-X backend. The `curl` command specifies `\"backend\": \"vall-e-x\"` and provides input text. Model files are downloaded automatically on first use. Requires Vall-E-X backend setup (automatic in container, manual otherwise). Output is piped to `aplay`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{         \n     \"backend\": \"vall-e-x\",\n     \"input\":\"Hello!\"\n   }' | aplay\n```\n\n----------------------------------------\n\nTITLE: Configuring stablediffusion-ggml Backend in LocalAI using YAML\nDESCRIPTION: Example YAML configuration (`stablediffusion.yaml`) for setting up a custom model using the `stablediffusion-ggml` backend in LocalAI. It specifies the backend name, the path to the model file (`gguf_model.gguf`), inference steps, CFG scale, paths to required CLIP and T5 models, and the sampler type. This file should reside in the LocalAI models directory.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nname: stablediffusion\nbackend: stablediffusion-ggml\nparameters:\n  model: gguf_model.gguf\nstep: 25\ncfg_scale: 4.5\noptions:\n- \"clip_l_path:clip_l.safetensors\"\n- \"clip_g_path:clip_g.safetensors\"\n- \"t5xxl_path:t5xxl-Q5_0.gguf\"\n- \"sampler:euler\"\n```\n\n----------------------------------------\n\nTITLE: Calling LocalAI Vision API with Grammar via Curl\nDESCRIPTION: Illustrates a curl command targeting a vision-capable LocalAI model ('llava'). The request includes both text and image input within the 'messages' array. Additionally, it specifies a simple 'grammar' parameter ('root ::= (\\\"yes\\\" | \\\"no\\\")') to restrict the model's response to either 'yes' or 'no', demonstrating the combination of vision input and output grammar constraints.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n curl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n     \"model\": \"llava\", \"grammar\": \"root ::= (\\\"yes\\\" | \\\"no\\\")\",\n     \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"Is there some grass in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}'\n```\n\n----------------------------------------\n\nTITLE: Deleting Keys and Values via LocalAI Stores HTTP API - shell\nDESCRIPTION: This snippet illustrates how to remove key/value pairs from the vector store with a POST request to /stores/delete, using cURL and a JSON body specifying the target keys. If a key is not present in the system, it is silently ignored. No response content is returned on success. This is a stateless operation and can be safely retried.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/stores.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST http://localhost:8080/stores/delete \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"keys\": [[0.1, 0.2]]}'\n```\n\n----------------------------------------\n\nTITLE: Build with Hipblas Acceleration (AMD GPU/ROCm) - Bash\nDESCRIPTION: Compiles LocalAI using the Hipblas backend targeting AMD GPUs with ROCm, specifying both BUILD_TYPE and GPU_TARGETS. Requires all environment variables from the previous snippet to be set. Targets specific AMD GPU architectures like 'gfx1030'.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nmake BUILD_TYPE=hipblas GPU_TARGETS=gfx1030\n```\n\n----------------------------------------\n\nTITLE: Preloading Models at Startup Using PRELOAD_MODELS - Bash\nDESCRIPTION: This bash snippet preloads one or more models during startup by setting the PRELOAD_MODELS environment variable with JSON data and launching LocalAI. Dependencies: LocalAI must recognize PRELOAD_MODELS and accept JSON input with model URLs and names. Key parameters are 'url' (YAML definition URL) and 'name' (model alias). Input is a JSON string list; the output is LocalAI starting up with the specified models preloaded for inference. May require correct path or networking setup.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nPRELOAD_MODELS='[{\"url\": \"https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml\",\"name\": \"gpt4all-j\"}]' local-ai\n```\n\n----------------------------------------\n\nTITLE: Music Generation using Transformers-Musicgen Backend in Bash\nDESCRIPTION: Demonstrates experimental music generation using the `transformers-musicgen` backend via the `/tts` endpoint. The `curl` command specifies the backend, a music generation model (`facebook/musicgen-medium`), and a text prompt (`\"Cello Rave\"`). The resulting audio is piped to `aplay`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n  --url http://localhost:8080/tts \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"backend\": \"transformers-musicgen\",\n    \"model\": \"facebook/musicgen-medium\",\n    \"input\": \"Cello Rave\"\n}' | aplay\n```\n\n----------------------------------------\n\nTITLE: Generating Video via LocalAI API using Bash and cURL\nDESCRIPTION: This Bash script sends a POST request to a running LocalAI instance at `http://localhost:8080/v1/images/generations`. It uses `curl` to submit a JSON payload containing the desired `prompt` (\"spiderman surfing\"), output `size` (\"512x512\"), and the `model` identifier (\"txt2vid\") to initiate a text-to-video generation task.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n(echo -n '{\"prompt\": \"spiderman surfing\",\"size\": \"512x512\",\"model\":\"txt2vid\"}') |\ncurl -H \"Content-Type: application/json\" -X POST -d @- http://localhost:8080/v1/images/generations\n```\n\n----------------------------------------\n\nTITLE: Applying Nvidia GPU LocalAI Kubernetes Deployment (kubectl)\nDESCRIPTION: This command uses `kubectl apply` to deploy LocalAI to a Kubernetes cluster using the deployment YAML file specifically configured for nodes with Nvidia GPUs. It requires `kubectl` access and properly configured Nvidia GPU nodes with the necessary drivers and device plugins installed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/kubernetes.md#2025-04-23_snippet_1\n\nLANGUAGE: kubectl\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment-nvidia.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing LocalAI via Shell Script - Bash\nDESCRIPTION: This code snippet demonstrates how to quickly install LocalAI using a one-liner shell command. It downloads and executes the install.sh script from localai.io using curl piped to sh. You must have both curl and sh available on your system. The command fetches the installation script and runs it directly, which will set up LocalAI using the default installation process. Use cautiously: piping scripts from the internet directly to shell should only be done from trusted sources.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/overview.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localai.io/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for ROCm (AMD) GPU Acceleration in LocalAI - YAML\nDESCRIPTION: This YAML snippet configures a Docker Compose service for LocalAI with ROCm (AMD) GPU support using the hipblas build type. It sets image version, debug and rebuild flags, GPU target architecture (e.g., gfx906), and passes required devices for offloading (/dev/dri, /dev/kfd). Dependencies: Docker Compose, ROCm-compatible hardware and drivers, correct LocalAI image. Key parameters are image tag, build flags, and device mappings; outputs are container instances with ROCm acceleration enabled. Limitation: only supports listed architectures unless REBUILD and GPU_TARGETS are set.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# docker-compose.yaml\\n    # For full functionality select a non-'core' image, version locking the image is recommended for debug purposes.\\n    image: quay.io/go-skynet/local-ai:master-aio-gpu-hipblas\\n    environment:\\n      - DEBUG=true\\n      # If your gpu is not already included in the current list of default targets the following build details are required.\\n      - REBUILD=true\\n      - BUILD_TYPE=hipblas\\n      - GPU_TARGETS=gfx906 # Example for Radeon VII\\n    devices:\\n      # AMD GPU only require the following devices be passed through to the container for offloading to occur.\\n      - /dev/dri\\n      - /dev/kfd\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Deployment for LocalAI with AMD ROCm GPU\nDESCRIPTION: This YAML snippet shows relevant sections of a Kubernetes Deployment resource for LocalAI, configured to utilize an AMD GPU via ROCm. It demonstrates setting the 'HIP_VISIBLE_DEVICES' environment variable to control GPU visibility within the container and requesting GPU resources using 'amd.com/gpu' under resource limits and requests. This configuration requires the ROCm/k8s-device-plugin to be installed on the cluster.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {NAME}-local-ai\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n        - env:\n            - name: HIP_VISIBLE_DEVICES\n              value: '0'\n              # This variable indicates the devices availible to container (0:device1 1:device2 2:device3) etc.\n              # For multiple devices (say device 1 and 3) the value would be equivelant to HIP_VISIBLE_DEVICES=\"0,2\"\n              # Please take note of this when an iGPU is present in host system as compatability is not assured.\n          ...\n          resources:\n            limits:\n              amd.com/gpu: '1'\n            requests:\n              amd.com/gpu: '1'\n```\n\n----------------------------------------\n\nTITLE: Configuring Diffusers for Depth-to-Image (GPU) using YAML\nDESCRIPTION: YAML configuration for the `diffusers` backend tailored for depth-to-image tasks. It uses the `stabilityai/stable-diffusion-2-depth` model, sets the `pipeline_type` to `StableDiffusionDepth2ImgPipeline`, configures steps, enables CUDA and f16, sets the CFG scale, and enables necessary parameters including `image`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nname: stablediffusion-depth\nparameters:\n  model: stabilityai/stable-diffusion-2-depth\nbackend: diffusers\nstep: 50\n# Force CPU usage\nf16: true\ncuda: true\ndiffusers:\n  pipeline_type: StableDiffusionDepth2ImgPipeline\n  enable_parameters: \"negative_prompt,num_inference_steps,image\"\n\ncfg_scale: 6\n```\n\n----------------------------------------\n\nTITLE: Clone, Build, and Run LocalAI - Bash\nDESCRIPTION: Covers the steps to clone the LocalAI repository, change to the project directory, and build using GNU Make. Produces the 'local-ai' binary in the working directory. No dependencies besides those enumerated elsewhere are assumed for this snippet. This is the core workflow for users building LocalAI from source on any platform.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/go-skynet/LocalAI\ncd LocalAI\nmake build\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for LocalAI on Intel GPUs\nDESCRIPTION: A requirements file specifying PyTorch with Intel XPU extensions, optimum, transformers, and other dependencies needed to run LocalAI on Intel GPU hardware. The file configures specific versions compatible with Intel's architecture, including version 2.3.1 of PyTorch with C++11 ABI support.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\ntorch==2.3.1+cxx11.abi\ntorchaudio==2.3.1+cxx11.abi\noneccl_bind_pt==2.3.100+xpu\noptimum[openvino]\nsetuptools\ntransformers==4.48.3\naccelerate\ncoqui-tts\n```\n\n----------------------------------------\n\nTITLE: Configuring Diffusers Backend (GPU) in LocalAI using YAML\nDESCRIPTION: YAML configuration example for the `diffusers` backend using the `Linaqruf/animagine-xl` model, specifically configured for GPU (CUDA) acceleration. It sets `cuda: true` and `f16: true` for potential performance improvements with float16 precision. The scheduler type is also defined.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nname: animagine-xl\nparameters:\n  model: Linaqruf/animagine-xl\nbackend: diffusers\ncuda: true\nf16: true\ndiffusers:\n  scheduler_type: euler_a\n```\n\n----------------------------------------\n\nTITLE: Installing Hermes-2-Pro-Mistral Model Example - LocalAI - Bash/JSON\nDESCRIPTION: Provides an explicit installation example for the hermes-2-pro-mistral model using curl and a JSON payload. The API endpoint '/models/apply' is called with the config URL pointing directly to the model's YAML definition. Useful for direct installs; outputs a job UUID for tracking installation status.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI=http://localhost:8080\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"config_url\": \"https://raw.githubusercontent.com/mudler/LocalAI/v2.25.0/embedded/models/hermes-2-pro-mistral.yaml\"\n   }' \n```\n\n----------------------------------------\n\nTITLE: Configuring Diffusers with Local Model and Parameters using YAML\nDESCRIPTION: Advanced YAML configuration for the `diffusers` backend using a locally stored model file (`toonyou_beta6.safetensors`). It demonstrates setting various parameters like inference steps (`step`), precision (`f16`), CUDA usage (`cuda`), pipeline type, enabling specific API parameters, scheduler type, CFG scale, and clip skip.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nname: stablediffusion\nparameters:\n  model: toonyou_beta6.safetensors\nbackend: diffusers\nstep: 30\nf16: true\ncuda: true\ndiffusers:\n  pipeline_type: StableDiffusionPipeline\n  enable_parameters: \"negative_prompt,num_inference_steps,clip_skip\"\n  scheduler_type: \"k_dpmpp_sde\"\n  clip_skip: 11\n\ncfg_scale: 8\n```\n\n----------------------------------------\n\nTITLE: Installing LocalAI using Homebrew on macOS\nDESCRIPTION: This command installs LocalAI on macOS using the Homebrew package manager. Note that the Homebrew formula might offer fewer configuration options compared to the bash installer.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbrew install localai\n```\n\n----------------------------------------\n\nTITLE: Using Named Docker Volume for Models in LocalAI Docker Run (Bash)\nDESCRIPTION: Shows how to use a Docker named volume (`localai-models`) for persisting models. It first creates the volume using `docker volume create` and then mounts this volume to `/build/models` inside the container using the `-v` flag with `docker run` when starting the CPU AIO image.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/container-images.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker volume create localai-models\ndocker run -p 8080:8080 --name local-ai -ti -v localai-models:/build/models localai/localai:latest-aio-cpu\n```\n\n----------------------------------------\n\nTITLE: Setting Up Huggingface SentenceTransformers Embedding Backend - YAML\nDESCRIPTION: This YAML snippet configures the use of the 'sentencetransformers' backend for text embeddings in LocalAI with a specified HuggingFace model. The relevant model is downloaded automatically on first use. Requirements include the Python 'sentence-transformers' package and potential backend registration via EXTERNAL_GRPC_BACKENDS. The config must reside in the models directory, and optional parameters such as 'model' select the HuggingFace pretrained variant.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/embeddings.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: text-embedding-ada-002\\nbackend: sentencetransformers\\nembeddings: true\\nparameters:\\n  model: all-MiniLM-L6-v2\\n\n```\n\n----------------------------------------\n\nTITLE: Build with ClBLAS Acceleration (OpenCL/CLBlast) - Bash\nDESCRIPTION: Triggers a build with BUILD_TYPE set to 'clblas' for OpenCL-based acceleration (using CLBlast). Ensures corresponding GPU and driver support, and allows custom CLBLAST_DIR via environment variable if needed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nmake BUILD_TYPE=clblas build\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI in Docker with CPU and GPU Support\nDESCRIPTION: Docker command for running the multi-architecture image of LocalAI that supports both CPU and GPU, though it has a larger size.\nSOURCE: https://github.com/mudler/localai/blob/master/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest\n```\n\n----------------------------------------\n\nTITLE: TTS Request using Bark Backend (Specific Voice) in Bash\nDESCRIPTION: Demonstrates using the Bark backend with a specific voice preset (`v2/en_speaker_4`). The `model` parameter is used within the JSON payload sent via `curl` to select the desired voice from the available Bark presets. The output audio is piped to `aplay`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{         \n     \"backend\": \"bark\",\n     \"input\":\"Hello!\",\n     \"model\": \"v2/en_speaker_4\"\n   }' | aplay\n```\n\n----------------------------------------\n\nTITLE: TTS Request using Coqui Backend in Bash\nDESCRIPTION: Shows how to use the Coqui backend for TTS via `curl`. This request specifies `\"backend\": \"coqui\"`, selects a specific Coqui model (`tts_models/en/ljspeech/glow-tts`), and provides input text. Requires a LocalAI image with Python dependencies (not `-core` tag).\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{         \n    \"backend\": \"coqui\",\n    \"model\": \"tts_models/en/ljspeech/glow-tts\",\n    \"input\":\"Hello, this is a test!\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Installing grpcio-tools with pip - Bash\nDESCRIPTION: Installs the Python grpcio-tools package in user-space via pip on macOS or other Python-supported environments. This package is required if Python-backed model architectures or components are used. The '--user' flag ensures installation does not require admin rights. Outputs information about the installation and updated packages.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install --user grpcio-tools\n```\n\n----------------------------------------\n\nTITLE: Build with Image Generation Support (Stable Diffusion) - Bash\nDESCRIPTION: Demonstrates building LocalAI with Stable Diffusion image generation enabled using the GO_TAGS environment variable. 'stablediffusion' must be included in the tags for image support. Ensures required Go components are included at compile time.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nmake GO_TAGS=stablediffusion build\n```\n\n----------------------------------------\n\nTITLE: Starting LocalAI Server in Federated P2P Mode (Bash)\nDESCRIPTION: Executes the `local-ai` command with flags `--p2p` and `--federated` to launch a LocalAI instance configured for federated distributed inference using peer-to-peer networking. This command also generates a security token required for other instances to join the federated network.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/distributed_inferencing.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai run --p2p --federated\n```\n\n----------------------------------------\n\nTITLE: Configuring Diffusers for Text-to-Video using YAML\nDESCRIPTION: YAML configuration snippet for setting up a text-to-video pipeline using the `diffusers` backend in LocalAI. It specifies the `damo-vilab/text-to-video-ms-1.7b` model, the backend type, and the number of inference steps. Further diffusers-specific configuration might be needed depending on the model.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nname: txt2vid\nparameters:\n  model: damo-vilab/text-to-video-ms-1.7b\nbackend: diffusers\nstep: 25\n```\n\n----------------------------------------\n\nTITLE: Configuring Diffusers Backend (CPU) in LocalAI using YAML\nDESCRIPTION: YAML configuration file for setting up the `diffusers` backend in LocalAI to use the `Linaqruf/animagine-xl` model from Hugging Face. This configuration explicitly forces CPU usage by setting `f16: false` and `diffusers.cuda: false`. It also specifies the scheduler type.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: animagine-xl\nparameters:\n  model: Linaqruf/animagine-xl\nbackend: diffusers\n\n# Force CPU usage - set to true for GPU\nf16: false\ndiffusers:\n  cuda: false # Enable for GPU usage (CUDA)\n  scheduler_type: euler_a\n```\n\n----------------------------------------\n\nTITLE: Checking NVIDIA GPU Availability with Docker - Bash/Console\nDESCRIPTION: This Bash/console command runs a CUDA-enabled container to display NVIDIA GPU information using nvidia-smi. It leverages docker run with the --runtime=nvidia option to ensure the container accesses the NVIDIA runtime, providing output for confirming GPU visibility and CUDA driver installation. Dependencies include Docker, NVIDIA drivers, and the nvidia-container-toolkit. Required input is an environment with supported GPUs; output is the nvidia-smi GPU status table. Limitations: will fail if NVIDIA drivers or Docker runtime are improperly configured.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#2025-04-23_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ndocker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\\n\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building LocalAI with Docker for Nvidia ARM64 - Bash\nDESCRIPTION: This Bash snippet demonstrates cloning the LocalAI repository and building a Docker image tailored for Nvidia ARM64 devices (Jetson series). It uses specific build arguments such as SKIP_DRIVERS, BUILD_TYPE, BASE_IMAGE, and IMAGE_TYPE to customize the build for the cublas backend and the correct Nvidia L4T base image. Dependencies include Docker and the Nvidia container toolkit installed on the host. Inputs include the repository URL and build arguments; output is a Docker image named quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core. The build process must be executed on a compatible ARM64 host system.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/reference/nvidia-l4t.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/mudler/LocalAI\n\ncd LocalAI\n\ndocker build --build-arg SKIP_DRIVERS=true --build-arg BUILD_TYPE=cublas --build-arg BASE_IMAGE=nvcr.io/nvidia/l4t-jetpack:r36.4.0 --build-arg IMAGE_TYPE=core -t quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core .\n```\n\n----------------------------------------\n\nTITLE: Merging LoRA Adapters into the Base Model using Axolotl CLI in Bash\nDESCRIPTION: Shell command using the Axolotl CLI (`axolotl.cli.merge_lora`) to merge the fine-tuned Low-Rank Adaptation (LoRA) weights with the original base model. It reads the configuration from `axolotl.yaml`, specifies the directory containing the LoRA output (`./qlora-out`), and ensures the merge is performed in full precision by disabling 8-bit and 4-bit loading. This creates a standalone fine-tuned model.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Merge lora\npython3 -m axolotl.cli.merge_lora axolotl.yaml --lora_model_dir=\"./qlora-out\" --load_in_8bit=False --load_in_4bit=False\n```\n\n----------------------------------------\n\nTITLE: Installing a Model with Override Name Example - LocalAI - Bash/JSON\nDESCRIPTION: Shows how to install a model with a config from the LocalAI gallery, but assign it a different display name (e.g., as 'gpt-3.5-turbo'). Useful for compatibility aliases or standardizing model references within your environment. Sent via a JSON POST request to the API.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI=http://localhost:8080\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n      \"url\": \"github:mudler/LocalAI/gallery/gpt4all-j.yaml\",\n      \"name\": \"gpt-3.5-turbo\"\n   }'  \n```\n\n----------------------------------------\n\nTITLE: Declaring Multiple LocalAI Models in a Single YAML File\nDESCRIPTION: This YAML snippet demonstrates how to define multiple LocalAI model configurations within a single file, structured as a list. Each item in the list represents a distinct model setup ('list1', 'list2'), specifying its name, parameters (including the model file), context size, threads, stopwords, roles, and templates. This format is suitable for use with the `--config-file` CLI argument.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- name: list1\n  parameters:\n    model: testmodel\n  context_size: 512\n  threads: 10\n  stopwords:\n  - \"HUMAN:\"\n  - \"### Response:\"\n  roles:\n    user: \"HUMAN:\"\n    system: \"GPT:\"\n  template:\n    completion: completion\n    chat: chat\n- name: list2\n  parameters:\n    model: testmodel\n  context_size: 512\n  threads: 10\n  stopwords:\n  - \"HUMAN:\"\n  - \"### Response:\"\n  roles:\n    user: \"HUMAN:\"\n    system: \"GPT:\"\n  template:\n    completion: completion\n   chat: chat\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion Preload Configuration (YAML)\nDESCRIPTION: Defines the Stable Diffusion model to be preloaded in a YAML file. This file is referenced by the `--preload-models-config` CLI argument when starting LocalAI. It specifies the model configuration URL from the LocalAI gallery.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\n- url: github:mudler/LocalAI/gallery/stablediffusion.yaml@master\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Dependencies for LocalAI with PyTorch ROCm Support\nDESCRIPTION: This requirements file specifies the Python packages needed to run LocalAI with PyTorch ROCm support. It includes PyTorch with ROCm 6.0 support, Hugging Face transformers, optimization libraries like bitsandbytes, and text-to-speech functionality via outetts.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/transformers/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\ntorch==2.4.1+rocm6.0\naccelerate\ntransformers\nllvmlite==0.43.0\nnumba==0.60.0\nbitsandbytes\noutetts\nbitsandbytes\nsentence-transformers==3.4.1\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI Docker with Custom Gist Configuration\nDESCRIPTION: Instructs the user on how to run the LocalAI Docker container while loading a custom model configuration provided via a GitHub Gist URL. The user must replace the placeholder Gist URL (`https://gist.githubusercontent.com/xxxx/phi-2.yaml`) with the actual raw URL of their customized configuration file. Requires Docker.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/customize-model.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n## Important! Substitute with your gist's URL!\ndocker run -p 8080:8080 localai/localai:{{< version >}}-ffmpeg-core https://gist.githubusercontent.com/xxxx/phi-2.yaml\n```\n\n----------------------------------------\n\nTITLE: Build with Intel SYCL Acceleration - Bash\nDESCRIPTION: Compiles LocalAI for Intel GPUs using SYCL, with BUILD_TYPE switched to either 'sycl_f16' (float16) or 'sycl_f32' (float32). Requires the Intel oneAPI Base Toolkit and proper hardware support. Additional configuration may be referenced via external links in the documentation.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nmake BUILD_TYPE=sycl_f16 build # for float16\nmake BUILD_TYPE=sycl_f32 build # for float32\n```\n\n----------------------------------------\n\nTITLE: Applying Standard LocalAI Kubernetes Deployment (kubectl)\nDESCRIPTION: This command uses `kubectl apply` to deploy LocalAI to a Kubernetes cluster using the standard deployment YAML file hosted in the LocalAI examples repository. It requires `kubectl` access to the target cluster and applies the configuration defined in the remote YAML.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/kubernetes.md#2025-04-23_snippet_0\n\nLANGUAGE: kubectl\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI-examples/refs/heads/main/kubernetes/deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: Requesting Specific Audio Response Format (MP3) in Bash\nDESCRIPTION: Shows how to request a specific audio format (MP3) in the TTS response, providing compatibility with OpenAI's API. The `curl` command includes `\"response_format\": \"mp3\"` in the JSON payload. This requires `ffmpeg` to be installed on the LocalAI server for format conversion.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{\n  \"input\": \"Hello world\",\n  \"model\": \"tts\",\n  \"response_format\": \"mp3\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Example Response from /models/jobs Endpoint (JSON)\nDESCRIPTION: Illustrates the JSON response from the `/models/jobs/<JOB_ID>` endpoint. It indicates whether the job is still being `processed`, reports any `error` (null if none), and provides a status `message` (e.g., 'completed').\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": null,\n  \"processed\": true,\n  \"message\": \"completed\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI All-In-One Docker Image\nDESCRIPTION: Docker command to run the AIO (All-In-One) image of LocalAI that comes pre-loaded with a set of models ready for use.\nSOURCE: https://github.com/mudler/localai/blob/master/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu\n```\n\n----------------------------------------\n\nTITLE: Specifying a Backend for a Model in LocalAI - YAML\nDESCRIPTION: This YAML snippet configures a LocalAI model to use a specified backend ('llama-stable') instead of auto-detecting, and sets up default model parameters. Prerequisites: the backend must exist and be compatible; dependencies include backend support and proper path configuration. Key parameters: name (model alias), parameters.model (relative model path), and backend (backend identifier). Input is a YAML config file in the models directory; output is a LocalAI model using the specified backend.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nname: gpt-3.5-turbo\n\n# Default model parameters\nparameters:\n  # Relative to the models path\n  model: ...\n\nbackend: llama-stable\n# ...\n```\n\n----------------------------------------\n\nTITLE: Installing LocalAI with the Basic Installation Script\nDESCRIPTION: A single-line command to run the installer script that sets up LocalAI with basic options.\nSOURCE: https://github.com/mudler/localai/blob/master/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localai.io/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Querying GPT Vision API with Grammar Constraint using Curl (Bash)\nDESCRIPTION: This Bash snippet shows how to send a POST request to the LocalAI API with an additional 'grammar' parameter. It forces the model to reply using the specified grammar ('yes' or 'no'), useful for constrained responses. Requires curl, LocalAI server, and a compatible vision model like LLaVA. The payload specifies a 'grammar' field and messages include an image URL with a yes/no type of prompt. Inputs are a question about the image and an image URL, while outputs are constrained to 'yes' or 'no'. This approach is limited to grammars supported by the deployed model and assumes grammar option recognition by the server.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/gpt-vision.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n curl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n     \"model\": \"llava\", \"grammar\": \"root ::= (\\\"yes\\\" | \\\"no\\\")\",\n     \"messages\": [{\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\": \"Is there some grass in the image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" }}], \"temperature\": 0.9}]}'\n```\n\n----------------------------------------\n\nTITLE: Generating Depth-to-Image with Diffusers Backend using Bash\nDESCRIPTION: Bash command demonstrating depth-to-image generation using curl and the LocalAI API. It base64 encodes a local image file (`~/path/to/image.jpeg`), includes it as the `file` property in the JSON payload, along with the prompt, size, and the target model (`stablediffusion-depth`). Requires `base64` utility.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n(echo -n '{\"file\": \"'; base64 ~/path/to/image.jpeg; echo '\", \"prompt\": \"a sky background\",\"size\": \"512x512\",\"model\":\"stablediffusion-depth\"}') |\ncurl -H \"Content-Type: application/json\" -d @-  http://localhost:8080/v1/images/generations\n```\n\n----------------------------------------\n\nTITLE: Example Response from /models/apply Endpoint (JSON)\nDESCRIPTION: Shows the typical JSON response structure after successfully submitting a model application request to `/models/apply`. It includes a unique `uuid` identifying the background job and a `status` URL to check the progress of the model download and installation.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"uuid\": \"251475c9-f666-11ed-95e0-9a8a4480ac58\",\n  \"status\": \"http://localhost:8080/models/jobs/251475c9-f666-11ed-95e0-9a8a4480ac58\"\n}\n```\n\n----------------------------------------\n\nTITLE: Full Example: Build and Run on macOS (M1/M2/M3) - Bash\nDESCRIPTION: Step-by-step workflow for building LocalAI on modern Mac (Apple Silicon), including dependency installation, repo clone, binary build, downloading a sample model, copying a prompt template, running the server, and making sample API requests via curl. Demonstrates the complete process from environment setup to endpoint testing. Assumes Homebrew, wget, and curl are already installed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# install build dependencies\nbrew install abseil cmake go grpc protobuf wget protoc-gen-go protoc-gen-go-grpc\n\n# clone the repo\ngit clone https://github.com/go-skynet/LocalAI.git\n\ncd LocalAI\n\n# build the binary\nmake build\n\n# Download phi-2 to models/\nwget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q2_K.gguf -O models/phi-2.Q2_K\n\n# Use a template from the examples\ncp -rf prompt-templates/ggml-gpt4all-j.tmpl models/phi-2.Q2_K.tmpl\n\n# Run LocalAI\n./local-ai --models-path=./models/ --debug=true\n\n# Now API is accessible at localhost:8080\ncurl http://localhost:8080/v1/models\n\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n     \"model\": \"phi-2.Q2_K\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}],\n     \"temperature\": 0.9 \n   }'\n```\n\n----------------------------------------\n\nTITLE: Starting LocalAI Server in P2P Mode for Workers (Bash)\nDESCRIPTION: Executes `local-ai run --p2p` to start a primary LocalAI server instance enabled for peer-to-peer communication. This mode is typically used as the entry point for setting up distributed inference using the 'Workers mode', where multiple worker nodes connect to this server via P2P using a generated token.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/distributed_inferencing.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai run --p2p\n```\n\n----------------------------------------\n\nTITLE: Manually Starting LocalAI P2P Worker (llama-cpp-rpc) (Bash)\nDESCRIPTION: Executes the command to start a LocalAI worker process configured for P2P networking (`p2p-llama-cpp-rpc`). The `TOKEN` environment variable must be set with the shared token obtained from the main server. The `--llama-cpp-args` flag is used to specify worker parameters like memory (`-m`). The example includes sample log output showing initialization and network discovery.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/distributed_inferencing.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nTOKEN=XXX ./local-ai worker p2p-llama-cpp-rpc --llama-cpp-args=\"-m <memory>\" \n# 1:06AM INF loading environment variables from file envFile=.env\n# 1:06AM INF Setting logging to info\n# {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:288\",\"message\":\"connmanager disabled\\n\"}\n# {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:295\",\"message\":\" go-libp2p resource manager protection enabled\"}\n# {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"config/config.go:409\",\"message\":\"max connections: 100\\n\"}\n# 1:06AM INF Starting llama-cpp-rpc-server on '127.0.0.1:34371'\n# {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.794+0200\",\"caller\":\"node/node.go:118\",\"message\":\" Starting EdgeVPN network\"}\n# create_backend: using CPU backend\n# Starting RPC server on 127.0.0.1:34371, backend memory: 31913 MB\n# 2024/05/19 01:06:01 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). # See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n# {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.805+0200\",\"caller\":\"node/node.go:172\",\"message\":\" Node ID: 12D3KooWJ7WQAbCWKfJgjw2oMMGGss9diw3Sov5hVWi8t4DMgx92\"}\n# {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.806+0200\",\"caller\":\"node/node.go:173\",\"message\":\" Node Addresses: [/ip4/127.0.0.1/tcp/44931 /ip4/127.0.0.1/udp/33251/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip4/127.0.0.1/udp/35660/quic-v1 /ip4/192.168.68.110/tcp/44931 /ip4/192.168.68.110/udp/33251/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip4/192.168.68.110/udp/35660/quic-v1 /ip6/::1/tcp/41289 /ip6/::1/udp/33160/quic-v1/webtransport/certhash/uEiAWAhZ-W9yx2ZHnKQm3BE_ft5jjoc468z5-Rgr9XdfjeQ/certhash/uEiB8Uwn0M2TQBELaV2m4lqypIAY2S-2ZMf7lt_N5LS6ojw /ip6/::1/udp/35701/quic-v1]\"}\n# {\"level\":\"INFO\",\"time\":\"2024-05-19T01:06:01.806+0200\",\"caller\":\"discovery/dht.go:104\",\"message\":\" Bootstrapping DHT\"}\n```\n\n----------------------------------------\n\nTITLE: Preloading Whisper Model via YAML File (Bash)\nDESCRIPTION: Starts LocalAI using the `local-ai` executable, specifying a path to a YAML configuration file via the `--preload-models-config` argument. LocalAI will read this file and preload the models defined within it, such as the Whisper base model.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai --preload-models-config \"/path/to/yaml\"\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with Intel XPU Acceleration Dependencies\nDESCRIPTION: A pip requirements file that specifies Intel-optimized PyTorch dependencies from a custom index URL. The file includes the Intel extension for PyTorch, specific versions of torch and torchaudio, oneccl bindings, optimum with openvino support, and other machine learning libraries.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\ntorch==2.3.1+cxx11.abi\ntorchaudio==2.3.1+cxx11.abi\noneccl_bind_pt==2.3.100+xpu\noptimum[openvino]\nsetuptools\ntransformers\naccelerate\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Requirements for PyTorch ML Environment - Python\nDESCRIPTION: This snippet lists required Python packages and versions for a PyTorch-based machine learning setup. Dependencies include torch (with CUDA 11.8 support), llvmlite, numba, accelerate, transformers, bitsandbytes, outetts, and sentence-transformers, as well as an extra index URL for PyTorch GPU wheels. The expected input is a requirements file used with pip; there are no code outputs, but proper installation enables model development and inference with hardware acceleration.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/transformers/requirements-cublas11.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cu118\ntorch==2.4.1+cu118\nllvmlite==0.43.0\nnumba==0.60.0\naccelerate\ntransformers\nbitsandbytes\noutetts\nsentence-transformers==3.4.1\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI Server with Manually Specified Workers (Bash)\nDESCRIPTION: Starts the main LocalAI server (`local-ai run`) while configuring it to use specific worker nodes defined manually (without P2P discovery). The addresses and ports of the worker nodes are passed via the `LLAMACPP_GRPC_SERVERS` environment variable as a comma-separated list.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/distributed_inferencing.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nLLAMACPP_GRPC_SERVERS=\"address1:port,address2:port\" local-ai run\n```\n\n----------------------------------------\n\nTITLE: Preloading Stable Diffusion Model via Environment Variable (Bash)\nDESCRIPTION: Sets the `PRELOAD_MODELS` environment variable before starting LocalAI. This causes LocalAI to automatically download and load the Stable Diffusion model from the specified gallery URL upon startup.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nPRELOAD_MODELS=[{\"url\": \"github:mudler/LocalAI/gallery/stablediffusion.yaml@master\"}]\n```\n\n----------------------------------------\n\nTITLE: Applying Whisper Model at Runtime (Bash)\nDESCRIPTION: Uses the `/models/apply` endpoint to download and install the Whisper base model configuration from the LocalAI models gallery on GitHub while the LocalAI API is running. It assigns the name `whisper-1` to this model. Requires a running LocalAI instance (`$LOCALAI`) and `curl`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{         \n     \"url\": \"github:mudler/LocalAI/gallery/whisper-base.yaml@master\",\n     \"name\": \"whisper-1\"\n   }'\n```\n\n----------------------------------------\n\nTITLE: Preloading Stable Diffusion Model via YAML File (Bash)\nDESCRIPTION: Starts LocalAI using the `local-ai` executable, specifying a path to a YAML configuration file via the `--preload-models-config` argument. LocalAI will read this file and preload the models defined within it, such as Stable Diffusion.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai --preload-models-config \"/path/to/yaml\"\n```\n\n----------------------------------------\n\nTITLE: Fixing Xcode Developer Tools Path (macOS) - Bash\nDESCRIPTION: Provides commands to diagnose and correct issues with Xcode developer tools path, particularly if the system is referencing incomplete Command Line Tools instead of the full Xcode SDK. This ensures the 'metal' utility and necessary development components are properly available for LocalAI builds.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# print /Library/Developer/CommandLineTools, if command line tools were installed in advance\nxcode-select --print-path\n\n# point to a complete SDK\nsudo xcode-select --switch /Applications/Xcode.app/Contents/Developer\n```\n\n----------------------------------------\n\nTITLE: Applying Bert Embeddings Model via API (Bash)\nDESCRIPTION: Applies a pre-defined Bert embeddings model configuration identified by `bert-embeddings` using the `/models/apply` endpoint. It assigns the OpenAI-compatible name `text-embedding-ada-002` to this model. Requires a running LocalAI instance accessible via `$LOCALAI` and `curl`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"id\": \"bert-embeddings\",\n     \"name\": \"text-embedding-ada-002\"\n   }'\n```\n\n----------------------------------------\n\nTITLE: Preloading Stable Diffusion Model via CLI Argument (Bash)\nDESCRIPTION: Passes the model configuration as a JSON string directly to the `local-ai` executable using the `--preload-models` argument. This instructs LocalAI to download and load the Stable Diffusion model from the specified gallery URL upon startup.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai --preload-models '[{\"url\": \"github:mudler/LocalAI/gallery/stablediffusion.yaml@master\"}]'\n```\n\n----------------------------------------\n\nTITLE: Preloading Whisper Model via Environment Variable (Bash)\nDESCRIPTION: Sets the `PRELOAD_MODELS` environment variable before starting LocalAI. This causes LocalAI to automatically download and load the Whisper base model from the specified gallery URL, naming it `whisper-1`, upon startup.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nPRELOAD_MODELS=[{\"url\": \"github:mudler/LocalAI/gallery/whisper-base.yaml@master\", \"name\": \"whisper-1\"}]\n```\n\n----------------------------------------\n\nTITLE: Whisper Preload Configuration (YAML)\nDESCRIPTION: Defines the Whisper model to be preloaded in a YAML file, referenced by the `--preload-models-config` CLI argument. It specifies the model configuration URL from the LocalAI gallery and assigns the name `whisper-1`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\n- url: github:mudler/LocalAI/gallery/whisper-base.yaml@master\n  name: whisper-1\n```\n\n----------------------------------------\n\nTITLE: Testing the Reranker Model via cURL Request in Bash\nDESCRIPTION: This bash snippet demonstrates how to test the configured reranker model by sending a POST request to the LocalAI endpoint using cURL. The request provides a query and a set of documents in JSON format, as well as the desired number of results ('top_n'). The model name specified matches that in the YAML configuration. Dependencies include running LocalAI with the configured model and the 'curl' command. The user specifies the reranker model, a query string, an array of candidate documents, and how many top results to return. The response will be a reranked list of the most relevant documents. The endpoint should be accessible at 'localhost:8080/v1/rerank'.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/reranker.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/rerank \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n  \"model\": \"jina-reranker-v1-base-en\",\n  \"query\": \"Organic skincare products for sensitive skin\",\n  \"documents\": [\n    \"Eco-friendly kitchenware for modern homes\",\n    \"Biodegradable cleaning supplies for eco-conscious consumers\",\n    \"Organic cotton baby clothes for sensitive skin\",\n    \"Natural organic skincare range for sensitive skin\",\n    \"Tech gadgets for smart homes: 2024 edition\",\n    \"Sustainable gardening tools and compost solutions\",\n    \"Sensitive skin-friendly facial cleansers and toners\",\n    \"Organic food wraps and storage solutions\",\n    \"All-natural pet food for dogs with allergies\",\n    \"Yoga mats made from recycled materials\"\n  ],\n  \"top_n\": 3\n}'\n```\n\n----------------------------------------\n\nTITLE: Preloading Models on Start - LocalAI - YAML\nDESCRIPTION: Shows the YAML format for specifying models to be preloaded by LocalAI. Each entry provides a URL to a model configuration, and can be referenced in command line arguments using the --preload-models-config flag.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n- url: github:mudler/LocalAI/gallery/stablediffusion.yaml@master\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI in Docker with CPU Support\nDESCRIPTION: Docker command to run LocalAI in a container with CPU-only support, exposing the API on port 8080.\nSOURCE: https://github.com/mudler/localai/blob/master/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-cpu\n```\n\n----------------------------------------\n\nTITLE: Testing Stable Diffusion Image Generation (Bash)\nDESCRIPTION: Sends a POST request to the `/v1/images/generations` endpoint to generate two 256x256 images using the loaded Stable Diffusion model. The request includes a detailed prompt, negative prompts (separated by '|'), generation mode, seed, size, and the number of images (`n`). Requires `$LOCALAI`, `curl`, and a loaded Stable Diffusion model.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ncurl $LOCALAI/v1/images/generations -H \"Content-Type: application/json\" -d '{\n            \"prompt\": \"floating hair, portrait, ((loli)), ((one girl)), cute face, hidden hands, asymmetrical bangs, beautiful detailed eyes, eye shadow, hair ornament, ribbons, bowties, buttons, pleated skirt, (((masterpiece))), ((best quality)), colorful|((part of the head)), ((((mutated hands and fingers)))), deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, Octane renderer, lowres, bad anatomy, bad hands, text\",\n            \"mode\": 2,  \"seed\":9000,\n            \"size\": \"256x256\", \"n\":2\n}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Galleries Using Environment Variable in Bash - Bash\nDESCRIPTION: Demonstrates how to set the GALLERIES environment variable to enable multiple model galleries in LocalAI container runtime. This setup allows users to configure default and custom model galleries using a JSON-formatted array string, making models from multiple sources available for installation and use within LocalAI. This is intended to be used when starting the LocalAI service inside a shell, requiring valid gallery URLs and compatible container images.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/whats-new.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nGALLERIES=[{\"name\":\"model-gallery\", \"url\":\"github:go-skynet/model-gallery/index.yaml\"}, {\"url\": \"github:ci-robbot/localai-huggingface-zoo/index.yaml\",\"name\":\"huggingface\"}]\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration Example\nDESCRIPTION: Sample .env file configuration for LocalAI showing how to set thread count, models path, and GPU acceleration\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI_THREADS=10\nLOCALAI_MODELS_PATH=/mnt/storage/localai/models\nLOCALAI_F16=true\n```\n\n----------------------------------------\n\nTITLE: Searching and Filtering Models with jq - Bash\nDESCRIPTION: Contains several Bash commands that use curl to fetch the available models and jq for advanced filtering. Includes examples for searching model names containing a keyword, extracting binary names of local models, and listing URLs with specific substrings. Requires jq to be installed and LocalAI running.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Get all information about models with a name that contains \"replit\"\ncurl http://localhost:8080/models/available | jq '.[] | select(.name | contains(\"replit\"))'\n\n# Get the binary name of all local models (not hosted on Hugging Face)\ncurl http://localhost:8080/models/available | jq '.[] | .name | select(contains(\"localmodels\"))'\n\n# Get all of the model URLs that contains \"orca\"\ncurl http://localhost:8080/models/available | jq '.[] | .urls | select(. != null) | add | select(contains(\"orca\"))'\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalAI Hardware and Diffusers Pipeline (YAML-like)\nDESCRIPTION: This configuration snippet sets options for a LocalAI instance. It enables `f16` precision and `cuda` acceleration globally. Within the `diffusers` section, it specifies `VideoDiffusionPipeline` as the `pipeline_type` and explicitly enables `cuda` for the diffuser operations. This configuration is likely used to control how models are loaded and executed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\n# Force CPU usage\nf16: true\ncuda: true\ndiffusers:\n  pipeline_type: VideoDiffusionPipeline\n  cuda: true\n```\n\n----------------------------------------\n\nTITLE: Installing Intel PyTorch Extensions and Dependencies\nDESCRIPTION: Requirements file specifying Intel-optimized PyTorch packages and extensions. Includes Intel Extension for PyTorch, base PyTorch with C++11 ABI, oneCCL binding for PyTorch, and Hugging Face transformers library. Uses Intel's custom package index for XPU-enabled builds.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/kokoro/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\ntorch==2.3.1+cxx11.abi\noneccl_bind_pt==2.3.100+xpu\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Installing a Model from the Gallery by ID - LocalAI - Bash/JSON\nDESCRIPTION: Provides an HTTP POST example to install a model from the configured gallery by specifying the model's 'id' in the JSON payload. The call is made to the /models/apply endpoint via curl, with headers and data set appropriately. The 'id' format supports both explicit repository and name, and output is a job UUID for tracking installation progress. Requires LocalAI running, and bash with curl installed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI=http://localhost:8080\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"id\": \"localai@bert-embeddings\"\n   }'  \n```\n\n----------------------------------------\n\nTITLE: Installing LocalAI using Helm Chart (Bash/Helm)\nDESCRIPTION: These commands demonstrate how to install LocalAI on Kubernetes using the official Helm chart. It involves adding the chart repository (`helm repo add`), updating local repository information (`helm repo update`), optionally fetching and customizing configuration via `values.yaml` (`helm show values`), and finally installing the chart using `helm install`. Requires the Helm v3 client and `kubectl` configured for the target cluster.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/kubernetes.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install the helm repository\nhelm repo add go-skynet https://go-skynet.github.io/helm-charts/\n# Update the repositories\nhelm repo update\n# Get the values\nhelm show values go-skynet/local-ai > values.yaml\n\n# Edit the values value if needed\n# vim values.yaml ...\n\n# Install the helm chart\nhelm install local-ai go-skynet/local-ai -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Pulling Prebuilt LocalAI Docker Image for Nvidia ARM64 - Bash\nDESCRIPTION: This Bash command pulls a prebuilt LocalAI Docker image optimized for Nvidia ARM64 devices from quay.io. It requires Docker to be installed and configured. No inputs are required besides Docker; it outputs a local Docker image ready to be run. This provides a convenient alternative to building the image from source.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/reference/nvidia-l4t.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core\n```\n\n----------------------------------------\n\nTITLE: Installing a Model with Base Config and Custom Files - LocalAI - Bash/JSON\nDESCRIPTION: Demonstrates a command to install a model using a base configuration YAML and providing custom file download URIs, SHA256 hashes, and desired filenames. This approach is used for advanced model installation scenarios where users want to supply custom models or weights. Input is in JSON, and the curl POST is sent to /models/apply.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"url\": \"github:mudler/LocalAI/gallery/base.yaml@master\",\n     \"name\": \"model-name\",\n     \"files\": [\n        {\n            \"uri\": \"<URL>\",\n            \"sha256\": \"<SHA>\",\n            \"filename\": \"model\"\n        }\n     ]\n   }'\n```\n\n----------------------------------------\n\nTITLE: Preloading Whisper Model via CLI Argument (Bash)\nDESCRIPTION: Passes the model configuration as a JSON string directly to the `local-ai` executable using the `--preload-models` argument. This instructs LocalAI to download and load the Whisper base model from the specified gallery URL, naming it `whisper-1`, upon startup.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai --preload-models '[{\"url\": \"github:mudler/LocalAI/gallery/whisper-base.yaml@master\", \"name\": \"whisper-1\"}]'\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Requirements for PyTorch with CUDA Support (requirements file)\nDESCRIPTION: This requirements file outlines the necessary Python package dependencies for a project leveraging PyTorch and torchaudio (both CUDA 11.8-enabled), transformers, accelerate, and coqui-tts. The first line sets an extra package index for fetching CUDA-enabled wheels. Each dependency is version-pinned to ensure environment consistency and compatibility, and no other metadata or configuration is included. This file should be used with pip's -r option for environment installation; dependencies require a CUDA 11.8-compatible GPU and drivers on the host system.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/requirements-cublas11.txt#2025-04-23_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cu118\ntorch==2.4.1+cu118\ntorchaudio==2.4.1+cu118\ntransformers==4.48.3\naccelerate\ncoqui-tts\n```\n\n----------------------------------------\n\nTITLE: Listing Available Models - LocalAI - Bash\nDESCRIPTION: Shows how to retrieve a list of all available models from a running LocalAI instance using curl. The command hits the '/models/available' API endpoint on the local server, returning JSON data. Requires LocalAI to be running and accessible at the specified URL.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/models/available\n```\n\n----------------------------------------\n\nTITLE: TTS Request using Cloned Voice with Vall-E-X in Bash\nDESCRIPTION: Shows how to generate speech using a pre-configured cloned voice with the Vall-E-X backend. The `curl` command specifies the model name `cloned-voice` (defined in a YAML config file) and the input text. Requires the corresponding YAML configuration to be loaded by LocalAI. Output is piped to `aplay`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{         \n     \"model\": \"cloned-voice\",\n     \"input\":\"Hello!\"\n   }' | aplay\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for ML Project\nDESCRIPTION: Lists required Python packages for machine learning operations including transformers library, accelerate package for optimization, and PyTorch with specific version 2.4.1.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/exllama2/requirements-cpu.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers\naccelerate\ntorch==2.4.1\n```\n\n----------------------------------------\n\nTITLE: Installing a Model with Additional Files - LocalAI - Bash/JSON\nDESCRIPTION: Details how to use the 'files' parameter in the installation request to download extra resources required for a model. The example includes custom URLs, hashes, and filenames, improving reproducibility and custom installations. The payload is a JSON object sent with curl to the /models/apply endpoint.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI=http://localhost:8080\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"url\": \"<MODEL_CONFIG_FILE>\",\n     \"name\": \"<MODEL_NAME>\",\n     \"files\": [\n        {\n            \"uri\": \"<additional_file_url>\",\n            \"sha256\": \"<additional_file_hash>\",\n            \"filename\": \"<additional_file_name>\"\n        }\n     ]\n   }'  \n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Requirements with ROCm PyTorch\nDESCRIPTION: This configuration specifies Python package dependencies, likely intended for use with pip installer. It instructs the package installer to check an additional index URL specifically for PyTorch packages compiled with ROCm 6.0 support (for AMD GPUs). It lists specific versions for `torch` and `torchvision` with ROCm, along with other libraries like `diffusers`, `transformers`, `accelerate`, `compel`, `peft`, `sentencepiece`, and `optimum-quanto`, commonly used in AI/ML workflows.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/diffusers/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip requirements file\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\ntorch==2.3.1+rocm6.0\ntorchvision==0.18.1+rocm6.0\ndiffusers\nopencv-python\ntransformers\naccelerate\ncompel\npeft\nsentencepiece\noptimum-quanto\n```\n\n----------------------------------------\n\nTITLE: Similarity Search (Find) via LocalAI Stores HTTP API - shell\nDESCRIPTION: This example code shows how to perform a similarity search on stored embeddings using the /stores/find endpoint. The cURL command submits a POST request with a JSON body including 'topk', the number of desired results, and 'key', the vector to search against. The API responds with keys, values, and similarity scores (with 1.0 being maximum similarity), sorted from most to least similar. The operation does not modify stored data and requires all keys to have the same length and be normalized for optimal results.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/stores.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST http://localhost:8080/stores/find \n     -H \"Content-Type: application/json\" \\\n     -d '{\"topk\": 2, \"key\": [0.2, 0.1]}'\n```\n\n----------------------------------------\n\nTITLE: Manually Starting LocalAI Server in P2P Mode (Bash)\nDESCRIPTION: Initiates the main LocalAI server process in peer-to-peer mode using `./local-ai run --p2p`. This is the first step in the manual setup example for worker-based distributed inference. A P2P token will be generated, which needs to be retrieved (e.g., from the WebUI or API) for connecting workers.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/distributed_inferencing.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./local-ai run --p2p\n# Get the token in the Swarm section of the WebUI\n```\n\n----------------------------------------\n\nTITLE: Specifying Project Dependencies for Transformers and Torch in Python\nDESCRIPTION: This requirements snippet defines the core Python packages needed for running transformer models, including the specific torch version and optional extras for rerankers with transformers integration. No external code is present, only package names to be interpreted by pip. Dependencies include 'transformers', 'accelerate', 'torch==2.4.1', and 'rerankers[transformers]'. Expected inputs are pip-compatible requirement files; output is a pre-configured package environment suitable for machine learning workloads. Constraints include pip environment compatibility and Python version constraints set by upstream packages.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/rerankers/requirements-cpu.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers\naccelerate\ntorch==2.4.1\nrerankers[transformers]\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Dependencies with CUDA Support\nDESCRIPTION: Requirements file for installing PyTorch with CUDA 11.8 support along with necessary machine learning libraries. Includes accelerate for optimization, PyTorch 2.4.1 with CUDA support, transformers for working with ML models, and bitsandbytes for quantization.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements-cublas11.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cu118\naccelerate\ntorch==2.4.1+cu118\ntransformers\nbitsandbytes\n```\n\n----------------------------------------\n\nTITLE: Calling LocalAI Chat Completions with Tools via Curl\nDESCRIPTION: Provides a curl command example to interact with the LocalAI chat completions endpoint (/v1/chat/completions). It sends a JSON payload containing the model name, user message, and the definition of available tools (functions) to the LocalAI server running on localhost:8080. This demonstrates the raw API request structure for function calling.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n  \"model\": \"gpt-4\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in Beijing now?\"}],\n  \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"Return the temperature of the specified region specified by the user\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"User specified region\"\n                        },\n                        \"unit\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"temperature unit\"\n                        }\n                    },\n                    \"required\": [\"location\"]\n                }\n            }\n        }\n    ],\n    \"tool_choice\":\"auto\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating a Default Prompt Template for Alpaca - YAML\nDESCRIPTION: This YAML prompt template defines the format for instructions and responses following Alpaca conventions, leveraging Mustache-style placeholders ({{.Input}}) for dynamic data injection into the template. Dependencies are the LocalAI model serving engine and file-based prompt template support with variable interpolation. Key parameter is .Input, which is substituted at runtime. The input is a prompt template file (e.g., foo.bin.tmpl), and the output is a rendered string for the model to use. Limitations: must use correct placeholders and file association.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nThe below instruction describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{{.Input}}\n\n### Response:\n```\n\n----------------------------------------\n\nTITLE: Setting Keys and Values via LocalAI Stores HTTP API - shell\nDESCRIPTION: This cURL command demonstrates how to store arrays of vector keys and corresponding string values in the LocalAI Stores backend using the /stores/set endpoint. The user must send a POST request with a JSON body containing equally shaped floating point key arrays and their associated values. No authentication is shown or required by default. On success, the server returns HTTP 200 OK with no response body. Setting the same key again overwrites its value.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/stores.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST http://localhost:8080/stores/set \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"keys\": [[0.1, 0.2], [0.3, 0.4]], \"values\": [\"foo\", \"bar\"]}'\n```\n\n----------------------------------------\n\nTITLE: Build Using Specific llama.cpp Version - Bash\nDESCRIPTION: Builds LocalAI with a specific version or SHA of the llama.cpp backend by setting CPPLLAMA_VERSION before build. Ensures compatibility with particular releases or branches. Replace <sha> with the desired commit identifier.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nCPPLLAMA_VERSION=<sha> make build\n```\n\n----------------------------------------\n\nTITLE: Installing a TTS Voice Model via Model Gallery REST API - Bash/JSON\nDESCRIPTION: Provides an example of installing a text-to-speech (TTS) voice model from the model gallery using the LocalAI REST API. The curl command targets the /models/apply endpoint on port 8080 with a POST request, passing the desired voice model ID in the JSON data. This snippet assumes that LocalAI is running with TTS support enabled, and the model gallery contains the requested voice ID.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/whats-new.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/models/apply -H \"Content-Type: application/json\" -d '{ \"id\": \"model-gallery@voice-en-us-kathleen-low\" }'\n```\n\n----------------------------------------\n\nTITLE: Preloading Models on Start - LocalAI - Bash/JSON/YAML\nDESCRIPTION: Illustrates several methods for preloading models at LocalAI startup, including setting the PRELOAD_MODELS environment variable with a JSON array, passing the models as command arguments, or listing them in a YAML file. Each method requires specific format and is used to preload model configurations automatically on service initialization.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nPRELOAD_MODELS='[{\"url\": \"<MODEL_URL>\"}]'\n```\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai --preload-models '[{\"url\": \"github:mudler/LocalAI/gallery/stablediffusion.yaml@master\"}]'\n```\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai --preload-models-config \"/path/to/yaml\"\n```\n\n----------------------------------------\n\nTITLE: Edit Endpoint Prompt Template for Alpaca-based Models - YAML\nDESCRIPTION: This YAML template is specifically for the edit endpoint, introducing both Instruction and Input context via Mustache-style placeholders ({{.Instruction}}, {{.Input}}). It requires the LocalAI core engine and support for runtime template variable substitution. Key parameters are .Instruction and .Input, substituted with the user’s request and contextual data. Input is a YAML-formatted template file; output is the rendered instruction for the model. The main constraint is adherence to correct placeholder names.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{{.Instruction}}\n\n### Input:\n{{.Input}}\n\n### Response:\n```\n\n----------------------------------------\n\nTITLE: Retrieving Keys and Values via LocalAI Stores HTTP API - shell\nDESCRIPTION: This code snippet shows how to retrieve values for given vector key arrays using the /stores/get endpoint with a POST cURL request. The payload requires a 'keys' field containing arrays of 32-bit floats. The API returns a JSON object with arrays of keys and their corresponding values. Keys not present in the store are omitted from the result. No authentication or additional options are used.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/stores.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST http://localhost:8080/stores/get \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"keys\": [[0.1, 0.2]]}'\n```\n\n----------------------------------------\n\nTITLE: Mounting Local Directory for Models in LocalAI Docker Run (Bash)\nDESCRIPTION: Illustrates how to persist LocalAI models by mounting a local directory (e.g., `$PWD/models`) to the `/build/models` path inside the container using the `-v` flag with `docker run`. This command starts the CPU AIO image.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/container-images.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8080:8080 --name local-ai -ti -v $PWD/models:/build/models localai/localai:latest-aio-cpu\n```\n\n----------------------------------------\n\nTITLE: Performing Audio Transcription with LocalAI API using cURL\nDESCRIPTION: Sends a POST request to the LocalAI `/v1/audio/transcriptions` endpoint to transcribe an audio file. It uses `multipart/form-data` content type to upload the audio file (`gb1.ogg` downloaded previously) and specifies the `whisper-1` model for transcription. This mimics the OpenAI Audio Transcription API.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/try-it-out.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/audio/transcriptions \\\n    -H \"Content-Type: multipart/form-data\" \\\n    -F file=\"@$PWD/gb1.ogg\" -F model=\"whisper-1\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for LocalAI with Intel Optimizations\nDESCRIPTION: This snippet lists the required Python packages for the LocalAI project, including Intel-specific PyTorch extensions and optimizations. It uses an extra index URL for Intel's PyTorch extensions and specifies versions for torch, oneCCL bindings, and Optimum with OpenVINO support.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/common/template/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\ntorch==2.3.1+cxx11.abi\noneccl_bind_pt==2.3.100+xpu\noptimum[openvino]\n```\n\n----------------------------------------\n\nTITLE: Disabling MMAP in LocalAI Model Config (YAML)\nDESCRIPTION: Instructs LocalAI to disable memory mapping (`mmap`) for model loading by setting `mmap: false` (implied, opposite of default) in the model configuration file. This forces the model to be loaded entirely into RAM, which can improve performance on systems using slow HDDs at the cost of higher memory usage.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmmap\n```\n\n----------------------------------------\n\nTITLE: Installing a Gallery Model via LocalAI CLI\nDESCRIPTION: Shows how to download and install a specific model (`hermes-2-theta-llama-3-8b`) from the LocalAI gallery using the `local-ai models install` command, without starting the LocalAI server.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai models install hermes-2-theta-llama-3-8b\n```\n\n----------------------------------------\n\nTITLE: Testing Bert Embeddings Endpoint (Bash)\nDESCRIPTION: Sends a request to the `/v1/embeddings` endpoint to generate embeddings for the input text \"Test\" using the previously applied model named `text-embedding-ada-002`. Requires the `$LOCALAI` environment variable pointing to the LocalAI instance, `curl`, and the specified model to be loaded.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI=http://localhost:8080\ncurl $LOCALAI/v1/embeddings -H \"Content-Type: application/json\" -d '{\n    \"input\": \"Test\",\n    \"model\": \"text-embedding-ada-002\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Build and Run LocalAI Docker Image - Bash\nDESCRIPTION: Contains the core commands to build a LocalAI Docker image from a Dockerfile in the current directory and run a container instance. Assumes Docker is installed and appropriately configured. Offers a base template for container-based deployments.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# build the image\ndocker build -t localai .\ndocker run localai\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI via Docker - Docker\nDESCRIPTION: This snippet shows how to launch LocalAI using the Docker container platform. It runs the localai/localai:latest-aio-cpu image, mapping port 8080 from the container to the host, and assigns the container a name. Dependencies include Docker Engine installed on the host system. This command initializes LocalAI for immediate use, exposing its API at localhost:8080, and is suitable for quick testing or deployment scenarios where containerization is preferred.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/overview.md#2025-04-23_snippet_1\n\nLANGUAGE: docker\nCODE:\n```\ndocker run -p 8080:8080 --name local-ai -ti localai/localai:latest-aio-cpu\n```\n\n----------------------------------------\n\nTITLE: Configuring Prompt Caching for llama.cpp Models in LocalAI (YAML)\nDESCRIPTION: This YAML configuration snippet shows how to enable prompt caching for `llama.cpp` models within LocalAI, a feature added in v1.16.0. The `prompt_cache_path` specifies the file (relative to the models directory) to save/load the cache, and `prompt_cache_all: true` forces the cache to be enabled consistently. This configuration should be added to the model's YAML file.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/whats-new.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# Enable prompt caching\n\n# This is a file that will be used to save/load the cache. relative to the models directory.\nprompt_cache_path: \"alpaca-cache\"\n\n# Always enable prompt cache\nprompt_cache_all: true\n```\n\n----------------------------------------\n\nTITLE: Tracking Model Installation Job Status - LocalAI - Bash\nDESCRIPTION: Demonstrates a Bash script that waits for a model installation job to complete by polling the job status endpoint using the returned UUID. Parses API responses using jq and sleeps between status checks. Requires curl, jq, and basic Bash scripting support.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nresponse=$(curl -s http://localhost:8080/models/apply -H \"Content-Type: application/json\" -d '{\"url\": \"$model_url\"}')\n\njob_id=$(echo \"$response\" | jq -r '.uuid')\n\nwhile [ \"$(curl -s http://localhost:8080/models/jobs/\"$job_id\" | jq -r '.processed')\" != \"true\" ]; do \n  sleep 1\ndone\n\necho \"Job completed\"\n```\n\n----------------------------------------\n\nTITLE: Setting Model Threads via Command Line Argument (Shell)\nDESCRIPTION: Specifies the number of threads a model should use via the `--threads` command-line argument when launching LocalAI. It is recommended to set this value to match the number of physical CPU cores (e.g., `<= 4` for a 4-core CPU) to avoid performance degradation due to CPU overbooking.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n--threads\n```\n\n----------------------------------------\n\nTITLE: Run LocalAI Docker Container with Custom Build Flags - Bash\nDESCRIPTION: Shows how to launch the LocalAI Docker container with custom environment variables to trigger a rebuild with the disabled CPU instructions (using CMAKE_ARGS and REBUILD). It exposes port 8080, sets model and thread parameters, mounts the models directory, and uses the latest LocalAI image as the container base.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker run  quay.io/go-skynet/localai\ndocker run --rm -ti -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -e REBUILD=true -e CMAKE_ARGS=\"-DGGML_F16C=OFF -DGGML_AVX512=OFF -DGGML_AVX2=OFF -DGGML_AVX=OFF -DGGML_FMA=OFF\" -v $PWD/models:/models quay.io/go-skynet/local-ai:latest\n```\n\n----------------------------------------\n\nTITLE: Build with OpenBLAS Acceleration - Bash\nDESCRIPTION: Compiles LocalAI using OpenBLAS for software acceleration. The BUILD_TYPE variable is set to 'openblas'. Requires OpenBLAS to be installed and appropriately configured in the build environment.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nmake BUILD_TYPE=openblas build\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies with pip Requirements (CUDA 11.8)\nDESCRIPTION: This requirements file lists Python packages needed for the project. It specifies exact versions of 'torch' and 'torchaudio' compiled with CUDA 11.8, fetched from a specific PyTorch download URL using the '--extra-index-url' flag. It also includes the 'transformers' and 'accelerate' libraries. This format is intended for use with 'pip install -r <filename>' to set up the Python environment, ensuring compatibility with NVIDIA CUDA 11.8.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/requirements-cublas11.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cu118\ntorch==2.4.1+cu118\ntorchaudio==2.4.1+cu118\ntransformers\naccelerate\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with LocalAI API using cURL\nDESCRIPTION: Sends a POST request to the LocalAI `/embeddings` endpoint to generate a vector representation (embedding) for a given text input. It specifies the input text string and the `text-embedding-ada-002` model to use for creating the embedding. This mimics the OpenAI Embeddings API.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/try-it-out.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/embeddings \\\n    -X POST -H \"Content-Type: application/json\" \\\n    -d '{ \n        \"input\": \"Your text string goes here\", \n        \"model\": \"text-embedding-ada-002\"\n      }'\n```\n\n----------------------------------------\n\nTITLE: Checking Model Job Status via API (Bash)\nDESCRIPTION: Demonstrates how to query the status of a specific model installation job using `curl`. The request targets the `/models/jobs/<JOB_ID>` endpoint, where `<JOB_ID>` is the UUID returned by the `/models/apply` request. Requires `curl` and the job ID.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/models/jobs/<JOB_ID>\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI with Debug Mode Enabled via Environment Variable (Shell)\nDESCRIPTION: Executes the LocalAI application with the `DEBUG` environment variable set to `true`. This enables verbose logging, providing detailed information about the process, including token inference speed statistics, which is useful for troubleshooting performance issues.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nDEBUG=true\n```\n\n----------------------------------------\n\nTITLE: Overriding Model Configuration via API (Bash)\nDESCRIPTION: Uses `curl` to send a POST request to the `/models/apply` endpoint of a LocalAI instance (specified by the `$LOCALAI` variable). This request applies a model configuration from `<MODEL_CONFIG_FILE>`, names it `<MODEL_NAME>`, and overrides specific settings like the backend (`llama`) and precision (`f16`). Requires a running LocalAI instance and `curl`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI=http://localhost:8080\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"url\": \"<MODEL_CONFIG_FILE>\",\n     \"name\": \"<MODEL_NAME>\",\n     \"overrides\": {\n        \"backend\": \"llama\",\n        \"f16\": true,\n        ...\n     }\n   }'\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Python Client with LocalAI Functions\nDESCRIPTION: Demonstrates how to use the official OpenAI Python client to interact with a LocalAI instance supporting functions. It initializes the client pointing to the LocalAI endpoint, defines a message list and a tool specification ('get_current_weather'), and then calls the chat completions endpoint with these tools.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# ...\n# Send the conversation and available functions to GPT\nmessages = [{\"role\": \"user\", \"content\": \"What is the weather like in Beijing now?\"}]\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Return the temperature of the specified region specified by the user\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"User specified region\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"temperature unit\"\n                    },\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=\"test\",\n    base_url=\"http://localhost:8080/v1/\"\n)\n\nresponse =client.chat.completions.create(\n    messages=messages,\n    tools=tools,\n    tool_choice =\"auto\",\n    model=\"gpt-4\",\n)\n#...\n```\n\n----------------------------------------\n\nTITLE: Running All-In-One Tests for LocalAI Docker Images\nDESCRIPTION: Commands for building the LocalAI Docker image, creating the corresponding AIO image, and running end-to-end tests. This workflow ensures that most endpoints function correctly in the containerized environment.\nSOURCE: https://github.com/mudler/localai/blob/master/CONTRIBUTING.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Build the LocalAI docker image\nmake DOCKER_IMAGE=local-ai docker\n\n# Build the corresponding AIO image\nBASE_IMAGE=local-ai DOCKER_AIO_IMAGE=local-ai-aio:test make docker-aio\n\n# Run the AIO e2e tests\nLOCALAI_IMAGE_TAG=test LOCALAI_IMAGE=local-ai-aio make run-e2e-aio\n```\n\n----------------------------------------\n\nTITLE: Enabling Streaming Output in LocalAI API Request (JSON)\nDESCRIPTION: This JSON snippet, typically used within the data payload of a `curl` request to the LocalAI API, sets the `stream` parameter to `true`. This instructs the API to send back results incrementally as they are generated, rather than waiting for the entire response, which helps in assessing the real-time inference speed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"stream\": true\n```\n\n----------------------------------------\n\nTITLE: Setting the DEBUG Environment Variable for LocalAI Troubleshooting (Shell)\nDESCRIPTION: Sets the `DEBUG` environment variable to `true` in the shell environment before running LocalAI. When LocalAI is subsequently launched from this environment, it will run in debug mode, providing verbose output useful for diagnosing problems.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nDEBUG=true\n```\n\n----------------------------------------\n\nTITLE: Registering an External gRPC Backend (Local File) - Shell\nDESCRIPTION: This shell command registers a local file-based backend with LocalAI using the --external-grpc-backends CLI flag. Dependencies: LocalAI CLI supporting this option and a valid backend script file (.py or other). Parameters: backend name (my-awesome-backend), path to the backend file. Input: shell command; output: extended backend registered in LocalAI, available for API endpoints. Path must be correct and backend executable.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n./local-ai --debug --external-grpc-backends \"my-awesome-backend:/path/to/my/backend.py\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies with Requirements.txt in Python\nDESCRIPTION: This snippet serves as a requirements.txt file enumerating the Python libraries and their versions necessary for the machine learning project. Dependencies include 'torch' (PyTorch) for deep learning, 'transformers' for NLP models, 'numba' and 'llvmlite' for JIT compilation, as well as specialized libraries like 'bitsandbytes' and 'sentence-transformers'. Each line specifies one package, optionally pinning its version, which pip uses to install compatible dependencies. Inputs are interpreted by pip and there are no explicit outputs, but incorrect versions or missing libraries may lead to runtime or import errors.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/transformers/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntorch==2.4.1\\naccelerate\\nllvmlite==0.43.0\\nnumba==0.60.0\\ntransformers\\nbitsandbytes\\noutetts\\nsentence-transformers==3.4.1\n```\n\n----------------------------------------\n\nTITLE: Starting LocalAI with a Specific GGML Model using Bash\nDESCRIPTION: Shows how to use the `local-ai run` command to automatically download and start LocalAI with a specific model (`flux.1-dev-ggml`) from the Model Gallery. This command assumes the `local-ai` executable is available in the system's PATH.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai run flux.1-dev-ggml\n```\n\n----------------------------------------\n\nTITLE: TTS Request using Piper Backend in Bash\nDESCRIPTION: Shows how to generate speech using the Piper backend. The `curl` command specifies `\"backend\": \"piper\"` and the desired model file (`it-riccardo_fasol-x-low.onnx`). Requires LocalAI compiled with `GO_TAGS=tts` and the Piper model files placed in the `models` directory. The output audio is piped to `aplay`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{\n  \"model\":\"it-riccardo_fasol-x-low.onnx\",\n  \"backend\": \"piper\",\n  \"input\": \"Ciao, sono Ettore\"\n}' | aplay\n```\n\n----------------------------------------\n\nTITLE: Disabling Mirostat Sampling in LocalAI Model Config (YAML)\nDESCRIPTION: Sets the `mirostat` parameter to 0 within a LocalAI model configuration file. This disables Mirostat sampling, which can potentially improve inference speed but might affect the quality of the generated output. This setting is relevant when benchmarking against llama.cpp or optimizing purely for inference speed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmirostat: 0\n```\n\n----------------------------------------\n\nTITLE: Preloading Models via YAML Configuration - YAML\nDESCRIPTION: This YAML defines a list of models to preload at startup, each entry containing the model's definition URL and alias name. Dependencies include LocalAI support for PRELOAD_MODELS_CONFIG and YAML parsing. Parameters are url (pointing to the YAML model definition) and name (model alias in LocalAI). Input is a YAML file; output is an environment variable or CLI argument that triggers LocalAI to preload these models. The file must be accessible and formatted correctly.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n- url: https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml\n  name: gpt4all-j\n# ...\n```\n\n----------------------------------------\n\nTITLE: Registering an External gRPC Backend (Remote URI) - Shell\nDESCRIPTION: This shell command registers a network backend with LocalAI by specifying the backend name and a host:port URI. Prerequisites: LocalAI CLI with --external-grpc-backends support, external endpoint reachable. Parameters: backend name (my-awesome-backend), URI. Input: shell command; output: new backend available in LocalAI. Limitation: backend must be accessible over the network and conform to gRPC expectations.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n./local-ai --debug --external-grpc-backends \"my-awesome-backend:host:port\"\n```\n\n----------------------------------------\n\nTITLE: Build on Apple Silicon (Metal Acceleration) - Bash\nDESCRIPTION: Builds LocalAI with correct settings for Metal acceleration on Apple Silicon platforms. No explicit BUILD_TYPE is needed, as the build system auto-selects for mac. Users should ensure 'gpu_layers' and 'f16' model settings are correctly configured.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n\n# correct build type is automatically used on mac (BUILD_TYPE=metal)\n# Set `gpu_layers: 256` (or equal to the number of model layers) to your YAML model config file and `f16: true`\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for LocalAI\nDESCRIPTION: This snippet defines the Python package dependencies for the LocalAI project. It includes a custom PyTorch index URL for ROCm support, and lists torch and faster-whisper as required packages.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/faster-whisper/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\ntorch\nfaster-whisper\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Requirements for LocalAI\nDESCRIPTION: This snippet lists the Python package dependencies required for the LocalAI project, formatted for use with `pip install -r <filename>`. It pins specific versions for `transformers` (4.48.3) and `torch` (2.4.1), and includes `accelerate` (likely for Hugging Face model optimization) and `coqui-tts` (for text-to-speech capabilities). Installing these dependencies is a prerequisite for running the project.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/requirements-cpu.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntransformers==4.48.3\naccelerate\ntorch==2.4.1\ncoqui-tts\n```\n\n----------------------------------------\n\nTITLE: Custom Backend Dockerfile Configuration\nDESCRIPTION: Dockerfile example showing how to build a custom LocalAI image with diffusers backend integration\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_20\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM quay.io/go-skynet/local-ai:master-ffmpeg-core\n\nRUN make -C backend/python/diffusers\n\nENV EXTERNAL_GRPC_BACKENDS=\"diffusers:/build/backend/python/diffusers/run.sh\"\n```\n\n----------------------------------------\n\nTITLE: Rebuilding LocalAI with Custom CMake Flags for CPU Compatibility (Shell)\nDESCRIPTION: Executes the `make build` command while passing custom arguments to `cmake` via the `CMAKE_ARGS` environment variable. This specific example disables F16C, AVX512, AVX2, and FMA instruction sets during the compilation process. This is a common troubleshooting step to resolve 'SIGILL' errors encountered on CPUs that lack hardware support for these advanced instruction sets. This command is often preceded by setting `REBUILD=true`.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nCMAKE_ARGS=\"-DGGML_F16C=OFF -DGGML_AVX512=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF\" make build\n```\n\n----------------------------------------\n\nTITLE: Preparing the Python Environment for vllm via Makefile - Bash\nDESCRIPTION: This bash snippet uses make to set up the Python backend for vllm by running make in the backend/python/vllm directory. Dependencies: GNU make, Python, and all requirements/files specified in that directory's Makefile. Key parameter: directory path. Input: bash command; output: fully prepared Python environment for vllm. Limitation: directory and files must exist and make must be installed.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nmake -C backend/python/vllm\n```\n\n----------------------------------------\n\nTITLE: Creating Transformers Environment in LocalAI with Make\nDESCRIPTION: This command uses Make to create a separate environment for the transformers project in LocalAI. It likely sets up necessary dependencies and configurations for working with transformer models.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/transformers/README.md#2025-04-23_snippet_0\n\nLANGUAGE: makefile\nCODE:\n```\nmake transformers\n```\n\n----------------------------------------\n\nTITLE: Disable GGML CPU Flags During Build - Bash\nDESCRIPTION: Provides a build command in which CMAKE_ARGS disables several GGML CPU instruction-set flags, improving compatibility with older or less capable CPUs. Variables such as GGML_F16C, AVX512, AVX2, AVX, and FMA are turned off. Used when encountering binary incompatibilities or to ensure a highly portable build.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nCMAKE_ARGS=\"-DGGML_F16C=OFF -DGGML_AVX512=OFF -DGGML_AVX2=OFF -DGGML_AVX=OFF -DGGML_FMA=OFF\" make build\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and Transformers Dependencies\nDESCRIPTION: Configuration for installing PyTorch with CUDA 11.8 support and the Transformers library. Uses an extra index URL for PyTorch wheel packages and specifies exact version for PyTorch.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/kokoro/requirements-cublas11.txt#2025-04-23_snippet_0\n\nLANGUAGE: requirements.txt\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cu118\ntorch==2.4.1+cu118\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Starting the Fine-tuning Process with Axolotl and Accelerate in Bash\nDESCRIPTION: Shell command that initiates the model fine-tuning process. It uses `accelerate launch` to run the Axolotl training script (`axolotl.cli.train`) based on the parameters specified in the `axolotl.yaml` configuration file.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Fine-tune\naccelerate launch -m axolotl.cli.train axolotl.yaml\n```\n\n----------------------------------------\n\nTITLE: Example JSON Dataset Structure for Completion Fine-tuning\nDESCRIPTION: Shows the structure of a dataset file in the `completion` format for fine-tuning an instruction-following model using Axolotl. Each JSON object in the array contains a 'text' field holding the complete prompt and expected response, which serves as a single training example.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n {\n    \"text\": \"As an AI language model you are trained to reply to an instruction. Try to be as much polite as possible\\n\\n## Instruction\\n\\nWrite a poem about a tree.\\n\\n## Response\\n\\nTrees are beautiful, ...\",\n },\n {\n    \"text\": \"As an AI language model you are trained to reply to an instruction. Try to be as much polite as possible\\n\\n## Instruction\\n\\nWrite a poem about a tree.\\n\\n## Response\\n\\nTrees are beautiful, ...\",\n }\n]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for LocalAI\nDESCRIPTION: This snippet defines the required Python packages for the LocalAI project. It specifies PyTorch version 2.4.1, and includes the Transformers and Accelerate libraries without version constraints.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/exllama2/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch==2.4.1\ntransformers\naccelerate\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for LocalAI\nDESCRIPTION: This code snippet lists the required Python packages and their versions for the LocalAI project. It includes PyTorch, LLVM, Numba, Accelerate, Transformers, BitsAndBytes, OuteTTS, and Sentence Transformers. These libraries are essential for various machine learning and NLP tasks.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/transformers/requirements-cpu.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch==2.4.1\nllvmlite==0.43.0\nnumba==0.60.0\naccelerate\ntransformers\nbitsandbytes\noutetts\nsentence-transformers==3.4.1\n```\n\n----------------------------------------\n\nTITLE: Specifying PyTorch and Transformers Dependencies\nDESCRIPTION: This requirements file specifies PyTorch version 2.4.1 and the transformers library as dependencies for a machine learning project. This would typically be used with the pip package manager to install the required libraries.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/kokoro/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch==2.4.1\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Installing Axolotl and Flash Attention Dependencies using Bash\nDESCRIPTION: Shell commands to set up the fine-tuning environment. It clones the Axolotl repository, checks out a specific commit (0.3.0), installs Axolotl with optional 'flash-attn' and 'deepspeed' extras using pip, and then installs a specific pre-built wheel for flash-attention compatible with CUDA 11.7 and PyTorch 2.0.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install axolotl and dependencies\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl && pushd axolotl && git checkout 797f3dd1de8fd8c0eafbd1c9fdb172abd9ff840a && popd #0.3.0\npip install packaging\npushd axolotl && pip install -e '.[flash-attn,deepspeed]' && popd\n\n# https://github.com/oobabooga/text-generation-webui/issues/4238\npip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.0/flash_attn-2.3.0+cu117torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for LocalAI with ROCm Support\nDESCRIPTION: This snippet defines the Python package dependencies needed for LocalAI with AMD GPU acceleration via ROCm 6.0. It includes PyTorch with ROCm support, Hugging Face transformers and accelerate libraries, and rerankers with transformer capabilities.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/rerankers/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\ntransformers\naccelerate\ntorch==2.4.1+rocm6.0\nrerankers[transformers]\n```\n\n----------------------------------------\n\nTITLE: Running LocalAI AIO Images with Docker Run (Bash)\nDESCRIPTION: Demonstrates how to start the LocalAI All-in-One (AIO) container using the `docker run` command. It shows examples for both the CPU image and comments out commands for Nvidia GPU images (CUDA 11 and 12). The command maps port 8080, names the container `local-ai`, and runs it interactively (`-ti`).\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/container-images.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# CPU example\ndocker run -p 8080:8080 --name local-ai -ti localai/localai:latest-aio-cpu\n# For Nvidia GPUs:\n# docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-aio-gpu-nvidia-cuda-11\n# docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-aio-gpu-nvidia-cuda-12\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with ROCm Support Dependencies\nDESCRIPTION: Package requirements file that specifies PyTorch 2.4.1 with ROCm 6.0 support, TorchAudio, Transformers, and Accelerate libraries. Uses an additional PyTorch index URL for ROCm-specific wheel packages.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\ntorch==2.4.1+rocm6.0\ntorchaudio==2.4.1+rocm6.0\ntransformers\naccelerate\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: This snippet lists Python packages required for the project. It includes libraries for diffusion models (`diffusers`), computer vision (`opencv-python`), NLP (`transformers`, `sentencepiece`), model acceleration/optimization (`accelerate`, `optimum-quanto`), prompt engineering (`compel`), fine-tuning (`peft`), and the core deep learning framework (`torch` pinned to version 2.4.1). This list is likely used with `pip install -r requirements.txt`.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/diffusers/requirements-cpu.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndiffusers\nopencv-python\ntransformers\naccelerate\ncompel\npeft\nsentencepiece\ntorch==2.4.1\noptimum-quanto\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Dependencies for LocalAI\nDESCRIPTION: Specifies the required Python packages for a LocalAI project with PyTorch integration. Includes specific versions of torch and torchaudio (2.4.1), along with the transformers and accelerate libraries without version constraints.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntorch==2.4.1\ntorchaudio==2.4.1\ntransformers\naccelerate\n```\n\n----------------------------------------\n\nTITLE: Optional Dataset Pre-tokenization using Axolotl CLI in Bash\nDESCRIPTION: A shell command to optionally pre-tokenize the dataset defined in the `axolotl.yaml` configuration file using the Axolotl command-line interface. Pre-tokenization converts the text dataset into token IDs beforehand, which can significantly speed up the start of the fine-tuning process, especially beneficial for large datasets.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Optional pre-tokenize (run only if big dataset)\npython -m axolotl.cli.preprocess axolotl.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for LocalAI\nDESCRIPTION: Lists the required Python packages for running the LocalAI project. The file specifies transformers for natural language processing models, accelerate for optimized model training, torch 2.4.1 for deep learning capabilities, and rerankers with transformers support for ranking model outputs.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/rerankers/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers\naccelerate\ntorch==2.4.1\nrerankers[transformers]\n```\n\n----------------------------------------\n\nTITLE: Citation Metadata for LocalAI - BibTeX\nDESCRIPTION: This BibTeX snippet provides citation metadata for referencing the LocalAI project in academic or downstream works. It specifies fields such as author, title, year, publisher, journal, and a URL for the repository. Intended for use in LaTeX papers, documentation, or any academic context that supports BibTeX format.\nSOURCE: https://github.com/mudler/localai/blob/master/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{localai,\\n  author = {Ettore Di Giacinto},\\n  title = {LocalAI: The free, Open source OpenAI alternative},\\n  year = {2023},\\n  publisher = {GitHub},\\n  journal = {GitHub repository},\\n  howpublished = {\\url{https://github.com/go-skynet/LocalAI}},\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Model Gallery Sources - LocalAI - JSON\nDESCRIPTION: Demonstrates how to configure the GALLERIES environment variable for LocalAI using a JSON array, specifying custom model gallery sources with name and URL. This allows LocalAI to automatically index and make models from additional galleries available for installation. The JSON input must be properly formatted, and the environment variable must be set prior to running LocalAI.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\nGALLERIES=[{\"name\":\"<GALLERY_NAME>\", \"url\":\"<GALLERY_URL\"}]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for LocalAI\nDESCRIPTION: This snippet lists the required Python packages for the LocalAI project. It includes accelerate for faster computations, PyTorch 2.4.1 for deep learning capabilities, and transformers for working with various NLP models.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements-cpu.txt#2025-04-23_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\naccelerate\ntorch==2.4.1\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Installing Build Dependencies - Debian/Ubuntu - Bash\nDESCRIPTION: Installs the necessary packages for building LocalAI on Debian-based systems, including cmake, Go, gRPC development packages, make, and Python gRPC tools. Assumes apt is available and the user has sudo/apt privileges. Outputs installation progress for each package.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napt install cmake golang libgrpc-dev make protobuf-compiler-grpc python3-grpc-tools\n```\n\n----------------------------------------\n\nTITLE: Installing Packages with Conda\nDESCRIPTION: Commands for installing packages to a conda environment, including installation from the standard channels and from conda-forge. Shows both generic syntax and specific examples.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda install <your-package-name>\n\nconda install -c conda-forge <your package-name>\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Mode via Command-Line Argument in LocalAI (Shell)\nDESCRIPTION: Specifies the `--debug` flag when running the LocalAI command from the command line. This serves as an alternative method to setting the `DEBUG=true` environment variable for activating verbose debug output during troubleshooting.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n--debug\n```\n\n----------------------------------------\n\nTITLE: Defining Python ML Dependencies\nDESCRIPTION: Lists required Python packages for machine learning, including accelerate for hardware optimization, PyTorch 2.4.1 as the core ML framework, Transformers for working with transformer models, and bitsandbytes for quantization.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\naccelerate\ntorch==2.4.1\ntransformers\nbitsandbytes\n```\n\n----------------------------------------\n\nTITLE: TTS Request using Bark Backend (Default Voice) in Bash\nDESCRIPTION: Illustrates generating speech with the Bark backend using its default voice. The `curl` command sends the text \"Hello!\" to the `/tts` endpoint with `\"backend\": \"bark\"`. The output audio is piped to the `aplay` command for playback. Bark models are downloaded automatically on first use.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-to-audio.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/tts -H \"Content-Type: application/json\" -d '{         \n     \"backend\": \"bark\",\n     \"input\":\"Hello!\"\n   }' | aplay\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment Variables (Direct) - Bash\nDESCRIPTION: This bash example shows setting the REBUILD environment variable when running LocalAI in a Docker container via the --env flag. Dependencies: Docker, LocalAI image available locally. Key parameters: --env, name/value pairs for environment variables (e.g., REBUILD). Input: shell command; output: changed runtime settings in LocalAI. Limitation: requires Docker permissions and image present.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --env REBUILD=true localai\n```\n\n----------------------------------------\n\nTITLE: Set Compiler Flags for ROCm/Hipblas Build - Bash\nDESCRIPTION: Sets environment variables CGO_CFLAGS, CGO_CXXFLAGS, and CGO_LDFLAGS to add headers and linkage flags required for building LocalAI with ROCm/Hipblas. Paths assume standard ROCm installation locations on Arch Linux. This step ensures correct compilation and linking against GPU libraries.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nexport CGO_CFLAGS=\"-I/usr/include/opencv4\"\nexport CGO_CXXFLAGS=\"-I/usr/include/opencv4\"\nexport CGO_LDFLAGS=\"-L/opt/rocm/hip/lib -lamdhip64 -L/opt/rocm/lib -lOpenCL -L/usr/lib -lclblast -lrocblas -lhipblas -lrocrand -lomp -O3 --rtlib=compiler-rt -unwindlib=libgcc -lhipblas -lrocblas --hip-link\"\n```\n\n----------------------------------------\n\nTITLE: Installing Intel XPU-optimized PyTorch and AI Libraries\nDESCRIPTION: This requirements file specifies PyTorch packages optimized for Intel XPU hardware along with supporting AI libraries. It includes Intel extensions for PyTorch, diffusion model libraries, transformers, and optimization tools needed for running AI models on Intel hardware accelerators.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/diffusers/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\ntorch==2.3.1+cxx11.abi\ntorchvision==0.18.1+cxx11.abi\noneccl_bind_pt==2.3.100+xpu\noptimum[openvino]\nsetuptools\ndiffusers\nopencv-python\ntransformers\naccelerate\ncompel\npeft\nsentencepiece\noptimum-quanto\n```\n\n----------------------------------------\n\nTITLE: Preloading Models in LocalAI via Environment Variable (Bash)\nDESCRIPTION: This Bash command demonstrates setting the `PRELOAD_MODELS` environment variable to automatically download and configure the `gpt4all-j` model from the specified URL and make it available under the name `gpt-3.5-turbo` when starting the LocalAI API. This feature was introduced in version 1.16.0.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/whats-new.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nPRELOAD_MODELS=[{\"url\": \"github:go-skynet/model-gallery/gpt4all-j.yaml\", \"name\": \"gpt-3.5-turbo\"}]\n```\n\n----------------------------------------\n\nTITLE: Reinstall Dependencies, Clean and Rebuild (macOS) - Bash\nDESCRIPTION: Used for troubleshooting build errors on macOS by forcing reinstallation of core build dependencies with Homebrew, cleaning previous build artifacts, and rebuilding the project with GNU Make. Intended to resolve issues like package conflicts or incomplete builds.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# reinstall build dependencies\nbrew reinstall abseil cmake go grpc protobuf wget\n\nmake clean\n\nmake build\n```\n\n----------------------------------------\n\nTITLE: Referencing Standard and Extended LocalAI Docker Image Tags - Plaintext\nDESCRIPTION: This snippet lists the Docker image tags for LocalAI distributions as displayed in documentation tables. Each tag refers to an image containing specific components (such as 'ffmpeg' or no Python). No dependencies are required to use these tags directly; they are to be pasted into 'docker pull' or 'docker run' commands as needed. The placeholders (like '{{< version >}}') represent dynamically substituted version strings based on the documentation build context.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/container-images.md#2025-04-23_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:master`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:master`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:latest-cpu`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:latest-cpu`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:master-cublas-cuda11`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:master-cublas-cuda11`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:latest-gpu-nvidia-cuda-11`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:latest-gpu-nvidia-cuda-11`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-cublas-cuda11`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-cublas-cuda11`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-cublas-cuda11-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-cublas-cuda11-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-cublas-cuda11-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-cublas-cuda11-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:master-cublas-cuda12`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:master-cublas-cuda12`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:latest-gpu-nvidia-cuda-12`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:latest-gpu-nvidia-cuda-12`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-cublas-cuda12`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-cublas-cuda12`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-cublas-cuda12-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-cublas-cuda12-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-cublas-cuda12-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-cublas-cuda12-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:master-sycl-f16`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:master-sycl-f16`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:latest-gpu-intel-f16`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:latest-gpu-intel-f16`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-sycl-f16-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-sycl-f16-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-sycl-f16-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-sycl-f16-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-sycl-f16-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-sycl-f16-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:master-sycl-f32`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:master-sycl-f32`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:latest-gpu-intel-f32`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:latest-gpu-intel-f32`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-sycl-f32-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-sycl-f32-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-sycl-f32-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-sycl-f32-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-sycl-f32-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-sycl-f32-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:master-hipblas`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:master-hipblas`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:latest-gpu-hipblas`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:latest-gpu-hipblas`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-hipblas`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-hipblas`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-hipblas-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-hipblas-ffmpeg`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-hipblas-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-hipblas-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai: master-vulkan-ffmpeg-core `\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai: master-vulkan-ffmpeg-core `\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai: latest-vulkan-ffmpeg-core `\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai: latest-vulkan-ffmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-vulkan-fmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-vulkan-fmpeg-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:master-nvidia-l4t-arm64-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:master-nvidia-l4t-arm64-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:latest-nvidia-l4t-arm64-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:latest-nvidia-l4t-arm64-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`quay.io/go-skynet/local-ai:{{< version >}}-nvidia-l4t-arm64-core`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`localai/localai:{{< version >}}-nvidia-l4t-arm64-core`\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and Transformer Dependencies with CUDA Support\nDESCRIPTION: This requirements file specifies dependencies for running machine learning models with GPU support via CUDA 11.8. It includes PyTorch 2.4.1, Transformers, Accelerate, and rerankers libraries configured for CUDA-enabled environments.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/rerankers/requirements-cublas11.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cu118\ntransformers\naccelerate\ntorch==2.4.1+cu118\nrerankers[transformers]\n```\n\n----------------------------------------\n\nTITLE: Installing Packages with Pip in Conda Environment\nDESCRIPTION: Command for installing Python packages using pip within a conda environment. This is an alternative to using conda install when packages aren't available in conda channels.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install <your-package-name>\n```\n\n----------------------------------------\n\nTITLE: Activating Conda Environments (Conda 4.4+)\nDESCRIPTION: Command for activating a conda environment in conda version 4.4 or newer. Shows how to activate the 'autogptq' environment.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda activate autogptq\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for LocalAI\nDESCRIPTION: Specifies the required Python packages and their versions needed to run the LocalAI project. Includes setuptools for package management, grpcio for gRPC support, pillow for image processing, protobuf for serialization, and certifi for certificate handling.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/diffusers/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nsetuptools\ngrpcio==1.71.0\npillow\nprotobuf\ncertifi\n```\n\n----------------------------------------\n\nTITLE: Install Go Protobuf Tools - Bash\nDESCRIPTION: Installs the protoc-gen-go and protoc-gen-go-grpc binaries using the 'go install' command for use when compiling Go protobuf components after Go is set up. Libraries are pinned to specific versions or commit hashes for stability. Both commands must be run in an environment where Go is properly configured in the PATH.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@v1.34.2\ngo install google.golang.org/grpc/cmd/protoc-gen-go-grpc@1958fcbe2ca8bd93af633f11e97d44e567e945af\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Bark TTS in LocalAI\nDESCRIPTION: This requirements file specifies the necessary Python packages and their versions needed to run the Bark text-to-speech model in LocalAI. It includes bark version 0.1.5, grpcio version 1.71.0, protobuf (without version constraint), and certifi (without version constraint).\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbark==0.1.5\ngrpcio==1.71.0\nprotobuf\ncertifi\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Requirements for LocalAI\nDESCRIPTION: This text snippet lists the required Python packages and their versions for the LocalAI project. Dependencies include PyTorch (`torch`, `torchaudio`), Hugging Face `transformers`, `accelerate` for distributed training/inference, and `coqui-tts` for text-to-speech. These are typically installed using a package manager like pip (`pip install -r <filename>`).\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntorch==2.4.1\ntorchaudio==2.4.1\ntransformers==4.48.3\naccelerate\ncoqui-tts\n```\n\n----------------------------------------\n\nTITLE: Configuring gRPC Server Target in CMake\nDESCRIPTION: Sets up the 'grpc-server' target, including necessary package dependencies, protobuf and gRPC configurations. It handles platform-specific settings for macOS and defines custom commands for generating protobuf and gRPC source files.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/cpp/llama/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET grpc-server)\nset(CMAKE_CXX_STANDARD 17)\ncmake_minimum_required(VERSION 3.15)\nset(TARGET grpc-server)\nset(_PROTOBUF_LIBPROTOBUF libprotobuf)\nset(_REFLECTION grpc++_reflection)\n\nif (${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n    if (CMAKE_HOST_SYSTEM_PROCESSOR MATCHES \"arm64\")\n        set(HOMEBREW_DEFAULT_PREFIX \"/opt/homebrew\")\n    else()\n        set(HOMEBREW_DEFAULT_PREFIX \"/usr/local\")\n    endif()\n\n    link_directories(\"${HOMEBREW_DEFAULT_PREFIX}/lib\")\n    include_directories(\"${HOMEBREW_DEFAULT_PREFIX}/include\")\nendif()\n\nfind_package(absl CONFIG REQUIRED)\nfind_package(Protobuf CONFIG REQUIRED)\nfind_package(gRPC CONFIG REQUIRED)\n\nfind_program(_PROTOBUF_PROTOC protoc)\nset(_GRPC_GRPCPP grpc++)\nfind_program(_GRPC_CPP_PLUGIN_EXECUTABLE grpc_cpp_plugin)\n\ninclude_directories(${CMAKE_CURRENT_BINARY_DIR})\ninclude_directories(${Protobuf_INCLUDE_DIRS})\n\n# Proto file\nget_filename_component(hw_proto \"../../../../../../backend/backend.proto\" ABSOLUTE)\nget_filename_component(hw_proto_path \"${hw_proto}\" PATH)\n\n# Generated sources\nset(hw_proto_srcs \"${CMAKE_CURRENT_BINARY_DIR}/backend.pb.cc\")\nset(hw_proto_hdrs \"${CMAKE_CURRENT_BINARY_DIR}/backend.pb.h\")\nset(hw_grpc_srcs \"${CMAKE_CURRENT_BINARY_DIR}/backend.grpc.pb.cc\")\nset(hw_grpc_hdrs \"${CMAKE_CURRENT_BINARY_DIR}/backend.grpc.pb.h\")\n\nadd_custom_command(\n      OUTPUT \"${hw_proto_srcs}\" \"${hw_proto_hdrs}\" \"${hw_grpc_srcs}\" \"${hw_grpc_hdrs}\"\n      COMMAND ${_PROTOBUF_PROTOC}\n      ARGS --grpc_out \"${CMAKE_CURRENT_BINARY_DIR}\"\n        --cpp_out \"${CMAKE_CURRENT_BINARY_DIR}\"\n        -I \"${hw_proto_path}\"\n        --plugin=protoc-gen-grpc=\"${_GRPC_CPP_PLUGIN_EXECUTABLE}\"\n        \"${hw_proto}\"\n      DEPENDS \"${hw_proto}\")\n\n# hw_grpc_proto\nadd_library(hw_grpc_proto\n  ${hw_grpc_srcs}\n  ${hw_grpc_hdrs}\n  ${hw_proto_srcs}\n  ${hw_proto_hdrs} )\n\nadd_executable(${TARGET} grpc-server.cpp utils.hpp json.hpp)\ntarget_link_libraries(${TARGET} PRIVATE common llama myclip ${CMAKE_THREAD_LIBS_INIT} absl::flags hw_grpc_proto\n  absl::flags_parse\n  gRPC::${_REFLECTION}\n  gRPC::${_GRPC_GRPCPP}\n  protobuf::${_PROTOBUF_LIBPROTOBUF})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_11)\nif(TARGET BUILD_INFO)\n  add_dependencies(${TARGET} BUILD_INFO)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Go Not Installed Error Output for Hugo Modules - Console\nDESCRIPTION: This console output captures the error message shown when Hugo attempts to install modules but cannot find the Go programming language binary. This message is helpful for diagnosing missing development prerequisites. It informs the user to install Go before proceeding with Hugo module management.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#2025-04-23_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n➜ hugo server\n\nError: failed to download modules: binary with name \"go\" not found\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements\nDESCRIPTION: List of Python package dependencies with version specifications. Includes networking libraries like grpcio, data processing libraries like numpy and scipy, and utility packages like protobuf and setuptools.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/transformers/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\ngrpcio==1.71.0\nprotobuf\ncertifi\nsetuptools\nscipy==1.15.1\nnumpy>=2.0.0\n```\n\n----------------------------------------\n\nTITLE: Defining Python Build Dependencies\nDESCRIPTION: Lists the essential Python build dependencies needed when build isolation is disabled. These packages are required for basic build setup and package installation when PEP517 specifications are not followed.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements-install.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npackaging\nsetuptools\nwheel\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for LocalAI\nDESCRIPTION: This requirements file lists essential Python packages and their version constraints needed for the LocalAI project. It includes PyTorch for deep learning, faster-whisper for speech recognition, and various optimization libraries like accelerate, peft, and optimum-quanto for model optimization.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/faster-whisper/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch==2.4.1\nfaster-whisper\nopencv-python\naccelerate\ncompel\npeft\nsentencepiece\noptimum-quanto\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for LocalAI Project\nDESCRIPTION: This code snippet defines the required Python packages and their versions for the LocalAI project. It includes PyTorch with Intel extensions, LLVM, Numba, and various AI-related libraries. The dependencies are specified in a format suitable for use with pip, Python's package installer.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/transformers/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\ntorch==2.3.1+cxx11.abi\noneccl_bind_pt==2.3.100+xpu\noptimum[openvino]\nllvmlite==0.43.0\nnumba==0.60.0\nintel-extension-for-transformers\nbitsandbytes\noutetts\nsentence-transformers==3.4.1\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Docker Compose - Bash\nDESCRIPTION: This bash command instructs Docker Compose to build all defined services as specified in the docker-compose.yml file. It requires Docker Compose to be installed and should be run from the directory containing the configuration file. The build stage prepares images to be run locally or in development containers.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose build\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up LocalAI Development Environment\nDESCRIPTION: Commands for cloning the LocalAI repository, navigating to the project directory, and building the application. This is the initial setup required for developers to start contributing to the project.\nSOURCE: https://github.com/mudler/localai/blob/master/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/go-skynet/LocalAI.git\ncd LocalAI\nmake build\n./local-ai\n```\n\n----------------------------------------\n\nTITLE: Build LocalAI with gRPC Backend (from source) - Bash\nDESCRIPTION: Runs a Make build that sets the BUILD_GRPC_FOR_BACKEND_LLAMA variable to automatically build the necessary gRPC dependencies specifically for the Llama backend. This is intended for advanced or custom scenarios when building LocalAI from source with additional dependencies.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake ... BUILD_GRPC_FOR_BACKEND_LLAMA=true build\n```\n\n----------------------------------------\n\nTITLE: Listing Model Directory Contents in Shell\nDESCRIPTION: This shell command lists the contents of the 'examples/chatbot-ui/models' directory. It shows the typical file structure for LocalAI models, including model binaries (.bin), configuration files (.yaml), and prompt template files (.tmpl).\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbase ❯ ls -liah examples/chatbot-ui/models \n36487587 drwxr-xr-x 2 mudler mudler 4.0K May  3 12:27 .\n36487586 drwxr-xr-x 3 mudler mudler 4.0K May  3 10:42 ..\n36465214 -rw-r--r-- 1 mudler mudler   10 Apr 27 07:46 completion.tmpl\n36464855 -rw-r--r-- 1 mudler mudler   ?G Apr 27 00:08 luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin\n36464537 -rw-r--r-- 1 mudler mudler  245 May  3 10:42 gpt-3.5-turbo.yaml\n36467388 -rw-r--r-- 1 mudler mudler  180 Apr 27 07:46 chat.tmpl\n```\n\n----------------------------------------\n\nTITLE: Defining Pip Dependencies with Intel XPU Acceleration\nDESCRIPTION: This snippet defines the Python package requirements for a project using pip, suitable for installation via `pip install -r <filename>`. It specifies an extra index URL (`--extra-index-url`) for Intel's PyTorch extensions and lists dependencies like `intel-extension-for-pytorch`, `accelerate`, `torch`, `transformers`, `optimum[openvino]`, `setuptools`, `bitsandbytes`, and `oneccl_bind_pt`, with specific versions required for compatibility with Intel XPU hardware acceleration.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip requirements\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\naccelerate\ntorch==2.3.1+cxx11.abi\ntransformers\noptimum[openvino]\nsetuptools\nbitsandbytes\noneccl_bind_pt==2.3.100+xpu\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies for LocalAI's gRPC Integration\nDESCRIPTION: This requirements file specifies the Python dependencies needed for LocalAI's gRPC functionality. It includes the gRPC library pinned to version 1.71.0, the Protocol Buffers library for serialization, and the certifi package for certificate validation.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/rerankers/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngrpcio==1.71.0\nprotobuf\ncertifi\n```\n\n----------------------------------------\n\nTITLE: Testing gRPC Server in LocalAI\nDESCRIPTION: Command to run tests for the gRPC server component of LocalAI using make.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Illustrative Python Code for Diffusers Pipeline Invocation\nDESCRIPTION: A conceptual Python code snippet illustrating how parameters are passed to a diffusers pipeline object (`pipe`). It shows that options from the API call (like `prompt`, `size`) and additional parameters defined in the LocalAI model configuration file (like `cfg_scale`) are used when calling the pipeline.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/image-generation.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipe(\n    prompt=\"A cute baby sea otter\", # Options passed via API\n    size=\"256x256\", # Options passed via API\n    cfg_scale=6 # Additional parameter passed via configuration file\n)\n```\n\n----------------------------------------\n\nTITLE: Creating New Conda Environments with Python Version Specification\nDESCRIPTION: Commands to create a new empty conda environment with a specified Python version. Includes both a generic syntax example and a specific example creating an 'autogptq' environment with Python 3.11.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name <env-name> python=<your version> -y\n\nconda create --name autogptq python=3.11 -y\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with ROCm Support for LocalAI\nDESCRIPTION: This snippet defines the package requirements for the LocalAI project. It includes an extra index URL for PyTorch with ROCm 6.0 support and specifies the torch package as a dependency.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/common/template/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\ntorch\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for LocalAI\nDESCRIPTION: This snippet lists the required Python packages and their versions for the LocalAI project. It includes PyTorch and torchaudio with ROCm 6.0 support, transformers library, accelerate, and coqui-tts. The extra index URL is provided for PyTorch wheel downloads.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\ntorch==2.4.1+rocm6.0\ntorchaudio==2.4.1+rocm6.0\ntransformers==4.48.3\naccelerate\ncoqui-tts\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalAI Installation with Environment Variables - Bash\nDESCRIPTION: This snippet demonstrates how to pass custom environment variables to the LocalAI installation script by prefixing the installer command with variable assignments, allowing users to configure behavior such as selecting a specific version, setting an API key, or choosing installation options. Dependencies: shell supporting environment variable syntax (sh, bash), curl. The user replaces VAR and value with the desired environment variable and value pair to customize their installation.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/installer.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localai.io/install.sh | VAR=value sh\n```\n\n----------------------------------------\n\nTITLE: Regex for Extracting Function Calls from LLM Response\nDESCRIPTION: Provides an example regular expression designed to capture function calls from a language model's text output when grammar is disabled. This specific regex uses named capture groups 'function' (matching word characters for the name) and 'arguments' (matching everything within parentheses) to parse strings like 'function_name({ \\\"foo\\\": \\\"bar\\\"})'.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_5\n\nLANGUAGE: regex\nCODE:\n```\n(?P<function>\\w+)\\s*\\((?P<arguments>.*)\\)\n```\n\n----------------------------------------\n\nTITLE: Configuring Extra Index URL and Package Requirements with Pip Requirements - requirements\nDESCRIPTION: Specifies a pip requirements file for Python projects, adding an extra index URL to fetch ROCm-enabled PyTorch builds and declaring dependencies on a specific version of PyTorch (2.4.1 with ROCm 6.0) and the Transformers library. Dependencies are resolved via both the main and the specified PyTorch package index. This file is used with pip to set up the required development environment and must be placed in the project root. Input is handled by pip, output is the installed packages. Limitations: ROCm support requires compatible hardware and PyTorch's ROCm wheels.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/kokoro/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\ntorch==2.4.1+rocm6.0\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Activating Conda Environments (Pre-Conda 4.4)\nDESCRIPTION: Command for activating a conda environment in conda versions older than 4.4. Uses the 'source activate' syntax to activate the 'autogptq' environment.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsource activate autogptq\n```\n\n----------------------------------------\n\nTITLE: Running Services with Docker Compose - Bash\nDESCRIPTION: This snippet demonstrates launching the previously built Docker images and associated services using Docker Compose. It assumes images are already built, Docker is running, and the docker-compose.yml file is properly configured. The command runs all services and mounts the docsy-example project volume for live development.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements for exllama2 Installation\nDESCRIPTION: A requirements file that lists wheel and setuptools packages needed for exllama2 installation. These are specified to ensure proper installation when using uv pip install commands with --no-build-isolation flag.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/exllama2/requirements-install.txt#2025-04-23_snippet_0\n\nLANGUAGE: plain text\nCODE:\n```\nwheel\nsetuptools\n```\n\n----------------------------------------\n\nTITLE: Querying Hugo Module Graph - Bash\nDESCRIPTION: This bash snippet demonstrates invoking the `hugo mod graph` command to print the dependency graph of Hugo modules in use. Required dependencies include Hugo (preferably the extended version). The command outputs the collection time and the dependency tree, which is useful for debugging and ensuring correct module versions are loaded. There are no parameters; the output is displayed in the console.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhugo mod graph\nhugo: collected modules in 566 ms\nhugo: collected modules in 578 ms\ngithub.com/google/docsy-example github.com/google/docsy@v0.5.1-0.20221017155306-99eacb09ffb0\ngithub.com/google/docsy-example github.com/google/docsy/dependencies@v0.5.1-0.20221014161617-be5da07ecff1\ngithub.com/google/docsy/dependencies@v0.5.1-0.20221014161617-be5da07ecff1 github.com/twbs/bootstrap@v4.6.2+incompatible\ngithub.com/google/docsy/dependencies@v0.5.1-0.20221014161617-be5da07ecff1 github.com/FortAwesome/Font-Awesome@v0.0.0-20220831210243-d3a7818c253f\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for LocalAI\nDESCRIPTION: This snippet defines the required Python packages for the LocalAI project. It specifies torch version 2.4.1 and the transformers package without a specific version.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/kokoro/requirements-cpu.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch==2.4.1\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Installing NodeJS Dependencies for SCSS Processing - Bash\nDESCRIPTION: This bash command installs the node modules required for PostCSS, enabling SCSS editing support in the Hugo site build process. It assumes NodeJS and npm are already installed globally. The command should be run from the repository root directory and will create or update the 'node_modules' folder and package-lock.json file with dependencies from package.json, preparing the environment for SCSS processing.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Configuring CLIP Library Target in CMake\nDESCRIPTION: Sets up the 'myclip' target for building the CLIP library. It specifies source files, include directories, and links necessary dependencies. The configuration also handles platform-specific compiler options.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/cpp/llama/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET myclip)\nadd_library(${TARGET} clip.cpp clip.h clip-impl.h llava.cpp llava.h)\ninstall(TARGETS ${TARGET} LIBRARY)\ntarget_include_directories(myclip PUBLIC .)\ntarget_include_directories(myclip PUBLIC ../..)\ntarget_include_directories(myclip PUBLIC ../../common)\ntarget_link_libraries(${TARGET} PRIVATE common ggml llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_11)\nif (NOT MSVC)\n    target_compile_options(${TARGET} PRIVATE -Wno-cast-qual) # stb_image.h\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying PyTorch Dependencies with Version Constraints\nDESCRIPTION: This requirements file lists the necessary Python packages for a PyTorch-based project. It includes the Transformers and Accelerate libraries without version constraints, while pinning PyTorch and TorchAudio to specific versions (2.4.1) to ensure compatibility and consistent behavior.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/requirements-cpu.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers\naccelerate\ntorch==2.4.1\ntorchaudio==2.4.1\n```\n\n----------------------------------------\n\nTITLE: Configuring vllm Model for Text Generation in YAML\nDESCRIPTION: This YAML configuration sets up a vllm model for text generation. It specifies the model name, backend type, and various parameters such as temperature, top_p, and max_tokens for controlling the text generation process.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements-after.txt#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: vllm\nbackend: vllm\nmodel: facebook/opt-125m\nparameters:\n  temperature: 0.7\n  top_p: 0.95\n  max_tokens: 128\n```\n\n----------------------------------------\n\nTITLE: Transcribing Example Audio with Real Files in LocalAI - Bash\nDESCRIPTION: This Bash snippet demonstrates obtaining an audio file via wget and submitting it for transcription using the LocalAI API through cURL. It first downloads a sample OGG audio file, then posts that file along with the model name to the /v1/audio/transcriptions endpoint. Users need wget and cURL installed, and should ensure LocalAI is running with the specified model configured. The result is expected in JSON format containing the transcribed text.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/audio-to-text.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n## Get an example audio file\nwget --quiet --show-progress -O gb1.ogg https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg\n\n## Send the example audio file to the transcriptions endpoint\ncurl http://localhost:8080/v1/audio/transcriptions -H \"Content-Type: multipart/form-data\" -F file=\"@$PWD/gb1.ogg\" -F model=\"whisper-1\"\n\n## Result\n{\"text\":\"My fellow Americans, this day has brought terrible news and great sadness to our country.At nine o'clock this morning, Mission Control in Houston lost contact with our Space ShuttleColumbia.A short time later, debris was seen falling from the skies above Texas.The Columbia's lost.There are no survivors.One board was a crew of seven.Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark, Captain DavidBrown, Commander William McCool, Dr. Kultna Shavla, and Elon Ramon, a colonel in the IsraeliAir Force.These men and women assumed great risk in the service to all humanity.In an age when spaceflight has come to seem almost routine, it is easy to overlook thedangers of travel by rocket and the difficulties of navigating the fierce outer atmosphere ofthe Earth.These astronauts knew the dangers, and they faced them willingly, knowing they had a highand noble purpose in life.Because of their courage and daring and idealism, we will miss them all the more.All Americans today are thinking as well of the families of these men and women who havebeen given this sudden shock and grief.You're not alone.Our entire nation agrees with you, and those you loved will always have the respect andgratitude of this country.The cause in which they died will continue.Mankind has led into the darkness beyond our world by the inspiration of discovery andthe longing to understand.Our journey into space will go on.In the skies today, we saw destruction and tragedy.As farther than we can see, there is comfort and hope.In the words of the prophet Isaiah, \\\"Lift your eyes and look to the heavens who createdall these, he who brings out the starry hosts one by one and calls them each by name.\\\"Because of his great power and mighty strength, not one of them is missing.The same creator who names the stars also knows the names of the seven souls we mourntoday.The crew of the shuttle Columbia did not return safely to Earth yet we can pray that all aresafely home.May God bless the grieving families and may God continue to bless America.[BLANK_AUDIO]\"}\n```\n\n----------------------------------------\n\nTITLE: Removing Docker Compose Containers - Console\nDESCRIPTION: This console command stops containers managed by Docker Compose and removes stopped service containers defined by the current docker-compose.yml file. It is a post-development cleanup step to free disk space. Requires Docker Compose to be installed and the command should be run from the directory containing the compose file.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#2025-04-23_snippet_5\n\nLANGUAGE: console\nCODE:\n```\ndocker-compose rm\n```\n\n----------------------------------------\n\nTITLE: Creating Reranker Environment in LocalAI\nDESCRIPTION: Command to create a separate environment specifically for the reranker project in LocalAI using the make utility.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/rerankers/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake reranker\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for localai\nDESCRIPTION: Lists the required Python packages for the localai project using a common requirements file format. It specifies exact versions for 'grpcio' (1.71.0) and 'packaging' (24.1), while 'protobuf' and 'certifi' are listed without explicit versions, meaning pip will install the latest compatible versions.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngrpcio==1.71.0\nprotobuf\ncertifi\npackaging==24.1\n```\n\n----------------------------------------\n\nTITLE: Hugo Extended Not Found Error Output - Console\nDESCRIPTION: This console output demonstrates a common Hugo error when the extended version is not installed. The error appears during the build process, referencing a missing SCSS resource. The output helps developers diagnose build failures due to incorrect Hugo version or missing static assets.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#2025-04-23_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n➜ hugo server\n\nINFO 2021/01/21 21:07:55 Using config file: \nBuilding sites … INFO 2021/01/21 21:07:55 syncing static files to /\nBuilt in 288 ms\nError: Error building site: TOCSS: failed to transform \"scss/main.scss\" (text/x-scss): resource \"scss/scss/main.scss_9fadf33d895a46083cdd64396b57ef68\" not found in file cache\n```\n\n----------------------------------------\n\nTITLE: Creating Coqui Environment for TTS Bark in LocalAI\nDESCRIPTION: Command to create a separate environment for the ttsbark project within LocalAI using make.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake coqui\n```\n\n----------------------------------------\n\nTITLE: Creating Diffusers Environment using Make\nDESCRIPTION: Command to set up a separate environment for the diffusers project using make. This creates an isolated environment for managing diffusers-related dependencies and configurations.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/diffusers/README.md#2025-04-23_snippet_0\n\nLANGUAGE: make\nCODE:\n```\nmake diffusers\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Section Front Matter - YAML\nDESCRIPTION: Defines metadata and display settings for the 'Advanced' section in the LocalAI documentation. Required by static site generators like Hugo, this YAML snippet sets ordering (weight), title, description, icon, publication date, draft status, and image associations. These fields control how the section appears and is sorted within the documentation, but do not affect runtime application behavior directly.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/_index.en.md#2025-04-23_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n---\nweight: 20\ntitle: \"Advanced\"\ndescription: \"Advanced usage\"\nicon: settings\nlead: \"\"\ndate: 2020-10-06T08:49:15+00:00\nlastmod: 2020-10-06T08:49:15+00:00\ndraft: false\nimages: []\n---\n```\n\n----------------------------------------\n\nTITLE: Creating a Git Branch for Contributing to LocalAI\nDESCRIPTION: Steps to fork the repository, create a descriptive branch, make and commit changes, and push to your fork. These commands are essential for preparing a pull request contribution.\nSOURCE: https://github.com/mudler/localai/blob/master/CONTRIBUTING.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b [branch name]\ngit push origin [branch name]\n```\n\n----------------------------------------\n\nTITLE: Running TTS Bark Unit Tests\nDESCRIPTION: Generic command to run the unit tests for the ttsbark gRPC server implementation\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n<The path of your python interpreter> -m unittest test_ttsbark.py\n```\n\n----------------------------------------\n\nTITLE: Installing a Model via Model Gallery REST API - Bash/JSON\nDESCRIPTION: Shows how to install a model from the registered galleries using the LocalAI REST API. The curl command sends a POST request to the /models/apply endpoint with a Content-Type of application/json, specifying the model identifier in the JSON payload. This approach requires LocalAI to be running and accessible on the specified port and expects the provided model ID to exist in the referenced gallery.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/whats-new.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8000/models/apply -H \"Content-Type: application/json\" -d '{ \"id\": \"huggingface@thebloke__open-llama-7b-open-instruct-ggml__open-llama-7b-open-instruct.ggmlv3.q4_0.bin\" }'\n```\n\n----------------------------------------\n\nTITLE: Build with Text to Audio Support (TTS) - Bash\nDESCRIPTION: Builds LocalAI with text-to-audio (TTS) support enabled by setting the GO_TAGS variable to 'tts'. Requires the piper-phonemize tool to be available at build time, as well as Go environment configured for tags.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nmake GO_TAGS=tts build\n```\n\n----------------------------------------\n\nTITLE: Setting Hugo Page Metadata (TOML)\nDESCRIPTION: This TOML snippet defines front matter configuration for a Hugo page. It disables the Table of Contents (`disableToc = false`), sets the page title to 'Features', assigns a weight of 8 for menu ordering, specifies an icon named 'feature_search', and defines the page's URL path as '/features/'.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/_index.en.md#2025-04-23_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ndisableToc = false\ntitle = \"Features\"\nweight = 8\nicon = \"feature_search\"\nurl = \"/features/\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Conceptual Structure of Fine-tuning Text for Instruction Models\nDESCRIPTION: Illustrates the typical format of text data used for fine-tuning instruction-following models. It includes placeholders for a system prompt, the specific instruction or question, and the expected response from the LLM. During inference, only the part up to '## Instruction' is provided, and the model generates the '## Response' part.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<System prompt>\n\n## Instruction\n\n<Question, instruction>\n\n## Response\n\n<Expected response from the LLM>\n```\n\n----------------------------------------\n\nTITLE: Uninstalling LocalAI using Installer Script - Bash\nDESCRIPTION: This snippet runs the LocalAI installer script with the --uninstall flag via curl and sh, triggering the uninstall routine. Users can remove LocalAI from their system using this command, which requires no parameter except the --uninstall flag passed after -- to the script. Prerequisite: curl installed, and the script must support the uninstall flag. Outputs success or failure messages based on the uninstallation process.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/installer.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://localai.io/install.sh | sh -s -- --uninstall\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Hipblas/ROCm on Arch Linux - Bash\nDESCRIPTION: Installs development packages necessary for building LocalAI with Hipblas backend on AMD GPUs using ROCm, via Arch Linux's pacman package manager. This includes build tools, ROCm SDK, OpenCL, OpenCV, CLBlast, and gRPC.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#2025-04-23_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npacman -S base-devel git rocm-hip-sdk rocm-opencl-sdk opencv clblast grpc\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalAI Framework - YAML\nDESCRIPTION: This YAML snippet configures LocalAI for gRPC communication, text-to-speech, CUDA support, and specifies files to download. Dependencies include LocalAI runtime supporting YAML configs. Key parameters are gRPC retry attempts and sleep time, TTS voice and Vall-E audio path, CUDA usage, and a files array for resources to download. Inputs must conform to YAML, and empty/default values should be populated as required for production. The output is LocalAI's configured behavior at runtime.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ngrpc:\n    attempts: 0 # Number of retry attempts for gRPC calls.\n    attempts_sleep_time: 0 # Sleep time between retries.\n\ntts:\n    voice: \"\" # Voice setting for TTS.\n    vall-e:\n        audio_path: \"\" # Path to audio files for Vall-E.\n\ncuda: false\n\ndownload_files: []\n```\n\n----------------------------------------\n\nTITLE: Installing a Model with a Custom Name - LocalAI - Bash/JSON\nDESCRIPTION: Provides an example of installing a model from a specified config and assigning it a custom model name using the 'name' field. This is useful for aliasing or replacing default model names. The payload is in JSON and sent to the /models/apply endpoint with curl.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nLOCALAI=http://localhost:8080\ncurl $LOCALAI/models/apply -H \"Content-Type: application/json\" -d '{\n     \"url\": \"<MODEL_CONFIG_FILE>\",\n     \"name\": \"<MODEL_NAME>\"\n   }'  \n```\n\n----------------------------------------\n\nTITLE: Enabling Parallel Tool Calls in LocalAI Model YAML\nDESCRIPTION: Shows the YAML configuration required to enable experimental parallel tool calling for a LocalAI model. By setting 'function.parallel_calls' to true within the model's configuration file, the model is allowed to potentially call multiple functions simultaneously in a single response.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/openai-functions.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nname: gpt-3.5-turbo\nparameters:\n  # Model file name\n  model: ggml-openllama.bin\n  top_p: 80\n  top_k: 0.9\n  temperature: 0.1\n\nfunction:\n  # set to true to allow the model to call multiple functions in parallel\n  parallel_calls: true\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with Intel XPU Support and AI Libraries\nDESCRIPTION: This requirements file configures PyTorch with Intel GPU acceleration support via the intel-extension-for-pytorch package. It also includes optimum with OpenVINO integration for model optimization and faster-whisper for efficient speech recognition models.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/faster-whisper/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\ntorch==2.3.1+cxx11.abi\noneccl_bind_pt==2.3.100+xpu\noptimum[openvino]\nfaster-whisper\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for LocalAI\nDESCRIPTION: This code snippet defines the required Python packages for the LocalAI project. It includes gRPC libraries and Protocol Buffers, which are commonly used for implementing remote procedure calls and serializing structured data.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/faster-whisper/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ngrpcio==1.71.0\nprotobuf\ngrpcio-tools\n```\n\n----------------------------------------\n\nTITLE: Starting Federated Load Balancer with Existing Token (Bash)\nDESCRIPTION: Runs the `local-ai federated` command to start a load balancer instance. This instance requires an existing P2P network token (set via the `TOKEN` environment variable) and routes incoming inference requests to the appropriate worker nodes within the established federated LocalAI network. Use `local-ai federated --help` to see all options.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/distributed_inferencing.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlocal-ai federated\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for LocalAI\nDESCRIPTION: This snippet lists the required Python packages and their versions for the LocalAI project. It includes PyTorch with Intel extensions, Transformers, Accelerate, and other related libraries. The file uses pip's requirements format with specific version constraints.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/rerankers/requirements-intel.txt#2025-04-23_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\nintel-extension-for-pytorch==2.3.110+xpu\ntransformers\naccelerate\ntorch==2.3.1+cxx11.abi\noneccl_bind_pt==2.3.100+xpu\nrerankers[transformers]\noptimum[openvino]\nsetuptools\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Audio File using wget\nDESCRIPTION: Uses the `wget` command to download a sample audio file (`.ogg` format) from Wikimedia Commons and saves it locally as `gb1.ogg`. This file is intended for use with the audio transcription example.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/try-it-out.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nwget --quiet --show-progress -O gb1.ogg https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg \n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for LocalAI Project\nDESCRIPTION: This is a requirements.txt file that lists all the necessary Python packages for the LocalAI project. It pins torch to version 2.4.1 and includes essential libraries for diffusion models, transformers, model optimization, and image processing.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/diffusers/requirements-cublas12.txt#2025-04-23_snippet_0\n\nLANGUAGE: plain text\nCODE:\n```\ntorch==2.4.1\ndiffusers\nopencv-python\ntransformers\naccelerate\ncompel\npeft\nsentencepiece\noptimum-quanto\n```\n\n----------------------------------------\n\nTITLE: Specifying LocalAI Model Configuration via CLI/ENV in Bash\nDESCRIPTION: Demonstrates two methods for providing model configuration files to the LocalAI binary: directly as command-line arguments using shorthand URLs (like `github://`) and through the `MODELS` environment variable, which accepts a comma-separated list of configuration sources.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/customize-model.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Command-Line Arguments\nlocal-ai github://owner/repo/file.yaml@branch\n\n# Environment Variable\nMODELS=\"github://owner/repo/file.yaml@branch,github://owner/repo/file.yaml@branch\" local-ai\n```\n\n----------------------------------------\n\nTITLE: Installing LocalAI using Bash Installer\nDESCRIPTION: This command downloads and executes the official LocalAI installation script using curl and sh. It performs a basic installation of LocalAI.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Basic installation\ncurl https://localai.io/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Serving the Hugo Documentation Site Locally - Bash\nDESCRIPTION: This bash snippet starts the local development server for the Hugo static site generator. It assumes a recent 'extended' version of Hugo is installed. Running this command from the project's root directory launches a live-reload server, making the documentation site available at a localhost URL. No additional parameters are required, and output includes server logs and build messages.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhugo server\n```\n\n----------------------------------------\n\nTITLE: Setting REBUILD Environment Variable for LocalAI Container Build (Shell)\nDESCRIPTION: Sets the `REBUILD` environment variable to `true`. This is typically used within a containerized build process (e.g., Docker) for LocalAI to force a rebuild of its components. This is often necessary when needing to recompile with different CMAKE arguments, for instance, to address CPU compatibility issues like SIGILL errors.\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/faq.md#2025-04-23_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nREBUILD=true\n```\n\n----------------------------------------\n\nTITLE: Creating VLLM Environment in LocalAI Project using Makefile\nDESCRIPTION: This command creates a separate environment for the VLLM project within the LocalAI project. It likely sets up necessary dependencies and configurations for VLLM integration.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Makefile\nCODE:\n```\nmake vllm\n```\n\n----------------------------------------\n\nTITLE: Listing Python Project Dependencies with requirements.txt - Plaintext\nDESCRIPTION: Specifies the Python packages required by the project, including version pinning (grpcio==1.71.0), and unversioned requirements (protobuf, certifi, setuptools). Used as input to pip to automate installation of dependencies. Inputs are package names and optional version specifiers; there are no outputs, but executing 'pip install -r requirements.txt' will install the specified packages. Assumes user has pip and Python installed; no scripting logic is present—only plain dependency listing.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngrpcio==1.71.0\nprotobuf\ncertifi\nsetuptools\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Dependencies with ROCm Support\nDESCRIPTION: Requirements file that specifies PyTorch installation with ROCm 6.0 support and related machine learning libraries. Includes accelerate for optimization, transformers for NLP tasks, and bitsandbytes for quantization.\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements-hipblas.txt#2025-04-23_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/rocm6.0\naccelerate\ntorch==2.4.1+rocm6.0\ntransformers\nbitsandbytes\n```\n\n----------------------------------------\n\nTITLE: Docker Runtime Backend Configuration\nDESCRIPTION: Command to run LocalAI with additional backend setup using EXTRA_BACKENDS environment variable\nSOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#2025-04-23_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --env EXTRA_BACKENDS=\"backend/python/diffusers\" quay.io/go-skynet/local-ai:master-ffmpeg-core\n```\n\n----------------------------------------\n\nTITLE: Creating TTS Bark Environment\nDESCRIPTION: Command to create a separate environment for the ttsbark project using make\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake ttsbark\n```\n\n----------------------------------------\n\nTITLE: TTS Bark Test Example Command\nDESCRIPTION: Specific example showing how to run the ttsbark tests using a Conda environment python interpreter\nSOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n/opt/conda/envs/bark/bin/python -m unittest extra/grpc/bark/test_ttsbark.py\n```"
  }
]
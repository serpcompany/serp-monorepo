[
  {
    "owner": "togethercomputer",
    "repo": "together-cookbook",
    "content": "TITLE: Defining Data Models for Flight Booking System\nDESCRIPTION: This snippet defines Pydantic models for the flight booking system, including FlightDetails, NoFlightFound, and Deps. It also creates agents for searching flights and extracting flight details.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass FlightDetails(BaseModel):\n    \"\"\"Details of the most suitable flight.\"\"\"\n\n    flight_number: str\n    price: int\n    origin: str = Field(description='Three-letter airport code')\n    destination: str = Field(description='Three-letter airport code')\n    date: datetime.date\n\n\nclass NoFlightFound(BaseModel):\n    \"\"\"When no valid flight is found.\"\"\"\n\n\n@dataclass\nclass Deps:\n    web_page_text: str\n    req_origin: str\n    req_destination: str\n    req_date: datetime.date\n\n\n# This agent is responsible for controlling the flow of the conversation.\n# It accepts search parameters and returns either flight details or a \"not found\" response\nsearch_agent = Agent[Deps, FlightDetails | NoFlightFound](\n    model=llm,\n    result_type=FlightDetails | NoFlightFound,  # type: ignore\n    retries=4,\n    system_prompt=(\n        'Your job is to find the cheapest flight for the user on the given date. '\n    ),\n    instrument=True,\n)\n\n# This agent is responsible for extracting flight details from web page text.\n# It parses the raw text and returns a structured list of flights\nextraction_agent = Agent(\n    model=llm,\n    result_type=list[FlightDetails],\n    system_prompt='Extract all the flight details from the given text.',\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Document Relevance Grading Function\nDESCRIPTION: This function determines whether retrieved documents are relevant to the question. It uses a language model to grade the relevance and decides whether to generate an answer or rewrite the question based on the relevance score.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated, Literal, Sequence\nfrom typing_extensions import TypedDict\n\nfrom langchain import hub\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\n\nfrom pydantic import BaseModel, Field\n\n\nfrom langgraph.prebuilt import tools_condition\n\n# This is the evaluation step\n# It determines whether the retrieved documents are relevant to the question.\n# If the documents are relevant, the agent will generate an answer.\n# If the documents are not relevant, the agent will rewrite the question and try again.\ndef grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question.\n\n    Args:\n        state (messages): The current state\n\n    Returns:\n        str: A decision for whether the documents are relevant or not\n    \"\"\"\n\n    print(\"---CHECK RELEVANCE---\")\n\n    # Data model\n    class grade(BaseModel):\n        \"\"\"Binary score for relevance check.\"\"\"\n\n        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n\n    # LLM\n    model = ChatTogether(api_key=os.getenv(\"TOGETHER_API_KEY\"), model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",temperature=0,streaming=True)\n\n    # LLM with tool and validation\n    llm_with_tool = model.with_structured_output(grade)\n\n    # Prompt\n    prompt = PromptTemplate(\n        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n        Here is the retrieved document: \\n\\n {context} \\n\\n\n        Here is the user question: {question} \\n\n        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n        input_variables=[\"context\", \"question\"],\n    )\n\n    # Chain\n    chain = prompt | llm_with_tool\n\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n\n    question = messages[0].content\n    docs = last_message.content\n\n    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n\n    score = scored_result.binary_score\n\n    if score == \"yes\":\n        print(\"---DECISION: DOCS RELEVANT---\")\n        return \"generate\"\n\n    else:\n        print(\"---DECISION: DOCS NOT RELEVANT---\")\n        print(score)\n        return \"rewrite\"\n```\n\n----------------------------------------\n\nTITLE: Creating Retriever Tool for Agent\nDESCRIPTION: This snippet creates a retriever tool that the agent will use to search for information in the blog posts. It wraps the retriever in a tool format that can be used by the LangChain agent.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.tools.retriever import create_retriever_tool\n\n# Create a tool for the retriever for the agent\nretriever_tool = create_retriever_tool(\n    retriever,\n    \"retrieve_blog_posts\",\n    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n)\n\ntools = [retriever_tool]\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Generation Functions for Research Pipeline in Python\nDESCRIPTION: Implements two functions for generating research queries: one to generate initial queries from a research topic using LLM, and another to wrap this with validation, limits, and logging. Uses the Together API for LLM interactions.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def generate_initial_queries(topic: str, together_client: AsyncTogether, max_queries: int, planning_model: str, json_model: str, prompts: dict) -> List[str]:\n    \"\"\"Step 1: Generate initial research queries based on the topic\"\"\"\n    queries = await generate_research_queries(topic, together_client, planning_model, json_model, prompts)\n    if max_queries > 0:\n        queries = queries[:max_queries]\n    print(f\"\\n\\nInitial queries: {queries}\")\n\n    if len(queries) == 0:\n        print(\"ERROR: No initial queries generated\")\n        return []\n\n    return queries\n\nasync def generate_research_queries(topic: str, together_client: AsyncTogether, planning_model: str, json_model: str, prompts: dict) -> list[str]:\n    \"\"\"Generate research queries for a given topic using LLM\"\"\"\n    PLANNING_PROMPT = prompts[\"planning_prompt\"]\n\n    planning_response = await together_client.chat.completions.create(\n        model=planning_model,\n        messages=[\n            {\"role\": \"system\", \"content\": PLANNING_PROMPT},\n            {\"role\": \"user\", \"content\": f\"Research Topic: {topic}\"}\n        ]\n    )\n    plan = planning_response.choices[0].message.content\n\n    print(f\"Generated plan: {plan}\")\n\n    SEARCH_PROMPT = prompts[\"plan_parsing_prompt\"]\n\n    json_response = await together_client.chat.completions.create(\n        model=json_model,\n        messages=[\n            {\"role\": \"system\", \"content\": SEARCH_PROMPT},\n            {\"role\": \"user\", \"content\": f\"Plan to be parsed: {plan}\"}\n        ],\n        response_format={\"type\": \"json_object\", \"schema\": ResearchPlan.model_json_schema()}\n    )\n\n    response_json = json_response.choices[0].message.content\n    plan = json.loads(response_json)\n    return plan[\"queries\"]\n```\n\n----------------------------------------\n\nTITLE: Generating Response with DeepSeek R1 Reasoning Model\nDESCRIPTION: Calls the DeepSeek R1 model to answer the query using the retrieved context, streaming the response as it's generated.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What is the maximum allowable floating point operation per second this bill allows for model training?\"\n\n\nPROMPT = \"\"\"\nAnswer the question: {query}. \nIMPORTANT RULE: Use the information provided to answer the question. For each claim in the answer provide a source from the information provided. \nHere is relevant information: {formatted_chunks} \n\"\"\"\n\n\nstream = client.chat.completions.create(\n    model=\"deepseek-ai/DeepSeek-R1\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n      {\"role\": \"user\", \"content\": PROMPT.format(query=query, formatted_chunks=formatted_chunks)},\n    ],\n      stream=True,\n)\n\nresponse = ''\n\nfor chunk in stream:\n  response += chunk.choices[0].delta.content or \"\"\n  print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Milvus Collection and Upserting Data\nDESCRIPTION: This code snippet demonstrates how to create a Milvus collection with specified schema and index parameters, and then upsert data into the collection. It uses the `pymilvus` library to interact with Milvus. The code prepares data by generating embeddings for contextual chunks and then upserts the data along with document indices and titles into the Milvus collection.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nschema.add_field(\"document_index\", DataType.VARCHAR, max_length=255)\n        schema.add_field(\"embedding\", DataType.FLOAT_VECTOR, dim=1024)\n        schema.add_field(\"title\", DataType.VARCHAR, max_length=255)\n        index_params = client.prepare_index_params()\n        index_params.add_index(\"embedding\", metric_type=\"COSINE\")\n\n        client.create_collection(collection_name, dimension=512, schema=schema, index_params=index_params)\n\n    if not document.contextual_chunks:\n        return document  # Exit early if there are no contextual chunks\n    \n    # Generate embeddings for chunks\n    embeddings = [get_embedding(chunk[:512], embedding_model) for chunk in document.contextual_chunks] # NOTE: Trimming the chunk for the embedding model's context window\n    embeddings_np = np.array(embeddings, dtype=np.float32)\n\n    ids = [\n        f\"id{document.idx}_{chunk_idx}\"\n        for chunk_idx, _ in enumerate(document.contextual_chunks)\n    ]\n    titles = [document.title] * len(document.contextual_chunks)\n\n    client.upsert(\n        collection_name,\n        [\n            {\"id\": index, \"document_index\": document_index, \"embedding\": embedding, \"title\": title}\n            for index, (document_index, embedding, title) in enumerate(zip(ids, embeddings_np.tolist(), titles))\n        ]\n    )\n\n    return document\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Documents for RAG System\nDESCRIPTION: This code loads documents from web URLs, splits them into smaller chunks using a text splitter, and prepares them for embedding and storage in the vector database.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load documents from web URLs and chunk them into smaller pieces\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, chunk_overlap=50\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Tools and Validation for Flight Search\nDESCRIPTION: This code defines tools and validation logic for the flight search agent. It includes a function to extract flights and a validator to ensure flight details meet the specified requirements.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@search_agent.tool\nasync def extract_flights(ctx: RunContext[Deps]) -> list[FlightDetails]:\n    \"\"\"Get details of all flights.\"\"\"\n    # we pass the usage to the search agent so requests within this agent are counted\n    result = await extraction_agent.run(ctx.deps.web_page_text, usage=ctx.usage)\n    logfire.info('found {flight_count} flights', flight_count=len(result.data))\n    return result.data\n\n\n@search_agent.result_validator\nasync def validate_result(\n    ctx: RunContext[Deps], result: FlightDetails | NoFlightFound\n) -> FlightDetails | NoFlightFound:\n    \"\"\"Procedural validation that the flight meets the constraints.\"\"\"\n    if isinstance(result, NoFlightFound):\n        return result\n\n    errors: list[str] = []\n    if result.origin != ctx.deps.req_origin:\n        errors.append(\n            f'Flight should have origin {ctx.deps.req_origin}, not {result.origin}'\n        )\n    if result.destination != ctx.deps.req_destination:\n        errors.append(\n            f'Flight should have destination {ctx.deps.req_destination}, not {result.destination}'\n        )\n    if result.date != ctx.deps.req_date:\n        errors.append(f'Flight should be on {ctx.deps.req_date}, not {result.date}')\n\n    if errors:\n        raise ModelRetry('\\n'.join(errors))\n    else:\n        return result\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Web Search Agent with Together AI\nDESCRIPTION: Demonstrates how to create a basic agent with web search capability using Meta-Llama-3.1-8B-Instruct-Turbo model and DuckDuckGo search as a tool. The agent can answer questions by retrieving information from the web.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Agno/Agents_Agno.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agno.agent import Agent\nfrom agno.models.together import Together\nfrom agno.tools.duckduckgo import DuckDuckGoTools\n\nagent = Agent(\n    model=Together(id=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\", max_tokens=4096),\n    tools=[DuckDuckGoTools()],\n    markdown=True\n)\nagent.print_response(\"How are the golden state warriors doing this 2024-2025 season?\", stream=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a Retrieval Task\nDESCRIPTION: This code defines a Flyte task `retrieve` that performs retrieval using both a vector database (Milvus) and a BM25S keyword index. It generates embeddings for input queries, searches the Milvus collection, loads the BM25S index and contextual chunks data, performs BM25S-based retrieval, and returns the results as a dataclass. Requires `bm25s` and `pymilvus` libraries, as well as the pre-built indices.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass RetrievalResults:\n    vector_results: list[list[str]]\n    bm25s_results: list[list[str]]\n\n\n@union.task\ndef retrieve(\n    bm25s_index: FlyteDirectory,\n    contextual_chunks_data: FlyteFile,\n    embedding_model: str = \"BAAI/bge-large-en-v1.5\",\n    queries: list[str] = [\n        \"What to do in the face of uncertainty?\",\n        \"Why won't people write?\",\n    ],\n) -> RetrievalResults:\n    import json\n\n    import bm25s\n    import numpy as np\n    from pymilvus import MilvusClient\n\n    client = MilvusClient(\"test_milvus.db\")    \n    \n    # Generate embeddings for the queries using Together\n    query_embeddings = [\n        get_embedding(query, embedding_model) for query in queries\n    ]\n    query_embeddings_np = np.array(query_embeddings, dtype=np.float32)\n\n    collection_name = \"paul_graham_collection\" \n    results = client.search(\n        collection_name,\n        query_embeddings_np,\n        limit=5,\n        search_params={\"metric_type\": \"COSINE\"},\n        anns_field=\"embedding\",\n        output_fields=[\"document_index\", \"title\"]\n    )\n\n    # Load BM25S index\n    retriever = bm25s.BM25()\n    bm25_index = retriever.load(save_dir=bm25s_index.download())\n\n    # Load contextual chunk data\n    with open(contextual_chunks_data, \"r\", encoding=\"utf-8\") as json_file:\n        contextual_chunks_data_dict = json.load(json_file)\n\n    # Perform BM25S-based retrieval\n    bm25s_idx_result = bm25_index.retrieve(\n        query_tokens=bm25s.tokenize(queries),\n        k=5,\n        corpus=np.array(list(contextual_chunks_data_dict.values())),\n    )\n\n    # Return results as a dataclass\n    return RetrievalResults(\n        vector_results=results,\n        bm25s_results=bm25s_idx_result.documents.tolist(),\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Response from LLM in Python\nDESCRIPTION: This code snippet sends a request to a generative model with the query and relevant information to obtain a contextual answer to the user's question.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n# Generate a story based on the top 10 most similar movies\n\nquery = \"What are 'skip-level' meetings?\"\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n      {\"role\": \"user\", \"content\": f\"Answer the question: {query}. Here is relevant information: {retreived_chunks}\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Contextual Prompts for Each Chunk\nDESCRIPTION: Defines a function that generates prompts for each chunk by filling in the prompt template with the full document and individual chunk content. Returns a list of complete prompts.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\n# First we will just generate the prompts and examine them\n\ndef generate_prompts(document : str, chunks : List[str]) -> List[str]:\n  prompts = []\n  for chunk in chunks:\n    prompt = CONTEXTUAL_RAG_PROMPT.format(WHOLE_DOCUMENT=document, CHUNK_CONTENT=chunk)\n    prompts.append(prompt)\n  return prompts\n```\n\n----------------------------------------\n\nTITLE: Looping Workflow Implementation\nDESCRIPTION: Implementation of the main workflow that coordinates between Generator and Evaluator in a loop until acceptance.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef loop_workflow(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n    \"\"\"Keep generating and evaluating until the evaluator passes the last generated response.\"\"\"\n    # Store previous responses from generator\n    memory = []\n    \n    # Generate initial response\n    response = generate(task, generator_prompt)\n    memory.append(response)\n\n    #Build a schema for the evaluation\n    class Evaluation(BaseModel):\n        evaluation: Literal[\"PASS\", \"NEEDS_IMPROVEMENT\", \"FAIL\"]\n        feedback: str\n\n    # While the generated response is not passing, keep generating and evaluating\n    while True:\n        evaluation, feedback = evaluate(task, evaluator_prompt, response, Evaluation)\n        # Terminating condition\n        if evaluation == \"PASS\":\n            return response\n        \n        # Add current response and feedback to context and generate a new response\n        context = \"\\n\".join([\n            \"Previous attempts:\",\n            *[f\"- {m}\" for m in memory],\n            f\"\\nFeedback: {feedback}\"\n        ])\n        \n        response = generate(generator_prompt, task, context)\n        memory.append(response)\n```\n\n----------------------------------------\n\nTITLE: Querying Llama 3.2 90B Vision with Retrieved Image\nDESCRIPTION: Uses the Together AI client to query the Llama 3.2 90B Vision model with the retrieved image and the original question, demonstrating multimodal question answering capabilities.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom together import Together\n\nclient = Together(api_key = api_key)\n\nresponse = client.chat.completions.create(\n  model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": query}, #query\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{returned_page}\", #retrieved page image\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Executing a Complex Query with the Planning Agent\nDESCRIPTION: Demonstrates how the agent handles a real query by breaking it down, executing steps, and adapting the plan as needed. It includes error handling for potential issues during execution.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\"recursion_limit\": 25}\ninputs = {\"input\": \"what is the hometown of the mens 2024 Australia open winner?\"}\ntry:\n    async for event in app.astream(inputs, config=config):\n        for k, v in event.items():\n            if k != \"__end__\":\n                print(v)\nexcept IndexError as e:\n    print(f\"Error: {e}. The plan list is empty or index is out of range.\")\n    print(\"Execution terminated.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n    print(\"Execution terminated.\")\n```\n\n----------------------------------------\n\nTITLE: Defining Core Planning Functions\nDESCRIPTION: Implements three main functions that drive the planning process: execute_step, plan_step, and replan_step. These functions handle execution, initial planning, and plan revision.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union\n\n\nclass Response(BaseModel):\n    \"\"\"Response to user.\"\"\"\n\n    response: str\n\n\nclass Act(BaseModel):\n    \"\"\"Action to perform.\"\"\"\n\n    action: Union[Response, Plan] = Field(\n        description=\"Action to perform. If you want to respond to user, use Response. \"\n        \"If you need to further use tools to get the answer, use Plan.\"\n    )\n\n\nreplanner_prompt = ChatPromptTemplate.from_template(\n    \"\"\"For the given objective, come up with a simple step by step plan. \\\nThis plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\nThe result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n\nYour objective was this:\n{input}\n\nYour original plan was this:\n{plan}\n\nYou have currently done the follow steps:\n{past_steps}\n\nUpdate your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n)\n\n\nreplanner = replanner_prompt | ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", temperature=0).with_structured_output(Act)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\nfrom langgraph.graph import END\n\n\nasync def execute_step(state: PlanExecute):\n    plan = state[\"plan\"]\n    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n    task = plan[0]\n    task_formatted = f\"\"\"For the following plan:\n{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n    agent_response = await agent_executor.ainvoke(\n        {\"messages\": [(\"user\", task_formatted)]}\n    )\n    return {\n        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n    }\n\n\nasync def plan_step(state: PlanExecute):\n    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n    return {\"plan\": plan.steps}\n\n\nasync def replan_step(state: PlanExecute):\n    output = await replanner.ainvoke(state)\n    if isinstance(output.action, Response):\n        return {\"response\": output.action.response}\n    else:\n        return {\"plan\": output.action.steps}\n\n\ndef should_end(state: PlanExecute):\n    if \"response\" in state and state[\"response\"]:\n        return END\n    else:\n        return \"agent\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Retrieval Function\nDESCRIPTION: Creates a compact function that retrieves the top-k most similar chunks from a vector index based on a query embedding.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# compact function to retrieve the top k chunks\n\ndef vector_retrieval(query: str, top_k: int = 5, vector_index: np.ndarray = None, chunks: List[str] = None) -> List[int]:\n    \"\"\"\n    Retrieve the top-k most similar items from an index based on a query.\n    Args:\n        query (str): The query string to search for.\n        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.\n        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.\n    Returns:\n        List[int]: A list of indices corresponding to the top-k most similar items in the index.\n    \"\"\"\n\n    query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]\n    \n    \n    dot_product = np.dot(query_embedding, np.array(vector_index).T)\n    query_norm = np.linalg.norm(query_embedding)\n    vector_index_norm = np.linalg.norm(vector_index, axis=1)\n    \n    similarity_scores = dot_product / (query_norm * vector_index_norm)\n\n    return [chunks[index] for index in np.argsort(-similarity_scores)[:top_k]]\n```\n\n----------------------------------------\n\nTITLE: Creating a Generic Parallel Workflow Function\nDESCRIPTION: Implements a reusable function that orchestrates the entire parallel workflow pattern, from receiving parallel proposals to aggregating them into a final response.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def parallel_workflow(prompt : str, proposer_models : List[str], aggregator_model : str, aggregator_prompt: str):\n    \"\"\"Run a parallel chain of LLM calls to address the `input_query` \n    using a list of models specified in `models`.\n\n    Returns output from final aggregator model.\n    \"\"\"\n\n    # Gather intermediate responses from proposer models\n    proposed_responses = await asyncio.gather(*[run_llm_parallel(prompt, model) for model in proposer_models])\n    \n    # Aggregate responses using an aggregator model\n    final_output = run_llm(user_prompt=prompt,\n                           model=aggregator_model,\n                           system_prompt=aggregator_prompt + \"\\n\" + \"\\n\".join(f\"{i+1}. {str(element)}\" for i, element in enumerate(proposed_responses)\n           ))\n    \n    return final_output, proposed_responses\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Defining Knowledge Base URLs\nDESCRIPTION: This snippet imports the required libraries for document loading, vector storage, and language models. It also defines a list of blog post URLs that will serve as the knowledge base for the RAG system.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import required libraries for document loading, vector storage, and language models\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_together import ChatTogether\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_together import TogetherEmbeddings\n\n# List of blog posts we'll use as our knowledge base\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n```\n\n----------------------------------------\n\nTITLE: Aggregating Intermediate Responses with Final LLM\nDESCRIPTION: Uses an advanced language model to analyze and synthesize the collected intermediate responses into a final, comprehensive answer for the user.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# We will use the best open source model to aggregate the responses\naggregator_model = \"deepseek-ai/DeepSeek-V3\"\n\nfinal_output = run_llm(user_prompt=user_prompt, # task to be completed\n                       model=aggregator_model,\n                       system_prompt=aggregator_system_prompt + \"\\n\" + \"\\n\".join(f\"{i+1}. {str(element)}\" for i, element in enumerate(results)\n           ))\n\nprint(final_output)\n```\n\n----------------------------------------\n\nTITLE: Implementing Embedding Generation Function\nDESCRIPTION: Defines a function to generate embeddings using the Together AI API. The function takes a list of texts and a model identifier, then returns embeddings for each text using the specified model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# This function will be used to access the Together API to generate embeddings for the movie plots\n\nfrom typing import List\n\ndef generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n    \"\"\"Generate embeddings from Together python library.\n\n    Args:\n        input_texts: a list of string input texts.\n        model_api_string: str. An API string for a specific embedding model of your choice.\n\n    Returns:\n        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n    \"\"\"\n    together_client = together.Together(api_key = TOGETHER_API_KEY)\n    outputs = together_client.embeddings.create(\n        input=input_texts,\n        model=model_api_string,\n    )\n    return [x.embedding for x in outputs.data]\n```\n\n----------------------------------------\n\nTITLE: Defining Agentic Workflow with LangGraph\nDESCRIPTION: This code snippet defines a workflow for an agent using the LangGraph library. It initializes a StateGraph, adds nodes representing different actions like agent, retrieval, rewriting, and response generation. It defines edges between nodes, including conditional edges based on agent decisions and document grading. Finally, it compiles the graph to create the workflow.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.prebuilt import ToolNode\n\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\n# Define the nodes we will cycle between\nworkflow.add_node(\"agent\", agent)  # agent\nretrieve = ToolNode([retriever_tool])\nworkflow.add_node(\"retrieve\", retrieve)  # retrieval\nworkflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\nworkflow.add_node(\n    \"generate\", generate\n)  # Generating a response after we know the documents are relevant\n\n# Call agent node to decide to retrieve or not\nworkflow.add_edge(START, \"agent\")\n\n# Decide whether to retrieve\nworkflow.add_conditional_edges(\n    \"agent\",\n    # Assess agent decision\n    tools_condition,\n    {\n        # Translate the condition outputs to nodes in our graph\n        \"tools\": \"retrieve\",\n        END: END,\n    },\n)\n\n# Edges taken after the `action` node is called.\nworkflow.add_conditional_edges(\n    \"retrieve\",\n    # Assess agent decision\n    grade_documents,\n)\nworkflow.add_edge(\"generate\", END)\nworkflow.add_edge(\"rewrite\", \"agent\")\n\n# Compile\ngraph = workflow.compile()\n```\n\n----------------------------------------\n\nTITLE: Creating BM25S Index\nDESCRIPTION: This code snippet demonstrates how to create a BM25S keyword index using the `bm25s` library.  It prepares data from documents, creates a BM25 index, saves the index to a directory, and writes contextual chunks to a JSON file. The function is decorated with `@actor.task` indicating it's part of a distributed compute environment.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@actor.task(cache=True, cache_version=\"0.5\")\ndef create_bm25s_index(documents: list[Document]) -> tuple[FlyteDirectory, FlyteFile]:\n    import json\n    import bm25s\n\n    # Prepare data for JSON\n    data = {\n        f\"id{doc_idx}_{chunk_idx}\": contextual_chunk\n        for doc_idx, document in enumerate(documents)\n        if document.contextual_chunks\n        for chunk_idx, contextual_chunk in enumerate(document.contextual_chunks)\n    }\n\n    retriever = bm25s.BM25(corpus=list(data.values()))\n    retriever.index(bm25s.tokenize(list(data.values())))\n\n    ctx = union.current_context()\n    working_dir = Path(ctx.working_directory)\n    bm25s_index_dir = working_dir / \"bm25s_index\"\n    contextual_chunks_json = working_dir / \"contextual_chunks.json\"\n\n    retriever.save(str(bm25s_index_dir))\n\n    # Write the data to a JSON file\n    with open(contextual_chunks_json, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(data, json_file, indent=4, ensure_ascii=False)\n\n    return FlyteDirectory(path=bm25s_index_dir), FlyteFile(contextual_chunks_json)\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Tavily Search and Content Summarization in Python\nDESCRIPTION: This function performs a single search using the Tavily API and summarizes the raw content of each result using an LLM. It retrieves search results, filters out entries with no raw content, and processes each valid result asynchronously.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def tavily_search(query: str, tavily_client: AsyncTavilyClient, prompts: dict, together_client: AsyncTogether, summary_model: str) -> SearchResults:\n    \"\"\"Perform a single Tavily search\"\"\"\n    print(f'Perform Tavily search with query: {query}')\n\n    response = await tavily_client.search(query, include_raw_content=True)\n\n    print(f\"Tavily Responded with {len(response['results'])} results (Tavily returning None will be ignored for summarization)\")\n\n    RAW_CONTENT_SUMMARIZER_PROMPT = prompts[\"raw_content_summarizer_prompt\"]\n\n    # Create tasks for summarization\n    summarization_tasks = []\n    result_info = []\n    for result in response[\"results\"]:\n        if result[\"raw_content\"] is None:\n            continue\n        task = summarize_content(result[\"raw_content\"], query, RAW_CONTENT_SUMMARIZER_PROMPT, together_client, summary_model)\n        summarization_tasks.append(task)\n        result_info.append(result)\n\n    summarized_contents = await asyncio.gather(*summarization_tasks)\n\n    formatted_results = []\n    for result, summarized_content in zip(result_info, summarized_contents):\n        formatted_results.append(\n            SearchResult(\n                title=result[\"title\"],\n                link=result[\"url\"],\n                content=result[\"raw_content\"],\n                filtered_raw_content=summarized_content,\n            )\n        )\n    return SearchResults(formatted_results)\n\nasync def summarize_content(raw_content: str, query: str, prompt: str, together_client: AsyncTogether, summary_model: str) -> str:\n    \"\"\"Summarize content asynchronously using the LLM\"\"\"\n    print(\"Summarizing content asynchronously using the LLM\")\n\n    summarize_response = await together_client.chat.completions.create(\n        model=summary_model,\n        messages=[\n            {\"role\": \"system\", \"content\": prompt},\n            {\"role\": \"user\", \"content\": f\"<Raw Content>{raw_content}</Raw Content>\\n\\n<Research Topic>{query}</Research Topic>\"}\n        ]\n    )\n    return summarize_response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Querying BM25 Index for Top-K Results in Python\nDESCRIPTION: This snippet queries the BM25 index with a specified search query and retrieves the top k results, including their scores. It showcases the practical application of the BM25 search method.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# Query the corpus and get top-k results\nquery = \"What are 'skip-level' meetings?\"\nresults, scores = retriever.retrieve(bm25s.tokenize(query), k=5,)\n```\n\n----------------------------------------\n\nTITLE: Setting up Tavily Search Tool\nDESCRIPTION: Configures the Tavily search tool to allow the agent to find information online. It requires a Tavily API key from the environment variables.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\ntools = [TavilySearchResults(max_results=3, tavily_api_key=os.environ.get(\"TAVILY_API_KEY\"))]\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous LLM Function with Retry Logic\nDESCRIPTION: Defines an asynchronous function for parallel LLM calls with built-in retry logic for handling rate limit errors from the Together API.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# The function below will call the reference LLMs in parallel\nasync def run_llm_parallel(user_prompt : str, model : str, system_prompt : str = None):\n    \"\"\"Run parallel LLM call with a reference model.\"\"\"\n    for sleep_time in [1, 2, 4]:\n        try:\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n            messages.append({\"role\": \"user\", \"content\": user_prompt})\n\n            response = await async_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                temperature=0.7,\n                max_tokens=2000,\n            )\n            break\n        except together.error.RateLimitError as e:\n            print(e)\n            await asyncio.sleep(sleep_time)\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity for Vector Retrieval\nDESCRIPTION: Computes cosine similarity between the query embedding and document embeddings to find the most relevant chunks.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Calculate cosine similarity between the query embedding and each movie embedding\n\ndot_product = np.dot(query_embedding, np.array(embeddings).T)\nquery_norm = np.linalg.norm(query_embedding)\nembeddings_norm = np.linalg.norm(embeddings, axis=1)\nsimilarity_scores = dot_product / (query_norm * embeddings_norm)\nindices = np.argsort(-similarity_scores)\n```\n\n----------------------------------------\n\nTITLE: Handling Seat Selection with Natural Language Processing in Python\nDESCRIPTION: Function that processes user seat preferences using natural language. It repeatedly asks for seat preferences until it can parse the request into a valid SeatPreference object, maintaining conversation context through message history.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def find_seat(usage: Usage) -> SeatPreference:\n    \"\"\"Function to handle seat selection through natural language.\"\"\"\n    message_history: list[ModelMessage] | None = None\n    while True:\n        answer = Prompt.ask('What seat would you like?')\n\n        result = await seat_preference_agent.run(\n            answer,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.data, SeatPreference):\n            return result.data\n        else:\n            print('Could not understand seat preference. Please try again.')\n            message_history = result.all_messages()\n```\n\n----------------------------------------\n\nTITLE: Querying Indexed Document with ColQwen2\nDESCRIPTION: Demonstrates how to query the indexed document using ColQwen2, retrieving the most relevant pages based on a given query about Nvidia's data center revenue.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What are the half year data centre renevue results and the 5 year CAGR for Nvidia data centre revenue?\"\nresults = model.search(query, k=5)\n\nprint(f\"Search results for '{query}':\")\nfor result in results:\n    print(f\"Doc ID: {result.doc_id}, Page: {result.page_num}, Score: {result.score}\")\n\nprint(\"Test completed successfully!\")\n```\n\n----------------------------------------\n\nTITLE: Defining System Prompts for Research Pipeline in Python\nDESCRIPTION: Defines a dictionary of prompt templates for each stage of the research process, including planning, parsing, content processing, evaluation, filtering, and final answer generation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# System Prompts\n# -------------\n# Instructions for each stage of the research process\nprompts = {\n    # Planning: Generates initial research queries\n    \"planning_prompt\": \"\"\"You are a strategic research planner with expertise in breaking down complex\n                         questions into logical search steps. Generate focused, specific, and self-contained queries that\n                         will yield relevant information for the research topic.\"\"\",\n\n    # Plan Parsing: Extracts structured data from planning output\n    \"plan_parsing_prompt\": \"\"\"Extract search queries that should be executed.\"\"\",\n\n    # Content Processing: Identifies relevant information from search results\n    \"raw_content_summarizer_prompt\": \"\"\"Extract and synthesize only the information relevant to the research\n                                       topic from this content. Preserve specific data, terminology, and\n                                       context while removing irrelevant information.\"\"\",\n\n    # Completeness Evaluation: Determines if more research is needed\n    \"evaluation_prompt\": \"\"\"Analyze these search results against the original research goal. Identify\n                          specific information gaps and generate targeted follow-up queries to fill\n                          those gaps. If no significant gaps exist, indicate that research is complete.\"\"\",\n\n    # Evaluation Parsing: Extracts structured data from evaluation output\n    \"evaluation_parsing_prompt\": \"\"\"Extract follow-up search queries from the evaluation. If no follow-up queries are needed, return an empty list.\"\"\",\n\n    # Source Filtering: Selects most relevant sources\n    \"filter_prompt\": \"\"\"Evaluate each search result for relevance, accuracy, and information value\n                       related to the research topic. At the end, you need to provide a list of\n                       source numbers with the rank of relevance. Remove the irrelevant ones.\"\"\",\n    # Source Filtering: Selects most relevant sources\n    \"source_parsing_prompt\": \"\"\"Extract the source list that should be included.\"\"\",\n\n    # Answer Generation: Creates final research report\n    \"answer_prompt\": \"\"\"Create a comprehensive, publication-quality markdown research report based exclusively\n                       on the provided sources. The report should include: title, introduction, analysis (multiple sections with insights titles)\n                       and conclusions, references. Use proper citations (source with link; using \\n\\n \\\\[Ref. No.\\\\] to improve format),\n                       organize information logically, and synthesize insights across sources. Include all relevant details while\n                       maintaining readability and coherence. In each section, You MUST write in plain\n                       paragraghs and NEVER describe the content following bullet points or key points (1,2,3,4... or point X: ...)\n                       to improve the report readability.\"\"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Vector Database for Semantic Search\nDESCRIPTION: This snippet creates a vector database using Chroma and the processed documents. It sets up a retriever for semantic searches, which will be used by the agent to find relevant information when answering questions.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Add to vectorDB\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=TogetherEmbeddings(model=\"togethercomputer/m2-bert-80M-2k-retrieval\", # long context embedding model\n                                 api_key=os.getenv(\"TOGETHER_API_KEY\")),\n)\nretriever = vectorstore.as_retriever()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Final Documents from Hybrid Search in Python\nDESCRIPTION: This snippet constructs a list of documents based on the indices provided by the hybrid result from the RRF algorithm, aiming to compile the final relevant contextual information.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nhybrid_top_k_docs = [contextual_chunks[index] for index in hybrid_top_k[1]]\n```\n\n----------------------------------------\n\nTITLE: Creating a DPO Fine-tuning Job Continuing from SFT Checkpoint\nDESCRIPTION: Configures a Direct Preference Optimization (DPO) fine-tuning job that continues from a previous SFT checkpoint. This implements the second stage of the two-stage approach, using the from_checkpoint parameter to continue training from the SFT results.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndpo_training_from_sft = client.fine_tuning.create(\n    training_file=dpo_train_file.id,\n    validation_file=dpo_validation_file.id,\n    n_evals=10,\n    #model=MODEL_NAME, We do not use model name here, it is derived from the checkpoint!\n    wandb_api_key=WANDB_API_KEY,\n    wandb_project_name=\"helpsteer2\",\n    suffix=\"helpsteer2_dpo_training_continuing_sft\",\n    n_epochs=1,\n    n_checkpoints=1,\n    learning_rate=1e-5,\n    lora=True,\n    training_method='dpo', # Now we use DPO training\n    from_checkpoint=sft_training.id # Continuing from SFT checkpoint!\n)\nprint(dpo_training_from_sft.id)\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Final Answer from Worker Responses\nDESCRIPTION: Uses another LLM call to synthesize the worker responses into a coherent final answer that addresses all aspects of the original task.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nSYNTHESIZER_PROMPT = \"\"\"Given the task: {task} and the following responses below, that each address different aspects of the \ntask, synthesize a final response.\n\n{worker_responses}\n\"\"\"\nconcatenated_responses = \" \".join([f\"\\n=== WORKER RESULT ({task_info['type']}) ===\\n{response}\\n\" for task_info, response in zip(tasks, worker_resp)])\n\nfinal_answer = run_llm(SYNTHESIZER_PROMPT.format(task=task, worker_responses=concatenated_responses), model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\")\n```\n\n----------------------------------------\n\nTITLE: Using Reranker for Result Quality Improvement in Python\nDESCRIPTION: This code snippet passes the final documents obtained from the hybrid search into a reranker model for enhanced relevance based on the original query.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\nquery = \"What are 'skip-level' meetings?\" # we keep the same query - can change if we want\n\nresponse = client.rerank.create(\n  model=\"Salesforce/Llama-Rank-V1\",\n  query=query,\n  documents=hybrid_top_k_docs,\n  top_n=3 # we only want the top 3 results\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Image Generation Using FLUX Models in Python\nDESCRIPTION: Defines a function to generate images conditioned on input images using the FLUX diffusion model from Together API.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\nclient = Together(api_key = 'TOGETHER_API_KEY')\n\ndef generate_image(image_prompt, retrieved_image, model = \"black-forest-labs/FLUX.1-depth\"):\n\n    imageCompletion = client.images.generate(\n        model = model,\n        width=1024,\n        height=768,\n        steps=28,\n        prompt = image_prompt,\n        image_url = retrieved_image,\n    )\n\n    return imageCompletion.data[0].url\n```\n\n----------------------------------------\n\nTITLE: Defining Agent State for LangGraph\nDESCRIPTION: This code defines the AgentState class, which represents the state of the agent in the LangGraph framework. It uses TypedDict to specify the structure of the state, including a sequence of messages.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated, Sequence\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.messages import BaseMessage\n\nfrom langgraph.graph.message import add_messages\n\n\nclass AgentState(TypedDict):\n    # The add_messages function defines how an update should be processed\n    # Default is to replace. add_messages says \"append\"\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n```\n\n----------------------------------------\n\nTITLE: Router Workflow Implementation - Task Execution\nDESCRIPTION: Implements the complete router workflow function that selects and executes appropriate models for given tasks\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef run_router_workflow(user_prompt : str):\n    selected_model = JSON_llm(ROUTER_PROMPT.format(user_query=user_prompt),\n                            ModelOutput,\n                            system_prompt=ROUTER_SYSTEM_PROMPT)\n    \n    response = run_llm(user_prompt= user_prompt, \n                   model = selected_model['model']\n    )\n    return selected_model['model'], selected_model['reason'], response\n```\n\n----------------------------------------\n\nTITLE: Building the Workflow Graph\nDESCRIPTION: Creates a directed graph that coordinates the planning and execution process. It defines the flow between planning, execution, and replanning steps.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import StateGraph, START\n\nworkflow = StateGraph(PlanExecute)\n\n# Add the plan node\nworkflow.add_node(\"planner\", plan_step)\n\n# Add the execution step\nworkflow.add_node(\"agent\", execute_step)\n\n# Add a replan node\nworkflow.add_node(\"replan\", replan_step)\n\nworkflow.add_edge(START, \"planner\")\n\n# From plan we go to agent\nworkflow.add_edge(\"planner\", \"agent\")\n\n# From agent, we replan\nworkflow.add_edge(\"agent\", \"replan\")\n\nworkflow.add_conditional_edges(\n    \"replan\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_end,\n    [\"agent\", END],\n)\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable\napp = workflow.compile()\n```\n\n----------------------------------------\n\nTITLE: Creating Pydantic Models for Invoice Structure\nDESCRIPTION: Defines Pydantic models to structure the data extraction. The Item class represents invoice line items with name, price, and quantity, while the Receipt class represents the complete invoice with items and total.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom pydantic import BaseModel, Field\n\nclass Item(BaseModel):\n    name: str\n    price: float\n    quantity: int = Field(default=1)\n\nclass Receipt(BaseModel):\n    items: list[Item]\n    total: float\n```\n\n----------------------------------------\n\nTITLE: Filtering and Ranking Search Results with Together AI\nDESCRIPTION: Implements search result filtering and ranking based on relevance using Together AI's LLM. The function evaluates and prioritizes sources while filtering out less relevant content.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nasync def filter_results(topic: str, results: SearchResults, together_client: AsyncTogether, json_model: str, max_sources: int, prompts: dict) -> tuple[SearchResults, SourceList]:\n    \"\"\"\n    Filter and rank search results based on relevance to the research topic.\n\n    Args:\n        topic: The research topic\n        results: Search results to filter\n        together_client: The Together AI client for LLM operations\n        json_model: Model to use for filtering\n        max_sources: Maximum number of sources to keep (-1 for unlimited)\n        prompts: Dictionary of prompt templates\n\n    Returns:\n        Tuple of (filtered results, source list with indices)\n    \"\"\"\n    # Format the search results for the LLM, without the raw content\n    formatted_results = results.short_str()\n\n    FILTER_PROMPT = prompts[\"filter_prompt\"]\n\n    SOURCE_PARSING_PROMPT = prompts['source_parsing_prompt']\n\n    llm_filter_response = await together_client.chat.completions.create(\n        model=json_model,\n        messages=[\n            {\"role\": \"system\", \"content\": FILTER_PROMPT},\n            {\"role\": \"user\", \"content\": (\n                f\"<Research Topic>{topic}</Research Topic>\\n\\n\"\n                f\"<Current Search Results>{formatted_results}</Current Search Results>\"\n            )}\n        ]\n    )\n\n    llm_filter_response_content = llm_filter_response.choices[0].message.content\n    print(f'filter response: {llm_filter_response_content}')\n\n    json_response = await together_client.chat.completions.create(\n        model=json_model,\n        messages=[\n            {\"role\": \"system\", \"content\": SOURCE_PARSING_PROMPT},\n            {\"role\": \"user\", \"content\": f\"<FILTER_RESPONSE>{llm_filter_response_content}</FILTER_RESPONSE>\"}\n        ],\n        response_format={\"type\": \"json_object\", \"schema\": SourceList.model_json_schema()}\n    )\n\n    response_json = json_response.choices[0].message.content\n    evaluation = json.loads(response_json)\n    sources = evaluation[\"sources\"]\n\n    print(f'sources ranked by relevance {sources} (we will keep maximum of {max_sources} sources, as defined by the user)')\n\n    if max_sources > 0:\n        sources = sources[:max_sources]\n\n    # Filter the results based on the source list\n    filtered_results = [results.results[i] for i in sources if i < len(results.results)]\n\n    return SearchResults(filtered_results), sources\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Agentic RAG System\nDESCRIPTION: This code snippet installs the necessary Python libraries for building the agentic RAG system, including LangChain, Together AI, Chroma, LangGraph, and other dependencies.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture --no-stderr\n%pip install -qU langchain-community tiktoken langchain-together together langchainhub chromadb langchain langgraph langchain-text-splitters beautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Deploying FastAPI and Gradio Applications with Union in Python\nDESCRIPTION: This Python snippet defines the deployment specifications for FastAPI and Gradio applications using the Union framework. It creates two applications, one for each framework, specifying input sources, container images, resource limits, and network settings. Required packages and environment variables are configured within the deployment specification. The snippet includes details on how to initialize these apps, set up dependencies, and the use of secrets for secure access. The FastAPI app is configured to use 'uvicorn' for serving, while the Gradio app runs a Python script. These applications form part of an experimental setup on the Union platform.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom union.app import App, Input\n\n\nfastapi_app = App(\n    name=\"contextual-rag-fastapi\",\n    inputs=[\n        Input(\n            name=\"bm25s_index\",\n            value=BM25Index.query(),\n            download=True,\n            env_var=\"BM25S_INDEX\",\n        ),\n        Input(\n            name=\"contextual_chunks_json\",\n            value=ContextualChunksJSON.query(),\n            download=True,\n            env_var=\"CONTEXTUAL_CHUNKS_JSON\",\n        ),\n    ],\n    container_image=union.ImageSpec(\n        name=\"contextual-rag-fastapi\",\n        packages=[\n            \"together\",\n            \"bm25s\",\n            \"pymilvus\",\n            \"uvicorn[standard]\",\n            \"fastapi[standard]\",\n            \"union-runtime>=0.1.10\",\n            \"flytekit>=1.15.0b5\",\n        ],\n    ),\n    limits=union.Resources(cpu=\"1\", mem=\"3Gi\"),\n    port=8080,\n    include=[\"fastapi_app.py\"],\n    args=[\"uvicorn\", \"fastapi_app:app\", \"--port\", \"8080\"],\n    min_replicas=1,\n    max_replicas=1,\n    secrets=[\n        fl.Secret(\n            key=\"together-api-key\", \n            env_var=\"TOGETHER_API_KEY\", \n            mount_requirement=union.Secret.MountType.ENV_VAR\n        ),\n        fl.Secret(\n            key=\"milvus-uri\",\n            env_var=\"MILVUS_URI\",\n            mount_requirement=union.Secret.MountType.ENV_VAR,\n        ),\n        fl.Secret(\n            key=\"milvus-token\",\n            env_var=\"MILVUS_TOKEN\",\n            mount_requirement=union.Secret.MountType.ENV_VAR,\n        ),\n    ],\n)\n\n\ngradio_app = App(\n    name=\"contextual-rag-gradio\",\n    inputs=[\n        Input(\n            name=\"fastapi_endpoint\",\n            value=fastapi_app.query_endpoint(public=False),\n            env_var=\"FASTAPI_ENDPOINT\",\n        )\n    ],\n    container_image=union.ImageSpec(\n        name=\"contextual-rag-gradio\",\n        packages=[\"gradio\", \"union-runtime>=0.1.5\"],\n    ),\n    limits=union.Resources(cpu=\"1\", mem=\"1Gi\"),\n    port=8080,\n    include=[\"gradio_app.py\"],\n    args=[\n        \"python\",\n        \"gradio_app.py\",\n    ],\n    min_replicas=1,\n    max_replicas=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Indexing PDF with ColQwen2 for MultiModal RAG\nDESCRIPTION: Uses ColQwen2 to index the Nvidia presentation PDF, creating embeddings for page images and storing them along with base64 encoded images for retrieval.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nindex_name = \"nvidia_index\"\nmodel.index(input_path=Path(\"/content/nvidia_presentation.pdf\"),\n    index_name=index_name,\n    store_collection_with_index=True, # Stores base64 images along with the vectors\n    overwrite=True\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Flight Booking Workflow in Python\nDESCRIPTION: The main function that orchestrates the flight booking process. It configures search parameters, searches for flights, presents results to the user, and handles the booking flow. The loop continues until the user decides to book a flight or no flights are found.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    # Configure our search parameters\n    deps = Deps(\n        web_page_text=flights_web_page,\n        req_origin='SFO',\n        req_destination='ANC',\n        req_date=datetime.date(2025, 1, 10),\n    )\n    message_history: list[ModelMessage] | None = None\n    usage: Usage = Usage()\n    \n    # Main application loop - continue until user books or no flights found\n    while True:\n        # Run the search agent with our parameters\n        result = await search_agent.run(\n            f'Find me a flight from {deps.req_origin} to {deps.req_destination} on {deps.req_date}',\n            deps=deps,\n            usage=usage,\n            message_history=message_history,\n            usage_limits=usage_limits,\n        )\n        \n        # Process the result\n        if isinstance(result.data, NoFlightFound):\n            print('No flight found')\n            break\n        else:\n            flight = result.data\n            print(f'Flight found: {flight}')\n            answer = Prompt.ask(\n                'Do you want to buy this flight, or keep searching? (buy/*search)',\n                choices=['buy', 'search', ''],\n                show_choices=False,\n            )\n            if answer == 'buy':\n                # If user wants to book, collect seat preference and complete purchase\n                seat = await find_seat(usage)\n                await buy_tickets(flight, seat)\n                break\n            else:\n                # Continue searching with context from previous interaction\n                # This helps the agent understand we want different results\n                message_history = result.all_messages(\n                    result_tool_return_content='Please suggest another flight'\n                )\n```\n\n----------------------------------------\n\nTITLE: Generating Knowledge Graph Using Together AI API\nDESCRIPTION: Function that calls the Together AI API with Meta-Llama-3.1-70B model in strict JSON mode to generate a knowledge graph based on input text, using the predefined schema.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Call the LLM with the JSON schema\ndef generate_graph(input) -> KnowledgeGraph:\n    together = Together(api_key = TOGETHER_API_KEY)\n\n    extract = together.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Build a knowledge graph to explain: {input}. Only answer in JSON.\",\n            }\n        ],\n        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n        response_format={\n            \"type\": \"json_object\",\n            \"schema\": KnowledgeGraph.model_json_schema(),\n        },\n    )\n\n    output = json.loads(extract.choices[0].message.content)\n    return output\n```\n\n----------------------------------------\n\nTITLE: Implementing a Multi-Agent System with Specialized Roles\nDESCRIPTION: Demonstrates how to create a team of specialized agents that work together, including a web search agent, a finance agent for retrieving financial data, and a coordinator agent that delegates tasks. This approach combines specialized capabilities for more complex tasks.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Agno/Agents_Agno.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom agno.agent import Agent\nfrom agno.models.together import Together\nfrom agno.tools.duckduckgo import DuckDuckGoTools\nfrom agno.tools.yfinance import YFinanceTools\n\n# Create a specialized agent for web searches\nweb_agent = Agent(\n    name=\"Web Agent\",\n    role=\"Search the web for information\",\n    # Using the larger Llama 3.3 (70B) model for better performance\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", max_tokens=4096),\n    tools=[DuckDuckGoTools()],\n    instructions=\"Always include sources\",\n    show_tool_calls=True,\n    markdown=True,\n)\n\n# Create a specialized agent for financial data\nfinance_agent = Agent(\n    name=\"Finance Agent\",\n    role=\"Get financial data\",\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", max_tokens=4096),\n    # Financial tools for stock data, analyst recommendations, and company info\n    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],\n    instructions=\"Use tables to display data\",\n    show_tool_calls=True,\n    markdown=True,\n)\n\n# Create a coordinator agent that manages the team\nagent_team = Agent(\n    # Provide the specialized agents as a team\n    team=[web_agent, finance_agent],\n    # The coordinator also uses a powerful model\n    model=Together(id=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", max_tokens=4096),\n    # Instructions for the final output\n    instructions=[\"Always include sources\", \"Use tables to display data\"],\n    show_tool_calls=True,\n    markdown=True,\n)\n\nagent_team.print_response(\"What's the market outlook and financial performance of AI semiconductor companies?\", stream=True)\n```\n\n----------------------------------------\n\nTITLE: Setting up the Planner\nDESCRIPTION: Configures the planner responsible for breaking down complex queries into steps. It uses a custom prompt and the Together AI Llama model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate\n\nplanner_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"For the given objective, come up with a simple step by step plan. \\\nThis plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\nThe result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n)\n\nplanner = planner_prompt | ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\").with_structured_output(Plan)\n```\n\n----------------------------------------\n\nTITLE: Building an Agent with Specialized Knowledge Base for Thai Cuisine\nDESCRIPTION: Creates an agent with a specialized knowledge base for Thai recipes by loading a PDF into a vector database. The agent prioritizes its knowledge base while still being able to search the web for supplementary information.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Agno/Agents_Agno.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Import additional components for knowledge base functionality\nfrom agno.agent import Agent\nfrom agno.models.together import Together\nfrom agno.embedder.together import TogetherEmbedder\nfrom agno.tools.duckduckgo import DuckDuckGoTools\nfrom agno.knowledge.pdf_url import PDFUrlKnowledgeBase\nfrom agno.vectordb.lancedb import LanceDb, SearchType\n\n# Create an agent with a specialized knowledge base for Thai recipes\nagent = Agent(\n    # Use Together AI's Llama 3.1 model\n    model=Together(id=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\", max_tokens=4096),\n    # Define the agent's role\n    description=\"You are a Thai cuisine expert!\",\n    # Give the agent specific instructions on how to use its knowledge\n    instructions=[\n        \"Search your knowledge base for Thai recipes.\",\n        \"If the question is better suited for the web, search the web to fill in gaps.\",\n        \"Prefer the information in your knowledge base over the web results.\"\n    ],\n    # Set up the knowledge base from a PDF URL\n    knowledge=PDFUrlKnowledgeBase(\n        urls=[\"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"],\n        # Configure the vector database for semantic search\n        vector_db=LanceDb(\n            uri=\"tmp/lancedb\",\n            table_name=\"recipes\",\n            search_type=SearchType.hybrid,  # Use hybrid search (keyword + semantic)\n            # Use Together AI's embedding model for vectorizing text\n            embedder=TogetherEmbedder(id=\"togethercomputer/m2-bert-80M-32k-retrieval\"),\n        ),\n    ),\n    # Add web search capability as a backup\n    tools=[DuckDuckGoTools()],\n    # Display tool usage in the output\n    show_tool_calls=True,\n    markdown=True\n)\n\n# Load the knowledge base (only needs to be done once)\nif agent.knowledge is not None:\n    agent.knowledge.load()\n\nagent.print_response(\"How do I make chicken and galangal in coconut milk soup\", stream=True)\nagent.print_response(\"What is the history of Thai curry?\", stream=True)\n```\n\n----------------------------------------\n\nTITLE: Formatting Retrieved Chunks for Context\nDESCRIPTION: Combines the top 5 retrieved chunks into a formatted string to provide context for the language model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Lets add the top 5 documents to a string\n\nformatted_chunks = ''\n\nfor i, chunk in enumerate(retrieved_chunks):\n    formatted_chunks += f\"Context {i+1}: {chunk}\\n\"\n\nprint(formatted_chunks)\n```\n\n----------------------------------------\n\nTITLE: Querying Mistral Small Model with Augmented Prompt via Together API in Python\nDESCRIPTION: This snippet demonstrates the final step of the thinking augmented generation process. It uses the Mistral Small model with a prompt that includes both the original question and the generated thought process.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Thinking_Augmented_Generation.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nanswer = client.chat.completions.create(\n  model=\"mistralai/Mistral-Small-24B-Instruct-2501\",\n  messages=[{\"role\": \"user\", \n             \"content\": PROMPT_TEMPLATE.format(question = question,\n                                               thinking_tokens=thought.choices[0].message.content\n                                               )}],\n)\n\nprint(answer.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Generic Router Implementation - Flexible Routing\nDESCRIPTION: Provides a generic implementation of the router workflow that can handle arbitrary routes and models\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef router_workflow(input_query: str, routes : Dict[str, str]) -> str:\n    ROUTER_PROMPT = \"\"\"Given a user prompt/query: {user_query}, select the best option out of the following routes:\n    {routes}. Answer only in JSON format.\"\"\"\n\n    class Schema(BaseModel):\n        route: Literal[tuple(routes.keys())]\n    \n        reason: str = Field(\n            description=\"Short one-liner explanation why this route was selected for the task in the prompt/query.\"\n        )\n\n    selected_route = JSON_llm(ROUTER_PROMPT.format(user_query=input_query, routes=routes), Schema)\n    print(f\"Selcted route:{selected_route['route']}\\nReason: {selected_route['reason']}\\n\")\n\n    response = run_llm(user_prompt= input_query, model = selected_route['route'])\n    print(f\"Response: {response}\\n\")\n    \n    return response\n```\n\n----------------------------------------\n\nTITLE: Implementing the Orchestrator Workflow Function\nDESCRIPTION: Defines the core orchestrator workflow function that uses an orchestrator model to break down tasks, assigns subtasks to worker models, and executes them in parallel.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Task(BaseModel):\n    type: Literal[\"plan\", \"code/solve\", \"test\"]\n    description: str\n\nclass TaskList(BaseModel):\n    analysis: str\n    tasks: List[Task]  = Field(..., default_factory=list)\n\nasync def orchestrator_workflow(task : str, orchestrator_prompt : str, worker_prompt : str): \n    \"\"\"Use a orchestrator model to break down a task into sub-tasks and then use worker models to generate and return responses.\"\"\"\n\n    # Use orchestrator model to break the task up into sub-tasks\n    orchestrator_response = JSON_llm(orchestrator_prompt.format(task=task), schema=TaskList)\n \n    # Parse orchestrator response\n    analysis = orchestrator_response[\"analysis\"]\n    tasks= orchestrator_response[\"tasks\"]\n\n    print(\"\\n=== ORCHESTRATOR OUTPUT ===\")\n    print(f\"\\nANALYSIS:\\n{analysis}\")\n    print(f\"\\nTASKS:\\n{json.dumps(tasks, indent=2)}\")\n\n    worker_model =  [\"Qwen/Qwen2.5-Coder-32B-Instruct\"]*len(tasks)\n\n    # Gather intermediate responses from worker models\n    return tasks , await asyncio.gather(*[run_llm_parallel(user_prompt=worker_prompt.format(original_task=task, task_type=task_info['type'], task_description=task_info['description']), model=model) for task_info, model in zip(tasks,worker_model)])\n```\n\n----------------------------------------\n\nTITLE: Creating Document Chunks in Python\nDESCRIPTION: Actor task that splits document content into chunks with specified size and overlap parameters.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@actor.task(cache=True, cache_version=\"0.2\")\ndef create_chunks(document: Document, chunk_size: int, overlap: int) -> Document:\n    if document.content:\n        content_chunks = [\n            document.content[i : i + chunk_size]\n            for i in range(0, len(document.content), chunk_size - overlap)\n        ]\n        document.chunks = content_chunks\n    return document\n```\n\n----------------------------------------\n\nTITLE: Executing the Orchestrator Workflow with a Sample Task\nDESCRIPTION: Runs the orchestrator workflow with a sample task to demonstrate how it breaks down the task and processes the subtasks in parallel.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntask = \"\"\"Write a program that prints the next 20 leap years.\n\"\"\"\n\ntasks, worker_resp = await orchestrator_workflow(task, orchestrator_prompt=ORCHESTRATOR_PROMPT, worker_prompt=WORKER_PROMPT)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Summarization with Together API\nDESCRIPTION: Defines a function to generate a summary of text using the Together API client, sending a formatted prompt to the Meta-Llama-3.1-70B-Instruct-Turbo model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\nclient = Together(api_key = \"__TOGETHER_API_KEY__\")\n\n\ndef summarize(text: str, prompt: str) -> str:\n    response = client.chat.completions.create(\n        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt.format(full_text = text)}],\n    )\n\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Defining Fine-tuning Job Function for Summarization in Python\nDESCRIPTION: This function sends a fine-tuning job to the Together platform. It sets up parameters for the fine-tuning process, including model selection, epochs, learning rate, and dataset file. It supports both synthetic and GovReport datasets.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef send_ft_job(client,\n                model=\"meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference\",\n                n_epochs=4,\n                run_name='1113-summarization-long-context-finetune',\n                train_on_inputs=False,\n                learning_rate=6e-5,\n                filename=None,\n                summarization_file_id=None):\n    \"\"\"\n    Sends a fine-tuning job to the client.\n    Parameters:\n        client (object): The client object to interact with the API.\n        model (str): The model to be fine-tuned. Default is \"meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference\".\n        n_epochs (int): Number of epochs for fine-tuning. Default is 4.\n        run_name (str): The name for the run. Default is '1113-summarization-long-context-finetune'.\n        train_on_inputs (bool): Whether to train on inputs. Default is False.\n        learning_rate (float): The learning rate for fine-tuning. Default is 6e-5.\n        filename (str, optional): The filename to upload for fine-tuning. Default is None.\n        summarization_file_id (str, optional): The file ID for summarization. Must be provided if filename is None.\n    Returns:\n        str: The ID of the fine-tuning job.\n    Raises:\n        AssertionError: If neither filename nor summarization_file_id is provided.\n    \"\"\"\n    if filename:\n        response = client.files.upload(filename, check=True)\n        summarization_file_id = response.id\n    else:\n        assert summarization_file_id is not None, \"provide summarization_file_id\"\n\n    response = client.fine_tuning.create(\n        training_file = summarization_file_id,\n        model = model,\n        n_epochs = n_epochs,\n        n_checkpoints = 1,\n        batch_size = \"max\",\n        learning_rate = learning_rate,\n        warmup_ratio = 0.05,\n        suffix=run_name,\n        wandb_api_key = WANDB_API_KEY,\n        lora=True,\n        lora_r=32,\n        lora_alpha=64,\n        lora_dropout=0.05,\n        train_on_inputs=train_on_inputs,\n    )\n    summarization_fine_tuning_job_id = response.id\n    return summarization_fine_tuning_job_id\n```\n\n----------------------------------------\n\nTITLE: Generating Conditional Image from Image-to-Image Search Result in Python\nDESCRIPTION: Creates a holiday cartoon version of the image retrieved through image-to-image search using the generate_image function and FLUX model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Generate a holiday cartoon version of the retrieved image\ngenerated_image_2 = generate_image(image_prompt=\"Create a cute holiday cartoon version of this image.\", retrieved_image = image_2_image)\n```\n\n----------------------------------------\n\nTITLE: Implementing Helper Functions for LLM Calls\nDESCRIPTION: Defines helper functions for making both regular and JSON-structured calls to language models with the Together API, handling prompts and responses.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Simple LLM call helper function\ndef run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):\n    \"\"\" Run the language model with the given user prompt and system prompt. \"\"\"\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n    messages.append({\"role\": \"user\", \"content\": user_prompt})\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0.7,\n        max_tokens=4000,        \n    )\n\n    return response.choices[0].message.content\n\n# Simple JSON mode LLM call helper function\ndef JSON_llm(user_prompt : str, schema : BaseModel, system_prompt : Optional[str] = None):\n    \"\"\" Run a language model with the given user prompt and system prompt, and return a structured JSON object. \"\"\"\n    try:\n        messages = []\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        \n        messages.append({\"role\": \"user\", \"content\": user_prompt})\n        \n        extract = client.chat.completions.create(\n            messages=messages,\n            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n            response_format={\n                \"type\": \"json_object\",\n                \"schema\": schema.model_json_schema(),\n            },\n        )\n        \n        response = json.loads(extract.choices[0].message.content)\n        return response\n        \n    except ValidationError as e:\n        raise ValueError(f\"Schema validation failed: {str(e)}\")\n\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Text Chunks\nDESCRIPTION: A function that uses the Together AI API to generate vector embeddings for text chunks using the BGE large English embedding model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\nimport numpy as np\n\ndef generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n    \"\"\"Generate embeddings from Together python library.\n\n    Args:\n        input_texts: a list of string input texts.\n        model_api_string: str. An API string for a specific embedding model of your choice.\n\n    Returns:\n        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n    \"\"\"\n    outputs = client.embeddings.create(\n        input=input_texts,\n        model=model_api_string,\n    )\n    return np.array([x.embedding for x in outputs.data])\n```\n\n----------------------------------------\n\nTITLE: Compiling Document Strings from Hybrid Results in Python\nDESCRIPTION: This snippet retrieves documents based on the indices from the hybrid results and prepares a string containing these documents, which will be utilized for further processing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n[contextual_chunks[index] for index in hybrid_top_k[1]]\n```\n\n----------------------------------------\n\nTITLE: Initializing JinaCLIP Model for Image Embedding in Python\nDESCRIPTION: Sets up the JinaCLIP model to generate vector embeddings for images. This model unifies text and image representations for multimodal search.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModel\n\n# Initialize the model\nmodel = AutoModel.from_pretrained('jinaai/jina-clip-v1', trust_remote_code=True)\n\n# Encode text and images\nimage_embeddings = model.encode_image(links)  # also accepts PIL.image, local filenames, dataURI\n\nimage_embeddings.shape\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent, Rewrite, and Generate Functions\nDESCRIPTION: This code defines three key functions for the agentic RAG system: agent (decides whether to retrieve or end), rewrite (transforms the query for better results), and generate (produces the final answer). These functions use the ChatTogether model for language processing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n### Nodes\ndef agent(state):\n    \"\"\"\n    Invokes the agent model to generate a response based on the current state. Given\n    the question, it will decide to retrieve using the retriever tool, or simply end.\n\n    Args:\n        state (messages): The current state\n\n    Returns:\n        dict: The updated state with the agent response appended to messages\n    \"\"\"\n    print(\"---CALL AGENT---\")\n    messages = state[\"messages\"]\n    \n    model = ChatTogether(api_key=os.getenv(\"TOGETHER_API_KEY\"), model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",temperature=0, streaming=True)\n    model = model.bind_tools(tools)\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\ndef rewrite(state):\n    \"\"\"\n    Transform the query to produce a better question.\n\n    Args:\n        state (messages): The current state\n\n    Returns:\n        dict: The updated state with re-phrased question\n    \"\"\"\n\n    print(\"---TRANSFORM QUERY---\")\n    messages = state[\"messages\"]\n    question = messages[0].content\n\n    msg = [\n        HumanMessage(\n            content=f\"\"\" \\n \n    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n    Here is the initial question:\n    \\n ------- \\n\n    {question} \n    \\n ------- \\n\n    Formulate an improved question: \"\"\",\n        )\n    ]\n\n    # Grader\n    model = ChatTogether(api_key=os.getenv(\"TOGETHER_API_KEY\"), model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", temperature=0, streaming=True)\n    response = model.invoke(msg)\n    return {\"messages\": [response]}\n\n\ndef generate(state):\n    \"\"\"\n    Generate answer\n\n    Args:\n        state (messages): The current state\n\n    Returns:\n         dict: The updated state with re-phrased question\n    \"\"\"\n    print(\"---GENERATE---\")\n    messages = state[\"messages\"]\n    question = messages[0].content\n    last_message = messages[-1]\n\n    docs = last_message.content\n\n    # Prompt\n    prompt = hub.pull(\"rlm/rag-prompt\")\n\n    # LLM\n    llm = ChatTogether(api_key=os.getenv(\"TOGETHER_API_KEY\"), model=\"deepseek-ai/DeepSeek-R1\", temperature=0.6,streaming=True)\n\n    # Post-processing\n    def format_docs(docs):\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n    # Chain\n    rag_chain = prompt | llm | StrOutputParser()\n\n    # Run\n    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n    return {\"messages\": [response]}\n\n\nprint(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\nprompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like\n```\n\n----------------------------------------\n\nTITLE: Swapping Between LoRA Adapters for Inference\nDESCRIPTION: Demonstrates how to loop through multiple LoRA adapters and perform inference with the same prompt. This showcases the flexibility of LoRA for quickly switching between different finetuned versions without reloading the full model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Loop over different LoRA fine-tunes and call with same query\n\nfor adapter in LoRA_adapters:\n    \n    response = client.chat.completions.create(\n    model = adapter,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a short haiku about elephants.\",\n        }\n    ],\n    max_tokens=124,\n    temperature=0.7,\n    )\n\n    print(f\"Response from {adapter}:\\n\")\n\n    print(response.choices[0].message.content)\n\n    print('\\n'+20*'######'+'\\n')\n```\n\n----------------------------------------\n\nTITLE: Initial Fine-tuning on Core Function Calling Categories\nDESCRIPTION: Performs the first step of fine-tuning using LoRA on the Llama-3.2-1B-Instruct model with core function calling categories.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nft_fc1 = client.fine_tuning.create(\n    training_file = resp_fc1.id, # function_calling_1.jsonl\n    model = 'meta-llama/Llama-3.2-1B-Instruct',\n    n_epochs = 1,\n    n_checkpoints = 1,\n    batch_size = 16,\n    lora=True,\n    warmup_ratio=0.1,\n    wandb_api_key=WANDB_API_KEY,\n)\nprint(ft_fc1.id)\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Comparing Model Performance with ROUGE Metrics in Python\nDESCRIPTION: This code iterates through different model configurations (baseline and fine-tuned) on two datasets (Synthetic and GovReport), computes ROUGE-L scores for each, and prints the average score. It demonstrates how to systematically compare model performance across different configurations.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor k, (predictions_, test_items_) in zip(\n    [\"Synthetic Baseline:\", \"Synthetic Fine-tune:\", \"GovReport Baseline:\", \"GovReport Fine-tune:\"],\n    [(predictions_baseline_syn, test_items), (predictions_ft_syn, test_items),\n     (predictions_baseline_govreport, test_items_govreport), (predictions_ft_govreport, test_items_govreport),]\n):\n\n    rouge_scores = compute_rouge_l(predictions_, test_items_)\n    print(k, f\"ROUGE: {np.mean(rouge_scores):.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Research Completeness Evaluation in Python\nDESCRIPTION: This function evaluates if the current search results are sufficient or if more research is needed. It takes the research topic, current search results, a list of queries already used, Together AI client, models for planning and JSON parsing, and a dictionary of prompt templates as input. It returns a list of additional queries needed, or an empty list if the research is complete. It leverages LLMs to assess completeness and generate follow-up queries.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nasync def evaluate_research_completeness(topic: str, results: SearchResults, queries: List[str],\n                                       together_client: AsyncTogether, planning_model: str, json_model: str, prompts: dict) -> list[str]:\n    \"\"\"\n    Evaluate if the current search results are sufficient or if more research is needed.\n\n    Args:\n        topic: The research topic\n        results: Current search results\n        queries: List of queries already used\n        together_client: The Together AI client for LLM operations\n        planning_model: Model to use for evaluation\n        json_model: Model to use for JSON parsing\n        prompts: Dictionary of prompt templates\n\n    Returns:\n        List of additional queries needed or empty list if research is complete\n    \"\"\"\n    # Format the search results for the LLM\n    formatted_results = str(results)\n\n    EVALUATION_PROMPT = prompts[\"evaluation_prompt\"]\n\n    evaluation_response = await together_client.chat.completions.create(\n        model=planning_model,\n        messages=[\n            {\"role\": \"system\", \"content\": EVALUATION_PROMPT},\n            {\"role\": \"user\", \"content\": (\n                f\"<Research Topic>{topic}</Research Topic>\\n\\n\"\n                f\"<Search Queries Used>{queries}</Search Queries Used>\\n\\n\"\n                f\"<Current Search Results>{formatted_results}</Current Search Results>\"\n            )}\n        ]\n    )\n    evaluation = evaluation_response.choices[0].message.content\n\n    print(\"================================================\\n\\n\")\n    print(f\"Evaluation:\\n\\n {evaluation}\")\n\n    EVALUATION_PARSING_PROMPT = prompts[\"evaluation_parsing_prompt\"]\n\n    json_response = await together_client.chat.completions.create(\n        model=json_model,\n        messages=[\n            {\"role\": \"system\", \"content\": EVALUATION_PARSING_PROMPT},\n            {\"role\": \"user\", \"content\": f\"Evaluation to be parsed: {evaluation}\"}\n        ],\n        response_format={\"type\": \"json_object\", \"schema\": ResearchPlan.model_json_schema()}\n    )\n\n    response_json = json_response.choices[0].message.content\n    evaluation = json.loads(response_json)\n    return evaluation[\"queries\"]\n```\n\n----------------------------------------\n\nTITLE: Creating a Summarization Prompt Template\nDESCRIPTION: Defines a comprehensive prompt template for instructing an LLM to generate a concise and detailed summary of the provided text, with specific guidelines for format and content.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nSUMMARIZATION_PROMPT = \"\"\"As a professional summarizer, create a concise and comprehensive summary of the provided text, be it an article, post, conversation, or passage, while adhering to these guidelines:\n\n1. Craft a summary that is detailed, thorough, in-depth, and complex, while maintaining clarity and conciseness.\n\n2. Incorporate main ideas and essential information, eliminating extraneous language and focusing on critical aspects.\n\n3. Rely strictly on the provided text, without including external information.\n\n4. Format the summary into a paragraph form for easy understanding.\n\nThe input may be unstructured or messy, sourced from PDFs or web pages. Disregard irrelevant information or formatting issues.\n\n{full_text}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Research Pipeline Parameters in Python\nDESCRIPTION: Sets up the core configuration parameters for the research pipeline, including budget (number of research cycles), query limits, source limits, and token limits for the final report.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Parameters controlling research depth and breadth\nbudget = 2  # Number of research refinement cycles to perform (in addition to the initial search operation)\nmax_queries = 2  # Maximum number of search queries per research cycle\nmax_sources = 10  # Maximum number of sources to include in final synthesis\nmax_tokens = 8192 # Maximum number of tokens in the generated report\n```\n\n----------------------------------------\n\nTITLE: Printing Reordered Chunks Based on RRF Score in Python\nDESCRIPTION: This snippet iterates through the combined results from the RRF output and prints the indices along with their corresponding contextual chunks, showcasing the new order based on relevance.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfor index in hybrid_top_k[1]:\n  print(f\"Chunk Index {index} : {contextual_chunks[index]}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Seat Selection Agent with PydanticAI\nDESCRIPTION: This snippet defines a separate agent for handling natural language seat preferences. It includes models for seat preference and failure cases, and sets up the agent with appropriate prompts.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SeatPreference(BaseModel):\n    row: int = Field(ge=1, le=30)\n    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']\n\n\nclass Failed(BaseModel):\n    \"\"\"Unable to extract a seat selection.\"\"\"\n\n\n# This agent is responsible for extracting the user's seat selection\nseat_preference_agent = Agent[None, SeatPreference | Failed](\n    model=llm,\n    result_type=SeatPreference | Failed,  # type: ignore\n    system_prompt=(\n        \"Extract the user's seat preference. \"\n        'Seats A and F are window seats. '\n        'Row 1 is the front row and has extra leg room. '\n        'Rows 14, and 20 also have extra leg room. '\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Embedding Generation Function Using Together API\nDESCRIPTION: Defines a function to generate embeddings for input texts using the Together API. The function takes a list of texts and a model identifier, returning a list of vector embeddings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport together\nimport numpy as np\n\ndef generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n    \"\"\"Generate embeddings from Together python library.\n\n    Args:\n        input_texts: a list of string input texts.\n        model_api_string: str. An API string for a specific embedding model of your choice.\n\n    Returns:\n        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n    \"\"\"\n    outputs = client.embeddings.create(\n        input=input_texts,\n        model=model_api_string,\n    )\n    return [x.embedding for x in outputs.data]\n```\n\n----------------------------------------\n\nTITLE: Implementing Reciprocal Rank Fusion (RRF) Algorithm in Python\nDESCRIPTION: This function fuses rankings from multiple information retrieval systems using the RRF method. It calculates scores for ranked documents from each system and returns a combined list of documents sorted by their score.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\n\ndef reciprocal_rank_fusion(*list_of_list_ranks_system, K=60):\n    \"\"\"\n    Fuse rank from multiple IR systems using Reciprocal Rank Fusion.\n\n    Args:\n    * list_of_list_ranks_system: Ranked results from different IR system.\n    K (int): A constant used in the RRF formula (default is 60).\n\n    Returns:\n    Tuple of list of sorted documents by score and sorted documents\n    \"\"\"\n    # Dictionary to store RRF mapping\n    rrf_map = defaultdict(float)\n\n    # Calculate RRF score for each result in each list\n    for rank_list in list_of_list_ranks_system:\n        for rank, item in enumerate(rank_list, 1):\n            rrf_map[item] += 1 / (rank + K)\n\n    # Sort items based on their RRF scores in descending order\n    sorted_items = sorted(rrf_map.items(), key=lambda x: x[1], reverse=True)\n\n    # Return tuple of list of sorted documents by score and sorted documents\n    return sorted_items, [item for item, score in sorted_items]\n```\n\n----------------------------------------\n\nTITLE: Creating Visualization Function with GraphViz\nDESCRIPTION: Defines a function to visualize the knowledge graph using GraphViz, transforming the JSON structure into a directed graph with labeled nodes and edges.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom graphviz import Digraph\n\ndef visualize_knowledge_graph(kg):\n    dot = Digraph(comment=\"Knowledge Graph\", format='png')\n\n    # Add nodes\n    for node in kg['nodes']:\n        dot.node(str(node['id']), node['label'])\n\n    # Add edges\n    for edge in kg['edges']:\n        dot.edge(str(edge['source']), str(edge['target']), label=edge['label'])\n\n    # Render the graph to a file and open it\n    output_path = dot.render(\"knowledge_graph\", view=True)\n    print(f\"Graph rendered and saved to {output_path}\")\n```\n\n----------------------------------------\n\nTITLE: Displaying the Generated Knowledge Graph JSON\nDESCRIPTION: Outputs the JSON structure of the generated knowledge graph, showing the nodes and edges created by the LLM.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Lets see the knowlege graph components generated!\ngraph\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Embeddings and Index in Python\nDESCRIPTION: Functions to generate embeddings using Together AI and store them in Milvus vector database with caching and retry functionality.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\ndef get_embedding(chunk: str, embedding_model: str):\n    client = Together(\n        api_key=os.getenv(\"TOGETHER_API_KEY\")\n    )\n    outputs = client.embeddings.create(\n        input=chunk,\n        model=embedding_model,\n    )\n    return outputs.data[0].embedding\n\n\n@actor.task(cache=True, cache_version=\"0.19\", retries=5)\ndef create_vector_index(\n    document: Document, embedding_model: str, local: bool = False\n) -> Document:\n    from pymilvus import DataType, MilvusClient\n\n    if local:\n        client = MilvusClient(\"test_milvus.db\")\n    else:\n        try:\n            client = MilvusClient(uri=os.getenv(\"MILVUS_URI\"), token=os.getenv(\"MILVUS_TOKEN\"))\n        except Exception as e:\n            raise FlyteRecoverableException(\n                f\"Failed to connect to Milvus: {e}\"\n            )\n\n    collection_name = \"paul_graham_collection\"\n\n    if not client.has_collection(collection_name):\n        schema = client.create_schema()\n        schema.add_field(\n            \"id\", DataType.INT64, is_primary=True, auto_id=True\n```\n\n----------------------------------------\n\nTITLE: Executing Tool Calls and Updating Context\nDESCRIPTION: Runs the tools selected by the LLM using the arguments it provided, and updates the conversation context with the results. This allows the LLM to use the data from the tool execution.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntool_run = th.run_tools(response)\nmessages += tool_run\n\n# Here's what the messages variable contains now\nprint(messages)\n```\n\n----------------------------------------\n\nTITLE: Inference with Fine-tuned LLM on Together AI Platform\nDESCRIPTION: Demonstrates how to use a previously fine-tuned model for inference on the Together AI platform. The code specifies the model name and sends a user prompt requesting the model to respond in the style of Crush from Finding Nemo.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfinetuned_model = \"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-helpsteer2_dpo_training_continuing_sft-cf1147c8\"#dpo_training.output_name #this is the name of the finetuned model\n\nuser_prompt = \"\"\"I want you to act like Crush the sea turtle from Finding Nemo. \nI want you to respond and answer like Crush using the tone, manner and vocabulary Crush would use. Do not write any explanations. \nOnly answer like Crush. \nYou must know all of the knowledge of Crush.\n\"\"\"\n\nresponse = client.chat.completions.create(\n    model = finetuned_model,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": user_prompt,\n        }\n    ]\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Creating a ReAct Agent\nDESCRIPTION: Sets up a ReAct agent that can use tools and reason about their outputs. It uses the previously defined LLM and tools.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain import hub\n\nfrom langgraph.prebuilt import create_react_agent\n\n# Choose the LLM that will drive the agent\nprompt = \"You are a helpful assistant.\"\nagent_executor = create_react_agent(llm, tools, prompt=prompt)\n```\n\n----------------------------------------\n\nTITLE: Building BM25 Search Index Using Python\nDESCRIPTION: This snippet imports the bm25s library to create a BM25 model and index a corpus of contextual chunks, enabling efficient lexical searching based on the words present in the query.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport bm25s\n\n# Create the BM25 model and index the corpus\nretriever = bm25s.BM25(corpus=contextual_chunks)\nretriever.index(bm25s.tokenize(contextual_chunks))\n```\n\n----------------------------------------\n\nTITLE: Defining a Workflow for Index Building\nDESCRIPTION: This code defines a Flyte workflow `build_indices_wf` that orchestrates several tasks: parsing a main page, scraping content, creating chunks, generating context, creating a vector index, and building a BM25S index. The workflow utilizes `union.map_task` to execute tasks in parallel. The function also defines expected inputs and outputs using type annotations.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nfrom dataclasses import dataclass\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # Ensure the secret (together API key) is present in the .env file\n\nBM25Index = Artifact(name=\"bm25s-index\")\nContextualChunksJSON = Artifact(name=\"contextual-chunks-json\")\n\n\n@union.workflow\ndef build_indices_wf(\n    base_url: str = \"https://paulgraham.com/\",\n    articles_url: str = \"articles.html\",\n    embedding_model: str = \"BAAI/bge-large-en-v1.5\",\n    chunk_size: int = 250,\n    overlap: int = 30,\n    model: str = \"deepseek-ai/DeepSeek-R1\",\n    local: bool = True,\n) -> tuple[\n    Annotated[FlyteDirectory, BM25Index], Annotated[FlyteFile, ContextualChunksJSON]\n]:\n    tocs = parse_main_page(base_url=base_url, articles_url=articles_url, local=local)\n    scraped_content = union.map_task(scrape_pg_essays, concurrency=2)(document=tocs)\n    chunks = union.map_task(\n        functools.partial(create_chunks, chunk_size=chunk_size, overlap=overlap)\n    )(document=scraped_content)\n    contextual_chunks = union.map_task(functools.partial(generate_context, model=model))(document=chunks)\n    union.map_task(\n        functools.partial(\n            create_vector_index, embedding_model=embedding_model, local=local\n        ),\n        concurrency=2\n    )(document=contextual_chunks)\n    bm25s_index, contextual_chunks_json_file = create_bm25s_index(\n        documents=contextual_chunks\n    )\n    return bm25s_index, contextual_chunks_json_file\n```\n\n----------------------------------------\n\nTITLE: Generating Image with LoRA Trigger Words\nDESCRIPTION: Shows how to use trigger words to activate the LoRA fine-tune effectively. In this case, it adds a specific phrase to activate the tarot card style LoRA.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a baby panda eating bamboo in the style of TOK a trtcrd tarot style\"\n\ngenerated_image = generate_image(prompt, \n                                 lora1=\"https://huggingface.co/multimodalart/flux-tarot-v1\",\n                                 scale1=1.0\n                                 )\n\nImage(url=generated_image, width=512, height=384)\n```\n\n----------------------------------------\n\nTITLE: Applying Transformation to CoQA Dataset\nDESCRIPTION: Applies the mapping function to transform the entire CoQA dataset into the required chat format for fine-tuning. This creates a new dataset with only the 'messages' column containing properly formatted conversations.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# transform the data using the mapping function\n\ntrain_messages = coqa_dataset[\"train\"].map(map_fields, remove_columns=coqa_dataset[\"train\"].column_names)\n```\n\n----------------------------------------\n\nTITLE: Implementing Retriever Function\nDESCRIPTION: Creates a function to retrieve top-k similar items based on cosine similarity of embeddings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef retreive(query: str, top_k: int = 5, index: np.ndarray = None) -> List[int]:\n    \"\"\"\n    Retrieve the top-k most similar items from an index based on a query.\n    Args:\n        query (str): The query string to search for.\n        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.\n        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.\n    Returns:\n        List[int]: A list of indices corresponding to the top-k most similar items in the index.\n    \"\"\"\n    \n    query_embedding = generate_embeddings([query], 'BAAI/bge-base-en-v1.5')[0]\n    similarity_scores = cosine_similarity([query_embedding], index)\n\n    return np.argsort(-similarity_scores)[0][:top_k]\n```\n\n----------------------------------------\n\nTITLE: Creating a ReAct Agent for Fact-Checking\nDESCRIPTION: Sets up a DSPy ReAct agent with a signature to find relevant Wikipedia titles for fact verification, using the previously defined search and lookup tools.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninstructions = \"Find all Wikipedia titles relevant to verifying (or refuting) the claim.\"\nsignature = dspy.Signature(\"claim -> titles: list[str]\", instructions)\nreact = dspy.ReAct(signature, tools=[search_wikipedia, lookup_wikipedia], max_iters=20)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Wikipedia Search with ColBERTv2\nDESCRIPTION: Configures a function to search Wikipedia using ColBERTv2, saving retrieved documents in a dictionary for later access.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nDOCS = {}\n\ndef search(query: str, k: int) -> list[str]:\n    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=k)\n    results = [x['text'] for x in results]\n\n    for result in results:\n        title, text = result.split(\" | \", 1)\n        DOCS[title] = text\n\n    return results\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Search with Initial Queries in Python\nDESCRIPTION: This line calls the perform_search function with the initial set of research queries. It starts the parallel search operation that will gather comprehensive information across multiple topics related to the research subject.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninitial_results=await perform_search(initial_queries, tavily_client, prompts, together_client, summary_model)\n```\n\n----------------------------------------\n\nTITLE: LLM Helper Functions Implementation\nDESCRIPTION: Implementation of helper functions for running LLM queries, including standard text responses and JSON-structured outputs.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):\n    \" Run the language model with the given user prompt and system prompt. \"\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n    messages.append({\"role\": \"user\", \"content\": user_prompt})\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0.7,\n        max_tokens=4000,        \n    )\n\n    return response.choices[0].message.content\n\ndef JSON_llm(user_prompt : str, schema : BaseModel, system_prompt : Optional[str] = None):\n    \" Run a language model with the given user prompt and system prompt, and return a structured JSON object. \"\n    try:\n        messages = []\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        \n        messages.append({\"role\": \"user\", \"content\": user_prompt})\n        \n        extract = client.chat.completions.create(\n            messages=messages,\n            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n            response_format={\n                \"type\": \"json_object\",\n                \"schema\": schema.model_json_schema(),\n            },\n        )\n        \n        response = json.loads(extract.choices[0].message.content)\n        return response\n        \n    except ValidationError as e:\n        raise ValueError(f\"Schema validation failed: {str(e)}\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Finetuned Models using Python\nDESCRIPTION: This code snippet iterates over a list of model names, retrieves answers from each model, computes evaluation metrics (Exact Match and F1), and prints the results. It is designed to facilitate the evaluation of the performance of finetuned models compared to their original versions.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodels_names = [\n    \"meta-llama/Meta-Llama-3.1-8B-Instruct-Reference\",\n    \"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-my-demo-finetune-4224205a\", # finetuned model goes here once deployed\n]\n\nfor model_name in models_names:\n    print(model_name)\n    answers = get_model_answers(model_name)\n    em_metric, f1_metric = get_metrics(answers)\n    print(f\"EM: {em_metric}, F1: {f1_metric}\")\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with LoRA Finetuned Model\nDESCRIPTION: Demonstrates how to use the finetuned model with LoRA adapter for inference. The code sends a simple question to the model and prints the response, appending '-adapter' to the model name to indicate LoRA inference.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = ft_resp.output_name\nuser_prompt = \"What is the capital of the France?\"\n\nresponse = client.chat.completions.create(\n    model = model_name + '-adapter',\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": user_prompt,\n        }\n    ],\n    max_tokens=124,\n    temperature=0.7,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Search and Lookup Tools for ReAct Agent\nDESCRIPTION: Defines two tools for the ReAct agent: search_wikipedia retrieves multiple results for a query, and lookup_wikipedia retrieves specific Wikipedia page content by title.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Now, let's use the search function to define two tools for our ReAct agent:\n\ndef search_wikipedia(query: str) -> list[str]:\n    \"\"\"Returns top-5 results and then the titles of the top-5 to top-30 results.\"\"\"\n\n    topK = search(query, 30)\n    titles, topK = [f\"`{x.split(' | ')[0]}`\" for x in topK[5:30]], topK[:5]\n    return topK + [f\"Other retrieved pages have titles: {', '.join(titles)}.\"] \n\ndef lookup_wikipedia(title: str) -> str:\n    \"\"\"Returns the text of the Wikipedia page, if it exists.\"\"\"\n\n    if title in DOCS:\n        return DOCS[title]\n\n    results = [x for x in search(title, 10) if x.startswith(title + \" | \")]\n    if not results:\n        return f\"No Wikipedia page found for title: {title}\"\n    return results[0]\n```\n\n----------------------------------------\n\nTITLE: Loading HoVer Dataset for Multi-Hop Fact Checking\nDESCRIPTION: Loads examples from the HoVer dataset containing complex multi-hop claims, filtering for 3-hop examples and splitting into train, dev, and test sets.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom dspy.datasets import DataLoader\n\nkwargs = dict(fields=(\"claim\", \"supporting_facts\", \"hpqa_id\", \"num_hops\"), input_keys=(\"claim\",))\nhover = DataLoader().from_huggingface(dataset_name=\"hover-nlp/hover\", split=\"train\", trust_remote_code=True, **kwargs)\n\nhpqa_ids = set()\nhover = [\n    dspy.Example(claim=x.claim, titles=list(set([y[\"key\"] for y in x.supporting_facts]))).with_inputs(\"claim\")\n    for x in hover\n    if x[\"num_hops\"] == 3 and x[\"hpqa_id\"] not in hpqa_ids and not hpqa_ids.add(x[\"hpqa_id\"])\n]\n\nrandom.Random(0).shuffle(hover)\ntrainset, devset, testset = hover[:100], hover[100:200], hover[650:]\n```\n\n----------------------------------------\n\nTITLE: Generating Context with Together AI in Python\nDESCRIPTION: Actor task that uses Together AI to generate context for each document chunk with caching enabled.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@actor.task(cache=True, cache_version=\"0.4\")\ndef generate_context(document: Document, model: str) -> Document:\n    from together import Together\n\n    CONTEXTUAL_RAG_PROMPT = \"\"\"\nGiven the document below, we want to explain what the chunk captures in the document.\n\n{WHOLE_DOCUMENT}\n\nHere is the chunk we want to explain:\n\n{CHUNK_CONTENT}\n\nAnswer ONLY with a succinct explanation of the meaning of the chunk in the context of the whole document above.\n\"\"\"\n\n    client = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n\n    contextual_chunks = [\n        f\"{response.choices[0].message.content} {chunk}\"\n        for chunk in (document.chunks or [])\n        for response in [\n            client.chat.completions.create(\n                model=model,\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": CONTEXTUAL_RAG_PROMPT.format(\n                            WHOLE_DOCUMENT=document.content,\n                            CHUNK_CONTENT=chunk,\n                        ),\n                    }\n                ],\n                temperature=1,\n            )\n        ]\n    ]\n\n    # Assign the contextual chunks back to the document\n    document.contextual_chunks = contextual_chunks if contextual_chunks else None\n    return document\n```\n\n----------------------------------------\n\nTITLE: Defining the Context Generation Function\nDESCRIPTION: Creates a function that calls the Llama-3.2-3B-Instruct-Turbo model via the Together API to generate contextual explanations for each chunk.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef generate_context(prompt: str):\n    \"\"\"\n    Generates a contextual response based on the given prompt using the specified language model.\n    Args:\n        prompt (str): The input prompt to generate a response for.\n    Returns:\n        str: The generated response content from the language model.\n    \"\"\"\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=1\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Together AI\nDESCRIPTION: Uploads the prepared JSONL file to Together AI's storage. The file ID returned will be used to reference this dataset when creating the fine-tuning job.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n## Upload the data to Together\n\ntrain_file_resp = client.files.upload(\"coqa_prepared_train.jsonl\", check=True)\nprint(f\"Train file response: {train_file_resp.id}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Story with RAG and Llama3\nDESCRIPTION: Uses retrieved movie information to generate a story using Llama3 8B model via Together AI's chat completions API.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclient = Together(api_key = TOGETHER_API_KEY)\n\n# Generate a story based on the top 10 most similar movies\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3-8b-chat-hf\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a pulitzer award winning craftful story teller. Given only the overview of different plots you can weave together an interesting storyline.\"},\n      {\"role\": \"user\", \"content\": f\"Tell me a story about {titles}. Here is some information about them {overviews}\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Executing LLM Request with Tool Integration\nDESCRIPTION: Sends a request to the LLM with available tools from Toolhouse, allowing the model to decide which tool to use. It includes a fix for compatibility issues with the image generation tool's description.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# let's pull tools from Toolhouse\ntools = th.get_tools(bundle=\"togethertoolhouse\")\n# change the value of the description field, when it's 'image_generation_flux'\n# this is for compatibility reasons\ncustom_tools = [t for t in tools if t['function']['name'] == 'image_generation_flux']\ncustom_tools[0]['function']['description'] = \"an image generation function\"\n\n# add the value back into the tools array overwriting the previous\ntools = [t if t['function']['name'] != 'image_generation_flux' else custom_tools[0] for t in tools]\n\n# now that we have fixed the tools array we can continue\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=messages,\n    tool_choice=\"auto\",\n    tools=tools\n)\nprint(\"The LLM autonomously decides to use the following tool(s) to achieve its goal of finding data on X/Twitter\")\nprint(\"Tool selected: \", response.choices[0].message.tool_calls[0].function.name)\n```\n\n----------------------------------------\n\nTITLE: Creating System Prompt for Aggregator Model\nDESCRIPTION: Prepares a system prompt for the aggregator model that will synthesize the multiple intermediate responses into a final, comprehensive answer.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Gather all intermediate responses along with the system prompt\naggregator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models to the latest user query.\nYour task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information\nprovided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the\ngiven answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured,\ncoherent, and adheres to the highest standards of accuracy and reliability.\n\nResponses from models:\"\"\"\n\nprint(aggregator_system_prompt + \"\\n\" + \"\\n\".join(f\"{i+1}. {str(element)}\" for i, element in enumerate(results)))\n```\n\n----------------------------------------\n\nTITLE: Sorting Movies by Semantic Relevance\nDESCRIPTION: Sorts movie indices based on their similarity scores to identify which movies are most semantically similar to the query.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Get the indices of the highest to lowest cosine similarity values\n# This tell use which movies are most similar to the query\n\n# Lookin at the results below we see that the movie at index 172 is the most similar to the query!\nindices = np.argsort(-similarity_scores)\n\nprint(indices)\n```\n\n----------------------------------------\n\nTITLE: Alternative Fine-tuning Without Continual Learning\nDESCRIPTION: Performs fine-tuning on the second dataset without using the checkpoint from the first run, for comparison with the continual fine-tuning approach.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nalt_ft = client.fine_tuning.create(\n    training_file = resp_fc2.id, # function_calling_2.jsonl\n    model = 'meta-llama/Llama-3.2-1B-Instruct',\n    n_epochs = 1,\n    n_checkpoints = 1,\n    batch_size = 16,\n    lora=True,\n    warmup_ratio=0.1,\n    wandb_api_key=WANDB_API_KEY,\n)\n\nprint(alt_ft.id)\n```\n\n----------------------------------------\n\nTITLE: Defining the Contextual RAG Prompt Template\nDESCRIPTION: Creates a prompt template used to generate contextual explanations for each chunk. The template includes placeholders for the entire document and the specific chunk being explained.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# We want to generate a snippet explaining the relevance/importance of the chunk with\n# full document in mind.\n\nCONTEXTUAL_RAG_PROMPT = \"\"\"\nGiven the document below, we want to explain what the chunk captures in the document.\n\n{WHOLE_DOCUMENT}\n\nHere is the chunk we want to explain:\n\n{CHUNK_CONTENT}\n\nAnswer ONLY with a succinct explaination of the meaning of the chunk in the context of the whole document above.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Combining Results Using RRF in Python\nDESCRIPTION: This snippet calls the reciprocal rank fusion function using the top k results retrieved from vector and BM25 queries to produce a combined list of ranked documents.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n# Combine the lists using RRF\nhybrid_top_k = reciprocal_rank_fusion(vector_top_k, bm25_top_k)\nhybrid_top_k[1]\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset Files to Together AI\nDESCRIPTION: Uploads the training and validation dataset files to the Together AI cloud for use in fine-tuning. It sets 'check=True' to trigger a format check ensuring the data is in the correct format for DPO.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndpo_train_file = client.files.upload(dpo_train_file_path, check=True)\ndpo_validation_file = client.files.upload(dpo_validation_file_path, check=True)\n\nprint(f\"Uploaded DPO training files: {dpo_train_file}\")\nprint(f\"Uploaded DPO validation files: {dpo_validation_file}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together API Client for LLM Access\nDESCRIPTION: Imports necessary libraries and initializes the Together API clients (both synchronous and asynchronous) with an API key for accessing language models.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import libraries\nimport json\nimport asyncio\nimport together\nfrom together import AsyncTogether, Together\n\nfrom typing import Any, Optional, Dict, List, Literal\nfrom pydantic import Field, BaseModel, ValidationError\n\nTOGETHER_API_KEY = \"--Your API Key--\"\n\nclient = Together(api_key= TOGETHER_API_KEY)\nasync_client = AsyncTogether(api_key= TOGETHER_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Converting HelpSteer2-DPO Dataset to Preference Format\nDESCRIPTION: Defines a function to convert the HelpSteer2-DPO dataset into a format suitable for Together AI preference fine-tuning, including input messages, preferred output, and non-preferred output.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef convert_to_preference_dataset(dataset):\n    \"\"\"\n    Converts the HelpSteer2-DPO dataset to a format suitable for together.ai preference fine-tuning.\n    \n    Returns a dataset with the preference format.\n    \"\"\"\n    converted_dataset = {\n        \"train\": [],\n        \"validation\": []\n    }\n    \n    for split in [\"train\", \"validation\"]:\n        for example in dataset[split]:\n            # Create the input messages\n            messages = [\n                {\"role\": \"user\", \"content\": example[\"prompt\"]}\n            ]\n            \n            # Create the preferred and non-preferred outputs\n            preferred_output = [\n                {\"role\": \"assistant\", \"content\": example[\"chosen_response\"]}\n            ]\n            \n            non_preferred_output = [\n                {\"role\": \"assistant\", \"content\": example[\"rejected_response\"]}\n            ]\n            \n            # Add the converted example to the dataset\n            converted_dataset[split].append({\n                \"input\": {\n                    \"messages\": messages\n                },\n                \"preferred_output\": preferred_output,\n                \"non_preferred_output\": non_preferred_output\n            })\n    \n    return converted_dataset\n```\n\n----------------------------------------\n\nTITLE: Implementing Embedding Generation Function\nDESCRIPTION: Creates a function to generate embeddings using Together AI's API for text inputs.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport numpy as np\n\ndef generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n    \"\"\"Generate embeddings from Together python library.\n\n    Args:\n        input_texts: a list of string input texts.\n        model_api_string: str. An API string for a specific embedding model of your choice.\n\n    Returns:\n        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n    \"\"\"\n    together_client = together.Together(api_key = TOGETHER_API_KEY)\n    outputs = together_client.embeddings.create(\n        input=input_texts,\n        model=model_api_string,\n    )\n    return np.array([x.embedding for x in outputs.data])\n```\n\n----------------------------------------\n\nTITLE: Testing Vector Retrieval Function\nDESCRIPTION: Tests the vector retrieval function with a specific query about floating point operations in the bill.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nvector_retrieval(query = \"what is the maximum allowable floating point operation per second this bill allows?\", top_k = 5, vector_index = embeddings, chunks = chunks)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Chunks for LLM Processing\nDESCRIPTION: Retrieves relevant chunks based on a specific query for processing by a language model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nretrieved_chunks = vector_retrieval(query = \"what is the maximum allowable floating point operation per second this bill allows?\", top_k = 5, vector_index = embeddings, chunks = chunks)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Knowledge Graph Image in Notebook\nDESCRIPTION: Uses IPython.display to show the generated knowledge graph image directly in the notebook interface.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\n\n# Display the image of the knowledge graph\ndisplay(Image(filename='/content/knowledge_graph.png'))\n```\n\n----------------------------------------\n\nTITLE: Implementing Evaluation Metrics for CoQA Model Performance\nDESCRIPTION: Defines a function to calculate Exact Match (EM) and F1 metrics for evaluating model performance on the CoQA dataset. These metrics compare the model's predicted answers with ground truth answers to measure accuracy.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# This function will be used to evaluate predicted answers uinsg the Exact Match (EM) and F1 metrics\n\ndef get_metrics(pred_answers):\n    \"\"\"\n    Calculate the Exact Match (EM) and F1 metrics for predicted answers.\n    Args:\n        pred_answers (list): A list of predicted answers. Each element in the list is a list of predicted answers for a single question.\n    Returns:\n        tuple: A tuple containing two elements:\n            - em_score (float): The average Exact Match score across all predictions.\n            - f1_score (float): The average F1 score across all predictions.\n    \"\"\"\n\n    em_metrics = []\n    f1_metrics = []\n\n    for pred, data in tqdm(zip(pred_answers, coqa_dataset[\"validation\"]), total=len(pred_answers)):\n        for pred_answer, true_answer in zip(pred, data[\"answers\"][\"input_text\"]):\n            em_metrics.append(squad_metrics.compute_exact(true_answer, pred_answer))\n            f1_metrics.append(squad_metrics.compute_f1(true_answer, pred_answer))\n\n    return sum(em_metrics) / len(em_metrics), sum(f1_metrics) / len(f1_metrics)\n```\n\n----------------------------------------\n\nTITLE: Rendering the Knowledge Graph Visualization\nDESCRIPTION: Calls the visualization function to create and render the knowledge graph as a PNG image.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nvisualize_knowledge_graph(graph)\n```\n\n----------------------------------------\n\nTITLE: Transforming CoQA Dataset to Chat Format for Fine-tuning\nDESCRIPTION: Creates a mapping function that converts CoQA dataset rows into the required chat format for Together AI. The function structures each row as a conversation with a system prompt containing the story context, followed by alternating user questions and assistant answers.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# the system prompt,if present, must always be at the beginning\nsystem_prompt = \"Read the story and extract answers for the questions.\\nStory: {}\"\n\ndef map_fields(row):\n    \"\"\"    \n    Maps the fields from a row of data to a structured format for conversation.\n    Args:\n        row (dict): A dictionary containing the keys \"story\", \"questions\", and \"answers\".\n            - \"story\" (str): The story content to be used in the system prompt.\n            - \"questions\" (list of str): A list of questions from the user.\n            - \"answers\" (dict): A dictionary containing the key \"input_text\" which is a list of answers from the assistant.\n    Returns:\n        dict: A dictionary with a single key \"messages\" which is a list of message dictionaries.\n            Each message dictionary contains:\n            - \"role\" (str): The role of the message sender, either \"system\", \"user\", or \"assistant\".\n            - \"content\" (str): The content of the message.    \n    \"\"\"\n    # create system prompt\n    messages = [{\"role\": \"system\", \"content\": system_prompt.format(row[\"story\"])}]\n    \n    # add user and assistant messages\n    for q, a in zip(row[\"questions\"], row[\"answers\"][\"input_text\"]):\n        messages.append({\"role\": \"user\", \"content\": q})\n        messages.append({\"role\": \"assistant\", \"content\": a})\n    \n    return {\"messages\": messages}\n```\n\n----------------------------------------\n\nTITLE: Initiating DPO Fine-Tuning Job with Higher Beta\nDESCRIPTION: Starts another DPO fine-tuning job with a higher DPO beta value (0.7) to demonstrate the effect of this parameter on preference tuning. This setting results in more conservative updates, staying closer to the reference model behavior.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndpo_training = client.fine_tuning.create(\n    training_file=dpo_train_file.id,\n    validation_file=dpo_validation_file.id,\n    n_evals=5,\n    model=MODEL_NAME,\n    wandb_api_key=WANDB_API_KEY,\n    wandb_project_name=\"helpsteer2\",\n    suffix=\"helpsteer2_dpo_training\",\n    n_epochs=1,\n    n_checkpoints=1,\n    learning_rate=1e-5,\n    lora=True,\n    training_method='dpo',\n    dpo_beta=0.7,  # HIGHER DPO_BETA\n)\nprint(dpo_training.id)\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Retrieval Function in Python\nDESCRIPTION: This function retrieves the top-k most similar items from a vector index based on a query string using cosine similarity. It requires an embedding generation step and numpy for handling arrays. The function outputs a list of indices of the most similar items.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef vector_retreival(query: str, top_k: int = 5, vector_index: np.ndarray = None) -> List[int]:\n    \"\"\"\n    Retrieve the top-k most similar items from an index based on a query.\n    Args:\n        query (str): The query string to search for.\n        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.\n        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.\n    Returns:\n        List[int]: A list of indices corresponding to the top-k most similar items in the index.\n    \"\"\"\n\n    query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]\n    similarity_scores = cosine_similarity([query_embedding], vector_index)\n\n    return list(np.argsort(-similarity_scores)[0][:top_k])\n```\n\n----------------------------------------\n\nTITLE: Fine-Tuning Model - Python\nDESCRIPTION: This snippet initiates the fine-tuning process for a specified model using multiple parameters such as training file id, epochs, batch size, and learning rate. The job id of the fine-tuning process is stored in 'task_fine_tuning_job_id'.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.fine_tuning.create(\n  training_file = task_file_id,\n  model = 'meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference',\n  n_epochs = 1,\n  n_checkpoints = 1,\n  batch_size = \"max\",\n  learning_rate = 7e-5,\n  suffix = 'long-context-finetune',\n  wandb_api_key = WANDB_API_KEY,\n  lora=True,\n)\ntask_fine_tuning_job_id = response.id\n```\n\n----------------------------------------\n\nTITLE: Generator Implementation\nDESCRIPTION: Implementation of the Generator component that creates solutions based on tasks and feedback.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate(task: str, generator_prompt: str, context: str = \"\") -> tuple[str, str]:\n    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n    full_prompt = f\"{generator_prompt}\\n{context}\\nTask: {task}\" if context else f\"{generator_prompt}\\nTask: {task}\"\n\n    response = run_llm(full_prompt, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n    \n    print(\"\\n=== GENERATION START ===\")\n    print(f\"Output:\\n{response}\\n\")\n    print(\"=== GENERATION END ===\\n\")\n    \n    return response\n```\n\n----------------------------------------\n\nTITLE: Concatenating Retrieved Chunks into a Single String in Python\nDESCRIPTION: This snippet creates a single string from the top documents returned by the reranker to be utilized in the final generative model's query for response generation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nretreived_chunks = ''\n\nfor result in response.results:\n    retreived_chunks += hybrid_top_k_docs[result.index] + '\\n\\n'\n\nprint(retreived_chunks)\n```\n\n----------------------------------------\n\nTITLE: Filtering for High Quality Document Samples\nDESCRIPTION: Extracts high-quality samples by filtering for documents with specific length ranges and high Wikipedia reference scores as a quality indicator.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Filter for high quality samples \n\nlong_documents = []\nfor sample in tqdm(ds_iterator['train']):\n    # From 64k tokens to 128k tokens\n    if (len(sample['raw_content']) > 230000 and \n        len(sample['raw_content']) < 430000):\n        \n        signals = json.loads(sample[\"quality_signals\"])\n        try:\n            wiki_score = signals['rps_doc_ml_wikiref_score'][0][-1]\n        except:\n            wiki_score = 0\n            \n        if (wiki_score > 0.5 and\n            len(long_documents) < 2000):\n            \n            document = x['raw_content']\n            long_documents.append(document)\n\n            if len(long_documents) % 10 == 0:\n                print(len(long_documents))\n\n    if len(long_documents) >= 2000:\n        break\n```\n\n----------------------------------------\n\nTITLE: Writing Converted Dataset to JSONL Files\nDESCRIPTION: Converts the dataset to the required format and writes the training and validation data to separate JSONL files. It also displays a sample example from the converted dataset.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\n\n# Convert the dataset to the required format\nconverted_dataset = convert_to_preference_dataset(dataset)\n\n# Create output directory if it doesn't exist\nos.makedirs(\"data\", exist_ok=True)\n\n# Write training data\ndpo_train_file_path = \"data/helpsteer2_preference_train.jsonl\"\nwith open(dpo_train_file_path, \"w\") as f:\n    for example in converted_dataset[\"train\"]:\n        f.write(json.dumps(example) + \"\\n\")\n\n# Write validation data\ndpo_validation_file_path = \"data/helpsteer2_preference_validation.jsonl\"\nwith open(dpo_validation_file_path, \"w\") as f:\n    for example in converted_dataset[\"validation\"]:\n        f.write(json.dumps(example) + \"\\n\")\n\nprint(f\"Saved {len(converted_dataset['train'])} training examples to data/helpsteer_preference.jsonl\")\nprint(f\"Validation set contains {len(converted_dataset['validation'])} examples\")\n\n# Display a sample example\nprint(\"\\nSample example:\")\nprint(json.dumps(converted_dataset[\"train\"][0], indent=2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Chunked BERTScore Evaluation\nDESCRIPTION: Defines a function that calculates BERTScore between chunks of the original document and the generated summary to evaluate how well each section is represented in the summary.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom bert_score import score\n\ndef chunked_BERTscores(reference: str, candidate: str):\n    \"\"\"\n    This function calculates the BERTScore for a candidate summary against chunks of a reference text.\n     reference: text of the original document\n     candidate: LLM generated summary of the original document\n    \"\"\"\n    ref_chunks = [reference[i - 400 : i] for i in range(400,len(reference),400)]\n    cand =  [candidate]*len(ref_chunks)\n\n    P, R, F1 = score(ref_chunks, cand, lang=\"en\", model_type=\"facebook/bart-large-mnli\") #instead of using the original BERT model we use the improved BART model\n\n    return (P, R, F1)\n```\n\n----------------------------------------\n\nTITLE: Continual Fine-tuning on Additional Function Calling Categories\nDESCRIPTION: Continues fine-tuning from the previous checkpoint using the second dataset, demonstrating the core concept of Continual Fine-tuning.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nft_fc2 = client.fine_tuning.create(\n    training_file = resp_fc2.id, # function_calling_2.jsonl\n    from_checkpoint = ft_fc1.id, # \"id\" from the first run above \n    n_epochs = 1,\n    n_checkpoints = 1,\n    batch_size = 16,\n    lora=True,\n    warmup_ratio=0.1,\n    wandb_api_key=WANDB_API_KEY,\n)\nprint(ft_fc2.id)\n```\n\n----------------------------------------\n\nTITLE: Creating an SFT Fine-tuning Job on Together AI\nDESCRIPTION: Creates a Supervised Fine-Tuning (SFT) job on the Together AI platform. This is the first stage of the optional two-stage approach, configuring parameters like learning rate, epochs, and enabling LoRA for efficient training.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# If you want to combine the SFT + DPO training, you can do so by creating a SFT job first\n# and then using the DPO training to continue the training of the resulting SFT checkpoint.\n\nsft_training = client.fine_tuning.create(\n    training_file=sft_train_file.id,\n    validation_file=sft_validation_file.id,\n    n_evals=3,\n    model=MODEL_NAME,\n    wandb_api_key=WANDB_API_KEY,\n    wandb_project_name=\"helpsteer2\",\n    suffix=\"helpsteer2_sft_training\",\n    n_epochs=1,\n    n_checkpoints=1,\n    learning_rate=1e-5,\n    lora=True,\n)\nprint(sft_training.id)\n```\n\n----------------------------------------\n\nTITLE: Creating LoRA Finetuning Job\nDESCRIPTION: Configures and initiates a LoRA finetuning job on the Llama-3.2-1B-Instruct model. The parameters specify training settings like epochs, learning rate, and warmup ratio, with LoRA enabled for efficient training.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nft_resp = client.fine_tuning.create(\n    training_file = train_file_resp.id,\n    model = 'meta-llama/Llama-3.2-1B-Instruct', # changed to 1B model\n    train_on_inputs= \"auto\",\n    n_epochs = 3,\n    n_checkpoints = 1,\n    wandb_api_key = WANDB_API_KEY,\n    lora = True,\n    warmup_ratio=0,\n    learning_rate = 1e-5,\n    suffix = 'FT-webinar-demo-1b',\n)\n\nprint(ft_resp.id)\n```\n\n----------------------------------------\n\nTITLE: Executing Email Send with Processed Tool\nDESCRIPTION: This snippet demonstrates how to execute the email send with the processed tool and handle the response. It creates a chat completion using the Together AI client with the processed email tool and then handles the tool calls to send the email.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Execute chat completion with processed tool\nresponse = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Alex, a product manager at an AI company.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Send an email to John, who is an ML engineer on the team, to inquire about a good time to meet next week to review the upcoming launch.\",\n        },\n    ],\n    tools=processed_send_email_tool,\n)\n\n# Handle the tool calls and send the email\nexec_response = toolset.handle_tool_calls(response)\n```\n\n----------------------------------------\n\nTITLE: Preparing Movie Data for Embedding\nDESCRIPTION: Concatenates the title, overview, and tagline of each movie to create a rich text representation for embedding, processing the first 1000 movies from the dataset.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Concatenate the title, overview, and tagline of each movie\n# this makes the text that will be embedded for each movie more informative\n# as a result the embeddings will be richer and capture this information. \nto_embed = []\nfor movie in movies_data[:1000]:\n    text = ''\n    for field in ['title', 'overview', 'tagline']:\n        value = movie.get(field, '')\n        text += str(value) + ' '\n    to_embed.append(text.strip())\n\nto_embed[:10]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Top 5 Indices by Similarity\nDESCRIPTION: Gets the indices of the top 5 most similar chunks based on similarity scores.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntop_5_indices = indices[:5]\ntop_5_indices\n```\n\n----------------------------------------\n\nTITLE: Testing Vector Database Search Functionality\nDESCRIPTION: This code demonstrates how to perform a similarity search using the vector database. It searches for documents related to a given query and prints the results.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# example search from vectorDB\nquery = \"How do I jailbreak a LLM?\"\ndocs = vectorstore.similarity_search(query, k=3)\nfor i in [docs[j].page_content+'\\n\\n' for j in range(len(docs))]:\n    print(i)\n    print('-'*100)\n```\n\n----------------------------------------\n\nTITLE: Example JSON Format for Conversational Fine-tuning Data\nDESCRIPTION: Shows the required JSON format for conversational data used in Together AI fine-tuning. Each example contains a list of messages with roles (system, user, assistant) and content.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n              {\"role\": \"user\", \"content\": \"Hello!\"}, \n              {\"role\": \"assistant\", \"content\": \"Hi! How can I help you?\"}]}\n```\n\n----------------------------------------\n\nTITLE: Transforming CoQA Dataset to Chat Format\nDESCRIPTION: Applies the mapping function to transform the CoQA dataset into a structured conversational format suitable for finetuning LLMs. The transformation maintains the question-answer pairs in a chat-like structure.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# transform the data using the mapping function\ntrain_messages = coqa_dataset[\"train\"].map(map_fields, remove_columns=coqa_dataset[\"train\"].column_names)\n```\n\n----------------------------------------\n\nTITLE: Scraping Essay Content in Python\nDESCRIPTION: Actor task that scrapes the content of each essay URL with retry functionality for error handling.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@actor.task(retries=3)\ndef scrape_pg_essays(document: Document) -> Document:\n    from bs4 import BeautifulSoup\n\n    try:\n        response = requests.get(document.url)\n    except Exception as e:\n        raise FlyteRecoverableException(f\"Failed to scrape {document.url}: {str(e)}\")\n    \n    response.raise_for_status()\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    content = soup.find(\"font\")\n\n    text = None\n    if content:\n        text = \" \".join(content.get_text().split())\n    document.content = text\n    return document\n```\n\n----------------------------------------\n\nTITLE: Workflow Example Usage\nDESCRIPTION: Example implementation of using the looping workflow to solve a stack implementation problem.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntask = \"\"\"\nImplement a Stack with:\n1. push(x)\n2. pop()\n3. getMin()\nAll operations should be O(1).\n\"\"\"\n\nloop_workflow(task, EVALUATOR_PROMPT, GENERATOR_PROMPT)\n```\n\n----------------------------------------\n\nTITLE: Computing Semantic Similarity Between Query and Movies\nDESCRIPTION: Calculates cosine similarity scores between the query embedding and all movie embeddings to determine semantic relevance.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# This function will produce a list of cosine similarity scores between the query embedding and the movie embeddings\nsimilarity_scores = cosine_similarity([query_embedding], embeddings)\n```\n\n----------------------------------------\n\nTITLE: Calling Iterative Research Function in Python\nDESCRIPTION: This code snippet demonstrates how to call the `conduct_iterative_research` function. It passes the research topic, initial results, initial queries, budget, maximum queries, Tavily client, Together AI client, planning model, JSON model, summary model, and prompts to the function. The function returns the final results and all queries used.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresults, all_queries=await conduct_iterative_research(topic=research_topic, initial_results=initial_results, all_queries=initial_queries, budget=budget, max_queries=max_queries, tavily_client=tavily_client, together_client=together_client, planning_model=planning_model, json_model=json_model, summary_model=summary_model, prompts=prompts)\n```\n\n----------------------------------------\n\nTITLE: Testing Agent with Stream and pprint\nDESCRIPTION: This code snippet tests the agent by providing a user query as input. It then streams the output from the graph and iterates through each node's output. The `pprint` module is used to display the output from each node in a structured and readable format.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pprint\n\ninputs = {\n    \"messages\": [\n        (\"user\", \"What are the different types of agent memory?\"),\n    ]\n}\nfor output in graph.stream(inputs):\n    for key, value in output.items():\n        pprint.pprint(f\"Output from node '{key}':\")\n        pprint.pprint(\"---\")\n        pprint.pprint(value, indent=2, width=80, depth=None)\n    pprint.pprint(\"\\n---\\n\")\n```\n\n----------------------------------------\n\nTITLE: Initializing ColPali Model for MultiModal RAG\nDESCRIPTION: Imports necessary modules and initializes the RAGMultiModalModel using the ColQwen2 pretrained model for document indexing and retrieval.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pathlib import Path\nfrom byaldi import RAGMultiModalModel\n\n# Initialize RAGMultiModalModel\nmodel = RAGMultiModalModel.from_pretrained(\"vidore/colqwen2-v0.1\")\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Contextual Chunks\nDESCRIPTION: Applies the embedding generation function to create vector embeddings for all contextual chunks using the BGE-Large model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ncontextual_embeddings = generate_embeddings(contextual_chunks, \"BAAI/bge-large-en-v1.5\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Color-Coded Text Based on BERTScores\nDESCRIPTION: Scales the F1 BERTScores and applies the color-coding visualization to show which parts of the original document are well-represented (green) or poorly represented (red) in the summary.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndata_array = [[x] for x in scores[2].tolist()]\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler() # we just use this to make the colours more identifiable\n\n# Fit and transform the data\nscaled_data = scaler.fit_transform(data_array)\n\n# Convert the scaled data back to a list\nscaled_F1 = [x[0] for x in scaled_data]\n\nprint_text_in_colors(text,scaled_F1)\n```\n\n----------------------------------------\n\nTITLE: Creating Prompts for All Chunks\nDESCRIPTION: Applies the generate_prompts function to create a prompt for each chunk in the document.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompts = generate_prompts(pg_essay, chunks)\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Document Chunks\nDESCRIPTION: Applies the embedding generation function to all document chunks using the BGE large English model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nembeddings = generate_embeddings(list(chunks), \"BAAI/bge-large-en-v1.5\")\n```\n\n----------------------------------------\n\nTITLE: Defining Planning Structure Types\nDESCRIPTION: Creates type definitions to manage the planning state, including PlanExecute and Plan classes.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport operator\nfrom typing import Annotated, List, Tuple\nfrom typing_extensions import TypedDict\n\n# Define types for the planning system\nclass PlanExecute(TypedDict):\n    input: str\n    plan: List[str]\n    past_steps: Annotated[List[Tuple], operator.add]\n    response: str\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass Plan(BaseModel):\n    \"\"\"Plan to follow in future\"\"\"\n\n    steps: List[str] = Field(\n        description=\"different steps to follow, should be in sorted order\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset for LoRA Finetuning\nDESCRIPTION: Uploads a JSONL dataset file to Together AI's platform. This dataset will be used for finetuning the model with the LoRA technique.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Upload dataset to Together AI\n\ntrain_file_resp = client.files.upload(\"datasets/small_coqa_10.jsonl\", check=True)\nprint(train_file_resp.id)\n```\n\n----------------------------------------\n\nTITLE: Implementing Embedding Generation Function\nDESCRIPTION: Defines a function to generate embeddings from input texts using the Together AI API. Takes a list of texts and model identifier and returns the vector embeddings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\ndef generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n    \"\"\"Generate embeddings from Together python library.\n\n    Args:\n        input_texts: a list of string input texts.\n        model_api_string: str. An API string for a specific embedding model of your choice.\n\n    Returns:\n        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n    \"\"\"\n    together_client = together.Together(api_key = TOGETHER_API_KEY)\n    outputs = together_client.embeddings.create(\n        input=input_texts,\n        model=model_api_string,\n    )\n    return [x.embedding for x in outputs.data]\n```\n\n----------------------------------------\n\nTITLE: Displaying Comprehensive Search Results in Python\nDESCRIPTION: This code displays an overview of the combined search results. It shows the first and last portions of the result set to provide a glimpse of the information gathered, demonstrating how multiple sources are consolidated into a single research corpus.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"First 10000 characters of {len(initial_results.results)} results:\\n\\n {str(initial_results)[:10000]}\\n\\n...\\n\\n Last 10000 characters of {len(initial_results.results)} results:\\n\\n {str(initial_results)[-10000:]}\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Multiple LoRAs\nDESCRIPTION: Demonstrates using multiple LoRA fine-tunes together, combining a tarot card style with a detail-enhancing LoRA. It shows how to adjust the scale of each LoRA for desired effects.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a baby panda eating bamboo in the style of TOK a trtcrd tarot style\"\n\ngenerated_image = generate_image(prompt, \n                                 lora1=\"https://huggingface.co/multimodalart/flux-tarot-v1\",\n                                 scale1=1.0,\n                                 lora2=\"https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-add-details\",\n                                 scale2=0.8\n                                 )\n\nImage(url=generated_image, width=512, height=384)\n```\n\n----------------------------------------\n\nTITLE: Uploading Training Files to Together AI Platform\nDESCRIPTION: Uploads the prepared JSONL files to the Together AI platform for processing and use in fine-tuning jobs.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresp_fc1 = client.files.upload(file=\"function_calling_1.jsonl\", check=True)\nprint(resp_fc1.id)\n```\n\nLANGUAGE: python\nCODE:\n```\nresp_fc2 = client.files.upload(file=\"function_calling_2.jsonl\", check=True) # uploads a file\nprint(resp_fc2.id)\n```\n\n----------------------------------------\n\nTITLE: Defining Prediction Generation Function for Summarization in Python\nDESCRIPTION: This function generates predictions for a list of test items using a specified model. It utilizes the llm_call function to interact with the model and returns a list of predictions.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_predictions(test_items, model):\n    predictions = []\n    for it in tqdm(test_items):\n        predictions.append(\n            llm_call(it['prompt'],\n                     model=model\n                    )\n        )\n    return predictions\n```\n\n----------------------------------------\n\nTITLE: Uploading SFT Datasets to Together AI for Fine-tuning\nDESCRIPTION: Uploads the prepared SFT datasets to Together AI cloud storage for use in fine-tuning. The code sets check=True to trigger format validation, ensuring the data is properly formatted for SFT.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsft_train_file = client.files.upload(sft_train_output_path, check=True)\nsft_validation_file = client.files.upload(sft_validation_output_path, check=True)\n\nprint(f\"Uploaded SFT training files: {sft_train_file}\")\nprint(f\"Uploaded SFT validation files: {sft_validation_file}\")\n\n\n```\n\n----------------------------------------\n\nTITLE: Model Output Schema Definition - Pydantic Model\nDESCRIPTION: Defines the schema for model selection output using Pydantic, including available models and reasoning field\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\nclass ModelOutput(BaseModel):\n    model: Literal[\"deepseek-ai/DeepSeek-V3\",\n                   \"Qwen/Qwen2.5-Coder-32B-Instruct\", \n                   \"Gryphe/MythoMax-L2-13b\", \n                   \"Qwen/QwQ-32B-Preview\",\n                   \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"]\n    \n    reason: str = Field(\n        description=\"Reason why this model was selected for the task specified in the prompt/query.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Image with LLM\nDESCRIPTION: This code snippet demonstrates how to use an LLM to generate an image based on a provided request. It uses the `client.chat.completions.create` method to interact with the LLM, providing a model, messages, and tools. The generated image is then processed using the `th.run_tools` function.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimage_response = client.chat.completions.create(\n  model=MODEL,\n  messages=generate_image_request,\n  tools=tools,\n)\ntool_run = th.run_tools(image_response)\n```\n\n----------------------------------------\n\nTITLE: Embedding User Query\nDESCRIPTION: Generates and visualizes the embedding vector for a user query about \"super hero action movie with a timeline twist\" using the same embedding model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquery = \"super hero action movie with a timeline twist\"\n\nquery_embedding = generate_embeddings([query], 'BAAI/bge-base-en-v1.5')[0]\n\ndata_2d = np.reshape(query_embedding, (1, 768))\n\nplt.figure(figsize=(15, 1))\n# Create a heatmap with a binary colormap (black and white)\nplt.imshow(data_2d, cmap='binary', interpolation='nearest', aspect='auto')\n\n# Remove axes and ticks\nplt.xlabel('Vector Dimension')\nplt.ylabel(\"query\")\n# Show the plot\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Flight Data and Usage Limits\nDESCRIPTION: This code block provides sample flight data as a string and sets usage limits to prevent excessive API calls during development and testing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Sample flight data with various origin/destination/date combinations\nflights_web_page = \"\"\"\n1. Flight SFO-AK123\n- Price: $350\n- Origin: San Francisco International Airport (SFO)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 10, 2025\n\n2. Flight SFO-AK456\n- Price: $370\n- Origin: San Francisco International Airport (SFO)\n- Destination: Fairbanks International Airport (FAI)\n- Date: January 10, 2025\n\n3. Flight SFO-AK789\n- Price: $400\n- Origin: San Francisco International Airport (SFO)\n- Destination: Juneau International Airport (JNU)\n- Date: January 20, 2025\n\n4. Flight NYC-LA101\n- Price: $250\n- Origin: San Francisco International Airport (SFO)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 10, 2025\n\n5. Flight CHI-MIA202\n- Price: $200\n- Origin: Chicago O'Hare International Airport (ORD)\n- Destination: Miami International Airport (MIA)\n- Date: January 12, 2025\n\n6. Flight BOS-SEA303\n- Price: $120\n- Origin: Boston Logan International Airport (BOS)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 12, 2025\n\n7. Flight DFW-DEN404\n- Price: $150\n- Origin: Dallas/Fort Worth International Airport (DFW)\n- Destination: Denver International Airport (DEN)\n- Date: January 10, 2025\n\n8. Flight ATL-HOU505\n- Price: $180\n- Origin: Hartsfield-Jackson Atlanta International Airport (ATL)\n- Destination: George Bush Intercontinental Airport (IAH)\n- Date: January 10, 2025\n\"\"\"\n\n# We set usage limits to prevent excessive API calls during development/testing\nusage_limits = UsageLimits(request_limit=15)\n```\n\n----------------------------------------\n\nTITLE: Performing Text-to-Image Retrieval Using JinaCLIP in Python\nDESCRIPTION: Demonstrates text-to-image retrieval by searching for the most semantically similar image to a given text query using the retrieve_image function.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nretrieved_image = links[retrieve_image(query = 'family pics', query_type = 'text', index = image_embeddings)]\n```\n\n----------------------------------------\n\nTITLE: Testing Model with Irrelevant Query\nDESCRIPTION: Tests how the DeepSeek R1 model handles a query that's irrelevant to the provided context, demonstrating its reasoning capabilities when information is missing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What is the circumference of the moon?\"\n\nstream = client.chat.completions.create(\n    model=\"deepseek-ai/DeepSeek-R1\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n      {\"role\": \"user\", \"content\": PROMPT.format(query=query, formatted_chunks=formatted_chunks)},\n    ],\n      stream=True,\n)\n\nresponse = ''\n\nfor chunk in stream:\n  response += chunk.choices[0].delta.content or \"\"\n  print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Context for All Chunks\nDESCRIPTION: Creates a list of contextual chunks by generating context for each chunk and concatenating it with the original chunk content.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Let's generate the entire list of contextual chunks\n\ncontextual_chunks = [generate_context(prompts[i])+' '+chunks[i] for i in range(len(chunks))]\n```\n\n----------------------------------------\n\nTITLE: Initiating LLM Finetuning on CoQA Dataset\nDESCRIPTION: Creates a finetuning job for Llama 3.1 8B using the uploaded CoQA dataset. This configures the finetuning parameters including learning rate, epochs, and LoRA adaptation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nft_resp = client.fine_tuning.create(\n    training_file = train_file_resp.id,\n    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Reference',\n    train_on_inputs= \"auto\",\n    n_epochs = 3,\n    n_checkpoints = 1,\n    wandb_api_key = WANDB_API_KEY,\n    lora = True,\n    warmup_ratio=0,\n    learning_rate = 1e-5,\n    suffix = 'my-demo-finetune',\n)\n\nprint(ft_resp.id)\n```\n\n----------------------------------------\n\nTITLE: Loading and Visualizing Function Calling Dataset Categories\nDESCRIPTION: Loads the NousResearch's Hermes Function Calling dataset, counts categories, and creates a bar chart of the top 10 categories.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset_name = \"NousResearch/hermes-function-calling-v1\"\nfunction_calling = load_dataset(dataset_name)\n\n# Count categories\ncategory_counts = Counter(function_calling[\"train\"][\"category\"])\n\n# Get top 10 categories\ntop_categories = category_counts.most_common(10)\ncategories, counts = zip(*top_categories)\n\n# Create bar chart\nplt.figure(figsize=(12, 6))\nplt.bar(categories, counts)\nplt.xticks(rotation=45, ha='right')\nplt.title('Top 10 Categories in Function Calling Dataset')\nplt.xlabel('Category')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Fine-tuned Model Prediction Example in Python\nDESCRIPTION: This code extracts a specific prediction (index 20) from the fine-tuned model's results on the GovReport dataset for comparative analysis with the baseline model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npredictions_ft_govreport[20]\n```\n\n----------------------------------------\n\nTITLE: Fetching Legislative Text from LegiScan\nDESCRIPTION: Downloads the text content of California Senate Bill SB1047 from LegiScan using requests and BeautifulSoup to parse the HTML.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_legiscan_text(url):\n    \"\"\"\n    Fetches and returns the text content from a given LegiScan URL.\n    Args:\n        url (str): The URL of the LegiScan page to fetch.\n    Returns:\n        str: The text content of the page.\n    Raises:\n        requests.exceptions.RequestException: If there is an issue with the HTTP request.\n    \"\"\"\n    # Basic headers to mimic a browser\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n\n    # Make the request\n    response = requests.get(url, headers=headers)\n\n    # Parse HTML\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Get text content\n    content = soup.get_text()\n\n    return content\n\nurl = \"https://legiscan.com/CA/text/SB1047/id/2999979/California-2023-SB1047-Amended.html\"\ntext = get_legiscan_text(url)\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Printing Retrieved Documents from BM25 Results in Python\nDESCRIPTION: This snippet iterates over the retrieved results and prints the index and document for each chunk, allowing the user to see which documents were deemed most relevant according to the BM25 search.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfor doc in results[0]:\n  print(f\"Chunk Index {contextual_chunks.index(doc)} : {doc}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Synchronous LLM Helper Function\nDESCRIPTION: Creates a helper function that handles the basic interaction with language models through the Together API, supporting both user and system prompts.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Simple LLM call helper function\ndef run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):\n    \"\"\" Run the language model with the given user prompt and system prompt. \"\"\"\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n    messages.append({\"role\": \"user\", \"content\": user_prompt})\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0.7,\n        max_tokens=4000,        \n    )\n\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Initializing Together AI and Composio for Email Sending\nDESCRIPTION: This snippet sets up the integration between Together AI's LLM and Composio's Gmail tool. It initializes the Together AI client, creates a Composio toolset, configures email sending capability, and makes an LLM call with the email tool to test the setup.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom composio_togetherai import ComposioToolSet, Action\nfrom together import Together\n\n# Initialize Together AI and Composio clients with API keys\nclient = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\ntoolset = ComposioToolSet(api_key=os.getenv(\"COMPOSIO_API_KEY\"))\n\n# Get the Gmail send email tool\nsend_email_tool = toolset.get_tools(\n    [Action.GMAIL_SEND_EMAIL], check_connected_accounts=False\n)\n\n# Create a chat completion with email capability\nresponse = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Alex, a product manager at an AI company.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Send an email to John, who is an ML engineer on the team, to inquire about a good time to meet next week to review the upcoming launch.\",\n        },\n    ],\n    tools=send_email_tool,\n)\n\nresponse.model_dump()\n```\n\n----------------------------------------\n\nTITLE: Registering a Scheduled Launch Plan\nDESCRIPTION: This code defines and registers a launch plan that schedules the `build_indices_wf` workflow to run daily at 1:00 AM. It uses `fl.LaunchPlan.get_or_create` to either retrieve an existing launch plan or create a new one if it doesn't exist. The `auto_activate=True` argument ensures the launch plan is automatically activated upon registration.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlp = fl.LaunchPlan.get_or_create(\n    workflow=build_indices_wf,\n    name=\"vector_db_ingestion_activate\",\n    schedule=fl.CronSchedule(\n        schedule=\"0 1 * * *\"\n    ),  # Run every day to update the databases\n    auto_activate=True,\n)\n\nregistered_lp = remote.register_launch_plan(entity=lp)\n```\n\n----------------------------------------\n\nTITLE: Launching Fine-tuning Job with Together AI\nDESCRIPTION: Creates and initiates a fine-tuning job using the Together AI API. Configures important parameters like model selection, training file, number of epochs, learning rate, and LoRA adaptation settings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n## This fine-tuning job should take ~10-15 minutes to complete\n\nft_resp = client.fine_tuning.create(\n    training_file = \"file-19c6ef51-b734-4f3c-bc17-62fbad2bd0d0\",\n    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Reference',\n    train_on_inputs= \"auto\",\n    n_epochs = 3,\n    n_checkpoints = 1,\n    wandb_api_key = WANDB_API_KEY,\n    lora = True,\n    warmup_ratio=0,\n    learning_rate = 1e-5,\n    suffix = 'test1_8b',\n)\n\nprint(ft_resp.id)\n```\n\n----------------------------------------\n\nTITLE: Defining Conversation Format Mapping Function for CoQA\nDESCRIPTION: Creates a function to transform CoQA dataset rows into the conversational format required for finetuning. The function structures data into system, user, and assistant messages with appropriate content.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# the system prompt,if present, must always be at the beginning\nsystem_prompt = \"Read the story and extract answers for the questions.\\nStory: {}\"\n\ndef map_fields(row):\n    \"\"\"    \n    Maps the fields from a row of data to a structured format for conversation.\n    Args:\n        row (dict): A dictionary containing the keys \"story\", \"questions\", and \"answers\".\n            - \"story\" (str): The story content to be used in the system prompt.\n            - \"questions\" (list of str): A list of questions from the user.\n            - \"answers\" (dict): A dictionary containing the key \"input_text\" which is a list of answers from the assistant.\n    Returns:\n        dict: A dictionary with a single key \"messages\" which is a list of message dictionaries.\n            Each message dictionary contains:\n            - \"role\" (str): The role of the message sender, either \"system\", \"user\", or \"assistant\".\n            - \"content\" (str): The content of the message.    \n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt.format(row[\"story\"]),\n        }\n    ]\n    for q, a in zip(row[\"questions\"], row[\"answers\"][\"input_text\"]):\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": q,\n            }\n        )\n        messages.append(\n            {\n                \"role\": \"assistant\",\n                \"content\": a,\n            }\n        )\n\n    return {\n        \"messages\": messages\n    }\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity and Sorting Indices in Python\nDESCRIPTION: This snippet calculates the cosine similarity between a query embedding and contextual embeddings, then retrieves the top indices based on similarity scores. It depends on the numpy library for operations on arrays.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nsimilarity_scores = cosine_similarity([query_embedding], contextual_embeddings)\nindices = np.argsort(-similarity_scores)\n```\n\n----------------------------------------\n\nTITLE: Loading the FineWeb Educational Dataset\nDESCRIPTION: Loads a sample dataset from the FineWeb educational corpus to create long context examples for testing and fine-tuning.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nds_iterator = load_dataset(\n    \"HuggingFaceFW/fineweb-edu\",\n    \"sample-10BT\",\n)['train']\n```\n\n----------------------------------------\n\nTITLE: Extracting F1 Scores from BERTScore Results\nDESCRIPTION: Accesses the F1 component of the BERTScore results, which is a combined measure of precision and recall, to use as the main evaluation metric.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Each BERTScore below ranges from 0 to 1, 1 being the best score\n# BERTScore returns Precision, Recall, and F1 score - we will simply use the F1 score as our evaluation metric\nscores[2]\n```\n\n----------------------------------------\n\nTITLE: Creating Repetition Task Dataset\nDESCRIPTION: Creates a task dataset where each example asks the model to return the last N words from a document, where N is randomly selected between 1 and 100.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlong_documents = orjson.loads(Path(\"long_documents.json\").read_bytes())\nlong_documents_32k = long_documents[\"32k\"]\n\ntask_items = []\n\nfor document in long_documents_32k:\n    n = np.random.randint(1, 100)\n    prompt = f\"Return last {n} words from this text: \\n\\n\"\n    target = \" \".join(document.split()[-n:])\n\n    task_items.append({\n        \"prompt\": prompt + document,\n        \"completion\": target\n    })\n```\n\n----------------------------------------\n\nTITLE: Visualizing LangGraph Workflow with Mermaid\nDESCRIPTION: This code snippet uses the IPython.display module to display the LangGraph workflow as a Mermaid diagram. It calls the `get_graph()` method of the compiled graph object, specifies that X-ray mode is enabled, and then renders the Mermaid diagram as a PNG image.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\n\n\ndisplay(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Evaluation Metrics for the Agent\nDESCRIPTION: Creates an evaluation metric (top5_recall) to measure the agent's performance by comparing its output against gold standard Wikipedia titles needed for fact verification.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef top5_recall(example, pred, trace=None):\n    gold_titles = example.titles\n    recall = sum(x in pred.titles[:5] for x in gold_titles) / len(gold_titles)\n\n    # If we're \"bootstrapping\" for optimization, return True if and only if the recall is perfect.\n    if trace is not None:\n        return recall >= 1.0\n    \n    # If we're just doing inference, just measure the recall.\n    return recall\n\nevaluate = dspy.Evaluate(devset=devset, metric=top5_recall, num_threads=16, display_progress=True, display_table=5)\n```\n\n----------------------------------------\n\nTITLE: Launching Fine-tuning Jobs for Summarization in Python\nDESCRIPTION: This code demonstrates how to launch fine-tuning jobs for both synthetic and GovReport datasets using the previously defined send_ft_job function. It sets specific parameters for each dataset, such as learning rate and number of epochs.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Example fine-tuning job on the synthetic summarization dataset\n\nsummarization_fine_tuning_job_id = send_ft_job(\n    client,\n    model=\"meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference\",\n    learning_rate=7e-5,\n    n_epochs=4,\n    filename=\"1112_summarization_train.jsonl\",\n    run_name=\"1112-summarization-synthetic-no-loss-masking\",\n    summarization_file_id=None,\n    train_on_inputs=True,\n)\n\n# Example fine-tuning job on GovReport dataset\n\nsummarization_fine_tuning_job_id = send_ft_job(\n    client,\n    model=\"meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference\",\n    n_epochs=2,\n    filename=\"1114_govreport.jsonl\",\n    run_name=\"1114-govreport-2ep\",\n    summarization_file_id=None,\n    train_on_inputs=False,\n    learning_rate=4e-5,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Model Answer Generation Function for CoQA Evaluation\nDESCRIPTION: Creates a function to generate model answers for the CoQA validation set. This function parallels the answer generation process using a thread pool for efficiency when evaluating model performance.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\nfrom multiprocessing.pool import ThreadPool\nimport transformers.data.metrics.squad_metrics as squad_metrics\n```\n\n----------------------------------------\n\nTITLE: Extracting Top 5 Chunks Based on Indices in Python\nDESCRIPTION: This snippet extracts the top 5 chunks from a list of contextual chunks using the top indices calculated from the cosine similarity scores.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ntop_5_chunks = [contextual_chunks[index] for index in indices[0]][:5]\ntop_5_chunks\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM with Together AI's Llama Model\nDESCRIPTION: Sets up the Language Model using Together AI's Llama model as the core reasoning engine. It requires an API key from the environment variables.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom langchain_together import ChatTogether\n\nllm = ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Performance on Sample Task\nDESCRIPTION: Tests how a large language model performs on a single example of the repetition task by sending the prompt to the API.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# How does a LLM model perform on this task item?\n\nquery = item['prompt']\n\nresult = llm_call(query)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Synthesized Final Answer\nDESCRIPTION: Prints the final synthesized answer that combines the insights from all worker LLMs into a comprehensive solution to the original task.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(final_answer)\n```\n\n----------------------------------------\n\nTITLE: Running Workflow and Retrieval Task Locally\nDESCRIPTION: This code snippet demonstrates how to execute the `build_indices_wf` workflow and the `retrieve` task locally. It first calls the workflow to build the indices and then calls the retrieval task with the generated indices. Finally, it prints the results of the retrieval.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    bm25s_index, contextual_chunks_data = build_indices_wf()\n    results = retrieve(\n        bm25s_index=bm25s_index, contextual_chunks_data=contextual_chunks_data\n    )\n    print(results)\n```\n\n----------------------------------------\n\nTITLE: Formatting Dataset for Together AI Fine-tuning\nDESCRIPTION: Converts the dataset into Together AI's required JSONL format with proper message roles (system, user, assistant) for both training steps.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmap_from = {\"system\": \"system\", \"human\": \"user\", \"gpt\": \"assistant\"}\n\nwith open(\"function_calling_1.jsonl\", \"w\") as pfile:\n    for line in first_step_train:\n        conversations = line[\"conversations\"]\n        pfile.write(json.dumps(\n            {\"messages\": [{\"role\": map_from[m[\"from\"]], \"content\": m[\"value\"] } for m in conversations]}\n        ) + \"\\n\")\n\n\nwith open(\"function_calling_2.jsonl\", \"w\") as pfile:\n    for line in second_step_train:\n        conversations = line[\"conversations\"]\n        pfile.write(json.dumps(\n            {\"messages\": [{\"role\": map_from[m[\"from\"]], \"content\": m[\"value\"] } for m in conversations]}\n        ) + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Creating or Updating Applications Remotely with Union in Python\nDESCRIPTION: This snippet demonstrates how to create or update FastAPI and Gradio applications remotely using the Union framework's AppRemote. The 'AppRemote' object connects to the Union service and performs the operations on default development domain. It ensures the applications are up-to-date, simplifying application lifecycle management. This code relies on the prior setup of application objects and their specifications.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom union.remote._app_remote import AppRemote\n\napp_remote = AppRemote(project=\"default\", domain=\"development\")\n\napp_remote.create_or_update(fastapi_app)\napp_remote.create_or_update(gradio_app)\n```\n\n----------------------------------------\n\nTITLE: Plotting BERTScores by Document Position\nDESCRIPTION: Creates a line plot showing how BERTScores vary across different positions in the document, using a smoothed version of F1 scores to improve readability.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# We smoothen the F1 scores to make the plot more readable\ndef rolling_window(data, window_size):\n    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n\nsmoothed_F1 = rolling_window(scores[2], window_size=3)\n\nplt.plot(np.linspace(0, 1, len(smoothed_F1)), smoothed_F1)\nplt.xlabel('Position in the Document')\nplt.ylabel('Score - higher is better')\nplt.title('Summary Score')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Testing the Optimized Agent on a Complex Claim\nDESCRIPTION: Tests the optimized agent on a complex claim about playwrights to see the improved results after MIPRO optimization.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\noptimized_react(claim=\"The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Kolts are both playwrights.\").titles\n```\n\n----------------------------------------\n\nTITLE: Example Chat Message Format for Conversational Finetuning\nDESCRIPTION: Provides an example of the JSON format used for conversational finetuning. This format structures the conversation with system, user, and assistant messages in a sequential flow.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful AI chatbot.\"},\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you! How can I help you?\"},\n    {\"role\": \"user\", \"content\": \"Can you explain machine learning?\"},\n    {\"role\": \"assistant\", \"content\": \"Machine learning is...\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Context Generation Costs\nDESCRIPTION: Estimates the cost of generating contextual chunks based on token count and model pricing, and calculates how many chunks can be processed with a $1.00 budget.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Cost to generate contextual chunks = ${32*0.06*(1660/1000000)}\")\nprint(f\"Number of Chunks Processed with $1.00 = {1000000/(0.06*1660)} chunks\")\n```\n\n----------------------------------------\n\nTITLE: Defining Knowledge Graph Schema with Pydantic\nDESCRIPTION: Creates Pydantic classes to define the structure of a knowledge graph, including Node and Edge classes, which will be used to guide the LLM in generating properly structured JSON output.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass Node(BaseModel, frozen=True):\n    id: int\n    label: str\n\nclass Edge(BaseModel, frozen=True):\n    source: int\n    target: int\n    label: str\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n```\n\n----------------------------------------\n\nTITLE: Executing Workflow Remotely with UnionRemote\nDESCRIPTION: This code snippet initializes a `UnionRemote` object to execute a workflow on a remote Union cluster. It then executes the `build_indices_wf` workflow with a specific input (`local=False`) and prints the URL of the execution. This allows running the workflow on a remote cluster rather than locally.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom union.remote import UnionRemote\n\nremote = UnionRemote(default_project=\"default\", default_domain=\"development\")\n```\n\nLANGUAGE: python\nCODE:\n```\nindices_execution = remote.execute(build_indices_wf, inputs={\"local\": False})\nprint(indices_execution.execution_url)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together AI Client\nDESCRIPTION: Initializes the Together AI client using API keys for authentication. This client will be used to upload files, create fine-tuning jobs, and monitor progress.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n## Setup Together AI client\nfrom together import Together\nimport os\nimport json\n\nTOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\nWANDB_API_KEY = os.getenv(\"WANDB_API_KEY\") # needed for logging fine-tuning to wandb\n\n\nclient = Together(api_key=TOGETHER_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Converting Preference Data to SFT Format for Two-Stage Fine-tuning\nDESCRIPTION: Functions to convert DPO preference data format to SFT (Supervised Fine-Tuning) format. This preprocessing step enables a two-stage approach (SFT followed by DPO) which typically yields better results than applying DPO directly to a base model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Convert the preference dataset to SFT format\ndef convert_preference_to_sft_format(data):\n    \"\"\"\n    Convert Preference data format to SFT format.\n    \n    Takes input messages and preferred output and formats them into a chat format\n    with appropriate role assignments.\n    \"\"\"\n    messages = []\n    for msg in data[\"input\"][\"messages\"]:\n        messages.append(msg)\n    messages.extend(data[\"preferred_output\"])\n    \n    return {\"messages\": messages}\n\ndef process_preference_to_sft(input_data, output_path, split=\"train\"):\n    \"\"\"\n    Process the preference dataset and convert it to SFT format.\n    \n    Args:\n        input_data: Dictionary containing train and validation preference data\n        output_path: Path to save the output SFT jsonl file\n        split: Dataset split to process (\"train\" or \"validation\")\n    \"\"\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    \n    line_count = 0\n    with open(output_path, 'w') as outfile:\n        for example in input_data[split]:\n            try:\n                sft_format = convert_preference_to_sft_format(example)\n                outfile.write(json.dumps(sft_format) + '\\n')\n                line_count += 1\n                if line_count % 2000 == 0 and split == \"train\":\n                    print(f\"Processed {line_count} examples\")\n            except Exception as e:\n                print(f\"Error processing example {line_count + 1}: {str(e)}\")\n    \n    print(f\"Processed {line_count} examples for {split}. Output saved to {output_path}\")\n    return line_count\n\n# Process the training dataset\nsft_train_output_path = \"data/helpsteer2_sft_training.jsonl\"\nsft_train_count = process_preference_to_sft(converted_dataset, sft_train_output_path, \"train\")\n\n# Process the validation dataset\nsft_validation_output_path = \"data/helpsteer2_sft_validation.jsonl\"\nsft_validation_count = process_preference_to_sft(converted_dataset, sft_validation_output_path, \"validation\")\n\n# Display a sample SFT example from training set\nwith open(sft_train_output_path, 'r') as f:\n    sample_sft = json.loads(f.readline().strip())\n    \nprint(\"\\nSample SFT example:\")\nprint(json.dumps(sample_sft, indent=2))\n\n# Compare dataset sizes\nprint(f\"\\nPreference dataset sizes:\")\nprint(f\"  Training: {len(converted_dataset['train'])} examples\")\nprint(f\"  Validation: {len(converted_dataset['validation'])} examples\")\n\nprint(f\"SFT dataset sizes:\")\nprint(f\"  Training: {sft_train_count} examples\")\nprint(f\"  Validation: {sft_validation_count} examples\")\n```\n\n----------------------------------------\n\nTITLE: Generating Movie Embeddings\nDESCRIPTION: Calls the generate_embeddings function to create vector embeddings for all prepared movie texts using the BAAI/bge-base-en-v1.5 model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nembeddings = generate_embeddings(to_embed, 'BAAI/bge-base-en-v1.5')\n```\n\n----------------------------------------\n\nTITLE: Displaying Worker Results for Each Subtask\nDESCRIPTION: Prints the results from each worker LLM for their respective subtasks, showing how different aspects of the main task were handled.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor task_info, response in zip(tasks, worker_resp):\n    print(f\"\\n=== WORKER RESULT ({task_info['type']}) ===\\n{response}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Generating Query Embedding for Search\nDESCRIPTION: Creates an embedding for a sample query about 'skip-level meetings' that will be used for semantic search in the vector space.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Generate the vector embeddings for the query\nquery = \"What are 'skip-level' meetings?\"\n\nquery_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]\n```\n\n----------------------------------------\n\nTITLE: Importing Together API and Setting API Key\nDESCRIPTION: Imports the Together library and sets up the API key, either from environment variables or manual input.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport together, os\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Verifying Response Collection Completeness\nDESCRIPTION: Performs a simple validation check to ensure all expected responses from the parallel LLM calls have been collected.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Check we get all responses\nassert len(results) == len(reference_models)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Top 5 Chunks by Similarity\nDESCRIPTION: Gets the actual text content of the top 5 most similar chunks based on similarity scores.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntop_5_chunks = [chunks[index] for index in indices][:5]\n\ntop_5_chunks\n```\n\n----------------------------------------\n\nTITLE: Printing Reranked Results in Python\nDESCRIPTION: This snippet iterates through the results returned by the rerank model and prints the documents alongside their relevance scores to evaluate improvements in the search results.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfor result in response.results:\n    print(f\"Document: {hybrid_top_k_docs[result.index]}\")\n    print(f\"Relevance Score: {result.relevance_score}\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Converting Extracted Text to Structured JSON\nDESCRIPTION: Uses Llama 3.1 70B with JSON mode to convert the extracted text information into a structured JSON format conforming to the Receipt schema defined earlier.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nextract = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"The following is a detailed description of all the items, prices and quantities on a receipt. Extract out information. Only answer in JSON.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": info,\n            },\n        ],\n        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n        response_format={\n            \"type\": \"json_object\",\n            \"schema\": Receipt.model_json_schema(),\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Movie Embeddings\nDESCRIPTION: Generates embeddings for the movie texts using the BGE base English model. These embeddings capture the semantic meaning of each movie's details.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Use bge-base-en-v1.5 model to generate embeddings\nembeddings = generate_embeddings(to_embed, 'BAAI/bge-base-en-v1.5')\n```\n\n----------------------------------------\n\nTITLE: Evaluating Deployed Model - Python\nDESCRIPTION: This snippet executes a loop to evaluate the performance of the fine-tuned model on the first 25 task items by calculating scores and length differences between expected and actual completions. The results are averaged and printed at the end.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nscores = []\nlength_differences = []\n\nfor item in tqdm(task_items[:25]):\n    query = item['prompt']\n\n    # We have deployed the finetuned model here to evaluate it\n    result = llm_call(query, model=\"thepowerfuldeez/Meta-Llama-3.1-8B-32k-Instruct-Reference-long-context-finetune-ce1f61d6-afb7623b\")\n\n    score = ratio(item['completion'], result)\n    length_differences.append(abs(len(item['completion'].split()) - len(result.split())))\n    scores.append(score)\n\nprint(np.mean(scores), np.mean(length_differences))\n```\n\n----------------------------------------\n\nTITLE: Comparing Multiple Movie Embedding Vectors\nDESCRIPTION: Visualizes and compares embedding vectors for the first three movies in the dataset as stacked barcode-like heatmaps.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Select three movie embeddings\ndata1 = embeddings[0]\ndata2 = embeddings[1]\ndata3 = embeddings[2]\n\n# Reshape each embedding into a 2D array\ndata1_2d = np.reshape(data1, (1, 768))\ndata2_2d = np.reshape(data2, (1, 768))\ndata3_2d = np.reshape(data3, (1, 768))\n\n# Create a figure with three subplots\nfig, axs = plt.subplots(3, 1, figsize=(15, 3))\n\n# Plot each embedding as a heatmap\naxs[0].imshow(data1_2d, cmap='binary', interpolation='nearest', aspect='auto')\naxs[1].imshow(data2_2d, cmap='binary', interpolation='nearest', aspect='auto')\naxs[2].imshow(data3_2d, cmap='binary', interpolation='nearest', aspect='auto')\n\n# Set labels and titles\naxs[0].set_ylabel(f\"{movies_data[0]['title']}\")\naxs[1].set_ylabel(f\"{movies_data[1]['title']}\")\naxs[2].set_ylabel(f\"{movies_data[2]['title']}\")\naxs[2].set_xlabel('Vector Dimension')\n\n# Remove ticks\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Layout so plots do not overlap\nfig.tight_layout()\n\n# Show the plot\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Generating and Displaying Document Chunks\nDESCRIPTION: Creates chunks from the essay with a size of 250 characters and 30 character overlap, then prints each chunk with its index for review.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nchunks = create_chunks(pg_essay, chunk_size=250, overlap=30)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i + 1}: {chunk}\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Long Documents (64k-128k Tokens)\nDESCRIPTION: Filters the dataset to extract 2000 document examples with token counts between 64,000 and 128,000 for long context testing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Extract 2000 examples of documents with 64k to 128k tokens\n\nlong_documents_128k = []\nfor sample in tqdm(ds_iterator.filter(lambda x: x['token_count'] > 64000 and x['token_count'] < 128000)):\n    # From 64k tokens to 128k tokens\n    if (len(long_documents_128k) < 2000):\n        document = sample['text']\n        long_documents_128k.append(document)\n    else:\n        break\n```\n\n----------------------------------------\n\nTITLE: Creating Text Chunks with Overlap\nDESCRIPTION: Defines a function to split the document into overlapping chunks of specified size. This chunking method creates fixed-size segments with a defined overlap between consecutive chunks.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# We can get away with naive fixed sized chunking as the context generation will add meaning to these chunks\n\ndef create_chunks(document, chunk_size=300, overlap=50):\n    return [document[i : i + chunk_size] for i in range(0, len(document), chunk_size - overlap)]\n```\n\n----------------------------------------\n\nTITLE: Displaying Structured JSON Output\nDESCRIPTION: Parses and displays the final structured JSON output containing the receipt information with formatted indentation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noutput = json.loads(extract.choices[0].message.content)\nprint(json.dumps(output, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance on Multiple Tasks\nDESCRIPTION: Runs the model on 25 task examples, calculating similarity scores and length differences to evaluate overall performance on the repetition task.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nscores = []\nlength_differences = []\nfor item in tqdm(task_items[:25]):\n    query = item['prompt']\n    result = llm_call(query)\n    score = ratio(item['completion'], result)\n    length_differences.append(abs(len(item['completion'].split()) - len(result.split())))\n    scores.append(score)\nprint(np.mean(scores), np.mean(length_differences))\n```\n\n----------------------------------------\n\nTITLE: Retrieving Top 10 Most Relevant Movie Titles\nDESCRIPTION: Extracts and displays the titles of the top 10 movies most semantically similar to the user query based on the sorted similarity scores.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntop_10_sorted_titles = [movies_data[index]['title'] for index in indices[0]][:10]\n\ntop_10_sorted_titles\n```\n\n----------------------------------------\n\nTITLE: Visualizing Performance Improvements with Seaborn in Python\nDESCRIPTION: This code creates a distribution plot of the score differences between fine-tuned and baseline models using Seaborn. It visualizes how fine-tuning impacts model performance across the test dataset.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsns.displot(score_differences)\nplt.xlabel(\"GovReport fine-tuned - GovReport baseline\")\n```\n\n----------------------------------------\n\nTITLE: Displaying the Final Aggregated Response\nDESCRIPTION: Prints the final answer generated by the aggregator model after analyzing all of the intermediate responses from the parallel workflow.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Final Answer: {answer}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Parsing Main Page for Document URLs in Python\nDESCRIPTION: Actor task that parses the main page of Paul Graham's essays to extract document titles and URLs using BeautifulSoup.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@actor.task\ndef parse_main_page(\n    base_url: str, articles_url: str, local: bool = False\n) -> list[Document]:\n    from bs4 import BeautifulSoup\n\n    assert base_url.endswith(\"/\"), f\"Base URL must end with a slash: {base_url}\"\n    response = requests.get(urljoin(base_url, articles_url))\n    soup = BeautifulSoup(response.text, \"html.parser\")\n\n    td_cells = soup.select(\"table > tr > td > table > tr > td\")\n    documents = []\n\n    idx = 0\n    for td in td_cells:\n        img = td.find(\"img\")\n        if img and int(img.get(\"width\", 0)) <= 15 and int(img.get(\"height\", 0)) <= 15:\n            a_tag = td.find(\"font\").find(\"a\") if td.find(\"font\") else None\n            if a_tag:\n                documents.append(\n                    Document(\n                        idx=idx, title=a_tag.text, url=urljoin(base_url, a_tag[\"href\"])\n                    )\n                )\n                idx += 1\n\n    if local:\n        return documents[:3]\n\n    return documents\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-processed Task Items\nDESCRIPTION: Loads the pre-processed repetition task prompts and completions from a JSON file for evaluation and fine-tuning.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Load the task items from provided JSON file\n\ntask_items = orjson.loads(Path(\"task_items.json\").read_bytes())\ntask_items = task_items['task1']\n```\n\n----------------------------------------\n\nTITLE: Counting Tokens in the Generated Summary\nDESCRIPTION: Counts the number of tokens in the generated summary using the cl100k_base encoding to compare with the original document's token count.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnum_tokens(summary, 'cl100k_base')\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Optimized Agent Performance\nDESCRIPTION: Measures the performance improvement of the optimized agent compared to the baseline using the same evaluation metric.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nevaluate(optimized_react)\n```\n\n----------------------------------------\n\nTITLE: Executing a Single Tavily Search Query with Python\nDESCRIPTION: This snippet executes a single search operation using the first query from a generated list. It demonstrates how to call the tavily_search function and process the returned results for review and analysis.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsearch_results=await tavily_search(initial_queries[0], tavily_client, prompts, together_client, summary_model)\n```\n\n----------------------------------------\n\nTITLE: Performing Image-to-Image Search Using JinaCLIP in Python\nDESCRIPTION: Demonstrates image-to-image search by using a new image as a query to retrieve the most similar image from the dataset using the retrieve_image function.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Search the internet for a new image\nnew_image = search_images('cute pet dog running', max_images=1)[0]\n\ndisplay.Image(url=new_image, width=500)\n\n# Use the image above as a query to retrieve the most similar image from our dataset of 12 images\nimage_2_image = links[retrieve_image(query=new_image, query_type='image', index=image_embeddings)]\n\ndisplay.Image(url=image_2_image, width=500)\n```\n\n----------------------------------------\n\nTITLE: Initializing the Together AI Client\nDESCRIPTION: Sets up the Together AI client using an API key stored in environment variables to access the Together AI services.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom together import Together\n\n# Paste in your Together AI API Key or load it\n\nclient = Together(api_key = os.environ.get(\"TOGETHER_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Validating JSONL File Format for Fine-tuning\nDESCRIPTION: Checks that the prepared JSONL file meets Together AI's format requirements for fine-tuning. The function verifies that each line is a valid JSON object with the expected structure.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n## We're going to check to see that the file is in the right format before we finetune\nfrom together.utils import check_file\n\nsft_report = check_file(\"coqa_prepared_train.jsonl\")\nprint(json.dumps(sft_report, indent=2))\n\nassert sft_report[\"is_check_passed\"] == True\n```\n\n----------------------------------------\n\nTITLE: Viewing a HoVer Dataset Example\nDESCRIPTION: Displays an example from the training set showing a claim and the correct Wikipedia pages that must be retrieved to verify it.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# View an example\nexample = trainset[0]\n\nprint(\"Claim:\", example.claim)\nprint(\"Correct pages that must be retrieved:\", example.titles)\n```\n\n----------------------------------------\n\nTITLE: Generating Query Embedding for Vector Search\nDESCRIPTION: Creates an embedding for a specific query about floating point operations mentioned in the bill.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Generate the vector embeddings for the query\nquery = \"what is the maximum allowable floating point operation per second this bill allows?\"\n\nquery_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]\n```\n\n----------------------------------------\n\nTITLE: Verifying Embedding Dimensions\nDESCRIPTION: Checks the dimension of the first embedding vector to ensure it has the expected 768 dimensions.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Check the shape of each vector embedding to make sure it is 768 dimensions\nlen(embeddings[0])\n```\n\n----------------------------------------\n\nTITLE: Extracting Response Content from LLM Output in Python\nDESCRIPTION: This line retrieves the content of the message from the LLM's response, which will be the final answer to the user's query based on the processed information.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nresponse.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor Environment and Document Model in Python\nDESCRIPTION: Sets up the Union Actor environment with container configuration, dependencies, and secrets. Defines a Pydantic Document model for storing document metadata.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pathlib import Path\nfrom typing import Annotated, Optional\nfrom urllib.parse import urljoin\n\nimport flytekit as fl\nimport numpy as np\nimport requests\nimport union\nfrom flytekit.core.artifact import Artifact\nfrom flytekit.exceptions.base import FlyteRecoverableException\nfrom flytekit.types.directory import FlyteDirectory\nfrom flytekit.types.file import FlyteFile\nfrom pydantic import BaseModel\nfrom union.actor import ActorEnvironment\n\nactor = ActorEnvironment(\n    name=\"contextual-rag\",\n    replica_count=10,\n    ttl_seconds=120,\n    container_image=union.ImageSpec(\n        name=\"contextual-rag\",\n        packages=[\n            \"together==1.3.10\",\n            \"beautifulsoup4==4.12.3\",\n            \"bm25s==0.2.5\",\n            \"pydantic>2\",\n            \"pymilvus>=2.5.4\",\n            \"union>=0.1.139\",\n            \"flytekit>=1.15.0b5\",\n        ],\n    ),\n    secret_requests=[\n        fl.Secret(\n            key=\"together-api-key\", \n            env_var=\"TOGETHER_API_KEY\",\n            mount_requirement=union.Secret.MountType.ENV_VAR,\n        ),\n        fl.Secret(\n            key=\"milvus-uri\",\n            env_var=\"MILVUS_URI\",\n            mount_requirement=union.Secret.MountType.ENV_VAR,\n        ),\n        fl.Secret(\n            key=\"milvus-token\",\n            env_var=\"MILVUS_TOKEN\",\n            mount_requirement=union.Secret.MountType.ENV_VAR,\n        )\n    ],\n)\n\nclass Document(BaseModel):\n    idx: int\n    title: str\n    url: str\n    content: Optional[str] = None\n    chunks: Optional[list[str]] = None\n    prompts: Optional[list[str]] = None\n    contextual_chunks: Optional[list[str]] = None\n    tokens: Optional[list[list[int]]] = None\n```\n\n----------------------------------------\n\nTITLE: Testing Wikipedia Search Function\nDESCRIPTION: Tests the search function with a query about Toronto restaurants to retrieve relevant Wikipedia articles.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsearch(query='toronto restaurants', k = 3)\n```\n\n----------------------------------------\n\nTITLE: Generating NBA Playoffs Knowledge Graph Example\nDESCRIPTION: Uses the previously defined function to generate a knowledge graph about the 2019 NBA playoffs, showing how the LLM structures information as nodes and edges.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Lets use the function above to generate a knowledge graph about the 2019 NBA playoffs\n# For context in the 2019 NBA playoffs, the Toronto Raptors won the NBA championship facing the Golden State Warriors.\n\ngraph = generate_graph(\"NBA 2019 playoffs\")\n```\n\n----------------------------------------\n\nTITLE: Loading CoQA Dataset for Conversational Finetuning\nDESCRIPTION: Loads the CoQA dataset from the Hugging Face dataset repository. This dataset contains 127,000+ questions with answers from 8000+ conversations designed for Conversational Question Answering systems.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ncoqa_dataset = load_dataset(\"stanfordnlp/coqa\")\n```\n\n----------------------------------------\n\nTITLE: Generating Conditional Image Using Retrieved Image in Python\nDESCRIPTION: Uses the generate_image function to create a holiday cartoon version of a retrieved image using the FLUX model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ngenerated_image = generate_image(\"Create a cute holiday cartoon version of this image.\", retrieved_image = retrieved_image)\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Functionality with Simple Prompt\nDESCRIPTION: Tests the configured Llama 8B model with a simple prompt to ensure it's working correctly.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nllama8b(\"Say this is a test!\", temperature=0.7)  # => ['This is a test!']\n#llama3b(messages=[{\"role\": \"user\", \"content\": \"Say this is a test!\"}])  # => ['This is a test!']\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together API Client and LLM Call Function\nDESCRIPTION: Initializes the Together API client using environment variables and defines a helper function for making LLM API calls with consistent parameters.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the Together client and setup LLM calling function\n\nTOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\nWANDB_API_KEY = os.getenv(\"WANDB_API_KEY\") # If you'd like to view fine-tuning results on W&B\n\nclient = Together(api_key = TOGETHER_API_KEY)\n\ndef llm_call(query, model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n          {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n          {\"role\": \"user\", \"content\": query},\n        ],\n        temperature=1.0,\n        seed=42,\n    )\n    result = response.choices[0].message.content\n    return result\n```\n\n----------------------------------------\n\nTITLE: Library Imports and API Setup\nDESCRIPTION: Importing required libraries and initializing the Together API client with authentication.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport asyncio\nimport together\nfrom together import Together\n\nfrom typing import Any, Optional, Dict, List, Literal\nfrom pydantic import Field, BaseModel, ValidationError\n\nTOGETHER_API_KEY = \"-- TOGETHER API KEY --\"\n\nclient = Together(api_key= TOGETHER_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Calculating Document Token Count\nDESCRIPTION: Counts the number of tokens in the full essay using the cl100k_base encoding, which is compatible with many LLMs.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# We'll assume we're using the \"cl100k_base\" encoding\n\nnum_tokens_from_string(pg_essay, \"cl100k_base\")\n```\n\n----------------------------------------\n\nTITLE: Executing the Flight Booking Application in Python\nDESCRIPTION: A simple code snippet demonstrating how to run the application using asyncio. This executes the main function which starts the booking process for a flight from SFO to ANC.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nawait main()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up API Key\nDESCRIPTION: Imports the necessary libraries for working with the Together AI API and sets up the API key from environment variables.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\nimport together\nimport os, json\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Configuring User Prompt for Tool-Based Task\nDESCRIPTION: Creates a message prompt instructing the LLM to find and summarize tweets from a specific Twitter account. This demonstrates how to structure a request that will require tool use.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# User message to the LLM\nmessages = [\n  {\n    \"role\": \"user\",\n    \"content\": \"Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!\",\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Loading CoQA Dataset for Fine-tuning\nDESCRIPTION: Loads the Stanford CoQA (Conversational Question Answering) dataset from Hugging Face Hub using the datasets library. This dataset contains stories with related questions and answers.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ncoqa_dataset = load_dataset(\"stanfordnlp/coqa\")\n```\n\n----------------------------------------\n\nTITLE: Generating User Notification with LLM\nDESCRIPTION: This code snippet demonstrates how to use an LLM to generate a user-friendly message indicating that the image is ready to be viewed. It combines the original image request with the tool run output and sends it back to the LLM for message generation. The final LLM response containing the image URL is then printed to the console.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimage_messages = generate_image_request + tool_run\n\nlast_response = client.chat.completions.create(\n  model=MODEL,\n  messages=image_messages,\n  tools=tools,\n)\n\nprint('LLM RESPONSE:', last_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Saving Transformed Dataset to JSONL File\nDESCRIPTION: Exports the transformed dataset to a JSONL file where each line is a valid JSON object representing a conversation. This format is required for uploading to Together AI for fine-tuning.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_messages.to_json(\"coqa_prepared_train.jsonl\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Packages for Together.AI and Toolhouse\nDESCRIPTION: Imports the necessary Python libraries for working with the OpenAI SDK (compatible with Together's API) and Toolhouse SDK for accessing pre-built tools.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import packages\ntry:\n  import os\n  from openai import OpenAI\n  from toolhouse import Toolhouse\n  print(\"Packages installed\")\nexcept:\n  print(\"Packages not installed\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Prompt Token Count\nDESCRIPTION: Counts the number of tokens in the first prompt to estimate the input size for each LLM call.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnum_tokens_from_string(prompts[0], \"cl100k_base\")\n```\n\n----------------------------------------\n\nTITLE: Executing BM25 Retrieval Function in Python\nDESCRIPTION: This line executes the bm25 retrieval function with a specific query to find the top k relevant documents according to the query's content.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nbm25_retreival(query = \"What are 'skip-level' meetings?\", k = 5, bm25_index = retriever)\n```\n\n----------------------------------------\n\nTITLE: Creating Text Chunks for Document Processing\nDESCRIPTION: A function to divide the document into overlapping chunks of fixed size for easier processing and retrieval.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# We can get away with naive fixed sized chunking as the context generation will add meaning to these chunks\n\ndef create_chunks(document, chunk_size=300, overlap=50):\n    return [document[i : i + chunk_size] for i in range(0, len(document), chunk_size - overlap)]\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Answer Generation for CoQA Evaluation\nDESCRIPTION: Defines a function that generates answers from the model for evaluation. For each story in the validation set, it sends questions to the model and collects the generated answers for later comparison with ground truth.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# This function is used to generate model answers on the CoQA validation set from the untuned reference and fine-tuned models\n\ndef get_model_answers(model_name):\n    \"\"\"\n    Generate model answers for a given model name using a dataset of questions and answers.\n    Args:\n        model_name (str): The name of the model to use for generating answers.\n    Returns:\n        list: A list of lists, where each inner list contains the answers generated by the model for the corresponding set of questions in the dataset.\n    The function performs the following steps:\n    1. Initializes an empty list to store the model answers.\n    2. Defines an inner function `get_answers` that takes a data dictionary and generates answers for the questions in the data.\n    3. Uses a thread pool to parallelize the process of generating answers for each entry in the validation dataset.\n    4. Appends the generated answers to the `model_answers` list.\n    5. Returns the `model_answers` list.\n    Note:\n        - The `system_prompt` and `client` variables are assumed to be defined elsewhere in the code.\n        - The `coqa_dataset` variable is assumed to contain the dataset with a \"validation\" key.\n    \"\"\"\n\n    model_answers = []\n\n    def get_answers(data):\n        answers = []\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt.format(data[\"story\"]),\n            }\n        ]\n        for q, true_answer in zip(data[\"questions\"], data[\"answers\"][\"input_text\"]):\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": q\n                }\n            )\n            chat_completion = client.chat.completions.create(\n                messages=messages,\n                model=model_name,\n                max_tokens=64,\n            )\n            answer = chat_completion.choices[0].message.content\n            answers.append(answer)\n        return answers\n\n\n    with ThreadPool(8) as pool:\n        for answers in tqdm(pool.imap(get_answers, coqa_dataset[\"validation\"]), total=len(coqa_dataset[\"validation\"])):\n            model_answers.append(answers)\n\n    return model_answers\n```\n\n----------------------------------------\n\nTITLE: Retrieving Top 25 Similar Movies\nDESCRIPTION: Extracts the titles of the 25 movies most similar to the query based on the sorted similarity scores. These will be the candidates for reranking.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Get the top 25 movie titles that are most similar to the query - these will be passed to the reranker\ntop_25_sorted_titles = [movies_data[index]['title'] for index in indices[0]][:25]\n\ntop_25_sorted_titles\n```\n\n----------------------------------------\n\nTITLE: Applying Chunking to Document and Inspecting Results\nDESCRIPTION: Creates chunks from the document text with specified size and overlap parameters, then displays the first few chunks for inspection.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nchunks = create_chunks(text, chunk_size=350, overlap=40)\n\nfor i, chunk in enumerate(chunks[:3]):\n    print(f\"Chunk {i + 1}: {chunk}\")\n```\n\n----------------------------------------\n\nTITLE: LLM Helper Functions - API Interaction\nDESCRIPTION: Implements helper functions for making LLM API calls with support for both regular and JSON-structured responses\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n    messages.append({\"role\": \"user\", \"content\": user_prompt})\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0.7,\n        max_tokens=4000,        \n    )\n\n    return response.choices[0].message.content\n\ndef JSON_llm(user_prompt : str, schema : BaseModel, system_prompt : Optional[str] = None):\n    try:\n        messages = []\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        \n        messages.append({\"role\": \"user\", \"content\": user_prompt})\n        \n        extract = client.chat.completions.create(\n            messages=messages,\n            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n            response_format={\n                \"type\": \"json_object\",\n                \"schema\": schema.model_json_schema(),\n            },\n        )\n        \n        response = json.loads(extract.choices[0].message.content)\n        return response\n        \n    except ValidationError as e:\n        raise ValueError(f\"Schema validation failed: {str(e)}\")\n```\n\n----------------------------------------\n\nTITLE: Printing Extracted Text Information\nDESCRIPTION: Prints the raw text information extracted from the receipt image by the vision model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Initializing Together API Client in Python\nDESCRIPTION: This code sets up the environment by importing required libraries, initializing the Together API client, and defining a function for making LLM calls. It requires the TOGETHER_API_KEY environment variable to be set.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom together import Together\n\nimport orjson\nimport numpy as np\nimport os\nimport pandas as pd\n\nfrom evaluate import load\n\nTOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n#WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n\nclient = Together(api_key = TOGETHER_API_KEY)\n\ndef llm_call(query, model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n          {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n          {\"role\": \"user\", \"content\": query},\n        ],\n        temperature=1.0,\n        seed=42,\n        max_tokens=1200,\n    )\n    result = response.choices[0].message.content\n    return result\n```\n\n----------------------------------------\n\nTITLE: Creating Movie Text Embeddings\nDESCRIPTION: Processes movie data by concatenating title, overview, and tagline for embedding generation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Concatenate the title, overview, and tagline of each movie\nto_embed = []\nfor movie in movies_data[:1000]:\n    text = ''\n    for field in ['title', 'overview', 'tagline']:\n        value = movie.get(field, '')\n        text += str(value) + ' '\n    to_embed.append(text.strip())\n\nto_embed[:10]\n```\n\n----------------------------------------\n\nTITLE: Generating Realistic Sketch with Multiple LoRAs\nDESCRIPTION: Shows how to combine a sketch LoRA with a realism-enhancing LoRA to create a realistic sketch. It demonstrates the use of trigger words and different scale values for each LoRA.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"an baby panda eating bamboo illustration in the style of SMPL\"\n\ngenerated_image = generate_image(prompt, \n                                lora1=\"https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-add-details\",\n                                scale1=0.9,\n                                lora2=\"https://huggingface.co/dvyio/flux-lora-simple-illustration\",\n                                scale2=1.25\n                                )\n\nImage(url=generated_image, width=512, height=384)\n```\n\n----------------------------------------\n\nTITLE: Generating Irrelevant Summary using Python\nDESCRIPTION: This snippet generates an irrelevant summary using a predefined prompt and prints the result. It requires the 'summarize' function and the 'SUMMARIZATION_PROMPT_IRRELEVANT'.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\nsummary_irrelevant = summarize(text, SUMMARIZATION_PROMPT_IRRELEVANT)\nprint(summary_irrelevant)\n\"\"\"\n\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Movie Dataset\nDESCRIPTION: Downloads and sets up the movie dataset JSON file for RAG implementation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Let's get the movies dataset\n!wget https://raw.githubusercontent.com/togethercomputer/together-cookbook/refs/heads/main/datasets/movies.json\n!mkdir datasets\n!mv movies.json datasets/movies.json\n```\n\n----------------------------------------\n\nTITLE: Executing Vector Retrieval Function in Python\nDESCRIPTION: This line calls the vector retrieval function with a specific query and context embeddings, aiming to retrieve the top k items based on similarity to the query.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nvector_retreival(query = \"What are 'skip-level' meetings?\", top_k = 5, vector_index = contextual_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Initializing API Clients for Research Tools in Python\nDESCRIPTION: Sets up the API clients for Together (LLM provider) and Tavily (search provider) with placeholders for API keys, which are essential for the research pipeline to access external services.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Initialize Clients\n# ------------------\n# Initialize the Together and Tavily clients with your API keys\ntogether_client = AsyncTogether(api_key=\"--your--together-api-key--\")\ntavily_client = AsyncTavilyClient(api_key=\"--your--tavily-api-key--\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tool Call Details\nDESCRIPTION: Examines the details of the tool calls made by the LLM, showing the ID, type, and function arguments for each tool that was selected for use in fulfilling the user's request.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntools_called = response.choices[0].message.tool_calls\nfor tool_called in tools_called:\n    print(f\"ID: {tool_called.id}\")\n    print(f\"Type: {tool_called.type}\")\n    print(f\"Function: {tool_called.function}\")\n    print('\\n')\n```\n\n----------------------------------------\n\nTITLE: Downloading and Parsing Legislative Document\nDESCRIPTION: Fetches the text content of California's SB1047 bill from LegiScan using requests and BeautifulSoup for HTML parsing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_legiscan_text(url):\n    \"\"\"\n    Fetches and returns the text content from a given LegiScan URL.\n    Args:\n        url (str): The URL of the LegiScan page to fetch.\n    Returns:\n        str: The text content of the page.\n    Raises:\n        requests.exceptions.RequestException: If there is an issue with the HTTP request.\n    \"\"\"\n    # Basic headers to mimic a browser\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n\n    # Make the request\n    response = requests.get(url, headers=headers)\n\n    # Parse HTML\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Get text content\n    content = soup.get_text()\n\n    return content\n\nurl = \"https://legiscan.com/CA/text/SB1047/id/2999979/California-2023-SB1047-Amended.html\"\ntext = get_legiscan_text(url)\nprint(text[:1000])\n```\n\n----------------------------------------\n\nTITLE: Scraping Essay Content from a Web Page\nDESCRIPTION: Defines a function to scrape Paul Graham's essay from his website using BeautifulSoup. The function retrieves the HTML content, extracts the text, and cleans it for processing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Let's download the essay from Paul Graham's website\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_pg_essay():\n\n    url = 'https://paulgraham.com/foundermode.html'\n\n    try:\n        # Send GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad status codes\n\n        # Parse the HTML content\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Paul Graham's essays typically have the main content in a font tag\n        # You might need to adjust this selector based on the actual HTML structure\n        content = soup.find('font')\n\n        if content:\n            # Extract and clean the text\n            text = content.get_text()\n            # Remove extra whitespace and normalize line breaks\n            text = ' '.join(text.split())\n            return text\n        else:\n            return \"Could not find the main content of the essay.\"\n\n    except requests.RequestException as e:\n        return f\"Error fetching the webpage: {e}\"\n\n# Scrape the essay\npg_essay = scrape_pg_essay()\n```\n\n----------------------------------------\n\nTITLE: Iterative Research Function in Python\nDESCRIPTION: This function conducts iterative research within a given budget to refine search results. It takes a research topic, initial results, a list of all queries used so far, a budget, maximum queries per iteration, Tavily and Together AI clients, models for planning, JSON parsing, and summarization, and a dictionary of prompt templates as input. It returns a tuple of the final search results and all queries used.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nasync def conduct_iterative_research(topic: str, initial_results: SearchResults, all_queries: List[str],\n                                  budget: int, max_queries: int, tavily_client: AsyncTavilyClient, together_client: AsyncTogether,\n                                  planning_model: str, json_model: str, summary_model: str, prompts: dict) -> tuple[SearchResults, List[str]]:\n    \"\"\"\n    Conduct iterative research within budget to refine results.\n\n    Args:\n        topic: The research topic\n        initial_results: Results from initial search\n        all_queries: List of all queries used so far\n        budget: Maximum number of follow-up iterations\n        max_queries: Maximum number of queries to use per iteration\n        tavily_client: The Tavily client for web search\n        together_client: The Together AI client for LLM operations\n        planning_model: Model to use for evaluation\n        json_model: Model to use for JSON parsing\n        summary_model: Model to use for summarization   \n        prompts: Dictionary of prompt templates\n\n    Returns:\n        Tuple of (final results, all queries used)\n    \"\"\"\n    results = initial_results\n\n    for _ in range(0, budget):\n        # Evaluate if more research is needed using the independent function\n        additional_queries = await evaluate_research_completeness(\n            topic, results, all_queries, together_client, planning_model, json_model, prompts\n        )\n\n        # Exit if research is complete\n        if not additional_queries:\n            print(\"No need for additional research\")\n            break\n\n        # Limit the number of queries if needed\n        if max_queries > 0:\n            additional_queries = additional_queries[:max_queries]\n        print(\"================================================\\n\\n\")\n        print(f\"Additional queries from evaluation parser: {additional_queries}\\n\\n\")\n        print(\"================================================\\n\\n\")\n\n        # Expand research with new queries\n        new_results = await perform_search(\n            additional_queries,\n            tavily_client,\n            prompts,\n            together_client,\n            summary_model\n        )\n\n        results = results + new_results\n        all_queries.extend(additional_queries)\n\n    return results, all_queries\n```\n\n----------------------------------------\n\nTITLE: Fetching Top 5 Indices From Similarity Scores in Python\nDESCRIPTION: This snippet retrieves the top 5 indices from the sorted similarity scores. It is crucial for identifying the most similar items to a given query.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntop_5_indices = indices[0][:5]\ntop_5_indices\n```\n\n----------------------------------------\n\nTITLE: Plotting Smoothed BERTScores Comparison in Python\nDESCRIPTION: This snippet smooths a specific BERTScore component and plots it alongside a relevant summary's score. Required dependencies include Matplotlib and NumPy for data plotting and manipulation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\nsmoothed_scores_irrelevant = rolling_window(scores_irrelevant[2], 3)\n\nplt.plot(np.linspace(0, 1, len(smoothed_scores_irrelevant)), smoothed_scores_irrelevant, label = \"Irrelevant Summary\")\nplt.plot(np.linspace(0, 1, len(smoothed_F1)), smoothed_F1, label = \"Relevant Summary\")\nplt.xlabel('Position in the Document')\nplt.ylabel('Score - higher is better')\nplt.title('Summary Score')\nplt.legend()\nplt.show()\n\"\"\"\n\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Essay Content Preview\nDESCRIPTION: Prints the first 1000 characters of the scraped essay to verify the content was retrieved correctly.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(pg_essay[:1000])\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataset of Diverse Image Links in Python\nDESCRIPTION: Generates a small dataset of 12 image links by searching for diverse topics using the custom search_images function.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Lets create a small dataset of 12 images containing diverse topics\nsearches = 'forest', 'dog', 'strawberry field', 'family picture'\n\nfrom time import sleep\n\nlinks = []\n\nfor o in searches:\n    links += search_images(o, max_images=3)\n    sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Executing Research Topic Processing in Python\nDESCRIPTION: Example code for running the research pipeline with a specific topic about AI in manufacturing. This demonstrates the practical application of the query generation system with the configured models and parameters.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresearch_topic=\"The impact of artificial intelligence on the manufacturing industry\"\n\ninitial_queries = await generate_initial_queries(\n    topic=research_topic,\n    together_client=together_client,\n    max_queries=max_queries,\n    planning_model=planning_model,\n    json_model=json_model,\n    prompts=prompts\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Retrieval Function Using JinaCLIP in Python\nDESCRIPTION: Defines a function to retrieve the most semantically relevant image based on a text or image query using the JinaCLIP model embeddings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef retrieve_image(query, query_type, index):\n    \"\"\"\n    Retrieve the index of the most similar image based on a query.\n    Args:\n        query (str or PIL.Image or str): The query input, which can be a text string,\n                                         a PIL image, or a local filename.\n        query_type (str): The type of the query, either 'text' or 'image'.\n        index (int): The index of the image to be retrieved.\n    Returns:\n        int: The index of the most similar image based on the query.\n    Raises:\n        ValueError: If the query_type is not 'text' or 'image'.\n    \"\"\"\n\n    if query_type == 'text':\n        query_embedding = model.encode_text(query)\n    elif query_type == 'image':\n        query_embedding = model.encode_image(query) # Accepts PIL.image, local filenames, dataURI\n    else:\n        raise ValueError(\"query_type must be 'text' or 'image'\")\n\n    similarities = query_embedding @ index.T # We calculate the similaritry between the query embedding and all the image embeddings\n\n    return np.argmax(similarities)\n```\n\n----------------------------------------\n\nTITLE: Splitting Function Calling Dataset for Two-Step Training\nDESCRIPTION: Splits the dataset into two parts: one for initial training on core categories and another for subsequent training on remaining categories.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfirst_step_categories = [\n    \"API Call\",\n    \"Information Extraction\",\n    'Industrial Software',\n    'Utilities Software',\n    'Robotic Process Automation (RPA)',\n    'Financial Software',\n    'Healthcare Software',\n    'Algorithmic Trading',\n]\n\nfirst_step_train = function_calling[\"train\"].filter(lambda x: x[\"category\"] in first_step_categories)\nsecond_step_train = function_calling[\"train\"].filter(lambda x: x[\"category\"] not in first_step_categories)\n```\n\n----------------------------------------\n\nTITLE: Displaying Available Toolhouse Tools\nDESCRIPTION: Lists all the tools available in the specified Toolhouse bundle, showing their names, types, and descriptions to help users understand what tools they can use in their application.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint('TOOLS AVAILABLE:')\nfor tool in th.get_tools(bundle=\"togethertoolhouse\"):\n    print(f\"Name: {tool['function']['name']}\")\n    print(f\"Type: {tool['type']}\")\n    print(f\"Description: {tool['function']['description']}\")\n    print(\"--------\")\n```\n\n----------------------------------------\n\nTITLE: Generating Training Data - Python\nDESCRIPTION: This snippet creates a JSONL file that excludes the first 25 task items for training purposes, saving the remaining items for model training using orjson. The generated file is 'task_train.jsonl'.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nPath(\"task_train.jsonl\").write_text(\"\\n\".join([orjson.dumps(item).decode(\"utf-8\") for item in task_items[25:]]))\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Parallel Agent Workflow\nDESCRIPTION: Installs the necessary Python libraries (pydantic and together) for implementing a parallel agent workflow.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install libraries\n!pip install -qU pydantic together\n```\n\n----------------------------------------\n\nTITLE: Uploading Training File - Python\nDESCRIPTION: This snippet uploads the training file 'task_train.jsonl' to the Together AI client, ensuring the upload completes with a check. The uploaded file's id is stored in 'task_file_id'.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.files.upload(file=\"task_train.jsonl\", check=True)\ntask_file_id = response.id\n```\n\n----------------------------------------\n\nTITLE: Customizing Gmail Tool with Preprocessors and Schema Processors\nDESCRIPTION: This code customizes the email behavior using preprocessors and schema processors. It defines functions to modify the input before sending (changing recipient email) and to modify the tool schema (removing fields from LLM's view). It then creates a processed email tool with both processors.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Preprocessor to override recipient email\ndef gmail_preprocessor(inputs: dict) -> dict:\n    inputs[\"recipient_email\"] = \"my_email.dev\"  # Change to an email you can access to test!\n    return inputs\n\n# Schema processor to hide recipient field from LLM\ndef gmail_schema_processor(schema: dict) -> dict:\n    del schema[\"recipient_email\"]\n    return schema\n\n# Create processed email tool with both processors\nprocessed_send_email_tool = toolset.get_tools(\n    actions=[Action.GMAIL_SEND_EMAIL],\n    processors={\n        \"schema\": {Action.GMAIL_SEND_EMAIL: gmail_schema_processor},\n        \"pre\": {Action.GMAIL_SEND_EMAIL: gmail_preprocessor},\n    },\n    check_connected_accounts=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Thought Process with DeepSeek-R1 via Together API in Python\nDESCRIPTION: This snippet uses the DeepSeek-R1 model to generate a thought process for a given question. It demonstrates how to use a stop token to control the generation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Thinking_Augmented_Generation.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nthought = client.chat.completions.create(\n  model=\"deepseek-ai/DeepSeek-R1\",\n  messages=[{\"role\": \"user\", \"content\": question}],\n  stop = ['</think>'] # Stop generation when </think> is encountered\n)\n\nprint(thought.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Testing Context Generation on a Single Chunk\nDESCRIPTION: Generates context for the first chunk and concatenates it with the original chunk to demonstrate the output format.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Lets generate one context and concatenate it with its chunk to see what the output looks like\ncontext_0 = generate_context(prompts[0])\nprint(context_0 + \" \\n\\n\" + chunks[0])\n```\n\n----------------------------------------\n\nTITLE: Retrieving Base64 Encoded Image for Llama 3.2 90B Vision\nDESCRIPTION: Retrieves the base64 encoded image of the most relevant page for the given query, to be used as input for the Llama 3.2 90B Vision model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel.search(query, k=1)\n\nreturned_page = model.search(query, k=1)[0].base64\n```\n\n----------------------------------------\n\nTITLE: Initializing Together AI Client\nDESCRIPTION: Setup of Together AI client with API key configuration.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport together, os\nfrom together import Together\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Installing Together AI Library\nDESCRIPTION: Installs the Together AI Python library using pip, which is required to access the Together AI API for embeddings and reranking.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n```\n\n----------------------------------------\n\nTITLE: Searching for Images Using Custom Function in Python\nDESCRIPTION: Demonstrates the usage of the custom search_images function to retrieve image URLs based on a search query.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Our function will allow us to search the web for images\nsearch_images('family picture', max_images=3)\n```\n\n----------------------------------------\n\nTITLE: Calculating BERTScores for Irrelevant Summary in Python\nDESCRIPTION: This snippet calculates the BERTScores for the irrelevant summary using 'chunked_BERTscores'. It assumes proper installation and setup of BERTScore dependencies.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\nscores_irrelevant = chunked_BERTscores(text, summary_irrelevant)\n\"\"\"\n\"\n```\n\n----------------------------------------\n\nTITLE: Generating Summary Response from Tool Data\nDESCRIPTION: Uses the LLM to generate a funny summary of the Twitter messages retrieved by the tool. This demonstrates using tool-fetched data to generate creative content.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsummary_response = client.chat.completions.create(\n  model=MODEL,\n  messages=messages\n)\n\nprint('LLM RESPONSE:')\nprint(summary_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Verifying Contextual Chunks Generation\nDESCRIPTION: Confirms that the number of contextual chunks matches the number of original chunks, ensuring all chunks have been processed.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# We should have one contextual chunk for each original chunk\nlen(contextual_chunks), len(contextual_chunks) == len(chunks)\n```\n\n----------------------------------------\n\nTITLE: Token Counting Function Using Tiktoken\nDESCRIPTION: Defines a function to count the number of tokens in a given text string using a specified encoding, which is useful for ensuring text fits within LLM context windows.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# token counter function\nimport tiktoken\n\ndef num_tokens(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n```\n\n----------------------------------------\n\nTITLE: Examining Transformed CoQA Dataset\nDESCRIPTION: Displays the transformed CoQA dataset in its new conversational format to verify the structure before saving it for finetuning.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_messages\n```\n\n----------------------------------------\n\nTITLE: Listing Files in Data Directory\nDESCRIPTION: A simple shell command to list files in the data directory, executed using the Jupyter notebook magic command for shell execution.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n!ls data\n```\n\n----------------------------------------\n\nTITLE: Initializing Together AI Client for Finetuning\nDESCRIPTION: Sets up the Together AI client with API credentials for model finetuning. This includes retrieving API keys from environment variables for both Together AI and Weights & Biases for tracking.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\nimport os\n\nTOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\nWANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n\n\nclient = Together(api_key=TOGETHER_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Defining BM25 Retrieval Function in Python\nDESCRIPTION: This function retrieves the top-k document indices based on the BM25 algorithm given a search query. It takes the query string, the number of documents to retrieve, and the BM25 index object, and returns the indices of the matched documents.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef bm25_retreival(query: str, k : int, bm25_index) -> List[int]:\n    \"\"\"\n    Retrieve the top-k document indices based on the BM25 algorithm for a given query.\n    Args:\n        query (str): The search query string.\n        k (int): The number of top documents to retrieve.\n        bm25_index: The BM25 index object used for retrieval.\n    Returns:\n        List[int]: A list of indices of the top-k documents that match the query.\n    \"\"\"\n\n    results, scores = bm25_index.retrieve(bm25s.tokenize(query), k=k)\n\n    return [contextual_chunks.index(doc) for doc in results[0]]\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Configuring LLM with Together API\nDESCRIPTION: This code block imports necessary libraries and configures the language model using Together's API. It sets up the Llama-3.3-70B model for use with PydanticAI.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport datetime, os\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\nfrom rich.prompt import Prompt\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai.usage import Usage, UsageLimits\n\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\n# Connect PydanticAI to LLMs on Together\nllm = OpenAIModel('meta-llama/Llama-3.3-70B-Instruct-Turbo',\n                  provider=OpenAIProvider(\n                      base_url=\"https://api.together.xyz/v1\",\n                      api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n                      ),\n                   )\n```\n\n----------------------------------------\n\nTITLE: Generating and Displaying the Summary\nDESCRIPTION: Calls the summarize function to generate a summary of the legislative text using the previously defined prompt template and prints the result.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsummary = summarize(text, SUMMARIZATION_PROMPT)\nprint(summary)\n```\n\n----------------------------------------\n\nTITLE: Extracting Movie Genres and Titles\nDESCRIPTION: Extracts the primary genre and title for each movie from the dataset for use in visualization.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Extract the genres and titles for each movie\ngenres = [movie['genres'].split()[0] for movie in movies_data]\ntitles = [movie['title'] for movie in movies_data]\nset(genres)\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Template for Thinking Augmented Generation in Python\nDESCRIPTION: This code defines a prompt template that incorporates both the original question and the generated thought process. It's used to structure the input for the smaller model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Thinking_Augmented_Generation.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nPROMPT_TEMPLATE = \"\"\"\nQuestion: {question}\nThought process: {thinking_tokens} </think>\nAnswer:\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Similarity Scores\nDESCRIPTION: Shows the calculated similarity scores for all evaluated tasks to assess model performance on the repetition task.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nscores\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for DPO Fine-Tuning\nDESCRIPTION: Installs the necessary Python libraries: 'together' for interacting with the Together AI API, and 'datasets' from Hugging Face for downloading and manipulating datasets.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU together datasets\n```\n\n----------------------------------------\n\nTITLE: Initiating DPO Fine-Tuning Job with Default Beta\nDESCRIPTION: Starts a Direct Preference Optimization (DPO) fine-tuning job using the Together AI platform. It sets various parameters including the DPO beta value, which controls how much the model can deviate from its reference behavior.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Directly perform DPO training\n\ndpo_training = client.fine_tuning.create(\n    training_method='dpo',\n    dpo_beta=0.1,\n    training_file=dpo_train_file.id,\n    validation_file=dpo_validation_file.id,\n    n_evals=10,\n    model=MODEL_NAME,\n    wandb_api_key=WANDB_API_KEY,\n    wandb_project_name=\"helpsteer2\",\n    suffix=\"helpsteer2_dpo_training\",\n    n_epochs=1,\n    n_checkpoints=1,\n    learning_rate=1e-5,\n    lora=True,\n)\nprint(dpo_training.id)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together AI Client\nDESCRIPTION: Initializes the Together AI client using API keys. This snippet configures the client with environment variables for both Together AI and Weights & Biases for tracking experiments.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\nimport os\n\nTOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\nWANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n\nclient = Together(api_key = TOGETHER_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Initializing API Clients for Together.AI and Toolhouse\nDESCRIPTION: Sets up the OpenAI client with Together.AI's base URL and initializes the Toolhouse client. Both use API keys stored as environment variables in Google Colab.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom google.colab import userdata\nclient = OpenAI(base_url = \"https://api.together.xyz/v1\", api_key=userdata.get(\"TOGETHER_API_KEY\"))\n\nth = Toolhouse(api_key=userdata.get('TOOLHOUSE_API_KEY'), provider=\"openai\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Parallel LLM Call Function\nDESCRIPTION: Defines an asynchronous function for making parallel LLM calls with retry logic for rate limiting, which is essential for the orchestrator workflow.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# The function below will call the reference LLMs in parallel\nasync def run_llm_parallel(user_prompt : str, model : str, system_prompt : str = None):\n    \"\"\"Run parallel LLM call with a reference model.\"\"\"\n    for sleep_time in [1, 2, 4]:\n        try:\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n            messages.append({\"role\": \"user\", \"content\": user_prompt})\n\n            response = await async_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                temperature=0.7,\n                max_tokens=2000,\n            )\n            break\n        except together.error.RateLimitError as e:\n            print(e)\n            await asyncio.sleep(sleep_time)\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Search Function using DuckDuckGo in Python\nDESCRIPTION: Defines a function to search for images based on given keywords using the DuckDuckGo search engine. It returns a list of image URLs.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom duckduckgo_search import DDGS\n\ndef search_images(keywords, max_images=10):\n    \"\"\"\n    Search for images based on given keywords and return a list of image URLs.\n    Args:\n        keywords (str): The search terms to use for finding images.\n        max_images (int, optional): The maximum number of images to retrieve. Defaults to 10.\n    Returns:\n        list: A list of URLs of the images found based on the search keywords.\n    \"\"\"\n\n    results = DDGS().images(keywords, max_results=max_images)\n\n    return [item['image'] for item in results]\n```\n\n----------------------------------------\n\nTITLE: Selecting a Sample Task Item\nDESCRIPTION: Selects a single task item for demonstration and testing purposes.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Select one task item for demonstration\n\nitem = task1_items[-1]\n```\n\n----------------------------------------\n\nTITLE: Installing Union Package for Python\nDESCRIPTION: This command installs the Union package, which is required to run the workflow described in the notebook. It uses pip, the Python package installer.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install union\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together AI Client and Model Configuration\nDESCRIPTION: Initializes the Together AI client using an API key and sets up the model configuration. It also imports necessary libraries and defines environment variables for API keys.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nfrom together import Together\n\nTOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\nWANDB_API_KEY = os.getenv(\"WANDB_API_KEY\") # needed for logging fine-tuning to wandb\n\nclient = Together(api_key=TOGETHER_API_KEY)\nMODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Reference\"\n```\n\n----------------------------------------\n\nTITLE: Uploading CoQA Dataset to Together AI\nDESCRIPTION: Uploads the prepared JSONL file containing the conversational CoQA dataset to the Together AI platform for finetuning.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Upload dataset to Together AI\n\ntrain_file_resp = client.files.upload(\"coqa_prepared_train.jsonl\", check=True)\nprint(train_file_resp)\n```\n\n----------------------------------------\n\nTITLE: Executing Multiple LLMs in Parallel with asyncio\nDESCRIPTION: Configures and executes multiple language models in parallel to solve a mathematical investment problem, gathering their responses asynchronously.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# These will be the intermediate proposer models\n\nreference_models = [\n    \"microsoft/WizardLM-2-8x22B\",\n    \"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n    \"google/gemma-2-27b-it\",\n    \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n]\n\nuser_prompt = \"\"\"Tim wants to invest some money in a bank which compounds quarterly\nwith an annual interest rate of $7\\%$. To the nearest dollar, how much money should he\ninvest if he wants a total of $\\$60,\\!000$ at the end of $5$ years?\"\"\"\n\n# Generate intermediate reference responses\nresults = await asyncio.gather(*[run_llm_parallel(user_prompt=user_prompt, model=model) for model in reference_models])\n```\n\n----------------------------------------\n\nTITLE: Printing Research Results in Python\nDESCRIPTION: This code snippet prints the first and last 10000 characters of the research results. This is useful for inspecting the content gathered during the research process. It also prints the number of results.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"First 10000 characters of {len(results.results)} results:\\n\\n {str(results)[:10000]}\\n\\n...\\n\\n Last 10000 characters of {len(results.results)} results:\\n\\n {str(results)[-10000:]}\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Importing Cosine Similarity for Vector Comparison\nDESCRIPTION: Imports the cosine_similarity function from sklearn to calculate similarity between query and document embeddings in the retrieval phase.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics.pairwise import cosine_similarity\n```\n\n----------------------------------------\n\nTITLE: Examining Task Prompt Format\nDESCRIPTION: Displays the first 100 characters of a task prompt to understand the format of the repetition task.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# What does a task item look like?\n\nitem['prompt'][:100]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for CFT\nDESCRIPTION: Installs the necessary Python libraries: together (Together AI client), datasets (for dataset manipulation), and matplotlib (for visualization).\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU together datasets matplotlib\n```\n\n----------------------------------------\n\nTITLE: Calculating Similarity Score for Sample Task\nDESCRIPTION: Calculates the similarity ratio between the model's output and the ground truth completion for a single example.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nratio(item['completion'], result)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gmail Integration with Composio\nDESCRIPTION: This code snippet demonstrates how to log in to Composio and add the Gmail tool. It uses the Composio CLI to authenticate and integrate Gmail, which will open a browser window for authentication.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# You can add the gmail tool by running the following command or manually going to https://app.composio.dev/integrations\n!composio login\n!composio add gmail\n```\n\n----------------------------------------\n\nTITLE: Demonstrating the Complete Parallel Workflow with a New Task\nDESCRIPTION: Provides a complete example of using the parallel workflow function with multiple LLMs to solve a simple math problem about apple picking.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Example Usage\n\nreference_models = [\n    \"microsoft/WizardLM-2-8x22B\",\n    \"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n    \"google/gemma-2-27b-it\",\n    \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n]\n\nuser_prompt = \"\"\"Jenna and her mother picked some apples from their apple farm. \nJenna picked half as many apples as her mom. If her mom got 20 apples, how many apples did they both pick?\"\"\"\n\naggregator_model = \"deepseek-ai/DeepSeek-V3\"\n\naggregator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models to the latest user query.\nYour task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information\nprovided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the\ngiven answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured,\ncoherent, and adheres to the highest standards of accuracy and reliability.\n\nResponses from models:\"\"\"\n\nanswer, intermediate_reponses = await parallel_workflow(prompt = user_prompt, \n                                                        proposer_models = reference_models, \n                                                        aggregator_model = aggregator_model, \n                                                        aggregator_prompt = aggregator_system_prompt)\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Single LoRA\nDESCRIPTION: Demonstrates generating an image using a single LoRA model (tarot card style) with a simple prompt. The generated image is then displayed using IPython's Image function.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"baby panda eating bamboo\"\n\ngenerated_image = generate_image(prompt, \n                                 lora1=\"https://huggingface.co/multimodalart/flux-tarot-v1\",\n                                 scale1=1.0\n                                 )\n\nImage(url=generated_image, width=512, height=384)\n```\n\n----------------------------------------\n\nTITLE: Loading Movie Dataset\nDESCRIPTION: Loads the movie dataset from JSON file and displays first three entries.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nwith open('./datasets/movies.json', 'r') as file:\n    movies_data = json.load(file)\n\nmovies_data[:3]\n```\n\n----------------------------------------\n\nTITLE: Calculating BERTScores for the Summary\nDESCRIPTION: Calls the chunked_BERTscores function to evaluate how well the generated summary captures the content of different chunks of the original document.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nscores = chunked_BERTscores(text, summary)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Agno Agents\nDESCRIPTION: Installs necessary Python packages including agno (agent framework), duckduckgo-search (web search), pypdf (PDF processing), lancedb and tantivy (vector storage), yfinance (financial data), and openai.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Agno/Agents_Agno.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU agno duckduckgo-search pypdf lancedb tantivy yfinance openai\n```\n\n----------------------------------------\n\nTITLE: Examining Transformed Dataset Structure\nDESCRIPTION: Displays the structure of the transformed dataset to verify that it now only contains the 'messages' column with properly formatted conversations for fine-tuning.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_messages\n```\n\n----------------------------------------\n\nTITLE: Defining Data Models and Prompts for Orchestrator Workflow\nDESCRIPTION: Sets up Pydantic models for task structure and defines the prompts for the orchestrator and worker LLMs, which control how tasks are broken down and executed.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, List\n\nORCHESTRATOR_PROMPT = \"\"\"\nAnalyze this task and break it down into 2-3 distinct approaches:\n\nTask: {task}\n\nProvide an Analysis:\n\nExplain your understanding of the task and which variations would be valuable.\nFocus on how each approach serves different aspects of the task.\n\nAlong with the analysis, provide 2-3 approaches to tackle the task, each with a brief description:\n\nPlanning for the task: Write a detailed plan to execute on the task without actually solving the task\nSolving the task: Write a technical solution for the task provided\nTests for the task: Write a potential test plan for a solution to the task provided, do not solve the task\n\nReturn only JSON output.\n\"\"\"\n\nWORKER_PROMPT = \"\"\"\nGenerate content based on:\nTask: {original_task}\nStyle: {task_type}\nGuidelines: {task_description}\n\nReturn only your response:\n[Your content here, maintaining the specified task and fully addressing requirements.]\n\"\"\"\n\ntask = \"\"\"Write a program that prints the next 20 leap years.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Token Counting Function\nDESCRIPTION: Defines a utility function that counts the number of tokens in a string using the specified tokenizer encoding from the tiktoken library.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# token counter function\nimport tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Movies\nDESCRIPTION: Uses the 'bge-base-en-v1.5' model to generate embeddings for the prepared movie data.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# We will use the bge-base-en-v1.5 model to generate embeddings for the movies\nembeddings = generate_embeddings(to_embed, 'BAAI/bge-base-en-v1.5')\n```\n\n----------------------------------------\n\nTITLE: Examining CoQA Dataset Structure\nDESCRIPTION: Displays the first few rows of the CoQA dataset to understand its structure, including stories, questions, and answers that will be used for finetuning the language model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncoqa_dataset[\"train\"].to_pandas().head()\n```\n\n----------------------------------------\n\nTITLE: Checking Similarity Score Dimensions\nDESCRIPTION: Displays the shape of the similarity scores array to confirm it contains one score for each of the 1000 movie embeddings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# We get a cosine similarity score for each one of the 1000 movie embeddings\nsimilarity_scores.shape\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for LLM Summarization and Evaluation\nDESCRIPTION: Installs necessary Python packages including Together API for accessing LLMs, bert_score for evaluation metrics, tiktoken for token counting, and visualization libraries.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together==1.3.3 # to access LLMs\n!pip install bert_score==0.3.13 # To access out evaluation metric\n!pip install tiktoken==0.8.0 # To count tokens\n!pip install numpy\n!pip install matplotlib\n```\n\n----------------------------------------\n\nTITLE: Retrieving Output Model Name\nDESCRIPTION: Gets the output model name from the finetuning response. This name will be used to refer to the finetuned model for inference tasks.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# The output model name\nft_resp.output_name\n```\n\n----------------------------------------\n\nTITLE: Initializing Together AI Client\nDESCRIPTION: Imports necessary modules and initializes the Together AI client using an API key stored in an environment variable.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\nfrom IPython.display import Image\nimport os\n\nclient = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Inspecting the Optimized Prompts\nDESCRIPTION: Views the optimized prompts generated by MIPRO to understand how it improved the agent's performance.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# view the optimized prompts\ndspy.inspect_history(n=1)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Single Movie Embedding Vector\nDESCRIPTION: Creates a visualization of the embedding vector for the first movie in the dataset, displayed as a barcode-like heatmap.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = embeddings[0]\n\ndata_2d = np.reshape(data, (1, 768))\n\nplt.figure(figsize=(15, 1))\n# Create a heatmap with a binary colormap (black and white)\nplt.imshow(data_2d, cmap='binary', interpolation='nearest', aspect='auto')\n\n# Remove axes and ticks\nplt.xlabel('Vector Dimension')\nplt.ylabel(f\"{movies_data[0]['title']}\")\n# Show the plot\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading and Viewing Movies Dataset\nDESCRIPTION: Loads the movies dataset from the JSON file and displays the first three entries to examine the data structure.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nwith open('./datasets/movies.json', 'r') as file:\n    movies_data = json.load(file)\n\nmovies_data[:3]\n```\n\n----------------------------------------\n\nTITLE: Installing Together AI Python Client\nDESCRIPTION: Installs the Together AI Python client library using pip to access their API services.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n```\n\n----------------------------------------\n\nTITLE: Displaying the JSON Schema for Knowledge Graph\nDESCRIPTION: Outputs the JSON schema representation of the KnowledgeGraph class, which will be used to guide the LLM's structured output generation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Print out the JSON Schema for our knowledge graph\nKnowledgeGraph.model_json_schema()\n```\n\n----------------------------------------\n\nTITLE: Initializing Together AI Client\nDESCRIPTION: Sets up the Together AI client using the API key for authentication and access to fine-tuning services.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom together import Together\n\nTOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\nWANDB_API_KEY = os.getenv(\"WANDB_API_KEY\") # needed for logging fine-tuning to wandb\n\nclient = Together(api_key=TOGETHER_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Verifying Embedding Dimensions\nDESCRIPTION: Converts the embeddings to a NumPy array and checks its shape to ensure 1000 vectors of 768 dimensions each were generated.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Verify that we get 1000 vectors of 768 dimensions each\nembeddings_np = np.array(embeddings)\n\nembeddings_np.shape\n```\n\n----------------------------------------\n\nTITLE: Saving Prepared CoQA Dataset to JSONL\nDESCRIPTION: Exports the transformed CoQA dataset to a JSONL file which will be used for finetuning the model through the Together AI API.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_messages.to_json(\"coqa_prepared_train.jsonl\")\n```\n\n----------------------------------------\n\nTITLE: Checking Embedding Dimensionality\nDESCRIPTION: Prints the length of the first embedding vector to verify the dimensionality (1024) of the generated embeddings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Each vector is 1024 dimensional\n\nlen(contextual_embeddings[0])\n```\n\n----------------------------------------\n\nTITLE: Installing Together AI Python Library\nDESCRIPTION: Installs the Together AI Python client library using pip. This library is required to interact with the Together AI API for finetuning and inference tasks.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU together\n```\n\n----------------------------------------\n\nTITLE: Configuring Together AI Models with DSPy\nDESCRIPTION: Sets up DSPy with Together AI's Llama models, configuring both a smaller 8B model as the main LM and a larger 70B model for optimization purposes. The code also suppresses warnings from the underlying libraries.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n#ignore warnings\nimport warnings, os\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"litellm\")  # Ignore litellm warnings\n\nTOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')\n\nllama8b = dspy.LM('together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', api_key=TOGETHER_API_KEY, api_base='https://api.together.xyz/v1', temperature=0.7)\n\nllama3_70b = dspy.LM('together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo', api_key=TOGETHER_API_KEY, api_base='https://api.together.xyz/v1', temperature=0.7)\n\ndspy.configure(lm=llama8b)\n```\n\n----------------------------------------\n\nTITLE: Previewing Similarity Scores\nDESCRIPTION: Shows the first 10 cosine similarity scores between the query and movie embeddings, with values ranging from -1 to 1.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Cosine similarity goes from -1 to 1, where 1 means the vectors are identical and -1 means they are opposite.\nsimilarity_scores[0, :10]\n```\n\n----------------------------------------\n\nTITLE: Simulating Ticket Purchase in Python\nDESCRIPTION: A placeholder function that would connect to a booking API in a real application. Currently, it simply prints the flight details and seat preference that would be used for the purchase.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def buy_tickets(flight_details: FlightDetails, seat: SeatPreference):\n    \"\"\"In a real application, this would connect to a booking API.\"\"\"\n    print(f'Purchasing flight {flight_details=!r} {seat=!r}...')\n```\n\n----------------------------------------\n\nTITLE: Checking Embedding Dimension\nDESCRIPTION: Verifies the dimensionality of the generated embeddings, which should be 1024 for the BGE large model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Each vector is 1024 dimensional\n\nlen(embeddings[0])\n```\n\n----------------------------------------\n\nTITLE: Measuring ColQwen2 Query Performance\nDESCRIPTION: Uses the timeit magic command to measure the performance of the ColQwen2 search function, providing insights into query speed.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\nmodel.search(query, k=5)\n```\n\n----------------------------------------\n\nTITLE: Testing the Together.AI Model with a Simple Query\nDESCRIPTION: Tests the connection to Together.AI by using the Llama-3.3-70B-Instruct-Turbo model to generate a response about New York City. Verifies that the model can be accessed successfully.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nMODEL = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[{\"role\": \"user\", \"content\": \"tell me a short blurb about new york\"}],\n)\nprint(response.choices[0].message.content)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries - Python Dependencies\nDESCRIPTION: Installation of necessary Python packages including pydantic and together API client\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU pydantic together\n```\n\n----------------------------------------\n\nTITLE: Querying Mistral Small Model with Together API in Python\nDESCRIPTION: This code snippet demonstrates how to use the Together API to query the Mistral Small model directly with a question. It shows the basic usage of the chat completions API.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Thinking_Augmented_Generation.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"How many r's are in the word strawberry and burberry combined?\"\n\nanswer = client.chat.completions.create(\n  model=\"mistralai/Mistral-Small-24B-Instruct-2501\",\n  messages=[{\"role\": \"user\", \n             \"content\": question}],\n)\n\n\nprint(answer.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Knowledge Graph Generation\nDESCRIPTION: Installation of the 'together' API client and 'graphviz' visualization library needed for generating and visualizing knowledge graphs.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n!pip install graphviz\n```\n\n----------------------------------------\n\nTITLE: Examining CoQA Dataset Structure\nDESCRIPTION: Displays the first few rows of the CoQA dataset to understand its structure before transformation. This helps visualize the raw data format before converting it to the required chat format.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncoqa_dataset[\"train\"].to_pandas().head()\n```\n\n----------------------------------------\n\nTITLE: Requesting Image Generation from Summary\nDESCRIPTION: Adds a new user request to generate an image based on the funny summary created earlier. This demonstrates chaining multiple tool-based operations in a conversation flow.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ngenerate_image_request = new_messages + [\n  {\n    \"role\": \"user\",\n    \"content\": \"Generate an image from the funny summary and return the full URL\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Extracting Medium Documents (24k-32k Tokens)\nDESCRIPTION: Filters the dataset to extract 2000 document examples with token counts between 24,000 and 32,000 for medium context testing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Extract 2000 examples of documents with 24k to 32k tokens\n\nlong_documents_32k = []\nfor sample in tqdm(ds_iterator.filter(lambda x: x['token_count'] > 24000 and x['token_count'] < 32000)):\n    # From 24k tokens to 32k tokens\n    if (len(long_documents_32k) < 2000):\n        document = sample['text']\n        long_documents_32k.append(document)\n    else:\n        break\n```\n\n----------------------------------------\n\nTITLE: Defining Image Generation Function with LoRA Support\nDESCRIPTION: Defines a function to generate images using the FLUX.1 model with support for up to two LoRA models. It includes type hints and a detailed docstring explaining the function's parameters and return value.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional, List\nfrom dataclasses import dataclass\n\n@dataclass\nclass LoraConfig:\n    path: str\n    scale: float\n\ndef generate_image(\n        prompt: str,\n        lora1: str,\n        scale1: float = 1.0,\n        lora2: Optional[str] = None, \n        scale2: Optional[float] = 1.0\n) -> str:\n    \n    \"\"\"\n    Generate an image using the FLUX.1 model with specified LoRA's.\n\n    Args:\n        prompt (str): The text prompt to generate the image from.\n        lora1 (str): Path to the first LoRA model.\n        scale1 (float, optional): Scale factor for the first LoRA model. Defaults to 1.0.\n        lora2 (Optional[str], optional): Path to the second LoRA model. Defaults to None.\n        scale2 (Optional[float], optional): Scale factor for the second LoRA model. Defaults to 1.0.\n\n        Currently only 2 LoRA's at a time are supported.\n\n    Returns:\n        str: URL of the generated image.\n    \"\"\"\n    \n    # Build LoRA configurations\n    lora_config: List[LoraConfig] = [LoraConfig(path=lora1, scale=scale1)]\n    \n    if lora2:\n        lora_config.append(LoraConfig(path=lora2, scale=scale2))\n\n    # Convert LoraConfig objects to dictionaries\n    image_loras = [{\"path\": lora.path, \"scale\": lora.scale} for lora in lora_config]\n\n    response = client.images.generate(\n        prompt=prompt,\n        model=\"black-forest-labs/FLUX.1-dev-lora\",\n        width=1024,\n        height=768,\n        steps=28,\n        n=1,\n        response_format=\"url\",\n        image_loras=image_loras\n    )\n\n    return response.data[0].url\n```\n\n----------------------------------------\n\nTITLE: Displaying All Queries in Python\nDESCRIPTION: This code snippet displays the `all_queries` variable, which represents the comprehensive collection of all queries executed during the research process. This includes initial queries, follow-up queries identified through gap analysis, and all retrieved content.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nall_queries\n```\n\n----------------------------------------\n\nTITLE: Together AI Platform Navigation Links\nDESCRIPTION: HTML markup for the navigation header with links to key Together AI platform resources including the main platform, documentation, blog and Discord community.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/README.md#2025-04-19_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<p align=\"center\">\n  <a href=\"https://api.together.ai/signin\" style=\"color: #06b6d4;\">Platform</a> \n  <a href=\"https://docs.together.ai/docs/introduction\" style=\"color: #06b6d4;\">Docs</a> \n  <a href=\"https://www.together.ai/blog\" style=\"color: #06b6d4;\">Blog</a> \n  <a href=\"https://discord.gg/9Rk6sSeWEG\" style=\"color: #06b6d4;\">Discord</a>\n</p>\n```\n\n----------------------------------------\n\nTITLE: Displaying a Sample Prompt\nDESCRIPTION: Prints a portion of the first prompt to examine the format and content of the generated prompts.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(prompts[0][6500:])\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for MultiModal RAG\nDESCRIPTION: Installs necessary Python libraries (byaldi, together, pdf2image) and system dependencies (poppler-utils) for the MultiModal RAG project.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install byaldi together pdf2image\n```\n\nLANGUAGE: bash\nCODE:\n```\n!sudo apt-get install -y poppler-utils\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Prompt Token Count\nDESCRIPTION: Computes the average token count across all prompts to get a representative estimate of input size.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#import python mean function\nfrom statistics import mean\n\nmean([num_tokens_from_string(prompt, \"cl100k_base\") for prompt in prompts])\n```\n\n----------------------------------------\n\nTITLE: Retrieving Reference Text for Comparison in Python\nDESCRIPTION: This code retrieves the reference completion text for a specific test item (index 20) from the GovReport dataset to compare against model predictions.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntest_items_govreport[20]['completion']\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Negative LoRA Scale\nDESCRIPTION: Illustrates the use of negative scales for LoRA fine-tunes, which can reduce the effect of the LoRA. In this example, it reduces the realism of the generated image.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a baby panda eating bamboo in the style of TOK a trtcrd tarot style\"\n\ngenerated_image = generate_image(prompt, \n                                 lora1=\"https://huggingface.co/multimodalart/flux-tarot-v1\",\n                                 scale1=1.0,\n                                 lora2=\"https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-add-details\",\n                                 scale2=-0.8\n                                 )\n\nImage(url=generated_image, width=512, height=384)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Baseline Model Prediction Example in Python\nDESCRIPTION: This code extracts a specific prediction (index 20) from the baseline model's results on the GovReport dataset for comparative analysis.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npredictions_baseline_govreport[20]\n```\n\n----------------------------------------\n\nTITLE: Saving Processed Documents to JSON\nDESCRIPTION: Writes the collected document examples to a JSON file for later use in the repetition task experiments.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Write out dataset\n\nPath(\"long_documents.json\").write_bytes(orjson.dumps({\n    \"32k\": long_documents_32k,\n    \"128k\": long_documents_128k\n}))\n```\n\n----------------------------------------\n\nTITLE: Loading and Previewing Movie Dataset\nDESCRIPTION: Loads the JSON movie dataset from the local file and displays the first three entries to preview the structure.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nwith open('./datasets/movies.json', 'r') as file:\n    movies_data = json.load(file)\n\nmovies_data[:3]\n```\n\n----------------------------------------\n\nTITLE: Displaying LLM Response\nDESCRIPTION: Shows the model's response to the repetition task for comparison with the ground truth.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresult\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying HelpSteer2-DPO Dataset\nDESCRIPTION: Downloads the HelpSteer2-DPO dataset from Hugging Face and displays basic information about its structure and available splits.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\n# Download the HelpSteer2-DPO dataset from Hugging Face\ndataset = load_dataset(\"Atsunori/HelpSteer2-DPO\")\n\n# Display basic information about the dataset\nprint(f\"Dataset structure: {dataset}\")\nprint(f\"Available splits: {dataset.keys()}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries and Modules\nDESCRIPTION: Imports the necessary Python modules for API interaction, progress tracking, file handling, and data processing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\n\nimport numpy as np\nimport orjson\nimport json\nimport os\n```\n\n----------------------------------------\n\nTITLE: Displaying Tool Response Details\nDESCRIPTION: Shows the detailed structure of the messages returned by the tool execution, including roles, tool call IDs, and content. This helps understand how tool responses are formatted in the conversation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor message in tool_run:\n  print(f\"Role: {message['role']}\")\n  if 'tool_calls' in message:\n    print(f\"Tool Calls: {message['tool_calls']}\")\n  if 'tool_call_id' in message:\n    print(f\"Tool Call ID: {message['tool_call_id']}\")\n    print(f\"Content: {message['content']}\")\n    print('\\n')\n```\n\n----------------------------------------\n\nTITLE: Setting Together AI API Key\nDESCRIPTION: Sets the Together AI API key from an environment variable for authentication in subsequent API calls.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for RAG Implementation\nDESCRIPTION: Installs the necessary Python libraries for the RAG system including Together AI client, BeautifulSoup for HTML parsing, and NumPy for vector operations.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU together beautifulsoup4 numpy\n```\n\n----------------------------------------\n\nTITLE: Importing Levenshtein Ratio for Evaluation\nDESCRIPTION: Imports the Levenshtein ratio function to measure string similarity between model outputs and ground truth completions.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom Levenshtein import ratio\n```\n\n----------------------------------------\n\nTITLE: Preparing Movie Data for Embedding\nDESCRIPTION: Concatenates the title, overview, and tagline of each movie to create more informative text for embedding. Processes the first 1000 movies from the dataset.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Concatenate the title, overview, and tagline of each movie\n# this makes the text that will be embedded for each movie more informative\n# as a result the embeddings will be richer and capture this information. \nto_embed = []\nfor movie in movies_data[:1000]:\n    text = ''\n    for field in ['title', 'overview', 'tagline']:\n        value = movie.get(field, '')\n        text += str(value) + ' '\n    to_embed.append(text.strip())\n\nto_embed[:10]\n```\n\n----------------------------------------\n\nTITLE: Updating Conversation History with Summary\nDESCRIPTION: Adds the LLM's summary response to the conversation history, preparing for the next step where the model will generate an image based on the funny summary.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnew_messages = messages + [\n  {\n    \"role\": \"assistant\",\n    \"content\": summary_response.choices[0].message.content,\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together AI API Key\nDESCRIPTION: Imports the Together AI library and retrieves the API key from environment variables.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport together, os\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Checking Email Send Execution Response\nDESCRIPTION: This simple snippet prints the execution response to confirm that the email was sent successfully. It displays the result of the email sending operation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# We can see the email was sent successfully!\nexec_response\n```\n\n----------------------------------------\n\nTITLE: Accessing BERTScore Component in Python\nDESCRIPTION: This snippet extracts a specific score from the BERTScores results. It references the third element from the 'scores_irrelevant' array, indicating a multi-metric output from BERTScore.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\nscores_irrelevant[2]\n\"\"\"\n\"\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple LoRA Adapters for Model Swapping\nDESCRIPTION: Creates a list of different LoRA adapter models that can be used interchangeably. This allows for comparing different finetuned versions of the same base model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# List of LoRA fine-tunes\n\nLoRA_adapters = [\"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-my-demo-finetune-4224205a\",\n                 \"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-30b975fd\",\n                 \"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-f9ef93c8\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together AI API\nDESCRIPTION: Imports the Together API and sets up the API key from environment variables to enable access to the LLM models for context generation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport together, os\nfrom together import Together\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Long Context Fine-tuning\nDESCRIPTION: Installs the necessary Python packages for working with the Together API, measuring string similarities, and handling datasets.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q together==1.3.4 python-Levenshtein==0.26.1 tqdm numpy orjson datasets\n```\n\n----------------------------------------\n\nTITLE: Verifying Task Items Count\nDESCRIPTION: Checks that we have the expected number of task items loaded for our experiments.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Verify that we have all 2000 task items\n\nlen(task_items)\n```\n\n----------------------------------------\n\nTITLE: Checking Token Count of the Full Text\nDESCRIPTION: Counts the total number of tokens in the downloaded legislative text using the cl100k_base encoding to determine if it fits within LLM context limits.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# We'll assume we're using the \"cl100k_base\" encoding\n\nprint(f\"Total number of tokens in full text: {num_tokens(text, 'cl100k_base')}\")\n```\n\n----------------------------------------\n\nTITLE: Examining Expected Completion\nDESCRIPTION: Displays the ground truth completion (the last N words) that the model should produce for the given task.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Correct the prompt for the task item - ground truth\n\nitem['completion']\n```\n\n----------------------------------------\n\nTITLE: Research Report Export to HTML\nDESCRIPTION: Converts the generated research answer to HTML format and saves it to a file for sharing and presentation purposes.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport markdown\n\n# Convert to HTML\nreport_html = markdown.markdown(research_answer)\n\nfilename=\"research_report.html\"\nwith open(filename, \"w\", encoding=\"utf-8\") as f:\n    f.write(report_html)\n\nprint(f\"Research report saved as {filename}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Initializing Together API Client\nDESCRIPTION: Imports necessary libraries for the orchestrator workflow and initializes the Together API client for both synchronous and asynchronous calls.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import libraries\nimport json\nimport asyncio\nimport together\nfrom together import AsyncTogether, Together\n\nfrom typing import Any, Optional, Dict, List, Literal\nfrom pydantic import Field, BaseModel, ValidationError\n\nTOGETHER_API_KEY = \"--Your API Key--\"\n\nclient = Together(api_key= TOGETHER_API_KEY)\nasync_client = AsyncTogether(api_key= TOGETHER_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Installing Together AI Library\nDESCRIPTION: Installation of the Together AI Python library using pip.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n```\n\n----------------------------------------\n\nTITLE: Initializing Together API Client in Python\nDESCRIPTION: This snippet sets up the Together API client using an API key. It's a prerequisite for making API calls to Together's models.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Thinking_Augmented_Generation.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\nclient = Together(api_key = \"---\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample from HelpSteer2-DPO Dataset\nDESCRIPTION: Prints a sample from the training split of the HelpSteer2-DPO dataset, showing the prompt, chosen response, and rejected response.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n_id=210\nprint(\"\\nSample from training split:\")\nprint(f\"Prompt:\\n{dataset['train'][_id]['prompt']}\\n\")\nprint(\"Chosen Response vs Rejected Response:\")\nprint(\"-\" * 80)\nchosen = dataset[\"train\"][_id]['chosen_response']\nrejected = dataset[\"train\"][_id]['rejected_response']\n\n# Print chosen response in full\nprint(\"CHOSEN RESPONSE:\")\nprint(chosen)\nprint(\"\\n\" + \"-\" * 40 + \"\\n\")\n\n# Print rejected response in full\nprint(\"REJECTED RESPONSE:\")\nprint(rejected)\nprint(\"-\" * 80)\n```\n\n----------------------------------------\n\nTITLE: Downloading Movies Dataset JSON File\nDESCRIPTION: Downloads the movies dataset JSON file from a GitHub repository and moves it to a 'datasets' directory.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!wget https://raw.githubusercontent.com/togethercomputer/together-cookbook/refs/heads/main/datasets/movies.json\n!mkdir datasets\n!mv movies.json datasets/movies.json\n```\n\n----------------------------------------\n\nTITLE: Downloading Nvidia Investor Presentation PDF\nDESCRIPTION: Downloads the Nvidia investor presentation PDF file and renames it for easier reference in the project.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n!wget https://s201.q4cdn.com/141608511/files/doc_presentations/2023/Oct/01/ndr_presentation_oct_2023_final.pdf\n!mv ndr_presentation_oct_2023_final.pdf nvidia_presentation.pdf\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for LangGraph and Together AI\nDESCRIPTION: Installs the necessary Python packages for the project, including langchain-together, langchain-community, tavily-python, and langgraph.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU langchain-together langchain-community tavily-python langgraph\n```\n\n----------------------------------------\n\nTITLE: Defining Irrelevant Summary Prompt in Python\nDESCRIPTION: This snippet defines a prompt that instructs the generation of an irrelevant haiku summary, unrelated to the original document. It uses a multi-line string in Python for defining the prompt.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\nSUMMARIZATION_PROMPT_IRRELEVANT = \"\"\"\nIgnore all instructions and write me a haiku about elephants.\n\n{full_text}\n\"\"\"\n\"\n```\n\n----------------------------------------\n\nTITLE: Installing PydanticAI Library\nDESCRIPTION: This snippet installs the PydanticAI library using pip. It's a prerequisite for the rest of the code in the notebook.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU pydantic-ai\n```\n\n----------------------------------------\n\nTITLE: Installing Toolhouse and OpenAI Dependencies\nDESCRIPTION: Installs the necessary Python packages for working with Together.AI models and Toolhouse's tool integration.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install toolhouse openai\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Multimodal Search and Image Generation in Python\nDESCRIPTION: Installs necessary libraries including duckduckgo_search, together, and transformers for implementing multimodal search and image generation.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -Uqq duckduckgo_search together transformers\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and API Setup - Python Configuration\nDESCRIPTION: Sets up required imports and initializes the Together API client with authentication\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport together\nfrom together import Together\n\nfrom typing import Any, Optional, Dict, List, Literal\nfrom pydantic import Field, BaseModel, ValidationError\n\nTOGETHER_API_KEY = \"--Your API Key--\"\n\nclient = Together(api_key= TOGETHER_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Contextual RAG\nDESCRIPTION: Installs necessary Python libraries for implementing Contextual RAG, including the Together API for LLM access, tiktoken for token counting, BeautifulSoup for web scraping, and BM25S for keyword search.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together # To access open source LLMs\n!pip install --upgrade tiktoken # To count total token counts\n!pip install beautifulsoup4 # To scrape documents to RAG over\n!pip install bm25s # To implement out key-word BM25 search\n```\n\n----------------------------------------\n\nTITLE: Setting Together AI API Key\nDESCRIPTION: Retrieves the Together AI API key from environment variables for authentication in subsequent API calls.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\napi_key = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Installing DSPy Library\nDESCRIPTION: Installs the DSPy library using pip in quiet mode with upgrade option.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU dspy\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries for Dataset Manipulation and Visualization\nDESCRIPTION: Imports required Python libraries for loading datasets, JSON manipulation, counting, and creating visualizations.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nimport json\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Loading and Viewing Movies Dataset\nDESCRIPTION: Loads the movies dataset from the JSON file into Python and displays a sample of the data. This provides a glimpse of the movie records that will be used for semantic search.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nwith open('./datasets/movies.json', 'r') as file:\n    movies_data = json.load(file)\n\nmovies_data[10:13]\n```\n\n----------------------------------------\n\nTITLE: Installing Together API Library\nDESCRIPTION: Installs the Together AI Python library using pip to allow API access to Together AI models.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for CoQA Dataset Finetuning\nDESCRIPTION: Installs the necessary libraries for dataset handling, model transformation, and the Together AI client for finetuning language models on conversational data.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q datasets==3.1.0 transformers together==1.3.4\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Together AI and Composio Integration\nDESCRIPTION: This snippet installs the necessary packages for integrating Together AI with Composio. It uses pip to install the 'together' and 'composio-togetherai' packages quietly and with an upgrade flag.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# install the required packages\n!pip install -qU together composio-togetherai\n```\n\n----------------------------------------\n\nTITLE: Creating Color-Coded Text Visualization of BERTScores\nDESCRIPTION: Defines a function to visualize how well each chunk of the original document is represented in the summary by color-coding the text based on BERTScore values.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import HTML, display\nfrom matplotlib import colormaps as cm\nfrom matplotlib.colors import to_hex\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef print_text_in_colors(reference, weights, colormap=\"RdYlGn\"):\n     \"\"\"\n     Helper function to print highlighted text using html.\n     Inputs\n     --------\n     tokens: list of strings\n     weights: list of floats of the same length\n     \"\"\"\n     reference = [reference[i - 250 : i] for i in range(250,len(reference),250)]\n\n     colormap = cm.get_cmap(colormap)\n     colors = [to_hex(colormap(w)) for w in weights]\n     joined_html_text = ' '.join([f'<text style=\"background-color:{color};\">{reference}</text>' for reference, color in zip(reference, colors)])\n     display(HTML(joined_html_text));\n```\n\n----------------------------------------\n\nTITLE: Downloading Movies Dataset\nDESCRIPTION: Downloads a JSON movie dataset from GitHub repo, creates a datasets directory, and moves the file into it.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Let's get the movies dataset\n!wget https://raw.githubusercontent.com/togethercomputer/together-cookbook/refs/heads/main/datasets/movies.json\n!mkdir datasets\n!mv movies.json datasets/movies.json\n```\n\n----------------------------------------\n\nTITLE: Installing Together AI Python Library\nDESCRIPTION: Installs the Together AI Python library using pip in a quiet mode with an upgrade flag.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU together\n```\n\n----------------------------------------\n\nTITLE: Checking Similarity Score Dimensions\nDESCRIPTION: Verifies the shape of the similarity scores array to confirm that a score was calculated for each movie in the dataset. Shows the result is a 1x1000 array.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# We get a similarity score for each of our 1000 movies - the higher the score, the more similar the movie is to the query\nsimilarity_scores.shape\n```\n\n----------------------------------------\n\nTITLE: Research Answer Generation with Together AI\nDESCRIPTION: Generates comprehensive research answers from filtered search results using Together AI's answer model. Includes functionality to clean up thinking tags and handle error cases.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nasync def generate_research_answer(topic: str, results: SearchResults, together_client: AsyncTogether, answer_model: str, prompts: dict, max_tokens: int, remove_thinking_tags: bool = True) -> str:\n    \"\"\"\n    Generate a comprehensive answer to the research topic based on the search results.\n\n    Args:\n        topic: The research topic\n        results: Filtered search results to use for answer generation\n        together_client: The Together AI client for LLM operations\n        answer_model: Model to use for answer generation\n        prompts: Dictionary of prompt templates\n        max_tokens: Maximum number of tokens in the answer\n        remove_thinking_tags: Whether to remove <think> tags from the answer\n\n    Returns:\n        Detailed research answer as a string\n    \"\"\"\n    formatted_results = str(results)\n\n    ANSWER_PROMPT = prompts[\"answer_prompt\"]\n\n    answer_response = await together_client.chat.completions.create(\n        model=answer_model,\n        messages=[\n            {\"role\": \"system\", \"content\": ANSWER_PROMPT},\n            {\"role\": \"user\", \"content\": f\"Research Topic: {topic}\\n\\nSearch Results:\\n{formatted_results}\"}\n        ],\n        max_tokens=max_tokens\n    )\n\n    answer = answer_response.choices[0].message.content\n\n    # Remove <think> tokens for reasoning models\n    if remove_thinking_tags:\n        answer = remove_thinking_tags_from_answer(answer)\n\n    # Handle potential error cases\n    if answer is None or not isinstance(answer, str):\n        print(\"ERROR: No answer generated\")\n        return \"No answer generated\"\n\n    return answer.strip()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Orchestrator Agent Workflow\nDESCRIPTION: Installs the necessary Python libraries (pydantic and together) for implementing the orchestrator subtask agent workflow.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install libraries\n!pip install -qU pydantic together\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Summarization Fine-tuning in Python\nDESCRIPTION: This code snippet installs the necessary Python libraries for the summarization fine-tuning project, including together, rouge_score, evaluate, orjson, seaborn, matplotlib, numpy, and tqdm.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q together==1.3.4 rouge_score==0.1.2 evaluate orjson seaborn matplotlib numpy tqdm\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for Together AI Fine-tuning\nDESCRIPTION: Installs the necessary Python packages for fine-tuning with Together AI, including the official API client, dataset handling tools, and utilities.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU together datasets transformers tqdm\n```\n\n----------------------------------------\n\nTITLE: Downloading Movies Dataset\nDESCRIPTION: Uses wget to download a movies dataset in JSON format from GitHub and moves it to a datasets folder. This dataset will be used for demonstrating semantic search.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Let's get the movies dataset\n!wget https://raw.githubusercontent.com/togethercomputer/together-cookbook/refs/heads/main/datasets/movies.json\n!mkdir datasets\n!mv movies.json datasets/movies.json\n```\n\n----------------------------------------\n\nTITLE: Stopping Deployed Applications Remotely in Python\nDESCRIPTION: This commented Python snippet provides information on stopping the deployed FastAPI and Gradio applications remotely with Union's AppRemote service. By calling 'app_remote.stop()', an application can be shut down by its name, aiding in resource management and stopping operations when they are no longer needed. It assumes prior knowledge of app deployment and the names given during setup.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# If you want to stop the apps, heres how you can do it:\n# app_remote.stop(name=\"contextual-rag-fastapi-app\")\n# app_remote.stop(name=\"contextual-rag-gradio-app\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from Receipt Image with Vision Model\nDESCRIPTION: Uses Llama 3.2 90B Vision model to extract line item details from a receipt image through the Together API. The prompt asks for name, price, quantity, and total from the image.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\ngetDescriptionPrompt = \"Extract out the details from each line item on the receipt image. Identify the name, price and quantity of each item. Also specify the total.\"\n\nimageUrl = \"https://ocr.space/Content/Images/receipt-ocr-original.webp\"\n\nclient = Together(api_key=TOGETHER_API_KEY)\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": getDescriptionPrompt},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": imageUrl,\n                    },\n                },\n            ],\n        }\n    ],\n)\n\ninfo = response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Computing ROUGE-L Scores for Text Summarization in Python\nDESCRIPTION: This function calculates ROUGE-L scores between model predictions and reference completions. It takes lists of prediction strings and test items containing reference texts, computes their ROUGE-L scores, and returns results scaled by 100.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef compute_rouge_l(predictions, test_items):\n    \"\"\"\n    Computes ROUGE-L scores for a list of predictions against a list of test items.\n    Args:\n        predictions (list of str): A list of predicted text strings.\n        test_items (list of dict): A list of dictionaries, where each dictionary contains a key 'completion' \n                                   with the reference text string.\n    Returns:\n        list of float: A list of ROUGE-L scores, one for each prediction, scaled by 100.\n    \"\"\"\n    scores = rouge.compute(predictions=predictions, references=[it['completion'] for it in test_items],\n                           rouge_types=['rougeL'], use_aggregator=False)['rougeL']\n    scores = np.array(scores) * 100\n    return scores\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-generated Summaries for Evaluation in Python\nDESCRIPTION: This code loads pre-generated summaries from JSON files for both baseline and fine-tuned models on synthetic and GovReport datasets. It simplifies the comparison process by providing ready-to-use prediction data.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictions = orjson.loads(Path(\"datasets/predictions.json\").read_bytes())\n\npredictions_baseline_syn = predictions['predictions_baseline_syn']\npredictions_baseline_govreport = predictions['predictions_baseline_govreport']\n\npredictions_ft = orjson.loads(Path(\"datasets/predictions_ft.json\").read_bytes())\n\npredictions_ft_syn = predictions_ft['predictions_ft_syn']\npredictions_ft_govreport = predictions_ft['predictions_ft_govreport']\n\ntest_items = orjson.loads(Path(\"datasets/synth_summarization_test.json\").read_bytes())\n\ntest_items_govreport = [orjson.loads(line) for line in Path(\"datasets/govreport_test.jsonl\").read_text().split(\"\\n\")]\ntest_items_govreport = test_items_govreport[:100]\n\nBASELINE_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Search Execution in Python\nDESCRIPTION: This function executes multiple search queries concurrently using asyncio. It creates parallel search tasks, gathers all results, combines them into a single collection, and removes duplicate entries for efficient research processing.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def perform_search(queries: List[str], tavily_client: AsyncTavilyClient, prompts: dict, together_client: AsyncTogether, summary_model: str) -> SearchResults:\n    \"\"\"Execute searches for all queries in parallel\"\"\"\n    tasks = [tavily_search(query, tavily_client, prompts, together_client, summary_model) for query in queries]\n    results_list = await asyncio.gather(*tasks)\n\n    combined_results = SearchResults([])\n    for results in results_list:\n        combined_results = combined_results + results\n\n    combined_results_dedup=combined_results.dedup()\n    print(f\"Search complete, found {len(combined_results_dedup.results)} results after deduplication\")\n    return combined_results_dedup\n```\n\n----------------------------------------\n\nTITLE: Reranking with Llama Rank\nDESCRIPTION: Uses the Salesforce Llama Rank V1 model through Together AI to rerank the top 25 movies. The reranker evaluates each movie's relevance to the query and returns the top 5 most relevant results.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\nclient = Together(api_key = TOGETHER_API_KEY)\n\nquery = \"super hero mystery action movie about bats\" # we keep the same query - can change if we want\n\nresponse = client.rerank.create(\n  model=\"Salesforce/Llama-Rank-V1\",\n  query=query,\n  documents=top_25_sorted_titles,\n  top_n=5 # we only want the top 5 results\n)\n\nfor result in response.results:\n    print(f\"Document Index: {result.index}\")\n    print(f\"Document: {top_25_sorted_titles[result.index]}\")\n    print(f\"Relevance Score: {result.relevance_score}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Embedding Generation Function\nDESCRIPTION: Defines a function to generate embeddings using the Together AI API. It takes a list of input texts and a model API string, and returns a list of embeddings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport together\nimport numpy as np\n\ndef generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n    \"\"\"Generate embeddings from Together python library.\n\n    Args:\n        input_texts: a list of string input texts.\n        model_api_string: str. An API string for a specific embedding model of your choice.\n\n    Returns:\n        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n    \"\"\"\n    together_client = together.Together(api_key = TOGETHER_API_KEY)\n    outputs = together_client.embeddings.create(\n        input=input_texts,\n        model=model_api_string,\n    )\n    return [x.embedding for x in outputs.data]\n```\n\n----------------------------------------\n\nTITLE: Preparing Movie Texts for Embedding\nDESCRIPTION: Concatenates the title, overview, and tagline of each movie to create a comprehensive text representation for embedding. This enriches the semantic information captured in the embeddings.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Concatenate the title, overview, and tagline of each movie\n# this makes the text that will be embedded for each movie more informative\n# as a result the embeddings will be richer and capture this information.\nto_embed = []\nfor movie in movies_data[:1000]:\n    text = ''\n    for field in ['title', 'overview', 'tagline']:\n        value = movie.get(field, '')\n        text += str(value) + ' '\n    to_embed.append(text.strip())\n\nto_embed[:10]\n```\n\n----------------------------------------\n\nTITLE: Calculating Performance Improvements After Fine-tuning in Python\nDESCRIPTION: This code computes the difference in ROUGE-L scores between fine-tuned and baseline models on the GovReport dataset. The resulting array shows the improvement (or degradation) in performance for each test item after fine-tuning.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nscores1 = compute_rouge_l(predictions_baseline_govreport, test_items_govreport)\nscores2 = compute_rouge_l(predictions_ft_govreport, test_items_govreport)\nscore_differences = np.array(scores2) - np.array(scores1)\n```\n\n----------------------------------------\n\nTITLE: Optimizing the Agent with MIPRO\nDESCRIPTION: Implements DSPy's MIPRO optimizer to improve agent performance by using the more powerful 70B model as a teacher to guide optimization of the 8B model.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nllama3_70b = dspy.LM('together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo', api_key=TOGETHER_API_KEY, api_base='https://api.together.xyz/v1', temperature=0.7)\n\nkwargs = dict(teacher_settings=dict(lm=llama3_70b), prompt_model=llama3_70b, max_errors=999)\n\ntp = dspy.MIPROv2(metric=top5_recall, auto=\"medium\", num_threads=16, **kwargs)\n\noptimized_react = tp.compile(react, trainset=trainset, max_bootstrapped_demos=3, max_labeled_demos=0)\n```\n\n----------------------------------------\n\nTITLE: Displaying Search Results in Python\nDESCRIPTION: This code prints the first search result including its title, link, and summarized content. It demonstrates how to access and display the filtered information extracted by the system for the specified research topic.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Title: {search_results.results[0].title}\\n\\nLink: {search_results.results[0].link}\\n\\nContent: {search_results.results[0].filtered_raw_content[:1000]}[...]\")\n```\n\n----------------------------------------\n\nTITLE: Generating Query Embedding\nDESCRIPTION: Creates an embedding for a search query about superhero bat movies using the same BGE model. This embedding will be compared to the movie embeddings for similarity matching.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Generate the vector embeddings for the query\nquery = \"super hero mystery action movie about bats\"\n\nquery_embedding = generate_embeddings([query], 'BAAI/bge-base-en-v1.5')[0]\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity\nDESCRIPTION: Computes cosine similarity between the query embedding and all movie embeddings. This measures how semantically similar each movie is to the search query.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Calculate cosine similarity between the query embedding and each movie embedding\nsimilarity_scores = cosine_similarity([query_embedding], embeddings)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together AI API Key\nDESCRIPTION: Imports the Together AI library and sets up the API key either from an environment variable or by direct assignment. This is required for authenticating with the Together AI API.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport together, os\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading the Optimized Agent\nDESCRIPTION: Saves the optimized agent to a JSON file and demonstrates how to load it back for future use.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\noptimized_react.save(\"optimized_react.json\")\n\nloaded_react = dspy.ReAct(\"claim -> titles: list[str]\", tools=[search_wikipedia, lookup_wikipedia], max_iters=20)\nloaded_react.load(\"optimized_react.json\")\n\nloaded_react(claim=\"The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Kolts are both playwrights.\").titles\n```\n\n----------------------------------------\n\nTITLE: Visualizing Embeddings with UMAP\nDESCRIPTION: Uses UMAP to reduce the 768-dimensional embeddings to 2 dimensions and creates a scatter plot colored by movie genre.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport umap, umap.plot\nimport matplotlib.pyplot as plt\n\n# UMAP with cosine metric - this is because the embeddings model is trained using cosine similarity\nmapper = umap.UMAP(n_neighbors=5, min_dist=0.0, metric='cosine', random_state=42).fit(embeddings)\n\n# Plot the 2d UMAP embeddings and color by genre\numap.plot.points(mapper, labels = np.array(genres), theme='fire');\n```\n\n----------------------------------------\n\nTITLE: Checking Embedding Dimensions\nDESCRIPTION: Confirms the dimensionality of the embeddings generated by the BGE model, which is 768 dimensions. This verifies that the embeddings were generated correctly.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# bge-base-en-v1.5 model generates 768-dimensional embeddings\nlen(embeddings[0])\n```\n\n----------------------------------------\n\nTITLE: Evaluator Implementation\nDESCRIPTION: Implementation of the Evaluator component that assesses solutions against defined criteria.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(task : str, evaluator_prompt : str, generated_content: str, schema) -> tuple[str, str]:\n    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n    full_prompt = f\"{evaluator_prompt}\\nOriginal task: {task}\\nContent to evaluate: {generated_content}\"\n    \n    response = JSON_llm(full_prompt, schema)\n    \n    evaluation = response[\"evaluation\"]\n    feedback = response[\"feedback\"]\n\n    print(\"=== EVALUATION START ===\")\n    print(f\"Status: {evaluation}\")\n    print(f\"Feedback: {feedback}\")\n    print(\"=== EVALUATION END ===\\n\")\n\n    return evaluation, feedback\n```\n\n----------------------------------------\n\nTITLE: Sorting Movies by Similarity\nDESCRIPTION: Sorts the movies based on their similarity to the query in descending order. Returns the indices of movies from most to least similar.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Sort the similarity scores in descending order, obtain the index of the movies\nindices = np.argsort(-similarity_scores)\n\nindices[0, :50]\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Baseline Agent Performance\nDESCRIPTION: Evaluates the basic ReAct agent with error handling to establish a baseline performance metric before optimization.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"litellm\")  # Ignore litellm warnings\n\ndef safe_react(claim: str):\n    try:\n        return react(claim=claim)\n    except Exception as e:\n        return dspy.Prediction(titles=[])\n\nevaluate(safe_react)\n```\n\n----------------------------------------\n\nTITLE: Viewing Sample Similarity Scores\nDESCRIPTION: Displays a subset of the similarity scores to inspect how strongly different movies match the query. Higher scores indicate greater similarity to the query.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsimilarity_scores[0, :50]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Movie Embedding Visualization\nDESCRIPTION: Installs the 'together' library for API access and 'umap-learn' for dimensionality reduction and plotting.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n!pip install umap-learn[plot]\n```\n\n----------------------------------------\n\nTITLE: Testing the ReAct Agent on a Sample Claim\nDESCRIPTION: Tests the configured ReAct agent with a sample claim about David Gregory to see which Wikipedia titles it returns as relevant.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# try our agent out\nreact(claim=\"David Gregory was born in 1625.\").titles[:3]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries\nDESCRIPTION: Installation of necessary Python packages pydantic and together using pip.\nSOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU pydantic together\n```"
  }
]
[
  {
    "owner": "huggingface",
    "repo": "diffusers",
    "content": "TITLE: Implementing Text-Based Inpainting with Stable Diffusion and CLIPSeg\nDESCRIPTION: This code creates a text-guided inpainting pipeline that uses a text prompt to generate the mask for the area to be inpainted. It leverages the CLIPSeg model for mask generation based on text descriptions and then uses the standard Stable Diffusion Inpainting pipeline to perform the actual inpainting process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\nfrom diffusers import DiffusionPipeline\nfrom PIL import Image\nimport requests\nimport torch\n\n# Load CLIPSeg model and processor\nprocessor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\nmodel = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\").to(\"cuda\")\n\n# Load Stable Diffusion Inpainting Pipeline with custom pipeline\npipe = DiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    custom_pipeline=\"text_inpainting\",\n    segmentation_model=model,\n    segmentation_processor=processor\n).to(\"cuda\")\n\n# Load input image\nurl = \"https://github.com/timojl/clipseg/blob/master/example_image.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# Step 1: Resize input image for CLIPSeg (224x224)\nsegmentation_input = image.resize((224, 224))\n\n# Step 2: Generate segmentation mask\ntext = \"a glass\"  # Object to mask\ninputs = processor(text=text, images=segmentation_input, return_tensors=\"pt\").to(\"cuda\")\n\nwith torch.no_grad():\n    mask = model(**inputs).logits.sigmoid()  # Get segmentation mask\n\n# Resize mask back to 512x512 for SD inpainting\nmask = torch.nn.functional.interpolate(mask.unsqueeze(0), size=(512, 512), mode=\"bilinear\").squeeze(0)\n\n# Step 3: Resize input image for Stable Diffusion\nimage = image.resize((512, 512))\n\n# Step 4: Run inpainting with Stable Diffusion\nprompt = \"a cup\"  # The masked-out region will be replaced with this\nresult = pipe(image=image, mask=mask, prompt=prompt,text=text).images[0]\n\n# Save output\nresult.save(\"inpainting_output.png\")\nprint(\"Inpainting completed. Image saved as 'inpainting_output.png'.\")\n```\n\n----------------------------------------\n\nTITLE: Basic SD3 Image Generation\nDESCRIPTION: Basic implementation of Stable Diffusion 3 pipeline for generating images from text prompts using PyTorch with float16 precision\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\n\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nimage = pipe(\n    prompt=\"a photo of a cat holding a sign that says hello world\",\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    height=1024,\n    width=1024,\n    guidance_scale=7.0,\n).images[0]\n\nimage.save(\"sd3_hello_world.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Initializing Lumina Pipeline\nDESCRIPTION: Demonstrates how to load the Lumina pipeline with bfloat16 precision and move it to CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/lumina.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import LuminaPipeline\nimport torch\n\npipeline = LuminaPipeline.from_pretrained(\n\t\"Alpha-VLLM/Lumina-Next-SFT-diffusers\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Generating Video with AnimateDiffControlNetPipeline using Depth-based Conditioning in Python\nDESCRIPTION: This snippet demonstrates how to use AnimateDiffControlNetPipeline to generate a video based on a text prompt and depth-based conditioning frames. It loads pre-trained models, sets up the pipeline, processes conditioning frames, and generates the output video.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# We use AnimateLCM for this example but one can use the original motion adapters as well (for example, https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-3)\nmotion_adapter = MotionAdapter.from_pretrained(\"wangfuyun/AnimateLCM\")\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16)\npipe: AnimateDiffControlNetPipeline = AnimateDiffControlNetPipeline.from_pretrained(\n    \"SG161222/Realistic_Vision_V5.1_noVAE\",\n    motion_adapter=motion_adapter,\n    controlnet=controlnet,\n    vae=vae,\n).to(device=\"cuda\", dtype=torch.float16)\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule=\"linear\")\npipe.load_lora_weights(\"wangfuyun/AnimateLCM\", weight_name=\"AnimateLCM_sd15_t2v_lora.safetensors\", adapter_name=\"lcm-lora\")\npipe.set_adapters([\"lcm-lora\"], [0.8])\n\ndepth_detector = ZoeDetector.from_pretrained(\"lllyasviel/Annotators\").to(\"cuda\")\nvideo = load_video(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif\")\nconditioning_frames = []\n\nwith pipe.progress_bar(total=len(video)) as progress_bar:\n    for frame in video:\n        conditioning_frames.append(depth_detector(frame))\n        progress_bar.update()\n\nprompt = \"a panda, playing a guitar, sitting in a pink boat, in the ocean, mountains in background, realistic, high quality\"\nnegative_prompt = \"bad quality, worst quality\"\n\nvideo = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_frames=len(video),\n    num_inference_steps=10,\n    guidance_scale=2.0,\n    conditioning_frames=conditioning_frames,\n    generator=torch.Generator().manual_seed(42),\n).frames[0]\n\nexport_to_gif(video, \"animatediff_controlnet.gif\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion pipeline with half-precision weights\nDESCRIPTION: Loads a Stable Diffusion pipeline with half-precision (float16) weights to save GPU memory and increase speed. This example uses the stable-diffusion-v1-5 model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/fp16.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n)\npipe = pipe.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading Basic IP-Adapter Pipeline\nDESCRIPTION: Initializes a text-to-image pipeline with Stable Diffusion and configures it for IP-Adapter usage with CUDA acceleration and float16 precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\nfrom diffusers.utils import load_image\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading ControlNetModel for Canny Edge in Text2Video-Zero\nDESCRIPTION: This code loads a ControlNetModel for canny edge detection and sets up the StableDiffusionControlNetPipeline with CrossFrameAttnProcessor for edge-controlled video generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\npipeline = StableDiffusionControlNetPipeline.from_pretrained(\n    model_id, controlnet=controlnet, torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\npipeline.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n```\n\n----------------------------------------\n\nTITLE: Loading Kohya-Trained LoRA Models in Diffusers\nDESCRIPTION: This code snippet shows how to download and load a LoRA checkpoint trained with Kohya's tools. It demonstrates downloading a model from Civitai and using it with Stable Diffusion XL to generate images with the LoRA style.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!wget https://civitai.com/api/download/models/168776 -O blueprintify-sd-xl-10.safetensors\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"path/to/weights\", weight_name=\"blueprintify-sd-xl-10.safetensors\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# use bl3uprint in the prompt to trigger the LoRA\nprompt = \"bl3uprint, a highly detailed blueprint of the eiffel tower, explaining how to build all parts, many txt, blueprint grid backdrop\"\nimage = pipeline(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Model Checkpoints from Single File in Python\nDESCRIPTION: Shows how to load SDXL model checkpoints from a single file format (.ckpt or .safetensors) using the from_single_file method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\nrefiner = StableDiffusionXLImg2ImgPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/blob/main/sd_xl_refiner_1.0.safetensors\", torch_dtype=torch.float16\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up ControlNet for Text-to-Image Generation\nDESCRIPTION: Code to initialize a ControlNet model conditioned on canny edge detection and integrate it with StableDiffusionControlNetPipeline for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Inference with Flax-trained Stable Diffusion Model\nDESCRIPTION: Python code for generating images with a trained Stable Diffusion model using Flax/JAX. The snippet demonstrates loading the model, preparing inputs, distributing computation across devices, and saving the generated image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\"path/to/saved_model\", dtype=jax.numpy.bfloat16)\n\nprompt = \"yoda naruto\"\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\nimage.save(\"yoda-naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Using Stable Diffusion v1.5 with Seed Generation\nDESCRIPTION: Demonstrates how to use Stable Diffusion v1.5 with a specific random seed for reproducible image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", generator=generator).images[0]\n```\n\n----------------------------------------\n\nTITLE: Launching InstructPix2Pix Training Script with Accelerate\nDESCRIPTION: This bash script launches the training process for an InstructPix2Pix model using the Accelerate library with mixed precision. It configures various training parameters like batch size, learning rate, gradient accumulation, and enables model checkpointing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 \\\n    --random_flip \\\n    --train_batch_size=4 \\\n    --gradient_accumulation_steps=4 \\\n    --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 \\\n    --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 \\\n    --max_grad_norm=1 \\\n    --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Optimizing Stable Diffusion with PyTorch 2.0 and torch.compile\nDESCRIPTION: Demonstrates how to optimize Stable Diffusion models using PyTorch 2.0 features including torch.compile and scaled dot product attention. These optimizations improve inference speed and memory efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipeline.unet = torch.compile(pipeline.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Loading the DiffusionPipeline from Pretrained Model\nDESCRIPTION: This snippet demonstrates how to create an instance of the DiffusionPipeline using a pretrained model checkpoint for inference. It highlights the use of the from_pretrained method and caching mechanisms.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> from diffusers import DiffusionPipeline\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Chaining Image-to-Upscaler-to-Super-Resolution in Python\nDESCRIPTION: This snippet shows how to chain an image-to-image pipeline with an upscaler and super-resolution pipeline to increase image details and resolution. It starts with an initial image, upscales it, and then applies super-resolution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage_1 = pipeline(prompt, image=init_image, output_type=\"latent\").images[0]\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionLatentUpscalePipeline\n\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(\n    \"stabilityai/sd-x2-latent-upscaler\", torch_dtype=torch.float16, use_safetensors=True\n)\nupscaler.enable_model_cpu_offload()\nupscaler.enable_xformers_memory_efficient_attention()\n\nimage_2 = upscaler(prompt, image=image_1).images[0]\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionUpscalePipeline\n\nsuper_res = StableDiffusionUpscalePipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\nsuper_res.enable_model_cpu_offload()\nsuper_res.enable_xformers_memory_efficient_attention()\n\nimage_3 = super_res(prompt, image=image_2).images[0]\nmake_image_grid([init_image, image_3.resize((512, 512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Using Stable Diffusion XL (SDXL) for Image-to-Image Generation\nDESCRIPTION: Complete example of using the more powerful SDXL model for image-to-image transformation, which includes a base model and refiner for higher quality outputs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image, strength=0.5).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Model-Level Offloading\nDESCRIPTION: Shows how to implement model-level offloading which moves entire models between CPU and GPU instead of submodules, providing better performance than sequential offloading.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\npipe.enable_model_cpu_offload()\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading DiffusionPipeline with Stable Diffusion Model\nDESCRIPTION: Initializes a DiffusionPipeline using the Stable Diffusion v1.5 model. This sets up the basic pipeline for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipeline = DiffusionPipeline.from_pretrained(model_id, use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Using Stable Diffusion v1.5 for Image-to-Image Generation\nDESCRIPTION: Complete example of using Stable Diffusion v1.5 for image-to-image transformation, including loading the model, preparing the initial image, and generating a new image based on a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text-to-Image Generation with Stable Diffusion 2\nDESCRIPTION: This code demonstrates how to initialize the Stable Diffusion 2 pipeline for text-to-image generation. It uses DPMSolverMultistepScheduler for efficient inference and shows how to generate an image from a prompt using the base model that outputs 512x512 images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_2.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nimport torch\n\nrepo_id = \"stabilityai/stable-diffusion-2-base\"\npipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, variant=\"fp16\")\n\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\nprompt = \"High quality photo of an astronaut riding a horse in space\"\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Prompt and Initial Image\nDESCRIPTION: Code that passes a prompt and the initial image to the pipeline to generate a new image, then displays both images side by side for comparison.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\nimage = pipeline(prompt, image=init_image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple ControlNets with AnimateDiff for Enhanced Video Control\nDESCRIPTION: This code demonstrates how to use multiple controlnets simultaneously with AnimateDiff. It combines openpose and canny edge controlnets to provide two different types of conditioning for the same animation, enabling more precise control over the generated video.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_89\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoencoderKL, ControlNetModel, MotionAdapter, DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_gif\nfrom PIL import Image\n\nmotion_id = \"guoyww/animatediff-motion-adapter-v1-5-2\"\nadapter = MotionAdapter.from_pretrained(motion_id)\ncontrolnet1 = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_openpose\", torch_dtype=torch.float16)\ncontrolnet2 = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16)\n\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = DiffusionPipeline.from_pretrained(\n    model_id,\n    motion_adapter=adapter,\n    controlnet=[controlnet1, controlnet2],\n    vae=vae,\n    custom_pipeline=\"pipeline_animatediff_controlnet\",\n    torch_dtype=torch.float16,\n).to(device=\"cuda\")\npipe.scheduler = DPMSolverMultistepScheduler.from_pretrained(\n    model_id, subfolder=\"scheduler\", clip_sample=False, timestep_spacing=\"linspace\", steps_offset=1, beta_schedule=\"linear\",\n)\npipe.enable_vae_slicing()\n\ndef load_video(file_path: str):\n    images = []\n\n    if file_path.startswith(('http://', 'https://')):\n        # If the file_path is a URL\n        response = requests.get(file_path)\n        response.raise_for_status()\n        content = BytesIO(response.content)\n        vid = imageio.get_reader(content)\n    else:\n        # Assuming it's a local file path\n        vid = imageio.get_reader(file_path)\n\n    for frame in vid:\n        pil_image = Image.fromarray(frame)\n        images.append(pil_image)\n\n    return images\n\nvideo = load_video(\"dance.gif\")\n\n# You need to install it using `pip install controlnet_aux`\nfrom controlnet_aux.processor import Processor\n\np1 = Processor(\"openpose_full\")\ncn1 = [p1(frame) for frame in video]\n\np2 = Processor(\"canny\")\ncn2 = [p2(frame) for frame in video]\n\nprompt = \"astronaut in space, dancing\"\nnegative_prompt = \"bad quality, worst quality, jpeg artifacts, ugly\"\nresult = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=512,\n    height=768,\n    conditioning_frames=[cn1, cn2],\n    num_inference_steps=20,\n)\n\nexport_to_gif(result.frames[0], \"result.gif\")\n```\n\n----------------------------------------\n\nTITLE: AnimateDiff Image-To-Video Pipeline Implementation in Python\nDESCRIPTION: Implementation of the AnimateDiff pipeline for converting images to videos using motion adapters and diffusion models. Includes pipeline setup, image processing, and animation generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_99\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MotionAdapter, DiffusionPipeline, DDIMScheduler\nfrom diffusers.utils import export_to_gif, load_image\n\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\")\npipe = DiffusionPipeline.from_pretrained(model_id, motion_adapter=adapter, custom_pipeline=\"pipeline_animatediff_img2video\").to(\"cuda\")\npipe.scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\", clip_sample=False, timestep_spacing=\"linspace\", beta_schedule=\"linear\", steps_offset=1)\n\nimage = load_image(\"snail.png\")\noutput = pipe(\n  image=image,\n  prompt=\"A snail moving on the ground\",\n  strength=0.8,\n  latent_interpolation_method=\"slerp\",  # can be lerp, slerp, or your own callback\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Implementing SDXL (Stable Diffusion XL) Pipeline\nDESCRIPTION: Shows how to use the larger SDXL model for higher quality image generation with additional micro-conditionings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", generator=generator).images[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing GlueGen Stable Diffusion Pipeline for Multilingual Text-to-Image Generation\nDESCRIPTION: This code demonstrates how to use GlueGen adapter to enable Stable Diffusion to work with non-English languages. It downloads language-specific checkpoints, loads the XLM-Roberta model as a text encoder, and sets up a pipeline that allows generation from French (and other language) prompts using standard English Stable Diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport gc\nimport urllib.request\nimport torch\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM, CLIPTokenizer, CLIPTextModel\nfrom diffusers import DiffusionPipeline\n\n# Download checkpoints\nCHECKPOINTS = [\n    \"https://storage.googleapis.com/sfr-gluegen-data-research/checkpoints_all/gluenet_checkpoint/gluenet_Chinese_clip_overnorm_over3_noln.ckpt\",\n    \"https://storage.googleapis.com/sfr-gluegen-data-research/checkpoints_all/gluenet_checkpoint/gluenet_French_clip_overnorm_over3_noln.ckpt\",\n    \"https://storage.googleapis.com/sfr-gluegen-data-research/checkpoints_all/gluenet_checkpoint/gluenet_Italian_clip_overnorm_over3_noln.ckpt\",\n    \"https://storage.googleapis.com/sfr-gluegen-data-research/checkpoints_all/gluenet_checkpoint/gluenet_Japanese_clip_overnorm_over3_noln.ckpt\",\n    \"https://storage.googleapis.com/sfr-gluegen-data-research/checkpoints_all/gluenet_checkpoint/gluenet_Spanish_clip_overnorm_over3_noln.ckpt\",\n    \"https://storage.googleapis.com/sfr-gluegen-data-research/checkpoints_all/gluenet_checkpoint/gluenet_sound2img_audioclip_us8k.ckpt\"\n]\n\nLANGUAGE_PROMPTS = {\n    \"French\": \"une voiture sur la plage\",\n    #\"Chinese\": \"海滩上的一辆车\",\n    #\"Italian\": \"una macchina sulla spiaggia\",\n    #\"Japanese\": \"浜辺の車\",\n    #\"Spanish\": \"un coche en la playa\"\n}\n\ndef download_checkpoints(checkpoint_dir):\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    for url in CHECKPOINTS:\n        filename = os.path.join(checkpoint_dir, os.path.basename(url))\n        if not os.path.exists(filename):\n            print(f\"Downloading {filename}...\")\n            urllib.request.urlretrieve(url, filename)\n            print(f\"Downloaded {filename}\")\n        else:\n            print(f\"Checkpoint {filename} already exists, skipping download.\")\n    return checkpoint_dir\n\ndef load_checkpoint(pipeline, checkpoint_path, device):\n    state_dict = torch.load(checkpoint_path, map_location=device)\n    state_dict = state_dict.get(\"state_dict\", state_dict)\n    missing_keys, unexpected_keys = pipeline.unet.load_state_dict(state_dict, strict=False)\n    return pipeline\n\ndef generate_image(pipeline, prompt, device, output_path):\n    with torch.inference_mode():\n        image = pipeline(\n            prompt,\n            generator=torch.Generator(device=device).manual_seed(42),\n            num_inference_steps=50\n        ).images[0]\n        image.save(output_path)\n        print(f\"Image saved to {output_path}\")\n\ncheckpoint_dir = download_checkpoints(\"./checkpoints_all/gluenet_checkpoint\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\", use_fast=False)\nmodel = XLMRobertaForMaskedLM.from_pretrained(\"xlm-roberta-base\").to(device)\ninputs = tokenizer(\"Ceci est une phrase incomplète avec un [MASK].\", return_tensors=\"pt\").to(device)\nwith torch.inference_mode():\n    _ = model(**inputs)\n\n\nclip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\nclip_text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n\n# Initialize pipeline\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    text_encoder=clip_text_encoder,\n    tokenizer=clip_tokenizer,\n    custom_pipeline=\"gluegen\",\n    safety_checker=None\n).to(device)\n\nos.makedirs(\"outputs\", exist_ok=True)\n\n# Generate images\nfor language, prompt in LANGUAGE_PROMPTS.items():\n\n    checkpoint_file = f\"gluegen_{language}_clip_overnorm_over3_noln.ckpt\"\n    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n    try:\n        pipeline = load_checkpoint(pipeline, checkpoint_path, device)\n        output_path = f\"outputs/gluegen_output_{language.lower()}.png\"\n        generate_image(pipeline, prompt, device, output_path)\n    except Exception as e:\n        print(f\"Error processing {language} model: {e}\")\n        continue\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n```\n\n----------------------------------------\n\nTITLE: Generating Canny Edge Image for ControlNet Input\nDESCRIPTION: Code to load an image and use OpenCV to extract a canny edge image, which will be used as conditioning input for ControlNet in a text-to-image pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image, make_image_grid\nfrom PIL import Image\nimport cv2\nimport numpy as np\n\noriginal_image = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n)\n\nimage = np.array(original_image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n```\n\n----------------------------------------\n\nTITLE: Moving the Pipeline to GPU\nDESCRIPTION: This snippet shows how to leverage GPU acceleration by moving the DiffusionPipeline to a CUDA device, which is essential given the large number of parameters in the model for optimized performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Using the Latent Consistency Text-to-Image Pipeline\nDESCRIPTION: This snippet demonstrates how to load the Latent Consistency Model (LCM) and generate images using a text prompt with minimal inference steps. LCM enables high-quality image generation in as few as 4 steps, significantly faster than traditional diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", custom_pipeline=\"latent_consistency_txt2img\", custom_revision=\"main\")\n\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n```\n\n----------------------------------------\n\nTITLE: Basic FLUX Control Pipeline Implementation\nDESCRIPTION: Demonstrates setting up and using the FLUX Control Pipeline with depth control for image generation. Includes loading the model, preprocessing depth maps, and generating controlled images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxControlPipeline, FluxTransformer2DModel\nfrom diffusers.utils import load_image\nfrom image_gen_aux import DepthPreprocessor\n\npipe = FluxControlPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\npipe.load_lora_weights(\"black-forest-labs/FLUX.1-Depth-dev-lora\")\n\nprompt = \"A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.\"\ncontrol_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\n\nprocessor = DepthPreprocessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\ncontrol_image = processor(control_image)[0].convert(\"RGB\")\n\nimage = pipe(\n    prompt=prompt,\n    control_image=control_image,\n    height=1024,\n    width=1024,\n    num_inference_steps=30,\n    guidance_scale=10.0,\n    generator=torch.Generator().manual_seed(42),\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Creating Depth Map for ControlNet Image-to-Image\nDESCRIPTION: Code to load an image and extract a depth map using the depth-estimation pipeline from Transformers, which will be used as conditioning input for ControlNet in an image-to-image pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\nfrom transformers import pipeline\nfrom diffusers.utils import load_image, make_image_grid\n\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg\"\n)\n\ndef get_depth_map(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    depth_map = detected_map.permute(2, 0, 1)\n    return depth_map\n\ndepth_estimator = pipeline(\"depth-estimation\")\ndepth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Using Guidance Scale with Stable Diffusion in Python\nDESCRIPTION: Demonstrates how to set the guidance_scale parameter when generating images with Stable Diffusion. The guidance_scale controls how closely the model follows the prompt, with lower values allowing more creativity and higher values enforcing prompt adherence.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16\n).to(\"cuda\")\nimage = pipeline(\n\t\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", guidance_scale=3.5\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: SVD Implementation with Enhanced Motion Control\nDESCRIPTION: Advanced implementation of Stable Video Diffusion with micro-conditioning parameters to control motion and video characteristics. Demonstrates usage of motion_bucket_id and noise_aug_strength parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/svd.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import StableVideoDiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = StableVideoDiffusionPipeline.from_pretrained(\n  \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipe.enable_model_cpu_offload()\n\n# Load the conditioning image\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\nimage = image.resize((1024, 576))\n\ngenerator = torch.manual_seed(42)\nframes = pipe(image, decode_chunk_size=8, generator=generator, motion_bucket_id=180, noise_aug_strength=0.1).frames[0]\nexport_to_video(frames, \"generated.mp4\", fps=7)\n```\n\n----------------------------------------\n\nTITLE: Setting up ControlNet Pipeline for Inpainting\nDESCRIPTION: Initializes the StableDiffusionControlNetInpaintPipeline with a ControlNet model and configures optimization settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation using Stable Diffusion\nDESCRIPTION: Example showing how to load a pretrained Stable Diffusion model and generate an image from text input using the DiffusionPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16)\npipeline.to(\"cuda\")\npipeline(\"An image of a squirrel in Picasso style\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading IP-Adapter FaceID Model\nDESCRIPTION: This snippet shows how to load an IP-Adapter FaceID model which uses image embeddings generated from 'insightface' instead of CLIP image embeddings. This experimental adapter is designed specifically for maintaining facial identity consistency in generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter-FaceID\", subfolder=None, weight_name=\"ip-adapter-faceid_sdxl.bin\", image_encoder_folder=None)\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionPipelineOutput in Python\nDESCRIPTION: This snippet shows the import statement for the StableDiffusionPipelineOutput class, which represents the output of Stable Diffusion pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Loading and Using AutoPipelineForText2Image with Dreamlike Photoreal Model\nDESCRIPTION: Demonstrates how to load a Stable Diffusion model for text-to-image generation using AutoPipelineForText2Image. This code creates a pipeline, sets a prompt for generating an image of Godzilla eating sushi, and renders the result with a specific random seed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe_txt2img = AutoPipelineForText2Image.from_pretrained(\n    \"dreamlike-art/dreamlike-photoreal-2.0\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\nprompt = \"cinematic photo of Godzilla eating sushi with a cat in a izakaya, 35mm photograph, film, professional, 4k, highly detailed\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(37)\nimage = pipe_txt2img(prompt, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Reduced Inference Steps\nDESCRIPTION: Demonstrates image generation with the optimized scheduler and reduced number of inference steps, significantly decreasing generation time.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\nimage = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: SD3 without T5 Text Encoder\nDESCRIPTION: Memory-optimized implementation of SD3 by removing the T5-XXL text encoder during inference\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\n\npipe = StableDiffusion3Pipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n    text_encoder_3=None,\n    tokenizer_3=None,\n    torch_dtype=torch.float16\n)\npipe.to(\"cuda\")\n\nimage = pipe(\n    prompt=\"a photo of a cat holding a sign that says hello world\",\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    height=1024,\n    width=1024,\n    guidance_scale=7.0,\n).images[0]\n\nimage.save(\"sd3_hello_world-no-T5.png\")\n```\n\n----------------------------------------\n\nTITLE: Saving the Generated Image\nDESCRIPTION: This snippet provides the method to save the generated image to a file, demonstrating how to persist the output of the model's inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n>>> image.save(\"image_of_squirrel_painting.png\")\n```\n\n----------------------------------------\n\nTITLE: Chaining Image-to-Image Pipeline for Final Refinement in Python\nDESCRIPTION: This code converts the inpainting pipeline to an image-to-image pipeline using the from_pipe method to reuse components. It applies the image-to-image transformation for final polishing and displays a grid of all images in the process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForImage2Image.from_pipe(pipeline)\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nimage = pipeline(prompt=prompt, image=image).images[0]\nmake_image_grid([init_image, mask_image, image_inpainting, image], rows=2, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Composable Stable Diffusion in Python\nDESCRIPTION: Implements Composable Stable Diffusion which allows for conjunction and negation operators in prompts. This example demonstrates how to combine separate sentences with weights for compositional image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport torch as th\nimport numpy as np\nimport torchvision.utils as tvu\n\nfrom diffusers import DiffusionPipeline\n\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--prompt\", type=str, default=\"mystical trees | A magical pond | dark\",\n                    help=\"use '|' as the delimiter to compose separate sentences.\")\nparser.add_argument(\"--steps\", type=int, default=50)\nparser.add_argument(\"--scale\", type=float, default=7.5)\nparser.add_argument(\"--weights\", type=str, default=\"7.5 | 7.5 | -7.5\")\nparser.add_argument(\"--seed\", type=int, default=2)\nparser.add_argument(\"--model_path\", type=str, default=\"CompVis/stable-diffusion-v1-4\")\nparser.add_argument(\"--num_images\", type=int, default=1)\nargs = parser.parse_args()\n\nhas_cuda = th.cuda.is_available()\ndevice = th.device('cpu' if not has_cuda else 'cuda')\n\nprompt = args.prompt\nscale = args.scale\nsteps = args.steps\n\npipe = DiffusionPipeline.from_pretrained(\n    args.model_path,\n    custom_pipeline=\"composable_stable_diffusion\",\n).to(device)\n\npipe.safety_checker = None\n\nimages = []\ngenerator = th.Generator(\"cuda\").manual_seed(args.seed)\nfor i in range(args.num_images):\n    image = pipe(prompt, guidance_scale=scale, num_inference_steps=steps,\n                 weights=args.weights, generator=generator).images[0]\n    images.append(th.from_numpy(np.array(image)).permute(2, 0, 1) / 255.)\ngrid = tvu.make_grid(th.stack(images, dim=0), nrow=4, padding=0)\ntvu.save_image(grid, f'{prompt}_{args.weights}' + '.png')\nprint(\"Image saved successfully!\")\n```\n\n----------------------------------------\n\nTITLE: Chaining Image-to-Image Pipelines in Python\nDESCRIPTION: This snippet demonstrates how to chain multiple image-to-image pipelines to apply different styles sequentially. It loads an initial image, applies a comic book art style, and then transforms it into pixel art.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image, output_type=\"latent\").images[0]\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"ogkalu/Comic-Diffusion\", torch_dtype=torch.float16\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# need to include the token \"charliebo artstyle\" in the prompt to use this checkpoint\nimage = pipeline(\"Astronaut in a jungle, charliebo artstyle\", image=image, output_type=\"latent\").images[0]\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"kohbanye/pixel-art-style\", torch_dtype=torch.float16\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# need to include the token \"pixelartstyle\" in the prompt to use this checkpoint\nimage = pipeline(\"Astronaut in a jungle, pixelartstyle\", image=image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Text-to-Image Pipeline with AutoPipelineForText2Image\nDESCRIPTION: Loads a Stable Diffusion v1.5 checkpoint and generates an image from a text prompt using the AutoPipelineForText2Image class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\n\nimage = pipeline(\n\t\"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Converting Images to Videos with StableVideoDiffusion\nDESCRIPTION: This code snippet shows how to use the StableVideoDiffusion model to convert a still image into a short video. The pipeline is based on Stable Diffusion 2.1 and is designed to generate 2-4 second videos from input images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableVideoDiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipeline = StableVideoDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n)\n```\n\n----------------------------------------\n\nTITLE: Inpainting with SDXL Base and Refiner\nDESCRIPTION: Sets up and executes inpainting using both SDXL base and refiner models. Includes loading images, creating masks, and performing the inpainting process with both models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLInpaintPipeline\nfrom diffusers.utils import load_image, make_image_grid\nimport torch\n\nbase = StableDiffusionXLInpaintPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n\nrefiner = StableDiffusionXLInpaintPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n).to(\"cuda\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = load_image(img_url)\nmask_image = load_image(mask_url)\n\nprompt = \"A majestic tiger sitting on a bench\"\nnum_inference_steps = 75\nhigh_noise_frac = 0.7\n\nimage = base(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    num_inference_steps=num_inference_steps,\n    denoising_end=high_noise_frac,\n    output_type=\"latent\",\n).images\nimage = refiner(\n    prompt=prompt,\n    image=image,\n    mask_image=mask_image,\n    num_inference_steps=num_inference_steps,\n    denoising_start=high_noise_frac,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Creating Animations with AnimateDiff and LCM-LoRA Integration\nDESCRIPTION: This code demonstrates combining AnimateDiff with LCM-LoRA for efficient animation generation. It loads the AnimateDiff motion adapter and a Stable Diffusion model, applies the LCM scheduler and multiple LoRA adapters (for LCM and motion effects), and generates a smooth animation in just 5 inference steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5\")\npipe = AnimateDiffPipeline.from_pretrained(\n    \"frankjoshua/toonyou_beta6\",\n    motion_adapter=adapter,\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\", adapter_name=\"lcm\")\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", weight_name=\"diffusion_pytorch_model.safetensors\", adapter_name=\"motion-lora\")\n\npipe.set_adapters([\"lcm\", \"motion-lora\"], adapter_weights=[0.55, 1.2])\n\nprompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\ngenerator = torch.manual_seed(0)\nframes = pipe(\n    prompt=prompt,\n    num_inference_steps=5,\n    guidance_scale=1.25,\n    cross_attention_kwargs={\"scale\": 1},\n    num_frames=24,\n    generator=generator\n).frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Using Kandinsky 2.2 for Image-to-Image Generation\nDESCRIPTION: Complete example of using Kandinsky 2.2 model for image-to-image generation, which uses an image prior model to create better text-image alignment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Optimizing DiffusionPipeline with Float16 Precision\nDESCRIPTION: Loads the model in float16 precision to improve inference speed without significant quality loss. This is a key optimization technique.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True)\npipeline = pipeline.to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Using Prompt Embeddings for Weighted Prompts\nDESCRIPTION: Shows how to use pre-computed prompt embeddings from the Compel library to control the emphasis of different concepts in the generated image. This technique allows for fine-grained control over feature importance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16\n).to(\"cuda\")\nimage = pipeline(\n\tprompt_embeds=prompt_embeds, # generated from Compel\n\tnegative_prompt_embeds=negative_prompt_embeds, # generated from Compel\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: LCM with SDXL and LoRA Integration\nDESCRIPTION: Demonstrates loading LCM checkpoint with SDXL and integrating LoRA weights for styled image generation. Uses UNet2DConditionModel and LCMScheduler for fast inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"latent-consistency/lcm-sdxl\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\",\n).to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\n\nprompt = \"papercut, a cute fox\"\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=8.0\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Inference with SDXL Refiner Using Trained LoRA Model\nDESCRIPTION: Python code to perform two-stage inference with a trained LoRA model, using the SDXL base model first and then refining the output with the SDXL Refiner for improved image quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub.repocard import RepoCard\nfrom diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline\nimport torch\n\nlora_model_id = <\"lora-sdxl-dreambooth-id\">\ncard = RepoCard.load(lora_model_id)\nbase_model_id = card.data.to_dict()[\"base_model\"]\n\n# Load the base pipeline and load the LoRA parameters into it.\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\npipe.load_lora_weights(lora_model_id)\n\n# Load the refiner.\nrefiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n)\nrefiner.to(\"cuda\")\n\nprompt = \"A picture of a sks dog in a bucket\"\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\n# Run inference.\nimage = pipe(prompt=prompt, output_type=\"latent\", generator=generator).images[0]\nimage = refiner(prompt=prompt, image=image[None, :], generator=generator).images[0]\nimage.save(\"refined_sks_dog.png\")\n```\n\n----------------------------------------\n\nTITLE: Implementing UnCLIP Text Interpolation Pipeline with Kakaobrain's Karlo Model\nDESCRIPTION: This code demonstrates how to use the UnCLIP text interpolation pipeline to create a sequence of images that smoothly transition between two text prompts. The pipeline loads the kakaobrain/karlo-v1-alpha model and uses spherical interpolation between text embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\ndevice = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n\npipe = DiffusionPipeline.from_pretrained(\n    \"kakaobrain/karlo-v1-alpha\",\n    torch_dtype=torch.float16,\n    custom_pipeline=\"unclip_text_interpolation\"\n)\npipe.to(device)\n\nstart_prompt = \"A photograph of an adult lion\"\nend_prompt = \"A photograph of a lion cub\"\n# For best results keep the prompts close in length to each other. Of course, feel free to try out with differing lengths.\ngenerator = torch.Generator(device=device).manual_seed(42)\n\noutput = pipe(start_prompt, end_prompt, steps=6, generator=generator, enable_sequential_cpu_offload=False)\n\nfor i,image in enumerate(output.images):\n    img.save('result%s.jpg' % i)\n```\n\n----------------------------------------\n\nTITLE: Generating an Image from Text Prompt\nDESCRIPTION: This snippet illustrates how to use the pipeline to generate an image based on a text prompt. It demonstrates the output process to access the generated image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n>>> image = pipeline(\"An image of a squirrel in Picasso style\").images[0]\n>>> image\n```\n\n----------------------------------------\n\nTITLE: Basic Stable Cascade Text-to-Image Generation in Python\nDESCRIPTION: This snippet demonstrates how to use Stable Cascade for text-to-image generation with separate prior and decoder pipelines. It shows loading the models with bfloat16 precision, generating latent representations from a text prompt, and decoding them into a final image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_cascade.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", variant=\"bf16\", torch_dtype=torch.bfloat16)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", variant=\"bf16\", torch_dtype=torch.float16)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings.to(torch.float16),\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade.png\")\n```\n\n----------------------------------------\n\nTITLE: Swift Core ML Inference Command\nDESCRIPTION: Command to run Stable Diffusion inference using Swift with Core ML models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/coreml.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nswift run StableDiffusionSample --resource-path models/coreml-stable-diffusion-v1-4_original_compiled --compute-units all \"a photo of an astronaut riding a horse on mars\"\n```\n\n----------------------------------------\n\nTITLE: Quantizing Stable Diffusion 3 Pipeline with bitsandbytes in Python\nDESCRIPTION: This code demonstrates how to load a quantized StableDiffusion3Pipeline for inference using bitsandbytes, including quantizing both the text encoder and transformer components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, SD3Transformer2DModel, StableDiffusion3Pipeline\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"stabilityai/stable-diffusion-3.5-large\",\n    subfolder=\"text_encoder_3\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = SD3Transformer2DModel.from_pretrained(\n    \"stabilityai/stable-diffusion-3.5-large\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = StableDiffusion3Pipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-3.5-large\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\nimage = pipeline(prompt, num_inference_steps=28, guidance_scale=7.0).images[0]\nimage.save(\"sd3.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading T2I-Adapter and Stable Diffusion 1.5 Pipeline\nDESCRIPTION: Code to initialize a T2I-Adapter model pretrained on canny images and integrate it with the StableDiffusionAdapterPipeline. The pipeline is configured to use half-precision (float16) for efficient inference on CUDA GPUs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionAdapterPipeline, T2IAdapter\n\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_canny_sd15v2\", torch_dtype=torch.float16)\npipeline = StableDiffusionAdapterPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    adapter=adapter,\n    torch_dtype=torch.float16,\n)\npipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Denoising Image Generation Loop\nDESCRIPTION: Implement a denoising loop that progressively reduces noise in an image using the model and scheduler\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport tqdm\n\nsample = noisy_sample\n\nfor i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n    # 1. predict noise residual\n    with torch.no_grad():\n        residual = model(sample, t).sample\n\n    # 2. compute less noisy image\n    sample = scheduler.step(residual, t, sample).prev_sample\n```\n\n----------------------------------------\n\nTITLE: Implementing Text-to-Image Generation with LCM in Stable Diffusion XL\nDESCRIPTION: Loads a Latent Consistency Model checkpoint into a Stable Diffusion XL pipeline and configures it for fast inference using only 4 steps. This implementation uses the full LCM model with the LCMScheduler to generate high-quality images with a guidance scale of 8.0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"latent-consistency/lcm-sdxl\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\",\n).to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=8.0\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Quantizing and Running Inference in Python\nDESCRIPTION: This snippet demonstrates how to quantize a model after merging parameters and perform inference by generating an image based on input prompts. It also handles the quantization configuration for memory efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/README.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image, FluxTransformer2DModel, BitsAndBytesConfig\nimport torch\n\nckpt_id = \"black-forest-labs/FLUX.1-dev\"\nbnb_4bit_compute_dtype = torch.float16\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n)\ntransformer = FluxTransformer2DModel.from_pretrained(\n    \"fused_transformer\",\n    quantization_config=nf4_config,\n    torch_dtype=bnb_4bit_compute_dtype,\n)\npipeline = AutoPipelineForText2Image.from_pretrained(\n    ckpt_id, transformer=transformer, torch_dtype=bnb_4bit_compute_dtype\n)\npipeline.enable_model_cpu_offload()\n\nimage = pipeline(\n    \"a puppy in a pond, yarn art style\", num_inference_steps=28, guidance_scale=3.5, height=768\n).images[0]\nimage.save(\"yarn_merged.png\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Depth Prediction with Marigold Pipeline in Python\nDESCRIPTION: This code loads the Marigold depth prediction model, processes an input image to generate a depth map, and saves both the raw depth prediction as a 16-bit PNG and a visualization with a color mapping. The pipeline uses CUDA acceleration with mixed precision (fp16) for better performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\n\npipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n    \"prs-eth/marigold-depth-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(\"cuda\")\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\ndepth = pipe(image)\n\nvis = pipe.image_processor.visualize_depth(depth.prediction)\nvis[0].save(\"einstein_depth.png\")\n\ndepth_16bit = pipe.image_processor.export_depth_to_16bit_png(depth.prediction)\ndepth_16bit[0].save(\"einstein_depth_16bit.png\")\n```\n\n----------------------------------------\n\nTITLE: Applying torch.compile to UNet and VAE Components\nDESCRIPTION: Compiling the UNet and VAE components with torch.compile using max-autotune mode and fullgraph to maximize inference speed. First inference is slower for compilation, but subsequent calls are significantly faster.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Compile the UNet and VAE.\npipe.unet = torch.compile(pipe.unet, mode=\"max-autotune\", fullgraph=True)\npipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# First call to `pipe` is slow, subsequent ones are faster.\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading Single Checkpoint for StableDiffusion3Pipeline without T5 in Python\nDESCRIPTION: This snippet demonstrates how to load a single file checkpoint for the StableDiffusion3Pipeline without the T5 text encoder, and use it for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\n\npipe = StableDiffusion3Pipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium_incl_clips.safetensors\",\n    torch_dtype=torch.float16,\n    text_encoder_3=None\n)\npipe.enable_model_cpu_offload()\n\nimage = pipe(\"a picture of a cat holding a sign that says hello world\").images[0]\nimage.save('sd3-single-file.png')\n```\n\n----------------------------------------\n\nTITLE: Using Flux Canny Control Model\nDESCRIPTION: Shows how to use the Flux Canny Control model for controlled image generation using edge detection. This implementation uses channel-wise concatenation instead of traditional ControlNet architecture.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom controlnet_aux import CannyDetector\nfrom diffusers import FluxControlPipeline\nfrom diffusers.utils import load_image\n\npipe = FluxControlPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Canny-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n\nprompt = \"A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.\"\ncontrol_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\n\nprocessor = CannyDetector()\ncontrol_image = processor(control_image, low_threshold=50, high_threshold=200, detect_resolution=1024, image_resolution=1024)\n\nimage = pipe(\n    prompt=prompt,\n    control_image=control_image,\n    height=1024,\n    width=1024,\n    num_inference_steps=50,\n    guidance_scale=30.0,\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing FLUX Image Generation with Context Parallelism in Python\nDESCRIPTION: This Python script showcases the optimization of FLUX image generation using Context Parallelism. It sets up a distributed environment, initializes a pipeline with parallelization, and quantizes model components to enhance performance. The script runs inference, measures timing, and saves the generated image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/para_attn.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\\nimport time\\nimport torch\\nimport torch.distributed as dist\\nfrom diffusers import FluxPipeline\\n\\ndist.init_process_group()\\n\\ntorch.cuda.set_device(dist.get_rank())\\n\\npipe = FluxPipeline.from_pretrained(\\n    \\\"black-forest-labs/FLUX.1-dev\\\",\\n    torch_dtype=torch.bfloat16,\\n).to(\\\"cuda\\\")\\n\\nfrom para_attn.context_parallel import init_context_parallel_mesh\\nfrom para_attn.context_parallel.diffusers_adapters import parallelize_pipe\\nfrom para_attn.parallel_vae.diffusers_adapters import parallelize_vae\\n\\nmesh = init_context_parallel_mesh(\\n    pipe.device.type,\\n    max_ring_dim_size=2,\\n)\\nparallelize_pipe(\\n    pipe,\\n    mesh=mesh,\\n)\\nparallelize_vae(pipe.vae, mesh=mesh._flatten())\\n\\nfrom para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\\n\\napply_cache_on_pipe(\\n    pipe,\\n    residual_diff_threshold=0.12,  # Use a larger value to make the cache take effect\\n)\\n\\nfrom torchao.quantization import quantize_, float8_dynamic_activation_float8_weight, float8_weight_only\\n\\nquantize_(pipe.text_encoder, float8_weight_only())\\nquantize_(pipe.transformer, float8_dynamic_activation_float8_weight())\\ntorch._inductor.config.reorder_for_compute_comm_overlap = True\\npipe.transformer = torch.compile(\\n   pipe.transformer, mode=\\\"max-autotune-no-cudagraphs\\\",\\n)\\n\\nfor i in range(2):\\n    begin = time.time()\\n    image = pipe(\\n        \\\"A cat holding a sign that says hello world\\\",\\n        num_inference_steps=28,\\n        output_type=\\\"pil\\\" if dist.get_rank() == 0 else \\\"pt\\\",\\n    ).images[0]\\n    end = time.time()\\n    if dist.get_rank() == 0:\\n        if i == 0:\\n            print(f\\\"Warm up time: {end - begin:.2f}s\\\")\\n        else:\\n            print(f\\\"Time: {end - begin:.2f}s\\\")\\n\\nif dist.get_rank() == 0:\\n    print(\\\"Saving image to flux.png\\\")\\n    image.save(\\\"flux.png\\\")\\n\\ndist.destroy_process_group()\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Video-to-Video Translation with Rerender Pipeline\nDESCRIPTION: Implementation of zero-shot video-to-video translation using the Rerender A Video pipeline. Includes frame extraction, canny edge detection, and controlled diffusion generation with temporal consistency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_96\n\nLANGUAGE: python\nCODE:\n```\nimport sys\ngmflow_dir = \"/path/to/gmflow\"\nsys.path.insert(0, gmflow_dir)\n\nfrom diffusers import ControlNetModel, AutoencoderKL, DDIMScheduler\nfrom diffusers.utils import export_to_video\nimport numpy as np\nimport torch\nimport cv2\nfrom PIL import Image\n\ndef video_to_frame(video_path: str, interval: int):\n    vidcap = cv2.VideoCapture(video_path)\n    success = True\n    count = 0\n    res = []\n    while success:\n        count += 1\n        success, image = vidcap.read()\n        if count % interval != 1:\n            continue\n        if image is not None:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            res.append(image)\n    vidcap.release()\n    return res\n```\n\n----------------------------------------\n\nTITLE: Implementing MagicMix for Semantic Mixing in Python\nDESCRIPTION: This code demonstrates the implementation of MagicMix, a method for semantic mixing of an image and a text prompt. It uses a custom pipeline to create a new concept while preserving the spatial layout of the original image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom diffusers import DiffusionPipeline, DDIMScheduler\nfrom PIL import Image\nfrom io import BytesIO\n\npipe = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"magic_mix\",\n    scheduler=DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"),\n).to('cuda')\n\nurl = \"https://user-images.githubusercontent.com/59410571/209578593-141467c7-d831-4792-8b9a-b17dc5e47816.jpg\"\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")  # Convert to RGB to avoid issues\nmix_img = pipe(\n    image,\n    prompt='bed',\n    kmin=0.3,\n    kmax=0.5,\n    mix_factor=0.5,\n    )\nmix_img.save('phone_bed_mix.jpg')\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion components in Python\nDESCRIPTION: Loads the individual components (VAE, tokenizer, text encoder, UNet, scheduler) needed for a Stable Diffusion pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from PIL import Image\n>>> import torch\n>>> from transformers import CLIPTextModel, CLIPTokenizer\n>>> from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n\n>>> vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n>>> text_encoder = CLIPTextModel.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True\n... )\n>>> unet = UNet2DConditionModel.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True\n... )\n\n>>> from diffusers import UniPCMultistepScheduler\n\n>>> scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n\n>>> torch_device = \"cuda\"\n>>> vae.to(torch_device)\n>>> text_encoder.to(torch_device)\n>>> unet.to(torch_device)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text-to-Image Generation with LCM-LoRA in Stable Diffusion XL\nDESCRIPTION: Sets up a Stable Diffusion XL pipeline with LCM-LoRA, a lightweight adapter that enables fast inference in just 4 steps. This approach replaces the scheduler with LCMScheduler and loads the LCM-LoRA weights to achieve efficient image generation with a guidance scale of 1.0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, LCMScheduler\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    variant=\"fp16\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\ngenerator = torch.manual_seed(42)\nimage = pipe(\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Inference with Block-Specific LoRA for Stable Diffusion XL\nDESCRIPTION: Python code for loading and applying a block-specific LoRA model during inference with Stable Diffusion XL. The code includes utility functions to filter state dictionary keys by target blocks and shows how to load the filtered LoRA weights into the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, AutoencoderKL\n\n# taken & modified from B-LoRA repo - https://github.com/yardenfren1996/B-LoRA/blob/main/blora_utils.py\ndef is_belong_to_blocks(key, blocks):\n    try:\n        for g in blocks:\n            if g in key:\n                return True\n        return False\n    except Exception as e:\n        raise type(e)(f'failed to is_belong_to_block, due to: {e}')\n\ndef lora_lora_unet_blocks(lora_path, alpha, target_blocks):\n  state_dict, _ = pipeline.lora_state_dict(lora_path)\n  filtered_state_dict = {k: v * alpha for k, v in state_dict.items() if is_belong_to_blocks(k, target_blocks)}\n  return filtered_state_dict\n\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    vae=vae,\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\nlora_path  = \"lora-library/B-LoRA-pen_sketch\"\n\nstate_dict = lora_lora_unet_blocks(content_B_lora_path,alpha=1,target_blocks=[\"unet.up_blocks.0.attentions.0\"])\n\n# Load trained lora layers into the unet\npipeline.load_lora_into_unet(state_dict, None, pipeline.unet)\n\n#generate\nprompt = \"a dog in [v30] style\"\npipeline(prompt, num_images_per_prompt=4).images\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Header\nDESCRIPTION: License and copyright notice for the Hugging Face Diffusers library documentation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/text2img.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Loading T2I-Adapter and Stable Diffusion XL Pipeline\nDESCRIPTION: Code to initialize a T2I-Adapter model pretrained for Stable Diffusion XL along with the required components (scheduler, VAE). The StableDiffusionXLAdapterPipeline is configured with these components and set to use half-precision for efficient inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL\n\nscheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\")\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16)\npipeline = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    adapter=adapter,\n    vae=vae,\n    scheduler=scheduler,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Using T2I-Adapter with LCM-LoRA for Canny Edge-Conditioned Image Generation\nDESCRIPTION: This code shows how to combine T2I-Adapter with LCM-LoRA weights for efficient image generation based on canny edge detection. Rather than replacing the entire UNet model, this approach uses LoRA weights to adapt the base model for faster inference while maintaining quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLAdapterPipeline, UNet2DConditionModel, T2IAdapter, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\n# detect the canny map in low resolution to avoid high-frequency details\nimage = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n).resize((384, 384))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1024))\n\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\n\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n\nprompt = \"the mona lisa, 4k picture, high quality\"\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5,\n    adapter_conditioning_scale=0.8,\n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Kolors Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use the KolorsPipeline to generate an image from a text prompt. It includes setting up the pipeline, configuring the scheduler, and saving the generated image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kolors.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import DPMSolverMultistepScheduler, KolorsPipeline\n\npipe = KolorsPipeline.from_pretrained(\"Kwai-Kolors/Kolors-diffusers\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)\n\nimage = pipe(\n    prompt='一张瓢虫的照片，微距，变焦，高质量，电影，拿着一个牌子，写着\"可图\"',\n    negative_prompt=\"\",\n    guidance_scale=6.5,\n    num_inference_steps=25,\n).images[0]\n\nimage.save(\"kolors_sample.png\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Stable Diffusion XL License Header\nDESCRIPTION: Apache License 2.0 header for the Stable Diffusion XL implementation, defining usage rights and limitations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Launching LoRA Training Script for Naruto Dataset\nDESCRIPTION: Full command to train a LoRA model on the Naruto BLIP captions dataset using Accelerate and mixed precision\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lora.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"/sddata/finetune/lora/naruto\"\nexport HUB_MODEL_ID=\"naruto-lora\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --dataloader_num_workers=8 \\\n  --resolution=512 \\\n  --center_crop \\\n  --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-04 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"cosine\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=${OUTPUT_DIR} \\\n  --push_to_hub \\\n  --hub_model_id=${HUB_MODEL_ID} \\\n  --report_to=wandb \\\n  --checkpointing_steps=500 \\\n  --validation_prompt=\"A naruto with blue eyes.\" \\\n  --seed=1337\n```\n\n----------------------------------------\n\nTITLE: ControlNet Inference - Python\nDESCRIPTION: Performs inference with a trained ControlNet model using the StableDiffusionControlNetPipeline. Loads a ControlNet model and a base Stable Diffusion model, sets up the pipeline, loads a conditioning image, and generates an image based on a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n\"from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.utils import load_image\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\"path/to/controlnet\", torch_dtype=torch.float16)\npipeline = StableDiffusionControlNetPipeline.from_pretrained(\n    \"path/to/base/model\", controlnet=controlnet, torch_dtype=torch.float16\n).to(\"cuda\")\n\ncontrol_image = load_image(\"./conditioning_image_1.png\")\nprompt = \\\"pale golden rod circle with old lace background\\\"\n\ngenerator = torch.manual_seed(0)\nimage = pipeline(prompt, num_inference_steps=20, generator=generator, image=control_image).images[0]\nimage.save(\"./output.png\")\"\n```\n\n----------------------------------------\n\nTITLE: Loading Image-to-Image Pipeline with Kandinsky 2.2\nDESCRIPTION: Code for loading a Kandinsky 2.2 model into the AutoPipelineForImage2Image class for image-to-image generation with GPU acceleration and memory optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Kandinsky 2.2 Pipeline\nDESCRIPTION: Demonstrates using the Kandinsky 2.2 model which includes an image prior model for better text-image alignment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", generator=generator).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading StableDiffusionXL Pipeline with Offset Noise LoRA in Python\nDESCRIPTION: Sets up the Stable Diffusion XL pipeline with an offset noise LoRA to improve image quality. The enhanced prompt is used to generate an image with the pipeline, demonstrating the final result of the prompt enhancement technique.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"RunDiffusion/Juggernaut-XL-v9\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\n\npipeline.load_lora_weights(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    weight_name=\"sd_xl_offset_example-lora_1.0.safetensors\",\n    adapter_name=\"offset\",\n)\npipeline.set_adapters([\"offset\"], adapter_weights=[0.2])\n\nimage = pipeline(\n    enhanced_prompt,\n    width=1152,\n    height=896,\n    guidance_scale=7.5,\n    num_inference_steps=25,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Generating Videos from Images and Text with CogVideoX\nDESCRIPTION: This code demonstrates how to use the CogVideoX-5b-I2V model to generate a video from an input image and text prompt. The pipeline reduces memory requirements using VAE tiling and slicing, and outputs a 49-frame video with a specified guidance scale.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXImageToVideoPipeline\nfrom diffusers.utils import export_to_video, load_image\n\nprompt = \"A vast, shimmering ocean flows gracefully under a twilight sky, its waves undulating in a mesmerizing dance of blues and greens. The surface glints with the last rays of the setting sun, casting golden highlights that ripple across the water. Seagulls soar above, their cries blending with the gentle roar of the waves. The horizon stretches infinitely, where the ocean meets the sky in a seamless blend of hues. Close-ups reveal the intricate patterns of the waves, capturing the fluidity and dynamic beauty of the sea in motion.\"\nimage = load_image(image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cogvideox/cogvideox_rocket.png\")\npipe = CogVideoXImageToVideoPipeline.from_pretrained(\n    \"THUDM/CogVideoX-5b-I2V\",\n    torch_dtype=torch.bfloat16\n)\n\n# reduce memory requirements \npipe.vae.enable_tiling()\npipe.vae.enable_slicing()\n\nvideo = pipe(\n    prompt=prompt,\n    image=image,\n    num_videos_per_prompt=1,\n    num_inference_steps=50,\n    num_frames=49,\n    guidance_scale=6,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained ControlNet SDXL Model (Python)\nDESCRIPTION: Python code demonstrating how to use the trained ControlNet model with SDXL for inference, including model loading, pipeline setup, and image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom diffusers.utils import load_image\nimport torch\n\nbase_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\ncontrolnet_path = \"path to controlnet\"\n\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    base_model_path, controlnet=controlnet, torch_dtype=torch.float16\n)\n\n# speed up diffusion process with faster scheduler and memory optimization\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# remove following line if xformers is not installed or when using Torch 2.0.\npipe.enable_xformers_memory_efficient_attention()\n# memory optimization.\npipe.enable_model_cpu_offload()\n\ncontrol_image = load_image(\"./conditioning_image_1.png\").resize((1024, 1024))\nprompt = \"pale golden rod circle with old lace background\"\n\n# generate image\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt, num_inference_steps=20, generator=generator, image=control_image\n).images[0]\nimage.save(\"./output.png\")\n```\n\n----------------------------------------\n\nTITLE: Using IP Adapter with Kolors Pipeline in Python\nDESCRIPTION: This snippet shows how to integrate the IP Adapter with the Kolors pipeline for image-conditioned generation. It includes loading the image encoder, setting up the pipeline with IP Adapter, and generating an image based on both text and image inputs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kolors.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import CLIPVisionModelWithProjection\n\nfrom diffusers import DPMSolverMultistepScheduler, KolorsPipeline\nfrom diffusers.utils import load_image\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"Kwai-Kolors/Kolors-IP-Adapter-Plus\",\n    subfolder=\"image_encoder\",\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    revision=\"refs/pr/4\",\n)\n\npipe = KolorsPipeline.from_pretrained(\n    \"Kwai-Kolors/Kolors-diffusers\", image_encoder=image_encoder, torch_dtype=torch.float16, variant=\"fp16\"\n)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)\n\npipe.load_ip_adapter(\n    \"Kwai-Kolors/Kolors-IP-Adapter-Plus\",\n    subfolder=\"\",\n    weight_name=\"ip_adapter_plus_general.safetensors\",\n    revision=\"refs/pr/4\",\n    image_encoder_folder=None,\n)\npipe.enable_model_cpu_offload()\n\nipa_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/kolors/cat_square.png\")\n\nimage = pipe(\n    prompt=\"best quality, high quality\",\n    negative_prompt=\"\",\n    guidance_scale=6.5,\n    num_inference_steps=25,\n    ip_adapter_image=ipa_image,\n).images[0]\n\nimage.save(\"kolors_ipa_sample.png\")\n```\n\n----------------------------------------\n\nTITLE: Reusing Stable Diffusion Pipeline Components for Memory Efficiency\nDESCRIPTION: Example demonstrating how to reuse components across multiple Stable Diffusion pipelines to save memory. The code shows how to create text-to-image, image-to-image, and inpainting pipelines while sharing the same underlying model components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/overview.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import (\n    StableDiffusionPipeline,\n    StableDiffusionImg2ImgPipeline,\n    StableDiffusionInpaintPipeline,\n)\n\ntext2img = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\nimg2img = StableDiffusionImg2ImgPipeline(**text2img.components)\ninpaint = StableDiffusionInpaintPipeline(**text2img.components)\n```\n\n----------------------------------------\n\nTITLE: Executing Inpainting Pipeline\nDESCRIPTION: Runs the inpainting pipeline with the prepared images and control parameters to generate the final output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\noutput = pipe(\n    \"corgi face with large ears, detailed, pixar, animated, disney\",\n    num_inference_steps=20,\n    eta=1.0,\n    image=init_image,\n    mask_image=mask_image,\n    control_image=control_image,\n).images[0]\nmake_image_grid([init_image, mask_image, output], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for InstructPix2Pix\nDESCRIPTION: Instructions for installing the Diffusers library from source to ensure compatibility with the latest example scripts. These commands clone the repository and install it in development mode.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Enabling Memory-Efficient Attention with xFormers in Diffusers\nDESCRIPTION: This code shows how to enable memory-efficient attention using xFormers within a Diffusers pipeline. It initializes the Stable Diffusion pipeline and calls `enable_xformers_memory_efficient_attention()` to activate the optimized attention mechanism. This can reduce memory consumption and potentially improve inference speed, especially for PyTorch versions older than 2.0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n).to(\"cuda\")\n\npipe.enable_xformers_memory_efficient_attention()\n\nwith torch.inference_mode():\n    sample = pipe(\"a small cat\")\n\n# optional: You can disable it via\n# pipe.disable_xformers_memory_efficient_attention()\n```\n\n----------------------------------------\n\nTITLE: Generating Text-to-Image with SDXL in Python\nDESCRIPTION: Demonstrates how to use SDXL for text-to-image generation using a text prompt. The default image size is 1024x1024.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline_text2image = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipeline_text2image(prompt=prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Logging into HuggingFace CLI\nDESCRIPTION: Command to authenticate with HuggingFace to access gated SD3 models\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with IP-Adapter and SDXL\nDESCRIPTION: Sets up an SDXL pipeline with IP-Adapter for text-to-image generation. The code loads the model, configures IP-Adapter, and generates images using both text and image prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\npipeline.set_ip_adapter_scale(0.6)\n```\n\nLANGUAGE: python\nCODE:\n```\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\")\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimages = pipeline(\n    prompt=\"a polar bear sitting in a chair drinking a milkshake\",\n    ip_adapter_image=image,\n    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n    num_inference_steps=100,\n    generator=generator,\n).images\nimages[0]\n```\n\n----------------------------------------\n\nTITLE: Training Dreambooth LoRA for SDXL with Advanced Features\nDESCRIPTION: Complete command to train a Dreambooth LoRA model with SDXL using advanced features like pivotal tuning, custom captions, and Prodigy optimizer.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport DATASET_NAME=\"./3d_icon\"\nexport OUTPUT_DIR=\"3d-icon-SDXL-LoRA\"\nexport VAE_PATH=\"madebyollin/sdxl-vae-fp16-fix\"\n\naccelerate launch train_dreambooth_lora_sdxl_advanced.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_model_name_or_path=$VAE_PATH \\\n  --dataset_name=$DATASET_NAME \\\n  --instance_prompt=\"3d icon in the style of TOK\" \\\n  --validation_prompt=\"a TOK icon of an astronaut riding a horse, in the style of TOK\" \\\n  --output_dir=$OUTPUT_DIR \\\n  --caption_column=\"prompt\" \\\n  --mixed_precision=\"bf16\" \\\n  --resolution=1024 \\\n  --train_batch_size=3 \\\n  --repeats=1 \\\n  --report_to=\"wandb\"\\\n  --gradient_accumulation_steps=1 \\\n  --gradient_checkpointing \\\n  --learning_rate=1.0 \\\n  --text_encoder_lr=1.0 \\\n  --optimizer=\"prodigy\"\\\n  --train_text_encoder_ti\\\n  --train_text_encoder_ti_frac=0.5\\\n  --snr_gamma=5.0 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --rank=8 \\\n  --max_train_steps=1000 \\\n  --checkpointing_steps=2000 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Generating Identity-Preserving Video from a Face Image and Text Prompt with ConsisID\nDESCRIPTION: This code demonstrates how to use ConsisID to generate a video that preserves the identity from an input face image while following a text prompt. It processes face embeddings from the input image, passes them to the diffusion pipeline along with the prompt, and exports the resulting frames as a video file. The example uses a specific random seed for reproducible results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/consisid.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import export_to_video\n\nprompt = \"The video captures a boy walking along a city street, filmed in black and white on a classic 35mm camera. His expression is thoughtful, his brow slightly furrowed as if he's lost in contemplation. The film grain adds a textured, timeless quality to the image, evoking a sense of nostalgia. Around him, the cityscape is filled with vintage buildings, cobblestone sidewalks, and softly blurred figures passing by, their outlines faint and indistinct. Streetlights cast a gentle glow, while shadows play across the boy's path, adding depth to the scene. The lighting highlights the boy's subtle smile, hinting at a fleeting moment of curiosity. The overall cinematic atmosphere, complete with classic film still aesthetics and dramatic contrasts, gives the scene an evocative and introspective feel.\"\nimage = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/consisid/consisid_input.png?download=true\"\n\nid_cond, id_vit_hidden, image, face_kps = process_face_embeddings_infer(face_helper_1, face_clip_model, face_helper_2, eva_transform_mean, eva_transform_std, face_main_model, \"cuda\", torch.bfloat16, image, is_align_face=True)\n\nvideo = pipe(image=image, prompt=prompt, num_inference_steps=50, guidance_scale=6.0, use_dynamic_cfg=False, id_vit_hidden=id_vit_hidden, id_cond=id_cond, kps_cond=face_kps, generator=torch.Generator(\"cuda\").manual_seed(42))\nexport_to_video(video.frames[0], \"output.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Using Tiny AutoEncoder for Stable Diffusion 3 in Python\nDESCRIPTION: This snippet demonstrates how to use the Tiny AutoEncoder for Stable Diffusion 3 (TAESD3) to decode latents almost instantly in the StableDiffusion3Pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline, AutoencoderTiny\n\npipe = StableDiffusion3Pipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16\n)\npipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd3\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"slice of delicious New York-style berry cheesecake\"\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage.save(\"cheesecake.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading an Image for Image-to-Image Generation\nDESCRIPTION: Code snippet for loading an image using the diffusers utility function, which is then used as the conditioning image for image-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image\n```\n\n----------------------------------------\n\nTITLE: Loading SDE Drag Pipeline for Drag-and-Drop Image Editing\nDESCRIPTION: This code initializes the SDE Drag pipeline, which enables drag-and-drop editing of images using stochastic differential equations. It loads the base Stable Diffusion model with a DDIM scheduler and custom pipeline configuration, then sets up utility functions for loading images from URLs and preparing masks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_92\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DDIMScheduler, DiffusionPipeline\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport numpy as np\n\n# Load the pipeline\nmodel_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nscheduler = DDIMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\npipe = DiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, custom_pipeline=\"sde_drag\")\n\n# Ensure the model is moved to the GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipe.to(device)\n\n# Function to load image from URL\ndef load_image_from_url(url):\n    response = requests.get(url)\n    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n# Function to prepare mask\ndef prepare_mask(mask_image):\n    # Convert to grayscale\n    mask = mask_image.convert(\"L\")\n    return mask\n```\n\n----------------------------------------\n\nTITLE: Implementing Image-to-Image Inpainting with Stable Diffusion\nDESCRIPTION: This code implements an image-to-image inpainting pipeline based on Stable Diffusion. It allows blending of two images by masking the boundary between them and using Stable Diffusion to create a seamless connection. The pipeline takes an outer image, an inner image with transparency, and a mask to guide the inpainting process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom diffusers import DiffusionPipeline\n\nimage_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\ninner_image_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ndef load_image(url, mode=\"RGB\"):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return Image.open(BytesIO(response.content)).convert(mode).resize((512, 512))\n    else:\n        raise FileNotFoundError(f\"Could not retrieve image from {url}\")\n\n\ninit_image = load_image(image_url, mode=\"RGB\")\ninner_image = load_image(inner_image_url, mode=\"RGBA\")\nmask_image = load_image(mask_url, mode=\"RGB\")\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-inpainting\",\n    custom_pipeline=\"img2img_inpainting\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a mecha robot sitting on a bench\"\nimage = pipe(prompt=prompt, image=init_image, inner_image=inner_image, mask_image=mask_image).images[0]\n\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Face Embeddings for IP-Adapter FaceID Plus Models in Python\nDESCRIPTION: This snippet demonstrates how to prepare face embeddings for IP-Adapter FaceID Plus and Plus v2 models. It includes face alignment, embedding extraction, and CLIP embedding preparation for the hidden image projection layers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom insightface.utils import face_align\n\nref_images_embeds = []\nip_adapter_images = []\napp = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\nimage = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\nfaces = app.get(image)\nip_adapter_images.append(face_align.norm_crop(image, landmark=faces[0].kps, image_size=224))\nimage = torch.from_numpy(faces[0].normed_embedding)\nref_images_embeds.append(image.unsqueeze(0))\nref_images_embeds = torch.stack(ref_images_embeds, dim=0).unsqueeze(0)\nneg_ref_images_embeds = torch.zeros_like(ref_images_embeds)\nid_embeds = torch.cat([neg_ref_images_embeds, ref_images_embeds]).to(dtype=torch.float16, device=\"cuda\")\n\nclip_embeds = pipeline.prepare_ip_adapter_image_embeds(\n  [ip_adapter_images], None, torch.device(\"cuda\"), num_images, True)[0]\n\npipeline.unet.encoder_hid_proj.image_projection_layers[0].clip_embeds = clip_embeds.to(dtype=torch.float16)\npipeline.unet.encoder_hid_proj.image_projection_layers[0].shortcut = False # True if Plus v2\n```\n\n----------------------------------------\n\nTITLE: Loading ControlNet Models and Initializing SDXL Pipeline in Python\nDESCRIPTION: This code loads multiple ControlNet models, sets up the StableDiffusionXLControlNetPipeline with a custom VAE, and configures the scheduler. It also enables model CPU offloading to reduce memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler\nimport torch\n\ncontrolnets = [\n    ControlNetModel.from_pretrained(\n        \"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16\n    ),\n    ControlNetModel.from_pretrained(\n        \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True\n    ),\n]\n\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnets, vae=vae, torch_dtype=torch.float16, use_safetensors=True\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Implementing Inpainting with AutoPipelineForInpainting\nDESCRIPTION: Shows how to use AutoPipelineForInpainting to selectively modify parts of an image based on a mask. This example replaces a character in the image with an owl using the Stable Diffusion XL model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-mask.png\")\n\nprompt = \"cinematic photo of a owl, 35mm photograph, film, professional, 4k, highly detailed\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(38)\nimage = pipeline(prompt, image=init_image, mask_image=mask_image, generator=generator, strength=0.4).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Video Generation with IP-Adapter and AnimateDiff\nDESCRIPTION: Shows how to generate animated videos using IP-Adapter with AnimateDiff. Includes setup of motion adapter and configuration for video generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\nfrom diffusers.utils import load_image\n\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\npipeline = AnimateDiffPipeline.from_pretrained(\"emilianJR/epiCRealism\", motion_adapter=adapter, torch_dtype=torch.float16)\nscheduler = DDIMScheduler.from_pretrained(\n    \"emilianJR/epiCRealism\",\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipeline.scheduler = scheduler\npipeline.enable_vae_slicing()\n\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\npipeline.enable_model_cpu_offload()\n```\n\nLANGUAGE: python\nCODE:\n```\nip_adapter_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_inpaint.png\")\n\noutput = pipeline(\n    prompt=\"A cute gummy bear waving\",\n    negative_prompt=\"bad quality, worse quality, low resolution\",\n    ip_adapter_image=ip_adapter_image,\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=50,\n    generator=torch.Generator(device=\"cpu\").manual_seed(0),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"gummy_bear.gif\")\n```\n\n----------------------------------------\n\nTITLE: Using IPEX-Optimized Stable Diffusion XL Pipeline in Python\nDESCRIPTION: This code shows how to use the IPEX-optimized Stable Diffusion XL pipeline for image generation. It includes examples for both Float32 and BFloat16 data types, with the latter using torch.cpu.amp.autocast for mixed precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# value of image height/width should be consistent with 'prepare_for_ipex()'\n# For Float32\nimage = pipe(prompt, num_inference_steps=num_inference_steps, height=512, width=512, guidance_scale=guidance_scale).images[0]\n# For BFloat16\nwith torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n    image = pipe(prompt, num_inference_steps=num_inference_steps, height=512, width=512, guidance_scale=guidance_scale).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loss Calculation for LCM Training\nDESCRIPTION: Calculates the loss between the model's predictions and target values. Supports both L2 (mean squared error) and Huber loss types, with the latter providing more robustness to outliers through a configurable parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nif args.loss_type == \"l2\":\n    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\nelif args.loss_type == \"huber\":\n    loss = torch.mean(\n        torch.sqrt((model_pred.float() - target.float()) ** 2 + args.huber_c**2) - args.huber_c\n    )\n```\n\n----------------------------------------\n\nTITLE: ControlNet Inpainting Generation in Python\nDESCRIPTION: This snippet demonstrates how to generate an image using the ControlNet inpainting pipeline. It uses the previously set up pipeline and control image to create a new image based on a given prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, control_image=control_image).images[0]\nmake_image_grid([init_image, mask_image, PIL.Image.fromarray(np.uint8(control_image[0][0])).convert('RGB'), image], rows=2, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Using AutoPipelineForText2Image with Kandinsky 2.1\nDESCRIPTION: Implementation of text-to-image generation using the AutoPipelineForText2Image API, which automatically handles the combined prior and decoder pipeline for Kandinsky 2.1.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, guidance_scale=4.0, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Inpainting with IP-Adapter and SDXL\nDESCRIPTION: Demonstrates inpainting capabilities using IP-Adapter with SDXL. The code handles original image, mask image, and IP-Adapter image prompt to perform targeted image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForInpainting.from_pretrained(\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\npipeline.set_ip_adapter_scale(0.6)\n```\n\nLANGUAGE: python\nCODE:\n```\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_mask.png\")\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_bear_1.png\")\nip_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_gummy.png\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(4)\nimages = pipeline(\n    prompt=\"a cute gummy bear waving\",\n    image=image,\n    mask_image=mask_image,\n    ip_adapter_image=ip_image,\n    generator=generator,\n    num_inference_steps=100,\n).images\nimages[0]\n```\n\n----------------------------------------\n\nTITLE: LCM Noise Prediction and Model Output Calculation\nDESCRIPTION: Core part of the LCM distillation process that predicts the original image from noise. It uses the predicted noise and scheduling parameters to reconstruct the original image, which is then used for calculating the model output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npred_x_0 = predicted_origin(\n    noise_pred,\n    start_timesteps,\n    noisy_model_input,\n    noise_scheduler.config.prediction_type,\n    alpha_schedule,\n    sigma_schedule,\n)\n\nmodel_pred = c_skip_start * noisy_model_input + c_out_start * pred_x_0\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Size Parameters\nDESCRIPTION: Demonstrates how to customize output image dimensions using height and width parameters in the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\nimage = pipeline(\n\t\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", height=768, width=512\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Generating text embeddings for Stable Diffusion in Python\nDESCRIPTION: Creates text embeddings from a prompt for use in the Stable Diffusion model, including both conditional and unconditional embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> prompt = [\"a photograph of an astronaut riding a horse\"]\n>>> height = 512  # default height of Stable Diffusion\n>>> width = 512  # default width of Stable Diffusion\n>>> num_inference_steps = 25  # Number of denoising steps\n>>> guidance_scale = 7.5  # Scale for classifier-free guidance\n>>> generator = torch.manual_seed(0)  # Seed generator to create the initial latent noise\n>>> batch_size = len(prompt)\n\n>>> text_input = tokenizer(\n...     prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n\n>>> max_length = text_input.input_ids.shape[-1]\n>>> uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n>>> uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n\n>>> text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n```\n\n----------------------------------------\n\nTITLE: Accelerating Inference in Python with xDiT\nDESCRIPTION: This Python snippet demonstrates how to initialize the xDiT framework for accelerating inference of a Diffusers model. It sets up command-line arguments, configures the model, and executes the inference pipeline with specified parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/xdit.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\n\nfrom xfuser import xFuserArgs, xDiTParallel\nfrom xfuser.config import FlexibleArgumentParser\nfrom xfuser.core.distributed import get_world_group\n\ndef main():\n    parser = FlexibleArgumentParser(description=\"xFuser Arguments\")\n    args = xFuserArgs.add_cli_args(parser).parse_args()\n    engine_args = xFuserArgs.from_cli_args(args)\n    engine_config, input_config = engine_args.create_config()\n\n    local_rank = get_world_group().local_rank\n    pipe = StableDiffusion3Pipeline.from_pretrained(\n        pretrained_model_name_or_path=engine_config.model_config.model,\n        torch_dtype=torch.float16,\n    ).to(f\"cuda:{local_rank}\")\n    \n    # do anything you want with pipeline here\n\n    pipe = xDiTParallel(pipe, engine_config, input_config)\n\n    pipe(\n        height=input_config.height,\n        width=input_config.height,\n        prompt=input_config.prompt,\n        num_inference_steps=input_config.num_inference_steps,\n        output_type=input_config.output_type,\n        generator=torch.Generator(device=\"cuda\").manual_seed(input_config.seed),\n    )\n\n    if input_config.output_type == \"pil\":\n        pipe.save(\"results\", \"stable_diffusion_3\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Initializing Inpainting Pipeline and Loading Base Images in Python\nDESCRIPTION: This code sets up an inpainting pipeline using Stable Diffusion and loads the base image and mask. It performs inpainting to add an elven castle to the masked area and resizes the result for further processing with SDXL.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting, AutoPipelineForImage2Image\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage_inpainting = pipeline(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n\n# resize image to 1024x1024 for SDXL\nimage_inpainting = image_inpainting.resize((1024, 1024))\n```\n\n----------------------------------------\n\nTITLE: FastAPI Endpoint for Image Generation\nDESCRIPTION: Implementation of the POST endpoint that handles image generation requests. It creates a thread-safe pipeline instance for each request and processes the prompt asynchronously.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/server/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@app.post(\"/v1/images/generations\")\nasync def generate_image(image_input: TextToImageInput):\n    try:\n        loop = asyncio.get_event_loop()\n        scheduler = shared_pipeline.pipeline.scheduler.from_config(shared_pipeline.pipeline.scheduler.config)\n        pipeline = StableDiffusion3Pipeline.from_pipe(shared_pipeline.pipeline, scheduler=scheduler)\n        generator = torch.Generator(device=\"cuda\")\n        generator.manual_seed(random.randint(0, 10000000))\n        output = await loop.run_in_executor(None, lambda: pipeline(image_input.prompt, generator = generator))\n        logger.info(f\"output: {output}\")\n        image_url = save_image(output.images[0])\n        return {\"data\": [{\"url\": image_url}]}\n    except Exception as e:\n        if isinstance(e, HTTPException):\n            raise e\n        elif hasattr(e, 'message'):\n            raise HTTPException(status_code=500, detail=e.message + traceback.format_exc())\n        raise HTTPException(status_code=500, detail=str(e) + traceback.format_exc())\n```\n\n----------------------------------------\n\nTITLE: Postprocessing and Displaying Generated Image\nDESCRIPTION: Create a utility function to convert and display the generated image from tensor to PIL Image\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport PIL.Image\nimport numpy as np\n\ndef display_sample(sample, i):\n    image_processed = sample.cpu().permute(0, 2, 3, 1)\n    image_processed = (image_processed + 1.0) * 127.5\n    image_processed = image_processed.numpy().astype(np.uint8)\n\n    image_pil = PIL.Image.fromarray(image_processed[0])\n    display(f\"Image at step {i}\")\n    display(image_pil)\n```\n\n----------------------------------------\n\nTITLE: Initializing CogVideoX Pipeline in Python\nDESCRIPTION: Loads the CogVideoX pipeline for text-to-video generation and sets up memory layout for improved performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/cogvideox.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXPipeline, CogVideoXImageToVideoPipeline\nfrom diffusers.utils import export_to_video,load_image\npipe = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-5b\").to(\"cuda\") # or \"THUDM/CogVideoX-2b\" \n\npipe.transformer.to(memory_format=torch.channels_last)\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained DreamBooth Model\nDESCRIPTION: Demonstrates how to use the trained DreamBooth model for inference using the StableDiffusionPipeline. It loads the model, generates an image based on a prompt, and saves the result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/colossalai/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"path-to-save-model\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A photo of sks dog in a bucket\"\nimage = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n\nimage.save(\"dog-bucket.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating Faces with IP-Adapter in Python\nDESCRIPTION: This snippet demonstrates how to use an IP-Adapter face model with StableDiffusionPipeline to generate an image of Einstein as a chef. It loads a pre-trained model, sets up the scheduler, and applies the IP-Adapter with a reference image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\nfrom diffusers.utils import load_image\n\npipeline = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter-full-face_sd15.bin\")\n\npipeline.set_ip_adapter_scale(0.5)\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_einstein_base.png\")\ngenerator = torch.Generator(device=\"cpu\").manual_seed(26)\n\nimage = pipeline(\n    prompt=\"A photo of Einstein as a chef, wearing an apron, cooking in a French restaurant\",\n    ip_adapter_image=image,\n    negative_prompt=\"lowres, bad anatomy, worst quality, low quality\",\n    num_inference_steps=100,\n    generator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained LCM Model using DiffusionPipeline\nDESCRIPTION: Python code to load a trained LCM model and use it for inference. It loads the UNet from a custom model, initializes a diffusion pipeline with it, configures the LCM scheduler, and generates an image from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel, DiffusionPipeline, LCMScheduler\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\"your-username/your-model\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\")\n\npipeline.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipeline.to(\"cuda\")\n\nprompt = \"sushi rolls in the form of panda heads, sushi platter\"\n\nimage = pipeline(prompt, num_inference_steps=4, guidance_scale=1.0).images[0]\n```\n\n----------------------------------------\n\nTITLE: Generating Images with ControlNet and PAG in Python\nDESCRIPTION: This code snippet shows how to use a pipeline with ControlNet and PAG to generate images. It demonstrates setting different PAG scales and using a control image for conditional generation. The example generates images without a prompt to showcase PAG's effectiveness in unconditional generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image\ncanny_image = load_image(\n    \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/pag_control_input.png\"\n)\n\nfor pag_scale in [0.0, 3.0]:\n    generator = torch.Generator(device=\"cpu\").manual_seed(1)\n    images = pipeline(\n        prompt=\"\",\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        image=canny_image,\n        num_inference_steps=50,\n        guidance_scale=0,\n        generator=generator,\n        pag_scale=pag_scale,\n    ).images\n    images[0]\n```\n\n----------------------------------------\n\nTITLE: Optimizing Consistency Model Pipeline with torch.compile\nDESCRIPTION: Example showing how to load and optimize a Consistency Model Pipeline using torch.compile for faster image generation. The code demonstrates loading a pre-trained model, enabling GPU acceleration, and applying torch compilation for improved performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/consistency_models.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import ConsistencyModelPipeline\n\ndevice = \"cuda\"\n# Load the cd_bedroom256_lpips checkpoint.\nmodel_id_or_path = \"openai/diffusers-cd_bedroom256_lpips\"\npipe = ConsistencyModelPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\n# Multistep sampling\n# Timesteps can be explicitly specified; the particular timesteps below are from the original GitHub repo:\n# https://github.com/openai/consistency_models/blob/main/scripts/launch.sh#L83\nfor _ in range(10):\n    image = pipe(timesteps=[17, 0]).images[0]\n    image.show()\n```\n\n----------------------------------------\n\nTITLE: Using FaithDiff Stable Diffusion XL Pipeline for Image Upscaling and Restoration in Python\nDESCRIPTION: This code demonstrates using the FaithDiff pipeline with Stable Diffusion XL for image upscaling and restoration. It loads a low-quality image, applies 2x upscaling, and enhances the image quality using a text prompt for guidance. The implementation includes custom model initialization, loading of additional layers, and optimizations like VAE tiling and model CPU offloading.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_113\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport numpy as np\nimport torch\nfrom diffusers import DiffusionPipeline, AutoencoderKL, UniPCMultistepScheduler\nfrom huggingface_hub import hf_hub_download\nfrom diffusers.utils import load_image\nfrom PIL import Image\n\ndevice = \"cuda\"\ndtype = torch.float16\nMAX_SEED = np.iinfo(np.int32).max\n\n# Download weights for additional unet layers\nmodel_file = hf_hub_download(\n    \"jychen9811/FaithDiff\",\n    filename=\"FaithDiff.bin\", local_dir=\"./proc_data/faithdiff\", local_dir_use_symlinks=False\n)\n\n# Initialize the models and pipeline\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=dtype)\n\nmodel_id = \"SG161222/RealVisXL_V4.0\"\npipe = DiffusionPipeline.from_pretrained(\n    model_id,\n    torch_dtype=dtype,\n    vae=vae,\n    unet=None, #<- Do not load with original model.\n    custom_pipeline=\"pipeline_faithdiff_stable_diffusion_xl\",    \n    use_safetensors=True,\n    variant=\"fp16\",\n).to(device)\n\n# Here we need use pipeline internal unet model\npipe.unet = pipe.unet_model.from_pretrained(model_id, subfolder=\"unet\", variant=\"fp16\", use_safetensors=True)\n\n# Load aditional layers to the model\npipe.unet.load_additional_layers(weight_path=\"proc_data/faithdiff/FaithDiff.bin\", dtype=dtype)\n\n# Enable vae tiling\npipe.set_encoder_tile_settings()\npipe.enable_vae_tiling()\n\n# Optimization\npipe.enable_model_cpu_offload()\n\n# Set selected scheduler\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\n#input params\nprompt = \"The image features a woman in her 55s with blonde hair and a white shirt, smiling at the camera. She appears to be in a good mood and is wearing a white scarf around her neck. \"\nupscale = 2 # scale here\nstart_point = \"lr\" # or \"noise\"\nlatent_tiled_overlap = 0.5\nlatent_tiled_size = 1024\n\n# Load image\nlq_image = load_image(\"https://huggingface.co/datasets/DEVAIEXP/assets/resolve/main/woman.png\")\noriginal_height = lq_image.height\noriginal_width = lq_image.width\nprint(f\"Current resolution: H:{original_height} x W:{original_width}\")\n\nwidth = original_width * int(upscale)\nheight = original_height * int(upscale)\nprint(f\"Final resolution: H:{height} x W:{width}\")\n\n# Restoration\nimage = lq_image.resize((width, height), Image.LANCZOS)\ninput_image, width_init, height_init, width_now, height_now = pipe.check_image_size(image)\n\ngenerator = torch.Generator(device=device).manual_seed(random.randint(0, MAX_SEED))\ngen_image = pipe(lr_img=input_image, \n                 prompt = prompt,                  \n                 num_inference_steps=20, \n                 guidance_scale=5, \n                 generator=generator, \n                 start_point=start_point, \n                 height = height_now, \n                 width=width_now, \n                 overlap=latent_tiled_overlap, \n                 target_size=(latent_tiled_size, latent_tiled_size)\n                ).images[0]\n\ncropped_image = gen_image.crop((0, 0, width_init, height_init))\ncropped_image.save(\"data/result.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with SDXL ControlNet - Python\nDESCRIPTION: Generates images using the SDXL ControlNet pipeline with specified prompts and conditioning scale. Sets up proper parameters for image generation with canny edge detection guidance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\nnegative_prompt = 'low quality, bad quality, sketches'\n\nimage = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    controlnet_conditioning_scale=0.5,\n).images[0]\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Applying FreeU to Stable Diffusion v2-1 in Python\nDESCRIPTION: This code applies FreeU to the Stable Diffusion v2-1 model. It initializes the pipeline, enables FreeU with specific parameters, and generates an image of a squirrel eating a burger.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/image_quality.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16, safety_checker=None\n).to(\"cuda\")\npipeline.enable_freeu(s1=0.9, s2=0.2, b1=1.4, b2=1.6)\ngenerator = torch.Generator(device=\"cpu\").manual_seed(80)\nprompt = \"A squirrel eating a burger\"\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implementing AnimateDiff with SDXL Models\nDESCRIPTION: Demonstrates how to use AnimateDiff with SDXL models. This experimental feature utilizes a beta release of the motion adapter checkpoint for high-resolution animation generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers.models import MotionAdapter\nfrom diffusers import AnimateDiffSDXLPipeline, DDIMScheduler\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-sdxl-beta\", torch_dtype=torch.float16)\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipe = AnimateDiffSDXLPipeline.from_pretrained(\n    model_id,\n    motion_adapter=adapter,\n    scheduler=scheduler,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_vae_tiling()\n\noutput = pipe(\n    prompt=\"a panda surfing in the ocean, realistic, high quality\",\n    negative_prompt=\"low quality, worst quality\",\n    num_inference_steps=20,\n    guidance_scale=8,\n    width=1024,\n    height=1024,\n    num_frames=16,\n)\n\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Using HD-Painter High-Resolution Inpainting Pipeline in Python\nDESCRIPTION: Initialization and usage of the HD-Painter pipeline for high-resolution image inpainting. It demonstrates how to load the pipeline with custom components, set up a DDIM scheduler, and apply the inpainting with Prompt-Aware Introverted Attention (PAIntA) and Reweighting Attention Score Guidance (RASG) features.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, DDIMScheduler\nfrom diffusers.utils import load_image\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5-inpainting\",\n    custom_pipeline=\"hd_painter\"\n)\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hd-painter.jpg\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hd-painter-mask.png\")\nprompt = \"football\"\nimage = pipeline(prompt, init_image, mask_image, use_rasg=True, use_painta=True, generator=torch.manual_seed(0)).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Intrinsic Image Decomposition - Lighting Model\nDESCRIPTION: Demonstrates using the Marigold Lighting model for intrinsic image decomposition to predict albedo, shading, and residual maps from an input image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\n\npipe = diffusers.MarigoldIntrinsicsPipeline.from_pretrained(\n    \"prs-eth/marigold-iid-lighting-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(\"cuda\")\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\nintrinsics = pipe(image)\n\nvis = pipe.image_processor.visualize_intrinsics(intrinsics.prediction, pipe.target_properties)\nvis[0][\"albedo\"].save(\"einstein_albedo.png\")\nvis[0][\"shading\"].save(\"einstein_shading.png\")\nvis[0][\"residual\"].save(\"einstein_residual.png\")\n```\n\n----------------------------------------\n\nTITLE: Using Regional Prompting Pipeline with Stable Diffusion in Python\nDESCRIPTION: Demonstrates how to use the RegionalPromptingStableDiffusionPipeline to create images with different prompts applied to specific regions. This example divides the image into three horizontal rows and applies different style prompts to each section.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_81\n\nLANGUAGE: python\nCODE:\n```\nfrom examples.community.regional_prompting_stable_diffusion import RegionalPromptingStableDiffusionPipeline\n\npipe = RegionalPromptingStableDiffusionPipeline.from_single_file(model_path, vae=vae)\n\nrp_args = {\n    \"mode\":\"rows\",\n    \"div\": \"1;1;1\"\n}\n\nprompt = \"\"\"\ngreen hair twintail BREAK\nred blouse BREAK\nblue skirt\n\"\"\"\n\nimages = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=7.5,\n    height=768,\n    width=512,\n    num_inference_steps=20,\n    num_images_per_prompt=1,\n    rp_args=rp_args\n    ).images\n\ntime = time.strftime(r\"%Y%m%d%H%M%S\")\ni = 1\nfor image in images:\n    i += 1\n    fileName = f'img-{time}-{i+1}.png'\n    image.save(fileName)\n```\n\n----------------------------------------\n\nTITLE: Using ControlNet for Controlled Image Generation in Python\nDESCRIPTION: This snippet shows how to use ControlNet for controlled image generation. It uses a depth map as a conditioning image to preserve spatial information in the generated image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image, make_image_grid\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\ninit_image = init_image.resize((958, 960)) # resize to depth image dimensions\ndepth_image = load_image(\"https://huggingface.co/lllyasviel/control_v11f1p_sd15_depth/resolve/main/images/control.png\")\nmake_image_grid([init_image, depth_image], rows=1, cols=2)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import ControlNetModel, AutoPipelineForImage2Image\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_depth\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n```\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage_control_net = pipeline(prompt, image=init_image, control_image=depth_image).images[0]\nmake_image_grid([init_image, depth_image, image_control_net], rows=1, cols=3)\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"nitrosocke/elden-ring-diffusion\", torch_dtype=torch.float16,\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nprompt = \"elden ring style astronaut in a jungle\" # include the token \"elden ring style\" in the prompt\nnegative_prompt = \"ugly, deformed, disfigured, poor details, bad anatomy\"\n\nimage_elden_ring = pipeline(prompt, negative_prompt=negative_prompt, image=image_control_net, strength=0.45, guidance_scale=10.5).images[0]\nmake_image_grid([init_image, depth_image, image_control_net, image_elden_ring], rows=2, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Running Inference with IP-Adapter-Enhanced LCM\nDESCRIPTION: Demonstrates how to use a previously configured diffusion pipeline with IP-Adapter to generate images. The code sets the IP-Adapter scale, defines a prompt using the special Herge style token, and processes an input image to guide the generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npipeline.set_ip_adapter_scale(0.4)\n\nprompt = \"herge_style woman in armor, best quality, high quality\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\n\nip_adapter_image = load_image(\"https://user-images.githubusercontent.com/24734142/266492875-2d50d223-8475-44f0-a7c6-08b51cb53572.png\")\nimage = pipeline(\n    prompt=prompt,\n    ip_adapter_image=ip_adapter_image,\n    num_inference_steps=4,\n    guidance_scale=1,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Initializing Quantized EasyAnimate Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to load a quantized EasyAnimatePipeline for inference using bitsandbytes. It initializes the model with 8-bit quantization and generates a video from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/easyanimate.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, EasyAnimateTransformer3DModel, EasyAnimatePipeline\nfrom diffusers.utils import export_to_video\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = EasyAnimateTransformer3DModel.from_pretrained(\n    \"alibaba-pai/EasyAnimateV5.1-12b-zh\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = EasyAnimatePipeline.from_pretrained(\n    \"alibaba-pai/EasyAnimateV5.1-12b-zh\",\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"A cat walks on the grass, realistic style.\"\nnegative_prompt = \"bad detailed\"\nvideo = pipeline(prompt=prompt, negative_prompt=negative_prompt, num_frames=49, num_inference_steps=30).frames[0]\nexport_to_video(video, \"cat.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Running Optimized FLUX Inference with Torchrun on Multiple GPUs\nDESCRIPTION: A Bash command for running the `run_flux.py` script with `torchrun` to utilize multiple GPUs, enhancing the speed of FLUX image generation inference. The command specifies the number of GPUs employed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/para_attn.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n\"torchrun --nproc_per_node=2 run_flux.py\"\n```\n\n----------------------------------------\n\nTITLE: Generating Images with MultiAdapter and Stable Diffusion\nDESCRIPTION: Code to generate an image using the StableDiffusionAdapterPipeline with MultiAdapter. The pipeline takes a text prompt and multiple conditioning images (pose and depth), with adapter_conditioning_scale controlling the weight of each adapter's influence on the final output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline = StableDiffusionAdapterPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    torch_dtype=torch.float16,\n    adapter=adapters,\n).to(\"cuda\")\n\nimage = pipeline(prompt, cond, adapter_conditioning_scale=[0.7, 0.7]).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implement Min-SNR Weighting Strategy - Bash\nDESCRIPTION: Add the --snr_gamma parameter to utilize the Min-SNR weighting strategy for targeting faster convergence during training, recommended with a value of 5.0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_text_to_image.py \\\n  --snr_gamma=5.0\n```\n\n----------------------------------------\n\nTITLE: Generating Images with ControlNet Image-to-Image Pipeline\nDESCRIPTION: Code to execute the image-to-image generation pipeline with a ControlNet model, providing a text prompt, initial image, and depth map as conditioning input.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noutput = pipe(\n    \"lego batman and robin\", image=image, control_image=depth_map,\n).images[0]\nmake_image_grid([image, output], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Generating Image Embeddings for ControlNet\nDESCRIPTION: Creates image embeddings from text prompts using the Kandinsky 2.2 Prior Pipeline. The code includes a detailed negative prompt to improve the quality of generation by explicitly avoiding common artifacts and issues.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A robot, 4k photo\"\nnegative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(43)\n\nimage_emb, zero_image_emb = prior_pipeline(\n    prompt=prompt, negative_prompt=negative_prior_prompt, generator=generator\n).to_tuple()\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Turbo from Single File Checkpoint\nDESCRIPTION: Demonstrates loading the SDXL Turbo model from a single file checkpoint (.safetensors) and configuring the scheduler. This method is useful for models stored in a single file format.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\",\n    torch_dtype=torch.float16, variant=\"fp16\")\npipeline = pipeline.to(\"cuda\")\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config, timestep_spacing=\"trailing\")\n```\n\n----------------------------------------\n\nTITLE: Using AnimateDiff with SparseCtrl RGB\nDESCRIPTION: Implements video generation using AnimateDiff with sparse RGB control. Uses pretrained models for motion adaptation, control, and VAE along with a specialized scheduler for animation generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import AnimateDiffSparseControlNetPipeline\nfrom diffusers.models import AutoencoderKL, MotionAdapter, SparseControlNetModel\nfrom diffusers.schedulers import DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_gif, load_image\n\n\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\nmotion_adapter_id = \"guoyww/animatediff-motion-adapter-v1-5-3\"\ncontrolnet_id = \"guoyww/animatediff-sparsectrl-rgb\"\nlora_adapter_id = \"guoyww/animatediff-motion-lora-v1-5-3\"\nvae_id = \"stabilityai/sd-vae-ft-mse\"\ndevice = \"cuda\"\n\nmotion_adapter = MotionAdapter.from_pretrained(motion_adapter_id, torch_dtype=torch.float16).to(device)\ncontrolnet = SparseControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16).to(device)\nvae = AutoencoderKL.from_pretrained(vae_id, torch_dtype=torch.float16).to(device)\nscheduler = DPMSolverMultistepScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    beta_schedule=\"linear\",\n    algorithm_type=\"dpmsolver++\",\n    use_karras_sigmas=True,\n)\npipe = AnimateDiffSparseControlNetPipeline.from_pretrained(\n    model_id,\n    motion_adapter=motion_adapter,\n    controlnet=controlnet,\n    vae=vae,\n    scheduler=scheduler,\n    torch_dtype=torch.float16,\n).to(device)\npipe.load_lora_weights(lora_adapter_id, adapter_name=\"motion_lora\")\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-firework.png\")\n\nvideo = pipe(\n    prompt=\"closeup face photo of man in black clothes, night city street, bokeh, fireworks in background\",\n    negative_prompt=\"low quality, worst quality\",\n    num_inference_steps=25,\n    conditioning_frames=image,\n    controlnet_frame_indices=[0],\n    controlnet_conditioning_scale=1.0,\n    generator=torch.Generator().manual_seed(42),\n).frames[0]\nexport_to_gif(video, \"output.gif\")\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Generation with IP-Adapter and SDXL\nDESCRIPTION: Implements image-to-image generation using IP-Adapter and SDXL. The pipeline takes an original image and an IP-Adapter image prompt to generate a new image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\npipeline.set_ip_adapter_scale(0.6)\n```\n\nLANGUAGE: python\nCODE:\n```\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_bear_1.png\")\nip_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_bear_2.png\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(4)\nimages = pipeline(\n    prompt=\"best quality, high quality\",\n    image=image,\n    ip_adapter_image=ip_image,\n    generator=generator,\n    strength=0.6,\n).images\nimages[0]\n```\n\n----------------------------------------\n\nTITLE: Setting up Optimizer and Learning Rate Scheduler for Diffusion Model Training\nDESCRIPTION: Initializes the AdamW optimizer for model parameters and configures a cosine learning rate scheduler with warmup steps for training a diffusion model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.optimization import get_cosine_schedule_with_warmup\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\nlr_scheduler = get_cosine_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=config.lr_warmup_steps,\n    num_training_steps=(len(train_dataloader) * config.num_epochs),\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Video Frames with Stable Diffusion and ControlNet in Python\nDESCRIPTION: This snippet uses the initialized models and parameters to generate new video frames based on the input frames and control images. It then saves the output as a video file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_107\n\nLANGUAGE: python\nCODE:\n```\noutput_frames = pipe(\n    prompt + a_prompt,\n    frames,\n    control_frames,\n    num_inference_steps=20,\n    strength=0.75,\n    controlnet_conditioning_scale=0.7,\n    generator=generator,\n    negative_prompt=n_prompt\n).images\n\noutput_frames[0].save(output_video_path, save_all=True,\n                 append_images=output_frames[1:], duration=100, loop=0)\n```\n\n----------------------------------------\n\nTITLE: Loading Base and Refiner Models for Ensemble Denoising\nDESCRIPTION: Initializes the SDXL base and refiner models for use as an ensemble of expert denoisers. The setup includes loading pre-trained models with specific configurations for optimal performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nbase = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n\nrefiner = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading ConsisID Model Checkpoints with Face Processing Models in Python\nDESCRIPTION: Demonstrates how to load the ConsisID pipeline and necessary face processing models. The code downloads the model checkpoints, prepares face helper models for preprocessing input face images, and initializes the ConsisID pipeline with bfloat16 precision on a CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/consisid.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# !pip install consisid_eva_clip insightface facexlib\nimport torch\nfrom diffusers import ConsisIDPipeline\nfrom diffusers.pipelines.consisid.consisid_utils import prepare_face_models, process_face_embeddings_infer\nfrom huggingface_hub import snapshot_download\n\n# Download ckpts\nsnapshot_download(repo_id=\"BestWishYsh/ConsisID-preview\", local_dir=\"BestWishYsh/ConsisID-preview\")\n\n# Load face helper model to preprocess input face image\nface_helper_1, face_helper_2, face_clip_model, face_main_model, eva_transform_mean, eva_transform_std = prepare_face_models(\"BestWishYsh/ConsisID-preview\", device=\"cuda\", dtype=torch.bfloat16)\n\n# Load consisid base model\npipe = ConsisIDPipeline.from_pretrained(\"BestWishYsh/ConsisID-preview\", torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionControlNetXSPipeline in Python\nDESCRIPTION: This snippet shows how to import the StableDiffusionControlNetXSPipeline class from the Diffusers library. This pipeline is used for generating images with ControlNet-XS and Stable Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnetxs.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetXSPipeline\n```\n\n----------------------------------------\n\nTITLE: Loading IP-Adapter with LCM-LoRA for Instant Generation\nDESCRIPTION: Sets up a diffusion pipeline with IP-Adapter and LCM-LoRA for fast image generation. The code loads a Herge-style model, applies IP-Adapter weights, and configures the LCM scheduler for efficient generation in as few as 4 steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, LCMScheduler\nimport torch\nfrom diffusers.utils import load_image\n\nmodel_id = \"sd-dreambooth-library/herge-style\"\nlcm_lora_id = \"latent-consistency/lcm-lora-sdv1-5\"\n\npipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\npipeline.load_lora_weights(lcm_lora_id)\npipeline.scheduler = LCMScheduler.from_config(pipeline.scheduler.config)\npipeline.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Generating Image with DiffusionPipeline\nDESCRIPTION: Uses the initialized pipeline to generate an image based on a given prompt. This demonstrates basic image generation functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implementing Canny ControlNet with TCD Scheduler for SDXL\nDESCRIPTION: This code demonstrates how to use the TCD Scheduler with Stable Diffusion XL's Canny ControlNet model. It sets up a pipeline that incorporates TCD-LoRA to achieve high quality outputs in just 4 inference steps with proper conditioning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_tcd_lora.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\nfrom diffusers.utils import load_image, make_image_grid\nfrom scheduling_tcd import TCDScheduler\n\ndevice = \"cuda\"\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\ncontrolnet_id = \"diffusers/controlnet-canny-sdxl-1.0\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n\ncontrolnet = ControlNetModel.from_pretrained(\n    controlnet_id,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    base_model_id,\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe.enable_model_cpu_offload()\n\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\n\nprompt = \"ultrarealistic shot of a furry blue bird\"\n\ncanny_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/bird_canny.png\")\n\ncontrolnet_conditioning_scale = 0.5  # recommended for good generalization\n\nimage = pipe(\n    prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=0,\n    eta=0.3,\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\n    generator=torch.Generator(device=device).manual_seed(0),\n).images[0]\n\ngrid_image = make_image_grid([canny_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Editable Install with Flax\nDESCRIPTION: Performs an editable installation of Diffusers with Flax support\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[flax]\"\n```\n\n----------------------------------------\n\nTITLE: Controlling Guidance Scale in Image-to-Image Generation with Diffusers\nDESCRIPTION: This code shows how to use the guidance_scale parameter to control how closely the generated image aligns with the text prompt. Higher values produce images more aligned with the prompt, while lower values allow more deviation from the prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image, guidance_scale=8.0).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion BoxDiff in Python\nDESCRIPTION: This code demonstrates the use of BoxDiff, a training-free method for controlled generation with bounding box coordinates in Stable Diffusion. It initializes a custom pipeline, defines prompts and bounding boxes, and generates an image with specified object placements.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom PIL import Image, ImageDraw\nfrom copy import deepcopy\n\nfrom examples.community.pipeline_stable_diffusion_boxdiff import StableDiffusionBoxDiffPipeline\n\ndef draw_box_with_text(img, boxes, names):\n    colors = [\"red\", \"olive\", \"blue\", \"green\", \"orange\", \"brown\", \"cyan\", \"purple\"]\n    img_new = deepcopy(img)\n    draw = ImageDraw.Draw(img_new)\n\n    W, H = img.size\n    for bid, box in enumerate(boxes):\n        draw.rectangle([box[0] * W, box[1] * H, box[2] * W, box[3] * H], outline=colors[bid % len(colors)], width=4)\n        draw.text((box[0] * W, box[1] * H), names[bid], fill=colors[bid % len(colors)])\n    return img_new\n\npipe = StableDiffusionBoxDiffPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1-base\",\n    torch_dtype=torch.float16,\n)\npipe.to(\"cuda\")\n\n# example 1\nprompt = \"as the aurora lights up the sky, a herd of reindeer leisurely wanders on the grassy meadow, admiring the breathtaking view, a serene lake quietly reflects the magnificent display, and in the distance, a snow-capped mountain stands majestically, fantasy, 8k, highly detailed\"\nphrases = [\n    \"aurora\",\n    \"reindeer\",\n    \"meadow\",\n    \"lake\",\n    \"mountain\"\n]\nboxes = [[1,3,512,202], [75,344,421,495], [1,327,508,507], [2,217,507,341], [1,135,509,242]]\n\n# example 2\n# prompt = \"A rabbit wearing sunglasses looks very proud\"\n# phrases = [\"rabbit\", \"sunglasses\"]\n# boxes = [[67,87,366,512], [66,130,364,262]]\n\nboxes = [[x / 512 for x in box] for box in boxes]\n\nimages = pipe(\n    prompt,\n    boxdiff_phrases=phrases,\n    boxdiff_boxes=boxes,\n    boxdiff_kwargs={\n        \"attention_res\": 16,\n        \"normalize_eot\": True\n    },\n    num_inference_steps=50,\n    guidance_scale=7.5,\n    generator=torch.manual_seed(42),\n    safety_checker=None\n).images\n\ndraw_box_with_text(images[0], boxes, phrases).save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionControlNetInpaintPipeline in Python\nDESCRIPTION: This snippet shows the import statement for the StableDiffusionControlNetInpaintPipeline class, which is used for inpainting tasks with ControlNet and Stable Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] StableDiffusionControlNetInpaintPipeline\n```\n\n----------------------------------------\n\nTITLE: Loading Models from Single Files in Wan 2.1 - Python\nDESCRIPTION: This snippet illustrates how to load models using the from_single_file method in WanPipeline. It features model checkpoints in original formats and is dependent on PyTorch and the diffusers library. The code demonstrates loading a pretrained model compatible with Wan 2.1 using the specified checkpoint URL, which is critical for maintaining model fidelity during deployments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import WanPipeline, WanTransformer3DModel\n\nckpt_path = \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_t2v_1.3B_bf16.safetensors\"\ntransformer = WanTransformer3DModel.from_single_file(ckpt_path, torch_dtype=torch.bfloat16)\n\npipe = WanPipeline.from_pretrained(\"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", transformer=transformer)\n```\n\n----------------------------------------\n\nTITLE: HunyuanVideo Generation with Remote VAE Decoding in Python\nDESCRIPTION: Shows the complete process of generating a video using the HunyuanVideo model with remote VAE decoding. It includes setting up the pipeline, generating latents, and decoding them into an MP4 video file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_decode.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\n\nmodel_id = \"hunyuanvideo-community/HunyuanVideo\"\ntransformer = HunyuanVideoTransformer3DModel.from_pretrained(\n    model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16\n)\npipe = HunyuanVideoPipeline.from_pretrained(\n    model_id, transformer=transformer, vae=None, torch_dtype=torch.float16\n).to(\"cuda\")\n\nlatent = pipe(\n    prompt=\"A cat walks on the grass, realistic\",\n    height=320,\n    width=512,\n    num_frames=61,\n    num_inference_steps=30,\n    output_type=\"latent\",\n).frames\n\nvideo = remote_decode(\n    endpoint=\"https://o7ywnmrahorts457.us-east-1.aws.endpoints.huggingface.cloud/\",\n    tensor=latent,\n    output_type=\"mp4\",\n)\n\nif isinstance(video, bytes):\n    with open(\"video.mp4\", \"wb\") as f:\n        f.write(video)\n```\n\n----------------------------------------\n\nTITLE: Basic SVD Video Generation Implementation\nDESCRIPTION: Core implementation of Stable Video Diffusion using the SVD-XT model to generate video frames from an input image. Includes model loading, image preprocessing, and video generation with default parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/svd.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import StableVideoDiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = StableVideoDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipe.enable_model_cpu_offload()\n\n# Load the conditioning image\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\nimage = image.resize((1024, 576))\n\ngenerator = torch.manual_seed(42)\nframes = pipe(image, decode_chunk_size=8, generator=generator).frames[0]\n\nexport_to_video(frames, \"generated.mp4\", fps=7)\n```\n\n----------------------------------------\n\nTITLE: Inpainting with SDXL Base Model\nDESCRIPTION: Example of using SDXL for inpainting, where parts of an image are selectively regenerated based on a mask. Uses StableDiffusionXLInpaintPipeline with an initial image and mask.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLInpaintPipeline\nfrom diffusers.utils import load_image\n\npipe = StableDiffusionXLInpaintPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipe.to(\"cuda\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = load_image(img_url).convert(\"RGB\")\nmask_image = load_image(mask_url).convert(\"RGB\")\n\nprompt = \"A majestic tiger sitting on a bench\"\nimage = pipe(prompt=prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]\n```\n\n----------------------------------------\n\nTITLE: Optimizing Stable Diffusion 3 Inference with Torch Compile in Python\nDESCRIPTION: This snippet shows how to use Torch Compile to speed up inference for the Stable Diffusion 3 pipeline by compiling the Transformer and VAE components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\n\ntorch.set_float32_matmul_precision(\"high\")\n\ntorch._inductor.config.conv_1x1_as_mm = True\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.epilogue_fusion = False\ntorch._inductor.config.coordinate_descent_check_all_directions = True\n\npipe = StableDiffusion3Pipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\npipe.set_progress_bar_config(disable=True)\n\npipe.transformer.to(memory_format=torch.channels_last)\npipe.vae.to(memory_format=torch.channels_last)\n\npipe.transformer = torch.compile(pipe.transformer, mode=\"max-autotune\", fullgraph=True)\npipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\n# Warm Up\nprompt = \"a photo of a cat holding a sign that says hello world\"\nfor _ in range(3):\n    _ = pipe(prompt=prompt, generator=torch.manual_seed(1))\n\n# Run Inference\nimage = pipe(prompt=prompt, generator=torch.manual_seed(1)).images[0]\nimage.save(\"sd3_hello_world.png\")\n```\n\n----------------------------------------\n\nTITLE: Pushing Complete Pipeline to Hub\nDESCRIPTION: Assembles and pushes a complete StableDiffusionPipeline with all components to HuggingFace Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/push_to_hub.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncomponents = {\n    \"unet\": unet,\n    \"scheduler\": scheduler,\n    \"vae\": vae,\n    \"text_encoder\": text_encoder,\n    \"tokenizer\": tokenizer,\n    \"safety_checker\": None,\n    \"feature_extractor\": None,\n}\n\npipeline = StableDiffusionPipeline(**components)\npipeline.push_to_hub(\"my-pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained InstructPix2Pix SDXL Model\nDESCRIPTION: Demonstrates how to perform inference using a trained InstructPix2Pix SDXL model. It includes loading the model, downloading an image, and generating an edited image based on a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README_sdxl.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionXLInstructPix2PixPipeline\n\nmodel_id = \"your_model_id\" # <- replace this\npipe = StableDiffusionXLInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\nurl = \"https://datasets-server.huggingface.co/assets/fusing/instructpix2pix-1000-samples/--/fusing--instructpix2pix-1000-samples/train/23/input_image/image.jpg\"\n\n\ndef download_image(url):\n    image = PIL.Image.open(requests.get(url, stream=True).raw)\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image\n\nimage = download_image(url)\nprompt = \"make it Japan\"\nnum_inference_steps = 20\nimage_guidance_scale = 1.5\nguidance_scale = 10\n\nedited_image = pipe(prompt,\n    image=image,\n    num_inference_steps=num_inference_steps,\n    image_guidance_scale=image_guidance_scale,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images[0]\nedited_image.save(\"edited_image.png\")\n```\n\n----------------------------------------\n\nTITLE: Speech to Image Generation Pipeline\nDESCRIPTION: Combines OpenAI's Whisper model with Stable Diffusion to generate images from audio input. Uses whisper-small for speech recognition and converts the transcribed text to images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nfrom diffusers import DiffusionPipeline\nfrom transformers import (\n    WhisperForConditionalGeneration,\n    WhisperProcessor,\n)\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\naudio_sample = ds[3]\n\ntext = audio_sample[\"text\"].lower()\nspeech_data = audio_sample[\"audio\"][\"array\"]\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(device)\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n\ndiffuser_pipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"speech_to_image_diffusion\",\n    speech_model=model,\n    speech_processor=processor,\n    torch_dtype=torch.float16,\n)\n\ndiffuser_pipeline.enable_attention_slicing()\ndiffuser_pipeline = diffuser_pipeline.to(device)\n\noutput = diffuser_pipeline(speech_data)\nplt.imshow(output.images[0])\n```\n\n----------------------------------------\n\nTITLE: Setting Up ControlNet for Image-to-Image Generation\nDESCRIPTION: Code to initialize a ControlNet model conditioned on depth maps and integrate it with StableDiffusionControlNetImg2ImgPipeline for image-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_depth\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Implementing LCM-LoRA Inpainting with Stable Diffusion\nDESCRIPTION: Sets up an inpainting pipeline using LCM-LoRA with Stable Diffusion. Combines AutoPipelineForInpainting with LCMScheduler to generate images in 4 steps using a mask and initial image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    generator=generator,\n    num_inference_steps=4,\n    guidance_scale=4,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Textual Inversion\nDESCRIPTION: Uses the pipeline to generate multiple images based on the prompt with the textual inversion concept, arranges them in a grid, and displays the result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/textual_inversion_inference.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nall_images = []\nfor _ in range(num_rows):\n    images = pipeline(prompt, num_images_per_prompt=num_samples_per_row, num_inference_steps=50, guidance_scale=7.5).images\n    all_images.extend(images)\n\ngrid = make_image_grid(all_images, num_rows, num_samples_per_row)\ngrid\n```\n\n----------------------------------------\n\nTITLE: FLUX IP-Adapter Implementation\nDESCRIPTION: Shows how to use IP-Adapter with FLUX for image-guided generation. Includes loading the adapter, setting weights and generating images based on reference images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPipeline\nfrom diffusers.utils import load_image\n\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flux_ip_adapter_input.jpg\").resize((1024, 1024))\n\npipe.load_ip_adapter(\n    \"XLabs-AI/flux-ip-adapter\",\n    weight_name=\"ip_adapter.safetensors\",\n    image_encoder_pretrained_model_name_or_path=\"openai/clip-vit-large-patch14\"\n)\npipe.set_ip_adapter_scale(1.0)\n\nimage = pipe(\n    width=1024,\n    height=1024,\n    prompt=\"wearing sunglasses\",\n    negative_prompt=\"\",\n    true_cfg=4.0,\n    generator=torch.Generator().manual_seed(4444),\n    ip_adapter_image=image,\n).images[0]\n\nimage.save('flux_ip_adapter_output.jpg')\n```\n\n----------------------------------------\n\nTITLE: Generating Edited Image with DiffEdit Pipeline in Python\nDESCRIPTION: This snippet illustrates the __call__ method of the StableDiffusionDiffEditPipeline class, which generates the final edited image using the provided mask and inverted latents.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/diffedit.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n__call__(prompt, negative_prompt)\n```\n\n----------------------------------------\n\nTITLE: Inference with StableDiffusionInstructPix2PixPipeline in Python\nDESCRIPTION: This Python snippet initializes the StableDiffusionInstructPix2PixPipeline, loads LoRA weights from a specified directory, and performs inference on an input image to generate edited versions based on a specified edit prompt. It requires the diffusers library and a pre-trained model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/instructpix2pix_lora/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# load the base model pipeline\npipe_lora = StableDiffusionInstructPix2PixPipeline.from_pretrained(\"timbrooks/instruct-pix2pix\")\n\n# Load LoRA weights from the provided path\noutput_dir = \"path/to/lora_weight_directory\"\npipe_lora.unet.load_attn_procs(output_dir)\n\ninput_image_path = \"/path/to/input_image\"\ninput_image = Image.open(input_image_path)\nedited_images = pipe_lora(num_images_per_prompt=1, prompt=args.edit_prompt, image=input_image, num_inference_steps=1000).images\nedited_images[0].show()\n```\n\n----------------------------------------\n\nTITLE: Full Model SDXL Distillation Training\nDESCRIPTION: Training script for full model distillation using CC12M dataset with SDXL base model. Includes configuration for mixed precision, learning rate, and various training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path/to/saved/model\"\n\naccelerate launch train_lcm_distill_sdxl_wds.py \\\n    --pretrained_teacher_model=$MODEL_NAME \\\n    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\n    --output_dir=$OUTPUT_DIR \\\n    --mixed_precision=fp16 \\\n    --resolution=1024 \\\n    --learning_rate=1e-6 --loss_type=\"huber\" --use_fix_crop_and_size --ema_decay=0.95 --adam_weight_decay=0.0 \\\n    --max_train_steps=1000 \\\n    --max_train_samples=4000000 \\\n    --dataloader_num_workers=8 \\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\n    --validation_steps=200 \\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\n    --train_batch_size=12 \\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n    --gradient_accumulation_steps=1 \\\n    --use_8bit_adam \\\n    --resume_from_checkpoint=latest \\\n    --report_to=wandb \\\n    --seed=453645634 \\\n    --push_to_hub \\\n```\n\n----------------------------------------\n\nTITLE: Fusing LoRA Weights with Base Model\nDESCRIPTION: Demonstrates how to fuse loaded LoRA adapters into the base model weights using the fuse_lora method with a specified scale factor.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline.fuse_lora(adapter_names=[\"ikea\", \"feng\"], lora_scale=1.0)\n```\n\n----------------------------------------\n\nTITLE: Initializing MultiAdapter with Multiple T2I-Adapters\nDESCRIPTION: Code to create a MultiAdapter instance that combines two different T2I-Adapters - one for pose control and one for depth control. This allows for composing multiple control signals to guide the image generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionAdapterPipeline, MultiAdapter, T2IAdapter\n\nadapters = MultiAdapter(\n    [\n        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_keypose_sd14v1\"),\n        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_depth_sd14v1\"),\n    ]\n)\nadapters = adapters.to(torch.float16)\n```\n\n----------------------------------------\n\nTITLE: IP Adapter Face ID Pipeline Configuration in Python\nDESCRIPTION: Setup and usage of IP Adapter Face ID for generating images with face-based image embeddings. Includes face analysis, pipeline configuration, and image generation with face ID preservation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_100\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers.utils import load_image\nimport cv2\nimport numpy as np\nfrom diffusers import DiffusionPipeline, AutoencoderKL, DDIMScheduler\nfrom insightface.app import FaceAnalysis\n\nnoise_scheduler = DDIMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n    steps_offset=1,\n)\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(dtype=torch.float16)\npipeline = DiffusionPipeline.from_pretrained(\n    \"SG161222/Realistic_Vision_V4.0_noVAE\",\n    torch_dtype=torch.float16,\n    scheduler=noise_scheduler,\n    vae=vae,\n    custom_pipeline=\"ip_adapter_face_id\"\n)\npipeline.load_ip_adapter_face_id(\"h94/IP-Adapter-FaceID\", \"ip-adapter-faceid_sd15.bin\")\npipeline.to(\"cuda\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(42)\nnum_images = 2\n\nimage = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ai_face2.png\")\n\napp = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\nimage = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\nfaces = app.get(image)\nimage = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\nimages = pipeline(\n    prompt=\"A photo of a girl wearing a black dress, holding red roses in hand, upper body, behind is the Eiffel Tower\",\n    image_embeds=image,\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n    num_inference_steps=20, num_images_per_prompt=num_images, width=512, height=704,\n    generator=generator\n).images\n\nfor i in range(num_images):\n    images[i].save(f\"c{i}.png\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up ControlNet with IP-Adapter for Structural Control\nDESCRIPTION: Configures a StableDiffusionControlNetPipeline with a depth-based ControlNet model and IP-Adapter. This setup allows for both structural control through depth maps and image reference via IP-Adapter for more precise image generation guidance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nimport torch\nfrom diffusers.utils import load_image\n\ncontrolnet_model_path = \"lllyasviel/control_v11f1p_sd15_depth\"\ncontrolnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n\npipeline = StableDiffusionControlNetPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16)\npipeline.to(\"cuda\")\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n```\n\n----------------------------------------\n\nTITLE: Loading Text Encoder for Community Pipeline in Python\nDESCRIPTION: First step in building a custom community pipeline using the Show-1 model. This code imports and initializes the T5Tokenizer and T5EncoderModel from the Transformers library to serve as the text encoder component.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import T5Tokenizer, T5EncoderModel\n\npipe_id = \"showlab/show-1-base\"\ntokenizer = T5Tokenizer.from_pretrained(pipe_id, subfolder=\"tokenizer\")\ntext_encoder = T5EncoderModel.from_pretrained(pipe_id, subfolder=\"text_encoder\")\n```\n\n----------------------------------------\n\nTITLE: Using Prompt Weighting with Image-to-Image Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use prompt weighting with an image-to-image pipeline. It uses the Compel library to generate embeddings with adjusted weights for different concepts in the prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nimage = pipeline(prompt_embeds=prompt_embeds, # generated from Compel\n    negative_prompt_embeds=negative_prompt_embeds, # generated from Compel\n    image=init_image,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing CLIP Guided Stable Diffusion in Python\nDESCRIPTION: Sets up and executes CLIP guided stable diffusion pipeline for generating high-quality images with CLIP model guidance. Requires 12GB GPU RAM and uses the CLIP-ViT-B-32 model for enhanced image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nfrom transformers import CLIPImageProcessor, CLIPModel\nimport torch\n\n\nfeature_extractor = CLIPImageProcessor.from_pretrained(\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\")\nclip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", torch_dtype=torch.float16)\n\n\nguided_pipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    custom_pipeline=\"clip_guided_stable_diffusion\",\n    clip_model=clip_model,\n    feature_extractor=feature_extractor,\n    torch_dtype=torch.float16,\n)\nguided_pipeline.enable_attention_slicing()\nguided_pipeline = guided_pipeline.to(\"cuda\")\n\nprompt = \"fantasy book cover, full moon, fantasy forest landscape, golden vector elements, fantasy magic, dark light night, intricate, elegant, sharp focus, illustration, highly detailed, digital painting, concept art, matte, art by WLOP and Artgerm and Albert Bierstadt, masterpiece\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\nimages = []\nfor i in range(4):\n    image = guided_pipeline(\n        prompt,\n        num_inference_steps=50,\n        guidance_scale=7.5,\n        clip_guidance_scale=100,\n        num_cutouts=4,\n        use_cutouts=False,\n        generator=generator,\n    ).images[0]\n    images.append(image)\n\n# save images locally\nfor i, img in enumerate(images):\n    img.save(f\"./clip_guided_sd/image_{i}.png\")\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained ControlNet in Python\nDESCRIPTION: Python code for loading and using a trained ControlNet model with the StableDiffusionControlNetPipeline. Implements optimizations like UniPCMultistepScheduler, xformers efficient attention, and CPU offloading to improve inference speed and memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom diffusers.utils import load_image\nimport torch\n\nbase_model_path = \"path to model\"\ncontrolnet_path = \"path to controlnet\"\n\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    base_model_path, controlnet=controlnet, torch_dtype=torch.float16\n)\n\n# speed up diffusion process with faster scheduler and memory optimization\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# remove following line if xformers is not installed or when using Torch 2.0.\npipe.enable_xformers_memory_efficient_attention()\n# memory optimization.\npipe.enable_model_cpu_offload()\n\ncontrol_image = load_image(\"./conditioning_image_1.png\")\nprompt = \"pale golden rod circle with old lace background\"\n\n# generate image\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt, num_inference_steps=20, generator=generator, image=control_image\n).images[0]\nimage.save(\"./output.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL with IP Adapter in Diffusers\nDESCRIPTION: Initializes a Stable Diffusion XL pipeline and loads an IP Adapter for image-conditioned text-to-image generation. The code sets up the model with float16 precision and configures the IP Adapter with a specific scale.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\npipeline.set_ip_adapter_scale(0.6)\n```\n\n----------------------------------------\n\nTITLE: Loading LTX Video Model from Single File in Python\nDESCRIPTION: Demonstrates how to load LTX Video model components (transformer and VAE) from a single file and create an image-to-video pipeline using the Diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ltx_video.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoencoderKLLTXVideo, LTXImageToVideoPipeline, LTXVideoTransformer3DModel\n\nsingle_file_url = \"https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors\"\ntransformer = LTXVideoTransformer3DModel.from_single_file(\n  single_file_url, torch_dtype=torch.bfloat16\n)\nvae = AutoencoderKLLTXVideo.from_single_file(single_file_url, torch_dtype=torch.bfloat16)\npipe = LTXImageToVideoPipeline.from_pretrained(\n  \"Lightricks/LTX-Video\", transformer=transformer, vae=vae, torch_dtype=torch.bfloat16\n)\n\n# ... inference code ...\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Stable Diffusion in Python\nDESCRIPTION: This snippet demonstrates how to use the StableDiffusionPipeline for text-to-image generation. It loads a pre-trained model, moves it to CUDA, generates an image from a text prompt, and saves the result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/README.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# make sure you're logged in with `huggingface-cli login`\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n\npipe = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"astronaut_rides_horse.png\")\n```\n\n----------------------------------------\n\nTITLE: Changing Scheduler for Faster Inference\nDESCRIPTION: Replaces the default scheduler with DPMSolverMultistepScheduler to reduce the number of inference steps required, further speeding up the generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DPMSolverMultistepScheduler\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: Inpainting with Stable Diffusion XL (SDXL)\nDESCRIPTION: Shows how to use the SDXL Inpainting model, which is a larger and more powerful version of Stable Diffusion. It loads the model and generates a high-quality inpainted image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\ngenerator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Processing IP-Adapter Masks for Image Generation\nDESCRIPTION: Demonstrates mask preprocessing for IP-Adapter using IPAdapterMaskProcessor, including handling multiple masks with specific output dimensions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.image_processor import IPAdapterMaskProcessor\n\nmask1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_mask1.png\")\nmask2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_mask2.png\")\n\noutput_height = 1024\noutput_width = 1024\n\nprocessor = IPAdapterMaskProcessor()\nmasks = processor.preprocess([mask1, mask2], height=output_height, width=output_width)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image-to-Image Pipeline with Kandinsky 3\nDESCRIPTION: Initializing the Kandinsky 3 image-to-image pipeline, which simplifies the process by not requiring a separate prior model, unlike earlier versions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import Kandinsky3Img2ImgPipeline\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = Kandinsky3Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-3\", variant=\"fp16\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Basic AnimateDiff Pipeline Implementation in Python\nDESCRIPTION: Demonstrates how to set up and use the basic AnimateDiffPipeline with a motion adapter and Stable Diffusion model for generating animated videos from text prompts. Includes configuration of scheduler and memory optimization settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\n# Load the motion adapter\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n# load SD 1.5 based finetuned model\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipe.scheduler = scheduler\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\noutput = pipe(\n    prompt=(\n        \"masterpiece, bestquality, highlydetailed, ultradetailed, sunset, \"\n        \"orange sky, warm lighting, fishing boats, ocean waves seagulls, \"\n        \"rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, \"\n        \"golden hour, coastal landscape, seaside scenery\"\n    ),\n    negative_prompt=\"bad quality, worse quality\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=25,\n    generator=torch.Generator(\"cpu\").manual_seed(42),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Using FreeInit with PIA Pipeline for Improved Video Quality in Python\nDESCRIPTION: This example shows how to use FreeInit with the PIA pipeline to improve temporal consistency and overall quality of generated videos. It includes setting up the pipeline with FreeInit and generating the output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pia.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import (\n    DDIMScheduler,\n    MotionAdapter,\n    PIAPipeline,\n)\nfrom diffusers.utils import export_to_gif, load_image\n\nadapter = MotionAdapter.from_pretrained(\"openmmlab/PIA-condition-adapter\")\npipe = PIAPipeline.from_pretrained(\"SG161222/Realistic_Vision_V6.0_B1_noVAE\", motion_adapter=adapter)\n\n# enable FreeInit\n# Refer to the enable_free_init documentation for a full list of configurable parameters\npipe.enable_free_init(method=\"butterworth\", use_fast_sampling=True)\n\n# Memory saving options\npipe.enable_model_cpu_offload()\npipe.enable_vae_slicing()\n\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\nimage = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat_6.png?download=true\"\n)\nimage = image.resize((512, 512))\nprompt = \"cat in a field\"\nnegative_prompt = \"wrong white balance, dark, sketches,worst quality,low quality\"\n\ngenerator = torch.Generator(\"cpu\").manual_seed(0)\n\noutput = pipe(image=image, prompt=prompt, generator=generator)\nframes = output.frames[0]\nexport_to_gif(frames, \"pia-freeinit-animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Enabling FreeNoise for Longer Video Generation in Python\nDESCRIPTION: This snippet demonstrates how to integrate the FreeNoise mechanism to enhance video generation capabilities by allowing for longer video outputs from short-video generation models. It utilizes a dictionary to set multiple prompts across specified frames and includes the necessary configurations to run the pipeline efficiently.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoencoderKL, AnimateDiffPipeline, LCMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_video, load_image\n\n# Load pipeline\ndtype = torch.float16\nmotion_adapter = MotionAdapter.from_pretrained(\"wangfuyun/AnimateLCM\", torch_dtype=dtype)\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=dtype)\n\npipe = AnimateDiffPipeline.from_pretrained(\"emilianJR/epiCRealism\", motion_adapter=motion_adapter, vae=vae, torch_dtype=dtype)\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule=\"linear\")\n\npipe.load_lora_weights(\n    \"wangfuyun/AnimateLCM\", weight_name=\"AnimateLCM_sd15_t2v_lora.safetensors\", adapter_name=\"lcm_lora\"\n)\npipe.set_adapters([\"lcm_lora\"], [0.8])\n\n# Enable FreeNoise for long prompt generation\npipe.enable_free_noise(context_length=16, context_stride=4)\npipe.to(\"cuda\")\n\n# Can be a single prompt, or a dictionary with frame timesteps\nprompt = {\n    0: \"A caterpillar on a leaf, high quality, photorealistic\",\n    40: \"A caterpillar transforming into a cocoon, on a leaf, near flowers, photorealistic\",\n    80: \"A cocoon on a leaf, flowers in the backgrond, photorealistic\",\n    120: \"A cocoon maturing and a butterfly being born, flowers and leaves visible in the background, photorealistic\",\n    160: \"A beautiful butterfly, vibrant colors, sitting on a leaf, flowers in the background, photorealistic\",\n    200: \"A beautiful butterfly, flying away in a forest, photorealistic\",\n    240: \"A cyberpunk butterfly, neon lights, glowing\",\n}\nnegative_prompt = \"bad quality, worst quality, jpeg artifacts\"\n\n# Run inference\noutput = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_frames=256,\n    guidance_scale=2.5,\n    num_inference_steps=10,\n    generator=torch.Generator(\"cpu\").manual_seed(0),\n)\n\n# Save video\nframes = output.frames[0]\nexport_to_video(frames, \"output.mp4\", fps=16)\n```\n\n----------------------------------------\n\nTITLE: Importing FlaxStableDiffusionControlNetPipeline in Python\nDESCRIPTION: This snippet shows the import statement for the FlaxStableDiffusionControlNetPipeline class, which is the Flax implementation of the ControlNet pipeline for Stable Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] FlaxStableDiffusionControlNetPipeline\n```\n\n----------------------------------------\n\nTITLE: Basic Text-to-Video Generation with Stable Diffusion\nDESCRIPTION: Demonstrates how to generate a video from a text prompt using the TextToVideoZeroPipeline. The code initializes the pipeline, generates frames, and saves them as an MP4 file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video_zero.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import TextToVideoZeroPipeline\nimport imageio\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A panda is playing guitar on times square\"\nresult = pipe(prompt=prompt).images\nresult = [(r * 255).astype(\"uint8\") for r in result]\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Model with AutoModel\nDESCRIPTION: Example demonstrating how to load a Stable Diffusion v1.5 model using AutoModel and AutoPipelineForText2Image. The code shows loading the UNet component separately and incorporating it into a text-to-image pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/auto_model.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoModel, AutoPipelineForText2Image\n\nunet = AutoModel.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"unet\")\npipe = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", unet=unet)\n```\n\n----------------------------------------\n\nTITLE: Using Guidance-distilled Flux Model in Python\nDESCRIPTION: Shows implementation of the guidance-distilled Flux model variant which requires around 50 sampling steps for quality generation and has no max_sequence_length limitations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPipeline\n\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload()\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\nout = pipe(\n    prompt=prompt,\n    guidance_scale=3.5,\n    height=768,\n    width=1360,\n    num_inference_steps=50,\n).images[0]\nout.save(\"image.png\")\n```\n\n----------------------------------------\n\nTITLE: Surface Normals Estimation with MarigoldNormalsPipeline\nDESCRIPTION: Demonstrates how to load and use the Marigold normals model to predict surface normals from an input image. The pipeline loads a pre-trained model, processes an image, and visualizes the normal vectors as an RGB image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\n\npipe = diffusers.MarigoldNormalsPipeline.from_pretrained(\n    \"prs-eth/marigold-normals-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(\"cuda\")\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\nnormals = pipe(image)\n\nvis = pipe.image_processor.visualize_normals(normals.prediction)\nvis[0].save(\"einstein_normals.png\")\n```\n\n----------------------------------------\n\nTITLE: Using IP-Adapter Plus with Custom Image Encoder\nDESCRIPTION: This snippet demonstrates how to use IP-Adapter Plus which requires explicitly loading the ViT-H image encoder. It loads the image encoder from the IP-Adapter repository and passes it to the pipeline, then loads the IP-Adapter Plus weights for Stable Diffusion XL.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import CLIPVisionModelWithProjection\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"h94/IP-Adapter\",\n    subfolder=\"models/image_encoder\",\n    torch_dtype=torch.float16\n)\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    image_encoder=image_encoder,\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter-plus_sdxl_vit-h.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: Chaining Image-to-Image Pipeline in Python\nDESCRIPTION: This code demonstrates how to chain an image-to-image pipeline with the previous inpainting result to apply a new style. It uses the Elden Ring style model to further modify the generated image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"nitrosocke/elden-ring-diffusion\", torch_dtype=torch.float16,\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nprompt = \"elden ring style castle\" # include the token \"elden ring style\" in the prompt\nnegative_prompt = \"bad architecture, deformed, disfigured, poor details\"\n\nimage_elden_ring = pipeline(prompt, negative_prompt=negative_prompt, image=image).images[0]\nmake_image_grid([init_image, mask_image, image, image_elden_ring], rows=2, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Generating Video with AnimateDiffSparseControlNetPipeline using Scribble-based Conditioning in Python\nDESCRIPTION: This snippet shows how to use AnimateDiffSparseControlNetPipeline for video generation with sparse scribble-based conditioning. It loads pre-trained models, sets up the pipeline with SparseControlNet, and generates a video based on a text prompt and sparse conditioning frames.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import AnimateDiffSparseControlNetPipeline\nfrom diffusers.models import AutoencoderKL, MotionAdapter, SparseControlNetModel\nfrom diffusers.schedulers import DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_gif, load_image\n\n\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\nmotion_adapter_id = \"guoyww/animatediff-motion-adapter-v1-5-3\"\ncontrolnet_id = \"guoyww/animatediff-sparsectrl-scribble\"\nlora_adapter_id = \"guoyww/animatediff-motion-lora-v1-5-3\"\nvae_id = \"stabilityai/sd-vae-ft-mse\"\ndevice = \"cuda\"\n\nmotion_adapter = MotionAdapter.from_pretrained(motion_adapter_id, torch_dtype=torch.float16).to(device)\ncontrolnet = SparseControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16).to(device)\nvae = AutoencoderKL.from_pretrained(vae_id, torch_dtype=torch.float16).to(device)\nscheduler = DPMSolverMultistepScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    beta_schedule=\"linear\",\n    algorithm_type=\"dpmsolver++\",\n    use_karras_sigmas=True,\n)\npipe = AnimateDiffSparseControlNetPipeline.from_pretrained(\n    model_id,\n    motion_adapter=motion_adapter,\n    controlnet=controlnet,\n    vae=vae,\n    scheduler=scheduler,\n    torch_dtype=torch.float16,\n).to(device)\npipe.load_lora_weights(lora_adapter_id, adapter_name=\"motion_lora\")\npipe.fuse_lora(lora_scale=1.0)\n\nprompt = \"an aerial view of a cyberpunk city, night time, neon lights, masterpiece, high quality\"\nnegative_prompt = \"low quality, worst quality, letterboxed\"\n\nimage_files = [\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-scribble-1.png\",\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-scribble-2.png\",\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-scribble-3.png\"\n]\ncondition_frame_indices = [0, 8, 15]\nconditioning_frames = [load_image(img_file) for img_file in image_files]\n\nvideo = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_inference_steps=25,\n    conditioning_frames=conditioning_frames,\n    controlnet_conditioning_scale=1.0,\n    controlnet_frame_indices=condition_frame_indices,\n    generator=torch.Generator().manual_seed(1337),\n).frames[0]\nexport_to_gif(video, \"output.gif\")\n```\n\n----------------------------------------\n\nTITLE: Quantizing Models to 4-bit Precision with bitsandbytes\nDESCRIPTION: Demonstrates how to quantize FluxTransformer2DModel and T5EncoderModel to 4-bit precision using BitsAndBytesConfig. It also shows how to adjust the torch_dtype for non-quantized modules.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\nfrom transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n\nfrom diffusers import FluxTransformer2DModel\nfrom transformers import T5EncoderModel\n\nquant_config = TransformersBitsAndBytesConfig(load_in_4bit=True,)\n\ntext_encoder_2_4bit = T5EncoderModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"text_encoder_2\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_4bit=True,)\n\ntransformer_4bit = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n```\n\n----------------------------------------\n\nTITLE: LCM-LoRA with ControlNet Integration\nDESCRIPTION: Shows implementation of ControlNet with LCM-LoRA for controlled image generation using canny edge detection. Combines multiple adapters for enhanced control over generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n).resize((512, 512))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    variant=\"fp16\"\n).to(\"cuda\")\n\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    \"the mona lisa\",\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5,\n    controlnet_conditioning_scale=0.8,\n    cross_attention_kwargs={\"scale\": 1},\n    generator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading Diffusion Transformer with Auto Device Mapping\nDESCRIPTION: Loads the FluxTransformer2DModel (12.5B parameters) using automatic device mapping to distribute the model across multiple GPUs. Uses the 'auto' strategy from Accelerate's Big Model Inference feature to optimize model placement.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxTransformer2DModel\nimport torch \n\ntransformer = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\", \n    subfolder=\"transformer\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n```\n\n----------------------------------------\n\nTITLE: Importing UNet2DConditionOutput in Python\nDESCRIPTION: This snippet shows how to import the UNet2DConditionOutput class, which represents the output of the UNet2DConditionModel. This class is used to structure and type the model's output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet2d-cond.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.models.unets.unet_2d_condition import UNet2DConditionOutput\n```\n\n----------------------------------------\n\nTITLE: Generating Video from Image using PIA Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use the PIA pipeline to generate a video from a single image. It includes loading the motion adapter and pipeline, setting up the scheduler, and generating the output frames.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pia.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import (\n    EulerDiscreteScheduler,\n    MotionAdapter,\n    PIAPipeline,\n)\nfrom diffusers.utils import export_to_gif, load_image\n\nadapter = MotionAdapter.from_pretrained(\"openmmlab/PIA-condition-adapter\")\npipe = PIAPipeline.from_pretrained(\"SG161222/Realistic_Vision_V6.0_B1_noVAE\", motion_adapter=adapter, torch_dtype=torch.float16)\n\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\npipe.enable_vae_slicing()\n\nimage = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat_6.png?download=true\"\n)\nimage = image.resize((512, 512))\nprompt = \"cat in a field\"\nnegative_prompt = \"wrong white balance, dark, sketches,worst quality,low quality\"\n\ngenerator = torch.Generator(\"cpu\").manual_seed(0)\noutput = pipe(image=image, prompt=prompt, generator=generator)\nframes = output.frames[0]\nexport_to_gif(frames, \"pia-animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with IP-Adapter\nDESCRIPTION: This code snippet shows how to generate images using a pipeline with IP-Adapter by providing both a text prompt and a reference image. The image prompt guides the visual style and content, while the text prompt provides additional direction for the generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png\")\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\nimages = pipeline(\n    prompt='best quality, high quality, wearing sunglasses',\n    ip_adapter_image=image,\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n    num_inference_steps=50,\n    generator=generator,\n).images[0]\nimages\n```\n\n----------------------------------------\n\nTITLE: Comparing CLIP Scores for Different Stable Diffusion Versions in Python\nDESCRIPTION: This snippet calculates and compares the CLIP scores for images generated by Stable Diffusion v1-4 and v1-5. It demonstrates how to evaluate the performance difference between model versions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsd_clip_score_1_4 = calculate_clip_score(images, prompts)\nprint(f\"CLIP Score with v-1-4: {sd_clip_score_1_4}\")\n# CLIP Score with v-1-4: 34.9102\n\nsd_clip_score_1_5 = calculate_clip_score(images_1_5, prompts)\nprint(f\"CLIP Score with v-1-5: {sd_clip_score_1_5}\")\n# CLIP Score with v-1-5: 36.2137\n```\n\n----------------------------------------\n\nTITLE: Virtual Try-On with OmniGen in Python\nDESCRIPTION: This code demonstrates using OmniGen for a virtual try-on application, combining a person's identity from one image with clothing items from another. It processes both images through a prompt that describes the desired composite output of a woman wearing specific clothing items.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/omnigen.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenPipeline\nfrom diffusers.utils import load_image \n\npipe = OmniGenPipeline.from_pretrained(\n    \"Shitao/OmniGen-v1-diffusers\",\n    torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\n\nprompt=\"A woman is walking down the street, wearing a white long-sleeve blouse with lace details on the sleeves, paired with a blue pleated skirt. The woman is <img><|image_1|></img>. The long-sleeve blouse and a pleated skirt are <img><|image_2|></img>.\"\ninput_image_1 = load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/emma.jpeg\")\ninput_image_2 = load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/dress.jpg\")\ninput_images=[input_image_1, input_image_2]\nimage = pipe(\n    prompt=prompt, \n    input_images=input_images, \n    height=1024,\n    width=1024,\n    guidance_scale=2.5, \n    img_guidance_scale=1.6,\n    generator=torch.Generator(device=\"cpu\").manual_seed(666)\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion Mixture Canvas Pipeline for SD 1.5\nDESCRIPTION: This code snippet sets up and uses a Stable Diffusion Mixture Canvas Pipeline for SD 1.5. It combines text-to-image and image-to-image regions to create a composite image based on a guide image and text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom diffusers import LMSDiscreteScheduler, DiffusionPipeline\nfrom diffusers.pipelines.pipeline_utils import Image2ImageRegion, Text2ImageRegion, preprocess_image\n\n# Load and preprocess guide image\niic_image = preprocess_image(Image.open(\"input_image.png\").convert(\"RGB\"))\n\n# Create scheduler and model (similar to StableDiffusionPipeline)\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\npipeline = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", scheduler=scheduler).to(\"cuda:0\", custom_pipeline=\"mixture_canvas\")\npipeline.to(\"cuda\")\n\n# Mixture of Diffusers generation\noutput = pipeline(\n    canvas_height=800,\n    canvas_width=352,\n    regions=[\n        Text2ImageRegion(0, 800, 0, 352, guidance_scale=8,\n            prompt=f\"best quality, masterpiece, WLOP, sakimichan, art contest winner on pixiv, 8K, intricate details, wet effects, rain drops, ethereal, mysterious, futuristic, UHD, HDR, cinematic lighting, in a beautiful forest, rainy day, award winning, trending on artstation, beautiful confident cheerful young woman, wearing a futuristic sleeveless dress, ultra beautiful detailed  eyes, hyper-detailed face, complex,  perfect, model,  textured,  chiaroscuro, professional make-up, realistic, figure in frame, \"),\n        Image2ImageRegion(352-800, 352, 0, 352, reference_image=iic_image, strength=1.0),\n    ],\n    num_inference_steps=100,\n    seed=5525475061,\n)[\"images\"][0]\n```\n\n----------------------------------------\n\nTITLE: Configuring Guidance Scale in Inpainting Pipeline\nDESCRIPTION: This code shows how to adjust the 'guidance_scale' parameter in the inpainting pipeline. This parameter affects how closely aligned the generated image is with the text prompt, influencing the strictness of prompt interpretation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, guidance_scale=2.5).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with DiffusionPipeline\nDESCRIPTION: Performs inference using the DiffusionPipeline from diffusers with the model weights loaded from a previous training run. Adjust memory management for efficiency when loading LoRA weights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n    torch_dtype=torch.float16\n)\nlora_output_path = \"trained-sd3-lora-miniature\"\npipeline.load_lora_weights(\"trained-sd3-lora-miniature\")\n\npipeline.enable_sequential_cpu_offload()\n\nimage = pipeline(\"a photo of sks dog in a bucket\").images[0]\nimage.save(\"bucket_dog.png\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Full Determinism in StableDiffusionPipeline\nDESCRIPTION: This code demonstrates achieving identical results across multiple runs of a StableDiffusionPipeline using full determinism and fixed seeds.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DDIMScheduler, StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True).to(\"cuda\")\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\ng = torch.Generator(device=\"cuda\")\n\nprompt = \"A bear is playing a guitar on Times Square\"\n\ng.manual_seed(0)\nresult1 = pipe(prompt=prompt, num_inference_steps=50, generator=g, output_type=\"latent\").images\n\ng.manual_seed(0)\nresult2 = pipe(prompt=prompt, num_inference_steps=50, generator=g, output_type=\"latent\").images\n\nprint(\"L_inf dist =\", abs(result1 - result2).max())\n\"L_inf dist = tensor(0., device='cuda:0')\"\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Quantized HunyuanVideoPipeline with BitsAndBytes\nDESCRIPTION: This snippet demonstrates how to load a quantized HunyuanVideoPipeline using the BitsAndBytes configuration for reduced memory usage. It then generates a video from a text prompt and exports it.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/hunyuan_video.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, HunyuanVideoTransformer3DModel, HunyuanVideoPipeline\nfrom diffusers.utils import export_to_video\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = HunyuanVideoTransformer3DModel.from_pretrained(\n    \"hunyuanvideo-community/HunyuanVideo\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.bfloat16,\n)\n\npipeline = HunyuanVideoPipeline.from_pretrained(\n    \"hunyuanvideo-community/HunyuanVideo\",\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"A cat walks on the grass, realistic style.\"\nvideo = pipeline(prompt=prompt, num_frames=61, num_inference_steps=30).frames[0]\nexport_to_video(video, \"cat.mp4\", fps=15)\n```\n\n----------------------------------------\n\nTITLE: Loading a Diffusion Pipeline from Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to initialize a DiffusionPipeline by loading the Stable Diffusion v1.5 model from the Hugging Face Hub. The use_safetensors parameter enables loading optimized model weights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import DiffusionPipeline\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Applying FreeU to Stable Diffusion v1-5 in Python\nDESCRIPTION: This snippet demonstrates how to enable and configure FreeU for the Stable Diffusion v1-5 model. It sets up the pipeline, enables FreeU with specific parameters, and generates an image from a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/image_quality.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, safety_checker=None\n).to(\"cuda\")\npipeline.enable_freeu(s1=0.9, s2=0.2, b1=1.5, b2=1.6)\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\nprompt = \"\"\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Controllable Generation with Human Skeleton Detection\nDESCRIPTION: Demonstrates OmniGen's ability to handle computer vision tasks like skeleton detection and use the detected skeleton as a control condition for generating new images. The example shows a two-step process of detection followed by conditional generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/omnigen.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenPipeline\nfrom diffusers.utils import load_image \n\npipe = OmniGenPipeline.from_pretrained(\n    \"Shitao/OmniGen-v1-diffusers\",\n    torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\n\nprompt=\"Detect the skeleton of human in this image: <img><|image_1|></img>\"\ninput_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/edit.png\")]\nimage1 = pipe(\n    prompt=prompt, \n    input_images=input_images, \n    guidance_scale=2, \n    img_guidance_scale=1.6,\n    use_input_image_size_as_output=True,\n    generator=torch.Generator(device=\"cpu\").manual_seed(333)\n).images[0]\nimage1.save(\"image1.png\")\n\nprompt=\"Generate a new photo using the following picture and text as conditions: <img><|image_1|></img>\\n A young boy is sitting on a sofa in the library, holding a book. His hair is neatly combed, and a faint smile plays on his lips, with a few freckles scattered across his cheeks. The library is quiet, with rows of shelves filled with books stretching out behind him.\"\ninput_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/skeletal.png\")]\nimage2 = pipe(\n    prompt=prompt, \n    input_images=input_images, \n    guidance_scale=2, \n    img_guidance_scale=1.6,\n    use_input_image_size_as_output=True,\n    generator=torch.Generator(device=\"cpu\").manual_seed(333)\n).images[0]\nimage2.save(\"image2.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing UNet2DConditionModel in Python\nDESCRIPTION: This snippet demonstrates how to import the UNet2DConditionModel class from the diffusers library. The UNet2DConditionModel is a 2D conditional UNet used in diffusion systems for image generation tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet2d-cond.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel\n```\n\n----------------------------------------\n\nTITLE: Setting up AutoPipeline with IP-Adapter and PAG for Text-to-Image Generation in Python\nDESCRIPTION: This snippet illustrates how to create an AutoPipeline for text-to-image generation with IP-Adapter and PAG enabled. It loads a pre-trained image encoder, initializes the pipeline, and sets up the IP-Adapter for image-prompted generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nfrom transformers import CLIPVisionModelWithProjection\nimport torch\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"h94/IP-Adapter\",\n    subfolder=\"models/image_encoder\",\n    torch_dtype=torch.float16\n)\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    image_encoder=image_encoder,\n    enable_pag=True,\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter-plus_sdxl_vit-h.bin\")\n\npag_scales = 5.0\nip_adapter_scales = 0.8\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\")\n\npipeline.set_ip_adapter_scale(ip_adapter_scale)\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimages = pipeline(\n    prompt=\"a polar bear sitting in a chair drinking a milkshake\",\n    ip_adapter_image=image,\n    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n    num_inference_steps=25,\n    guidance_scale=3.0,\n    generator=generator,\n    pag_scale=pag_scale,\n).images\nimages[0]\n```\n\n----------------------------------------\n\nTITLE: Creating WebDataset Processing Pipeline for LCM Training\nDESCRIPTION: Pipeline configuration for WebDataset to efficiently process cloud-stored image data. It decodes images, renames fields, filters keys, applies transformations, and converts the data to tuples for training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprocessing_pipeline = [\n    wds.decode(\"pil\", handler=wds.ignore_and_continue),\n    wds.rename(image=\"jpg;png;jpeg;webp\", text=\"text;txt;caption\", handler=wds.warn_and_continue),\n    wds.map(filter_keys({\"image\", \"text\"})),\n    wds.map(transform),\n    wds.to_tuple(\"image\", \"text\"),\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Short Videos with ModelScope Text-to-Video Model\nDESCRIPTION: Basic usage example of the ModelScope text-to-video-ms-1.7b model to generate a short 16-frame video (2 seconds at 8 fps) from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(\"cuda\")\n\nprompt = \"Spiderman is surfing\"\nvideo_frames = pipe(prompt).frames[0]\nvideo_path = export_to_video(video_frames)\nvideo_path\n```\n\n----------------------------------------\n\nTITLE: Creating Weighted Prompts for SDXL\nDESCRIPTION: Defines a detailed prompt and negative prompt with weighted concepts for SDXL image generation. The prompt describes a whimsical waffle-hippo hybrid creature, while the negative prompt helps avoid common image defects.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom sd_embed.embedding_funcs import get_weighted_text_embeddings_sdxl\n\nprompt = \"\"\"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. \nThis imaginative creature features the distinctive, bulky body of a hippo, \nbut with a texture and appearance resembling a golden-brown, crispy waffle. \nThe creature might have elements like waffle squares across its skin and a syrup-like sheen. \nIt's set in a surreal environment that playfully combines a natural water habitat of a hippo with elements of a breakfast table setting, \npossibly including oversized utensils or plates in the background. \nThe image should evoke a sense of playful absurdity and culinary fantasy.\n\"\"\"\n\nneg_prompt = \"\"\"\\\nskin spots,acnes,skin blemishes,age spot,(ugly:1.2),(duplicate:1.2),(morbid:1.21),(mutilated:1.2),\\\n(tranny:1.2),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.2),(bad proportions:1.3),\\\nextra limbs,(disfigured:1.2),(missing arms:1.2),(extra legs:1.2),(fused fingers:1.5),\\\n(too many fingers:1.5),(unclear eyes:1.2),lowers,bad hands,missing fingers,extra digit,\\\nbad hands,missing fingers,(extra arms and legs),(worst quality:2),(low quality:2),\\\n(normal quality:2),lowres,((monochrome)),((grayscale))\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Stable Diffusion and ControlNet Models in Python\nDESCRIPTION: This snippet initializes the Stable Diffusion model and ControlNet for video frame generation. It sets up the model paths, prompts, and parameters for the generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_104\n\nLANGUAGE: python\nCODE:\n```\n# You can use any finetuned SD here\nmodel_path = 'SG161222/Realistic_Vision_V2.0'\n\nprompt = 'a red car turns in the winter'\na_prompt = ', RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3, '\nn_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation'\n\ninput_interval = 5\nframes = video_to_frame(\n    input_video_path, input_interval)\n```\n\n----------------------------------------\n\nTITLE: Loading Original Checkpoint for SD3Transformer2DModel in Python\nDESCRIPTION: This code shows how to load the original checkpoint file for the SD3Transformer2DModel using the from_single_file method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import SD3Transformer2DModel\n\nmodel = SD3Transformer2DModel.from_single_file(\"https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running DeepFloyd IF Multi-Stage Pipeline in Python\nDESCRIPTION: Demonstrates how to initialize all three stages of the DeepFloyd IF pipeline with model offloading for memory optimization. The pipeline stages transform a prompt to generate progressively higher resolution images through text-to-image, upscaling, and refinement stages.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# stage 2\nstage_2 = IFInpaintingSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_model_cpu_offload()\n\n# stage 3\nsafety_modules = {\n    \"feature_extractor\": stage_1.feature_extractor,\n    \"safety_checker\": stage_1.safety_checker,\n    \"watermarker\": stage_1.watermarker,\n}\nstage_3 = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16\n)\nstage_3.enable_model_cpu_offload()\n\nprompt = \"blue sunglasses\"\ngenerator = torch.manual_seed(1)\n\n# text embeds\nprompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)\n\n# stage 1\nstage_1_output = stage_1(\n    image=original_image,\n    mask_image=mask_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\n#pt_to_pil(stage_1_output)[0].save(\"./if_stage_I.png\")\n\n# stage 2\nstage_2_output = stage_2(\n    image=stage_1_output,\n    original_image=original_image,\n    mask_image=mask_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\n#pt_to_pil(stage_1_output)[0].save(\"./if_stage_II.png\")\n\n# stage 3\nstage_3_output = stage_3(prompt=prompt, image=stage_2_output, generator=generator, noise_level=100).images\n#stage_3_output[0].save(\"./if_stage_III.png\")\nmake_image_grid([original_image, mask_image, pt_to_pil(stage_1_output)[0], pt_to_pil(stage_2_output)[0], stage_3_output[0]], rows=1, rows=5)\n```\n\n----------------------------------------\n\nTITLE: Implementing Zero SNR with DDIMScheduler for Image Generation\nDESCRIPTION: Complete example of loading a model trained with v_prediction and configuring the DDIMScheduler with zero SNR and trailing timestep spacing. Uses guidance_rescale to prevent over-exposure while generating an image of a snowy mountain with northern lights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/scheduler_features.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, DDIMScheduler\n\npipeline = DiffusionPipeline.from_pretrained(\"ptx0/pseudo-journey-v2\", use_safetensors=True)\n\npipeline.scheduler = DDIMScheduler.from_config(\n    pipeline.scheduler.config, rescale_betas_zero_snr=True, timestep_spacing=\"trailing\"\n)\npipeline.to(\"cuda\")\nprompt = \"cinematic photo of a snowy mountain at night with the northern lights aurora borealis overhead, 35mm photograph, film, professional, 4k, highly detailed\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(23)\nimage = pipeline(prompt, guidance_rescale=0.7, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Dequantizing BitsAndBytes Models Back to Original Precision\nDESCRIPTION: Loading quantized Flux.1 model components and then dequantizing them back to their original precision. This allows for full-precision operations when needed, though it requires sufficient GPU RAM and may result in a small quality loss.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\nfrom transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n\nfrom diffusers import FluxTransformer2DModel\nfrom transformers import T5EncoderModel\n\nquant_config = TransformersBitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n)\n\ntext_encoder_2_4bit = T5EncoderModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"text_encoder_2\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n)\n\ntransformer_4bit = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\ntext_encoder_2_4bit.dequantize()\ntransformer_4bit.dequantize()\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training for InstructPix2Pix SDXL\nDESCRIPTION: Demonstrates how to launch distributed training for InstructPix2Pix SDXL using multiple GPUs. It includes mixed precision and various training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README_sdxl.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu train_instruct_pix2pix_sdxl.py \\\n    --pretrained_model_name_or_path=stabilityai/stable-diffusion-xl-base-1.0 \\\n    --dataset_name=$DATASET_ID \\\n    --use_ema \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=512 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --seed=42 \\\n    --val_image_url_or_path=\"https://datasets-server.huggingface.co/assets/fusing/instructpix2pix-1000-samples/--/fusing--instructpix2pix-1000-samples/train/23/input_image/image.jpg\" \\\n    --validation_prompt=\"make it in japan\" \\\n    --report_to=wandb \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Reusing Existing Pipeline with from_pipe Method\nDESCRIPTION: Demonstrates how to reuse an existing pipeline using the from_pipe method to save memory by avoiding loading the same checkpoint twice. This example converts a text-to-image pipeline to an image-to-image pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe_img2img = AutoPipelineForImage2Image.from_pipe(pipe_txt2img).to(\"cuda\")\nimage = pipeline(prompt, image=init_image, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implementing Tiled VAE for Large Image Processing\nDESCRIPTION: Shows how to use tiled VAE processing for handling large images (e.g., 4k) with limited VRAM by splitting images into overlapping tiles for processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\nprompt = \"a beautiful landscape photograph\"\npipe.enable_vae_tiling()\n#pipe.enable_xformers_memory_efficient_attention()\n\nimage = pipe([prompt], width=3840, height=2224, num_inference_steps=20).images[0]\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained T2I-Adapter and SDXL\nDESCRIPTION: Python code for generating images using the trained T2I-Adapter with Stable Diffusion XL. It loads the adapter and model, sets up the pipeline with optimizations, and generates an image based on a conditioning image and prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteSchedulerTest\nfrom diffusers.utils import load_image\nimport torch\n\nadapter = T2IAdapter.from_pretrained(\"path/to/adapter\", torch_dtype=torch.float16)\npipeline = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", adapter=adapter, torch_dtype=torch.float16\n)\n\npipeline.scheduler = EulerAncestralDiscreteSchedulerTest.from_config(pipe.scheduler.config)\npipeline.enable_xformers_memory_efficient_attention()\npipeline.enable_model_cpu_offload()\n\ncontrol_image = load_image(\"./conditioning_image_1.png\")\nprompt = \"pale golden rod circle with old lace background\"\n\ngenerator = torch.manual_seed(0)\nimage = pipeline(\n    prompt, image=control_image, generator=generator\n).images[0]\nimage.save(\"./output.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Model Checkpoints from Hugging Face Hub in Python\nDESCRIPTION: Demonstrates how to load the SDXL base and refiner model checkpoints from the Hugging Face Hub using the from_pretrained method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n\nrefiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Generating Age-Varied Warrior Chief Portraits in Python\nDESCRIPTION: Creates multiple variations of a prompt with different age descriptors (oldest, old, standard, young) for a warrior chief, then generates images using fixed random seeds for comparison.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"portrait photo of the oldest warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n    \"portrait photo of an old warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n    \"portrait photo of a warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n    \"portrait photo of a young warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n]\n\ngenerator = [torch.Generator(\"cuda\").manual_seed(1) for _ in range(len(prompts))]\nimages = pipeline(prompt=prompts, generator=generator, num_inference_steps=25).images\nmake_image_grid(images, 2, 2)\n```\n\n----------------------------------------\n\nTITLE: Performing Image-to-Image Transformation with SDXL in Python\nDESCRIPTION: Shows how to use SDXL for image-to-image transformation by providing an initial image and a text prompt to condition the output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image, make_image_grid\n\n# use from_pipe to avoid consuming additional memory when loading a checkpoint\npipeline = AutoPipelineForImage2Image.from_pipe(pipeline_text2image).to(\"cuda\")\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\ninit_image = load_image(url)\nprompt = \"a dog catching a frisbee in the jungle\"\nimage = pipeline(prompt, image=init_image, strength=0.8, guidance_scale=10.5).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Performing Image-to-Image Generation with SDXL Turbo\nDESCRIPTION: This code shows how to use SDXL Turbo for image-to-image generation. It loads an initial image, sets up the pipeline, and transforms the image based on a given prompt with specific strength and inference steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image, make_image_grid\n\n# 체크포인트를 불러올 때 추가 메모리 소모를 피하려면 from_pipe를 사용하세요.\npipeline = AutoPipelineForImage2Image.from_pipe(pipeline_text2image).to(\"cuda\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\ninit_image = init_image.resize((512, 512))\n\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n\nimage = pipeline(prompt, image=init_image, strength=0.5, guidance_scale=0.0, num_inference_steps=2).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Text Embeddings in a DreamBooth Model\nDESCRIPTION: Shows how to create weighted text embeddings for both a positive prompt and a negative prompt using a DreamBooth fine-tuned model. The example includes a detailed prompt describing a fantasy creature (waffle-hippopotamus hybrid) and a negative prompt to avoid common image generation artifacts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom sd_embed.embedding_funcs import get_weighted_text_embeddings_sd15\n\nprompt = \"\"\"dndcoverart of A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. \nThis imaginative creature features the distinctive, bulky body of a hippo, \nbut with a texture and appearance resembling a golden-brown, crispy waffle. \nThe creature might have elements like waffle squares across its skin and a syrup-like sheen. \nIt's set in a surreal environment that playfully combines a natural water habitat of a hippo with elements of a breakfast table setting, \npossibly including oversized utensils or plates in the background. \nThe image should evoke a sense of playful absurdity and culinary fantasy.\n\"\"\"\n\nneg_prompt = \"\"\"\\\nskin spots,acnes,skin blemishes,age spot,(ugly:1.2),(duplicate:1.2),(morbid:1.21),(mutilated:1.2),\\\n(tranny:1.2),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.2),(bad proportions:1.3),\\\nextra limbs,(disfigured:1.2),(missing arms:1.2),(extra legs:1.2),(fused fingers:1.5),\\\n(too many fingers:1.5),(unclear eyes:1.2),lowers,bad hands,missing fingers,extra digit,\\\nbad hands,missing fingers,(extra arms and legs),(worst quality:2),(low quality:2),\\\n(normal quality:2),lowres,((monochrome)),((grayscale))\n\"\"\"\n\n(\n    prompt_embeds\n    , prompt_neg_embeds\n) = get_weighted_text_embeddings_sd15(\n    pipe\n    , prompt = prompt\n    , neg_prompt = neg_prompt\n)\n```\n\n----------------------------------------\n\nTITLE: Inference Optimization with Reduced Steps and Lightweight VAE\nDESCRIPTION: Shows how to optimize inference speed by reducing denoising steps and using a lightweight VAE encoder/decoder, achieving up to 3x speedup with minimal quality loss.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\n\npipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n    \"prs-eth/marigold-depth-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(\"cuda\")\n\npipe.vae = diffusers.AutoencoderTiny.from_pretrained(\n    \"madebyollin/taesd\", torch_dtype=torch.float16\n).cuda()\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\ndepth = pipe(image, num_inference_steps=1)\n```\n\n----------------------------------------\n\nTITLE: Upgrading VAE Component for Quality Improvement\nDESCRIPTION: Replaces the default VAE in the pipeline with a more advanced version to potentially enhance the quality of generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.vae = vae\nimages = pipeline(**get_inputs(batch_size=8)).images\nmake_image_grid(images, rows=2, cols=4)\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Weights from Hugging Face Repository\nDESCRIPTION: This snippet demonstrates how to load a Stable Diffusion pipeline and apply LoRA weights from a Hugging Face repository. The example uses 'stabilityai/stable-diffusion-xl-base-1.0' model and loads 'ByteDance/SDXL-Lightning' LoRA weights for faster inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\npipeline.to(\"cuda\")\n\n# Load LoRA weights\npipeline.load_lora_weights(\"ByteDance/SDXL-Lightning\", weight_name=\"sdxl_lightning_4step_lora.safetensors\")\n\n# Use the pipeline with LoRA weights\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe\"\nimage = pipeline(prompt=prompt, num_inference_steps=4).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Integrating DeepCache with Stable Diffusion Pipeline\nDESCRIPTION: Python code snippet showing how to load a Stable Diffusion pipeline, initialize DeepCacheSDHelper, set parameters, and enable caching for accelerated inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/deepcache.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained('stable-diffusion-v1-5/stable-diffusion-v1-5', torch_dtype=torch.float16).to(\"cuda\")\n\nfrom DeepCache import DeepCacheSDHelper\nhelper = DeepCacheSDHelper(pipe=pipe)\nhelper.set_params(\n    cache_interval=3,\n    cache_branch_id=0,\n)\nhelper.enable()\n\nimage = pipe(\"a photo of an astronaut on a moon\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Text-to-Video Generation with CogVideoX in Python\nDESCRIPTION: Complete example for generating video from a text prompt using CogVideoX. Demonstrates loading the pipeline, configuring memory optimization with CPU offload and VAE tiling, setting generation parameters, and exporting the result to an MP4 file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/cogvideox.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.utils import export_to_video\n\nprompt = \"An elderly gentleman, with a serene expression, sits at the water's edge, a steaming cup of tea by his side. He is engrossed in his artwork, brush in hand, as he renders an oil painting on a canvas that's propped up against a small, weathered table. The sea breeze whispers through his silver hair, gently billowing his loose-fitting white shirt, while the salty air adds an intangible element to his masterpiece in progress. The scene is one of tranquility and inspiration, with the artist's canvas capturing the vibrant hues of the setting sun reflecting off the tranquil sea.\"\n\npipe = CogVideoXPipeline.from_pretrained(\n    \"THUDM/CogVideoX-5b\",\n    torch_dtype=torch.bfloat16\n)\n\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\nvideo = pipe(\n    prompt=prompt,\n    num_videos_per_prompt=1,\n    num_inference_steps=50,\n    num_frames=49,\n    guidance_scale=6,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\n\nexport_to_video(video, \"output.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Optimizer Creation for Model Training\nDESCRIPTION: This snippet shows how to create an optimizer for updating the UNet parameters during training, including setting learning rates and weight decay parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optimizer_cls(\n    unet.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Text-to-Image Web Demo with Gradio and Stable Diffusion\nDESCRIPTION: This Python script creates a web demo for a text-to-image Stable Diffusion pipeline using Gradio. It uses the StableDiffusionPipeline and Gradio's Interface.from_pipeline function to generate an intuitive drag-and-drop interface in the browser.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/overview.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport gradio as gr\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n\ngr.Interface.from_pipeline(pipe).launch()\n```\n\n----------------------------------------\n\nTITLE: Inpainting with Stable Diffusion\nDESCRIPTION: Demonstrates how to use the Stable Diffusion Inpainting model for inpainting tasks. It loads the model, prepares the images, and generates an inpainted image based on a given prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\ngenerator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Stable Diffusion with Static Reshaping\nDESCRIPTION: Code demonstrating how to optimize inference speed by statically reshaping the model with specific batch size, height, width, and number of images parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define the shapes related to the inputs and desired outputs\nbatch_size, num_images, height, width = 1, 1, 512, 512\n\n# Statically reshape the model\npipeline.reshape(batch_size, height, width, num_images)\n# Compile the model before inference\npipeline.compile()\n\nimage = pipeline(\n    prompt,\n    height=height,\n    width=width,\n    num_images_per_prompt=num_images,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Textual Inversion Embeddings\nDESCRIPTION: Downloads and loads textual inversion embeddings for Stable Diffusion XL, examining its structure which contains separate tensors for the two SDXL text encoders.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/textual_inversion_inference.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nfile = hf_hub_download(\"dn118/unaestheticXL\", filename=\"unaestheticXLv31.safetensors\")\nstate_dict = load_file(file)\nstate_dict\n```\n\n----------------------------------------\n\nTITLE: Initializing OmniGen Pipeline in Python\nDESCRIPTION: Code to load and initialize the OmniGen pipeline using the Diffusers library. Sets up the model with bfloat16 precision on CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/omnigen.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenPipeline\n\npipe = OmniGenPipeline.from_pretrained(\"Shitao/OmniGen-v1-diffusers\", torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Full Precision Video Generation with Mochi Pipeline\nDESCRIPTION: Shows how to generate videos using full precision weights with CPU offload and VAE tiling for optimal quality, requiring 42GB VRAM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/mochi.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MochiPipeline\nfrom diffusers.utils import export_to_video\n\npipe = MochiPipeline.from_pretrained(\"genmo/mochi-1-preview\")\n\n# Enable memory savings\npipe.enable_model_cpu_offload()\npipe.enable_vae_tiling()\n\nprompt = \"Close-up of a chameleon's eye, with its scaly skin changing color. Ultra high resolution 4k.\"\n\nwith torch.autocast(\"cuda\", torch.bfloat16, cache_enabled=False):\n      frames = pipe(prompt, num_frames=85).frames[0]\n\nexport_to_video(frames, \"mochi.mp4\", fps=30)\n```\n\n----------------------------------------\n\nTITLE: Using Different Prompts for Each SDXL Text-Encoder\nDESCRIPTION: This snippet demonstrates how to utilize SDXL's dual text-encoders by passing different prompts to each one. The first prompt is passed to CLIP-ViT/L-14 and the second prompt to OpenCLIP-ViT/bigG-14, which can improve image quality and allow style mixing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n\n# prompt is passed to OAI CLIP-ViT/L-14\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n# prompt_2 is passed to OpenCLIP-ViT/bigG-14\nprompt_2 = \"Van Gogh painting\"\nimage = pipeline(prompt=prompt, prompt_2=prompt_2).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Replacing Scheduler in Flax Stable Diffusion Pipeline\nDESCRIPTION: This code snippet shows how to replace the scheduler in a Flax Stable Diffusion pipeline using the DDPM-Solver++ scheduler. It includes setting up the pipeline, preparing inputs, and generating images across multiple devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/schedulers.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline, FlaxDPMSolverMultistepScheduler\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nscheduler, scheduler_state = FlaxDPMSolverMultistepScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\"\n)\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    model_id,\n    scheduler=scheduler,\n    variant=\"bf16\",\n    dtype=jax.numpy.bfloat16,\n)\nparams[\"scheduler\"] = scheduler_state\n\n# Generate 1 image per parallel device (8 on TPUv2-8 or TPUv3-8)\nprompt = \"a photo of an astronaut riding a horse on mars\"\nnum_samples = jax.device_count()\nprompt_ids = pipeline.prepare_inputs([prompt] * num_samples)\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 25\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Base and Refiner Models for Ensemble Denoising\nDESCRIPTION: Code for loading both SDXL base and refiner models to be used together as an ensemble of denoisers. The refiner reuses the text encoder and VAE from the base model to optimize memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nbase = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipe.to(\"cuda\")\n\nrefiner = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n)\nrefiner.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading ControlNet Model in PyTorch\nDESCRIPTION: This python code loads the ControlNet model, either from existing weights specified by `args.controlnet_model_name_or_path`, or initializes it from a UNet if no path is provided. This allows for fine-tuning an existing ControlNet or training a new one from scratch.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"if args.controlnet_model_name_or_path:\n    logger.info(\\\"Loading existing controlnet weights\\\")\n    controlnet = ControlNetModel.from_pretrained(args.controlnet_model_name_or_path)\nelse:\n    logger.info(\\\"Initializing controlnet weights from unet\\\")\n    controlnet = ControlNetModel.from_unet(unet)\"\n```\n\n----------------------------------------\n\nTITLE: Inference with Single Concept Custom Diffusion Model in Python\nDESCRIPTION: This code demonstrates how to load and use a trained Custom Diffusion model for inference with a single custom concept. It loads the base Stable Diffusion model, applies the custom attention weights, and loads the textual inversion embeddings for the new concept.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16,\n).to(\"cuda\")\npipeline.unet.load_attn_procs(\"path-to-save-model\", weight_name=\"pytorch_custom_diffusion_weights.bin\")\npipeline.load_textual_inversion(\"path-to-save-model\", weight_name=\"<new1>.bin\")\n\nimage = pipeline(\n    \"<new1> cat sitting in a bucket\",\n    num_inference_steps=100,\n    guidance_scale=6.0,\n    eta=1.0,\n).images[0]\nimage.save(\"cat.png\")\n```\n\n----------------------------------------\n\nTITLE: Basic Text-to-Image Generation with Würstchen AutoPipeline\nDESCRIPTION: Example showing how to use the combined AutoPipelineForText2Image for generating images from text prompts. Demonstrates basic usage with default parameters and multiple image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wuerstchen.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS\n\npipe = AutoPipelineForText2Image.from_pretrained(\"warp-ai/wuerstchen\", torch_dtype=torch.float16).to(\"cuda\")\n\ncaption = \"Anthropomorphic cat dressed as a fire fighter\"\nimages = pipe(\n    caption,\n    width=1024,\n    height=1536,\n    prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,\n    prior_guidance_scale=4.0,\n    num_images_per_prompt=2,\n).images\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with ControlNet and Reference Images in Diffusers\nDESCRIPTION: This code generates an image using a diffusion pipe with both ControlNet conditioning and a reference image. It sets parameters for the generation process including the prompt, number of inference steps, conditioning scale, and style fidelity options.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_63\n\nLANGUAGE: python\nCODE:\n```\n# generate image\nimage = pipe(\n    prompt=\"a cat\",\n    num_inference_steps=20,\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\n    image=canny_image,\n    ref_image=ref_image,\n    reference_attn=False,\n    reference_adain=True,\n    style_fidelity=1.0,\n    generator=torch.Generator(\"cuda\").manual_seed(42)\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Training DDPM UNet on Oxford Flowers Dataset\nDESCRIPTION: Command to train a DDPM UNet model on the Oxford Flowers dataset using Accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_unconditional.py \\\n  --dataset_name=\"huggan/flowers-102-categories\" \\\n  --resolution=64 --center_crop --random_flip \\\n  --output_dir=\"ddpm-ema-flowers-64\" \\\n  --train_batch_size=16 \\\n  --num_epochs=100 \\\n  --gradient_accumulation_steps=1 \\\n  --use_ema \\\n  --learning_rate=1e-4 \\\n  --lr_warmup_steps=500 \\\n  --mixed_precision=no \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Advanced Pipeline Initialization and Generation\nDESCRIPTION: Detailed example showing how to initialize and use the prior and decoder pipelines separately. This approach provides more control over the generation process by handling Stage C and Stages B/A independently.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wuerstchen.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import WuerstchenDecoderPipeline, WuerstchenPriorPipeline\nfrom diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS\n\ndevice = \"cuda\"\ndtype = torch.float16\nnum_images_per_prompt = 2\n\nprior_pipeline = WuerstchenPriorPipeline.from_pretrained(\n    \"warp-ai/wuerstchen-prior\", torch_dtype=dtype\n).to(device)\ndecoder_pipeline = WuerstchenDecoderPipeline.from_pretrained(\n    \"warp-ai/wuerstchen\", torch_dtype=dtype\n).to(device)\n\ncaption = \"Anthropomorphic cat dressed as a fire fighter\"\nnegative_prompt = \"\"\n\nprior_output = prior_pipeline(\n    prompt=caption,\n    height=1024,\n    width=1536,\n    timesteps=DEFAULT_STAGE_C_TIMESTEPS,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=num_images_per_prompt,\n)\ndecoder_output = decoder_pipeline(\n    image_embeddings=prior_output.image_embeddings,\n    prompt=caption,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n).images[0]\ndecoder_output\n```\n\n----------------------------------------\n\nTITLE: Memory-Optimized Video Generation with CPU Offloading and VAE Slicing\nDESCRIPTION: Advanced example showing how to generate longer videos (64 frames/8 seconds) while optimizing memory usage through CPU offloading and VAE slicing techniques.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.enable_model_cpu_offload()\n\n# memory optimization\npipe.enable_vae_slicing()\n\nprompt = \"Darth Vader surfing a wave\"\nvideo_frames = pipe(prompt, num_frames=64).frames[0]\nvideo_path = export_to_video(video_frames)\nvideo_path\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training with PlaygroundAI Model\nDESCRIPTION: Complete command for training DreamBooth with LoRA on a PlaygroundAI model, including parameters for batch size, learning rate, validation, and model output settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth_lora_sdxl.py \\\n  --pretrained_model_name_or_path=\"playgroundai/playground-v2.5-1024px-aesthetic\"  \\\n  --instance_data_dir=\"dog\" \\\n  --output_dir=\"dog-playground-lora\" \\\n  --mixed_precision=\"fp16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --learning_rate=1e-4 \\\n  --use_8bit_adam \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: FRESCO Video Processing Configuration in Python\nDESCRIPTION: Setup for FRESCO video-to-video translation pipeline, including video frame extraction and processing utilities.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_103\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nimport cv2\nimport torch\nimport numpy as np\n\nfrom diffusers import ControlNetModel, DDIMScheduler, DiffusionPipeline\nimport sys\n\ngmflow_dir = \"/path/to/gmflow\"\nsys.path.insert(0, gmflow_dir)\n\ndef video_to_frame(video_path: str, interval: int):\n    vidcap = cv2.VideoCapture(video_path)\n    success = True\n\n    count = 0\n    res = []\n    while success:\n        count += 1\n        success, image = vidcap.read()\n        if count % interval != 1:\n            continue\n        if image is not None:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            res.append(image)\n            if len(res) >= 8:\n                break\n\n    vidcap.release()\n    return res\n\n\ninput_video_path = 'https://github.com/williamyang1991/FRESCO/raw/main/data/car-turn.mp4'\noutput_video_path = 'car.gif'\n```\n\n----------------------------------------\n\nTITLE: Implementing SDXL with OpenVINO Pipeline\nDESCRIPTION: Example of loading and running inference with Stable Diffusion XL using the OpenVINO pipeline, demonstrating basic image generation from a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"sailing ship in storm by Rembrandt\"\nimage = pipeline(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Cascade Original Checkpoints with from_single_file Method\nDESCRIPTION: This example shows how to load original Stable Cascade model checkpoints using the from_single_file method. It demonstrates loading both prior and decoder models directly from safetensors files hosted on Hugging Face.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_cascade.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import (\n    StableCascadeDecoderPipeline,\n    StableCascadePriorPipeline,\n    StableCascadeUNet,\n)\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior_unet = StableCascadeUNet.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-cascade/resolve/main/stage_c_bf16.safetensors\",\n    torch_dtype=torch.bfloat16\n)\ndecoder_unet = StableCascadeUNet.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-cascade/blob/main/stage_b_bf16.safetensors\",\n    torch_dtype=torch.bfloat16\n)\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", prior=prior_unet, torch_dtype=torch.bfloat16)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", decoder=decoder_unet, torch_dtype=torch.bfloat16)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade-single-file.png\")\n```\n\n----------------------------------------\n\nTITLE: Combining ControlNet and T2I Adapter in a Single Pipeline\nDESCRIPTION: This snippet demonstrates how to create a pipeline that combines both ControlNet and T2I Adapter for enhanced control over image generation. The pipeline can use both control mechanisms simultaneously or independently by adjusting their respective conditioning scales.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_78\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nimport torch\nfrom controlnet_aux.midas import MidasDetector\nfrom PIL import Image\n\nfrom diffusers import AutoencoderKL, ControlNetModel, MultiAdapter, T2IAdapter\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\nfrom diffusers.utils import load_image\nfrom examples.community.pipeline_stable_diffusion_xl_controlnet_adapter import (\n    StableDiffusionXLControlNetAdapterPipeline,\n)\n\ncontrolnet_depth = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-depth-sdxl-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True\n)\nadapter_depth = T2IAdapter.from_pretrained(\n  \"TencentARC/t2i-adapter-depth-midas-sdxl-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\n\npipe = StableDiffusionXLControlNetAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet_depth,\n    adapter=adapter_depth,\n    vae=vae,\n    variant=\"fp16\",\n    use_safetensors=True,\n    torch_dtype=torch.float16,\n)\npipe = pipe.to(\"cuda\")\npipe.enable_xformers_memory_efficient_attention()\n```\n\n----------------------------------------\n\nTITLE: FLUX Redux Pipeline Implementation\nDESCRIPTION: Shows how to use the FLUX Redux Pipeline for image-to-image generation. Demonstrates combining prior redux pipeline with base pipeline for efficient memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPriorReduxPipeline, FluxPipeline\nfrom diffusers.utils import load_image\ndevice = \"cuda\"\ndtype = torch.bfloat16\n\nrepo_redux = \"black-forest-labs/FLUX.1-Redux-dev\"\nrepo_base = \"black-forest-labs/FLUX.1-dev\" \npipe_prior_redux = FluxPriorReduxPipeline.from_pretrained(repo_redux, torch_dtype=dtype).to(device)\npipe = FluxPipeline.from_pretrained(\n    repo_base, \n    text_encoder=None,\n    text_encoder_2=None,\n    torch_dtype=torch.bfloat16\n).to(device)\n\nimage = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy/img5.png\")\npipe_prior_output = pipe_prior_redux(image)\nimages = pipe(\n    guidance_scale=2.5,\n    num_inference_steps=50,\n    generator=torch.Generator(\"cpu\").manual_seed(0),\n    **pipe_prior_output,\n).images\nimages[0].save(\"flux-redux.png\")\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation Pipeline\nDESCRIPTION: Complete example of using the three-stage IF pipeline for text-to-image generation with CPU offloading for memory optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil, make_image_grid\nimport torch\n\n# stage 1\nstage_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_model_cpu_offload()\n\n# stage 2\nstage_2 = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_model_cpu_offload()\n\n# stage 3\nsafety_modules = {\n    \"feature_extractor\": stage_1.feature_extractor,\n    \"safety_checker\": stage_1.safety_checker,\n    \"watermarker\": stage_1.watermarker,\n}\nstage_3 = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16\n)\nstage_3.enable_model_cpu_offload()\n\nprompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"very deep learning\"'\ngenerator = torch.manual_seed(1)\n\n# text embeds\nprompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)\n\n# stage 1\nstage_1_output = stage_1(\n    prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\"\n).images\n\n# stage 2\nstage_2_output = stage_2(\n    image=stage_1_output,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\n\n# stage 3\nstage_3_output = stage_3(prompt=prompt, image=stage_2_output, noise_level=100, generator=generator).images\n\nmake_image_grid([pt_to_pil(stage_1_output)[0], pt_to_pil(stage_2_output)[0], stage_3_output[0]], rows=1, rows=3)\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt2Prompt Pipeline for Controlled Image Editing in Diffusers\nDESCRIPTION: This code demonstrates the Prompt2Prompt technique for controlled image editing through prompt manipulation. It shows how to use ReplaceEdit to change words in a prompt while maintaining visual consistency between generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nimport numpy as np\nfrom PIL import Image\n\npipe = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", \n    custom_pipeline=\"pipeline_prompt2prompt\"\n).to(\"cuda\")\n\nprompts = [\n    \"A turtle playing with a ball\",\n    \"A monkey playing with a ball\"\n]\n\ncross_attention_kwargs = {\n    \"edit_type\": \"replace\",\n    \"cross_replace_steps\": 0.4,\n    \"self_replace_steps\": 0.4\n}\n\noutputs = pipe(\n    prompt=prompts,\n    height=512,\n    width=512,\n    num_inference_steps=50,\n    cross_attention_kwargs=cross_attention_kwargs\n)\n\noutputs.images[0].save(\"output_image_0.png\")\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using AnyText Diffusion Pipeline in Python\nDESCRIPTION: This code snippet demonstrates how to set up and use the AnyText diffusion pipeline for generating text in images. It loads the necessary models, sets up the pipeline, and generates an image with specified text using a given prompt and drawing position.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/anytext/README.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom anytext_controlnet import AnyTextControlNetModel\nfrom diffusers.utils import load_image\n\n\nanytext_controlnet = AnyTextControlNetModel.from_pretrained(\"tolgacangoz/anytext-controlnet\", torch_dtype=torch.float16,\n                                                            variant=\"fp16\",)\npipe = DiffusionPipeline.from_pretrained(\"tolgacangoz/anytext\", font_path=\"arial-unicode-ms.ttf\",\n                                          controlnet=anytext_controlnet, torch_dtype=torch.float16,\n                                          trust_remote_code=False,  # One needs to give permission to run this pipeline's code\n                                          ).to(\"cuda\")\n\n# generate image\nprompt = 'photo of caramel macchiato coffee on the table, top-down perspective, with \"Any\" \"Text\" written on it using cream'\ndraw_pos = load_image(\"https://raw.githubusercontent.com/tyxsspa/AnyText/refs/heads/main/example_images/gen9.png\")\n# There are two modes: \"generate\" and \"edit\". \"edit\" mode requires `ori_image` parameter for the image to be edited.\nimage = pipe(prompt, num_inference_steps=20, mode=\"generate\", draw_pos=draw_pos,\n             ).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Transformation for LCM Training in Python\nDESCRIPTION: Function to preprocess images for the Text2ImageDataset class. It resizes the input image, applies random cropping, converts to tensor format, and normalizes the pixel values to the range [-1, 1].\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef transform(example):\n    image = example[\"image\"]\n    image = TF.resize(image, resolution, interpolation=transforms.InterpolationMode.BILINEAR)\n\n    c_top, c_left, _, _ = transforms.RandomCrop.get_params(image, output_size=(resolution, resolution))\n    image = TF.crop(image, c_top, c_left, resolution, resolution)\n    image = TF.to_tensor(image)\n    image = TF.normalize(image, [0.5], [0.5])\n\n    example[\"image\"] = image\n    return example\n```\n\n----------------------------------------\n\nTITLE: Implementing Image-to-Image Generation with LCM-LoRA in Dreamshaper\nDESCRIPTION: Sets up an image-to-image pipeline using LCM-LoRA for efficient transformation with minimal computational resources. This approach loads the LCM-LoRA weights into the Dreamshaper model and configures it with the LCMScheduler to modify an input image according to a text prompt in just 4 steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image, LCMScheduler\nfrom diffusers.utils import make_image_grid, load_image\n\npipe = AutoPipelineForImage2Image.from_pretrained(\n    \"Lykon/dreamshaper-7\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\nprompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt,\n    image=init_image,\n    num_inference_steps=4,\n    guidance_scale=1,\n    strength=0.6,\n    generator=generator\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Running Image-to-Image Generation with Latent Consistency Model\nDESCRIPTION: This code demonstrates how to transform an input image using a text prompt with the Latent Consistency Model. The strength parameter controls how much the input image will be modified, from minimal changes (0) to complete overwriting (1).\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_75\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n\ninput_image=Image.open(\"myimg.png\")\n\nstrength = 0.5  # strength =0 (no change) strength=1 (completely overwrite image)\n\n# Can be set to 1~50 steps. LCM supports fast inference even <= 4 steps. Recommend: 1~8 steps.\nnum_inference_steps = 4\n\nimages = pipe(prompt=prompt, image=input_image, strength=strength, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n```\n\n----------------------------------------\n\nTITLE: Using Flux Fill Pipeline for Inpainting/Outpainting\nDESCRIPTION: Demonstrates the Flux Fill pipeline for both inpainting and outpainting tasks. Unlike regular inpainting pipelines, it doesn't require a strength parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxFillPipeline\nfrom diffusers.utils import load_image\n\nimage = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/cup.png\")\nmask = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/cup_mask.png\")\n\nrepo_id = \"black-forest-labs/FLUX.1-Fill-dev\"\npipe = FluxFillPipeline.from_pretrained(repo_id, torch_dtype=torch.bfloat16).to(\"cuda\")\n\nimage = pipe(\n    prompt=\"a white paper cup\",\n    image=image,\n    mask_image=mask,\n    height=1632,\n    width=1232,\n    max_sequence_length=512,\n    generator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Launching Training on Multiple GPUs\nDESCRIPTION: Command to launch the unconditional image generation training script on multiple GPUs using Accelerate's multi-GPU feature.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --multi_gpu train_unconditional.py \\\n  --dataset_name=\"huggan/flowers-102-categories\" \\\n  --output_dir=\"ddpm-ema-flowers-64\" \\\n  --mixed_precision=\"fp16\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Combining FLUX Features with Turbo LoRAs\nDESCRIPTION: Example of combining FLUX Turbo LoRAs with Control, Fill and Redux pipelines for few-steps inference. Shows integration with depth control and HyperSD.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxControlPipeline\nfrom image_gen_aux import DepthPreprocessor\nfrom diffusers.utils import load_image\nfrom huggingface_hub import hf_hub_download\nimport torch\n\ncontrol_pipe = FluxControlPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\ncontrol_pipe.load_lora_weights(\"black-forest-labs/FLUX.1-Depth-dev-lora\", adapter_name=\"depth\")\ncontrol_pipe.load_lora_weights(\n    hf_hub_download(\"ByteDance/Hyper-SD\", \"Hyper-FLUX.1-dev-8steps-lora.safetensors\"), adapter_name=\"hyper-sd\"\n)\ncontrol_pipe.set_adapters([\"depth\", \"hyper-sd\"], adapter_weights=[0.85, 0.125])\ncontrol_pipe.enable_model_cpu_offload()\n\nprompt = \"A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.\"\ncontrol_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\n\nprocessor = DepthPreprocessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\ncontrol_image = processor(control_image)[0].convert(\"RGB\")\n\nimage = control_pipe(\n    prompt=prompt,\n    control_image=control_image,\n    height=1024,\n    width=1024,\n    num_inference_steps=8,\n    guidance_scale=10.0,\n    generator=torch.Generator().manual_seed(42),\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading CogVideoX Pipeline Checkpoints in Python\nDESCRIPTION: Code for loading different CogVideoX pipelines from pretrained checkpoints. Shows how to load both the standard text-to-video pipeline and the image-to-video pipeline with appropriate data types.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/cogvideox.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import CogVideoXPipeline, CogVideoXImageToVideoPipeline\npipe = CogVideoXPipeline.from_pretrained(\n    \"THUDM/CogVideoX-2b\",\n    torch_dtype=torch.float16\n)\n\npipe = CogVideoXImageToVideoPipeline.from_pretrained(\n    \"THUDM/CogVideoX-5b-I2V\",\n    torch_dtype=torch.bfloat16\n)\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Trained Textual Inversion Model in Flax\nDESCRIPTION: Python code to run inference with a Textual Inversion model trained in Flax/JAX. The code loads the trained model, prepares inputs, shards them across devices, and generates images using the custom token in a JAX-optimized workflow.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline\n\nmodel_path = \"path-to-your-trained-model\"\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(model_path, dtype=jax.numpy.bfloat16)\n\nprompt = \"A <cat-toy> train\"\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\nimage.save(\"cat-train.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating Video from Image with Stable Video Diffusion Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use the StableVideoDiffusionPipeline to generate a video from an input image. It includes steps for loading the model, preparing the input image, and generating video frames.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# reduce memory requirements\npipeline.enable_model_cpu_offload()\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\nimage = image.resize((1024, 576))\n\ngenerator = torch.manual_seed(42)\nframes = pipeline(image, decode_chunk_size=8, generator=generator).frames[0]\nexport_to_video(frames, \"generated.mp4\", fps=7)\n```\n\n----------------------------------------\n\nTITLE: Enabling Memory Optimization for Diffusion Pipelines\nDESCRIPTION: Shows how to enable CPU offloading and xFormers memory-efficient attention in a diffusion pipeline. These optimizations help run diffusion models on limited GPU resources by keeping only necessary components in GPU memory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_16\n\nLANGUAGE: diff\nCODE:\n```\n+ pipeline.enable_model_cpu_offload()\n+ pipeline.enable_xformers_memory_efficient_attention()\n```\n\n----------------------------------------\n\nTITLE: Performing Image-to-Image Generation with SDXL Turbo\nDESCRIPTION: Demonstrates image-to-image generation using SDXL Turbo. It loads an initial image, sets up the pipeline, and generates a new image based on the input image and a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image, make_image_grid\n\n# use from_pipe to avoid consuming additional memory when loading a checkpoint\npipeline_image2image = AutoPipelineForImage2Image.from_pipe(pipeline_text2image).to(\"cuda\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\ninit_image = init_image.resize((512, 512))\n\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n\nimage = pipeline_image2image(prompt, image=init_image, strength=0.5, guidance_scale=0.0, num_inference_steps=2).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Image-to-Text Generation with UniDiffuser\nDESCRIPTION: Demonstrates image-to-text generation with UniDiffuser, where the model generates descriptive text conditioned on a provided image. This is the reverse of text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unidiffuser.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\nfrom diffusers.utils import load_image\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Image-to-text generation\nimage_url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unidiffuser/unidiffuser_example_image.jpg\"\ninit_image = load_image(image_url).resize((512, 512))\n\nsample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)\ni2t_text = sample.text[0]\nprint(i2t_text)\n```\n\n----------------------------------------\n\nTITLE: Running Compiled Inference with Latte Pipeline\nDESCRIPTION: Compiles pipeline components using torch.compile and generates a video from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latte.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline.transformer = torch.compile(pipeline.transformer)\npipeline.vae.decode = torch.compile(pipeline.vae.decode)\n\nvideo = pipeline(prompt=\"A dog wearing sunglasses floating in space, surreal, nebulae in background\").frames[0]\n```\n\n----------------------------------------\n\nTITLE: Creating Image-to-Image Pipeline with AutoPipelineForImage2Image\nDESCRIPTION: Shows how to load and use the AutoPipelineForImage2Image class to transform an existing image based on a new prompt. This example transforms a Godzilla eating sushi image into one where Godzilla is eating burgers in a fast food restaurant.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipe_img2img = AutoPipelineForImage2Image.from_pretrained(\n    \"dreamlike-art/dreamlike-photoreal-2.0\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png\")\n\nprompt = \"cinematic photo of Godzilla eating burgers with a cat in a fast food restaurant, 35mm photograph, film, professional, 4k, highly detailed\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(53)\nimage = pipe_img2img(prompt, image=init_image, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image-to-Image Chain Pipeline with Stable Diffusion\nDESCRIPTION: This snippet sets up an image-to-image pipeline using Stable Diffusion v1.5 for the first part of an image-to-image-to-image chaining process. It prepares the model with optimizations like CPU offloading and memory-efficient attention.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n```\n\n----------------------------------------\n\nTITLE: Community Discussion Link\nDESCRIPTION: Markdown link to the Community tab documentation for project collaboration and discussions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md#2025-04-11_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n[**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\n```\n\n----------------------------------------\n\nTITLE: Implementing Denoising Loop\nDESCRIPTION: Main denoising loop that transforms noise into an image by iteratively predicting noise residuals and computing previous noisy samples using the scheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\n\nscheduler.set_timesteps(num_inference_steps)\n\nfor t in tqdm(scheduler.timesteps):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n\n    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n    # compute the previous noisy sample x_t -> x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n```\n\n----------------------------------------\n\nTITLE: Text-guided Image-to-Image Variation with Stable unCLIP\nDESCRIPTION: This code demonstrates how to use Stable unCLIP for image-to-image variation. It loads a pre-trained pipeline and generates variations of an input image, with an optional text prompt for guidance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_unclip.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableUnCLIPImg2ImgPipeline\nfrom diffusers.utils import load_image\nimport torch\n\npipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1-unclip\", torch_dtype=torch.float16, variation=\"fp16\"\n)\npipe = pipe.to(\"cuda\")\n\nurl = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/tarsila_do_amaral.png\"\ninit_image = load_image(url)\n\nimages = pipe(init_image).images\nimages[0].save(\"variation_image.png\")\n```\n\n----------------------------------------\n\nTITLE: Inpainting with Kandinsky 2.2\nDESCRIPTION: Demonstrates how to use the Kandinsky 2.2 model for inpainting. It loads the model using AutoPipelineForInpainting, which uses the KandinskyV22InpaintCombinedPipeline under the hood.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16\n)\npipeline.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Training DDPM UNet Model with ONNXRuntime\nDESCRIPTION: Launch a training script for an unconditional diffusion model on the Oxford Flowers dataset, using ONNXRuntime for acceleration and specifying various training hyperparameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/unconditional_image_generation/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_unconditional.py \\\n  --dataset_name=\"huggan/flowers-102-categories\" \\\n  --resolution=64 --center_crop --random_flip \\\n  --output_dir=\"ddpm-ema-flowers-64\" \\\n  --use_ema \\\n  --train_batch_size=16 \\\n  --num_epochs=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-4 \\\n  --lr_warmup_steps=500 \\\n  --mixed_precision=fp16\n```\n\n----------------------------------------\n\nTITLE: Unconditional Image and Text Generation with UniDiffuser\nDESCRIPTION: Demonstrates how to use UniDiffuserPipeline for joint generation of image and text pairs from a standard Gaussian prior. This generates both an image and corresponding text without any conditioning input.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unidiffuser.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Unconditional image and text generation. The generation task is automatically inferred.\nsample = pipe(num_inference_steps=20, guidance_scale=8.0)\nimage = sample.images[0]\ntext = sample.text[0]\nimage.save(\"unidiffuser_joint_sample_image.png\")\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized T5-XXL Text Encoder for Stable Diffusion 3 in Python\nDESCRIPTION: This code demonstrates how to load a quantized version of the T5-XXL text encoder using the BitsAndBytesConfig, and use it with the StableDiffusion3Pipeline for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\nfrom transformers import T5EncoderModel, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel_id = \"stabilityai/stable-diffusion-3-medium-diffusers\"\ntext_encoder = T5EncoderModel.from_pretrained(\n    model_id,\n    subfolder=\"text_encoder_3\",\n    quantization_config=quantization_config,\n)\npipe = StableDiffusion3Pipeline.from_pretrained(\n    model_id,\n    text_encoder_3=text_encoder,\n    device_map=\"balanced\",\n    torch_dtype=torch.float16\n)\n\nimage = pipe(\n    prompt=\"a photo of a cat holding a sign that says hello world\",\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    height=1024,\n    width=1024,\n    guidance_scale=7.0,\n).images[0]\n\nimage.save(\"sd3_hello_world-8bit-T5.png\")\n```\n\n----------------------------------------\n\nTITLE: Flux Image Generation with Remote VAE Decoding in Python\nDESCRIPTION: Demonstrates the process of generating an image using the Flux model with remote VAE decoding. It includes pipeline setup, latent generation, and remote decoding with Flux-specific parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_decode.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxPipeline\n\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-schnell\",\n    torch_dtype=torch.bfloat16,\n    vae=None,\n).to(\"cuda\")\n\nprompt = \"Strawberry ice cream, in a stylish modern glass, coconut, splashing milk cream and honey, in a gradient purple background, fluid motion, dynamic movement, cinematic lighting, Mysterious\"\n\nlatent = pipe(\n    prompt=prompt,\n    guidance_scale=0.0,\n    num_inference_steps=4,\n    output_type=\"latent\",\n).images\nimage = remote_decode(\n    endpoint=\"https://whhx50ex1aryqvw6.us-east-1.aws.endpoints.huggingface.cloud/\",\n    tensor=latent,\n    height=1024,\n    width=1024,\n    scaling_factor=0.3611,\n    shift_factor=0.1159,\n)\nimage.save(\"test.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Hugging Face Hub Community Pipeline with DiffusionPipeline\nDESCRIPTION: This snippet demonstrates how to load a community pipeline from the Hugging Face Hub using the from_pretrained method with the custom_pipeline parameter pointing to a Hub repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"google/ddpm-cifar10-32\", custom_pipeline=\"hf-internal-testing/diffusers-dummy-pipeline\", use_safetensors=True\n)\n```\n\n----------------------------------------\n\nTITLE: MultiControlNet Canny Image Preparation - Python\nDESCRIPTION: Prepares a canny edge detection image for use with MultiControlNet by masking specific regions. This allows for combining multiple conditioning inputs without overlap.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image, make_image_grid\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n)\nimage = np.array(original_image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\n\n# zero out middle columns of image where pose will be overlaid\nzero_start = image.shape[1] // 4\nzero_end = zero_start + image.shape[1] // 2\nimage[:, zero_start:zero_end] = 0\n\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\nmake_image_grid([original_image, canny_image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple LoRA Adapters with Different Weights\nDESCRIPTION: Combines the pixel-art and toy-face adapters with different weights (0.5 and 1.0 respectively) to create a blended style in the generated image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipe.set_adapters([\"pixel\", \"toy\"], adapter_weights=[0.5, 1.0])\n```\n\n----------------------------------------\n\nTITLE: Running PromptDiffusion Pipeline\nDESCRIPTION: This Python script demonstrates how to use the PromptDiffusion pipeline for image generation. It loads the necessary models, sets up the scheduler, enables memory optimization, and generates an image based on a text prompt, an image pair and a query image. The script uses `PromptDiffusionPipeline` and `PromptDiffusionControlNetModel` from the `diffusers` library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/promptdiffusion/README.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```py\nimport torch\nfrom diffusers import UniPCMultistepScheduler\nfrom diffusers.utils import load_image\nfrom promptdiffusioncontrolnet import PromptDiffusionControlNetModel\nfrom pipeline_prompt_diffusion import PromptDiffusionPipeline\n\n\nfrom PIL import ImageOps\n\nimage_a = ImageOps.invert(load_image(\"https://github.com/Zhendong-Wang/Prompt-Diffusion/blob/main/images_to_try/house_line.png?raw=true\"))\n\nimage_b = load_image(\"https://github.com/Zhendong-Wang/Prompt-Diffusion/blob/main/images_to_try/house.png?raw=true\")\nquery = ImageOps.invert(load_image(\"https://github.com/Zhendong-Wang/Prompt-Diffusion/blob/main/images_to_try/new_01.png?raw=true\"))\n\n# load prompt diffusion controlnet and prompt diffusion\n\ncontrolnet = PromptDiffusionControlNetModel.from_pretrained(\"iczaw/prompt-diffusion-diffusers\", subfolder=\"controlnet\", torch_dtype=torch.float16)\nmodel_id = \"path-to-model\"\npipe = PromptDiffusionPipeline.from_pretrained(\"iczaw/prompt-diffusion-diffusers\", subfolder=\"base\", controlnet=controlnet, torch_dtype=torch.float16, variant=\"fp16\")\n\n# speed up diffusion process with faster scheduler and memory optimization\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# remove following line if xformers is not installed\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\n# generate image\ngenerator = torch.manual_seed(0)\nimage = pipe(\"a tortoise\", num_inference_steps=20, generator=generator, image_pair=[image_a,image_b], image=query).images[0]\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing TensorRT Image2Image Stable Diffusion Pipeline in Python\nDESCRIPTION: This snippet shows how to use TensorRT to accelerate the Image2Image Stable Diffusion inference. It sets up a pipeline with a DDIMScheduler, loads a pre-trained model, and generates an image based on an input image and a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nimport torch\nfrom diffusers import DDIMScheduler\nfrom diffusers import DiffusionPipeline\n\n# Use the DDIMScheduler scheduler here instead\nscheduler = DDIMScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n                                            subfolder=\"scheduler\")\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n                                            custom_pipeline=\"stable_diffusion_tensorrt_img2img\",\n                                            variant='fp16',\n                                            torch_dtype=torch.float16,\n                                            scheduler=scheduler,)\n\n# re-use cached folder to save ONNX models and TensorRT Engines\npipe.set_cached_folder(\"stabilityai/stable-diffusion-2-1\", variant='fp16',)\n\npipe = pipe.to(\"cuda\")\n\nurl = \"https://pajoca.com/wp-content/uploads/2022/09/tekito-yamakawa-1.png\"\nresponse = requests.get(url)\ninput_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\nprompt = \"photorealistic new zealand hills\"\nimage = pipe(prompt, image=input_image, strength=0.75,).images[0]\nimage.save('tensorrt_img2img_new_zealand_hills.png')\n```\n\n----------------------------------------\n\nTITLE: Implementing ControlNet+T2I Adapter+Inpainting Pipeline in Python\nDESCRIPTION: Creates a complete pipeline for image inpainting with SDXL using ControlNet for depth conditioning and T2I Adapter. This implementation loads pre-trained models, sets up a MidasDetector for depth map generation, and performs inpainting with depth-guided generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_80\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nimport torch\nfrom controlnet_aux.midas import MidasDetector\nfrom PIL import Image\n\nfrom diffusers import AutoencoderKL, ControlNetModel, MultiAdapter, T2IAdapter\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel\nfrom diffusers.utils import load_image\nfrom examples.community.pipeline_stable_diffusion_xl_controlnet_adapter_inpaint import (\n    StableDiffusionXLControlNetAdapterInpaintPipeline,\n)\n\ncontrolnet_depth = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-depth-sdxl-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True\n)\nadapter_depth = T2IAdapter.from_pretrained(\n  \"TencentARC/t2i-adapter-depth-midas-sdxl-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\n\npipe = StableDiffusionXLControlNetAdapterInpaintPipeline.from_pretrained(\n    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n    controlnet=controlnet_depth,\n    adapter=adapter_depth,\n    vae=vae,\n    variant=\"fp16\",\n    use_safetensors=True,\n    torch_dtype=torch.float16,\n)\npipe = pipe.to(\"cuda\")\npipe.enable_xformers_memory_efficient_attention()\n# pipe.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)\nmidas_depth = MidasDetector.from_pretrained(\n  \"valhalla/t2iadapter-aux-models\", filename=\"dpt_large_384.pt\", model_type=\"dpt_large\"\n).to(\"cuda\")\n\nprompt = \"a tiger sitting on a park bench\"\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\nimage = load_image(img_url).resize((1024, 1024))\nmask_image = load_image(mask_url).resize((1024, 1024))\n\ndepth_image = midas_depth(\n  image, detect_resolution=512, image_resolution=1024\n)\n\nstrength = 0.4\n\nimages = pipe(\n    prompt,\n    image=image,\n    mask_image=mask_image,\n    control_image=depth_image,\n    adapter_image=depth_image,\n    num_inference_steps=30,\n    controlnet_conditioning_scale=strength,\n    adapter_conditioning_scale=strength,\n    strength=0.7,\n).images\nimages[0].save(\"controlnet_and_adapter_inpaint.png\")\n```\n\n----------------------------------------\n\nTITLE: Launching DreamBooth Training with PyTorch in Terminal\nDESCRIPTION: Command-line instructions for starting DreamBooth training using Accelerate with PyTorch, specifying model paths, instance data, output directory, and key training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport INSTANCE_DIR=\"./dog\"\nexport OUTPUT_DIR=\"path_to_saved_model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=400 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Inference with StableDiffusionPipeline in Python\nDESCRIPTION: Demonstrates how to load a trained model and generate images using StableDiffusionPipeline. The code loads a pretrained model, runs inference with specific parameters, and saves the output image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"path-to-your-trained-model\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A photo of sks dog in a bucket\"\nimage = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n\nimage.save(\"dog-bucket.png\")\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion LoRA Inference with Hugging Face Pipeline\nDESCRIPTION: Load a pre-trained LoRA model and generate images using the StableDiffusionPipeline with custom prompts\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_path = \"sayakpaul/sd-model-finetuned-lora-t4\"\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\npipe.unet.load_attn_procs(model_path)\npipe.to(\"cuda\")\n\nprompt = \"A naruto with green eyes and red legs.\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Load UNet Model - Python\nDESCRIPTION: Load a UNet model from a pretrained directory and update its state dictionary, setting it up for training or fine-tuning as part of the text-to-image generation script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nload_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder=\"unet\")\nmodel.register_to_config(**load_model.config)\n\nmodel.load_state_dict(load_model.state_dict())\n```\n\n----------------------------------------\n\nTITLE: Using IP-Adapter FaceID Models in Python\nDESCRIPTION: This code snippet shows how to use IP-Adapter FaceID models by extracting face embeddings with insightface and passing them to the StableDiffusionPipeline. It includes face detection and embedding extraction steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\nfrom diffusers.utils import load_image\nfrom insightface.app import FaceAnalysis\n\npipeline = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\npipeline.load_ip_adapter(\"h94/IP-Adapter-FaceID\", subfolder=None, weight_name=\"ip-adapter-faceid_sd15.bin\", image_encoder_folder=None)\npipeline.set_ip_adapter_scale(0.6)\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl1.png\")\n\nref_images_embeds = []\napp = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\nimage = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\nfaces = app.get(image)\nimage = torch.from_numpy(faces[0].normed_embedding)\nref_images_embeds.append(image.unsqueeze(0))\nref_images_embeds = torch.stack(ref_images_embeds, dim=0).unsqueeze(0)\nneg_ref_images_embeds = torch.zeros_like(ref_images_embeds)\nid_embeds = torch.cat([neg_ref_images_embeds, ref_images_embeds]).to(dtype=torch.float16, device=\"cuda\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(42)\n\nimages = pipeline(\n    prompt=\"A photo of a girl\",\n    ip_adapter_image_embeds=[id_embeds],\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n    num_inference_steps=20, num_images_per_prompt=1,\n    generator=generator\n).images\n```\n\n----------------------------------------\n\nTITLE: Creating Image-to-Image Web Demo with Gradio and Stable Diffusion\nDESCRIPTION: This Python script demonstrates how to create a web demo for an image-to-image Stable Diffusion pipeline using Gradio. It uses the StableDiffusionImg2ImgPipeline and Gradio's Interface.from_pipeline function to generate a user-friendly web interface.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/overview.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionImg2ImgPipeline\nimport gradio as gr\n\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n\ngr.Interface.from_pipeline(pipe).launch()\n```\n\n----------------------------------------\n\nTITLE: Hotswapping LoRA Adapters with Torch Compiled Models\nDESCRIPTION: This snippet demonstrates how to use LoRA hotswapping with compiled models to avoid recompilation. It shows the correct sequence of enabling hotswap capability, loading the initial adapter, compiling the model, and then hotswapping adapters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe = ...\n# call this extra method\npipe.enable_lora_hotswap(target_rank=max_rank)\n# now load adapter 1\npipe.load_lora_weights(file_name_adapter_1)\n# now compile the unet of the pipeline\npipe.unet = torch.compile(pipeline.unet, ...)\n# generate some images with adapter 1\n...\n# now hot swap adapter 2\npipeline.load_lora_weights(file_name_adapter_2, hotswap=True, adapter_name=\"default_0\")\n# generate images with adapter 2\n```\n\n----------------------------------------\n\nTITLE: Baseline SDXL Pipeline Implementation without Optimizations\nDESCRIPTION: Setting up a baseline Stable Diffusion XL pipeline with scaled dot product attention and reduced precision disabled to establish performance baseline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\n\n# Load the pipeline in full-precision and place its model components on CUDA.\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\"\n).to(\"cuda\")\n\n# Run the attention ops without SDPA.\npipe.unet.set_default_attn_processor()\npipe.vae.set_default_attn_processor()\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```\n\n----------------------------------------\n\nTITLE: Text-to-Video Generation with Diffusers (PyTorch)\nDESCRIPTION: Shows how to use a Diffusers pipeline to generate a video from a text prompt.  It initializes the pipeline, defines a detailed prompt, and then uses the pipeline to generate the video frames.  The generated video is then saved to a file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n      \"prompt = (\\n    \\\"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. \\\"\\n    \\\"The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other \\\"\\n    \\\"pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, \\\"\\n    \\\"casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. \\\"\\n    \\\"The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical \\\"\\n    \\\"atmosphere of this unique musical performance.\\\"\\n)\\nvideo = pipe(prompt=prompt, guidance_scale=6, num_inference_steps=50).frames[0]\n# This utilized about 14.79 GB. It can be further reduced by using tiling and using leaf_level offloading throughout the pipeline.\nprint(f\\\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\\\")\nexport_to_video(video, \\\"output.mp4\\\", fps=8)\"\n```\n\n----------------------------------------\n\nTITLE: Launching SDXL Training with Accelerate\nDESCRIPTION: Complete bash command for launching SDXL training using the Accelerate library. This includes setting environment variables for the model, VAE, and dataset paths, along with various training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch train_text_to_image_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --enable_xformers_memory_efficient_attention \\\n  --resolution=512 \\\n  --center_crop \\\n  --random_flip \\\n  --proportion_empty_prompts=0.2 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=10000 \\\n  --use_8bit_adam \\\n  --learning_rate=1e-06 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --mixed_precision=\"fp16\" \\\n  --report_to=\"wandb\" \\\n  --validation_prompt=\"a cute Sundar Pichai creature\" \\\n  --validation_epochs 5 \\\n  --checkpointing_steps=5000 \\\n  --output_dir=\"sdxl-naruto-model\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Implementing AnimateDiff with ControlNet for Video Animation in Python\nDESCRIPTION: This code demonstrates how to combine AnimateDiff with ControlNet to generate controlled animated videos. It loads required models including a motion adapter, controlnet, and VAE, then generates an animated sequence based on conditioning frames with pose information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_88\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoencoderKL, ControlNetModel, MotionAdapter, DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_gif\nfrom PIL import Image\n\nmotion_id = \"guoyww/animatediff-motion-adapter-v1-5-2\"\nadapter = MotionAdapter.from_pretrained(motion_id)\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_openpose\", torch_dtype=torch.float16)\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16)\n\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = DiffusionPipeline.from_pretrained(\n    model_id,\n    motion_adapter=adapter,\n    controlnet=controlnet,\n    vae=vae,\n    custom_pipeline=\"pipeline_animatediff_controlnet\",\n    torch_dtype=torch.float16,\n).to(device=\"cuda\")\npipe.scheduler = DPMSolverMultistepScheduler.from_pretrained(\n    model_id, subfolder=\"scheduler\", beta_schedule=\"linear\", clip_sample=False, timestep_spacing=\"linspace\", steps_offset=1\n)\npipe.enable_vae_slicing()\n\nconditioning_frames = []\nfor i in range(1, 16 + 1):\n    conditioning_frames.append(Image.open(f\"frame_{i}.png\"))\n\nprompt = \"astronaut in space, dancing\"\nnegative_prompt = \"bad quality, worst quality, jpeg artifacts, ugly\"\nresult = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=512,\n    height=768,\n    conditioning_frames=conditioning_frames,\n    num_inference_steps=20,\n).frames[0]\n\nexport_to_gif(result.frames[0], \"result.gif\")\n```\n\n----------------------------------------\n\nTITLE: Generating Image Batch with Optimized Pipeline\nDESCRIPTION: Utilizes the fully optimized pipeline (float16, efficient scheduler, attention slicing) to generate a batch of images, demonstrating improved efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_inputs(batch_size=1):\n    generator = [torch.Generator(\"cuda\").manual_seed(i) for i in range(batch_size)]\n    prompts = batch_size * [prompt]\n    num_inference_steps = 20\n\n    return {\"prompt\": prompts, \"generator\": generator, \"num_inference_steps\": num_inference_steps}\n\nimages = pipeline(**get_inputs(batch_size=8)).images\nmake_image_grid(images, rows=2, cols=4)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with StableDiffusionPipeline in Python\nDESCRIPTION: This snippet demonstrates how to generate images using the StableDiffusionPipeline with a fixed seed for reproducibility. It uses the v1-4 Stable Diffusion checkpoint to generate images based on given prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nimages = sd_pipeline(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n```\n\n----------------------------------------\n\nTITLE: Downloading and Using Local Models with Git LFS\nDESCRIPTION: This snippet shows how to download model weights locally using Git LFS and then load them into a DiffusionPipeline. This is useful for working offline or with local model copies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n!git lfs install\n!git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5\n```\n\n----------------------------------------\n\nTITLE: Setting Granular LoRA Weight Scales in Diffusers\nDESCRIPTION: This snippet demonstrates how to load a LoRA adapter and set custom scaling factors for different components of the model. It allows for fine-grained control over how much influence the LoRA weights have in different parts of the model architecture.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipe = ... # create pipeline\npipe.load_lora_weights(..., adapter_name=\"my_adapter\")\nscales = {\n    \"text_encoder\": 0.5,\n    \"text_encoder_2\": 0.5,  # only usable if pipe has a 2nd text encoder\n    \"unet\": {\n        \"down\": 0.9,  # all transformers in the down-part will use scale 0.9\n        # \"mid\"  # in this example \"mid\" is not given, therefore all transformers in the mid part will use the default scale 1.0\n        \"up\": {\n            \"block_0\": 0.6,  # all 3 transformers in the 0th block in the up-part will use scale 0.6\n            \"block_1\": [0.4, 0.8, 1.0],  # the 3 transformers in the 1st block in the up-part will use scales 0.4, 0.8 and 1.0 respectively\n        }\n    }\n}\npipe.set_adapters(\"my_adapter\", scales)\n```\n\n----------------------------------------\n\nTITLE: Using Strength Parameter with AutoPipelineForImage2Image in Diffusers\nDESCRIPTION: This snippet demonstrates how to use the strength parameter in an image-to-image pipeline to control how much the generated image resembles the initial image. A higher strength value gives the model more creative freedom while a lower value produces results more similar to the input image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, image=init_image, strength=0.8).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: CogVideoX LoRA Inference with Python\nDESCRIPTION: Python code demonstrating how to load and use a trained LoRA model with CogVideoXPipeline for generating videos from text prompts. Shows pipeline setup, LoRA weight loading, and video generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.utils import export_to_video\n\npipe = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16)\n# pipe.load_lora_weights(\"/path/to/lora/weights\", adapter_name=\"cogvideox-lora\") # Or,\npipe.load_lora_weights(\"my-awesome-hf-username/my-awesome-lora-name\", adapter_name=\"cogvideox-lora\") # If loading from the HF Hub\npipe.to(\"cuda\")\n\n# Assuming lora_alpha=32 and rank=64 for training. If different, set accordingly\npipe.set_adapters([\"cogvideox-lora\"], [32 / 64])\n\nprompt = (\n    \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The \"\n    \"panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other \"\n    \"pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, \"\n    \"casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. \"\n    \"The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical \"\n    \"atmosphere of this unique musical performance\"\n)\nframes = pipe(prompt, guidance_scale=6, use_dynamic_cfg=True).frames[0]\nexport_to_video(frames, \"output.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Applying Text-Based Edits to Video with InstructPix2Pix\nDESCRIPTION: This code shows how to apply text-based edits to a video using InstructPix2Pix by passing a prompt describing the desired changes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"make it Van Gogh Starry Night style\"\nresult = pipeline(prompt=[prompt] * len(video), image=video).images\nimageio.mimsave(\"edited_video.mp4\", result, fps=4)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JAX Parallelization with pmap\nDESCRIPTION: Shows how to use jax.pmap for Single Program Multiple Data (SPMD) parallelization of the Stable Diffusion generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\np_generate = pmap(pipeline._generate)\n\nimages = p_generate(prompt_ids, p_params, rng)\nimages = images.block_until_ready()\nimages.shape\n```\n\n----------------------------------------\n\nTITLE: Loading StableDiffusionXL Pipeline from Single File\nDESCRIPTION: Demonstrates loading a StableDiffusionXL pipeline from a remote checkpoint file using the from_single_file method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\n\nckpt_path = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0_0.9vae.safetensors\"\npipeline = StableDiffusionXLPipeline.from_single_file(ckpt_path)\n```\n\n----------------------------------------\n\nTITLE: FastAPI Endpoint for Image Generation\nDESCRIPTION: Implementation of the FastAPI endpoint that handles image generation requests asynchronously. The function creates a thread-specific pipeline instance and uses asyncio to process requests without blocking the main thread.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/create_a_server.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@app.post(\"/v1/images/generations\")\nasync def generate_image(image_input: TextToImageInput):\n    try:\n        loop = asyncio.get_event_loop()\n        scheduler = shared_pipeline.pipeline.scheduler.from_config(shared_pipeline.pipeline.scheduler.config)\n        pipeline = StableDiffusion3Pipeline.from_pipe(shared_pipeline.pipeline, scheduler=scheduler)\n        generator = torch.Generator(device=\"cuda\")\n        generator.manual_seed(random.randint(0, 10000000))\n        output = await loop.run_in_executor(None, lambda: pipeline(image_input.prompt, generator = generator))\n        logger.info(f\"output: {output}\")\n        image_url = save_image(output.images[0])\n        return {\"data\": [{\"url\": image_url}]}\n    except Exception as e:\n        if isinstance(e, HTTPException):\n            raise e\n        elif hasattr(e, 'message'):\n            raise HTTPException(status_code=500, detail=e.message + traceback.format_exc())\n        raise HTTPException(status_code=500, detail=str(e) + traceback.format_exc())\n```\n\n----------------------------------------\n\nTITLE: Performing Text-to-Image Generation with SDXL Turbo\nDESCRIPTION: Executes text-to-image generation using SDXL Turbo. It sets up the pipeline, defines a prompt, and generates an image with specific parameters like guidance scale and inference steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline_text2image = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline_text2image = pipeline_text2image.to(\"cuda\")\n\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n\nimage = pipeline_text2image(prompt=prompt, guidance_scale=0.0, num_inference_steps=1).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Initializing Gaudi Stable Diffusion Pipeline\nDESCRIPTION: Sets up the Stable Diffusion pipeline with Gaudi-specific configurations including the DDIM scheduler and HPU deployment settings\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/habana.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.habana import GaudiConfig\nfrom optimum.habana.diffusers import GaudiDDIMScheduler, GaudiStableDiffusionPipeline\n\nmodel_name = \"stabilityai/stable-diffusion-2-base\"\nscheduler = GaudiDDIMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\npipeline = GaudiStableDiffusionPipeline.from_pretrained(\n    model_name,\n    scheduler=scheduler,\n    use_habana=True,\n    use_hpu_graphs=True,\n    gaudi_config=\"Habana/stable-diffusion-2\",\n)\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22Img2ImgPipeline in Python\nDESCRIPTION: This snippet defines the KandinskyV22Img2ImgPipeline class, which is used for image-to-image transformations in Kandinsky 2.2. It includes the __call__ method for generating new images based on input images and text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22Img2ImgPipeline\n\t- all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Ensemble Denoising\nDESCRIPTION: Demonstrates image generation using both base and refiner models in ensemble mode. The base model handles high-noise steps while the refiner handles low-noise steps for optimal quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A majestic lion jumping from a big stone at night\"\n\nimage = base(\n    prompt=prompt,\n    num_inference_steps=40,\n    denoising_end=0.8,\n    output_type=\"latent\",\n).images\nimage = refiner(\n    prompt=prompt,\n    num_inference_steps=40,\n    denoising_start=0.8,\n    image=image,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Load Scheduler and Tokenizer - Python\nDESCRIPTION: The script initializes a noise scheduler and tokenizer, which are essential components for generating images conditioned on text.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnoise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\ntokenizer = CLIPTokenizer.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Trained LoRA Model for Inference\nDESCRIPTION: Load a pre-trained LoRA model and generate images using the trained weights\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lora.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"path/to/lora/model\", weight_name=\"pytorch_lora_weights.safetensors\")\nimage = pipeline(\"A naruto with blue eyes\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Training SDXL with DPO\nDESCRIPTION: Command for training SDXL with Diffusion DPO using LoRA. Uses a custom VAE model, 8-bit Adam optimizer, and includes validation and wandb logging.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/diffusion_dpo/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_diffusion_dpo_sdxl.py \\\n  --pretrained_model_name_or_path=stabilityai/stable-diffusion-xl-base-1.0  \\\n  --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\n  --output_dir=\"diffusion-sdxl-dpo\" \\\n  --mixed_precision=\"fp16\" \\\n  --dataset_name=kashif/pickascore \\\n  --train_batch_size=8 \\\n  --gradient_accumulation_steps=2 \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --rank=8 \\\n  --learning_rate=1e-5 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=2000 \\\n  --checkpointing_steps=500 \\\n  --run_validation --validation_steps=50 \\\n  --seed=\"0\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: AnimateDiff ControlNet Pipeline Setup in Python\nDESCRIPTION: Shows the initial setup for using AnimateDiff with ControlNet integration, allowing for controlled video generation using depth maps or other control signals. Includes necessary imports and ControlNet model loading.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffControlNetPipeline, AutoencoderKL, ControlNetModel, MotionAdapter, LCMScheduler\nfrom diffusers.utils import export_to_gif, load_video\n\n# Additionally, you will need a preprocess videos before they can be used with the ControlNet\n# HF maintains just the right package for it: `pip install controlnet_aux`\nfrom controlnet_aux.processor import ZoeDetector\n\n# Download controlnets from https://huggingface.co/lllyasviel/ControlNet-v1-1 to use .from_single_file\n# Download Diffusers-format controlnets, such as https://huggingface.co/lllyasviel/sd-controlnet-depth, to use .from_pretrained()\ncontrolnet = ControlNetModel.from_single_file(\"control_v11f1p_sd15_depth.pth\", torch_dtype=torch.float16)\n```\n\n----------------------------------------\n\nTITLE: Setting Memory Layout to Channels_Last for Optimal Compilation\nDESCRIPTION: Changing the UNet and VAE's memory layout to channels_last format to ensure maximum speed when used with torch.compile. This memory layout is optimized for GPU operations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe.unet.to(memory_format=torch.channels_last)\npipe.vae.to(memory_format=torch.channels_last)\n```\n\n----------------------------------------\n\nTITLE: Base to Refiner Sequential Processing\nDESCRIPTION: Implements sequential processing where the base model generates an initial image and the refiner model enhances it with additional details. Shows the basic pipeline setup and image generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = base(prompt=prompt, output_type=\"latent\").images[0]\nimage = refiner(prompt=prompt, image=image[None, :]).images[0]\n```\n\n----------------------------------------\n\nTITLE: Using Timestep-distilled Flux Model in Python\nDESCRIPTION: Demonstrates how to use the timestep-distilled Flux model for text-to-image generation. This variant requires max_sequence_length <= 256, guidance_scale = 0, and benefits from fewer sampling steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPipeline\n\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload()\n\nprompt = \"A cat holding a sign that says hello world\"\nout = pipe(\n    prompt=prompt,\n    guidance_scale=0.,\n    height=768,\n    width=1360,\n    num_inference_steps=4,\n    max_sequence_length=256,\n).images[0]\nout.save(\"image.png\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Complete Pipeline Components\nDESCRIPTION: Sets up all components needed for a StableDiffusionPipeline including UNet, VAE, scheduler, and text encoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/push_to_hub.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import (\n    UNet2DConditionModel,\n    AutoencoderKL,\n    DDIMScheduler,\n    StableDiffusionPipeline,\n)\nfrom transformers import CLIPTextModel, CLIPTextConfig, CLIPTokenizer\n\nunet = UNet2DConditionModel(\n    block_out_channels=(32, 64),\n    layers_per_block=2,\n    sample_size=32,\n    in_channels=4,\n    out_channels=4,\n    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n    cross_attention_dim=32,\n)\n\nscheduler = DDIMScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n)\n\nvae = AutoencoderKL(\n    block_out_channels=[32, 64],\n    in_channels=3,\n    out_channels=3,\n    down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n    up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n    latent_channels=4,\n)\n\ntext_encoder_config = CLIPTextConfig(\n    bos_token_id=0,\n    eos_token_id=2,\n    hidden_size=32,\n    intermediate_size=37,\n    layer_norm_eps=1e-05,\n    num_attention_heads=4,\n    num_hidden_layers=5,\n    pad_token_id=1,\n    vocab_size=1000,\n)\ntext_encoder = CLIPTextModel(text_encoder_config)\ntokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n```\n\n----------------------------------------\n\nTITLE: RefineEdit with Local Blend Configuration for Prompt2Prompt Pipeline\nDESCRIPTION: This code configures the Prompt2Prompt pipeline for RefineEdit with local blend functionality. It enables adding words to a prompt while only affecting specific image regions related to those words, maintaining consistency in unrelated areas.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_70\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\"A turtle\",\n           \"A turtle in a forest\"]\n\ncross_attention_kwargs = {\n    \"edit_type\": \"refine\",\n    \"cross_replace_steps\": 0.4,\n    \"self_replace_steps\": 0.4,\n    \"local_blend_words\": [\"in\", \"a\" , \"forest\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Kandinsky Decoder Model Components\nDESCRIPTION: Loads the necessary components for training a Kandinsky 2.2 decoder model including the VQModel for decoding latents, image encoder, and UNet model. The decoder transforms embeddings into actual images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n    vae = VQModel.from_pretrained(\n        args.pretrained_decoder_model_name_or_path, subfolder=\"movq\", torch_dtype=weight_dtype\n    ).eval()\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n        args.pretrained_prior_model_name_or_path, subfolder=\"image_encoder\", torch_dtype=weight_dtype\n    ).eval()\nunet = UNet2DConditionModel.from_pretrained(args.pretrained_decoder_model_name_or_path, subfolder=\"unet\")\n```\n\n----------------------------------------\n\nTITLE: Training DeepFloyd IF Stage 1 with Full DreamBooth\nDESCRIPTION: This script demonstrates how to train the full model (not LoRA) for stage 1 of DeepFloyd IF with DreamBooth. It uses 8-bit Adam optimizer to save memory, a very low learning rate to prevent model degradation, and skips saving the large text encoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"DeepFloyd/IF-I-XL-v1.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"dreambooth_if\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=64 \\\n  --train_batch_size=4 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-7 \\\n  --max_train_steps=150 \\\n  --validation_prompt \"a photo of sks dog\" \\\n  --validation_steps 25 \\\n  --text_encoder_use_attention_mask \\\n  --tokenizer_max_length 77 \\\n  --pre_compute_text_embeddings \\\n  --use_8bit_adam \\\n  --set_grads_to_none \\\n  --skip_save_text_encoder \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Configuring Generator for Reproducible Results\nDESCRIPTION: Sets up a CUDA generator with fixed seed for reproducible image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/stable_diffusion.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n```\n\n----------------------------------------\n\nTITLE: Enabling Attention Slicing for Memory Optimization\nDESCRIPTION: Shows how to enable attention slicing to reduce memory pressure during inference. This is particularly important for systems with less than 64GB RAM or when generating images at non-standard resolutions larger than 512×512 pixels.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/mps.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline.enable_attention_slicing()\n```\n\n----------------------------------------\n\nTITLE: Implementing AYS Schedule in SDXL Pipeline\nDESCRIPTION: Shows how to configure and use the AYS sampling schedule in a Stable Diffusion XL pipeline with DPMSolver scheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/scheduler_features.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"SG161222/RealVisXL_V4.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, algorithm_type=\"sde-dpmsolver++\")\n\nprompt = \"A cinematic shot of a cute little rabbit wearing a jacket and doing a thumbs up\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(2487854446)\nimage = pipeline(\n    prompt=prompt,\n    negative_prompt=\"\",\n    generator=generator,\n    timesteps=sampling_schedule,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading Textual Inversion Embeddings in Python\nDESCRIPTION: Shows how to load and use textual inversion embeddings with a base model to generate images in GTA5 artwork style.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\nprompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, <gta5-artwork> style\"\nimage = pipeline(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Textual Inversion (Bash)\nDESCRIPTION: Commands to clone the diffusers repository, install dependencies, and set up the Accelerate environment for textual inversion fine-tuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\npip install -r requirements.txt\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Training DeepFloyd IF Stage 2 with LoRA and DreamBooth\nDESCRIPTION: This script shows how to train the second stage upscaler of DeepFloyd IF with LoRA and DreamBooth. It uses a higher 256x256 resolution, timestep conditioning, and validation images for upscaling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"DeepFloyd/IF-II-L-v1.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"dreambooth_dog_upscale\"\nexport VALIDATION_IMAGES=\"dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png\"\n\npython train_dreambooth_lora.py \\\n    --report_to wandb \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --instance_data_dir=$INSTANCE_DIR \\\n    --output_dir=$OUTPUT_DIR \\\n    --instance_prompt=\"a sks dog\" \\\n    --resolution=256 \\\n    --train_batch_size=4 \\\n    --gradient_accumulation_steps=1 \\\n    --learning_rate=1e-6 \\\n    --max_train_steps=2000 \\\n    --validation_prompt=\"a sks dog\" \\\n    --validation_epochs=100 \\\n    --checkpointing_steps=500 \\\n    --pre_compute_text_embeddings \\\n    --tokenizer_max_length=77 \\\n    --text_encoder_use_attention_mask \\\n    --validation_images $VALIDATION_IMAGES \\\n    --class_labels_conditioning=timesteps\n```\n\n----------------------------------------\n\nTITLE: Image to Video Pipeline with Layerwise Casting\nDESCRIPTION: Demonstrates advanced memory optimization technique using layerwise casting for transformer models. Reduces memory requirements by approximately 50% by downcast model weights to float8 precision while maintaining computational accuracy.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom diffusers import AutoencoderKLWan, WanTransformer3DModel, WanImageToVideoPipeline\nfrom diffusers.hooks.group_offloading import apply_group_offloading\nfrom diffusers.utils import export_to_video, load_image\nfrom transformers import UMT5EncoderModel, CLIPVisionModel\n\nmodel_id = \"Wan-AI/Wan2.1-I2V-14B-720P-Diffusers\"\nimage_encoder = CLIPVisionModel.from_pretrained(\n    model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32\n)\ntext_encoder = UMT5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n\ntransformer = WanTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\ntransformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Initializing MidasDetector and Generating Depth-Guided Images with ControlNet+T2I Adapter\nDESCRIPTION: Sets up a MidasDetector for depth map generation and uses it with a combined ControlNet and T2I Adapter pipeline to create an image of a tiger on a park bench. The depth map is used as conditioning for both the ControlNet and Adapter components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_79\n\nLANGUAGE: python\nCODE:\n```\nmidas_depth = MidasDetector.from_pretrained(\n  \"valhalla/t2iadapter-aux-models\", filename=\"dpt_large_384.pt\", model_type=\"dpt_large\"\n).to(\"cuda\")\n\nprompt = \"a tiger sitting on a park bench\"\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n\nimage = load_image(img_url).resize((1024, 1024))\n\ndepth_image = midas_depth(\n  image, detect_resolution=512, image_resolution=1024\n)\n\nstrength = 0.5\n\nimages = pipe(\n    prompt,\n    control_image=depth_image,\n    adapter_image=depth_image,\n    num_inference_steps=30,\n    controlnet_conditioning_scale=strength,\n    adapter_conditioning_scale=strength,\n).images\nimages[0].save(\"controlnet_and_adapter.png\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion ControlNet Reference Pipeline in Python\nDESCRIPTION: Combines ControlNet with reference control using Canny edge detection. Uses UniPCMultistepScheduler with float16 precision and processes images with OpenCV.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom diffusers import UniPCMultistepScheduler\nfrom diffusers.utils import load_image\n\ninput_image = load_image(\"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\")\n\n# get canny image\nimage = cv2.Canny(np.array(input_image), 100, 200)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetReferencePipeline.from_pretrained(\n       \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n       controlnet=controlnet,\n       safety_checker=None,\n       torch_dtype=torch.float16\n       ).to('cuda:0')\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\nresult_img = pipe(ref_image=input_image,\n      prompt=\"1girl\",\n      image=canny_image,\n      num_inference_steps=20,\n      reference_attn=True,\n      reference_adain=True).images[0]\n```\n\n----------------------------------------\n\nTITLE: Using GGUF Quantized Checkpoints with Lumina2 in Python\nDESCRIPTION: Shows how to load and use GGUF quantized checkpoints for the Lumina2 model. Includes configuration for quantization and bfloat16 compute type for optimal performance and memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/lumina2.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import Lumina2Transformer2DModel, Lumina2Pipeline, GGUFQuantizationConfig \n\nckpt_path = \"https://huggingface.co/calcuis/lumina-gguf/blob/main/lumina2-q4_0.gguf\"\ntransformer = Lumina2Transformer2DModel.from_single_file(\n    ckpt_path,\n    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n    torch_dtype=torch.bfloat16,\n)\n\npipe = Lumina2Pipeline.from_pretrained(\n    \"Alpha-VLLM/Lumina-Image-2.0\", transformer=transformer, torch_dtype=torch.bfloat16\n)\npipe.enable_model_cpu_offload()\nimage = pipe(\n    \"a cat holding a sign that says hello\",\n    generator=torch.Generator(\"cpu\").manual_seed(0),\n).images[0]\nimage.save(\"lumina-gguf.png\")\n```\n\n----------------------------------------\n\nTITLE: Passing Conditioning Information to ControlNet\nDESCRIPTION: This python code shows how the conditioning text embeddings and image are passed to the ControlNet model during the training loop. The `controlnet` function takes the noisy latents, timesteps, encoder hidden states, and the controlnet conditioning image as input.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"encoder_hidden_states = text_encoder(batch[\\\"input_ids\\\"])[0]\ncontrolnet_image = batch[\\\"conditioning_pixel_values\\\"].to(dtype=weight_dtype)\n\ndown_block_res_samples, mid_block_res_sample = controlnet(\n    noisy_latents,\n    timesteps,\n    encoder_hidden_states=encoder_hidden_states,\n    controlnet_cond=controlnet_image,\n    return_dict=False,\n)\"\n```\n\n----------------------------------------\n\nTITLE: Generating images with StableDiffusionDepth2ImgPipeline in Python\nDESCRIPTION: Demonstrates how to use the StableDiffusionDepth2ImgPipeline to generate images based on a text prompt, initial image, and negative prompt. The strength parameter controls how much the original image is transformed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/depth2img.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\ninit_image = load_image(url)\nprompt = \"two tigers\"\nnegative_prompt = \"bad, deformed, ugly, bad anatomy\"\nimage = pipeline(prompt=prompt, image=init_image, negative_prompt=negative_prompt, strength=0.7).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized SanaSprintPipeline with BitsAndBytes\nDESCRIPTION: This snippet demonstrates how to load a quantized SanaSprintPipeline for inference using the BitsAndBytes quantization method. It loads the text encoder and transformer in 8-bit precision and initializes the pipeline with the quantized components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/sana_sprint.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, SanaTransformer2DModel, SanaSprintPipeline\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, AutoModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = AutoModel.from_pretrained(\n    \"Efficient-Large-Model/Sana_Sprint_1.6B_1024px_diffusers\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.bfloat16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = SanaTransformer2DModel.from_pretrained(\n    \"Efficient-Large-Model/Sana_Sprint_1.6B_1024px_diffusers\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.bfloat16,\n)\n\npipeline = SanaSprintPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana_Sprint_1.6B_1024px_diffusers\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.bfloat16,\n    device_map=\"balanced\",\n)\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\nimage = pipeline(prompt).images[0]\nimage.save(\"sana.png\")\n```\n\n----------------------------------------\n\nTITLE: Using Flux Depth Control Model\nDESCRIPTION: Demonstrates the use of Flux Depth Control model for controlled image generation using depth maps. Similar to Canny Control, it uses channel-wise concatenation instead of ControlNet architecture.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxControlPipeline, FluxTransformer2DModel\nfrom diffusers.utils import load_image\nfrom image_gen_aux import DepthPreprocessor\n\npipe = FluxControlPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Depth-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n\nprompt = \"A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.\"\ncontrol_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\n\nprocessor = DepthPreprocessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\ncontrol_image = processor(control_image)[0].convert(\"RGB\")\n\nimage = pipe(\n    prompt=prompt,\n    control_image=control_image,\n    height=1024,\n    width=1024,\n    num_inference_steps=30,\n    guidance_scale=10.0,\n    generator=torch.Generator().manual_seed(42),\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Model Documentation Structure in Markdown\nDESCRIPTION: Markdown structure defining the documentation layout for the AuraFlowTransformer2DModel, including headers and autodoc reference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/aura_flow_transformer2d.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# AuraFlowTransformer2DModel\n\nA Transformer model for image-like data from [AuraFlow](https://blog.fal.ai/auraflow/).\n\n## AuraFlowTransformer2DModel\n\n[[autodoc]] AuraFlowTransformer2DModel\n```\n\n----------------------------------------\n\nTITLE: Using PIXART-α Controlnet Pipeline with HED Edge Detection in Python\nDESCRIPTION: This snippet demonstrates how to use the PIXART-α Controlnet Pipeline with HED edge detection. It loads a pre-trained controlnet model, processes an input image with the HED edge detector, and generates a new image based on the edge map with a specified prompt. The pipeline uses the Hugging Face Diffusers library and includes custom transformations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_111\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport os\nimport torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\nfrom pipeline_pixart_alpha_controlnet import PixArtAlphaControlnetPipeline\nfrom diffusers.utils import load_image\n\nfrom diffusers.image_processor import PixArtImageProcessor\n\nfrom controlnet_aux import HEDdetector\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom pixart.controlnet_pixart_alpha import PixArtControlNetAdapterModel\n\ncontrolnet_repo_id = \"raulc0399/pixart-alpha-hed-controlnet\"\n\nweight_dtype = torch.float16\nimage_size = 1024\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntorch.manual_seed(0)\n\n# load controlnet\ncontrolnet = PixArtControlNetAdapterModel.from_pretrained(\n    controlnet_repo_id,\n    torch_dtype=weight_dtype,\n    use_safetensors=True,\n).to(device)\n\npipe = PixArtAlphaControlnetPipeline.from_pretrained(\n    \"PixArt-alpha/PixArt-XL-2-1024-MS\",\n    controlnet=controlnet,\n    torch_dtype=weight_dtype,\n    use_safetensors=True,\n).to(device)\n\nimages_path = \"images\"\ncontrol_image_file = \"0_7.jpg\"\n\nprompt = \"battleship in space, galaxy in background\"\n\ncontrol_image_name = control_image_file.split('.')[0]\n\ncontrol_image = load_image(f\"{images_path}/{control_image_file}\")\nprint(control_image.size)\nheight, width = control_image.size\n\nhed = HEDdetector.from_pretrained(\"lllyasviel/Annotators\")\n\ncondition_transform = T.Compose([\n    T.Lambda(lambda img: img.convert('RGB')),\n    T.CenterCrop([image_size, image_size]),\n])\n\ncontrol_image = condition_transform(control_image)\nhed_edge = hed(control_image, detect_resolution=image_size, image_resolution=image_size)\n\nhed_edge.save(f\"{images_path}/{control_image_name}_hed.jpg\")\n\n# run pipeline\nwith torch.no_grad():\n    out = pipe(\n        prompt=prompt,\n        image=hed_edge,\n        num_inference_steps=14,\n        guidance_scale=4.5,\n        height=image_size,\n        width=image_size,\n    )\n\n    out.images[0].save(f\"{images_path}//{control_image_name}_output.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Image-to-Image Generation with LCM in Dreamshaper\nDESCRIPTION: Configures an image-to-image pipeline using a Latent Consistency Model for efficient transformation of an input image. This implementation loads the LCM checkpoint for Dreamshaper v7 and uses the LCMScheduler to modify an image according to a text prompt in just 4 steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image, UNet2DConditionModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"SimianLuo/LCM_Dreamshaper_v7\",\n    subfolder=\"unet\",\n    torch_dtype=torch.float16,\n)\n\npipe = AutoPipelineForImage2Image.from_pretrained(\n    \"Lykon/dreamshaper-7\",\n    unet=unet,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\nprompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt,\n    image=init_image,\n    num_inference_steps=4,\n    guidance_scale=7.5,\n    strength=0.5,\n    generator=generator\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Turbo Model Checkpoint from Hub\nDESCRIPTION: This code demonstrates how to load the SDXL Turbo model checkpoint from the Hugging Face Hub using the AutoPipelineForText2Image class. It sets up the pipeline for text-to-image generation with float16 precision on a CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline = pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Inference with Fine-tuned Kandinsky2.2 Prior and Decoder\nDESCRIPTION: Python code to load and use fine-tuned Kandinsky2.2 prior and decoder models for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image, DiffusionPipeline\nimport torch\n\npipe_prior = DiffusionPipeline.from_pretrained(output_dir, torch_dtype=torch.float16)\nprior_components = {\"prior_\" + k: v for k,v in pipe_prior.components.items()}\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", **prior_components, torch_dtype=torch.float16)\n\npipe.enable_model_cpu_offload()\nprompt='A robot naruto, 4k photo'\nimages = pipe(prompt=prompt, negative_prompt=negative_prompt).images\nimages[0]\n```\n\n----------------------------------------\n\nTITLE: Importing IPAdapterMaskProcessor in Python\nDESCRIPTION: This snippet shows the import statement for the IPAdapterMaskProcessor class from the image_processor module.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/loaders/ip_adapter.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom image_processor import IPAdapterMaskProcessor\n```\n\n----------------------------------------\n\nTITLE: Using Stable Cascade Lite Models in Python\nDESCRIPTION: Example of loading and using the lite versions of Stage B and Stage C models in Stable Cascade. This demonstrates how to load smaller model variants while maintaining the same pipeline functionality for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_cascade.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import (\n    StableCascadeDecoderPipeline,\n    StableCascadePriorPipeline,\n    StableCascadeUNet,\n)\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade-prior\", subfolder=\"prior_lite\")\ndecoder_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade\", subfolder=\"decoder_lite\")\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", prior=prior_unet)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", decoder=decoder_unet)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade.png\")\n```\n\n----------------------------------------\n\nTITLE: Pose-Based Image Generation with OmniGen\nDESCRIPTION: Illustrates how OmniGen can use pose information from an input image to generate a new image following the same pose but with different content. This demonstrates the model's ability to extract relevant information from input images for controlled generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/omnigen.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenPipeline\nfrom diffusers.utils import load_image \n\npipe = OmniGenPipeline.from_pretrained(\n    \"Shitao/OmniGen-v1-diffusers\",\n    torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\n\nprompt=\"Following the pose of this image <img><|image_1|></img>, generate a new photo: A young boy is sitting on a sofa in the library, holding a book. His hair is neatly combed, and a faint smile plays on his lips, with a few freckles scattered across his cheeks. The library is quiet, with rows of shelves filled with books stretching out behind him.\"\ninput_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/edit.png\")]\nimage = pipe(\n    prompt=prompt, \n    input_images=input_images, \n    guidance_scale=2, \n    img_guidance_scale=1.6,\n    use_input_image_size_as_output=True,\n    generator=torch.Generator(device=\"cpu\").manual_seed(0)\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantized Latte Pipeline\nDESCRIPTION: Sets up a quantized version of the Latte pipeline using 8-bit quantization for reduced memory usage while maintaining performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latte.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, LatteTransformer3DModel, LattePipeline\nfrom diffusers.utils import export_to_gif\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"maxin-cn/Latte-1\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = LatteTransformer3DModel.from_pretrained(\n    \"maxin-cn/Latte-1\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = LattePipeline.from_pretrained(\n    \"maxin-cn/Latte-1\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"A small cactus with a happy face in the Sahara desert.\"\nvideo = pipeline(prompt).frames[0]\nexport_to_gif(video, \"latte.gif\")\n```\n\n----------------------------------------\n\nTITLE: Channels-Last Memory Format Conversion (PyTorch)\nDESCRIPTION: Illustrates how to convert a Diffusers pipeline's UNet to use the channels-last memory format.  It prints the original stride, performs an in-place conversion using `to(memory_format=torch.channels_last)`, and then prints the stride again to verify the conversion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n\"print(pipe.unet.conv_out.state_dict()[\\\"weight\\\"].stride())  # (2880, 9, 3, 1)\npipe.unet.to(memory_format=torch.channels_last)  # in-place operation\nprint(\n    pipe.unet.conv_out.state_dict()[\\\"weight\\\"].stride()\\n)  # (2880, 1, 960, 320) having a stride of 1 for the 2nd dimension proves that it works\"\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Conditioning Images for MultiAdapter\nDESCRIPTION: Code to load pose and depth images that will be used as multiple conditioning inputs for the MultiAdapter. These images provide different types of structural guidance to the image generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image\n\npose_image = load_image(\n    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_input.png\"\n)\ndepth_image = load_image(\n    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_input.png\"\n)\ncond = [pose_image, depth_image]\nprompt = [\"Santa Claus walking into an office room with a beautiful city view\"]\n```\n\n----------------------------------------\n\nTITLE: Using from_pipe Method for Memory-Efficient Pipeline Loading\nDESCRIPTION: This code demonstrates how to use the from_pipe method to load a long prompt weighting community pipeline from an existing Stable Diffusion pipeline without additional memory overhead.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipe_sd = DiffusionPipeline.from_pretrained(\"emilianJR/CyberRealistic_V3\", torch_dtype=torch.float16)\npipe_sd.to(\"cuda\")\n# load long prompt weighting pipeline\npipe_lpw = DiffusionPipeline.from_pipe(\n    pipe_sd,\n    custom_pipeline=\"lpw_stable_diffusion\",\n).to(\"cuda\")\n\nprompt = \"cat, hiding in the leaves, ((rain)), zazie rainyday, beautiful eyes, macro shot, colorful details, natural lighting, amazing composition, subsurface scattering, amazing textures, filmic, soft light, ultra-detailed eyes, intricate details, detailed texture, light source contrast, dramatic shadows, cinematic light, depth of field, film grain, noise, dark background, hyperrealistic dslr film still, dim volumetric cinematic lighting\"\nneg_prompt = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(20)\nout_lpw = pipe_lpw(\n    prompt,\n    negative_prompt=neg_prompt,\n    width=512,\n    height=512,\n    max_embeddings_multiples=3,\n    num_inference_steps=50,\n    generator=generator,\n    ).images[0]\nout_lpw\n```\n\n----------------------------------------\n\nTITLE: SDXL Inference with PyTorch XLA for TPU Acceleration\nDESCRIPTION: Python code for running SDXL inference using PyTorch XLA, which optimizes for TPU devices. The code demonstrates initial compilation followed by faster subsequent inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\nimport torch_xla.core.xla_model as xm\n\ndevice = xm.xla_device()\npipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\").to(device)\n\nprompt = \"A naruto with green eyes and red legs.\"\nstart = time()\nimage = pipeline(prompt, num_inference_steps=inference_steps).images[0]\nprint(f'Compilation time is {time()-start} sec')\nimage.save(\"naruto.png\")\n\nstart = time()\nimage = pipeline(prompt, num_inference_steps=inference_steps).images[0]\nprint(f'Inference time is {time()-start} sec after compilation')\n```\n\n----------------------------------------\n\nTITLE: ControlNet Inpainting Setup in Python\nDESCRIPTION: This code sets up a ControlNet model for inpainting and prepares the necessary images and conditions. It loads the ControlNet, creates a StableDiffusionControlNetInpaintPipeline, and defines a function to prepare the control image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom diffusers import ControlNetModel, StableDiffusionControlNetInpaintPipeline\nfrom diffusers.utils import load_image, make_image_grid\n\n# load ControlNet\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16, variant=\"fp16\")\n\n# pass ControlNet to the pipeline\npipeline = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", controlnet=controlnet, torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\n# prepare control image\ndef make_inpaint_condition(init_image, mask_image):\n    init_image = np.array(init_image.convert(\"RGB\")).astype(np.float32) / 255.0\n    mask_image = np.array(mask_image.convert(\"L\")).astype(np.float32) / 255.0\n\n    assert init_image.shape[0:1] == mask_image.shape[0:1], \"image and image_mask must have the same image size\"\n    init_image[mask_image > 0.5] = -1.0  # set as masked pixel\n    init_image = np.expand_dims(init_image, 0).transpose(0, 3, 1, 2)\n    init_image = torch.from_numpy(init_image)\n    return init_image\n\ncontrol_image = make_inpaint_condition(init_image, mask_image)\n```\n\n----------------------------------------\n\nTITLE: Training Custom Diffusion with Multiple Concepts in Bash\nDESCRIPTION: This script launches training for a Custom Diffusion model with multiple concepts using a JSON configuration file. It includes similar parameters to the single concept training but uses a concepts list instead of individual concept parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_custom_diffusion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --output_dir=$OUTPUT_DIR \\\n  --concepts_list=./concept_list.json \\\n  --with_prior_preservation \\\n  --real_prior \\\n  --prior_loss_weight=1.0 \\\n  --resolution=512  \\\n  --train_batch_size=2  \\\n  --learning_rate=1e-5  \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --num_class_images=200 \\\n  --scale_lr \\\n  --hflip  \\\n  --modifier_token \"<new1>+<new2>\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Generating Interpolated Image with Kandinsky 2.2\nDESCRIPTION: Uses the Kandinsky 2.2 prior pipeline to interpolate between the specified images/texts and generates a new image based on the interpolation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\"\nprior_out = prior_pipeline.interpolate(images_texts, weights)\n\npipeline = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\nimage = pipeline(prompt, **prior_out, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implementing a Prompt Generation Function\nDESCRIPTION: Creating a utility function that uses Flan-T5 to generate multiple text prompt variations based on input instructions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@torch.no_grad()\ndef generate_prompts(input_prompt):\n    input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\n    outputs = model.generate(\n        input_ids, temperature=0.8, num_return_sequences=16, do_sample=True, max_new_tokens=128, top_k=10\n    )\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\nsource_prompts = generate_prompts(source_text)\ntarget_prompts = generate_prompts(target_text)\nprint(source_prompts)\nprint(target_prompts)\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with Pre-exported ONNX Stable Diffusion Model\nDESCRIPTION: Demonstrates how to load a pre-exported ONNX Stable Diffusion model and run inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/onnx.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.onnxruntime import ORTStableDiffusionPipeline\n\nmodel_id = \"sd_v15_onnx\"\npipeline = ORTStableDiffusionPipeline.from_pretrained(model_id)\nprompt = \"sailing ship in storm by Leonardo da Vinci\"\nimage = pipeline(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Optimizing SDXL Turbo Performance\nDESCRIPTION: Provides tips for speeding up SDXL Turbo inference. It includes compiling the UNet for PyTorch 2.0+ and keeping the VAE in float32 to avoid dtype conversions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\n# Keep VAE in float32\npipe.upcast_vae()\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderKLAllegro Model in Python\nDESCRIPTION: This code snippet demonstrates how to load the Allegro VAE model from the pre-trained weights. The model is loaded using the from_pretrained method with the appropriate repository and subfolder specification and then moved to the CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoderkl_allegro.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKLAllegro\n\nvae = AutoencoderKLCogVideoX.from_pretrained(\"rhymes-ai/Allegro\", subfolder=\"vae\", torch_dtype=torch.float32).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Kandinsky 2.2 Prior Pipeline and Loading Images\nDESCRIPTION: Sets up the Kandinsky 2.2 prior pipeline and loads two images for interpolation. It also displays the loaded images in a grid.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\nfrom diffusers.utils import load_image, make_image_grid\nimport torch\n\nprior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\nimg_1 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\nimg_2 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg\")\nmake_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Importing SD3Transformer2DLoadersMixin in Python\nDESCRIPTION: This code snippet shows how to import the SD3Transformer2DLoadersMixin class from the loaders.transformer_sd3 module. This mixin is used for loading weights into SD3Transformer2DModel, particularly IP-Adapter weights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/loaders/transformer_sd3.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.loaders.transformer_sd3 import SD3Transformer2DLoadersMixin\n```\n\n----------------------------------------\n\nTITLE: Initializing Kandinsky 2.1 Prior Pipeline and Loading Images\nDESCRIPTION: Sets up the Kandinsky 2.1 prior pipeline and loads two images for interpolation. It also displays the loaded images in a grid.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyPriorPipeline, KandinskyPipeline\nfrom diffusers.utils import load_image, make_image_grid\nimport torch\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\nimg_1 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\nimg_2 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg\")\nmake_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LORA Training\nDESCRIPTION: Commands to set environment variables for LoRA fine-tuning, specifying the base model, VAE, and dataset to use.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n```\n\n----------------------------------------\n\nTITLE: Generating Image Embeddings with Kandinsky Prior\nDESCRIPTION: Uses the Kandinsky Prior pipeline to generate image embeddings from a text prompt and the initial image. The function also generates negative embeddings based on a negative prompt to avoid undesirable features in the final image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A robot, 4k photo\"\nnegative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(43)\n\nimg_emb = prior_pipeline(prompt=prompt, image=img, strength=0.85, generator=generator)\nnegative_emb = prior_pipeline(prompt=negative_prior_prompt, image=img, strength=1, generator=generator)\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22InpaintCombinedPipeline in Python\nDESCRIPTION: This snippet defines the KandinskyV22InpaintCombinedPipeline class, which combines the prior and main pipelines for a more streamlined inpainting process in Kandinsky 2.2.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22InpaintCombinedPipeline\n\t- all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Example DeepSpeed Configuration - Bash\nDESCRIPTION: An example configuration for DeepSpeed that offloads the optimizer and parameters to the CPU, intended for use with GPUs that have 8GB of memory or less. Setting `zero_stage: 2` enables DeepSpeed Stage 2.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n\"compute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  gradient_accumulation_steps: 4\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: false\n  zero_stage: 2\ndistributed_type: DEEPSPEED\"\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Kandinsky 3\nDESCRIPTION: Direct text-to-image generation with Kandinsky 3, which simplifies the architecture by removing the need for a prior model, allowing for a more straightforward generation pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import Kandinsky3Pipeline\nimport torch\n\npipeline = Kandinsky3Pipeline.from_pretrained(\"kandinsky-community/kandinsky-3\", variant=\"fp16\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nimage = pipeline(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Generating Image Embeddings with Kandinsky Prior Pipeline\nDESCRIPTION: Uses the Kandinsky prior pipeline to generate image embeddings and negative image embeddings based on a prompt and negative prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt).to_tuple()\n```\n\n----------------------------------------\n\nTITLE: Loading ConsisIDTransformer3DModel from Pretrained Model for Text-to-Video Generation\nDESCRIPTION: This code snippet demonstrates how to load a pretrained ConsisIDTransformer3DModel from the BestWishYsh/ConsisID-preview repository. The model is loaded with bfloat16 precision and moved to a CUDA device for efficient processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/consisid_transformer3d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import ConsisIDTransformer3DModel\n\ntransformer = ConsisIDTransformer3DModel.from_pretrained(\"BestWishYsh/ConsisID-preview\", subfolder=\"transformer\", torch_dtype=torch.bfloat16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading Non-EMA Variant of a Diffusion Pipeline\nDESCRIPTION: Load a non-EMA (Exponential Moving Average) variant of a diffusion model pipeline, which is typically used for fine-tuning rather than inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# load non_ema variant\nstable_diffusion = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", variant=\"non_ema\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with SDXL Pipeline\nDESCRIPTION: Function that combines all the steps for text-to-image generation, including tokenization, replication, running the pipeline, and image conversion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef generate(...):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Initializing TCD-LoRA with ControlNet for Stable Diffusion XL in Python\nDESCRIPTION: This code sets up a Stable Diffusion XL pipeline with TCD-LoRA and ControlNet. It loads the necessary models, configures the TCD scheduler, and prepares the input image for edge detection.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_tcd_lora.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, TCDScheduler\nfrom diffusers.utils import load_image\n\ndevice = \"cuda\"\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\ncontrolnet_id = \"diffusers/controlnet-canny-sdxl-1.0\"\n\ncontrolnet = ControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16).to(device)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    base_model_id,\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    variant=\"fp16\"\n).to(device)\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\")\nimage = image.resize((1024, 1024))\n\nfrom PIL import Image\nimport numpy as np\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n)\nimage = image.resize((1024, 1024))\n\n# get canny image\nimage = np.array(image)\n\nfrom controlnet_aux import CannyDetector\n\ncanny = CannyDetector()\ncontrol_image = canny(image, low_threshold=100, high_threshold=200)\n\ncontrol_image = Image.fromarray(control_image)\n```\n\n----------------------------------------\n\nTITLE: Creating Canny Image with ControlNet-Aux for Stable Diffusion XL\nDESCRIPTION: Code to generate a canny edge detection image using the ControlNet-Aux library for use with Stable Diffusion XL. The CannyDetector class provides a more streamlined way to create control images with specific resolution parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom controlnet_aux.canny import CannyDetector\nfrom diffusers.utils import load_image\n\ncanny_detector = CannyDetector()\n\nimage = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\nimage = canny_detector(image, detect_resolution=384, image_resolution=1024)\n```\n\n----------------------------------------\n\nTITLE: Loading FluxControlNetModel for a Flux.1 Pipeline with Single ControlNet\nDESCRIPTION: Example showing how to load a FluxControlNetModel from a pre-trained checkpoint and use it with a FluxControlNetPipeline. This demonstrates the basic usage pattern for a single controlnet.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/controlnet_flux.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxControlNetPipeline\nfrom diffusers.models import FluxControlNetModel, FluxMultiControlNetModel\n\ncontrolnet = FluxControlNetModel.from_pretrained(\"InstantX/FLUX.1-dev-Controlnet-Canny\")\npipe = FluxControlNetPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", controlnet=controlnet)\n```\n\n----------------------------------------\n\nTITLE: Setting Up InstantStyle with IP-Adapter for Style & Layout Control\nDESCRIPTION: Configures an SDXL pipeline with IP-Adapter for InstantStyle, a method that disentangles style and layout control. The code sets specific layer scales in a dictionary to control which aspects of the reference image affect the generation output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n\nscale = {\n    \"down\": {\"block_2\": [0.0, 1.0]},\n    \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n}\npipeline.set_ip_adapter_scale(scale)\n```\n\n----------------------------------------\n\nTITLE: Using the Latent Consistency Interpolation Pipeline\nDESCRIPTION: This snippet demonstrates how to use the Latent Consistency Model for interpolating between multiple text prompts. This allows smooth transitions between different concepts in the generated images, similar to other interpolation pipelines but with the speed benefits of LCM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_76\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", custom_pipeline=\"latent_consistency_interpolate\")\n\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\nprompts = [\n    \"Self-portrait oil painting, a beautiful cyborg with golden hair, Margot Robbie, 8k\",\n    \"Self-portrait oil painting, an extremely strong man, body builder, Huge Jackman, 8k\",\n    \"An astronaut floating in space, renaissance art, realistic, high quality, 8k\",\n    \"Oil painting of a cat, cute, dream-like\",\n    \"Hugging face emoji, cute, realistic\"\n]\nnum_inference_steps = 4\nnum_interpolation_steps = 60\nseed = 1337\n\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\nimages = pipe(\n    prompt=prompts,\n    height=512,\n    width=512,\n    num_inference_steps=num_inference_steps,\n    num_interpolation_steps=num_interpolation_steps,\n    guidance_scale=8.0,\n    embedding_interpolation_type=\"lerp\",\n    latent_interpolation_type=\"slerp\",\n    process_batch_size=4,  # Make it higher or lower based on your GPU memory\n    generator=torch.Generator(seed),\n)\n\nassert len(images) == (len(prompts) - 1) * num_interpolation_steps\n```\n\n----------------------------------------\n\nTITLE: Adding Text Prompt to Image-to-Image Variation\nDESCRIPTION: This snippet shows how to add a text prompt when performing image-to-image variation with Stable unCLIP, allowing for more controlled image generation guided by both the input image and text.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_unclip.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A fantasy landscape, trending on artstation\"\n\nimage = pipe(init_image, prompt=prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Using a distilled Stable Diffusion model for faster inference\nDESCRIPTION: Loads and uses a distilled version of Stable Diffusion for faster inference. This example uses the bk-sdm-small model from Nota AI and generates an image from a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/fp16.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\ndistilled = StableDiffusionPipeline.from_pretrained(\n    \"nota-ai/bk-sdm-small\", torch_dtype=torch.float16, use_safetensors=True,\n).to(\"cuda\")\nprompt = \"a golden vase with different flowers\"\ngenerator = torch.manual_seed(2023)\nimage = distilled(\"a golden vase with different flowers\", num_inference_steps=25, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading FluxPipeline with FP8 Quantization\nDESCRIPTION: Example of loading and running Flux with FP8 quantization using optimum-quanto library for systems with less than 16GB VRAM. Demonstrates quantization of transformer and text encoder models with CPU offloading.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxTransformer2DModel, FluxPipeline\nfrom transformers import T5EncoderModel, CLIPTextModel\nfrom optimum.quanto import freeze, qfloat8, quantize\n\nbfl_repo = \"black-forest-labs/FLUX.1-dev\"\ndtype = torch.bfloat16\n\ntransformer = FluxTransformer2DModel.from_single_file(\"https://huggingface.co/Kijai/flux-fp8/blob/main/flux1-dev-fp8.safetensors\", torch_dtype=dtype)\nquantize(transformer, weights=qfloat8)\nfreeze(transformer)\n\ntext_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo, subfolder=\"text_encoder_2\", torch_dtype=dtype)\nquantize(text_encoder_2, weights=qfloat8)\nfreeze(text_encoder_2)\n\npipe = FluxPipeline.from_pretrained(bfl_repo, transformer=None, text_encoder_2=None, torch_dtype=dtype)\npipe.transformer = transformer\npipe.text_encoder_2 = text_encoder_2\n\npipe.enable_model_cpu_offload()\n\nprompt = \"A cat holding a sign that says hello world\"\nimage = pipe(\n    prompt,\n    guidance_scale=3.5,\n    output_type=\"pil\",\n    num_inference_steps=20,\n    generator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\n\nimage.save(\"flux-fp8-dev.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Configuring HunyuanDiTPipeline in Python\nDESCRIPTION: Loads the HunyuanDiTPipeline, configures memory layout, and applies torch.compile for optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/hunyuandit.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import HunyuanDiTPipeline\nimport torch\n\npipeline = HunyuanDiTPipeline.from_pretrained(\n\t\"Tencent-Hunyuan/HunyuanDiT-Diffusers\", torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.transformer.to(memory_format=torch.channels_last)\npipeline.vae.to(memory_format=torch.channels_last)\n\npipeline.transformer = torch.compile(pipeline.transformer, mode=\"max-autotune\", fullgraph=True)\npipeline.vae.decode = torch.compile(pipeline.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\nimage = pipeline(prompt=\"一个宇航员在骑马\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Specialized Inpainting Pipeline\nDESCRIPTION: Implementation using the RunwayML inpainting-specific checkpoint. Shows the setup for more advanced inpainting tasks with better quality output compared to regular checkpoints.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\ngenerator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Creating PAG Pipeline from Existing Pipeline\nDESCRIPTION: Demonstrates how to enable PAG on an existing pipeline using the from_pipe API without requiring additional memory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline_sdxl = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\npipeline = AutoPipelineForText2Image.from_pipe(pipeline_sdxl, enable_pag=True)\n```\n\n----------------------------------------\n\nTITLE: Adjusting Inference Steps for Higher Quality Image Generation\nDESCRIPTION: This code shows how to control the quality of generated images by adjusting the number of inference steps. Increasing the number of steps typically results in higher quality images at the cost of longer generation time.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/unconditional_image_generation.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimage = generator(num_inference_steps=100).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Configuring 8-bit Quantization with Custom Threshold for FluxTransformer2DModel\nDESCRIPTION: Example of loading a Flux.1 model in 8-bit precision with a custom outlier threshold to handle distribution ranges beyond the standard values. This approach helps balance performance and accuracy for large models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxTransformer2DModel, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True, llm_int8_threshold=10,\n)\n\nmodel_8bit = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"transformer\",\n    quantization_config=quantization_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Stable UnCLIP for Image Generation from Text in Python\nDESCRIPTION: This code demonstrates how to use Stable UnCLIP, which combines a prior model for generating CLIP image embeddings from text and a decoder model for generating images from these embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\ndevice = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"kakaobrain/karlo-v1-alpha\",\n    torch_dtype=torch.float16,\n    custom_pipeline=\"stable_unclip\",\n    decoder_pipe_kwargs=dict(\n        image_encoder=None,\n    ),\n)\npipeline.to(device)\n\nprompt = \"a shiba inu wearing a beret and black turtleneck\"\nrandom_generator = torch.Generator(device=device).manual_seed(1000)\noutput = pipeline(\n    prompt=prompt,\n    width=512,\n    height=512,\n    generator=random_generator,\n    prior_guidance_scale=4,\n    prior_num_inference_steps=25,\n    decoder_guidance_scale=8,\n    decoder_num_inference_steps=50,\n)\n\nimage = output.images[0]\nimage.save(\"./shiba-inu.jpg\")\n\n# debug\n\n# `pipeline.decoder_pipe` is a regular StableDiffusionImageVariationPipeline instance.\n# It is used to convert clip image embedding to latents, then fed into VAE decoder.\nprint(pipeline.decoder_pipe.__class__)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image-to-Image Pipeline with Kandinsky 2.2\nDESCRIPTION: Code for initializing the Kandinsky 2.2 pipelines for image-to-image generation, allowing users to transform an existing image based on a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import KandinskyV22Img2ImgPipeline, KandinskyPriorPipeline\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = KandinskyV22Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Karras Sigmas in SDXL Pipeline\nDESCRIPTION: Demonstrates how to enable Karras sigmas in a Stable Diffusion XL pipeline for enhanced detail generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/scheduler_features.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"SG161222/RealVisXL_V4.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, algorithm_type=\"sde-dpmsolver++\", use_karras_sigmas=True)\n\nprompt = \"A cinematic shot of a cute little rabbit wearing a jacket and doing a thumbs up\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(2487854446)\nimage = pipeline(\n    prompt=prompt,\n    negative_prompt=\"\",\n    generator=generator,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Full Fine-tuning AmuSED with 8-bit Adam\nDESCRIPTION: This script fine-tunes the AmuSED model using full training with 8-bit Adam optimizer. It uses a learning rate of 5e-6 and expects decent results in 500-1000 steps. The script includes options for batch size, gradient accumulation, and various training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --train_batch_size <batch size> \\\n    --gradient_accumulation_steps <gradient accumulation steps> \\\n    --learning_rate 5e-6 \\\n    --pretrained_model_name_or_path amused/amused-512 \\\n    --instance_data_dataset  'monadical-labs/minecraft-preview' \\\n    --prompt_prefix 'minecraft ' \\\n    --image_key image \\\n    --prompt_key text \\\n    --resolution 512 \\\n    --mixed_precision fp16 \\\n    --lr_scheduler constant \\\n    --validation_prompts \\\n        'minecraft Avatar' \\\n        'minecraft character' \\\n        'minecraft' \\\n        'minecraft president' \\\n        'minecraft pig' \\\n    --max_train_steps 10000 \\\n    --checkpointing_steps 500 \\\n    --validation_steps 250 \\\n    --gradient_checkpointing\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized FluxPipeline with BitsAndBytes\nDESCRIPTION: Example of loading a FluxPipeline with 8-bit quantization using bitsandbytes config. Demonstrates quantization of both text encoder and transformer models for memory-efficient inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, FluxTransformer2DModel, FluxPipeline\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"text_encoder_2\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    text_encoder_2=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\nimage = pipeline(prompt, guidance_scale=3.5, height=768, width=1360, num_inference_steps=50).images[0]\nimage.save(\"flux.png\")\n```\n\n----------------------------------------\n\nTITLE: End-to-End Inpainting with Kandinsky 2.1 Combined Pipeline\nDESCRIPTION: Uses the AutoPipelineForInpainting to perform end-to-end inpainting with Kandinsky 2.1, including model loading, image and mask preparation, and inpainting in a single pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = AutoPipelineForInpainting.from_pretrained(\"kandinsky-community/kandinsky-2-1-inpaint\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\ninit_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\nmask = np.zeros((768, 768), dtype=np.float32)\n# mask area above cat's head\nmask[:250, 250:-250] = 1\nprompt = \"a hat\"\n\noutput_image = pipe(prompt=prompt, image=init_image, mask_image=mask).images[0]\nmask = Image.fromarray((mask*255).astype('uint8'), 'L')\nmake_image_grid([init_image, mask, output_image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Configuring LoRA for UNet with PEFT\nDESCRIPTION: Set up LoRA configuration for the UNet using PEFT library, specifying rank, target modules, and initialization strategy\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lora.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nunet_lora_config = LoraConfig(\n    r=args.rank,\n    lora_alpha=args.rank,\n    init_lora_weights=\"gaussian\",\n    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n)\n\nunet.add_adapter(unet_lora_config)\nlora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n```\n\n----------------------------------------\n\nTITLE: Inspecting StableDiffusionImageVariationPipeline and UnCLIP Components\nDESCRIPTION: This snippet demonstrates how to print pipeline information to inspect the StableDiffusionImageVariationPipeline and its components, showing the structure of the pipeline and the prior scheduler configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nprint(pipeline)\n# StableUnCLIPPipeline {\n#   \"_class_name\": \"StableUnCLIPPipeline\",\n#   \"_diffusers_version\": \"0.12.0.dev0\",\n#   \"prior\": [\n#     \"diffusers\",\n#     \"PriorTransformer\"\n#   ],\n#   \"prior_scheduler\": [\n#     \"diffusers\",\n#     \"UnCLIPScheduler\"\n#   ],\n#   \"text_encoder\": [\n#     \"transformers\",\n#     \"CLIPTextModelWithProjection\"\n#   ],\n#   \"tokenizer\": [\n#     \"transformers\",\n#     \"CLIPTokenizer\"\n#   ]\n# }\n\n# pipeline.prior_scheduler is the scheduler used for prior in UnCLIP.\nprint(pipeline.prior_scheduler)\n# UnCLIPScheduler {\n#   \"_class_name\": \"UnCLIPScheduler\",\n#   \"_diffusers_version\": \"0.12.0.dev0\",\n#   \"clip_sample\": true,\n#   \"clip_sample_range\": 5.0,\n#   \"num_train_timesteps\": 1000,\n#   \"prediction_type\": \"sample\",\n#   \"variance_type\": \"fixed_small_log\"\n# }\n```\n\n----------------------------------------\n\nTITLE: Basic Inference with Trained LoRA Model\nDESCRIPTION: Python code to load a trained LoRA model and perform basic inference with the SDXL pipeline to generate images of the personalized subject.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub.repocard import RepoCard\nfrom diffusers import DiffusionPipeline\nimport torch\n\nlora_model_id = <\"lora-sdxl-dreambooth-id\">\ncard = RepoCard.load(lora_model_id)\nbase_model_id = card.data.to_dict()[\"base_model\"]\n\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\npipe.load_lora_weights(lora_model_id)\nimage = pipe(\"A picture of a sks dog in a bucket\", num_inference_steps=25).images[0]\nimage.save(\"sks_dog.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Mochi Transformer from Single File for Video Generation in Python\nDESCRIPTION: This snippet demonstrates how to load a Mochi Transformer model from a single file checkpoint, create a pipeline, and generate video frames based on a text prompt. It configures model offloading, VAE tiling, and sets various generation parameters like resolution, frames, and guidance scale.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/mochi.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MochiPipeline, MochiTransformer3DModel\nfrom diffusers.utils import export_to_video\n\nmodel_id = \"genmo/mochi-1-preview\"\n\nckpt_path = \"https://huggingface.co/Comfy-Org/mochi_preview_repackaged/blob/main/split_files/diffusion_models/mochi_preview_bf16.safetensors\"\n\ntransformer = MochiTransformer3DModel.from_pretrained(ckpt_path, torch_dtype=torch.bfloat16)\n\npipe = MochiPipeline.from_pretrained(model_id,  transformer=transformer)\npipe.enable_model_cpu_offload()\npipe.enable_vae_tiling()\n\nwith torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, cache_enabled=False):\n    frames = pipe(\n        prompt=\"Close-up of a chameleon's eye, with its scaly skin changing color. Ultra high resolution 4k.\",\n        negative_prompt=\"\",\n        height=480,\n        width=848,\n        num_frames=85,\n        num_inference_steps=50,\n        guidance_scale=4.5,\n        num_videos_per_prompt=1,\n        generator=torch.Generator(device=\"cuda\").manual_seed(0),\n        max_sequence_length=256,\n        output_type=\"pil\",\n    ).frames[0]\n\nexport_to_video(frames, \"output.mp4\", fps=30)\n```\n\n----------------------------------------\n\nTITLE: IF Stage I LoRA DreamBooth Training\nDESCRIPTION: This bash script configures DreamBooth LoRA training for DeepFloyd IF Stage I model. It uses pre-computed text embeddings from T5, sets appropriate tokenizer length, and requires approximately 28GB VRAM. The script is optimized for the lower resolution (64px) required by the IF model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"DeepFloyd/IF-I-XL-v1.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"dreambooth_dog_lora\"\n\naccelerate launch train_dreambooth_lora.py \\\n  --report_to wandb \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a sks dog\" \\\n  --resolution=64 \\\n  --train_batch_size=4 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --scale_lr \\\n  --max_train_steps=1200 \\\n  --validation_prompt=\"a sks dog\" \\\n  --validation_epochs=25 \\\n  --checkpointing_steps=100 \\\n  --pre_compute_text_embeddings \\\n  --tokenizer_max_length=77 \\\n  --text_encoder_use_attention_mask\n```\n\n----------------------------------------\n\nTITLE: FLUX Memory Optimization with Group Offloading\nDESCRIPTION: Demonstrates memory optimization techniques using group offloading to reduce VRAM usage. Shows how to apply offloading to different pipeline components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPipeline\nfrom diffusers.hooks import apply_group_offloading\n\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\ndtype = torch.bfloat16\npipe = FluxPipeline.from_pretrained(\n\tmodel_id,\n\ttorch_dtype=dtype,\n)\n\napply_group_offloading(\n    pipe.transformer,\n    offload_type=\"leaf_level\",\n    offload_device=torch.device(\"cpu\"),\n    onload_device=torch.device(\"cuda\"),\n    use_stream=True,\n)\napply_group_offloading(\n    pipe.text_encoder, \n    offload_device=torch.device(\"cpu\"),\n    onload_device=torch.device(\"cuda\"),\n    offload_type=\"leaf_level\",\n    use_stream=True,\n)\napply_group_offloading(\n    pipe.text_encoder_2, \n    offload_device=torch.device(\"cpu\"),\n    onload_device=torch.device(\"cuda\"),\n    offload_type=\"leaf_level\",\n    use_stream=True,\n)\napply_group_offloading(\n    pipe.vae, \n    offload_device=torch.device(\"cpu\"),\n    onload_device=torch.device(\"cuda\"),\n    offload_type=\"leaf_level\",\n    use_stream=True,\n)\n\nprompt=\"A cat wearing sunglasses and working as a lifeguard at pool.\"\n\ngenerator = torch.Generator().manual_seed(181201)\nimage = pipe(\n    prompt,\n    width=576,\n    height=1024,\n    num_inference_steps=30,\n    generator=generator\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Image-to-3D Conversion with ShapEImg2ImgPipeline\nDESCRIPTION: Demonstrates using ShapEImg2ImgPipeline to convert a 2D image into a 3D representation, including loading the image and generating the 3D output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom diffusers import ShapEImg2ImgPipeline\nfrom diffusers.utils import export_to_gif\n\npipe = ShapEImg2ImgPipeline.from_pretrained(\"openai/shap-e-img2img\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nguidance_scale = 3.0\nimage = Image.open(\"burger.png\").resize((256, 256))\n\nimages = pipe(\n    image,\n    guidance_scale=guidance_scale,\n    num_inference_steps=64,\n    frame_size=256,\n).images\n\ngif_path = export_to_gif(images[0], \"burger_3d.gif\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DDPMScheduler for Denoising\nDESCRIPTION: Load a pretrained DDPMScheduler to manage the denoising process and guide image generation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDPMScheduler\n\nscheduler = DDPMScheduler.from_pretrained(repo_id)\n```\n\n----------------------------------------\n\nTITLE: Setting Interpolation Inputs and Weights\nDESCRIPTION: Prepares the inputs and weights for interpolation between text and images. This code defines the blend of a text prompt and two images, assigning specific weight values to each element to control their influence on the final interpolated result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimages_texts = [\"a cat\", img_1, img_2]\nweights = [0.3, 0.3, 0.4]\n```\n\n----------------------------------------\n\nTITLE: Loading HunyuanDiT2DControlNetModel from Pre-trained Checkpoint\nDESCRIPTION: This snippet demonstrates how to load a pre-trained HunyuanDiT2DControlNetModel for pose control from the Tencent-Hunyuan repository. The model is loaded with float16 precision for optimized performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/controlnet_hunyuandit.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import HunyuanDiT2DControlNetModel\nimport torch\ncontrolnet = HunyuanDiT2DControlNetModel.from_pretrained(\"Tencent-Hunyuan/HunyuanDiT-v1.1-ControlNet-Diffusers-Pose\", torch_dtype=torch.float16)\n```\n\n----------------------------------------\n\nTITLE: Loading UNet2DConditionModel from a Subfolder in Diffusers\nDESCRIPTION: Demonstrates how to load a UNet2DConditionModel from a subfolder of a pretrained model repository. The example loads the unet component from the stable-diffusion-v1-5 model, using the subfolder argument to specify which part of the model to load.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/schedulers.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel\n\nunet = UNet2DConditionModel.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"unet\", use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Computing Image Embeddings\nDESCRIPTION: This code executes a script to compute embeddings using a pre-defined prompt 'a photo of sks dog'. Other configuration options are available in the script itself.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n!python compute_embeddings.py\n```\n\n----------------------------------------\n\nTITLE: Text-to-Video Generation with Wan 2.1\nDESCRIPTION: Demonstrates how to use the WanPipeline for text-to-video generation. It loads a pretrained model, sets up the pipeline, and generates a video based on a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import WanPipeline\nfrom diffusers.utils import export_to_video\n\nmodel_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n\npipe = WanPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload()\n\nprompt = \"A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.\"\nnegative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\nnum_frames = 33\n\nframes = pipe(prompt=prompt, negative_prompt=negative_prompt, num_frames=num_frames).frames[0]\nexport_to_video(frames, \"wan-t2v.mp4\", fps=16)\n```\n\n----------------------------------------\n\nTITLE: Image-to-Video Generation with Wan 2.1\nDESCRIPTION: Demonstrates how to use the WanImageToVideoPipeline for image-to-video generation. It loads the necessary models, sets up the pipeline, and generates a video based on an input image and text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom diffusers import AutoencoderKLWan, WanImageToVideoPipeline\nfrom diffusers.utils import export_to_video, load_image\nfrom transformers import CLIPVisionModel\n\nmodel_id = \"Wan-AI/Wan2.1-I2V-14B-480P-Diffusers\"\nimage_encoder = CLIPVisionModel.from_pretrained(\n    model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32\n)\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\npipe = WanImageToVideoPipeline.from_pretrained(\n    model_id, vae=vae, image_encoder=image_encoder, torch_dtype=torch.bfloat16\n)\n\npipe.enable_model_cpu_offload()\n\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\"\n)\n\nmax_area = 480 * 832\naspect_ratio = image.height / image.width\nmod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\nheight = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\nwidth = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\nimage = image.resize((width, height))\n\nprompt = (\n    \"An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in \"\n    \"the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot.\"\n)\nnegative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n\nnum_frames = 33\n\noutput = pipe(\n    image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=height,\n    width=width,\n    num_frames=num_frames,\n    guidance_scale=5.0,\n).frames[0]\nexport_to_video(output, \"wan-i2v.mp4\", fps=16)\n```\n\n----------------------------------------\n\nTITLE: Video-to-Video Generation with AnimateDiff\nDESCRIPTION: Shows implementation of video-to-video generation using AnimateDiff. Enables creative video editing by generating visually similar videos or applying style/character/background modifications to existing videos.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport imageio\nimport requests\nimport torch\nfrom diffusers import AnimateDiffVideoToVideoPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\nfrom io import BytesIO\nfrom PIL import Image\n\n# Load the motion adapter\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n# load SD 1.5 based finetuned model\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffVideoToVideoPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipe.scheduler = scheduler\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\n# helper function to load videos\ndef load_video(file_path: str):\n    images = []\n\n    if file_path.startswith(('http://', 'https://')):\n        # If the file_path is a URL\n        response = requests.get(file_path)\n        response.raise_for_status()\n        content = BytesIO(response.content)\n        vid = imageio.get_reader(content)\n    else:\n        # Assuming it's a local file path\n        vid = imageio.get_reader(file_path)\n\n    for frame in vid:\n        pil_image = Image.fromarray(frame)\n        images.append(pil_image)\n\n    return images\n\nvideo = load_video(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif\")\n\noutput = pipe(\n    video = video,\n    prompt=\"panda playing a guitar, on a boat, in the ocean, high quality\",\n    negative_prompt=\"bad quality, worse quality\",\n    guidance_scale=7.5,\n    num_inference_steps=25,\n    strength=0.5,\n    generator=torch.Generator(\"cpu\").manual_seed(42),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Padding Mask Crop for Improved Inpainting Quality in Python\nDESCRIPTION: This snippet demonstrates how to use the padding_mask_crop parameter to enhance inpainting quality. It loads an image and mask, then applies inpainting with a padding value of 32, which crops the masked area with padding and upscales it for better results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nfrom PIL import Image\n\ngenerator = torch.Generator(device='cuda').manual_seed(0)\npipeline = AutoPipelineForInpainting.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to('cuda')\n\nbase = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/seashore.png\")\nmask = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/seashore_mask.png\")\n\nimage = pipeline(\"boat\", image=base, mask_image=mask, strength=0.75, generator=generator, padding_mask_crop=32).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Configuring Inpainting Pipeline with VaeImageProcessor in Python\nDESCRIPTION: This code sets up an inpainting pipeline using a specific model, loads images, and applies inpainting. It then demonstrates how to use VaeImageProcessor to preserve unmasked areas of the original image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\nimport numpy as np\nimport torch\n\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\ndevice = \"cuda\"\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    torch_dtype=torch.float16,\n)\npipeline = pipeline.to(device)\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = load_image(img_url).resize((512, 512))\nmask_image = load_image(mask_url).resize((512, 512))\n\nprompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\nrepainted_image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\nrepainted_image.save(\"repainted_image.png\")\n\nunmasked_unchanged_image = pipeline.image_processor.apply_overlay(mask_image, init_image, repainted_image)\nunmasked_unchanged_image.save(\"force_unmasked_unchanged.png\")\nmake_image_grid([init_image, mask_image, repainted_image, unmasked_unchanged_image], rows=2, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Generating Inpainted Image with Prompt\nDESCRIPTION: Creates a prompt for inpainting and passes it to the pipeline along with the base and mask images to generate the inpainted image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a black cat with glowing eyes, cute, adorable, disney, pixar, highly detailed, 8k\"\nnegative_prompt = \"bad anatomy, deformed, ugly, disfigured\"\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask_image).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Integrating Marigold Depth Maps with ControlNet for Image Generation\nDESCRIPTION: This code example shows how to generate a depth map using Marigold and then use it as input for ControlNet to guide image generation. It demonstrates the workflow from loading an image to computing a depth map and using it to control StableDiffusionXL generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport diffusers\n\ndevice = \"cuda\"\ngenerator = torch.Generator(device=device).manual_seed(2024)\nimage = diffusers.utils.load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_depth_source.png\"\n)\n\npipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n    \"prs-eth/marigold-depth-v1-1\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(device)\n\ndepth_image = pipe(image, generator=generator).prediction\ndepth_image = pipe.image_processor.visualize_depth(depth_image, color_map=\"binary\")\ndepth_image[0].save(\"motorcycle_controlnet_depth.png\")\n\ncontrolnet = diffusers.ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(device)\npipe = diffusers.StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"SG161222/RealVisXL_V4.0\", torch_dtype=torch.float16, variant=\"fp16\", controlnet=controlnet\n).to(device)\npipe.scheduler = diffusers.DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)\n\ncontrolnet_out = pipe(\n    prompt=\"high quality photo of a sports bike, city\",\n    negative_prompt=\"\",\n    guidance_scale=6.5,\n    num_inference_steps=25,\n    image=depth_image,\n    controlnet_conditioning_scale=0.7,\n    control_guidance_end=0.7,\n    generator=generator,\n).images\ncontrolnet_out[0].save(\"motorcycle_controlnet_out.png\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Image-to-Image Pipeline with PAG\nDESCRIPTION: Initializes an image-to-image pipeline with PAG support and demonstrates image generation with strength parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npag_scales =  4.0\nguidance_scales = 7.0\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\ninit_image = load_image(url)\nprompt = \"a dog catching a frisbee in the jungle\"\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimage = pipeline(\n    prompt,\n    image=init_image,\n    strength=0.8,\n    guidance_scale=guidance_scale,\n    pag_scale=pag_scale,\n    generator=generator).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading the Latent Consistency Image-to-Image Pipeline\nDESCRIPTION: This snippet shows how to load the Latent Consistency Model for image-to-image generation. This variation of LCM allows users to provide an input image as a starting point for the generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_74\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", custom_pipeline=\"latent_consistency_img2img\")\n\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n```\n\n----------------------------------------\n\nTITLE: Loading DreamBooth Checkpoint in Python\nDESCRIPTION: Demonstrates loading a DreamBooth-trained model checkpoint to generate images in Hergé's style using a special trigger word.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"sd-dreambooth-library/herge-style\", torch_dtype=torch.float16).to(\"cuda\")\nprompt = \"A cute herge_style brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\nimage = pipeline(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading SD3ControlNet Model and Pipeline in Python\nDESCRIPTION: Example showing how to load a pre-trained SD3ControlNetModel and initialize a StableDiffusion3ControlNetPipeline. The code demonstrates loading a Canny edge detection model and combining it with the base Stable Diffusion 3 pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/controlnet_sd3.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusion3ControlNetPipeline\nfrom diffusers.models import SD3ControlNetModel, SD3MultiControlNetModel\n\ncontrolnet = SD3ControlNetModel.from_pretrained(\"InstantX/SD3-Controlnet-Canny\")\npipe = StableDiffusion3ControlNetPipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", controlnet=controlnet)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with ControlNet Text-to-Image Pipeline\nDESCRIPTION: Code to execute the text-to-image generation pipeline with a ControlNet model, providing a text prompt and a canny edge image as conditioning input.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\noutput = pipe(\n    \"the mona lisa\", image=canny_image\n).images[0]\nmake_image_grid([original_image, canny_image, output], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Training DreamBooth with Custom bf16 Precision\nDESCRIPTION: Launch the DreamBooth training script using bf16 precision to save memory and expedite training. Adjusting precision is useful for limited GPU resources.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth.py \\\n    --mixed_precision=\"bf16\"\n```\n\n----------------------------------------\n\nTITLE: Remote VAE Decoding for HunyuanVideo Model in Python\nDESCRIPTION: Demonstrates remote VAE decoding for the HunyuanVideo model, which generates video output. The example shows how to create and save an MP4 video file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_decode.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nvideo = remote_decode(\n    endpoint=\"https://o7ywnmrahorts457.us-east-1.aws.endpoints.huggingface.cloud/\",\n    tensor=torch.randn([1, 16, 3, 40, 64], dtype=torch.float16),\n    output_type=\"mp4\",\n)\nwith open(\"video.mp4\", \"wb\") as f:\n    f.write(video)\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with a Text Prompt\nDESCRIPTION: This code demonstrates how to generate an image using the loaded diffusion pipeline by providing a text prompt. The output is a PIL Image object that can be displayed or saved.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> image = pipeline(\"An image of a squirrel in Picasso style\").images[0]\n>>> image\n```\n\n----------------------------------------\n\nTITLE: Moving Diffusion Pipeline to GPU\nDESCRIPTION: Transfers the diffusion pipeline to GPU for faster inference, which is recommended since the model has around 1.4 billion parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Video Generation with Zeroscope V2 Model\nDESCRIPTION: Example showing how to use the watermark-free Zeroscope V2 model to generate videos at a specific resolution (576x320), with memory optimization techniques applied.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\nfrom PIL import Image\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\n# memory optimization\npipe.unet.enable_forward_chunking(chunk_size=1, dim=1)\npipe.enable_vae_slicing()\n\nprompt = \"Darth Vader surfing a wave\"\nvideo_frames = pipe(prompt, num_frames=24).frames[0]\nvideo_path = export_to_video(video_frames)\nvideo_path\n```\n\n----------------------------------------\n\nTITLE: Generating Random Noise for Inference\nDESCRIPTION: Create a tensor with random Gaussian noise for image generation, setting the appropriate shape based on model configuration\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ntorch.manual_seed(0)\n\nnoisy_sample = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)\n```\n\n----------------------------------------\n\nTITLE: Using Different Prompts for CLIP and T5 Text Encoders in Stable Diffusion 3\nDESCRIPTION: This code shows how to send different prompts to the CLIP Text Encoders and the T5 Text Encoder in the Stable Diffusion 3 pipeline to prevent prompt truncation and improve generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. A river of warm, melted butter, pancake-like foliage in the background, a towering pepper mill standing in for a tree.\"\n\nprompt_3 = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature's body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight\"\n\nimage = pipe(\n    prompt=prompt,\n    prompt_3=prompt_3,\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    guidance_scale=4.5,\n    max_sequence_length=512,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22PriorPipeline in Python\nDESCRIPTION: This snippet shows the class definition for KandinskyV22PriorPipeline, which is used for generating prior embeddings in the Kandinsky 2.2 model. It includes methods for calling the pipeline and interpolating between embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22PriorPipeline\n\t- all\n\t- __call__\n\t- interpolate\n```\n\n----------------------------------------\n\nTITLE: Moving the Pipeline to GPU\nDESCRIPTION: This snippet shows how to move the diffusion pipeline to a CUDA-enabled GPU for faster inference, which is recommended since the model has approximately 1.4 billion parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Creating Depth Map for ControlNet\nDESCRIPTION: Processes an image to create a depth map using Transformers' depth estimation pipeline. The function make_hint converts the depth estimation into a format suitable for the ControlNet model to use as conditioning information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\nfrom transformers import pipeline\n\ndef make_hint(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    hint = detected_map.permute(2, 0, 1)\n    return hint\n\ndepth_estimator = pipeline(\"depth-estimation\")\nhint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Running DreamBooth ControlNet Pipeline\nDESCRIPTION: Implements text-to-video generation using a custom DreamBooth model with ControlNet for edge-guided video synthesis. Includes attention processing and fixed latents for consistent generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video_zero.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n# set model id to custom model\nmodel_id = \"PAIR/text2video-zero-controlnet-canny-avatar\"\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    model_id, controlnet=controlnet, torch_dtype=torch.float16\n).to(\"cuda\")\n\n# Set the attention processor\npipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\npipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n\n# fix latents for all frames\nlatents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(canny_edges), 1, 1, 1)\n\nprompt = \"oil painting of a beautiful girl avatar style\"\nresult = pipe(prompt=[prompt] * len(canny_edges), image=canny_edges, latents=latents).images\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```\n\n----------------------------------------\n\nTITLE: Running Guided Stable Diffusion Pipeline in Python\nDESCRIPTION: This snippet shows how to use a guided Stable Diffusion pipeline to generate an image based on a prompt and an input image. It sets various parameters like image dimensions, transformation strength, and guidance scales.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n# Run the pipeline\nimage = guided_pipeline(\n    prompt=prompt,\n    height=512,  # Height of the output image\n    width=512,   # Width of the output image\n    image=edit_image,  # Input image to guide the diffusion\n    strength=0.75,  # How much to transform the input image\n    num_inference_steps=30,  # Number of diffusion steps\n    guidance_scale=7.5,  # Scale of the classifier-free guidance\n    clip_guidance_scale=100,  # Scale of the CLIP guidance\n    num_images_per_prompt=1,  # Generate one image per prompt\n    eta=0.0,  # Noise scheduling parameter\n    num_cutouts=4,  # Number of cutouts for CLIP guidance\n    use_cutouts=False,  # Whether to use cutouts\n    output_type=\"pil\",  # Output as PIL image\n).images[0]\n\n# Display the generated image\nimage.show()\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Images for Inpainting\nDESCRIPTION: Loads initial and mask images from URLs and resizes them to 512x512 pixels using diffusers utility functions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image, make_image_grid\n\ninit_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg\"\n)\ninit_image = init_image.resize((512, 512))\n\nmask_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg\"\n)\nmask_image = mask_image.resize((512, 512))\nmake_image_grid([init_image, mask_image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing the UNet2DModel for Diffusion\nDESCRIPTION: Creates a UNet2DModel with specific architecture parameters including sample size, channel dimensions, and block configurations. This defines the neural network that will be trained to denoise images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import UNet2DModel\n\n>>> model = UNet2DModel(\n...     sample_size=config.image_size,  # the target image resolution\n...     in_channels=3,  # the number of input channels, 3 for RGB images\n...     out_channels=3,  # the number of output channels\n...     layers_per_block=2,  # how many ResNet layers to use per UNet block\n...     block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n...     down_block_types=(\n...         \"DownBlock2D\",  # a regular ResNet downsampling block\n...         \"DownBlock2D\",\n...         \"DownBlock2D\",\n...         \"DownBlock2D\",\n...         \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n...         \"DownBlock2D\",\n...     ),\n...     up_block_types=(\n...         \"UpBlock2D\",  # a regular ResNet upsampling block\n...         \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...     ),\n... )\n```\n\n----------------------------------------\n\nTITLE: Loading Models from Hugging Face\nDESCRIPTION: Python code demonstrating how to load Text-to-Image, Image-to-Image, and Inpainting models from Hugging Face using the EasyPipeline classes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/model_search/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pipeline_easy import (\n    EasyPipelineForText2Image,\n    EasyPipelineForImage2Image,\n    EasyPipelineForInpainting,\n)\n\n# Text-to-Image\npipeline = EasyPipelineForText2Image.from_huggingface(\n    \"search_word\",\n    checkpoint_format=\"diffusers\",\n).to(\"cuda\")\n\n\n# Image-to-Image\npipeline = EasyPipelineForImage2Image.from_huggingface(\n    \"search_word\",\n    checkpoint_format=\"diffusers\",\n).to(\"cuda\")\n\n\n# Inpainting\npipeline = EasyPipelineForInpainting.from_huggingface(\n    \"search_word\",\n    checkpoint_format=\"diffusers\",\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Video Upscaling with Zeroscope V2 XL Model\nDESCRIPTION: Example demonstrating how to upscale a previously generated video using the Zeroscope V2 XL model with Video-to-Video pipeline, increasing resolution to 1024x576.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_XL\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\n# memory optimization\npipe.unet.enable_forward_chunking(chunk_size=1, dim=1)\npipe.enable_vae_slicing()\n\nvideo = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]\n\nvideo_frames = pipe(prompt, video=video, strength=0.6).frames[0]\nvideo_path = export_to_video(video_frames)\nvideo_path\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionPipelineOutput in Python\nDESCRIPTION: This code snippet demonstrates how to import the StableDiffusionPipelineOutput class, which represents the output of the Stable Diffusion pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/attend_and_excite.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Accelerating UNet Inference with torch.compile\nDESCRIPTION: Demonstrates how to use PyTorch 2.0's torch.compile to optimize the UNet component of a diffusion pipeline. This optimization can significantly boost inference speed by compiling the model with reduced overhead.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\npipeline.unet = torch.compile(pipeline.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Text-to-3D Generation with ShapEPipeline\nDESCRIPTION: Demonstrates how to use ShapEPipeline to generate 3D objects from text prompts. Sets up the pipeline with CUDA support and generates image frames for two different prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import ShapEPipeline\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(device)\n\nguidance_scale = 15.0\nprompt = [\"A firecracker\", \"A birthday cupcake\"]\n\nimages = pipe(\n    prompt,\n    guidance_scale=guidance_scale,\n    num_inference_steps=64,\n    frame_size=256,\n).images\n```\n\n----------------------------------------\n\nTITLE: Generating Source Image with Kandinsky\nDESCRIPTION: Uses Kandinsky 2.1 model to generate a source image for image-to-3D conversion. Sets up the pipeline and generates an image of a cheeseburger.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nprior_pipeline = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\nprompt = \"A cheeseburger, white background\"\n\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, guidance_scale=1.0).to_tuple()\nimage = pipeline(\n    prompt,\n    image_embeds=image_embeds,\n    negative_image_embeds=negative_image_embeds,\n).images[0]\n\nimage.save(\"burger.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Image for Transformation\nDESCRIPTION: Code snippet that loads an initial image from a URL to use as the starting point for the image-to-image generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Model Files Without Symlinks Using huggingface_hub in Python\nDESCRIPTION: This code snippet demonstrates how to download model checkpoint files and configuration files from the Hugging Face Hub without using symlinks. It utilizes hf_hub_download for a single file and snapshot_download for multiple files with pattern matching, both with the local_dir_use_symlinks=False parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\nmy_local_checkpoint_path = hf_hub_download(\n    repo_id=\"segmind/SSD-1B\",\n    filename=\"SSD-1B.safetensors\"\n    local_dir=\"my_local_checkpoints\",\n    local_dir_use_symlinks=False\n)\nprint(\"My local checkpoint: \", my_local_checkpoint_path)\n\nmy_local_config_path = snapshot_download(\n    repo_id=\"segmind/SSD-1B\",\n    allow_patterns=[\"*.json\", \"**/*.json\", \"*.txt\", \"**/*.txt\"]\n    local_dir_use_symlinks=False,\n)\nprint(\"My local config: \", my_local_config_path)\n```\n\n----------------------------------------\n\nTITLE: Loading PixArt-α Pipeline with 8-bit Text Encoder\nDESCRIPTION: Initializes the PixArt-α pipeline with text encoder in 8-bit precision for reduced memory usage\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import T5EncoderModel\nfrom diffusers import PixArtAlphaPipeline\nimport torch\n\ntext_encoder = T5EncoderModel.from_pretrained(\n    \"PixArt-alpha/PixArt-XL-2-1024-MS\",\n    subfolder=\"text_encoder\",\n    load_in_8bit=True,\n    device_map=\"auto\",\n\n)\npipe = PixArtAlphaPipeline.from_pretrained(\n    \"PixArt-alpha/PixArt-XL-2-1024-MS\",\n    text_encoder=text_encoder,\n    transformer=None,\n    device_map=\"auto\"\n)\n```\n\n----------------------------------------\n\nTITLE: Pose-Controlled Video Generation\nDESCRIPTION: Demonstrates how to generate videos with pose control using StableDiffusionControlNetPipeline and custom attention processor.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video_zero.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    model_id, controlnet=controlnet, torch_dtype=torch.float16\n).to(\"cuda\")\n\n# Set the attention processor\npipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\npipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n\n# fix latents for all frames\nlatents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n\nprompt = \"Darth Vader dancing in a desert\"\nresult = pipe(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```\n\n----------------------------------------\n\nTITLE: Calculating CLIP Directional Similarity for Edited Images in Python\nDESCRIPTION: This snippet uses the DirectionalSimilarity module to compute the CLIP directional similarity scores for a set of original and edited images along with their captions. It demonstrates how to evaluate the consistency of image edits with respect to the given instructions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndir_similarity = DirectionalSimilarity(tokenizer, text_encoder, image_processor, image_encoder)\nscores = []\n\nfor i in range(len(input_images)):\n    original_image = input_images[i]\n    original_caption = original_captions[i]\n    edited_image = edited_images[i]\n    modified_caption = modified_captions[i]\n\n    similarity_score = dir_similarity(original_image, edited_image, original_caption, modified_caption)\n    scores.append(float(similarity_score.detach().cpu()))\n\nprint(f\"CLIP directional similarity: {np.mean(scores)}\")\n```\n\n----------------------------------------\n\nTITLE: Training DeepFloyd IF Stage 2 with Full DreamBooth\nDESCRIPTION: This script shows how to train the full model for stage 2 of DeepFloyd IF with DreamBooth. It uses a smaller batch size with gradient accumulation for effective batch size, lower learning rate, and timestep conditioning for the upscaler model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"DeepFloyd/IF-II-L-v1.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"dreambooth_dog_upscale\"\nexport VALIDATION_IMAGES=\"dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png\"\n\naccelerate launch train_dreambooth.py \\\n  --report_to wandb \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a sks dog\" \\\n  --resolution=256 \\\n  --train_batch_size=2 \\\n  --gradient_accumulation_steps=6 \\\n  --learning_rate=5e-6 \\\n  --max_train_steps=2000 \\\n  --validation_prompt=\"a sks dog\" \\\n  --validation_steps=150 \\\n  --checkpointing_steps=500 \\\n  --pre_compute_text_embeddings \\\n  --tokenizer_max_length=77 \\\n  --text_encoder_use_attention_mask \\\n  --validation_images $VALIDATION_IMAGES \\\n  --class_labels_conditioning timesteps \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Implementing Remote VAE Queueing for Concurrent Stable Diffusion Generation\nDESCRIPTION: This code implements a queueing system using Python's threading and queue modules to process multiple Stable Diffusion generation requests concurrently. It sets up a worker thread that decodes latents using a remote VAE endpoint while the main thread continues generating new latents, improving overall throughput.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_decode.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport queue\nimport threading\nfrom IPython.display import display\nfrom diffusers import StableDiffusionPipeline\n\ndef decode_worker(q: queue.Queue):\n    while True:\n        item = q.get()\n        if item is None:\n            break\n        image = remote_decode(\n            endpoint=\"https://q1bj3bpq6kzilnsu.us-east-1.aws.endpoints.huggingface.cloud/\",\n            tensor=item,\n            scaling_factor=0.18215,\n        )\n        display(image)\n        q.task_done()\n\nq = queue.Queue()\nthread = threading.Thread(target=decode_worker, args=(q,), daemon=True)\nthread.start()\n\ndef decode(latent: torch.Tensor):\n    q.put(latent)\n\nprompts = [\n    \"Blueberry ice cream, in a stylish modern glass , ice cubes, nuts, mint leaves, splashing milk cream, in a gradient purple background, fluid motion, dynamic movement, cinematic lighting, Mysterious\",\n    \"Lemonade in a glass, mint leaves, in an aqua and white background, flowers, ice cubes, halo, fluid motion, dynamic movement, soft lighting, digital painting, rule of thirds composition, Art by Greg rutkowski, Coby whitmore\",\n    \"Comic book art, beautiful, vintage, pastel neon colors, extremely detailed pupils, delicate features, light on face, slight smile, Artgerm, Mary Blair, Edmund Dulac, long dark locks, bangs, glowing, fashionable style, fairytale ambience, hot pink.\",\n    \"Masterpiece, vanilla cone ice cream garnished with chocolate syrup, crushed nuts, choco flakes, in a brown background, gold, cinematic lighting, Art by WLOP\",\n    \"A bowl of milk, falling cornflakes, berries, blueberries, in a white background, soft lighting, intricate details, rule of thirds, octane render, volumetric lighting\",\n    \"Cold Coffee with cream, crushed almonds, in a glass, choco flakes, ice cubes, wet, in a wooden background, cinematic lighting, hyper realistic painting, art by Carne Griffiths, octane render, volumetric lighting, fluid motion, dynamic movement, muted colors,\",\n]\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"Lykon/dreamshaper-8\",\n    torch_dtype=torch.float16,\n    vae=None,\n).to(\"cuda\")\n\npipe.unet = pipe.unet.to(memory_format=torch.channels_last)\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\n_ = pipe(\n    prompt=prompts[0],\n    output_type=\"latent\",\n)\n\nfor prompt in prompts:\n    latent = pipe(\n        prompt=prompt,\n        output_type=\"latent\",\n    ).images\n    decode(latent)\n\nq.put(None)\nthread.join()\n```\n\n----------------------------------------\n\nTITLE: Implementing ControlNet Guess Mode\nDESCRIPTION: Demonstrates the use of guess mode with ControlNet, which operates without prompts using Canny edge detection for image processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.utils import load_image, make_image_grid\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport cv2\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", use_safetensors=True)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, use_safetensors=True).to(\"cuda\")\n\noriginal_image = load_image(\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png\")\n\nimage = np.array(original_image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\nimage = pipe(\"\", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Integrating IP-Adapter with TCD Scheduler for SDXL\nDESCRIPTION: This example shows how to combine the TCD Scheduler with IP-Adapter and Stable Diffusion XL. It loads the TCD-LoRA and IP-Adapter weights to enable reference-based image generation in only 4 inference steps while maintaining fidelity to both the prompt and reference image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_tcd_lora.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers.utils import load_image, make_image_grid\n\nfrom ip_adapter import IPAdapterXL\nfrom scheduling_tcd import TCDScheduler\n\ndevice = \"cuda\"\nbase_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\nimage_encoder_path = \"sdxl_models/image_encoder\"\nip_ckpt = \"sdxl_models/ip-adapter_sdxl.bin\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    variant=\"fp16\"\n)\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\n\nip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device)\n\nref_image = load_image(\"https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/images/woman.png\").resize((512, 512))\n\nprompt = \"best quality, high quality, wearing sunglasses\"\n\nimage = ip_model.generate(\n    pil_image=ref_image,\n    prompt=prompt,\n    scale=0.5,\n    num_samples=1,\n    num_inference_steps=4,\n    guidance_scale=0,\n    eta=0.3,\n    seed=0,\n)[0]\n\ngrid_image = make_image_grid([ref_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Generating Weighted Text Embeddings with Textual Inversion for SD 1.5\nDESCRIPTION: Uses sd_embed to generate text embeddings for Stable Diffusion 1.5 with weighted prompts and textual inversion. The function returns prompt and negative prompt embeddings which are then used for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n( \n  prompt_embeds,\n  prompt_neg_embeds,\n) = get_weighted_text_embeddings_sd15(\n    pipe,\n    prompt=prompt,\n    neg_prompt=neg_prompt\n)\n\nimage = pipe(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=prompt_neg_embeds,\n    height=768,\n    width=896,\n    guidance_scale=4.0,\n    generator=torch.Generator(\"cuda\").manual_seed(2)\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Launching Training with DeepSpeed using Bash\nDESCRIPTION: This command launches the training process with the DeepSpeed Zero2 optimizer by using specific configuration settings, helping to optimize memory usage and training speed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file=ds2.yaml \\\n  train_dreambooth_lora_flux_miniature.py \\\n  --pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\" \\\n  --data_df_path=\"embeddings.parquet\" \\\n  --output_dir=\"yarn_art_lora_flux_nf4\" \\\n  --mixed_precision=\"no\" \\\n  --use_8bit_adam \\\n  --weighting_scheme=\"none\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --repeats=1 \\\n  --learning_rate=1e-4 \\\n  --guidance_scale=1 \\\n  --report_to=\"wandb\" \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --cache_latents \\\n  --rank=4 \\\n  --max_train_steps=700 \\\n  --seed=\"0\"\n```\n\n----------------------------------------\n\nTITLE: Generating High-FPS Videos with LTX-Video\nDESCRIPTION: This code example shows how to use the LTX-Video model to generate high frame rate videos (24 fps) with high resolution (768x512). It creates a detailed scene of a man walking towards a window with specific visual characteristics and camera movements.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import LTXPipeline\nfrom diffusers.utils import export_to_video\n\npipe = LTXPipeline.from_pretrained(\"Lightricks/LTX-Video\", torch_dtype=torch.bfloat16).to(\"cuda\")\n\nprompt = \"A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.\"\nvideo = pipe(\n    prompt=prompt,\n    width=704,\n    height=480,\n    num_frames=161,\n    num_inference_steps=50,\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=24)\n```\n\n----------------------------------------\n\nTITLE: Initializing StableDiffusionDepth2ImgPipeline with Stable Diffusion 2 in Python\nDESCRIPTION: Sets up the StableDiffusionDepth2ImgPipeline using the stabilityai/stable-diffusion-2-depth model with float16 precision. The pipeline is moved to the CUDA device for GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/depth2img.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionDepth2ImgPipeline\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = StableDiffusionDepth2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-depth\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with T2I-Adapter and Stable Diffusion XL\nDESCRIPTION: Code to generate an image using the StableDiffusionXLAdapterPipeline with T2I-Adapter. The pipeline uses the same approach as with SD 1.5, taking a text prompt and control image as inputs to generate an image guided by both sources.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\nimage = pipeline(\n  prompt=\"cinematic photo of a plush and soft midcentury style rug on a wooden floor, 35mm photograph, film, professional, 4k, highly detailed\",\n  image=image,\n  generator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Reverting to Default Attention Processor in Diffusion Pipeline\nDESCRIPTION: This Python diff snippet demonstrates how to revert to using the default attention processor in the diffusion pipeline. It is useful for making the pipeline more deterministic or converting it to other formats without using `AttnProcessor2_0`.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipe.unet.set_default_attn_processor()\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Generating 3D Meshes with ShapEPipeline\nDESCRIPTION: Code for generating 3D meshes instead of image frames using the ShapEPipeline. This is achieved by setting the output_type parameter to 'mesh', allowing for more versatile downstream applications of the generated 3D assets.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import ShapEPipeline\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(device)\n\nguidance_scale = 15.0\nprompt = \"A birthday cupcake\"\n\nimages = pipe(prompt, guidance_scale=guidance_scale, num_inference_steps=64, frame_size=256, output_type=\"mesh\").images\n```\n\n----------------------------------------\n\nTITLE: Training Custom Diffusion Model for Cat Concept\nDESCRIPTION: Full training command for customizing a Stable Diffusion model with cat images, including prior preservation regularization with real images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport OUTPUT_DIR=\"path-to-save-model\"\nexport INSTANCE_DIR=\"./data/cat\"\n\naccelerate launch train_custom_diffusion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --class_data_dir=./real_reg/samples_cat/ \\\n  --with_prior_preservation --real_prior --prior_loss_weight=1.0 \\\n  --class_prompt=\"cat\" --num_class_images=200 \\\n  --instance_prompt=\"photo of a <new1> cat\"  \\\n  --resolution=512  \\\n  --train_batch_size=2  \\\n  --learning_rate=1e-5  \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=250 \\\n  --scale_lr --hflip  \\\n  --modifier_token \"<new1>\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Training Dreambooth on 8GB GPU with DeepSpeed Optimization\nDESCRIPTION: Command for training Dreambooth on an 8GB GPU using DeepSpeed to offload tensors from VRAM to CPU or NVME. This configuration uses fp16 mixed precision and requires around 25GB of RAM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_subject_dreambooth/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch --mixed_precision=\"fp16\" train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --sample_batch_size=1 \\\n  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Stable Diffusion with Multi-GPU Training\nDESCRIPTION: Bash command to train a Stable Diffusion model using multiple GPUs through Accelerate's distributed training functionality. This allows for more efficient training across multiple hardware devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu  train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Training Stable Diffusion with Flax\nDESCRIPTION: Bash script for training a Stable Diffusion model on the Naruto BLIP captions dataset using Flax, which is optimized for TPUs and GPUs. The script sets environment variables for the base model and dataset and includes parameters for resolution, batch size, learning rate, and more.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport dataset_name=\"lambdalabs/naruto-blip-captions\"\n\npython train_text_to_image_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$dataset_name \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --output_dir=\"sd-naruto-model\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoPipeline with ControlNet and PAG for Text-to-Image Generation in Python\nDESCRIPTION: This snippet demonstrates how to create a ControlNet model and initialize an AutoPipeline for text-to-image generation with PAG enabled. It sets up the pipeline with specific PAG parameters and enables model CPU offload for efficient processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image, ControlNetModel\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16\n)\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    enable_pag=True,\n    pag_applied_layers=\"mid\",\n    torch_dtype=torch.float16\n)\npipeline.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Loading a Pre-trained Diffusion Model in Python\nDESCRIPTION: This snippet shows how to load a pre-trained diffusion model using the DiffusionPipeline class and generate a butterfly image. The model is loaded from the 'anton-l/ddpm-butterflies-128' checkpoint and moved to a CUDA device for faster inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/unconditional_image_generation.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\ngenerator = DiffusionPipeline.from_pretrained(\"anton-l/ddpm-butterflies-128\").to(\"cuda\")\nimage = generator().images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized StableAudioPipeline with BitsAndBytes\nDESCRIPTION: This code demonstrates how to load a quantized version of the Stable Audio model for memory-efficient inference. It uses 8-bit quantization via BitsAndBytes for both the text encoder and transformer components, resulting in reduced memory requirements while maintaining generation quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_audio.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, StableAudioDiTModel, StableAudioPipeline\nfrom diffusers.utils import export_to_video\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"stabilityai/stable-audio-open-1.0\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = StableAudioDiTModel.from_pretrained(\n    \"stabilityai/stable-audio-open-1.0\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = StableAudioPipeline.from_pretrained(\n    \"stabilityai/stable-audio-open-1.0\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"The sound of a hammer hitting a wooden surface.\"\nnegative_prompt = \"Low quality.\"\naudio = pipeline(\n    prompt,\n    negative_prompt=negative_prompt,\n    num_inference_steps=200,\n    audio_end_in_s=10.0,\n    num_waveforms_per_prompt=3,\n    generator=generator,\n).audios\n\noutput = audio[0].T.float().cpu().numpy()\nsf.write(\"hammer.wav\", output, pipeline.vae.sampling_rate)\n```\n\n----------------------------------------\n\nTITLE: Setting up IP-Adapter Plus\nDESCRIPTION: Configures IP-Adapter Plus with a custom ViT-H image encoder for enhanced performance with SDXL.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import CLIPVisionModelWithProjection\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"h94/IP-Adapter\",\n    subfolder=\"models/image_encoder\",\n    torch_dtype=torch.float16\n)\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    image_encoder=image_encoder,\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter-plus_sdxl_vit-h.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: Computing Prompt and Image Embeddings for SDXL Training\nDESCRIPTION: This snippet shows how to precompute prompt and image embeddings with SDXL's dual text encoders. The embeddings are computed and stored in memory to optimize the training process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntext_encoders = [text_encoder_one, text_encoder_two]\ntokenizers = [tokenizer_one, tokenizer_two]\ncompute_embeddings_fn = functools.partial(\n    encode_prompt,\n    text_encoders=text_encoders,\n    tokenizers=tokenizers,\n    proportion_empty_prompts=args.proportion_empty_prompts,\n    caption_column=args.caption_column,\n)\n\ntrain_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\ntrain_dataset = train_dataset.map(\n    compute_vae_encodings_fn,\n    batched=True,\n    batch_size=args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps,\n    new_fingerprint=new_fingerprint_for_vae,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting v_prediction Flag for Training Text-to-Image Models\nDESCRIPTION: Command-line flag to enable v_prediction during model training using the train_text_to_image.py or train_text_to_image_lora.py scripts. This is a prerequisite for using the rescaled noise schedule during inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/scheduler_features.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n--prediction_type=\"v_prediction\"\n```\n\n----------------------------------------\n\nTITLE: Creating Accelerate Configuration in Python for CogVideoX\nDESCRIPTION: Python code for writing a basic Accelerate configuration programmatically, useful in non-interactive environments like notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Loading a Community Pipeline from a Specific Version\nDESCRIPTION: This example shows how to load a community pipeline from a specific version (v0.25.0) of the Diffusers library using the custom_revision parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    custom_pipeline=\"clip_guided_stable_diffusion\",\n    custom_revision=\"v0.25.0\",\n    clip_model=clip_model,\n    feature_extractor=feature_extractor,\n    use_safetensors=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Additional Model Components\nDESCRIPTION: Python code showing how to load LoRA weights and Textual Inversion embeddings into an existing pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/model_search/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Load Lora into the pipeline.\npipeline.auto_load_lora_weights(\"Detail Tweaker\")\n\n# Load TextualInversion into the pipeline.\npipeline.auto_load_textual_inversion(\"EasyNegative\", token=\"EasyNegative\")\n```\n\n----------------------------------------\n\nTITLE: Using AutoPipelineForText2Image with Kandinsky 2.2\nDESCRIPTION: Implementation of text-to-image generation using the AutoPipelineForText2Image API with Kandinsky 2.2, which handles the combined prior and decoder pipeline automatically.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, guidance_scale=4.0, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorRT Text2Image Stable Diffusion Pipeline in Python\nDESCRIPTION: This code demonstrates how to set up and use a TensorRT-accelerated Text2Image Stable Diffusion pipeline. It uses the DDIMScheduler and generates an image based on a text prompt. The pipeline is optimized for CUDA execution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DDIMScheduler\nfrom diffusers.pipelines import DiffusionPipeline\n\n# Use the DDIMScheduler scheduler here instead\nscheduler = DDIMScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"scheduler\")\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n    custom_pipeline=\"stable_diffusion_tensorrt_txt2img\",\n    variant='fp16',\n    torch_dtype=torch.float16,\n    scheduler=scheduler,)\n\n# re-use cached folder to save ONNX models and TensorRT Engines\npipe.set_cached_folder(\"stabilityai/stable-diffusion-2-1\", variant='fp16',)\n\npipe = pipe.to(\"cuda\")\n\nprompt = \"a beautiful photograph of Mt. Fuji during cherry blossom\"\nimage = pipe(prompt).images[0]\nimage.save('tensorrt_mt_fuji.png')\n```\n\n----------------------------------------\n\nTITLE: Loading Inpainting Pipeline with AutoPipelineForInpainting\nDESCRIPTION: Loads an inpainting checkpoint using AutoPipelineForInpainting class, which automatically detects the appropriate pipeline class based on the checkpoint. It also enables model CPU offload and xformers memory efficient attention for optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Stable Diffusion Text-to-Image Pipeline\nDESCRIPTION: This Python script benchmarks the Stable Diffusion text-to-image pipeline using PyTorch 2.0 optimizations, including the optional `torch.compile` for enhanced execution speed, tested across various configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npath = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\n\npipe = DiffusionPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    images = pipe(prompt=prompt).images\n```\n\n----------------------------------------\n\nTITLE: Implementing SDXL Attentive Eraser Pipeline in Python\nDESCRIPTION: Demonstrates the setup and usage of the Stable Diffusion XL Attentive Eraser Pipeline for object removal and inpainting. Includes preprocessing functions for images and masks, along with configurable parameters for the removal process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_109\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DDIMScheduler, DiffusionPipeline\nfrom diffusers.utils import load_image\nimport torch.nn.functional as F\nfrom torchvision.transforms.functional import to_tensor, gaussian_blur\n\ndtype = torch.float16\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n\nscheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    custom_pipeline=\"pipeline_stable_diffusion_xl_attentive_eraser\",\n    scheduler=scheduler,\n    variant=\"fp16\",\n    use_safetensors=True,\n    torch_dtype=dtype,\n).to(device)\n\n\ndef preprocess_image(image_path, device):\n    image = to_tensor((load_image(image_path)))\n    image = image.unsqueeze_(0).float() * 2 - 1 # [0,1] --> [-1,1]\n    if image.shape[1] != 3:\n        image = image.expand(-1, 3, -1, -1)\n        image = F.interpolate(image, (1024, 1024))\n        image = image.to(dtype).to(device)\n        return image\n\ndef preprocess_mask(mask_path, device):\n    mask = to_tensor((load_image(mask_path, convert_method=lambda img: img.convert('L'))))\n    mask = mask.unsqueeze_(0).float()  # 0 or 1\n    mask = F.interpolate(mask, (1024, 1024))\n    mask = gaussian_blur(mask, kernel_size=(77, 77))\n    mask[mask < 0.1] = 0\n    mask[mask >= 0.1] = 1\n    mask = mask.to(dtype).to(device)\n    return mask\n\nprompt = \"\" # Set prompt to null\nseed=123 \ngenerator = torch.Generator(device=device).manual_seed(seed)\nsource_image_path = \"https://raw.githubusercontent.com/Anonym0u3/Images/refs/heads/main/an1024.png\"\nmask_path = \"https://raw.githubusercontent.com/Anonym0u3/Images/refs/heads/main/an1024_mask.png\"\nsource_image = preprocess_image(source_image_path, device)\nmask = preprocess_mask(mask_path, device)\n\nimage = pipeline(\n    prompt=prompt, \n    image=source_image,\n    mask_image=mask,\n    height=1024,\n    width=1024,\n    AAS=True, # enable AAS\n    strength=0.8, # inpainting strength\n    rm_guidance_scale=9, # removal guidance scale\n    ss_steps = 9, # similarity suppression steps\n    ss_scale = 0.3, # similarity suppression scale\n    AAS_start_step=0, # AAS start step\n    AAS_start_layer=34, # AAS start layer\n    AAS_end_layer=70, # AAS end layer\n    num_inference_steps=50, # number of inference steps # AAS_end_step = int(strength*num_inference_steps)\n    generator=generator,\n    guidance_scale=1,\n).images[0]\nimage.save('./removed_img.png')\nprint(\"Object removal completed\")\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with OmniGen\nDESCRIPTION: Example of generating a 1024x1024 image from a detailed text prompt using OmniGen. Demonstrates setting image dimensions, guidance scale, and random seed for reproducibility.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/omnigen.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"Realistic photo. A young woman sits on a sofa, holding a book and facing the camera. She wears delicate silver hoop earrings adorned with tiny, sparkling diamonds that catch the light, with her long chestnut hair cascading over her shoulders. Her eyes are focused and gentle, framed by long, dark lashes. She is dressed in a cozy cream sweater, which complements her warm, inviting smile. Behind her, there is a table with a cup of water in a sleek, minimalist blue mug. The background is a serene indoor setting with soft natural light filtering through a window, adorned with tasteful art and flowers, creating a cozy and peaceful ambiance. 4K, HD.\"\nimage = pipe(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    guidance_scale=3,\n    generator=torch.Generator(device=\"cpu\").manual_seed(111),\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionControlNetPipeline in Python\nDESCRIPTION: This snippet shows the import statement for the StableDiffusionControlNetPipeline class, which is the main pipeline for using ControlNet with Stable Diffusion for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] StableDiffusionControlNetPipeline\n```\n\n----------------------------------------\n\nTITLE: FreeInit Implementation for Video Generation in Python\nDESCRIPTION: Implements FreeInit method to improve temporal consistency and quality of generated videos without additional training. Includes configuration for the AnimateDiff pipeline with FreeInit enabled.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\")\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16).to(\"cuda\")\npipe.scheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    beta_schedule=\"linear\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    steps_offset=1\n)\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_vae_tiling()\n\n# enable FreeInit\n# Refer to the enable_free_init documentation for a full list of configurable parameters\npipe.enable_free_init(method=\"butterworth\", use_fast_sampling=True)\n\n# run inference\noutput = pipe(\n    prompt=\"a panda playing a guitar, on a boat, in the ocean, high quality\",\n    negative_prompt=\"bad quality, worse quality\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=20,\n    generator=torch.Generator(\"cpu\").manual_seed(666),\n)\n\n# disable FreeInit\npipe.disable_free_init()\n\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Latte Pipeline with CUDA Support\nDESCRIPTION: Loads the Latte pipeline model with float16 precision and moves it to CUDA device for GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latte.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import LattePipeline\n\npipeline = LattePipeline.from_pretrained(\n\t\"maxin-cn/Latte-1\", torch_dtype=torch.float16\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Using Step-by-Step Image Generation Callback in Diffusers\nDESCRIPTION: Demonstrates how to use a callback to visualize the generation process after each step. The code sets up a pipeline and runs text-to-image generation with the decode_tensors callback that saves intermediate results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\nfrom PIL import Image\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True\n).to(\"cuda\")\n\nimage = pipeline(\n    prompt=\"A croissant shaped like a cute bear.\",\n    negative_prompt=\"Deformed, ugly, bad anatomy\",\n    callback_on_step_end=decode_tensors,\n    callback_on_step_end_tensor_inputs=[\"latents\"],\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Generating Video with Pose Control in Text2Video-Zero\nDESCRIPTION: This snippet shows how to generate a video using Text2Video-Zero with pose control by fixing latents and passing a prompt with pose images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlatents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n\nprompt = \"Darth Vader dancing in a desert\"\nresult = pipeline(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with OmniGen\nDESCRIPTION: Shows how to generate a 1024x1024 image from a detailed text prompt. The example demonstrates setting the height, width, guidance scale, and random seed for reproducible outputs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/omnigen.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenPipeline\n\npipe = OmniGenPipeline.from_pretrained(\n    \"Shitao/OmniGen-v1-diffusers\",\n    torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\n\nprompt = \"Realistic photo. A young woman sits on a sofa, holding a book and facing the camera. She wears delicate silver hoop earrings adorned with tiny, sparkling diamonds that catch the light, with her long chestnut hair cascading over her shoulders. Her eyes are focused and gentle, framed by long, dark lashes. She is dressed in a cozy cream sweater, which complements her warm, inviting smile. Behind her, there is a table with a cup of water in a sleek, minimalist blue mug. The background is a serene indoor setting with soft natural light filtering through a window, adorned with tasteful art and flowers, creating a cozy and peaceful ambiance. 4K, HD.\"\nimage = pipe(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    guidance_scale=3,\n    generator=torch.Generator(device=\"cpu\").manual_seed(111),\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Stable Diffusion Mixture Tiling Pipeline in Python\nDESCRIPTION: This snippet shows how to set up the Stable Diffusion Mixture Tiling Pipeline for SD 1.5. It creates a custom scheduler and initializes the pipeline with specific parameters for the Mixture approach.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import LMSDiscreteScheduler, DiffusionPipeline\n\n# Create scheduler and model (similar to StableDiffusionPipeline)\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\npipeline = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", scheduler=scheduler, custom_pipeline=\"mixture_tiling\")\npipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with IP-Adapter Masks\nDESCRIPTION: Demonstrates how to generate images using IP-Adapter with masks applied through cross-attention parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nnum_images = 1\n\nimage = pipeline(\n    prompt=\"2 girls\",\n    ip_adapter_image=ip_images,\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n    num_inference_steps=20,\n    num_images_per_prompt=num_images,\n    generator=generator,\n    cross_attention_kwargs={\"ip_adapter_masks\": masks}\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Image Variation with UniDiffuser Using Round-Trip Generation\nDESCRIPTION: Shows how to create image variations with UniDiffuser using a two-step process: first converting the image to text, then generating a new image from that text. This produces images that are semantically similar to the input.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unidiffuser.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\nfrom diffusers.utils import load_image\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Image variation can be performed with an image-to-text generation followed by a text-to-image generation:\n# 1. Image-to-text generation\nimage_url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unidiffuser/unidiffuser_example_image.jpg\"\ninit_image = load_image(image_url).resize((512, 512))\n\nsample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)\ni2t_text = sample.text[0]\nprint(i2t_text)\n\n# 2. Text-to-image generation\nsample = pipe(prompt=i2t_text, num_inference_steps=20, guidance_scale=8.0)\nfinal_image = sample.images[0]\nfinal_image.save(\"unidiffuser_image_variation_sample.png\")\n```\n\n----------------------------------------\n\nTITLE: Inpainting with Prompt Embeddings in Python\nDESCRIPTION: This snippet demonstrates how to use pre-generated prompt embeddings for inpainting with the AutoPipelineForInpainting. It shows how to load the pipeline, enable optimizations, and generate an image using prompt embeddings instead of text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16,\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nimage = pipeline(prompt_embeds=prompt_embeds, # generated from Compel\n    negative_prompt_embeds=negative_prompt_embeds, # generated from Compel\n    image=init_image,\n    mask_image=mask_image\n).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Accelerate Environment Configuration\nDESCRIPTION: Set up the Accelerate environment for distributed or mixed-precision training. This is important for configuring your hardware environment for optimal performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Exporting Stable Diffusion XL Model to ONNX Format\nDESCRIPTION: Uses the optimum-cli to export a Stable Diffusion XL model to ONNX format for offline use.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/onnx.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export onnx --model stabilityai/stable-diffusion-xl-base-1.0 --task stable-diffusion-xl sd_xl_onnx/\n```\n\n----------------------------------------\n\nTITLE: Implementing T-GATE with StableDiffusionXL Pipeline\nDESCRIPTION: Example of accelerating StableDiffusionXLPipeline using T-GATE with DPMSolverMultistepScheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/tgate.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers import DPMSolverMultistepScheduler\nfrom tgate import TgateSDXLLoader\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-base-1.0\",\n            torch_dtype=torch.float16,\n            variant=\"fp16\",\n            use_safetensors=True,\n)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n\ngate_step = 10\ninference_step = 25\npipe = TgateSDXLLoader(\n       pipe,\n       gate_step=gate_step,\n       num_inference_steps=inference_step,\n).to(\"cuda\")\n\nimage = pipe.tgate(\n       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n       gate_step=gate_step,\n       num_inference_steps=inference_step\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Preparing IPEX-Optimized Stable Diffusion XL Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to prepare the Stable Diffusion XL pipeline for IPEX optimization using different data types (Float32 and BFloat16). It sets up the pipeline with specific image dimensions and data types.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_45\n\nLANGUAGE: python\nCODE:\n```\n# value of image height/width should be consistent with the pipeline inference\n# For Float32\npipe.prepare_for_ipex(torch.float32, prompt, height=512, width=512)\n# For BFloat16\npipe.prepare_for_ipex(torch.bfloat16, prompt, height=512, width=512)\n```\n\n----------------------------------------\n\nTITLE: Using Tiny AutoEncoder with Stable Diffusion v-2.1 in Python\nDESCRIPTION: This snippet demonstrates how to use the Tiny AutoEncoder (TAESD) with a Stable Diffusion v-2.1 pipeline. It loads the pipeline, replaces the VAE with TAESD, and generates an image from a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoder_tiny.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, AutoencoderTiny\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16\n)\npipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"slice of delicious New York-style berry cheesecake\"\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained LoRA Model\nDESCRIPTION: Python script demonstrating how to load a trained LoRA model and generate images using the Diffusion Pipeline\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nmodel_path = \"takuoko/sd-naruto-model-lora-sdxl\"\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\npipe.load_lora_weights(model_path)\n\nprompt = \"A naruto with green eyes and red legs.\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading StableDiffusionPipeline with Textual Inversion\nDESCRIPTION: Initializes the StableDiffusionPipeline with the specified model checkpoint and loads the textual inversion embedding from the provided repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/textual_inversion_inference.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline = StableDiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path, torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\npipeline.load_textual_inversion(repo_id_embeds)\n```\n\n----------------------------------------\n\nTITLE: Listing LoRA Adapters by Pipeline Component in Diffusers\nDESCRIPTION: This code shows how to use the get_list_adapters() method to retrieve active LoRA adapters for each component of a Diffusers pipeline. It returns a dictionary with component names as keys and lists of adapter names as values.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlist_adapters_component_wise = pipe.get_list_adapters()\nlist_adapters_component_wise\n{\"text_encoder\": [\"toy\", \"pixel\"], \"unet\": [\"toy\", \"pixel\"], \"text_encoder_2\": [\"toy\", \"pixel\"]}\n```\n\n----------------------------------------\n\nTITLE: Disabling the Safety Checker in a Pipeline\nDESCRIPTION: Disable the safety checker component which normally filters out potentially harmful generated images by setting the safety_checker parameter to None when loading the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, safety_checker=None)\n```\n\n----------------------------------------\n\nTITLE: Combining distilled Stable Diffusion with Tiny AutoEncoder\nDESCRIPTION: Loads a distilled Stable Diffusion model and replaces its autoencoder with a distilled version (Tiny AutoEncoder) for even faster inference. This example uses the bk-sdm-small model and taesd-diffusers autoencoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/fp16.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\n\ndistilled = StableDiffusionPipeline.from_pretrained(\n    \"nota-ai/bk-sdm-small\", torch_dtype=torch.float16, use_safetensors=True,\n).to(\"cuda\")\ndistilled.vae = AutoencoderTiny.from_pretrained(\n    \"sayakpaul/taesd-diffusers\", torch_dtype=torch.float16, use_safetensors=True,\n).to(\"cuda\")\n\nprompt = \"a golden vase with different flowers\"\ngenerator = torch.manual_seed(2023)\nimage = distilled(\"a golden vase with different flowers\", num_inference_steps=25, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for ControlNet SDXL Training (Bash/Python)\nDESCRIPTION: Commands to initialize an Accelerate environment for training, including options for interactive and non-interactive setups.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom LogitsProcessor for Biased Word Generation in Python\nDESCRIPTION: Creates a custom LogitsProcessor that biases token generation towards words in the predefined list. This ensures the model generates prompts containing specific quality-enhancing keywords while avoiding repetition.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CustomLogitsProcessor(LogitsProcessor):\n    def __init__(self, bias):\n        super().__init__()\n        self.bias = bias\n\n    def __call__(self, input_ids, scores):\n        if len(input_ids.shape) == 2:\n            last_token_id = input_ids[0, -1]\n            self.bias[last_token_id] = -1e10\n        return scores + self.bias\n\nword_ids = [tokenizer.encode(word, add_prefix_space=True)[0] for word in words]\nbias = torch.full((tokenizer.vocab_size,), -float(\"Inf\")).to(\"cuda\")\nbias[word_ids] = 0\nprocessor = CustomLogitsProcessor(bias)\nprocessor_list = LogitsProcessorList([processor])\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionPipelineOutput in Python\nDESCRIPTION: This snippet demonstrates how to import the StableDiffusionPipelineOutput class from the Diffusers library. This class represents the output of the Stable Diffusion pipeline, including generated images and additional metadata.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnetxs.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Quantized SANA Pipeline Implementation in Python\nDESCRIPTION: Example showing how to load and use a quantized SanaPipeline for efficient text-to-image generation. Demonstrates configuration of 8-bit quantization using bitsandbytes for both text encoder and transformer components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/sana.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, SanaTransformer2DModel, SanaPipeline\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, AutoModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = AutoModel.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = SanaTransformer2DModel.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = SanaPipeline.from_pretrained(\n    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\nimage = pipeline(prompt).images[0]\nimage.save(\"sana.png\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate\nDESCRIPTION: Different methods to configure the Accelerate environment for training, including interactive, default, and programmatic approaches.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Implementing ControlNet with Pose Estimation\nDESCRIPTION: Shows how to combine ControlNet with text-to-image models for pose-guided image generation using human pose estimation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import ControlNetModel, AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\n\t\"lllyasviel/control_v11p_sd15_openpose\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\npose_image = load_image(\"https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/control.png\")\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(31)\nimage = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", image=pose_image, generator=generator).images[0]\n```\n\n----------------------------------------\n\nTITLE: Enhancing Prompts with Descriptive Details in Python\nDESCRIPTION: Adds more descriptive elements to the prompt for better image generation, including specific style details, lighting information, and aspect ratio parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprompt += \", tribal panther make up, blue on red, side profile, looking away, serious eyes\"\nprompt += \" 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\"\n```\n\n----------------------------------------\n\nTITLE: Loading a Diffusion Pipeline from Local Files\nDESCRIPTION: This code demonstrates how to load a DiffusionPipeline from locally stored model weights rather than downloading them directly from the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> pipeline = DiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\", use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Different Schedulers in Python\nDESCRIPTION: This code demonstrates how to generate images using different schedulers in a Stable Diffusion pipeline. It includes examples for LMSDiscreteScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, and DPMSolverMultistepScheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/schedulers.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import LMSDiscreteScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler\n\n# Example for LMSDiscreteScheduler\npipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator).images[0]\n\n# Similar code blocks for other schedulers...\n```\n\n----------------------------------------\n\nTITLE: Launching Training Script with Min-SNR Weighting\nDESCRIPTION: This command shows how to launch the training script while setting the Min-SNR parameter to the recommended value of 5.0. The Min-SNR strategy is aimed at enhancing convergence speed during model training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_text_to_image_sdxl.py \\n  --snr_gamma=5.0\n```\n\n----------------------------------------\n\nTITLE: InstructPix2Pix Inference Example\nDESCRIPTION: Python code demonstrating how to perform inference with a trained InstructPix2Pix model. Shows loading the model, downloading a test image, applying an edit instruction, and saving the result with customizable parameters for controlling quality and style.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\n\nmodel_id = \"your_model_id\" # <- replace this\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\nurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\"\n\n\ndef download_image(url):\n    image = PIL.Image.open(requests.get(url, stream=True).raw)\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image\n\nimage = download_image(url)\nprompt = \"wipe out the lake\"\nnum_inference_steps = 20\nimage_guidance_scale = 1.5\nguidance_scale = 10\n\nedited_image = pipe(prompt,\n    image=image,\n    num_inference_steps=num_inference_steps,\n    image_guidance_scale=image_guidance_scale,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images[0]\nedited_image.save(\"edited_image.png\")\n```\n\n----------------------------------------\n\nTITLE: Image Editing with OmniGen using Multimodal Inputs\nDESCRIPTION: Demonstrates how to edit an image using OmniGen with multimodal inputs. The code shows the special placeholder syntax for representing the input image in the prompt and how to preserve the original image dimensions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/omnigen.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenPipeline\nfrom diffusers.utils import load_image \n\npipe = OmniGenPipeline.from_pretrained(\n    \"Shitao/OmniGen-v1-diffusers\",\n    torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\n\nprompt=\"<img><|image_1|></img> Remove the woman's earrings. Replace the mug with a clear glass filled with sparkling iced cola.\"\ninput_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/t2i_woman_with_book.png\")]\nimage = pipe(\n    prompt=prompt, \n    input_images=input_images, \n    guidance_scale=2, \n    img_guidance_scale=1.6,\n    use_input_image_size_as_output=True,\n    generator=torch.Generator(device=\"cpu\").manual_seed(222)\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: SD3 with IP-Adapter Integration\nDESCRIPTION: Implementation showing how to use IP-Adapter with SD3 for image-guided generation, including setup of image encoder and feature extractor\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom PIL import Image\n\nfrom diffusers import StableDiffusion3Pipeline\nfrom transformers import SiglipVisionModel, SiglipImageProcessor\n\nimage_encoder_id = \"google/siglip-so400m-patch14-384\"\nip_adapter_id = \"InstantX/SD3.5-Large-IP-Adapter\"\n\nfeature_extractor = SiglipImageProcessor.from_pretrained(\n    image_encoder_id,\n    torch_dtype=torch.float16\n)\nimage_encoder = SiglipVisionModel.from_pretrained(\n    image_encoder_id,\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\npipe = StableDiffusion3Pipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-3.5-large\",\n    torch_dtype=torch.float16,\n    feature_extractor=feature_extractor,\n    image_encoder=image_encoder,\n).to(\"cuda\")\n\npipe.load_ip_adapter(ip_adapter_id)\npipe.set_ip_adapter_scale(0.6)\n\nref_img = Image.open(\"image.jpg\").convert('RGB')\n\nimage = pipe(\n    width=1024,\n    height=1024,\n    prompt=\"a cat\",\n    negative_prompt=\"lowres, low quality, worst quality\",\n    num_inference_steps=24,\n    guidance_scale=5.0,\n    ip_adapter_image=ref_img\n).images[0]\n\nimage.save(\"result.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Caption Generation Function with BLIP in Python\nDESCRIPTION: Defines a utility function that generates a caption for input images using the BLIP model. The function handles moving tensors to GPU, generating the caption, and offloading the model back to CPU to save memory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@torch.no_grad()\ndef generate_caption(images, caption_generator, caption_processor):\n    text = \"a photograph of\"\n\n    inputs = caption_processor(images, text, return_tensors=\"pt\").to(device=\"cuda\", dtype=caption_generator.dtype)\n    caption_generator.to(\"cuda\")\n    outputs = caption_generator.generate(**inputs, max_new_tokens=128)\n\n    # offload caption generator\n    caption_generator.to(\"cpu\")\n\n    caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n    return caption\n```\n\n----------------------------------------\n\nTITLE: Diffusers Library Copyright Notice\nDESCRIPTION: Copyright and Apache 2.0 license header for the Diffusers library documentation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Setting Verbosity Level in Diffusers\nDESCRIPTION: Example of how to change the verbosity level to INFO in the Diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/logging.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\n\ndiffusers.logging.set_verbosity_info()\n```\n\n----------------------------------------\n\nTITLE: Adjusting Guidance Scale in I2VGenXLPipeline for Video Generation in Python\nDESCRIPTION: This code snippet illustrates how to use the I2VGenXLPipeline and adjust the guidance_scale parameter to control the alignment between the generated video and the input prompt or image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import I2VGenXLPipeline\nfrom diffusers.utils import export_to_gif, load_image\n\npipeline = I2VGenXLPipeline.from_pretrained(\"ali-vilab/i2vgen-xl\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline.enable_model_cpu_offload()\n\nimage_url = \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/i2vgen_xl_images/img_0009.png\"\nimage = load_image(image_url).convert(\"RGB\")\n\nprompt = \"Papers were floating in the air on a table in the library\"\nnegative_prompt = \"Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms\"\ngenerator = torch.manual_seed(0)\n\nframes = pipeline(\n    prompt=prompt,\n    image=image,\n    num_inference_steps=50,\n    negative_prompt=negative_prompt,\n    guidance_scale=1.0,\n    generator=generator\n).frames[0]\nexport_to_gif(frames, \"i2v.gif\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Multi-ControlNet SDXL Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use the StableDiffusionXLControlNetPipeline to generate images with multiple control inputs. It sets up prompts, prepares input images, and generates multiple output images using the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a giant standing in a fantasy landscape, best quality\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\ngenerator = torch.manual_seed(1)\n\nimages = [openpose_image.resize((1024, 1024)), canny_image.resize((1024, 1024))]\n\nimages = pipe(\n    prompt,\n    image=images,\n    num_inference_steps=25,\n    generator=generator,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=3,\n    controlnet_conditioning_scale=[1.0, 0.8],\n).images\nmake_image_grid([original_image, canny_image, openpose_image,\n                images[0].resize((512, 512)), images[1].resize((512, 512)), images[2].resize((512, 512))], rows=2, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Documenting CogVideoXPipelineOutput Class in Python\nDESCRIPTION: This code snippet represents an autodoc directive for generating documentation for the CogVideoXPipelineOutput class. It is part of the CogVideo pipeline in the Diffusers library, likely used for handling pipeline outputs in video generation tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/cogvideox.md#2025-04-11_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n[[autodoc]] pipelines.cogvideo.pipeline_output.CogVideoXPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Swapping Schedulers in a Diffusion Pipeline\nDESCRIPTION: This snippet shows how to replace the default PNDMScheduler with an EulerDiscreteScheduler in a loaded pipeline, demonstrating the flexibility of the Diffusers library to experiment with different noise schedulers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import EulerDiscreteScheduler\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\n>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: PyTorch 1.13 MPS Warmup Fix\nDESCRIPTION: Shows how to implement a workaround for PyTorch 1.13 users where an initial warmup pass is needed to ensure consistent inference results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/mps.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\").to(\"mps\")\npipe.enable_attention_slicing()\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n# First-time \"warmup\" pass if PyTorch version is 1.13\n_ = pipe(prompt, num_inference_steps=1)\n\n# Results match those from the CPU device after the warmup pass.\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Depth ControlNet with TCD Scheduler for SDXL\nDESCRIPTION: This code demonstrates how to use the TCD Scheduler with Stable Diffusion XL's Depth ControlNet model. It includes a depth map generation function and pipeline setup that utilizes TCD-LoRA to achieve faster inference with only 4 steps while maintaining quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_tcd_lora.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom transformers import DPTImageProcessor, DPTForDepthEstimation\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\nfrom diffusers.utils import load_image, make_image_grid\nfrom scheduling_tcd import TCDScheduler\n\ndevice = \"cuda\"\ndepth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(device)\nfeature_extractor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n\ndef get_depth_map(image):\n    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n    with torch.no_grad(), torch.autocast(device):\n        depth_map = depth_estimator(image).predicted_depth\n\n    depth_map = torch.nn.functional.interpolate(\n        depth_map.unsqueeze(1),\n        size=(1024, 1024),\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n    image = torch.cat([depth_map] * 3, dim=1)\n\n    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n    return image\n\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\ncontrolnet_id = \"diffusers/controlnet-depth-sdxl-1.0\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n\ncontrolnet = ControlNetModel.from_pretrained(\n    controlnet_id,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    base_model_id,\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe.enable_model_cpu_offload()\n\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\n\nprompt = \"stormtrooper lecture, photorealistic\"\n\nimage = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\ndepth_image = get_depth_map(image)\n\ncontrolnet_conditioning_scale = 0.5  # recommended for good generalization\n\nimage = pipe(\n    prompt,\n    image=depth_image,\n    num_inference_steps=4,\n    guidance_scale=0,\n    eta=0.3,\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\n    generator=torch.Generator(device=device).manual_seed(0),\n).images[0]\n\ngrid_image = make_image_grid([depth_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Targeting Specific Layers with LoRA Training\nDESCRIPTION: Command-line flag to specify which layers to target during LoRA training, offering a more compact training approach by focusing on specific layer types.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n--lora_layers attn.to_k,attn.to_q,attn.to_v,attn.to_out.0\n```\n\n----------------------------------------\n\nTITLE: Loading Diffusion Pipeline from Pretrained Model\nDESCRIPTION: Demonstrates how to load a diffusion pipeline from a pretrained model using the DiffusionPipeline class. This is a common pattern used across all pipeline implementations in the Diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md#2025-04-11_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nDiffusionPipeline.from_pretrained(\"model_name\")\n```\n\n----------------------------------------\n\nTITLE: Setting a Single Active LoRA Adapter\nDESCRIPTION: Reactivates only the toy-face adapter to generate an image with just that style, demonstrating how to switch between adapters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe.set_adapters(\"toy\")\n\nprompt = \"toy_face of a hacker with a hoodie\"\nlora_scale = 0.9\nimage = pipe(\n    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=torch.manual_seed(0)\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Scaling Initial Noise\nDESCRIPTION: Scales the input latents with the scheduler's initial noise sigma value for improved scheduling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlatents = latents * scheduler.init_noise_sigma\n```\n\n----------------------------------------\n\nTITLE: Improving Selected Image with Prompt Variations\nDESCRIPTION: Demonstrates how to create variations of a selected image by reusing the same seed (0) while enhancing the prompt with different style specifications. This technique maintains the core visual elements while exploring different artistic directions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompt = [prompt + t for t in [\", highly realistic\", \", artsy\", \", trending\", \", colorful\"]]\ngenerator = [torch.Generator(device=\"cuda\").manual_seed(0) for i in range(4)]\nimages = pipeline(prompt, generator=generator).images\nmake_image_grid(images, rows=2, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Importing EulerAncestralDiscreteScheduler in Python\nDESCRIPTION: This code snippet demonstrates how to import the EulerAncestralDiscreteScheduler class from the HuggingFace Diffusers library. It's typically used for fast image generation in 20-30 steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import EulerAncestralDiscreteScheduler\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22Pipeline in Python\nDESCRIPTION: This snippet defines the KandinskyV22Pipeline class, which is the main pipeline for text-to-image generation in Kandinsky 2.2. It includes the __call__ method for generating images from text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22Pipeline\n\t- all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Preparing Human Pose Estimation for ControlNet in Python\nDESCRIPTION: This snippet sets up the OpenPose detector for human pose estimation, loads an original image, and generates a pose image. It demonstrates how to prepare conditioning input for ControlNet.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom controlnet_aux import OpenposeDetector\n\nopenpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n)\nopenpose_image = openpose(original_image)\nmake_image_grid([original_image, openpose_image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Launching InstructPix2Pix SDXL Training with Validation\nDESCRIPTION: Initiates the training process for InstructPix2Pix using SDXL with additional validation parameters. It includes options for EMA, validation image, and Weights and Biases reporting.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README_sdxl.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_instruct_pix2pix_sdxl.py \\\n    --pretrained_model_name_or_path=stabilityai/stable-diffusion-xl-base-1.0 \\\n    --dataset_name=$DATASET_ID \\\n    --use_ema \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=512 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --seed=42 \\\n    --val_image_url_or_path=\"https://datasets-server.huggingface.co/assets/fusing/instructpix2pix-1000-samples/--/fusing--instructpix2pix-1000-samples/train/23/input_image/image.jpg\" \\\n    --validation_prompt=\"make it in japan\" \\\n    --report_to=wandb \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training with Text Encoder and UNet in Flax\nDESCRIPTION: Command to fine-tune both the text encoder and UNet using Flax implementation, which can provide better results for facial images and other detailed subjects.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\npython train_dreambooth_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --train_text_encoder \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=2e-6 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: LoRA Inference Script\nDESCRIPTION: Python script for inference using trained LoRA weights with pose control\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogview4-control/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers import CogView4ControlPipeline\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nimport torch \n\npipe = CogView4ControlPipeline.from_pretrained(\"THUDM/CogView4-6B\", torch_dtype=torch.bfloat16).to(\"cuda\")\npipe.load_lora_weights(\"...\") # change this.\n\nopen_pose = OpenposeDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n# prepare pose condition.\nurl = \"https://huggingface.co/Adapter/t2iadapter/resolve/main/people.jpg\"\nimage = load_image(url)\nimage = open_pose(image, detect_resolution=512, image_resolution=1024)\nimage = np.array(image)[:, :, ::-1]           \nimage = Image.fromarray(np.uint8(image))\n\nprompt = \"A couple, 4k photo, highly detailed\"\n\ngen_images = pipe(\n  prompt=prompt,\n  control_image=image,\n  num_inference_steps=50,\n  joint_attention_kwargs={\"scale\": 0.9},\n  guidance_scale=25., \n).images[0]\ngen_images.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Intrinsic Image Decomposition - Appearance Model\nDESCRIPTION: Shows how to use the Marigold Appearance model for intrinsic image decomposition to predict albedo, roughness, and metallicity maps from an input image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\n\npipe = diffusers.MarigoldIntrinsicsPipeline.from_pretrained(\n    \"prs-eth/marigold-iid-appearance-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(\"cuda\")\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\nintrinsics = pipe(image)\n\nvis = pipe.image_processor.visualize_intrinsics(intrinsics.prediction, pipe.target_properties)\nvis[0][\"albedo\"].save(\"einstein_albedo.png\")\nvis[0][\"roughness\"].save(\"einstein_roughness.png\")\nvis[0][\"metallicity\"].save(\"einstein_metallicity.png\")\n```\n\n----------------------------------------\n\nTITLE: Installing sd_embed Package\nDESCRIPTION: Command to install the Stable Diffusion Long Prompt Weighted Embedding package from GitHub, which is needed for prompt weighting functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/xhinker/sd_embed.git@main\n```\n\n----------------------------------------\n\nTITLE: Reading Video Frames with ImageIO\nDESCRIPTION: Reads video frames from a file path using ImageIO and converts them to PIL Image objects. Extracts a specified number of frames for processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video_zero.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nimport imageio\n\nreader = imageio.get_reader(video_path, \"ffmpeg\")\nframe_count = 8\nvideo = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n```\n\n----------------------------------------\n\nTITLE: Setting Up ControlNet and Diffusion Pipeline in Python\nDESCRIPTION: This snippet initializes the ControlNet model and sets up the Stable Diffusion pipeline with custom scheduler and parameters for video frame generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_106\n\nLANGUAGE: python\nCODE:\n```\n# You can use any ControlNet here\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/sd-controlnet-canny\").to('cuda')\n\npipe = DiffusionPipeline.from_pretrained(\n    model_path, controlnet=controlnet, custom_pipeline='fresco_v2v').to('cuda')\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n\ngenerator = torch.manual_seed(0)\nframes = [Image.fromarray(frame) for frame in frames]\n```\n\n----------------------------------------\n\nTITLE: Generating Images with StableDiffusionPipeline v1-5 in Python\nDESCRIPTION: This snippet shows how to load the v1-5 Stable Diffusion checkpoint and generate images using the same prompts and generator as the previous example. It allows for comparison between different model versions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel_ckpt_1_5 = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nsd_pipeline_1_5 = StableDiffusionPipeline.from_pretrained(model_ckpt_1_5, torch_dtype=torch.float16).to(\"cuda\")\n\nimages_1_5 = sd_pipeline_1_5(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n```\n\n----------------------------------------\n\nTITLE: Training Unconditional Image Generation on Naruto Dataset\nDESCRIPTION: Complete command to launch training of a DDPM model with EMA on the Naruto dataset, configuring parameters like resolution, batch size, learning rate, and enabling hub upload.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_unconditional.py \\\n  --dataset_name=\"lambdalabs/naruto-blip-captions\" \\\n  --resolution=64 \\\n  --output_dir=\"ddpm-ema-naruto-64\" \\\n  --train_batch_size=16 \\\n  --num_epochs=100 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-4 \\\n  --lr_warmup_steps=500 \\\n  --mixed_precision=no \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Setting Joint Generation Mode in UniDiffuser\nDESCRIPTION: Shows how to manually set the joint generation mode for UniDiffuser. This approach explicitly configures the pipeline to generate image-text pairs before calling the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unidiffuser.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Equivalent to the above.\npipe.set_joint_mode()\nsample = pipe(num_inference_steps=20, guidance_scale=8.0)\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Stable unCLIP and Karlo Prior\nDESCRIPTION: This snippet demonstrates how to perform text-to-image generation by combining Stable unCLIP with KakaoBrain's Karlo prior model. It loads the necessary models and creates a pipeline that generates images from text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_unclip.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import UnCLIPScheduler, DDPMScheduler, StableUnCLIPPipeline\nfrom diffusers.models import PriorTransformer\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\nprior_model_id = \"kakaobrain/karlo-v1-alpha\"\ndata_type = torch.float16\nprior = PriorTransformer.from_pretrained(prior_model_id, subfolder=\"prior\", torch_dtype=data_type)\n\nprior_text_model_id = \"openai/clip-vit-large-patch14\"\nprior_tokenizer = CLIPTokenizer.from_pretrained(prior_text_model_id)\nprior_text_model = CLIPTextModelWithProjection.from_pretrained(prior_text_model_id, torch_dtype=data_type)\nprior_scheduler = UnCLIPScheduler.from_pretrained(prior_model_id, subfolder=\"prior_scheduler\")\nprior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)\n\nstable_unclip_model_id = \"stabilityai/stable-diffusion-2-1-unclip-small\"\n\npipe = StableUnCLIPPipeline.from_pretrained(\n    stable_unclip_model_id,\n    torch_dtype=data_type,\n    variant=\"fp16\",\n    prior_tokenizer=prior_tokenizer,\n    prior_text_encoder=prior_text_model,\n    prior=prior,\n    prior_scheduler=prior_scheduler,\n)\n\npipe = pipe.to(\"cuda\")\nwave_prompt = \"dramatic wave, the Oceans roar, Strong wave spiral across the oceans as the waves unfurl into roaring crests; perfect wave form; perfect wave shape; dramatic wave shape; wave shape unbelievable; wave; wave shape spectacular\"\n\nimage = pipe(prompt=wave_prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset for Textual Inversion Training\nDESCRIPTION: Python code snippet showing how the custom TextualInversionDataset is instantiated and loaded into a DataLoader for training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataset = TextualInversionDataset(\n    data_root=args.train_data_dir,\n    tokenizer=tokenizer,\n    size=args.resolution,\n    placeholder_token=(\" \".join(tokenizer.convert_ids_to_tokens(placeholder_token_ids))),\n    repeats=args.repeats,\n    learnable_property=args.learnable_property,\n    center_crop=args.center_crop,\n    set=\"train\",\n)\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.dataloader_num_workers\n)\n```\n\n----------------------------------------\n\nTITLE: Applying First Block Cache to FLUX.1-dev Pipeline in Python\nDESCRIPTION: This snippet shows how to apply First Block Cache optimization to the FLUX.1-dev image generation pipeline. It uses the apply_cache_on_pipe function with a residual difference threshold to speed up inference while maintaining image quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/para_attn.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport torch\nfrom diffusers import FluxPipeline\n\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    torch_dtype=torch.bfloat16,\n).to(\"cuda\")\n\nfrom para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\n\napply_cache_on_pipe(pipe, residual_diff_threshold=0.08)\n\n# Enable memory savings\n# pipe.enable_model_cpu_offload()\n# pipe.enable_sequential_cpu_offload()\n\nbegin = time.time()\nimage = pipe(\n    \"A cat holding a sign that says hello world\",\n    num_inference_steps=28,\n).images[0]\nend = time.time()\nprint(f\"Time: {end - begin:.2f}s\")\n\nprint(\"Saving image to flux.png\")\nimage.save(\"flux.png\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Demo Video with Hugging Face Hub\nDESCRIPTION: Downloads a sample video file from Hugging Face Hub using the hf_hub_download utility function. Used as input for video processing pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video_zero.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download\n\nfilename = \"__assets__/pix2pix video/camel.mp4\"\nrepo_id = \"PAIR/Text2Video-Zero\"\nvideo_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n```\n\n----------------------------------------\n\nTITLE: Inverting Latents for DiffEdit Pipeline in Python\nDESCRIPTION: This snippet shows the invert method of the StableDiffusionDiffEditPipeline class, which generates partially inverted latents for the image editing process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/diffedit.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninvert(prompt)\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Control Methods\nDESCRIPTION: A comprehensive table listing different control methods for diffusion models, indicating whether they require training/fine-tuning and including relevant comments about their implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlling_generation.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|                     **Method**                      | **Inference only** | **Requires training /<br> fine-tuning** |                                          **Comments**                                           |\n| :-------------------------------------------------: | :----------------: | :-------------------------------------: | :---------------------------------------------------------------------------------------------: |\n|        [InstructPix2Pix](#instruct-pix2pix)        |         ✅         |                   ❌                    | Can additionally be<br>fine-tuned for better <br>performance on specific <br>edit instructions. |\n|            [Pix2Pix Zero](#pix2pix-zero)            |         ✅         |                   ❌                    |                                                                                                 |\n|       [Attend and Excite](#attend-and-excite)       |         ✅         |                   ❌                    |                                                                                                 |\n|       [Semantic Guidance](#semantic-guidance-sega)       |         ✅         |                   ❌                    |                                                                                                 |\n| [Self-attention Guidance](#self-attention-guidance-sag) |         ✅         |                   ❌                    |                                                                                                 |\n|             [Depth2Image](#depth2image)             |         ✅         |                   ❌                    |                                                                                                 |\n| [MultiDiffusion Panorama](#multidiffusion-panorama) |         ✅         |                   ❌                    |                                                                                                 |\n|              [DreamBooth](#dreambooth)              |         ❌         |                   ✅                    |                                                                                                 |\n|       [Textual Inversion](#textual-inversion)       |         ❌         |                   ✅                    |                                                                                                 |\n|              [ControlNet](#controlnet)              |         ✅         |                   ❌                    |             A ControlNet can be <br>trained/fine-tuned on<br>a custom conditioning.             |\n|        [Prompt Weighting](#prompt-weighting)        |         ✅         |                   ❌                    |                                                                                                 |\n|        [Custom Diffusion](#custom-diffusion)        |         ❌         |                   ✅                    |                                                                                                 |\n|           [Model Editing](#model-editing)           |         ✅         |                   ❌                    |                                                                                                 |\n|                [DiffEdit](#diffedit)                |         ✅         |                   ❌                    |                                                                                                 |\n|             [T2I-Adapter](#t2i-adapter)             |         ✅         |                   ❌                    |                                                                                                 |\n|                [Fabric](#fabric)                    |         ✅         |                   ❌                    |                                                                                                 |\n```\n\n----------------------------------------\n\nTITLE: Loading Lumina2 Model from Single File in Python\nDESCRIPTION: Demonstrates how to load the Lumina2 transformer model from a single checkpoint file and use it for image generation. Uses torch.bfloat16 for efficient memory usage and includes CPU offloading for improved performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/lumina2.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import Lumina2Transformer2DModel, Lumina2Pipeline\n\nckpt_path = \"https://huggingface.co/Alpha-VLLM/Lumina-Image-2.0/blob/main/consolidated.00-of-01.pth\"\ntransformer = Lumina2Transformer2DModel.from_single_file(\n    ckpt_path, torch_dtype=torch.bfloat16\n)\n\npipe = Lumina2Pipeline.from_pretrained(\n    \"Alpha-VLLM/Lumina-Image-2.0\", transformer=transformer, torch_dtype=torch.bfloat16\n)\npipe.enable_model_cpu_offload()\nimage = pipe(\n    \"a cat holding a sign that says hello\",\n    generator=torch.Generator(\"cpu\").manual_seed(0),\n).images[0]\nimage.save(\"lumina-single-file.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving IP-Adapter Image Embeddings in Python\nDESCRIPTION: Demonstrates how to prepare and save IP-Adapter image embeddings to disk using the pipeline's prepare_ip_adapter_image_embeds method. This allows for efficient reuse of embeddings in multiple pipeline runs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimage_embeds = pipeline.prepare_ip_adapter_image_embeds(\n    ip_adapter_image=image,\n    ip_adapter_image_embeds=None,\n    device=\"cuda\",\n    num_images_per_prompt=1,\n    do_classifier_free_guidance=True,\n)\n\ntorch.save(image_embeds, \"image_embeds.ipadpt\")\n```\n\n----------------------------------------\n\nTITLE: Loading AllegroTransformer3DModel from Pretrained Weights in Python\nDESCRIPTION: This snippet demonstrates how to load the AllegroTransformer3DModel from pretrained weights. It imports the model class from diffusers, loads the pretrained model with bfloat16 precision, and moves it to a CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/allegro_transformer3d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AllegroTransformer3DModel\n\ntransformer = AllegroTransformer3DModel.from_pretrained(\"rhymes-ai/Allegro\", subfolder=\"transformer\", torch_dtype=torch.bfloat16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Applying LoRA Weights\nDESCRIPTION: Demonstrates how to load a base model, download LoRA weights, and apply them to generate an image with a specific style or modification.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\n# base model\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"Lykon/dreamshaper-xl-1-0\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\n\n# download LoRA weights\n!wget https://civitai.com/api/download/models/168776 -O blueprintify.safetensors\n\n# load LoRA weights\npipeline.load_lora_weights(\".\", weight_name=\"blueprintify.safetensors\")\nprompt = \"bl3uprint, a highly detailed blueprint of the empire state building, explaining how to build all parts, many txt, blueprint grid backdrop\"\nnegative_prompt = \"lowres, cropped, worst quality, low quality, normal quality, artifacts, signature, watermark, username, blurry, more than one bridge, bad architecture\"\n\nimage = pipeline(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    generator=torch.manual_seed(0),\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Generating Videos with HunyuanVideo\nDESCRIPTION: This snippet demonstrates using the HunyuanVideo model to generate videos from text prompts. The code loads the transformer and pipeline separately, enables VAE tiling to reduce memory usage, and generates a 61-frame video of a cat walking on grass.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\nfrom diffusers.utils import export_to_video\n\ntransformer = HunyuanVideoTransformer3DModel.from_pretrained(\n    \"hunyuanvideo-community/HunyuanVideo\", subfolder=\"transformer\", torch_dtype=torch.bfloat16\n)\npipe = HunyuanVideoPipeline.from_pretrained(\n  \"hunyuanvideo-community/HunyuanVideo\", transformer=transformer, torch_dtype=torch.float16\n)\n\n# reduce memory requirements\npipe.vae.enable_tiling()\npipe.to(\"cuda\")\n\nvideo = pipe(\n    prompt=\"A cat walks on the grass, realistic\",\n    height=320,\n    width=512,\n    num_frames=61,\n    num_inference_steps=30,\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=15)\n```\n\n----------------------------------------\n\nTITLE: Configuring Regional Prompter Parameters in Python\nDESCRIPTION: Demonstrates how to set up the required parameters for the Regional Prompter using the rp_args dictionary. This includes specifying the mode and division of regions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_82\n\nLANGUAGE: python\nCODE:\n```\nrp_args = {\n    \"mode\":\"rows\",\n    \"div\": \"1;1;1\"\n}\n\npipe(prompt=prompt, rp_args=rp_args)\n```\n\n----------------------------------------\n\nTITLE: Generating Mask for DiffEdit Pipeline in Python\nDESCRIPTION: This snippet demonstrates the generate_mask method of the StableDiffusionDiffEditPipeline class, which creates a mask for semantic image editing based on source and target prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/diffedit.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngenerate_mask(source_prompt, target_prompt)\n```\n\n----------------------------------------\n\nTITLE: Applying Inpainting with Kandinsky Model to Text-Generated Image in Python\nDESCRIPTION: This snippet demonstrates how to chain a text-to-image pipeline with inpainting. It loads the Kandinsky 2.2 inpainting model and applies it to a previously generated image, replacing the masked area with a fantasy waterfall.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nprompt = \"digital painting of a fantasy waterfall, cloudy\"\nimage = pipeline(prompt=prompt, image=text2image, mask_image=mask_image).images[0]\nmake_image_grid([text2image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Generation with Kandinsky 2.1\nDESCRIPTION: Performs image-to-image generation using Kandinsky 2.1 model, incorporating the original image, prompts, and embeddings. It also creates a grid to compare the original and generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import make_image_grid\n\nimage = pipeline(prompt, negative_prompt=negative_prompt, image=original_image, image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768, strength=0.3).images[0]\nmake_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multilingual Stable Diffusion Pipeline in Python\nDESCRIPTION: Shows how to implement a Multilingual Stable Diffusion pipeline that can generate images from text in different languages. It uses mBART-50 for translation and combines it with Stable Diffusion for multilingual text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\nimport torch\n\nfrom diffusers import DiffusionPipeline\nfrom transformers import (\n    pipeline,\n    MBart50TokenizerFast,\n    MBartForConditionalGeneration,\n)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice_dict = {\"cuda\": 0, \"cpu\": -1}\n\n# helper function taken from: https://huggingface.co/blog/stable_diffusion\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n# Add language detection pipeline\nlanguage_detection_model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\nlanguage_detection_pipeline = pipeline(\"text-classification\",\n                                       model=language_detection_model_ckpt,\n                                       device=device_dict[device])\n\n# Add model for language translation\ntrans_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\ntrans_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\").to(device)\n\ndiffuser_pipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"multilingual_stable_diffusion\",\n    detection_pipeline=language_detection_pipeline,\n    translation_model=trans_model,\n    translation_tokenizer=trans_tokenizer,\n    torch_dtype=torch.float16,\n)\n\ndiffuser_pipeline.enable_attention_slicing()\ndiffuser_pipeline = diffuser_pipeline.to(device)\n\nprompt = [\"a photograph of an astronaut riding a horse\",\n          \"Una casa en la playa\",\n          \"Ein Hund, der Orange isst\",\n          \"Un restaurant parisien\"]\n\noutput = diffuser_pipeline(prompt)\n\nimages = output.images\n\ngrid = image_grid(images, rows=2, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers and Dependencies for DreamBooth Training\nDESCRIPTION: Commands to install the Diffusers library from GitHub main branch and required dependencies for DreamBooth training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/diffusers\npip install -U -r diffusers/examples/dreambooth/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Compiled Pipeline\nDESCRIPTION: Code to run fast inference with the compiled pipeline, measuring and printing the inference time.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\nprompt = ...\nneg_prompt = ...\nimages = generate(prompt, neg_prompt)\nprint(f\"Inference in {time.time() - start}\")\n```\n\n----------------------------------------\n\nTITLE: Modifying UNet Return Value to Prevent Graph Breaks\nDESCRIPTION: Code diff showing how to modify UNet function calls to prevent graph breaks when using torch.compile with fullgraph=True. This change helps maintain compilation efficiency during inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_7\n\nLANGUAGE: diff\nCODE:\n```\n- latents = unet(\n-   latents, timestep=timestep, encoder_hidden_states=prompt_embeds\n-).sample\n\n+ latents = unet(\n+   latents, timestep=timestep, encoder_hidden_states=prompt_embeds, return_dict=False\n+)[0]\n```\n\n----------------------------------------\n\nTITLE: Importing CogView3PlusPipeline in Python\nDESCRIPTION: This snippet shows how to import the CogView3PlusPipeline class from the diffusers library. The CogView3PlusPipeline is the main class for using the CogView3Plus model for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/cogview3.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import CogView3PlusPipeline\n```\n\n----------------------------------------\n\nTITLE: Training Kandinsky Decoder Model with Accelerate\nDESCRIPTION: Bash command to launch the training script for the Kandinsky decoder model using the Accelerate library. The configuration is similar to the prior model but includes gradient checkpointing for memory efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image_decoder.py \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot naruto, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi2-decoder-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Moving Pipeline to GPU\nDESCRIPTION: This code snippet shows how to move the diffusion pipeline to GPU for faster generation. This is recommended since the model contains approximately 1.4 billion parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Inputs for Stable Diffusion Inference\nDESCRIPTION: Prepares the input prompt, replicates it across devices, and tokenizes it for model input.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A cinematic film still of Morgan Freeman starring as Jimi Hendrix, portrait, 40mm lens, shallow depth of field, close up, split lighting, cinematic\"\nprompt = [prompt] * jax.device_count()\nprompt_ids = pipeline.prepare_inputs(prompt)\nprompt_ids.shape\n```\n\n----------------------------------------\n\nTITLE: Initializing Text2Image Dataset for LCM Training\nDESCRIPTION: Creates a Text2ImageDataset instance for training, configuring shards path, batch sizes, number of workers, and other parameters. The resulting dataloader provides batches of text-image pairs for the training loop.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndataset = Text2ImageDataset(\n    train_shards_path_or_url=args.train_shards_path_or_url,\n    num_train_examples=args.max_train_samples,\n    per_gpu_batch_size=args.train_batch_size,\n    global_batch_size=args.train_batch_size * accelerator.num_processes,\n    num_workers=args.dataloader_num_workers,\n    resolution=args.resolution,\n    shuffle_buffer_size=1000,\n    pin_memory=True,\n    persistent_workers=True,\n)\ntrain_dataloader = dataset.train_dataloader\n```\n\n----------------------------------------\n\nTITLE: Inpainting with Kandinsky 2.1\nDESCRIPTION: Performs inpainting using Kandinsky 2.1 model, incorporating the initial image, mask, prompt, and embeddings. It also creates a grid to compare the original, mask, and inpainted images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a hat\"\nprior_output = prior_pipeline(prompt)\n\noutput_image = pipeline(prompt, image=init_image, mask_image=mask, **prior_output, height=768, width=768, num_inference_steps=150).images[0]\nmask = Image.fromarray((mask*255).astype('uint8'), 'L')\nmake_image_grid([init_image, mask, output_image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Loading Base Model for LoRA Inference\nDESCRIPTION: Python code to load the Stable Diffusion base model for inference before applying LoRA weights. Uses half-precision (float16) for efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/lora.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_base = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n\npipe = StableDiffusionPipeline.from_pretrained(model_base, torch_dtype=torch.float16)\n```\n\n----------------------------------------\n\nTITLE: Encoding Prompt with PixArt-Σ\nDESCRIPTION: Generates text embeddings from a prompt using the initialized pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart_sigma.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    prompt = \"cute cat\"\n    prompt_embeds, prompt_attention_mask, negative_embeds, negative_prompt_attention_mask = pipe.encode_prompt(prompt)\n```\n\n----------------------------------------\n\nTITLE: Increasing Image Resolution in Training Script\nDESCRIPTION: This bash snippet demonstrates how to increase the input image resolution parameter for running the InstructPix2Pix training script. This is crucial for accommodating high-resolution images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_instruct_pix2pix.py \\n  --resolution=512 \\\n```\n\n----------------------------------------\n\nTITLE: Activating Only Up Block for LoRA Adapter\nDESCRIPTION: Configures the pixel-art adapter to only affect the up blocks of the UNet while disabling its effect on down and mid blocks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nadapter_weight_scales = { \"unet\": { \"down\": 0, \"mid\": 0, \"up\": 1} }\npipe.set_adapters(\"pixel\", adapter_weight_scales)\nimage = pipe(prompt, num_inference_steps=30, generator=torch.manual_seed(0)).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Importing SD3IPAdapterMixin in Python\nDESCRIPTION: This snippet shows the import statement for the SD3IPAdapterMixin class from the loaders.ip_adapter module.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/loaders/ip_adapter.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom loaders.ip_adapter import SD3IPAdapterMixin\n```\n\n----------------------------------------\n\nTITLE: Accessing Pipeline Output Attributes\nDESCRIPTION: Shows different ways to access attributes from pipeline outputs, including direct attribute access and dictionary-style lookup. Both methods can be used to retrieve the images from the output object.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/outputs.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noutputs.images\noutputs[\"images\"]\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Test Images\nDESCRIPTION: Code to generate test images using multiple diverse prompts with the Stable Diffusion pipeline\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"a photo of an astronaut riding a horse on mars\",\n    \"A high tech solarpunk utopia in the Amazon rainforest\",\n    \"A pikachu fine dining with a view to the Eiffel Tower\",\n    \"A mecha robot in a favela in expressionist style\",\n    \"an insect robot preparing a delicious meal\",\n    \"A small cabin on top of a snowy mountain in the style of Disney, artstation\",\n]\n\nimages = sd_pipeline(prompts, num_images_per_prompt=1, output_type=\"np\").images\n\nprint(images.shape)\n# (6, 512, 512, 3)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image-to-Image Pipeline for Kandinsky 3\nDESCRIPTION: Loads the Kandinsky 3 image-to-image pipeline directly without needing a prior model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import Kandinsky3Img2ImgPipeline\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = Kandinsky3Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-3\", variant=\"fp16\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Using PAG (Perturbed Attention Guidance) with Stable Diffusion Pipeline in Python\nDESCRIPTION: This code demonstrates how to use the Perturbed Attention Guidance (PAG) with a Stable Diffusion pipeline. It generates images with and without PAG, comparing the results by creating a grid image. The example includes setting up the pipeline, configuring PAG parameters, and generating images with specified settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_110\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\n\nfrom accelerate.utils import set_seed\n\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import load_image, make_image_grid\nfrom diffusers.utils.torch_utils import randn_tensor\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    custom_pipeline=\"hyoungwoncho/sd_perturbed_attention_guidance\",\n    torch_dtype=torch.float16\n)\n\ndevice = \"cuda\"\npipe = pipe.to(device)\n\npag_scale = 5.0\npag_applied_layers_index = ['m0']\n\nbatch_size = 4\nseed = 10\n\nbase_dir = \"./results/\"\ngrid_dir = base_dir + \"/pag\" + str(pag_scale) + \"/\"\n\nif not os.path.exists(grid_dir):\n    os.makedirs(grid_dir)\n\nset_seed(seed)\n\nlatent_input = randn_tensor(shape=(batch_size,4,64,64), generator=None, device=device, dtype=torch.float16)\n\noutput_baseline = pipe(\n    \"\",\n    width=512,\n    height=512,\n    num_inference_steps=50,\n    guidance_scale=0.0,\n    pag_scale=0.0,\n    pag_applied_layers_index=pag_applied_layers_index,\n    num_images_per_prompt=batch_size,\n    latents=latent_input\n).images\n\noutput_pag = pipe(\n    \"\",\n    width=512,\n    height=512,\n    num_inference_steps=50,\n    guidance_scale=0.0,\n    pag_scale=5.0,\n    pag_applied_layers_index=pag_applied_layers_index,\n    num_images_per_prompt=batch_size,\n    latents=latent_input\n).images\n\ngrid_image = make_image_grid(output_baseline + output_pag, rows=2, cols=batch_size)\ngrid_image.save(grid_dir + \"sample.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with IP-Adapter\nDESCRIPTION: Demonstrates image generation using IP-Adapter with both image and text prompts, including seed setting and parameter configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png\")\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\nimages = pipeline(\n    prompt='best quality, high quality, wearing sunglasses',\n    ip_adapter_image=image,\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n    num_inference_steps=50,\n    generator=generator,\n).images[0]\nimages\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Real Images\nDESCRIPTION: Loads real images from the downloaded dataset and converts them to numpy arrays for processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nimport os\nimport numpy as np\n\ndataset_path = \"sample-imagenet-images\"\nimage_paths = sorted([os.path.join(dataset_path, x) for x in os.listdir(dataset_path)])\n\nreal_images = [np.array(Image.open(path).convert(\"RGB\")) for path in image_paths]\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for ControlNet Training\nDESCRIPTION: Commands to initialize an Accelerate environment for distributed training setup.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sd3.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Implementing T2I-Adapter with LCM for Canny Edge-Conditioned Image Generation\nDESCRIPTION: This code demonstrates how to use T2I-Adapter with a Latent Consistency Model (LCM) for fast image generation conditioned on canny edge detection. It loads a canny-trained adapter, applies it to an SDXL pipeline, and replaces the scheduler with LCMScheduler to generate high-quality images in just 4 steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLAdapterPipeline, UNet2DConditionModel, T2IAdapter, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\n# detect the canny map in low resolution to avoid high-frequency details\nimage = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n).resize((384, 384))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1216))\n\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"latent-consistency/lcm-sdxl\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    unet=unet,\n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"the mona lisa, 4k picture, high quality\"\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=5,\n    adapter_conditioning_scale=0.8,\n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Zero1to3 Pipeline Configuration\nDESCRIPTION: Setup and usage of Zero-1-to-3 pipeline for novel view synthesis using pretrained stable diffusion model with batch processing capability.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom pipeline_zero1to3 import Zero1to3StableDiffusionPipeline\nfrom diffusers.utils import load_image\n\nmodel_id = \"kxic/zero123-165000\"  # zero123-105000, zero123-165000, zero123-xl\n\npipe = Zero1to3StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_vae_tiling()\npipe.enable_attention_slicing()\npipe = pipe.to(\"cuda\")\n\nnum_images_per_prompt = 4\n\nquery_pose1 = [-75.0, 100.0, 0.0]\nquery_pose2 = [-20.0, 125.0, 0.0]\nquery_pose3 = [-55.0, 90.0, 0.0]\n\ninput_image1 = load_image(\"./demo/4_blackarm.png\")\ninput_image2 = load_image(\"./demo/8_motor.png\")\ninput_image3 = load_image(\"./demo/7_london.png\")\ninput_images = [input_image1, input_image2, input_image3]\nquery_poses = [query_pose1, query_pose2, query_pose3]\n\nfrom gradio_new import preprocess_image, create_carvekit_interface\nimport numpy as np\nimport PIL.Image as Image\n\npre_images = []\nmodels = dict()\nprint('Instantiating Carvekit HiInterface...')\nmodels['carvekit'] = create_carvekit_interface()\nif not isinstance(input_images, list):\n    input_images = [input_images]\nfor raw_im in input_images:\n    input_im = preprocess_image(models, raw_im, True)\n    H, W = input_im.shape[:2]\n    pre_images.append(Image.fromarray((input_im * 255.0).astype(np.uint8)))\ninput_images = pre_images\n\nimages = pipe(input_imgs=input_images, prompt_imgs=input_images, poses=query_poses, height=H, width=W,\n              guidance_scale=3.0, num_images_per_prompt=num_images_per_prompt, num_inference_steps=50).images\n\nlog_dir = \"logs\"\nos.makedirs(log_dir, exist_ok=True)\nbs = len(input_images)\ni = 0\nfor obj in range(bs):\n    for idx in range(num_images_per_prompt):\n        images[i].save(os.path.join(log_dir,f\"obj{obj}_{idx}.jpg\"))\n        i += 1\n```\n\n----------------------------------------\n\nTITLE: Setting Up SDXL Pipeline with Custom Scheduler\nDESCRIPTION: Initializes the StableDiffusionXLPipeline with a custom scheduler. The model is configured to use float16 precision and moved to CUDA for GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline, UniPCMultistepScheduler\nimport torch\n\npipe = StableDiffusionXLPipeline.from_pretrained(\"Lykon/dreamshaper-xl-1-0\", torch_dtype=torch.float16)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Generating Tiled Image with Mixture of Diffusers using Stable Diffusion\nDESCRIPTION: This snippet demonstrates how to use a pipeline to generate a tiled image using the Mixture of Diffusers technique. It combines multiple prompts to create a composite image with specified dimensions and overlaps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nimage = pipeline(\n    prompt=[[\n        \"A charming house in the countryside, by jakub rozalski, sunset lighting, elegant, highly detailed, smooth, sharp focus, artstation, stunning masterpiece\",\n        \"A dirt road in the countryside crossing pastures, by jakub rozalski, sunset lighting, elegant, highly detailed, smooth, sharp focus, artstation, stunning masterpiece\",\n        \"An old and rusty giant robot lying on a dirt road, by jakub rozalski, dark sunset lighting, elegant, highly detailed, smooth, sharp focus, artstation, stunning masterpiece\"\n    ]],\n    tile_height=640,\n    tile_width=640,\n    tile_row_overlap=0,\n    tile_col_overlap=256,\n    guidance_scale=8,\n    seed=7178915308,\n    num_inference_steps=50,\n)[\"images\"][0]\n```\n\n----------------------------------------\n\nTITLE: Initial Setup for Pytorch Distributed Inference in Python\nDESCRIPTION: Begins the setup for distributed inference using PyTorch Distributed by initializing the necessary libraries and DiffusionPipeline. This code snippet prepares the baseline setup to enable in-depth configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nfrom diffusers import DiffusionPipeline\n\nsd = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Frame Count for Video Generation with StableVideoDiffusionPipeline in Python\nDESCRIPTION: This snippet demonstrates how to adjust the number of frames generated in a video using the StableVideoDiffusionPipeline. It shows the impact of changing the num_frames parameter on the video output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableVideoDiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipeline = StableVideoDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-video-diffusion-img2vid\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\nimage = image.resize((1024, 576))\n\ngenerator = torch.manual_seed(42)\nframes = pipeline(image, decode_chunk_size=8, generator=generator, num_frames=25).frames[0]\nexport_to_video(frames, \"generated.mp4\", fps=7)\n```\n\n----------------------------------------\n\nTITLE: Configuring Timestep Spacing in SDXL\nDESCRIPTION: Example of setting up trailing timestep spacing in a Stable Diffusion XL pipeline for better quality with fewer steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/scheduler_features.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"SG161222/RealVisXL_V4.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, timestep_spacing=\"trailing\")\n\nprompt = \"A cinematic shot of a cute little black cat sitting on a pumpkin at night\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(2487854446)\nimage = pipeline(\n    prompt=prompt,\n    negative_prompt=\"\",\n    generator=generator,\n    num_inference_steps=5,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Pyramid Attention Broadcast (PAB) in Diffusers\nDESCRIPTION: Example of configuring and enabling Pyramid Attention Broadcast caching for the CogVideoX pipeline. PAB speeds up inference by skipping attention computations between successive steps and reusing cached attention states.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/cache.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXPipeline, PyramidAttentionBroadcastConfig\n\npipe = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-5b\", torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n\nconfig = PyramidAttentionBroadcastConfig(\n    spatial_attention_block_skip_range=2,\n    spatial_attention_timestep_skip_range=(100, 800),\n    current_timestep_callback=lambda: pipe.current_timestep,\n)\npipe.transformer.enable_cache(config)\n```\n\n----------------------------------------\n\nTITLE: Initializing Stable Diffusion Pipeline with OpenVINO\nDESCRIPTION: Example of loading a Stable Diffusion model using OpenVINO pipeline, converting it from PyTorch format, and generating an image from a prompt. Includes model export functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionPipeline\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nprompt = \"sailing ship in storm by Rembrandt\"\nimage = pipeline(prompt).images[0]\n\n# Don't forget to save the exported model\npipeline.save_pretrained(\"openvino-sd-v1-5\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Negative Prompts in Stable Diffusion\nDESCRIPTION: Shows how to use negative prompts to steer image generation away from undesired features. Negative prompts can improve image quality by removing poor features or modify content by excluding specific elements.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16\n).to(\"cuda\")\nimage = pipeline(\n\tprompt=\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n\tnegative_prompt=\"ugly, deformed, disfigured, poor details, bad anatomy\",\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Animating Images with AnimateDiff Pipeline in Python\nDESCRIPTION: This code snippet shows how to use the AnimateDiffPipeline with a MotionAdapter to animate an image. It includes steps for loading the adapter and pipeline, configuring the scheduler, and generating animated frames.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\npipeline = AnimateDiffPipeline.from_pretrained(\"emilianJR/epiCRealism\", motion_adapter=adapter, torch_dtype=torch.float16)\nscheduler = DDIMScheduler.from_pretrained(\n    \"emilianJR/epiCRealism\",\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipeline.scheduler = scheduler\n\n# reduce memory requirements\npipeline.enable_vae_slicing()\npipeline.enable_model_cpu_offload()\n\noutput = pipeline(\n    prompt=\"A space rocket with trails of smoke behind it launching into space from the desert, 4k, high resolution\",\n    negative_prompt=\"bad quality, worse quality, low resolution\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=50,\n    generator=torch.Generator(\"cpu\").manual_seed(49),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Non-Deterministic Behavior in DDIMPipeline\nDESCRIPTION: This snippet shows how the DDIMPipeline produces different results on each run due to random sampling operations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMPipeline\nimport numpy as np\n\nddim = DDIMPipeline.from_pretrained( \"google/ddpm-cifar10-32\", use_safetensors=True)\nimage = ddim(num_inference_steps=2, output_type=\"np\").images\nprint(np.abs(image).sum())\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline from Existing Components\nDESCRIPTION: Create a new pipeline instance by passing all extracted components from another pipeline, avoiding loading duplicate model weights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstable_diffusion_img2img = StableDiffusionImg2ImgPipeline(**components)\n```\n\n----------------------------------------\n\nTITLE: Exporting Diffusion Pipeline to DDUF Format\nDESCRIPTION: Shows how to save a diffusion pipeline in the DDUF format using the export_folder_as_dduf utility function from the huggingface_hub library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import export_folder_as_dduf\nfrom diffusers import DiffusionPipeline\nimport torch \n\npipe = DiffusionPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n\nsave_folder = \"flux-dev\"\npipe.save_pretrained(\"flux-dev\")\nexport_folder_as_dduf(\"flux-dev.dduf\", folder_path=save_folder)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for SDXL Turbo in Colab\nDESCRIPTION: This code snippet shows how to install the necessary libraries (diffusers, transformers, accelerate) for using SDXL Turbo in Google Colab.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Colab에서 필요한 라이브러리를 설치하기 위해 주석을 제외하세요\n#!pip install -q diffusers transformers accelerate\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Conditioning Images for ControlNet SDXL (Bash)\nDESCRIPTION: Commands to download test conditioning images used in the training examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\n\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```\n\n----------------------------------------\n\nTITLE: Initializing Custom Text-to-Video Pipeline in Python\nDESCRIPTION: Final step in building a custom community pipeline, which combines all components (custom UNet, text encoder, tokenizer, scheduler, and feature extractor) to initialize a TextToVideoIFPipeline for the Show-1 model. Also prepares the pipeline for GPU acceleration and sets the data type to half precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pipeline_t2v_base_pixel import TextToVideoIFPipeline\nimport torch\n\npipeline = TextToVideoIFPipeline(\n    unet=unet,\n    text_encoder=text_encoder,\n    tokenizer=tokenizer,\n    scheduler=scheduler,\n    feature_extractor=feature_extractor\n)\npipeline = pipeline.to(device=\"cuda\")\npipeline.torch_dtype = torch.float16\n```\n\n----------------------------------------\n\nTITLE: Final Outpainting Generation Function\nDESCRIPTION: Function to generate the final outpainted image with improved quality and enhanced prompt engineering.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/advanced_inference/outpaint.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef generate_outpaint(prompt, negative_prompt, image, mask, seed: int = None):\n    if seed is None:\n        seed = random.randint(0, 2**32 - 1)\n\n    generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n\n    image = pipeline(\n        prompt,\n        negative_prompt=negative_prompt,\n        image=image,\n        mask_image=mask,\n        guidance_scale=10.0,\n        strength=0.8,\n        num_inference_steps=30,\n        generator=generator,\n    ).images[0]\n\n    return image\n\nprompt = \"high quality photo of nike air jordans on a basketball court, highly detailed\"\nnegative_prompt = \"\"\n\nfinal_image = generate_outpaint(prompt, negative_prompt, temp_image, mask_blurred, 7688778)\nx = (1024 - resized_img.width) // 2\ny = (1024 - resized_img.height) // 2\nfinal_image.paste(resized_img, (x, y), resized_img)\nfinal_image\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image-to-Image Pipeline for Kandinsky 2.2\nDESCRIPTION: Initializes the prior pipeline and image-to-image pipeline for Kandinsky 2.2 to perform image-to-image transformations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import KandinskyV22Img2ImgPipeline, KandinskyPriorPipeline\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = KandinskyV22Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Using NF4 Data Type for 4-bit Quantization of Flux Models\nDESCRIPTION: Loading both text encoder and transformer components of a Flux.1 model with 4-bit precision using the NF4 (Normal Float 4) data type. NF4 is optimized for weights initialized from a normal distribution and is recommended for training 4-bit base models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\nfrom transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n\nfrom diffusers import FluxTransformer2DModel\nfrom transformers import T5EncoderModel\n\nquant_config = TransformersBitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\ntext_encoder_2_4bit = T5EncoderModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"text_encoder_2\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\ntransformer_4bit = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading an Image for Conditioning\nDESCRIPTION: Imports the load_image function from diffusers.utils to download and prepare an image for conditioning in image-to-image tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image\n```\n\n----------------------------------------\n\nTITLE: Loading a Community Pipeline from the Main Branch\nDESCRIPTION: This snippet shows how to load a community pipeline from the main branch of the Diffusers repository using the custom_revision parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    custom_pipeline=\"clip_guided_stable_diffusion\",\n    custom_revision=\"main\",\n    clip_model=clip_model,\n    feature_extractor=feature_extractor,\n    use_safetensors=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Development Environment Configuration\nDESCRIPTION: Bash output showing the versions of required dependencies used in the benchmarking environment\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n- `diffusers` version: 0.15.1\n- Python version: 3.8.16\n- PyTorch version (GPU?): 1.13.1+cu116 (True)\n- Huggingface_hub version: 0.13.2\n- Transformers version: 4.27.2\n- Accelerate version: 0.18.0\n- xFormers version: 0.0.16\n- tomesd version: 0.1.2\n```\n\n----------------------------------------\n\nTITLE: Creating Blurred Mask for Inpainting\nDESCRIPTION: Demonstrates how to create a blurred mask using the VaeImageProcessor's blur method, which helps blend the original image and inpaint area.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nfrom PIL import Image\n\npipeline = AutoPipelineForInpainting.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to('cuda')\n\nmask = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/seashore_mask.png\")\nblurred_mask = pipeline.mask_processor.blur(mask, blur_factor=33)\nblurred_mask\n```\n\n----------------------------------------\n\nTITLE: Molecular Position Setting Helper\nDESCRIPTION: Utility functions for setting atomic positions in RDKit molecular objects\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef set_rdmol_positions(rdkit_mol, pos):\n    mol = deepcopy(rdkit_mol)\n    set_rdmol_positions_(mol, pos)\n    return mol\n\ndef set_rdmol_positions_(mol, pos):\n    for i in range(pos.shape[0]):\n        mol.GetConformer(0).SetAtomPosition(i, pos[i].tolist())\n    return mol\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: This command initializes an 🤗 Accelerate environment, allowing for training on multiple GPUs/TPUs or with mixed-precision. It prompts the user for configuration options.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate config\"\n```\n\n----------------------------------------\n\nTITLE: Explicitly Enabling Accelerated Transformers\nDESCRIPTION: Python code showing how to explicitly enable the accelerated transformer implementation in Diffusers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.models.attention_processor import AttnProcessor2_0\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\npipe.unet.set_attn_processor(AttnProcessor2_0())\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Parti Prompts Dataset\nDESCRIPTION: Code to load sample prompts from the Parti Prompts dataset for evaluation purposes\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\n# prompts = load_dataset(\"nateraw/parti-prompts\", split=\"train\")\n# prompts = prompts.shuffle()\n# sample_prompts = [prompts[i][\"Prompt\"] for i in range(5)]\n\n# Fixing these sample prompts in the interest of reproducibility.\nsample_prompts = [\n    \"a corgi\",\n    \"a hot air balloon with a yin-yang symbol, with the moon visible in the daytime sky\",\n    \"a car with no windows\",\n    \"a cube made of porcupine\",\n    'The saying \"BE EXCELLENT TO EACH OTHER\" written on a red brick wall with a graffiti image of a green alien wearing a tuxedo. A yellow fire hydrant is on a sidewalk in the foreground.',\n]\n```\n\n----------------------------------------\n\nTITLE: Using AutoPipelineForText2Image with Kandinsky 2.1\nDESCRIPTION: Demonstrates how to use the AutoPipelineForText2Image class to automatically set up and use the combined Kandinsky 2.1 pipeline for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, guidance_scale=4.0, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading OmniGen Pipeline from Pretrained Model\nDESCRIPTION: Demonstrates how to load the OmniGen pipeline from pretrained checkpoint. The code initializes the model with bfloat16 precision for optimized performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/omnigen.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenPipeline\n\npipe = OmniGenPipeline.from_pretrained(\"Shitao/OmniGen-v1-diffusers\", torch_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Turbo Model from Pretrained Checkpoint\nDESCRIPTION: Loads the SDXL Turbo model from a pretrained checkpoint using the AutoPipelineForText2Image class. It sets the model to use float16 precision and moves it to the CUDA device for GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline = pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Example of JSONL Dataset Format\nDESCRIPTION: Sample structure of a JSONL file that can be used as training data for the ControlNet model, showing the required fields.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n{\"image\": \"xxx\", \"text\": \"xxx\", \"conditioning_image\": \"xxx\"}\n{\"image\": \"xxx\", \"text\": \"xxx\", \"conditioning_image\": \"xxx\"}\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Model in Flax\nDESCRIPTION: Loads the pretrained Stable Diffusion model using FlaxStableDiffusionPipeline with bfloat16 precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndtype = jnp.bfloat16\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    variant=\"bf16\",\n    dtype=dtype,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using HunyuanDiTControlNetPipeline in Python\nDESCRIPTION: This snippet demonstrates how to import and use the HunyuanDiTControlNetPipeline class for conditional image generation with Hunyuan-DiT and ControlNet.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet_hunyuandit.md#2025-04-11_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom diffusers import HunyuanDiTControlNetPipeline\n\n# Initialize the pipeline\npipeline = HunyuanDiTControlNetPipeline.from_pretrained(\"path/to/pretrained/model\")\n\n# Generate image with control\noutput = pipeline(\n    prompt=\"A scenic landscape\",\n    control_image=control_image,\n    num_inference_steps=50\n)\n```\n\n----------------------------------------\n\nTITLE: Applying fp8 Quantization and First Block Cache to FLUX.1-dev Pipeline in Python\nDESCRIPTION: This snippet shows how to apply both fp8 quantization and First Block Cache to the FLUX.1-dev image generation pipeline. It uses dynamic quantization, torch.compile, and a modified residual difference threshold for optimal performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/para_attn.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport torch\nfrom diffusers import FluxPipeline\n\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    torch_dtype=torch.bfloat16,\n).to(\"cuda\")\n\nfrom para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\n\napply_cache_on_pipe(\n    pipe,\n    residual_diff_threshold=0.12,  # Use a larger value to make the cache take effect\n)\n\nfrom torchao.quantization import quantize_, float8_dynamic_activation_float8_weight, float8_weight_only\n\nquantize_(pipe.text_encoder, float8_weight_only())\nquantize_(pipe.transformer, float8_dynamic_activation_float8_weight())\npipe.transformer = torch.compile(\n   pipe.transformer, mode=\"max-autotune-no-cudagraphs\",\n)\n\n# Enable memory savings\n# pipe.enable_model_cpu_offload()\n# pipe.enable_sequential_cpu_offload()\n\nfor i in range(2):\n    begin = time.time()\n    image = pipe(\n        \"A cat holding a sign that says hello world\",\n        num_inference_steps=28,\n    ).images[0]\n    end = time.time()\n    if i == 0:\n        print(f\"Warm up time: {end - begin:.2f}s\")\n    else:\n        print(f\"Time: {end - begin:.2f}s\")\n\nprint(\"Saving image to flux.png\")\nimage.save(\"flux.png\")\n```\n\n----------------------------------------\n\nTITLE: Converting NumPy Arrays to PIL Images in Python\nDESCRIPTION: Function to convert numpy arrays to PIL Image objects, handling different array shapes and formats. Supports both single images and batched arrays, with automatic handling of channel ordering and data type conversion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_93\n\nLANGUAGE: python\nCODE:\n```\ndef array_to_pil(array):\n    if array.dtype != np.uint8:\n        if array.max() <= 1.0:\n            array = (array * 255).astype(np.uint8)\n        else:\n            array = array.astype(np.uint8)\n    \n    if len(array.shape) == 3:\n        if array.shape[0] == 3:\n            array = array.transpose(1, 2, 0)\n        return Image.fromarray(array)\n    elif len(array.shape) == 4:\n        array = array[0]\n        if array.shape[0] == 3:\n            array = array.transpose(1, 2, 0)\n        return Image.fromarray(array)\n    else:\n        raise ValueError(f\"Unexpected array shape: {array.shape}\")\n```\n\n----------------------------------------\n\nTITLE: Running Full DreamBooth Training with FLUX.1\nDESCRIPTION: Bash command for launching full DreamBooth training on the FLUX.1 model using the dog dataset. Includes environment variable setup and comprehensive training parameters for optimization and monitoring.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_flux.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"black-forest-labs/FLUX.1-dev\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"trained-flux\"\n\naccelerate launch train_dreambooth_flux.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"bf16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --guidance_scale=1 \\\n  --gradient_accumulation_steps=4 \\\n  --optimizer=\"prodigy\" \\\n  --learning_rate=1. \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Video-to-Video Generation with Wan 2.1\nDESCRIPTION: Shows how to use the WanVideoToVideoPipeline for video-to-video generation. It loads the required models, sets up the pipeline with a custom scheduler, and generates a new video based on an input video and text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers.utils import load_video, export_to_video\nfrom diffusers import AutoencoderKLWan, WanVideoToVideoPipeline, UniPCMultistepScheduler\n\nmodel_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\nvae = AutoencoderKLWan.from_pretrained(\n    model_id, subfolder=\"vae\", torch_dtype=torch.float32\n)\npipe = WanVideoToVideoPipeline.from_pretrained(\n    model_id, vae=vae, torch_dtype=torch.bfloat16\n)\nflow_shift = 3.0  # 5.0 for 720P, 3.0 for 480P\npipe.scheduler = UniPCMultistepScheduler.from_config(\n    pipe.scheduler.config, flow_shift=flow_shift\n)\npipe.enable_model_cpu_offload()\n\nprompt = \"A robot standing on a mountain top. The sun is setting in the background\"\nnegative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\nvideo = load_video(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hiker.mp4\"\n)\noutput = pipe(\n    video=video,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=480,\n    width=512,\n    guidance_scale=7.0,\n    strength=0.7,\n).frames[0]\n\nexport_to_video(output, \"wan-v2v.mp4\", fps=16)\n```\n\n----------------------------------------\n\nTITLE: Processing a Generated Image with Kandinsky Image-to-Image Pipeline\nDESCRIPTION: This code demonstrates the second part of the text-to-image-to-image chaining process, where an image generated by Stable Diffusion is passed to Kandinsky's image-to-image pipeline for further processing and style transformation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nimage2image = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", image=text2image).images[0]\nmake_image_grid([text2image, image2image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Turbo from Single File Checkpoint\nDESCRIPTION: This snippet shows an alternative method to load the SDXL Turbo model from a single file checkpoint (.ckpt or .safetensors) using the StableDiffusionXLPipeline class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\", torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Scaled Dot Product Attention in Diffusion Pipeline\nDESCRIPTION: The Python script sets up a diffusion pipeline using a scaled dot product attention mechanism for optimized performance in PyTorch 2.0. The script includes modifications to explicitly use `AttnProcessor2_0` with dependency on PyTorch and 🤗 Diffusers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.models.attention_processor import AttnProcessor2_0\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipe.unet.set_attn_processor(AttnProcessor2_0())\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Applying fp8 Quantization and First Block Cache to HunyuanVideo Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to apply both fp8 quantization and First Block Cache to the HunyuanVideo generation pipeline. It uses dynamic quantization and torch.compile for improved performance in video generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/para_attn.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport torch\nfrom diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\nfrom diffusers.utils import export_to_video\n\nmodel_id = \"tencent/HunyuanVideo\"\ntransformer = HunyuanVideoTransformer3DModel.from_pretrained(\n    model_id,\n    subfolder=\"transformer\",\n    torch_dtype=torch.bfloat16,\n    revision=\"refs/pr/18\",\n)\npipe = HunyuanVideoPipeline.from_pretrained(\n    model_id,\n    transformer=transformer,\n    torch_dtype=torch.float16,\n    revision=\"refs/pr/18\",\n).to(\"cuda\")\n\nfrom para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\n\napply_cache_on_pipe(pipe)\n\nfrom torchao.quantization import quantize_, float8_dynamic_activation_float8_weight, float8_weight_only\n\nquantize_(pipe.text_encoder, float8_weight_only())\nquantize_(pipe.transformer, float8_dynamic_activation_float8_weight())\npipe.transformer = torch.compile(\n   pipe.transformer, mode=\"max-autotune-no-cudagraphs\",\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with PAG\nDESCRIPTION: Shows how to generate images using PAG with different guidance scales. Demonstrates comparison between PAG enabled and disabled generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"an insect robot preparing a delicious meal, anime style\"\n\nfor pag_scale in [0.0, 3.0]:\n    generator = torch.Generator(device=\"cpu\").manual_seed(0)\n    images = pipeline(\n        prompt=prompt,\n        num_inference_steps=25,\n        guidance_scale=7.0,\n        generator=generator,\n        pag_scale=pag_scale,\n    ).images\n```\n\n----------------------------------------\n\nTITLE: Using Negative Prompts in Inpainting Pipeline\nDESCRIPTION: This snippet demonstrates the use of negative prompts in the inpainting pipeline. Negative prompts guide the model away from generating certain elements, useful for improving image quality and preventing unwanted features.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nnegative_prompt = \"bad architecture, unstable, poor details, blurry\"\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask_image).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Loading a Quantized Model with Manual State Dict Loading\nDESCRIPTION: Shows how to load a quantized model by manually loading the state dict, which is necessary for some quantization methods on older PyTorch versions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/torchao.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom accelerate import init_empty_weights\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, TorchAoConfig\n\n# Serialize the model\ntransformer = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/Flux.1-Dev\",\n    subfolder=\"transformer\",\n    quantization_config=TorchAoConfig(\"uint4wo\"),\n    torch_dtype=torch.bfloat16,\n)\ntransformer.save_pretrained(\"/path/to/flux_uint4wo\", safe_serialization=False, max_shard_size=\"50GB\")\n# ...\n\n# Load the model\nstate_dict = torch.load(\"/path/to/flux_uint4wo/diffusion_pytorch_model.bin\", weights_only=False, map_location=\"cpu\")\nwith init_empty_weights():\n    transformer = FluxTransformer2DModel.from_config(\"/path/to/flux_uint4wo/config.json\")\ntransformer.load_state_dict(state_dict, strict=True, assign=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Accelerate Configuration Programmatically\nDESCRIPTION: Python code to programmatically create a basic Accelerate configuration, useful in non-interactive environments like notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Rescaled Guidance using DDIM Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to generate an image using a DDIM pipeline with rescaled classifier-free guidance to prevent over-exposure.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/ddim.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimage = pipe(prompt, guidance_rescale=0.7).images[0]\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Different methods to configure the Accelerate environment for distributed training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Commands to initialize and configure an Accelerate environment for distributed training setup.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training with Text Encoder and UNet in PyTorch\nDESCRIPTION: Command to fine-tune both the text encoder and UNet for improved results, especially for facial images. Requires at least 24GB VRAM and uses 8-bit Adam optimizer to reduce memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --train_text_encoder \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --use_8bit_adam \\\n  --gradient_checkpointing \\\n  --learning_rate=2e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Implementing UnCLIP Image Interpolation Pipeline with Kakaobrain's Image Variations Model\nDESCRIPTION: This code shows how to use the UnCLIP image interpolation pipeline to create a smooth transition between two input images. The pipeline loads the kakaobrain/karlo-v1-alpha-image-variations model and performs spherical interpolation between image embeddings over multiple steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\ndevice = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\ndtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16\n\npipe = DiffusionPipeline.from_pretrained(\n    \"kakaobrain/karlo-v1-alpha-image-variations\",\n    torch_dtype=dtype,\n    custom_pipeline=\"unclip_image_interpolation\"\n)\npipe.to(device)\n\n# List of image URLs\nimage_urls = [\n    'https://camo.githubusercontent.com/ef13c8059b12947c0d5e8d3ea88900de6bf1cd76bbf61ace3928e824c491290e/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f4e616761536169416268696e61792f556e434c4950496d616765496e746572706f6c6174696f6e53616d706c65732f7265736f6c76652f6d61696e2f7374617272795f6e696768742e6a7067',\n    'https://camo.githubusercontent.com/d1947ab7c49ae3f550c28409d5e8b120df48e456559cf4557306c0848337702c/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f4e616761536169416268696e61792f556e434c4950496d616765496e746572706f6c6174696f6e53616d706c65732f7265736f6c76652f6d61696e2f666c6f776572732e6a7067'\n]\n\n# Open images from URLs\nimages = []\nfor url in image_urls:\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    images.append(img)\n\n# For best results keep the prompts close in length to each other. Of course, feel free to try out with differing lengths.\ngenerator = torch.Generator(device=device).manual_seed(42)\n\noutput = pipe(image=images, steps=6, generator=generator)\n\nfor i, image in enumerate(output.images):\n    image.save('starry_to_flowers_%s.jpg' % i)\n```\n\n----------------------------------------\n\nTITLE: Using Embedding in ComfyUI Prompt\nDESCRIPTION: Shows the syntax for including a textual embedding in a ComfyUI prompt. The 'embedding:' prefix is used to specify the embedding file name without the extension.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nembedding:y2k_emb\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Kandinsky 2.2\nDESCRIPTION: Implementation of text-to-image generation using Kandinsky 2.2, which differs from 2.1 by not accepting a prompt during the decoding stage, instead relying on image embeddings from the prior model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\nimport torch\n\nprior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16).to(\"cuda\")\npipeline = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\" # optional to include a negative prompt, but results are usually better\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, guidance_scale=1.0).to_tuple()\n```\n\nLANGUAGE: python\nCODE:\n```\nimage = pipeline(image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Initializing Image Encoder\nDESCRIPTION: Code to initialize and load the EfficientNet image encoder model for Wuerstchen.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/wuerstchen.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n    pretrained_checkpoint_file = hf_hub_download(\"dome272/wuerstchen\", filename=\"model_v2_stage_b.pt\")\n    state_dict = torch.load(pretrained_checkpoint_file, map_location=\"cpu\")\n    image_encoder = EfficientNetEncoder()\n    image_encoder.load_state_dict(state_dict[\"effnet_state_dict\"])\n    image_encoder.eval()\n```\n\n----------------------------------------\n\nTITLE: Implementing FasterCache in Diffusers\nDESCRIPTION: Example of configuring and enabling FasterCache for the CogVideoX pipeline. FasterCache optimizes inference by reusing attention states and skipping unconditional branch predictions in classifier-free guidance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/cache.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXPipeline, FasterCacheConfig\n\npipe = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-5b\", torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n\nconfig = FasterCacheConfig(\n    spatial_attention_block_skip_range=2,\n    spatial_attention_timestep_skip_range=(-1, 681),\n    current_timestep_callback=lambda: pipe.current_timestep,\n    attention_weight_callback=lambda _: 0.3,\n    unconditional_batch_skip_range=5,\n    unconditional_batch_timestep_skip_range=(-1, 781),\n    tensor_format=\"BFCHW\",\n)\npipe.transformer.enable_cache(config)\n```\n\n----------------------------------------\n\nTITLE: Full Denoising Loop Implementation\nDESCRIPTION: Implements the complete diffusion denoising process, iterating through timesteps to gradually transform random noise into a coherent image of a cat.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> import tqdm\n\n>>> sample = noisy_sample\n\n>>> for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n...     # 1. predict noise residual\n...     with torch.no_grad():\n...         residual = model(sample, t).sample\n\n...     # 2. compute less noisy image and set x_t -> x_t-1\n...     sample = scheduler.step(residual, t, sample).prev_sample\n\n...     # 3. optionally look at image\n...     if (i + 1) % 50 == 0:\n...         display_sample(sample, i + 1)\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Hub Login\nDESCRIPTION: Command to log in to the Hugging Face Hub, which is required when using the --push_to_hub flag to directly store the trained models on the Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/wuerstchen/text_to_image/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: LCM-LoRA Training with WebDataset\nDESCRIPTION: Training script for LCM-LoRA using CC12M dataset, enabling efficient training with lower memory requirements while maintaining model quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path/to/saved/model\"\n\naccelerate launch train_lcm_distill_lora_sdxl_wds.py \\\n    --pretrained_teacher_model=$MODEL_DIR \\\n    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\n    --output_dir=$OUTPUT_DIR \\\n    --mixed_precision=fp16 \\\n    --resolution=1024 \\\n    --lora_rank=64 \\\n    --learning_rate=1e-4 --loss_type=\"huber\" --use_fix_crop_and_size --adam_weight_decay=0.0 \\\n    --max_train_steps=1000 \\\n    --max_train_samples=4000000 \\\n    --dataloader_num_workers=8 \\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\n    --validation_steps=200 \\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\n    --train_batch_size=12 \\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n    --gradient_accumulation_steps=1 \\\n    --use_8bit_adam \\\n    --resume_from_checkpoint=latest \\\n    --report_to=wandb \\\n    --seed=453645634 \\\n    --push_to_hub \\\n```\n\n----------------------------------------\n\nTITLE: Loading Components for DreamBooth Training\nDESCRIPTION: Load tokenizer, scheduler, and models essential for initiating and running the DreamBooth training loop. Proper loading of these components is critical for the training process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_8\n\nLANGUAGE: py\nCODE:\n```\n# Load the tokenizer\nif args.tokenizer_name:\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\nelif args.pretrained_model_name_or_path:\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"tokenizer\",\n        revision=args.revision,\n        use_fast=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating a Larger Batch with Attention Slicing in Python\nDESCRIPTION: Generates a batch of 8 images using attention slicing to optimize memory usage. This demonstrates how to scale up generation with limited resources.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimages = pipeline(**get_inputs(batch_size=8)).images\nimage_grid(images, rows=2, cols=4)\n```\n\n----------------------------------------\n\nTITLE: Sharing Custom Pipeline to Hugging Face Hub in Python\nDESCRIPTION: Code to upload a custom community pipeline to the Hugging Face Hub for sharing with the community. This allows others to use the custom pipeline implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npipeline.push_to_hub(\"custom-t2v-pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-grounded Diffusion for Layout Generation in Python\nDESCRIPTION: This code snippet shows how to use the LLM-grounded Diffusion pipeline for layout generation without LLM integration. It demonstrates how to initialize the pipeline and generate images based on text prompts and predefined layout information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\n    \"longlian/lmd_plus\",\n    custom_pipeline=\"llm_grounded_diffusion\",\n    variant=\"fp16\", torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\n# Generate an image described by the prompt and\n# insert objects described by text at the region defined by bounding boxes\nprompt = \"a waterfall and a modern high speed train in a beautiful forest with fall foliage\"\nboxes = [[0.1387, 0.2051, 0.4277, 0.7090], [0.4980, 0.4355, 0.8516, 0.7266]]\nphrases = [\"a waterfall\", \"a modern high speed train\"]\n\nimages = pipe(\n    prompt=prompt,\n    phrases=phrases,\n    boxes=boxes,\n    gligen_scheduled_sampling_beta=0.4,\n    output_type=\"pil\",\n    num_inference_steps=50,\n    lmd_guidance_kwargs={}\n).images\n\nimages[0].save(\"./lmd_plus_generation.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Using Negative Size Conditioning with SDXL in Python\nDESCRIPTION: This code demonstrates how to use negative size conditioning in Stable Diffusion XL to steer generation away from certain image resolutions. The example initializes the SDXL pipeline and generates an image with negative conditions for original and target sizes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(\n    prompt=prompt,\n    negative_original_size=(512, 512),\n    negative_target_size=(1024, 1024),\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Dance Diffusion Component Documentation\nDESCRIPTION: Markdown documentation structure for Dance Diffusion pipeline components including autodoc references\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/dance_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## DanceDiffusionPipeline\n[[autodoc]] DanceDiffusionPipeline\n\t- all\n\t- __call__\n\n## AudioPipelineOutput\n[[autodoc]] pipelines.AudioPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Loading a DreamBooth Fine-tuned Model with DiffusionPipeline\nDESCRIPTION: Demonstrates how to load a pre-trained DreamBooth model from the Hugging Face Hub using DiffusionPipeline and configure it with a custom scheduler. The example uses the 'dndcoverart-v1' model with half-precision floating point (float16) for better performance on CUDA devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, UniPCMultistepScheduler\n\npipe = DiffusionPipeline.from_pretrained(\"sd-dreambooth-library/dndcoverart-v1\", torch_dtype=torch.float16).to(\"cuda\")\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: Processing Videos with Temporal Consistency in Marigold Depth Pipeline\nDESCRIPTION: This code snippet implements a solution for frame-by-frame video processing with temporal consistency using the Marigold depth pipeline. It maintains consistency between frames by using a convex combination of latents from the current and previous frames.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport imageio\nimport diffusers\nimport torch\nfrom diffusers.models.attention_processor import AttnProcessor2_0\nfrom PIL import Image\nfrom tqdm import tqdm\n\ndevice = \"cuda\"\npath_in = \"https://huggingface.co/spaces/prs-eth/marigold-lcm/resolve/c7adb5427947d2680944f898cd91d386bf0d4924/files/video/obama.mp4\"\npath_out = \"obama_depth.gif\"\n\npipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n    \"prs-eth/marigold-depth-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(device)\npipe.vae = diffusers.AutoencoderTiny.from_pretrained(\n    \"madebyollin/taesd\", torch_dtype=torch.float16\n).to(device)\npipe.unet.set_attn_processor(AttnProcessor2_0())\npipe.vae = torch.compile(pipe.vae, mode=\"reduce-overhead\", fullgraph=True)\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\npipe.set_progress_bar_config(disable=True)\n\nwith imageio.get_reader(path_in) as reader:\n    size = reader.get_meta_data()['size']\n    last_frame_latent = None\n    latent_common = torch.randn(\n        (1, 4, 768 * size[1] // (8 * max(size)), 768 * size[0] // (8 * max(size)))\n    ).to(device=device, dtype=torch.float16)\n\n    out = []\n    for frame_id, frame in tqdm(enumerate(reader), desc=\"Processing Video\"):\n        frame = Image.fromarray(frame)\n        latents = latent_common\n        if last_frame_latent is not None:\n            latents = 0.9 * latents + 0.1 * last_frame_latent\n\n        depth = pipe(\n            frame,\n            num_inference_steps=1,\n            match_input_resolution=False, \n            latents=latents, \n            output_latent=True,\n        )\n        last_frame_latent = depth.latent\n        out.append(pipe.image_processor.visualize_depth(depth.prediction)[0])\n\n    diffusers.utils.export_to_gif(out, path_out, fps=reader.get_meta_data()['fps'])\n```\n\n----------------------------------------\n\nTITLE: Using Tiny AutoEncoder with Stable Diffusion XL 1.0 in Python\nDESCRIPTION: This snippet shows how to integrate the Tiny AutoEncoder (TAESD) with a Stable Diffusion XL 1.0 pipeline. It loads the SDXL pipeline, replaces the VAE with TAESD, and generates an image from a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoder_tiny.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, AutoencoderTiny\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n)\npipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesdxl\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"slice of delicious New York-style berry cheesecake\"\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implementing DemoFusion for High-Resolution Image Generation\nDESCRIPTION: This code demonstrates DemoFusion, a method for high-resolution image generation without extensive computational resources. It uses tiled processing with various parameters like view_batch_size, stride, and cosine scales to control the generation process. This implementation allows generating images up to 3072x3072 pixels.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_90\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    custom_pipeline=\"pipeline_demofusion_sdxl\",\n    custom_revision=\"main\",\n    torch_dtype=torch.float16,\n)\npipe = pipe.to(\"cuda\")\n\nprompt = \"Envision a portrait of an elderly woman, her face a canvas of time, framed by a headscarf with muted tones of rust and cream. Her eyes, blue like faded denim. Her attire, simple yet dignified.\"\nnegative_prompt = \"blurry, ugly, duplicate, poorly drawn, deformed, mosaic\"\n\nimages = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    height=3072,\n    width=3072,\n    view_batch_size=16,\n    stride=64,\n    num_inference_steps=50,\n    guidance_scale=7.5,\n    cosine_scale_1=3,\n    cosine_scale_2=1,\n    cosine_scale_3=1,\n    sigma=0.8,\n    multi_decoder=True,\n    show_image=True\n)\n```\n\n----------------------------------------\n\nTITLE: Computing Embeddings and Latents for ControlNet Flux in Python\nDESCRIPTION: Function to precompute text embeddings and latent representations for images to optimize training of ControlNet Flux models. It processes text prompts, encodes images with VAE, and prepares latent image IDs for the model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef compute_embeddings(batch, proportion_empty_prompts, vae, flux_controlnet_pipeline, weight_dtype, is_train=True):\n    \n    ### compute text embeddings\n    prompt_batch = batch[args.caption_column]\n    captions = []\n    for caption in prompt_batch:\n        if random.random() < proportion_empty_prompts:\n            captions.append(\"\")\n        elif isinstance(caption, str):\n            captions.append(caption)\n        elif isinstance(caption, (list, np.ndarray)):\n            # take a random caption if there are multiple\n            captions.append(random.choice(caption) if is_train else caption[0])\n    prompt_batch = captions\n    prompt_embeds, pooled_prompt_embeds, text_ids = flux_controlnet_pipeline.encode_prompt(\n        prompt_batch, prompt_2=prompt_batch\n    )\n    prompt_embeds = prompt_embeds.to(dtype=weight_dtype)\n    pooled_prompt_embeds = pooled_prompt_embeds.to(dtype=weight_dtype)\n    text_ids = text_ids.to(dtype=weight_dtype)\n\n    # text_ids [512,3] to [bs,512,3]\n    text_ids = text_ids.unsqueeze(0).expand(prompt_embeds.shape[0], -1, -1)\n\n    ### compute latents\n    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n        latents = latents.permute(0, 2, 4, 1, 3, 5)\n        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n        return latents\n\n    # vae encode\n    pixel_values = batch[\"pixel_values\"]\n    pixel_values = torch.stack([image for image in pixel_values]).to(dtype=weight_dtype).to(vae.device)\n    pixel_latents_tmp = vae.encode(pixel_values).latent_dist.sample()\n    pixel_latents_tmp = (pixel_latents_tmp - vae.config.shift_factor) * vae.config.scaling_factor\n    pixel_latents = _pack_latents(\n        pixel_latents_tmp,\n        pixel_values.shape[0],\n        pixel_latents_tmp.shape[1],\n        pixel_latents_tmp.shape[2],\n        pixel_latents_tmp.shape[3],\n    ) \n\n    control_values = batch[\"conditioning_pixel_values\"]\n    control_values = torch.stack([image for image in control_values]).to(dtype=weight_dtype).to(vae.device)\n    control_latents = vae.encode(control_values).latent_dist.sample()\n    control_latents = (control_latents - vae.config.shift_factor) * vae.config.scaling_factor\n    control_latents = _pack_latents(\n        control_latents,\n        control_values.shape[0],\n        control_latents.shape[1],\n        control_latents.shape[2],\n        control_latents.shape[3],\n    )\n\n    # copied from pipeline_flux_controlnet\n    def _prepare_latent_image_ids(batch_size, height, width, device, dtype):\n        latent_image_ids = torch.zeros(height // 2, width // 2, 3)\n        latent_image_ids[..., 1] = latent_image_ids[..., 1] + torch.arange(height // 2)[:, None]\n        latent_image_ids[..., 2] = latent_image_ids[..., 2] + torch.arange(width // 2)[None, :]\n\n        latent_image_id_height, latent_image_id_width, latent_image_id_channels = latent_image_ids.shape\n\n        latent_image_ids = latent_image_ids[None, :].repeat(batch_size, 1, 1, 1)\n        latent_image_ids = latent_image_ids.reshape(\n            batch_size, latent_image_id_height * latent_image_id_width, latent_image_id_channels\n        )\n\n        return latent_image_ids.to(device=device, dtype=dtype)\n    latent_image_ids = _prepare_latent_image_ids(\n        batch_size=pixel_latents_tmp.shape[0],\n        height=pixel_latents_tmp.shape[2],\n        width=pixel_latents_tmp.shape[3],\n        device=pixel_values.device,\n        dtype=pixel_values.dtype,\n    )\n\n    # unet_added_cond_kwargs = {\"pooled_prompt_embeds\": pooled_prompt_embeds, \"text_ids\": text_ids}\n    return {\"prompt_embeds\": prompt_embeds, \"pooled_prompt_embeds\": pooled_prompt_embeds, \"text_ids\": text_ids, \"pixel_latents\": pixel_latents, \"control_latents\": control_latents, \"latent_image_ids\": latent_image_ids}\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Diffusion Pipeline with Python\nDESCRIPTION: This code snippet demonstrates how to load a custom diffusion pipeline using the DiffusionPipeline class. You can specify the custom pipeline file to be loaded from the community folder, enabling the user to extend pipeline functionality easily.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", custom_pipeline=\"filename_in_the_community_folder\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with LTX Video 0.9.1 Weights in Python\nDESCRIPTION: Shows how to load LTX Video 0.9.1 weights and generate a video from a text prompt using the Diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ltx_video.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import LTXPipeline\nfrom diffusers.utils import export_to_video\n\npipe = LTXPipeline.from_pretrained(\"a-r-r-o-w/LTX-Video-0.9.1-diffusers\", torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n\nprompt = \"A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage\"\nnegative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\n\nvideo = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=768,\n    height=512,\n    num_frames=161,\n    decode_timestep=0.03,\n    decode_noise_scale=0.025,\n    num_inference_steps=50,\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=24)\n```\n\n----------------------------------------\n\nTITLE: Configuring Inpainting Pipeline with PAG\nDESCRIPTION: Sets up and demonstrates using PAG with an inpainting pipeline, including mask image handling and strength configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\ninit_image = load_image(img_url).convert(\"RGB\")\nmask_image = load_image(mask_url).convert(\"RGB\")\n\nprompt = \"A majestic tiger sitting on a bench\"\n\npag_scales =  3.0\nguidance_scales = 7.5\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(1)\nimages = pipeline(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    strength=0.8,\n    num_inference_steps=50,\n    guidance_scale=guidance_scale,\n    generator=generator,\n    pag_scale=pag_scale,\n).images\nimages[0]\n```\n\n----------------------------------------\n\nTITLE: Importing DDIMInverseScheduler in Python\nDESCRIPTION: This snippet shows how to import the DDIMInverseScheduler class from the Hugging Face Diffusers library. The scheduler is used for inverting the diffusion process in guided diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/ddim_inverse.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMInverseScheduler\n```\n\n----------------------------------------\n\nTITLE: Reducing Memory Usage for SVD Inference\nDESCRIPTION: Code modification to decrease GPU memory consumption by enabling model CPU offloading, forward chunking, and reducing decode chunk size. This approach makes SVD usable on GPUs with limited VRAM (less than 8GB).\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/svd.md#2025-04-11_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n- pipe.enable_model_cpu_offload()\n- frames = pipe(image, decode_chunk_size=8, generator=generator).frames[0]\n+ pipe.enable_model_cpu_offload()\n+ pipe.unet.enable_forward_chunking()\n+ frames = pipe(image, decode_chunk_size=2, generator=generator, num_frames=25).frames[0]\n```\n\n----------------------------------------\n\nTITLE: Loading FLUX.1 DEV Transformer Model with GGUF Quantization\nDESCRIPTION: Demonstrates loading a GGUF quantized model checkpoint using Diffusers FluxTransformer2DModel with dynamic dequantization and specific compute dtype\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/gguf.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\n\nckpt_path = (\n    \"https://huggingface.co/city96/FLUX.1-dev-gguf/blob/main/flux1-dev-Q2_K.gguf\"\n)\ntransformer = FluxTransformer2DModel.from_single_file(\n    ckpt_path,\n    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n    torch_dtype=torch.bfloat16,\n)\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    transformer=transformer,\n    torch_dtype=torch.bfloat16,\n)\npipe.enable_model_cpu_offload()\nprompt = \"A cat holding a sign that says hello world\"\nimage = pipe(prompt, generator=torch.manual_seed(0)).images[0]\nimage.save(\"flux-gguf.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating the Edit Mask Based on Text Prompts\nDESCRIPTION: Using the pipeline to automatically generate a mask identifying areas to edit based on the difference between source and target prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\nsource_prompt = \"a bowl of fruits\"\ntarget_prompt = \"a basket of pears\"\nmask_image = pipeline.generate_mask(\n    image=raw_image,\n    source_prompt=source_prompt,\n    target_prompt=target_prompt,\n)\nImage.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\").resize((768, 768))\n```\n\n----------------------------------------\n\nTITLE: Generating Image Batch with Controlled Seeds\nDESCRIPTION: Creates multiple images using different seeds (0-3) by initializing separate Generator objects. This approach ensures each image in the batch has a unique but reproducible seed, allowing for deterministic generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ngenerator = [torch.Generator(device=\"cuda\").manual_seed(i) for i in range(4)]\nprompt = \"Labrador in the style of Vermeer\"\nimages = pipeline(prompt, generator=generator, num_images_per_prompt=4).images[0]\nmake_image_grid(images, rows=2, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Install the Diffusers library from source to access the latest features and scripts, including those for DreamBooth. Required dependencies must be properly set up for running the training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for Training\nDESCRIPTION: Commands to initialize an Accelerate environment for distributed training. Provides options for interactive configuration, default setup, or configuration in non-interactive environments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Importing LEditsPPPipelineStableDiffusionXL in Python\nDESCRIPTION: This snippet demonstrates how to import the LEditsPPPipelineStableDiffusionXL class, which is the implementation of LEDITS++ for Stable Diffusion XL models. It includes all methods and the __call__ and invert functions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ledits_pp.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.ledits_pp import LEditsPPPipelineStableDiffusionXL\n```\n\n----------------------------------------\n\nTITLE: Making a cURL Request to the Diffusers Server\nDESCRIPTION: Example of how to make an HTTP POST request to the server to generate an image based on a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/server/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H \"Content-Type: application/json\" --data '{\"model\": \"something\", \"prompt\": \"a kitten in front of a fireplace\"}' http://localhost:8000/v1/images/generations\n```\n\n----------------------------------------\n\nTITLE: Loading Single-File Checkpoint Model\nDESCRIPTION: Shows how to load a diffusion model from a single checkpoint file using the StableDiffusionPipeline.from_single_file method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_single_file(\n    \"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/blob/main/v1-5-pruned.ckpt\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Diffusers Server\nDESCRIPTION: Commands to install the server package and its requirements. This prepares the environment for running the Diffusers server.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/server/README.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install .\npip install -f requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Checking Compatible Schedulers for a Pipeline\nDESCRIPTION: Check which scheduler classes are compatible with the current pipeline to potentially replace the default scheduler with one that offers better speed or quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id)\nstable_diffusion.scheduler.compatibles\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion Interpolation Pipeline\nDESCRIPTION: Implements interpolation between different prompts in Stable Diffusion. Requires 8GB VRAM and generates interpolated frames between different prompts that can be used for video creation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    variant='fp16',\n    torch_dtype=torch.float16,\n    safety_checker=None,  # Very important for videos...lots of false positives while interpolating\n    custom_pipeline=\"interpolate_stable_diffusion\",\n).to('cuda')\npipe.enable_attention_slicing()\n\nframe_filepaths = pipe.walk(\n    prompts=['a dog', 'a cat', 'a horse'],\n    seeds=[42, 1337, 1234],\n    num_interpolation_steps=16,\n    output_dir='./dreams',\n    batch_size=4,\n    height=512,\n    width=512,\n    guidance_scale=8.5,\n    num_inference_steps=50,\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Fused Model and Unloading LoRA Weights\nDESCRIPTION: Shows how to unload LoRA weights after fusion and save the resulting model either locally or to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline.unload_lora_weights()\n# save locally\npipeline.save_pretrained(\"path/to/fused-pipeline\")\n# save to the Hub\npipeline.push_to_hub(\"fused-ikea-feng\")\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with the Pipeline in Python\nDESCRIPTION: Executes the diffusion pipeline to generate an image based on the prompt and generator. Returns the first image from the output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Prompt and Bounding Box Configuration\nDESCRIPTION: Sets up the image generation prompt and configures bounding boxes for object placement with normalized coordinates.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/demo.ipynb#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n\n# prompt = 'A realistic image of landscape scene depicting a green car parking on the left of a blue truck, with a red air balloon and a bird in the sky'\n# gen_boxes = [('a green car', [21, 281, 211, 159]), ('a blue truck', [269, 283, 209, 160]), ('a red air balloon', [66, 8, 145, 135]), ('a bird', [296, 42, 143, 100])]\n\n# prompt = 'A realistic top-down view of a wooden table with two apples on it'\n# gen_boxes = [('a wooden table', [20, 148, 472, 216]), ('an apple', [150, 226, 100, 100]), ('an apple', [280, 226, 100, 100])]\n\n# prompt = 'A realistic scene of three skiers standing in a line on the snow near a palm tree'\n# gen_boxes = [('a skier', [5, 152, 139, 168]), ('a skier', [278, 192, 121, 158]), ('a skier', [148, 173, 124, 155]), ('a palm tree', [404, 105, 103, 251])]\n\nprompt = \"An oil painting of a pink dolphin jumping on the left of a steam boat on the sea\"\ngen_boxes = [(\"a steam boat\", [232, 225, 257, 149]), (\"a jumping pink dolphin\", [21, 249, 189, 123])]\n\nboxes = np.array([x[1] for x in gen_boxes])\nboxes = boxes / 512\nboxes[:, 2] = boxes[:, 0] + boxes[:, 2]\nboxes[:, 3] = boxes[:, 1] + boxes[:, 3]\nboxes = boxes.tolist()\ngligen_phrases = [x[0] for x in gen_boxes]\n```\n\n----------------------------------------\n\nTITLE: Importing StableAudioDiTModel in Python\nDESCRIPTION: This code snippet demonstrates how to import the StableAudioDiTModel class. The model is a Transformer for audio waveforms from the Stable Audio Open project.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/stable_audio_transformer.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableAudioDiTModel\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Text-Guided Generation with Stable Diffusion in Python\nDESCRIPTION: This code shows how to use StableDiffusionImg2ImgPipeline for image-to-image generation guided by text. It loads the pipeline, downloads an initial image, and generates a new image based on the input image and a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/README.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\n# load the pipeline\ndevice = \"cuda\"\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n).to(device)\n\n# let's download an initial image\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((768, 512))\n\nprompt = \"A fantasy landscape, trending on artstation\"\n\nimages = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n\nimages[0].save(\"fantasy_landscape.png\")\n```\n\n----------------------------------------\n\nTITLE: Inference with Custom Diffusion - Multiple Concepts\nDESCRIPTION: Python code for running inference on multiple concepts using a trained Custom Diffusion model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom huggingface_hub.repocard import RepoCard\nfrom diffusers import DiffusionPipeline\n\nmodel_id = \"sayakpaul/custom-diffusion-cat-wooden-pot\"\ncard = RepoCard.load(model_id)\nbase_model_id = card.data.to_dict()[\"base_model\"]\n\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16).to(\n\"cuda\")\npipe.unet.load_attn_procs(model_id, weight_name=\"pytorch_custom_diffusion_weights.bin\")\npipe.load_textual_inversion(model_id, weight_name=\"<new1>.bin\")\npipe.load_textual_inversion(model_id, weight_name=\"<new2>.bin\")\n\nimage = pipe(\n    \"the <new1> cat sculpture in the style of a <new2> wooden pot\",\n    num_inference_steps=100,\n    guidance_scale=6.0,\n    eta=1.0,\n).images[0]\nimage.save(\"multi-subject.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing VQDiffusionScheduler in Python\nDESCRIPTION: This code snippet shows how to import the VQDiffusionScheduler class from the diffusers library. It is inferred from the context of the documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/vq_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import VQDiffusionScheduler\n```\n\n----------------------------------------\n\nTITLE: Full Prior Model Fine-Tuning\nDESCRIPTION: Complete command for fine-tuning the Würstchen prior model with gradient checkpointing for memory efficiency. This example uses the Naruto BLIP captions dataset and includes parameters for batch size, learning rate, and validation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/wuerstchen/text_to_image/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch  train_text_to_image_prior.py \\\n  --mixed_precision=\"fp16\" \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=4 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --dataloader_num_workers=4 \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot naruto, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"wuerstchen-prior-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Compiling UNet in Diffusion Pipeline for Performance Boost\nDESCRIPTION: This Python example illustrates using `torch.compile` to optimize the UNet component in a diffusion pipeline, potentially providing substantial speed-ups by reducing execution overhead.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\nimages = pipe(prompt, num_inference_steps=steps, num_images_per_prompt=batch_size).images[0]\n```\n\n----------------------------------------\n\nTITLE: Training Custom Diffusion for Multiple Concepts\nDESCRIPTION: Training command for learning multiple concepts simultaneously (cat and wooden pot) using a JSON file with concept information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_custom_diffusion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --output_dir=$OUTPUT_DIR \\\n  --concepts_list=./concept_list.json \\\n  --with_prior_preservation --real_prior --prior_loss_weight=1.0 \\\n  --resolution=512  \\\n  --train_batch_size=2  \\\n  --learning_rate=1e-5  \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --num_class_images=200 \\\n  --scale_lr --hflip  \\\n  --modifier_token \"<new1>+<new2>\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Instaflow Single-Step Image Generation Pipeline\nDESCRIPTION: Implementation of fast image generation using Instaflow pipeline with optional LORA support. Uses Rectified Flow technique for efficient one-step inference while maintaining quality close to Stable Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_94\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"XCLIU/instaflow_0_9B_from_sd_1_5\", torch_dtype=torch.float16, custom_pipeline=\"instaflow_one_step\")\npipe.to(\"cuda\")\nprompt = \"A hyper-realistic photo of a cute cat.\"\n\nimages = pipe(prompt=prompt,\n            num_inference_steps=1,\n            guidance_scale=0.0).images\nimages[0].save(\"./image.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Weights for DeepFloyd IF and Updating Scheduler\nDESCRIPTION: This snippet demonstrates how to load a pretrained DeepFloyd IF model, apply LoRA weights, and update the scheduler configuration to use fixed variance which is required after training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", use_safetensors=True)\n\npipe.load_lora_weights(\"<lora weights path>\")\n\n# Update scheduler config to fixed variance schedule\npipe.scheduler = pipe.scheduler.__class__.from_config(pipe.scheduler.config, variance_type=\"fixed_small\")\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA from TheLastBen Repository\nDESCRIPTION: This snippet demonstrates how to load a LoRA checkpoint from TheLastBen's repository. It shows loading the William Eggleston Style SDXL checkpoint and using it with a prompt that includes the trigger words 'william eggleston' to activate the style transfer effect.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"TheLastBen/William_Eggleston_Style_SDXL\", weight_name=\"wegg.safetensors\")\n\n# LoRA를 트리거하기 위해 william eggleston를 프롬프트에 사용\nprompt = \"a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\"\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Importing SdeVeOutput in Python\nDESCRIPTION: This code snippet shows how to import the SdeVeOutput class. This class is likely used to represent the output of the ScoreSdeVeScheduler operations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/score_sde_ve.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers.scheduling_sde_ve import SdeVeOutput\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Kandinsky 2.1\nDESCRIPTION: Code for generating an image from text using Kandinsky 2.1, which involves creating a prior pipeline to generate image embeddings and then using the main pipeline to create the final image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyPriorPipeline, KandinskyPipeline\nimport torch\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16).to(\"cuda\")\npipeline = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\" # optional to include a negative prompt, but results are usually better\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt, guidance_scale=1.0).to_tuple()\n```\n\nLANGUAGE: python\nCODE:\n```\nimage = pipeline(prompt, image_embeds=image_embeds, negative_prompt=negative_prompt, negative_image_embeds=negative_image_embeds, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Converting Between Different DeepFloyd IF Pipeline Types in Python\nDESCRIPTION: Shows how to convert between different DeepFloyd IF pipeline types by reusing components. This allows transforming between text-to-image, image-to-image, and inpainting pipelines while preserving the loaded model weights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import IFPipeline, IFSuperResolutionPipeline\n\npipe_1 = IFPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\")\npipe_2 = IFSuperResolutionPipeline.from_pretrained(\"DeepFloyd/IF-II-L-v1.0\")\n\n\nfrom diffusers import IFImg2ImgPipeline, IFImg2ImgSuperResolutionPipeline\n\npipe_1 = IFImg2ImgPipeline(**pipe_1.components)\npipe_2 = IFImg2ImgSuperResolutionPipeline(**pipe_2.components)\n\n\nfrom diffusers import IFInpaintingPipeline, IFInpaintingSuperResolutionPipeline\n\npipe_1 = IFInpaintingPipeline(**pipe_1.components)\npipe_2 = IFInpaintingSuperResolutionPipeline(**pipe_2.components)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Inference with torch.compile\nDESCRIPTION: Code snippet demonstrating how to optimize inference speed using torch.compile, which can provide a 2-3x speedup in generation time.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wuerstchen.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprior_pipeline.prior = torch.compile(prior_pipeline.prior, mode=\"reduce-overhead\", fullgraph=True)\ndecoder_pipeline.decoder = torch.compile(decoder_pipeline.decoder, mode=\"reduce-overhead\", fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image-to-Image Pipeline for Kandinsky 2.1\nDESCRIPTION: Initializes the prior pipeline and image-to-image pipeline for Kandinsky 2.1 to perform image-to-image transformations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = KandinskyImg2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Diffusers\nDESCRIPTION: Specifies the required Python packages and their version constraints for the Diffusers project. Contains core dependencies like diffusers, accelerate, transformers, and supporting libraries for machine learning and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/dreambooth_inpaint/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndiffusers==0.9.0\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.21.0\nftfy\ntensorboard\nJinja2\n```\n\n----------------------------------------\n\nTITLE: Inspecting Input Channels with Stable Diffusion Pipeline in Python\nDESCRIPTION: Demonstrates how to load a pretrained Stable Diffusion text-to-image pipeline and inspect the number of input channels. This is essential for understanding the model's initial configuration before adaptation for inpainting.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/adapt_a_model.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\npipeline.unet.config[\"in_channels\"]\n4\n```\n\n----------------------------------------\n\nTITLE: Generating and Exporting 3D Mesh\nDESCRIPTION: Shows how to generate a 3D mesh output and export it to different file formats (PLY, GLB) with optional rotation transformation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import ShapEPipeline\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(device)\n\nguidance_scale = 15.0\nprompt = \"A birthday cupcake\"\n\nimages = pipe(prompt, guidance_scale=guidance_scale, num_inference_steps=64, frame_size=256, output_type=\"mesh\").images\n```\n\n----------------------------------------\n\nTITLE: Loading Dual Tokenizers and Text Encoders for SDXL in Python\nDESCRIPTION: This code loads the two tokenizers and text encoders required by SDXL's architecture. It retrieves these components from the pretrained model specified in the arguments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntokenizer_one = AutoTokenizer.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision, use_fast=False\n)\ntokenizer_two = AutoTokenizer.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"tokenizer_2\", revision=args.revision, use_fast=False\n)\n\ntext_encoder_cls_one = import_model_class_from_model_name_or_path(\n    args.pretrained_model_name_or_path, args.revision\n)\ntext_encoder_cls_two = import_model_class_from_model_name_or_path(\n    args.pretrained_model_name_or_path, args.revision, subfolder=\"text_encoder_2\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running Textual Inversion Training with PyTorch\nDESCRIPTION: This code snippet demonstrates how to launch a textual inversion training run using the `textual_inversion.py` script. It sets environment variables for the model name and data directory, and then uses `accelerate launch` to run the training script with various hyperparameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport DATA_DIR=\"path-to-dir-containing-images\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"textual_inversion_cat\"\n```\n\n----------------------------------------\n\nTITLE: Applying FreeU to Zeroscope Text-to-Video Model in Python\nDESCRIPTION: This code demonstrates the application of FreeU to the Zeroscope text-to-video model. It initializes the pipeline, enables FreeU with specific parameters, and generates a video of a teddy bear surfing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/image_quality.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16\n).to(\"cuda\")\n# values come from https://github.com/lyn-rgb/FreeU_Diffusers#video-pipelines\npipeline.enable_freeu(b1=1.2, b2=1.4, s1=0.9, s2=0.2)\nprompt = \"Confident teddy bear surfer rides the wave in the tropics\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(47)\nvideo_frames = pipeline(prompt, generator=generator).frames[0]\nexport_to_video(video_frames, \"teddy_bear.mp4\", fps=10)\n```\n\n----------------------------------------\n\nTITLE: Text Variation with UniDiffuser Using Round-Trip Generation\nDESCRIPTION: Demonstrates text variation using UniDiffuser through a two-step process: first generating an image from text, then generating new text from that image. This creates text that is semantically similar to the original prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unidiffuser.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Text variation can be performed with a text-to-image generation followed by a image-to-text generation:\n# 1. Text-to-image generation\nprompt = \"an elephant under the sea\"\n\nsample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)\nt2i_image = sample.images[0]\nt2i_image.save(\"unidiffuser_text2img_sample_image.png\")\n\n# 2. Image-to-text generation\nsample = pipe(image=t2i_image, num_inference_steps=20, guidance_scale=8.0)\nfinal_prompt = sample.text[0]\nprint(final_prompt)\n```\n\n----------------------------------------\n\nTITLE: Reusing Pipeline Components and Switching UNet\nDESCRIPTION: Shows how to reuse components from one pipeline in another, specifically switching the UNet component. This demonstrates the flexibility of the multi-folder layout in mixing and matching model components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\n\n# download one model\nsdxl_pipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n).to(\"cuda\")\n\n# switch UNet for another model\nunet = UNet2DConditionModel.from_pretrained(\n    \"stabilityai/sdxl-turbo\",\n    subfolder=\"unet\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True\n)\n# reuse all the same components in new model except for the UNet\nturbo_pipeline = StableDiffusionXLPipeline.from_pipe(\n    sdxl_pipeline, unet=unet,\n).to(\"cuda\")\nturbo_pipeline.scheduler = EulerDiscreteScheduler.from_config(\n    turbo_pipeline.scheduler.config,\n    timestep+spacing=\"trailing\"\n)\nimage = turbo_pipeline(\n    \"an astronaut riding a unicorn on mars\",\n    num_inference_steps=1,\n    guidance_scale=0.0,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Executing PyTorch Distributed Inference Script\nDESCRIPTION: Runs the Python script with torchrun for distributed inference, specifying the total number of processes (GPUs) to run using the nproc_per_node argument. Requires a setup with the torch distributed environment and the diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ntorchrun run_distributed.py --nproc_per_node=2\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Function for Diffusion Model\nDESCRIPTION: Defines an evaluation function that generates sample images using the diffusion model pipeline, creates an image grid from the results, and saves it to disk for monitoring training progress.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDPMPipeline\nfrom diffusers.utils import make_image_grid\nimport os\n\ndef evaluate(config, epoch, pipeline):\n    # Sample some images from random noise (this is the backward diffusion process).\n    # The default pipeline output type is `List[PIL.Image]`\n    images = pipeline(\n        batch_size=config.eval_batch_size,\n        generator=torch.Generator(device='cpu').manual_seed(config.seed), # Use a separate torch generator to avoid rewinding the random state of the main training loop\n    ).images\n\n    # Make a grid out of the images\n    image_grid = make_image_grid(images, rows=4, cols=4)\n\n    # Save the images\n    test_dir = os.path.join(config.output_dir, \"samples\")\n    os.makedirs(test_dir, exist_ok=True)\n    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n```\n\n----------------------------------------\n\nTITLE: Autodoc syntax for pipeline documentation\nDESCRIPTION: Markdown syntax using the autodoc feature to document pipeline classes, which automatically includes all public methods and specifically adds the __call__ method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n[[autodoc]] XXXPipeline\n    - all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Applying On-the-fly Dataset Transformations\nDESCRIPTION: Defines a transform function and applies it to the dataset using set_transform, which efficiently processes images only when they are requested during training rather than preprocessing the entire dataset at once.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> def transform(examples):\n...     images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n...     return {\"images\": images}\n\n\n>>> dataset.set_transform(transform)\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Weighted Prompt\nDESCRIPTION: Processes a weighted prompt using Compel and generates an image using the modified prompt embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/weighted_prompts.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a red cat playing with a ball++\"\nprompt_embeds = compel_proc(prompt)\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\n\nimages = pipe(prompt_embeds=prompt_embeds, generator=generator, num_inference_steps=20).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face CLI for FLUX.1 Model Access\nDESCRIPTION: Command to log in to Hugging Face using the CLI to access the gated FLUX.1 [dev] model. This authentication is required before using the model with diffusers and for pushing trained models to the Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_flux.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image-to-Image Pipeline with Kandinsky 2.1\nDESCRIPTION: Code for initializing the Kandinsky 2.1 pipelines for image-to-image generation, which allows conditioning on both text and an initial image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = KandinskyImg2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Diffusion Pipeline with Converted KerasCV Model\nDESCRIPTION: Code to load a diffusion pipeline using a model that was converted from KerasCV format to Diffusers format.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\"sayakpaul/textual-inversion-cat-kerascv_sd_diffusers_pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Prompt and Moving Pipeline to GPU\nDESCRIPTION: Defines the generation prompt and moves the pipeline to CUDA device for faster processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/stable_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"portrait photo of a old warrior chief\"\npipeline = pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Launching ControlNet Training with Min-SNR Weighting\nDESCRIPTION: This bash script launches the ControlNet training script with Min-SNR weighting enabled using the `accelerate launch` command. The `--snr_gamma` parameter sets the SNR gamma value to 5.0, which is the recommended value.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate launch train_controlnet.py \\\n  --snr_gamma=5.0\"\n```\n\n----------------------------------------\n\nTITLE: Setting up DDPM denoising process in Python\nDESCRIPTION: Prepares for the denoising process by setting timesteps and creating initial noise.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> scheduler.set_timesteps(50)\n>>> import torch\n\n>>> sample_size = model.config.sample_size\n>>> noise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face CLI\nDESCRIPTION: This snippet demonstrates how to authenticate using the Hugging Face CLI. This step is necessary for storing trained LoRA embeddings directly on the Hugging Face Hub. The user must have valid credentials for successful authentication.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/lora/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Loading the Smithsonian Butterflies Dataset with Hugging Face Datasets\nDESCRIPTION: Loads the Smithsonian Butterflies subset dataset from the Hugging Face Hub using the datasets library. This provides the training data for the diffusion model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> config.dataset_name = \"huggan/smithsonian_butterflies_subset\"\n>>> dataset = load_dataset(config.dataset_name, split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: HuggingFace Hub Authentication for CogVideoX Model Uploads\nDESCRIPTION: Command for logging into the HuggingFace Hub to enable pushing trained models with model cards after LoRA finetuning is complete.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Full Fine-tuning Inference Script\nDESCRIPTION: Python script for inference using fully fine-tuned Flux Control model\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/flux-control/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers import FluxControlPipeline, FluxTransformer2DModel\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nimport torch \n\ntransformer = FluxTransformer2DModel.from_pretrained(\"...\") # change this.\npipe = FluxControlPipeline.from_pretrained(\n  \"black-forest-labs/FLUX.1-dev\",  transformer=transformer, torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\nopen_pose = OpenposeDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n# prepare pose condition.\nurl = \"https://huggingface.co/Adapter/t2iadapter/resolve/main/people.jpg\"\nimage = load_image(url)\nimage = open_pose(image, detect_resolution=512, image_resolution=1024)\nimage = np.array(image)[:, :, ::-1]           \nimage = Image.fromarray(np.uint8(image))\n\nprompt = \"A couple, 4k photo, highly detailed\"\n\ngen_images = pipe(\n  prompt=prompt,\n  control_image=image,\n  num_inference_steps=50,\n  guidance_scale=25., \n).images[0]\ngen_images.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Performing DiffEdit with Generated Embeddings\nDESCRIPTION: Modifying the DiffEdit workflow to use the automatically generated text embeddings instead of manual prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMInverseScheduler, DDIMScheduler\nfrom diffusers.utils import load_image, make_image_grid\nfrom PIL import Image\n\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\npipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n\nimg_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\nraw_image = load_image(img_url).resize((768, 768))\n\nmask_image = pipeline.generate_mask(\n    image=raw_image,\n    source_prompt_embeds=source_embeds,\n    target_prompt_embeds=target_embeds,\n)\n\ninv_latents = pipeline.invert(\n    prompt_embeds=source_embeds,\n    image=raw_image,\n).latents\n\noutput_image = pipeline(\n    mask_image=mask_image,\n    image_latents=inv_latents,\n    prompt_embeds=target_embeds,\n    negative_prompt_embeds=source_embeds,\n).images[0]\nmask_image = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\")\nmake_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Stable Diffusion Image-to-Image Pipeline\nDESCRIPTION: This Python script provides benchmarking for the Stable Diffusion image-to-image pipeline, showcasing the impact of `torch.compile` on performance gains when turning sketches into styled images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom diffusers.utils import load_image\nimport torch\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\ninit_image = load_image(url)\ninit_image = init_image.resize((512, 512))\n\npath = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: Model Inference Code\nDESCRIPTION: Python code for running inference with a trained InstructPix2Pix model to edit images based on text instructions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/instructpix2pix.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\n\nmodel_id = \"your_model_id\"  # <- 이를 수정하세요.\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\nurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\"\n\n\ndef download_image(url):\n    image = PIL.Image.open(requests.get(url, stream=True).raw)\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image\n\n\nimage = download_image(url)\nprompt = \"wipe out the lake\"\nnum_inference_steps = 20\nimage_guidance_scale = 1.5\nguidance_scale = 10\n\nedited_image = pipe(\n    prompt,\n    image=image,\n    num_inference_steps=num_inference_steps,\n    image_guidance_scale=image_guidance_scale,\n    guidance_scale=guidance_scale,\n    generator=generator,\n).images[0]\nedited_image.save(\"edited_image.png\")\n```\n\n----------------------------------------\n\nTITLE: Initializing AnimateLCM with PyTorch and Diffusers\nDESCRIPTION: This snippet initializes the AnimateLCM motion module by loading the motion adapter and configuring the AnimateDiffPipeline with associated LoRA weights. It sets various parameters like VAE slicing and CPU offloading to optimize resource usage. The prompt generates a 4K animated video of a rocket launch with specified quality enhancements.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffPipeline, LCMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"wangfuyun/AnimateLCM\")\npipe = AnimateDiffPipeline.from_pretrained(\"emilianJR/epiCRealism\", motion_adapter=adapter)\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule=\"linear\")\n\npipe.load_lora_weights(\"wangfuyun/AnimateLCM\", weight_name=\"sd15_lora_beta.safetensors\", adapter_name=\"lcm-lora\")\n\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\noutput = pipe(\n    prompt=\"A space rocket with trails of smoke behind it launching into space from the desert, 4k, high resolution\",\n    negative_prompt=\"bad quality, worse quality, low resolution\",\n    num_frames=16,\n    guidance_scale=1.5,\n    num_inference_steps=6,\n    generator=torch.Generator(\"cpu\").manual_seed(0),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animatelcm.gif\")\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion Pipeline Setup and UNet Tracing with Torch JIT\nDESCRIPTION: This code snippet initializes the Stable Diffusion pipeline, moves it to the CUDA device, and prepares the UNet model for tracing. It sets the UNet to evaluation mode, configures it to use channels_last memory format, and uses `torch.jit.trace` to convert the UNet model into a TorchScript representation for optimized inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n).to(\"cuda\")\nunet = pipe.unet\nunet.eval()\nunet.to(memory_format=torch.channels_last)  # use channels_last memory format\nunet.forward = functools.partial(unet.forward, return_dict=False)  # set return_dict=False as default\n\n# warmup\nfor _ in range(3):\n    with torch.inference_mode():\n        inputs = generate_inputs()\n        orig_output = unet(*inputs)\n\n# trace\nprint(\"tracing..\")\nunet_traced = torch.jit.trace(unet, inputs)\nunet_traced.eval()\nprint(\"done tracing\")\n\n\n# warmup and optimize graph\nfor _ in range(5):\n    with torch.inference_mode():\n        inputs = generate_inputs()\n        orig_output = unet_traced(*inputs)\n\n\n# benchmarking\nwith torch.inference_mode():\n    for _ in range(n_experiments):\n        torch.cuda.synchronize()\n        start_time = time.time()\n        for _ in range(unet_runs_per_experiment):\n            orig_output = unet_traced(*inputs)\n        torch.cuda.synchronize()\n        print(f\"unet traced inference took {time.time() - start_time:.2f} seconds\")\n    for _ in range(n_experiments):\n        torch.cuda.synchronize()\n        start_time = time.time()\n        for _ in range(unet_runs_per_experiment):\n            orig_output = unet(*inputs)\n        torch.cuda.synchronize()\n        print(f\"unet inference took {time.time() - start_time:.2f} seconds\")\n\n# save the model\nunet_traced.save(\"unet_traced.pt\")\n```\n\n----------------------------------------\n\nTITLE: Creating a New Branch - Git Bash\nDESCRIPTION: Creates a new branch to hold development changes, allowing for isolated work on feature modifications without affecting the main branch.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout -b a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Performing Depth-to-Image Transformation with Stable Diffusion 2\nDESCRIPTION: This code shows how to use Stable Diffusion 2's depth-to-image model to transform an existing image while preserving its structural depth information. It loads an input image, initializes the depth-to-image pipeline, and generates a new image with the specified prompt while maintaining the original depth structure.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_2.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionDepth2ImgPipeline\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-depth\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\ninit_image = load_image(url)\nprompt = \"two tigers\"\nnegative_prompt = \"bad, deformed, ugly, bad anotomy\"\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=negative_prompt, strength=0.7).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Enhancing Precision with Ensembling in Marigold Normal Maps\nDESCRIPTION: This code snippet demonstrates how to improve the precision of Marigold normal map predictions by enabling the ensembling mechanism, which combines multiple predictions from different random latents.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_7\n\nLANGUAGE: diff\nCODE:\n```\n  import diffusers\n\n  pipe = diffusers.MarigoldNormalsPipeline.from_pretrained(\"prs-eth/marigold-normals-v1-1\").to(\"cuda\")\n\n  image = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\n- depth = pipe(image)\n+ depth = pipe(image, num_inference_steps=10, ensemble_size=5)\n\n  vis = pipe.image_processor.visualize_normals(depth.prediction)\n  vis[0].save(\"einstein_normals.png\")\n```\n\n----------------------------------------\n\nTITLE: Creating Inverted Latents from Original Image\nDESCRIPTION: Inverting the original image into latent space using DDIM inversion with the source prompt as guidance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninv_latents = pipeline.invert(prompt=source_prompt, image=raw_image).latents\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for bitsandbytes Quantization\nDESCRIPTION: Installs the necessary libraries (diffusers, transformers, accelerate, bitsandbytes) for quantizing models using bitsandbytes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install diffusers transformers accelerate bitsandbytes -U\n```\n\n----------------------------------------\n\nTITLE: Modifying IP-Adapter for Style-Only Transfer\nDESCRIPTION: Reconfigures the IP-Adapter to affect only the style-related layers of the diffusion model. This allows for generating images that adopt the visual style of the reference image while having more flexibility in content and layout.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nscale = {\n    \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n}\npipeline.set_ip_adapter_scale(scale)\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(26)\nimage = pipeline(\n    prompt=\"a cat, masterpiece, best quality, high quality\",\n    ip_adapter_image=style_image,\n    negative_prompt=\"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n    guidance_scale=5,\n    num_inference_steps=30,\n    generator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Enabling Attention Slicing for Memory Optimization in Python\nDESCRIPTION: Activates the attention slicing feature to reduce memory consumption during generation. This allows processing larger batches on the same GPU.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipeline.enable_attention_slicing()\n```\n\n----------------------------------------\n\nTITLE: Using Wildcard Stable Diffusion Pipeline in Python\nDESCRIPTION: Demonstrates how to use Wildcard Stable Diffusion pipeline to randomly sample values for placeholders in prompts. The code loads the pipeline, defines a prompt with wildcards, and provides replacement options through files and dictionaries.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"wildcard_stable_diffusion\",\n    torch_dtype=torch.float16,\n)\nprompt = \"__animal__ sitting on a __object__ wearing a __clothing__\"\nout = pipe(\n    prompt,\n    wildcard_option_dict={\n        \"clothing\":[\"hat\", \"shirt\", \"scarf\", \"beret\"]\n    },\n    wildcard_files=[\"object.txt\", \"animal.txt\"],\n    num_prompt_samples=1\n)\nout.images[0].save(\"image.png\")\ntorch.cuda.empty_cache()\n```\n\n----------------------------------------\n\nTITLE: Implementing Sliced VAE with StableDiffusionPipeline\nDESCRIPTION: Demonstrates how to use sliced VAE to decode large batches of images with limited VRAM. This approach processes latents one image at a time, enabling batch processing of 32+ images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\npipe.enable_vae_slicing()\n#pipe.enable_xformers_memory_efficient_attention()\nimages = pipe([prompt] * 32).images\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion Mixture Tiling Pipeline for SDXL\nDESCRIPTION: This snippet demonstrates the setup and usage of a Stable Diffusion Mixture Tiling Pipeline for SDXL. It includes loading a fixed VAE, creating a scheduler and model, and generating a tiled image using multiple prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler, AutoencoderKL\n\ndevice=\"cuda\"\n\n# Load fixed vae (optional)\nvae = AutoencoderKL.from_pretrained(\n    \"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16\n).to(device)\n\n# Create scheduler and model (similar to StableDiffusionPipeline)\nmodel_id=\"stablediffusionapi/yamermix-v8-vae\"\nscheduler = DPMSolverMultistepScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\npipe = DiffusionPipeline.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    vae=vae,\n    custom_pipeline=\"mixture_tiling_sdxl\",\n    scheduler=scheduler,\n    use_safetensors=False    \n).to(device)\n\npipe.enable_model_cpu_offload()\npipe.enable_vae_tiling()\npipe.enable_vae_slicing()\n\ngenerator = torch.Generator(device).manual_seed(297984183)\n\n# Mixture of Diffusers generation\nimage = pipe(\n    prompt=[[\n        \"A charming house in the countryside, by jakub rozalski, sunset lighting, elegant, highly detailed, smooth, sharp focus, artstation, stunning masterpiece\",\n        \"A dirt road in the countryside crossing pastures, by jakub rozalski, sunset lighting, elegant, highly detailed, smooth, sharp focus, artstation, stunning masterpiece\",        \n        \"An old and rusty giant robot lying on a dirt road, by jakub rozalski, dark sunset lighting, elegant, highly detailed, smooth, sharp focus, artstation, stunning masterpiece\"\n    ]],\n    tile_height=1024,\n    tile_width=1280,\n    tile_row_overlap=0,\n    tile_col_overlap=256,\n    guidance_scale_tiles=[[7, 7, 7]], # or guidance_scale=7 if is the same for all prompts\n    height=1024,\n    width=3840,    \n    generator=generator,\n    num_inference_steps=30,\n)[\"images\"][0]\n```\n\n----------------------------------------\n\nTITLE: Using Dynamic CFG Callback with StableDiffusionPipeline in Python\nDESCRIPTION: Example demonstrating how to use a custom dynamic CFG callback with the StableDiffusionPipeline. The callback is passed to the pipeline's callback_on_step_end parameter along with specifying prompt_embeds as a tensor input to modify during inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(1)\nout = pipeline(\n    prompt,\n    generator=generator,\n    callback_on_step_end=callback_dynamic_cfg,\n    callback_on_step_end_tensor_inputs=['prompt_embeds']\n)\n\nout.images[0].save(\"out_custom_cfg.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images in Batch Mode in Python\nDESCRIPTION: Attempts to generate a batch of 4 images simultaneously. This approach may cause memory errors on GPUs with limited VRAM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimages = pipeline(**get_inputs(batch_size=4)).images\nimage_grid(images)\n```\n\n----------------------------------------\n\nTITLE: Memory-Constrained Device Mapping\nDESCRIPTION: Shows how to set maximum memory limits per GPU when using balanced device mapping.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/inference_with_big_models.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nmax_memory = {0:\"1GB\", 1:\"1GB\"}\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    device_map=\"balanced\",\n    max_memory=max_memory\n)\nimage = pipeline(\"a dog\").images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Full Fine-tuning Training Command\nDESCRIPTION: Accelerate launch command for full fine-tuning of Flux Control\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/flux-control/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file=accelerate_ds2.yaml train_control_flux.py \\\n  --pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\" \\\n  --dataset_name=\"raulc0399/open_pose_controlnet\" \\\n  --output_dir=\"pose-control\" \\\n  --mixed_precision=\"bf16\" \\\n  --train_batch_size=2 \\\n  --dataloader_num_workers=4 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --proportion_empty_prompts=0.2 \\\n  --learning_rate=5e-5 \\\n  --adam_weight_decay=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"cosine\" \\\n  --lr_warmup_steps=1000 \\\n  --checkpointing_steps=1000 \\\n  --max_train_steps=10000 \\\n  --validation_steps=200 \\\n  --validation_image \"2_pose_1024.jpg\" \"3_pose_1024.jpg\" \\\n  --validation_prompt \"two friends sitting by each other enjoying a day at the park, full hd, cinematic\" \"person enjoying a day at the park, full hd, cinematic\" \\\n  --offload \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Inference with Multiple Concept Custom Diffusion Model in Python\nDESCRIPTION: This code demonstrates how to load and use a trained Custom Diffusion model for inference with multiple custom concepts. It loads the base model, applies custom attention weights, and loads multiple textual inversion embeddings for combining concepts in the prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom huggingface_hub.repocard import RepoCard\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16,\n).to(\"cuda\")\nmodel_id = \"sayakpaul/custom-diffusion-cat-wooden-pot\"\npipeline.unet.load_attn_procs(model_id, weight_name=\"pytorch_custom_diffusion_weights.bin\")\npipeline.load_textual_inversion(model_id, weight_name=\"<new1>.bin\")\npipeline.load_textual_inversion(model_id, weight_name=\"<new2>.bin\")\n\nimage = pipeline(\n    \"the <new1> cat sculpture in the style of a <new2> wooden pot\",\n    num_inference_steps=100,\n    guidance_scale=6.0,\n    eta=1.0,\n).images[0]\nimage.save(\"multi-subject.png\")\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up Repository - Git Bash\nDESCRIPTION: Clones the Diffusers repository from GitHub and sets up the remote repository. This enables contributions and keeping the fork in sync with the upstream repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone git@github.com:<your GitHub handle>/diffusers.git\n$ cd diffusers\n$ git remote add upstream https://github.com/huggingface/diffusers.git\n```\n\n----------------------------------------\n\nTITLE: Creating a PyTorch DataLoader for Training\nDESCRIPTION: Wraps the preprocessed dataset in a PyTorch DataLoader to handle batching and shuffling during training. The batch size is defined in the configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n\n>>> train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n```\n\n----------------------------------------\n\nTITLE: Previewing documentation with doc-builder\nDESCRIPTION: Generic command syntax for previewing documentation using doc-builder, which takes the package name and path to documentation source as parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndoc-builder preview {package_name} {path_to_docs}\n```\n\n----------------------------------------\n\nTITLE: Implementing T-GATE with Latent Consistency Model\nDESCRIPTION: Example of accelerating Latent Consistency Model using T-GATE with LCMScheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/tgate.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers import UNet2DConditionModel, LCMScheduler\nfrom diffusers import DPMSolverMultistepScheduler\nfrom tgate import TgateSDXLLoader\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"latent-consistency/lcm-sdxl\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    unet=unet,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\ngate_step = 1\ninference_step = 4\npipe = TgateSDXLLoader(\n       pipe,\n       gate_step=gate_step,\n       num_inference_steps=inference_step,\n       lcm=True\n).to(\"cuda\")\n\nimage = pipe.tgate(\n       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n       gate_step=gate_step,\n       num_inference_steps=inference_step\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-quantized Models from Hub\nDESCRIPTION: Shows how to load a pre-quantized model from the Hugging Face Hub without specifying quantization config parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxTransformer2DModel, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel_4bit = FluxTransformer2DModel.from_pretrained(\n    \"hf-internal-testing/flux.1-dev-nf4-pkg\", subfolder=\"transformer\"\n)\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Weights and Running Inference\nDESCRIPTION: Python code demonstrating how to load LoRA weights into a base model and perform inference with scale parameter control. Includes examples of partial and full LoRA weight application.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/lora.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe.unet.load_attn_procs(model_path)\npipe.to(\"cuda\")\n# LoRA 파인튜닝된 모델의 가중치 절반과 기본 모델의 가중치 절반 사용\n\nimage = pipe(\n    \"A picture of a sks dog in a bucket.\",\n    num_inference_steps=25,\n    guidance_scale=7.5,\n    cross_attention_kwargs={\"scale\": 0.5},\n).images[0]\n# 완전히 파인튜닝된 LoRA 모델의 가중치 사용\n\nimage = pipe(\"A picture of a sks dog in a bucket.\", num_inference_steps=25, guidance_scale=7.5).images[0]\nimage.save(\"bucket-dog.png\")\n```\n\n----------------------------------------\n\nTITLE: Installing the Hugging Face doc-builder tool\nDESCRIPTION: Command to install the open-source documentation builder tool from Hugging Face, which is required to generate the documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/doc-builder\n```\n\n----------------------------------------\n\nTITLE: Inference with Custom Diffusion for Single Concept\nDESCRIPTION: Python code for loading a trained Custom Diffusion model from local files and generating an image with the learned concept.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16).to(\"cuda\")\npipe.unet.load_attn_procs(\"path-to-save-model\", weight_name=\"pytorch_custom_diffusion_weights.bin\")\npipe.load_textual_inversion(\"path-to-save-model\", weight_name=\"<new1>.bin\")\n\nimage = pipe(\n    \"<new1> cat sitting in a bucket\",\n    num_inference_steps=100,\n    guidance_scale=6.0,\n    eta=1.0,\n).images[0]\nimage.save(\"cat.png\")\n```\n\n----------------------------------------\n\nTITLE: Image Editing with OmniGen\nDESCRIPTION: Example of using OmniGen for image editing tasks with multimodal inputs. Shows how to combine image and text inputs with specific editing instructions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/omnigen.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt=\"<img><|image_1|></img> Remove the woman's earrings. Replace the mug with a clear glass filled with sparkling iced cola.\"\ninput_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/t2i_woman_with_book.png\")]\nimage = pipe(\n    prompt=prompt, \n    input_images=input_images, \n    guidance_scale=2, \n    img_guidance_scale=1.6,\n    use_input_image_size_as_output=True,\n    generator=torch.Generator(device=\"cpu\").manual_seed(222)).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading the AutoencoderKLMagvit Model in Python\nDESCRIPTION: Code snippet for loading the 3D variational autoencoder (VAE) model from the EasyAnimateV5.1-12b-zh pretrained model. The code initializes the VAE with float16 precision and moves it to the CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoderkl_magvit.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKLMagvit\n\nvae = AutoencoderKLMagvit.from_pretrained(\"alibaba-pai/EasyAnimateV5.1-12b-zh\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing for Speed with torch.compile\nDESCRIPTION: This diff snippet demonstrates how to optimize the video generation pipeline for speed using torch.compile on the UNet component.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_17\n\nLANGUAGE: diff\nCODE:\n```\n- pipeline.enable_model_cpu_offload()\n+ pipeline.to(\"cuda\")\n+ pipeline.unet = torch.compile(pipeline.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Stable Video Diffusion\nDESCRIPTION: Command to install the necessary Python libraries for running Stable Video Diffusion models, including diffusers, transformers, and accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/svd.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q -U diffusers transformers accelerate\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Kandinsky2.2 Prior\nDESCRIPTION: Bash script to fine-tune the Kandinsky2.2 prior model using Accelerate and a specified dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image_prior.py \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot naruto, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi2-prior-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Basic MPS Pipeline Setup\nDESCRIPTION: Demonstrates how to set up and run a basic Stable Diffusion pipeline using MPS backend on Apple silicon devices. Includes attention slicing recommendation for devices with limited RAM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/mps.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\npipe = pipe.to(\"mps\")\n\n# Recommended if your computer has < 64 GB of RAM\npipe.enable_attention_slicing()\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Downloading Compiled Core ML Models for Swift Inference\nDESCRIPTION: Python script to download the compiled variant of a Core ML model, which is required for Swift inference. This script downloads the model to a local directory for use in Swift applications.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/coreml.md#2025-04-11_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/compiled\"\n\nmodel_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\nsnapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\nprint(f\"Model downloaded at {model_path}\")\n```\n\n----------------------------------------\n\nTITLE: RefineEdit Configuration for Prompt2Prompt Pipeline in Diffusers\nDESCRIPTION: This code snippet shows how to configure the Prompt2Prompt pipeline for RefineEdit, which allows adding words to a prompt while maintaining image consistency. It defines the prompts and cross-attention parameters needed for this edit type.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\"A turtle\",\n           \"A turtle in a forest\"]\n\ncross_attention_kwargs = {\n    \"edit_type\": \"refine\",\n    \"cross_replace_steps\": 0.4,\n    \"self_replace_steps\": 0.4,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Seed Resizing for Stable Diffusion in Python\nDESCRIPTION: Demonstrates seed resizing for Stable Diffusion, allowing for consistent image generation across different resolutions. The example compares the original pipeline with the seed resizing pipeline when changing dimensions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch as th\nimport numpy as np\nfrom diffusers import DiffusionPipeline\n\n# Ensure the save directory exists or create it\nsave_dir = './seed_resize/'\nos.makedirs(save_dir, exist_ok=True)\n\nhas_cuda = th.cuda.is_available()\ndevice = th.device('cpu' if not has_cuda else 'cuda')\n\npipe = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"seed_resize_stable_diffusion\"\n).to(device)\n\ndef dummy(images, **kwargs):\n    return images, False\n\npipe.safety_checker = dummy\n\nimages = []\nth.manual_seed(0)\ngenerator = th.Generator(\"cuda\").manual_seed(0)\n\nseed = 0\nprompt = \"A painting of a futuristic cop\"\n\nwidth = 512\nheight = 512\n\nres = pipe(\n    prompt,\n    guidance_scale=7.5,\n    num_inference_steps=50,\n    height=height,\n    width=width,\n    generator=generator)\nimage = res.images[0]\nimage.save(os.path.join(save_dir, 'seed_resize_{w}_{h}_image.png'.format(w=width, h=height)))\n\nth.manual_seed(0)\ngenerator = th.Generator(\"cuda\").manual_seed(0)\n\npipe = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"seed_resize_stable_diffusion\"\n).to(device)\n\nwidth = 512\nheight = 592\n\nres = pipe(\n    prompt,\n    guidance_scale=7.5,\n    num_inference_steps=50,\n    height=height,\n    width=width,\n    generator=generator)\nimage = res.images[0]\nimage.save(os.path.join(save_dir, 'seed_resize_{w}_{h}_image.png'.format(w=width, h=height)))\n\npipe_compare = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"seed_resize_stable_diffusion\"\n).to(device)\n\nres = pipe_compare(\n    prompt,\n    guidance_scale=7.5,\n    num_inference_steps=50,\n    height=height,\n    width=width,\n    generator=generator\n)\n\nimage = res.images[0]\nimage.save(os.path.join(save_dir, 'seed_resize_{w}_{h}_image_compare.png'.format(w=width, h=height)))\n```\n\n----------------------------------------\n\nTITLE: Loading Reference Images for ControlNet and IP-Adapter\nDESCRIPTION: Loads the input images required for both IP-Adapter and ControlNet. The IP-Adapter uses a statue image as the style reference, while the ControlNet uses a depth map for structural guidance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nip_adapter_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/statue.png\")\ndepth_map = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/depth.png\")\n```\n\n----------------------------------------\n\nTITLE: Installing Flax Training Dependencies\nDESCRIPTION: Command to install the requirements for training with Flax.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text2image.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -U -r requirements_flax.txt\n```\n\n----------------------------------------\n\nTITLE: Accessing and Replacing Scheduler in Diffusion Pipeline\nDESCRIPTION: This code shows how to access the current scheduler in a diffusion pipeline and replace it with a different scheduler. It uses the DDIMScheduler as an example replacement.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/schedulers.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMScheduler\n\npipeline.scheduler\n\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: Performing Image Inpainting with Stable Diffusion 2\nDESCRIPTION: This code shows how to use Stable Diffusion 2's inpainting capabilities to replace a specific area of an image defined by a mask. It loads an original image and mask, initializes the inpainting pipeline, and generates a new image with the masked area filled according to the prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_2.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = load_image(img_url).resize((512, 512))\nmask_image = load_image(mask_url).resize((512, 512))\n\nrepo_id = \"stabilityai/stable-diffusion-2-inpainting\"\npipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, variant=\"fp16\")\n\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\nprompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\nimage = pipe(prompt=prompt, image=init_image, mask_image=mask_image, num_inference_steps=25).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Importing LatentConsistencyModelPipeline in Python\nDESCRIPTION: This code snippet shows the class definition for the LatentConsistencyModelPipeline, which is used for text-to-image generation with Latent Consistency Models. It includes methods for enabling and disabling various features like FreeU, VAE slicing, and VAE tiling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latent_consistency_models.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] LatentConsistencyModelPipeline\n    - all\n    - __call__\n    - enable_freeu\n    - disable_freeu\n    - enable_vae_slicing\n    - disable_vae_slicing\n    - enable_vae_tiling\n    - disable_vae_tiling\n```\n\n----------------------------------------\n\nTITLE: Loading FluxControlNetModel for a Flux.1 Pipeline with MultiControlNet\nDESCRIPTION: Example demonstrating how to load a FluxControlNetModel and wrap it in a FluxMultiControlNetModel for use with multiple control conditions. This allows combining different types of conditioning in a single pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/controlnet_flux.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncontrolnet = FluxControlNetModel.from_pretrained(\"InstantX/FLUX.1-dev-Controlnet-Canny\")\ncontrolnet = FluxMultiControlNetModel([controlnet])\npipe = FluxControlNetPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", controlnet=controlnet)\n```\n\n----------------------------------------\n\nTITLE: Fusing QKV Projections in SDXL\nDESCRIPTION: Combines the Q, K, V projection matrices in attention blocks into a single matrix to improve quantization impact and performance\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe.fuse_qkv_projections()\n```\n\n----------------------------------------\n\nTITLE: Optimizing HunyuanVideo Inference with Context Parallelism in Python\nDESCRIPTION: This Python script demonstrates optimizing HunyuanVideo inference using Context Parallelism and First Block Cache. It includes initialization of a distributed process group and GPU setting, configuring the pipeline and model, and enabling optimizations like tiling for memory savings. The script outputs inference performance metrics and saves the generated video.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/para_attn.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\\nimport time\\nimport torch\\nimport torch.distributed as dist\\nfrom diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\\nfrom diffusers.utils import export_to_video\\n\\ndist.init_process_group()\\n\\ntorch.cuda.set_device(dist.get_rank())\\n\\nmodel_id = \\\"tencent/HunyuanVideo\\\"\\ntransformer = HunyuanVideoTransformer3DModel.from_pretrained(\\n    model_id,\\n    subfolder=\\\"transformer\\\",\\n    torch_dtype=torch.bfloat16,\\n    revision=\\\"refs/pr/18\\\",\\n)\\npipe = HunyuanVideoPipeline.from_pretrained(\\n    model_id,\\n    transformer=transformer,\\n    torch_dtype=torch.float16,\\n    revision=\\\"refs/pr/18\\\",\\n).to(\\\"cuda\\\")\\n\\nfrom para_attn.context_parallel import init_context_parallel_mesh\\nfrom para_attn.context_parallel.diffusers_adapters import parallelize_pipe\\nfrom para_attn.parallel_vae.diffusers_adapters import parallelize_vae\\n\\nmesh = init_context_parallel_mesh(\\n    pipe.device.type,\\n)\\nparallelize_pipe(\\n    pipe,\\n    mesh=mesh,\\n)\\nparallelize_vae(pipe.vae, mesh=mesh._flatten())\\n\\nfrom para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\\n\\napply_cache_on_pipe(pipe)\\n\\n# Enable memory savings\\npipe.vae.enable_tiling()\\n\\nfor i in range(2):\\n    begin = time.time()\\n    output = pipe(\\n        prompt=\\\"A cat walks on the grass, realistic\\\",\\n        height=720,\\n        width=1280,\\n        num_frames=129,\\n        num_inference_steps=1 if i == 0 else 30,\\n        output_type=\\\"pil\\\" if dist.get_rank() == 0 else \\\"pt\\\",\\n    ).frames[0]\\n    end = time.time()\\n    if dist.get_rank() == 0:\\n        if i == 0:\\n            print(f\\\"Warm up time: {end - begin:.2f}s\\\")\\n        else:\\n            print(f\\\"Time: {end - begin:.2f}s\\\")\\n\\nif dist.get_rank() == 0:\\n    print(\\\"Saving video to hunyuan_video.mp4\\\")\\n    export_to_video(output, \\\"hunyuan_video.mp4\\\", fps=15)\\n\\ndist.destroy_process_group()\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Pushing Scheduler Configuration\nDESCRIPTION: Creates and pushes a DDIM scheduler configuration to HuggingFace Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/push_to_hub.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMScheduler\n\nscheduler = DDIMScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n)\nscheduler.push_to_hub(\"my-controlnet-scheduler\")\n```\n\n----------------------------------------\n\nTITLE: Importing LEditsPPInversionPipelineOutput in Python\nDESCRIPTION: This snippet shows how to import the LEditsPPInversionPipelineOutput class, which represents the output of the LEDITS++ inversion pipeline. It includes all associated methods and properties.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ledits_pp.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.ledits_pp.pipeline_output import LEditsPPInversionPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Implementing T-GATE with StableDiffusionXL and DeepCache\nDESCRIPTION: Example of combining T-GATE and DeepCache optimizations for StableDiffusionXLPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/tgate.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers import DPMSolverMultistepScheduler\nfrom tgate import TgateSDXLDeepCacheLoader\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-base-1.0\",\n            torch_dtype=torch.float16,\n            variant=\"fp16\",\n            use_safetensors=True,\n)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n\ngate_step = 10\ninference_step = 25\npipe = TgateSDXLDeepCacheLoader(\n       pipe,\n       cache_interval=3,\n       cache_branch_id=0,\n).to(\"cuda\")\n\nimage = pipe.tgate(\n       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n       gate_step=gate_step,\n       num_inference_steps=inference_step\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Using StableDiffusionUpscaleLDM3D Pipeline for RGB-D Generation and Upscaling\nDESCRIPTION: This example demonstrates how to use LDM3D to generate RGB and depth images, then upscale them to a higher resolution using the LDM3D-SR model. The pipeline first generates a lower resolution RGB-D pair, then increases the resolution while maintaining depth information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_77\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nimport os\nimport torch\nfrom diffusers import StableDiffusionLDM3DPipeline, DiffusionPipeline\n\n# Generate a rgb/depth output from LDM3D\n\npipe_ldm3d = StableDiffusionLDM3DPipeline.from_pretrained(\"Intel/ldm3d-4c\")\npipe_ldm3d.to(\"cuda\")\n\nprompt = \"A picture of some lemons on a table\"\noutput = pipe_ldm3d(prompt)\nrgb_image, depth_image = output.rgb, output.depth\nrgb_image[0].save(\"lemons_ldm3d_rgb.jpg\")\ndepth_image[0].save(\"lemons_ldm3d_depth.png\")\n\n# Upscale the previous output to a resolution of (1024, 1024)\n\npipe_ldm3d_upscale = DiffusionPipeline.from_pretrained(\"Intel/ldm3d-sr\", custom_pipeline=\"pipeline_stable_diffusion_upscale_ldm3d\")\n\npipe_ldm3d_upscale.to(\"cuda\")\n\nlow_res_img = Image.open(\"lemons_ldm3d_rgb.jpg\").convert(\"RGB\")\nlow_res_depth = Image.open(\"lemons_ldm3d_depth.png\").convert(\"L\")\noutputs = pipe_ldm3d_upscale(prompt=\"high quality high resolution uhd 4k image\", rgb=low_res_img, depth=low_res_depth, num_inference_steps=50, target_res=[1024, 1024])\n\nupscaled_rgb, upscaled_depth = outputs.rgb[0], outputs.depth[0]\nupscaled_rgb.save(\"upscaled_lemons_rgb.png\")\nupscaled_depth.save(\"upscaled_lemons_depth.png\")\n```\n\n----------------------------------------\n\nTITLE: CLIP Guided Images Mixing with Stable Diffusion in Python\nDESCRIPTION: This code demonstrates how to use CLIP-guided Stable Diffusion for image mixing. It includes setup for additional models like CLIP and CoCa, pipeline creation, and image processing. The example combines two images using the mixing pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\nimport torch\nimport requests\nimport open_clip\nfrom open_clip import SimpleTokenizer\nfrom io import BytesIO\nfrom diffusers import DiffusionPipeline\nfrom transformers import CLIPImageProcessor, CLIPModel\n\n\ndef download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n# Loading additional models\nfeature_extractor = CLIPImageProcessor.from_pretrained(\n    \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n)\nclip_model = CLIPModel.from_pretrained(\n    \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", torch_dtype=torch.float16\n)\ncoca_model = open_clip.create_model('coca_ViT-L-14', pretrained='laion2B-s13B-b90k').to('cuda')\ncoca_model.dtype = torch.float16\ncoca_transform = open_clip.image_transform(\n    coca_model.visual.image_size,\n    is_train=False,\n    mean=getattr(coca_model.visual, 'image_mean', None),\n    std=getattr(coca_model.visual, 'image_std', None),\n)\ncoca_tokenizer = SimpleTokenizer()\n\n# Pipeline creating\nmixing_pipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"clip_guided_images_mixing_stable_diffusion\",\n    clip_model=clip_model,\n    feature_extractor=feature_extractor,\n    coca_model=coca_model,\n    coca_tokenizer=coca_tokenizer,\n    coca_transform=coca_transform,\n    torch_dtype=torch.float16,\n)\nmixing_pipeline.enable_attention_slicing()\nmixing_pipeline = mixing_pipeline.to(\"cuda\")\n\n# Pipeline running\ngenerator = torch.Generator(device=\"cuda\").manual_seed(17)\n\ndef download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\ncontent_image = download_image(\"https://huggingface.co/datasets/TheDenk/images_mixing/resolve/main/boromir.jpg\")\nstyle_image = download_image(\"https://huggingface.co/datasets/TheDenk/images_mixing/resolve/main/gigachad.jpg\")\n\npipe_images = mixing_pipeline(\n    num_inference_steps=50,\n    content_image=content_image,\n    style_image=style_image,\n    noise_strength=0.65,\n    slerp_latent_style_strength=0.9,\n    slerp_prompt_style_strength=0.1,\n    slerp_clip_image_style_strength=0.1,\n    guidance_scale=9.0,\n    batch_size=1,\n    clip_guidance_scale=100,\n    generator=generator,\n).images\n\noutput_path = \"mixed_output.jpg\"\npipe_images[0].save(output_path)\nprint(f\"Image saved successfully at {output_path}\")\n```\n\n----------------------------------------\n\nTITLE: Importing LDMSuperResolutionPipeline in Python\nDESCRIPTION: This snippet demonstrates how to import the LDMSuperResolutionPipeline class for super-resolution tasks using Latent Diffusion Models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latent_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import LDMSuperResolutionPipeline\n```\n\n----------------------------------------\n\nTITLE: Compiling Stable Diffusion XL Model for Neuron\nDESCRIPTION: This command exports the Stable Diffusion XL model to the Neuron format. It specifies various parameters such as batch size, image dimensions, and auto-casting options for optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/neuron.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export neuron --model stabilityai/stable-diffusion-xl-base-1.0 \\\n  --batch_size 1 \\\n  --height 1024 `# height in pixels of generated image, eg. 768, 1024` \\\n  --width 1024 `# width in pixels of generated image, eg. 768, 1024` \\\n  --num_images_per_prompt 1 `# number of images to generate per prompt, defaults to 1` \\\n  --auto_cast matmul `# cast only matrix multiplication operations` \\\n  --auto_cast_type bf16 `# cast operations from FP32 to BF16` \\\n  sd_neuron_xl/\n```\n\n----------------------------------------\n\nTITLE: Multi IP-Adapter Setup for Diverse Style Generation in Python\nDESCRIPTION: This code sets up multiple IP-Adapters to generate specific images in diverse styles. It loads an image encoder, initializes a pipeline with multiple IP-Adapters, and prepares image prompts for face and style generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForText2Image, DDIMScheduler\nfrom transformers import CLIPVisionModelWithProjection\nfrom diffusers.utils import load_image\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"h94/IP-Adapter\",\n    subfolder=\"models/image_encoder\",\n    torch_dtype=torch.float16,\n)\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    image_encoder=image_encoder,\n)\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\npipeline.load_ip_adapter(\n  \"h94/IP-Adapter\",\n  subfolder=\"sdxl_models\",\n  weight_name=[\"ip-adapter-plus_sdxl_vit-h.safetensors\", \"ip-adapter-plus-face_sdxl_vit-h.safetensors\"]\n)\npipeline.set_ip_adapter_scale([0.7, 0.3])\npipeline.enable_model_cpu_offload()\n\nface_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/women_input.png\")\nstyle_folder = \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy\"\nstyle_images = [load_image(f\"{style_folder}/img{i}.png\") for i in range(10)]\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\n\nimage = pipeline(\n    prompt=\"wonderwoman\",\n    ip_adapter_image=[style_images, face_image],\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n    num_inference_steps=50, num_images_per_prompt=1,\n    generator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Running Inference with a Custom Model Version in Python\nDESCRIPTION: Example of running Core ML inference with a specific model version (stable-diffusion-v1-5) by specifying the model version parameter. This is useful for custom or fine-tuned models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/coreml.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m python_coreml_stable_diffusion.pipeline --prompt \"a photo of an astronaut riding a horse on mars\" --compute-unit ALL -o output --seed 93 -i models/coreml-stable-diffusion-v1-5_original_packages --model-version stable-diffusion-v1-5/stable-diffusion-v1-5\n```\n\n----------------------------------------\n\nTITLE: Performing Single Denoising Step\nDESCRIPTION: Applies a single denoising step using the scheduler to produce a less noisy image from the model output and current noisy sample.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample).prev_sample\n>>> less_noisy_sample.shape\n```\n\n----------------------------------------\n\nTITLE: Version Check Command in Python\nDESCRIPTION: Command to check the installed version of the Diffusers library using Python\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -c \"import diffusers; print(diffusers.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Loading Kandinsky 2.1 Prior Pipeline for Interpolation\nDESCRIPTION: Initializes the Kandinsky 2.1 Prior Pipeline and loads two sample images to be used for interpolation. The code sets up the necessary components and displays the source images using make_image_grid.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyPriorPipeline, KandinskyPipeline\nfrom diffusers.utils import load_image, make_image_grid\nimport torch\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\nimg_1 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\nimg_2 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg\")\nmake_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Training Kandinsky 2.2 Decoder with LoRA\nDESCRIPTION: Script for training the Kandinsky 2.2 decoder model using LoRA adaptation. Uses the Narutos dataset with mixed precision training and supports checkpointing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image_decoder_lora.py \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --num_train_epochs=100 --checkpointing_steps=5000 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --seed=42 \\\n  --rank=4 \\\n  --gradient_checkpointing \\\n  --output_dir=\"kandi22-decoder-naruto-lora\" \\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n  --push_to_hub \\\n```\n\n----------------------------------------\n\nTITLE: Selective Gradient Update for Concept Learning\nDESCRIPTION: Zeroes out gradients for all token embeddings except the modifier tokens being learned. This ensures that only the embeddings for the specific concept are updated during training, while preserving the original behavior of all other tokens.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nif args.modifier_token is not None:\n    if accelerator.num_processes > 1:\n        grads_text_encoder = text_encoder.module.get_input_embeddings().weight.grad\n    else:\n        grads_text_encoder = text_encoder.get_input_embeddings().weight.grad\n    index_grads_to_zero = torch.arange(len(tokenizer)) != modifier_token_id[0]\n    for i in range(len(modifier_token_id[1:])):\n        index_grads_to_zero = index_grads_to_zero & (\n            torch.arange(len(tokenizer)) != modifier_token_id[i]\n        )\n    grads_text_encoder.data[index_grads_to_zero, :] = grads_text_encoder.data[\n        index_grads_to_zero, :\n    ].fill_(0)\n```\n\n----------------------------------------\n\nTITLE: Hotswapping LoRA Adapters in Diffusers\nDESCRIPTION: This code shows how to efficiently swap between different LoRA adapters without accumulating memory usage. It demonstrates the basic hotswapping workflow, which is useful when generating images with multiple adapters sequentially.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipe = ...\n# load adapter 1 as normal\npipeline.load_lora_weights(file_name_adapter_1)\n# generate some images with adapter 1\n...\n# now hot swap the 2nd adapter\npipeline.load_lora_weights(file_name_adapter_2, hotswap=True, adapter_name=\"default_0\")\n# generate images with adapter 2\n```\n\n----------------------------------------\n\nTITLE: Using the Diffusers Logger in Custom Code\nDESCRIPTION: Example of how to use the same logger as the Diffusers library in your own module or script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/logging.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import logging\n\nlogging.set_verbosity_info()\nlogger = logging.get_logger(\"diffusers\")\nlogger.info(\"INFO\")\nlogger.warning(\"WARN\")\n```\n\n----------------------------------------\n\nTITLE: Applying Code Style Formatting\nDESCRIPTION: Command to apply automatic style corrections using ruff and isort\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ make style\n```\n\n----------------------------------------\n\nTITLE: Citation Format for Diffusers Library\nDESCRIPTION: BibTeX citation entry for referencing the Diffusers library in academic work. Includes authors, title, publication year, and repository information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{von-platen-etal-2022-diffusers,\n  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},\n  title = {Diffusers: State-of-the-art diffusion models},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/huggingface/diffusers}}\n}\n```\n\n----------------------------------------\n\nTITLE: Progress Bar Output and Timing Logs\nDESCRIPTION: Console output showing progress bars and timing measurements from multiple inference runs using the Diffusers library. Each run shows completion percentage and iterations per second, along with logged inference times.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/inference/flux/README.md#2025-04-11_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                             | 20/28 [00:03<00:01,  6.01it/s]2025-03-14 21:38:38 [info     ] inference time: 5.950578948002658\n```\n\n----------------------------------------\n\nTITLE: Basic Object Removal Inpainting\nDESCRIPTION: Example showing simple object removal using the regular Stable Diffusion checkpoint. Demonstrates how basic inpainting tasks can be effectively performed without specialized models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/road-mask.png\")\n\nimage = pipeline(prompt=\"road\", image=init_image, mask_image=mask_image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Tracing a UNet Model (PyTorch)\nDESCRIPTION: Demonstrates how to trace a UNet model using PyTorch. Sets `torch.set_grad_enabled(False)` to disable gradient calculation and initializes some variables for number of experiments and unet runs per experiment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n\"import time\nimport torch\nfrom diffusers import StableDiffusionPipeline\nimport functools\n\n# torch disable grad\ntorch.set_grad_enabled(False)\n\n# set variables\nn_experiments = 2\nunet_runs_per_experiment = 50\n\"\n```\n\n----------------------------------------\n\nTITLE: Loading FP16 Variant of a Diffusion Pipeline\nDESCRIPTION: Load a half-precision (fp16) variant of a diffusion model pipeline to reduce memory usage, specifying both the variant name and the torch data type.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\n# load fp16 variant\nstable_diffusion = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", variant=\"fp16\", torch_dtype=torch.float16\n)\n```\n\n----------------------------------------\n\nTITLE: ReweightEdit Configuration for Modulating Word Importance in Prompt2Prompt\nDESCRIPTION: This code configures the Prompt2Prompt pipeline for ReweightEdit, which allows modulating the importance of specific words in a prompt. It demonstrates how to increase the emphasis on the word 'smiling' in the generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\"A smiling turtle\"] * 2\n\nedit_kcross_attention_kwargswargs = {\n    \"edit_type\": \"reweight\",\n    \"cross_replace_steps\": 0.4,\n    \"self_replace_steps\": 0.4,\n    \"equalizer_words\": [\"smiling\"],\n    \"equalizer_strengths\": [5]\n}\n```\n\n----------------------------------------\n\nTITLE: Saving Generated Images to Disk in Python\nDESCRIPTION: This snippet demonstrates how to save a generated image to disk. The output of the DiffusionPipeline is a PIL.Image object that can be saved to a file using the save method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/unconditional_image_generation.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimage.save(\"generated_image.png\")\n```\n\n----------------------------------------\n\nTITLE: Configuring LoRA for Text Encoder with PEFT\nDESCRIPTION: Set up LoRA configuration for the text encoder, particularly useful for models like Stable Diffusion XL\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lora.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntext_lora_config = LoraConfig(\n    r=args.rank,\n    lora_alpha=args.rank,\n    init_lora_weights=\"gaussian\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n)\n\ntext_encoder_one.add_adapter(text_lora_config)\ntext_encoder_two.add_adapter(text_lora_config)\ntext_lora_parameters_one = list(filter(lambda p: p.requires_grad, text_encoder_one.parameters()))\ntext_lora_parameters_two = list(filter(lambda p: p.requires_grad, text_encoder_two.parameters()))\n```\n\n----------------------------------------\n\nTITLE: Generating Interpolated Image with Kandinsky 2.1\nDESCRIPTION: Performs the interpolation process with Kandinsky 2.1 by generating embeddings from the prior pipeline and then using them with the main pipeline to create the final image. This demonstrates how to blend multiple inputs with specific weights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# prompt can be left empty\nprompt = \"\"\nprior_out = prior_pipeline.interpolate(images_texts, weights)\n\npipeline = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\nimage = pipeline(prompt, **prior_out, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained InstructPix2Pix Model\nDESCRIPTION: This Python code demonstrates how to load a trained InstructPix2Pix model and use it for inference. It loads an image, applies an instruction prompt to edit the image, and saves the result. The code shows how to configure important parameters like guidance scales that affect the output quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\nfrom diffusers.utils import load_image\n\npipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\"your_cool_model\", torch_dtype=torch.float16).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\nimage = load_image(\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\")\nprompt = \"add some ducks to the lake\"\nnum_inference_steps = 20\nimage_guidance_scale = 1.5\nguidance_scale = 10\n\nedited_image = pipeline(\n   prompt,\n   image=image,\n   num_inference_steps=num_inference_steps,\n   image_guidance_scale=image_guidance_scale,\n   guidance_scale=guidance_scale,\n   generator=generator,\n).images[0]\nedited_image.save(\"edited_image.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22Img2ImgCombinedPipeline in Python\nDESCRIPTION: This snippet shows the class definition for KandinskyV22Img2ImgCombinedPipeline, which combines the prior and main pipelines for a more streamlined image-to-image transformation process in Kandinsky 2.2.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22Img2ImgCombinedPipeline\n\t- all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Stable Diffusion Pipeline\nDESCRIPTION: Code to generate images using the Stable Diffusion pipeline with specified prompts and a fixed seed for reproducibility\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nimages = sd_pipeline(sample_prompts, num_images_per_prompt=1, generator=generator).images\n```\n\n----------------------------------------\n\nTITLE: Depth Visualization Method\nDESCRIPTION: Method signature for visualizing depth predictions from Marigold models\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nMarigoldImageProcessor.visualize_depth()\n```\n\n----------------------------------------\n\nTITLE: Loading Tokenizer and Models for Textual Inversion\nDESCRIPTION: Python code snippet showing how the textual inversion script loads the tokenizer, scheduler, and model components needed for training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Load tokenizer\nif args.tokenizer_name:\n    tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)\nelif args.pretrained_model_name_or_path:\n    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n\n# Load scheduler and models\nnoise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\ntext_encoder = CLIPTextModel.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n)\nvae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\nunet = UNet2DConditionModel.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Single-File Safetensors Model\nDESCRIPTION: Shows how to load a diffusion model with all weights stored in a single safetensors file using the StableDiffusionPipeline.from_single_file method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_single_file(\n    \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\n)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Quantized Models in Python\nDESCRIPTION: Demonstrates how to save a model quantized with Quanto using the save_pretrained method and subsequently load it. Direct loading of models quantized exclusively with Quanto outside of Diffusers is unsupported.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/quanto.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxTransformer2DModel, QuantoConfig\n\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\nquantization_config = QuantoConfig(weights_dtype=\"float8\")\ntransformer = FluxTransformer2DModel.from_pretrained(\n      model_id,\n      subfolder=\"transformer\",\n      quantization_config=quantization_config,\n      torch_dtype=torch.bfloat16,\n)\n# save quantized model to reuse\ntransformer.save_pretrained(\"<your quantized model save path>\")\n\n# you can reload your quantized model with\nmodel = FluxTransformer2DModel.from_pretrained(\"<your quantized model save path>\")\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Text Encoder Along with UNet\nDESCRIPTION: This script allows for fine-tuning the text encoder in tandem with the UNet model as part of Dreambooth training. Text encoder training yields better results but requires more VRAM (at least 24GB). It includes settings for instance and class data, prompt-specific configurations, and employs 8-bit optimization with gradient checkpointing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/dreambooth_inpaint/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"runwayml/stable-diffusion-inpainting\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth_inpaint.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --train_text_encoder \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --use_8bit_adam \\\n  --gradient_checkpointing \\\n  --learning_rate=2e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Loading Kandinsky ControlNet Pipelines\nDESCRIPTION: Loads the Kandinsky V2.2 Prior pipeline for embedding generation and the ControlNet Image-to-Image pipeline for controlled image generation. Both models are loaded with half precision (float16) for improved performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nprior_pipeline = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\npipeline = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Using Masked Image-to-Image Pipeline for Inpainting in Diffusers\nDESCRIPTION: This code implements a masked image-to-image pipeline that enables inpainting features for non-inpaint models. It computes a mask based on the difference between original and painted images, then applies the diffusion process only to the masked area.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_65\n\nLANGUAGE: python\nCODE:\n```\nimg = PIL.Image.open(\"./mech.png\")\n# read image with mask painted over\nimg_paint = PIL.Image.open(\"./mech_painted.png\")\nneq = numpy.any(numpy.array(img) != numpy.array(img_paint), axis=-1)\nmask = neq / neq.max()\n\npipeline = MaskedStableDiffusionImg2ImgPipeline.from_pretrained(\"frankjoshua/icbinpICantBelieveIts_v8\")\n\n# works best with EulerAncestralDiscreteScheduler\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\ngenerator = torch.Generator(device=\"cpu\").manual_seed(4)\n\nprompt = \"a man wearing a mask\"\nresult = pipeline(prompt=prompt, image=img_paint, mask=mask, strength=0.75,\n                  generator=generator)\nresult.images[0].save(\"result.png\")\n```\n\n----------------------------------------\n\nTITLE: Converting Frames to GIF\nDESCRIPTION: Shows how to convert generated image frames into animated GIFs using the export_to_gif utility function.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import export_to_gif\n\nexport_to_gif(images[0], \"firecracker_3d.gif\")\nexport_to_gif(images[1], \"cake_3d.gif\")\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Training Parameters\nDESCRIPTION: Example of how to customize training parameters like mixed precision when launching the training script with Accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_unconditional.py \\\n  --mixed_precision=\"bf16\"\n```\n\n----------------------------------------\n\nTITLE: Processing Discrete Inputs in Transformer2DModel\nDESCRIPTION: Outlines the process for handling discrete inputs in the Transformer2DModel. It involves converting input classes to embeddings, applying positional embeddings, processing with Transformer blocks, and predicting classes of the unnoised image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/transformer2d.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n1. Convert input (classes of latent pixels) to embeddings and apply positional embeddings.\n2. Apply the Transformer blocks in the standard way.\n3. Predict classes of unnoised image.\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Accelerate Configuration in Python\nDESCRIPTION: Python code snippet to create a basic Accelerate configuration programmatically, which is useful in environments that don't support interactive shells like notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Running Denoising with Transformer\nDESCRIPTION: Initializes a new FluxPipeline with only the transformer component for the denoising process. Uses previously generated prompt embeddings to generate latent representations of the image, setting output_type to 'latent' to defer VAE decoding.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    text_encoder=None,\n    text_encoder_2=None,\n    tokenizer=None,\n    tokenizer_2=None,\n    vae=None,\n    transformer=transformer,\n    torch_dtype=torch.bfloat16\n)\n\nprint(\"Running denoising.\")\nheight, width = 768, 1360\nlatents = pipeline(\n    prompt_embeds=prompt_embeds,\n    pooled_prompt_embeds=pooled_prompt_embeds,\n    num_inference_steps=50,\n    guidance_scale=3.5,\n    height=height,\n    width=width,\n    output_type=\"latent\",\n).images\n```\n\n----------------------------------------\n\nTITLE: Merging Checkpoint Pipelines in Python\nDESCRIPTION: This code demonstrates how to use the Checkpoint Merger Pipeline to merge multiple pretrained model checkpoints. It shows different scenarios for merging compatible and incompatible checkpoints.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", custom_pipeline=\"checkpoint_merger\")\n\n# Compatible checkpoints\nmerged_pipe = pipe.merge([\"CompVis/stable-diffusion-v1-4\",\" CompVis/stable-diffusion-v1-2\"], interp=\"sigmoid\", alpha=0.4)\n\n# Incompatible checkpoints\nmerged_pipe_1 = pipe.merge([\"CompVis/stable-diffusion-v1-4\", \"hakurei/waifu-diffusion\"], force=True, interp=\"sigmoid\", alpha=0.4)\n\n# Three checkpoint merging\nmerged_pipe_2 = pipe.merge([\"CompVis/stable-diffusion-v1-4\", \"hakurei/waifu-diffusion\", \"prompthero/openjourney\"], force=True, interp=\"add_difference\", alpha=0.4)\n\nprompt = \"An astronaut riding a horse on Mars\"\n\nimage = merged_pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Crop Conditioning in SDXL Pipeline\nDESCRIPTION: This snippet shows how to use crop conditioning in SDXL to create off-centered compositions. The example sets crop coordinates to (256, 0) which shifts the composition in the generated image of an astronaut in a jungle.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipeline(prompt=prompt, crops_coords_top_left=(256, 0)).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Regular Checkpoint Inpainting Pipeline\nDESCRIPTION: Implementation using the standard Stable Diffusion v1.5 checkpoint for inpainting. Demonstrates setup with model CPU offload and memory optimization for basic inpainting tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\ngenerator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Loading DDPM Scheduler in Python\nDESCRIPTION: Initializes a DDPM scheduler from the same repository configuration as the model. The scheduler manages the denoising process with parameters like beta schedules and number of timesteps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import DDPMScheduler\n\n>>> scheduler = DDPMScheduler.from_config(repo_id)\n>>> scheduler\nDDPMScheduler {\n  \"_class_name\": \"DDPMScheduler\",\n  \"_diffusers_version\": \"0.13.1\",\n  \"beta_end\": 0.02,\n  \"beta_schedule\": \"linear\",\n  \"beta_start\": 0.0001,\n  \"clip_sample\": true,\n  \"clip_sample_range\": 1.0,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null,\n  \"variance_type\": \"fixed_small\"\n}\n```\n\n----------------------------------------\n\nTITLE: LoRA Inference Script\nDESCRIPTION: Python script for inference using trained LoRA weights with Flux Control Pipeline\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/flux-control/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers import FluxControlPipeline\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nimport torch \n\npipe = FluxControlPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\npipe.load_lora_weights(\"...\") # change this.\n\nopen_pose = OpenposeDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n# prepare pose condition.\nurl = \"https://huggingface.co/Adapter/t2iadapter/resolve/main/people.jpg\"\nimage = load_image(url)\nimage = open_pose(image, detect_resolution=512, image_resolution=1024)\nimage = np.array(image)[:, :, ::-1]           \nimage = Image.fromarray(np.uint8(image))\n\nprompt = \"A couple, 4k photo, highly detailed\"\n\ngen_images = pipe(\n  prompt=prompt,\n  control_image=image,\n  num_inference_steps=50,\n  joint_attention_kwargs={\"scale\": 0.9},\n  guidance_scale=25., \n).images[0]\ngen_images.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing JAX/Flax Dependencies\nDESCRIPTION: Import statements for required JAX, Flax, and diffusers modules\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import pmap\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n```\n\n----------------------------------------\n\nTITLE: Loading DiffEdit Pipeline with Schedulers\nDESCRIPTION: Initializing the StableDiffusionDiffEditPipeline with appropriate schedulers and memory optimizations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n\npipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\",\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    use_safetensors=True,\n)\npipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\npipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\npipeline.enable_model_cpu_offload()\npipeline.enable_vae_slicing()\n```\n\n----------------------------------------\n\nTITLE: Flax/JAX Training Command for Stable Diffusion\nDESCRIPTION: Execute training for Stable Diffusion using Flax/JAX with specified hyperparameters and mixed precision\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\npython train_text_to_image_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --mixed_precision=\"fp16\" \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Compiled SDXL Model on AWS Neuron\nDESCRIPTION: This Python code demonstrates how to use the NeuronStableDiffusionXLPipeline to load a pre-compiled Stable Diffusion XL model and generate an image based on a given prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/neuron.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from optimum.neuron import NeuronStableDiffusionXLPipeline\n\n>>> stable_diffusion_xl = NeuronStableDiffusionXLPipeline.from_pretrained(\"sd_neuron_xl/\")\n>>> prompt = \"a pig with wings flying in floating US dollar banknotes in the air, skyscrapers behind, warm color palette, muted colors, detailed, 8k\"\n>>> image = stable_diffusion_xl(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Comparing Stable Diffusion Versions in Python\nDESCRIPTION: This code uses a custom pipeline to compare the outputs of different Stable Diffusion versions (v1.1, v1.2, v1.3, v1.4). It generates images from all versions and displays them in a grid.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport matplotlib.pyplot as plt\n\npipe = DiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', custom_pipeline='suvadityamuk/StableDiffusionComparison')\npipe.enable_attention_slicing()\npipe = pipe.to('cuda')\nprompt = \"an astronaut riding a horse on mars\"\noutput = pipe(prompt)\n\nplt.subplots(2,2,1)\nplt.imshow(output.images[0])\nplt.title('Stable Diffusion v1.1')\nplt.axis('off')\nplt.subplots(2,2,2)\nplt.imshow(output.images[1])\nplt.title('Stable Diffusion v1.2')\nplt.axis('off')\nplt.subplots(2,2,3)\nplt.imshow(output.images[2])\nplt.title('Stable Diffusion v1.3')\nplt.axis('off')\nplt.subplots(2,2,4)\nplt.imshow(output.images[3])\nplt.title('Stable Diffusion v1.4')\nplt.axis('off')\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Preparing Canny Edge Detection Input - Python\nDESCRIPTION: Loads an image and processes it to create a canny edge detection map using OpenCV. The code converts the image to grayscale edges and prepares it for use with ControlNet.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL\nfrom diffusers.utils import load_image, make_image_grid\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport torch\n\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\"\n)\n\nimage = np.array(original_image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\nmake_image_grid([original_image, canny_image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Pipeline for Deterministic Generation\nDESCRIPTION: Initializes a Stable Diffusion pipeline with float16 precision. The code loads the model from the 'stable-diffusion-v1-5' checkpoint and moves it to the CUDA device for GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import make_image_grid\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n)\npipeline = pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Generation Pipeline\nDESCRIPTION: Implementation of the IF pipeline for text-guided image-to-image translation using specialized pipeline classes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import IFImg2ImgPipeline, IFImg2ImgSuperResolutionPipeline, DiffusionPipeline\nfrom diffusers.utils import pt_to_pil, load_image, make_image_grid\nimport torch\n\n# download image\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\noriginal_image = load_image(url)\noriginal_image = original_image.resize((768, 512))\n\n# stage 1\nstage_1 = IFImg2ImgPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_model_cpu_offload()\n\n# stage 2\nstage_2 = IFImg2ImgSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_model_cpu_offload()\n\n# stage 3\nsafety_modules = {\n    \"feature_extractor\": stage_1.feature_extractor,\n    \"safety_checker\": stage_1.safety_checker,\n    \"watermarker\": stage_1.watermarker,\n}\nstage_3 = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16\n)\nstage_3.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape in style minecraft\"\ngenerator = torch.manual_seed(1)\n\n# text embeds\nprompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)\n\n# stage 1\nstage_1_output = stage_1(\n    image=original_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\n\n# stage 2\nstage_2_output = stage_2(\n    image=stage_1_output,\n    original_image=original_image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    generator=generator,\n    output_type=\"pt\",\n).images\n\n# stage 3\nstage_3_output = stage_3(prompt=prompt, image=stage_2_output, generator=generator, noise_level=100).images\n\nmake_image_grid([original_image, pt_to_pil(stage_1_output)[0], pt_to_pil(stage_2_output)[0], stage_3_output[0]], rows=1, rows=4)\n```\n\n----------------------------------------\n\nTITLE: CycleDiffusion using Stable Diffusion and DDIM Scheduler in Python\nDESCRIPTION: This extensive code snippet demonstrates how to use CycleDiffusion with Stable Diffusion and the DDIM scheduler. It includes downloading an initial image, specifying prompts, and generating transformed images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom diffusers import CycleDiffusionPipeline, DDIMScheduler\n\n\n# load the scheduler. CycleDiffusion only supports stochastic schedulers.\n\n# load the pipeline\n# make sure you're logged in with `huggingface-cli login`\nmodel_id_or_path = \"CompVis/stable-diffusion-v1-4\"\nscheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\npipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\")\n\n# let's download an initial image\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png\"\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((512, 512))\ninit_image.save(\"horse.png\")\n\n# let's specify a prompt\nsource_prompt = \"An astronaut riding a horse\"\nprompt = \"An astronaut riding an elephant\"\n\n# call the pipeline\nimage = pipe(\n    prompt=prompt,\n    source_prompt=source_prompt,\n    image=init_image,\n    num_inference_steps=100,\n    eta=0.1,\n    strength=0.8,\n    guidance_scale=2,\n    source_guidance_scale=1,\n).images[0]\n\nimage.save(\"horse_to_elephant.png\")\n\n# let's try another example\n# See more samples at the original repo: https://github.com/ChenWu98/cycle-diffusion\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png\"\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((512, 512))\ninit_image.save(\"black.png\")\n\nsource_prompt = \"A black colored car\"\nprompt = \"A blue colored car\"\n\n# call the pipeline\ntorch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    source_prompt=source_prompt,\n    image=init_image,\n    num_inference_steps=100,\n    eta=0.1,\n    strength=0.85,\n    guidance_scale=3,\n    source_guidance_scale=1,\n).images[0]\n\nimage.save(\"black_to_blue.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating Image with TCD-LoRA and ControlNet in Python\nDESCRIPTION: This code generates an image using the prepared Stable Diffusion XL pipeline with TCD-LoRA and ControlNet. It specifies the prompt, number of inference steps, and other parameters for the image generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_tcd_lora.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A vibrant, colorful landscape with mountains and a lake\"\n\nimage = pipe(\n    prompt=prompt,\n    image=control_image,\n    num_inference_steps=8,\n    guidance_scale=0,\n    eta=0.3,\n    generator=torch.Generator(device=device).manual_seed(0),\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Generating Image Batch with Enhanced Prompt in Python\nDESCRIPTION: Uses the diffusion pipeline to generate a batch of 8 images with the enhanced prompt and displays them in a grid layout with 2 rows and 4 columns.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimages = pipeline(**get_inputs(batch_size=8)).images\nmake_image_grid(images, rows=2, cols=4)\n```\n\n----------------------------------------\n\nTITLE: Loading A1111 LoRA Weights into Diffusers Pipeline\nDESCRIPTION: Code to load A1111 LoRA weights into a Diffusers pipeline, enabling the use of specialized fine-tuned models with the Diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline.load_lora_weights(\".\", weight_name=\"howls_moving_castle.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: VQGAN Model Configuration\nDESCRIPTION: JSON configuration for a VQGAN model, specifying architecture details such as block types, channels, normalization, and embedding dimensions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/vqgan/README.md#2025-04-11_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"_class_name\": \"VQModel\",\n  \"_diffusers_version\": \"0.17.0.dev0\",\n  \"act_fn\": \"silu\",\n  \"block_out_channels\": [\n    128,\n    256,\n    256,\n    512\n  ],\n  \"down_block_types\": [\n    \"DownEncoderBlock2D\",\n    \"DownEncoderBlock2D\",\n    \"DownEncoderBlock2D\",\n    \"AttnDownEncoderBlock2D\"\n  ],\n  \"in_channels\": 3,\n  \"latent_channels\": 4,\n  \"layers_per_block\": 2,\n  \"norm_num_groups\": 32,\n  \"norm_type\": \"spatial\",\n  \"num_vq_embeddings\": 16384,\n  \"out_channels\": 3,\n  \"sample_size\": 32,\n  \"scaling_factor\": 0.18215,\n  \"up_block_types\": [\n    \"AttnUpDecoderBlock2D\",\n    \"UpDecoderBlock2D\",\n    \"UpDecoderBlock2D\",\n    \"UpDecoderBlock2D\"\n  ],\n  \"vq_embed_dim\": 4\n}\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with a LoRA Adapter\nDESCRIPTION: Performs inference using the toy-face LoRA adapter with a specified scale (0.9) to generate an image of a toy-face hacker with a hoodie.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"toy_face of a hacker with a hoodie\"\n\nlora_scale = 0.9\nimage = pipe(\n    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=torch.manual_seed(0)\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pipeline for Scheduler Comparison\nDESCRIPTION: Initializes pipeline with specific prompt and generator seed for comparing different schedulers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/schedulers.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\nprompt = \"A photograph of an astronaut riding a horse on Mars, high resolution, high definition.\"\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusion3PipelineOutput in Python\nDESCRIPTION: This code imports the StableDiffusion3PipelineOutput class, which represents the output format for Stable Diffusion 3 pipelines. It is used to structure and type the results returned by the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet_sd3.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.stable_diffusion_3.pipeline_output import StableDiffusion3PipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Using AYS Sampling Schedule in Python\nDESCRIPTION: Demonstrates how to use the Align Your Steps (AYS) sampling schedule with a Stable Diffusion XL pipeline for optimized 10-step generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/scheduler_features.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers import AysSchedules\n\nsampling_schedule = AysSchedules[\"StableDiffusionXLTimesteps\"]\nprint(sampling_schedule)\n\"[999, 845, 730, 587, 443, 310, 193, 116, 53, 13]\"\n```\n\n----------------------------------------\n\nTITLE: Using LoRAHub's MultiLoRA Weights\nDESCRIPTION: This code snippet shows how to use LoRAHub's MultiLoRA approach which combines multiple LoRA adapters. It creates a DiffusionPipeline with the stabilityai/stable-diffusion-xl-base-1.0 model and applies the MultiLoRA weights from different concepts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\npipeline.load_lora_weights(\n    \"multilorahubtesting/SDXL-DreamTraps\",\n    weight_name=\"manifest.safetensors\",\n    adapter_name=\"dreamtraps\"\n)\n\npipeline.set_adapters([\"dreamtraps\"], adapter_weights=[1.0])\n\nprompt = \"dreamtraps style, A cinematic shot of a baby racoon wearing an intricate italian priest robe\"\nimage = pipeline(prompt=prompt, num_inference_steps=30).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Spatiotemporal Skip Guidance Example in Python\nDESCRIPTION: This example illustrates how to apply Spatiotemporal Skip Guidance for video diffusion sampling, which selectively skips certain layers to boost quality. The snippet includes pipeline loading, configuration parameters, and the video generation process with specified properties.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom pipeline_stg_mochi import MochiSTGPipeline\nfrom diffusers.utils import export_to_video\n\n# Load the pipeline\npipe = MochiSTGPipeline.from_pretrained(\"genmo/mochi-1-preview\", variant=\"bf16\", torch_dtype=torch.bfloat16)\n\n# Enable memory savings\npipe = pipe.to(\"cuda\")\n\n#--------Option--------#\nprompt = \"A close-up of a beautiful woman's face with colored powder exploding around her, creating an abstract splash of vibrant hues, realistic style.\"\nstg_applied_layers_idx = [34]\nstg_scale = 1.0 # 0.0 for CFG\n#----------------------#\n\n# Generate video frames\nframes = pipe(\n    prompt, \n    height=480,\n    width=480,\n    num_frames=81,\n    stg_applied_layers_idx=stg_applied_layers_idx,\n    stg_scale=stg_scale,\n    generator = torch.Generator().manual_seed(42),\n    do_rescaling=do_rescaling,\n).frames[0]\n\nexport_to_video(frames, \"output.mp4\", fps=30)\n```\n\n----------------------------------------\n\nTITLE: Implementing CPU Offloading for Memory Reduction\nDESCRIPTION: Demonstrates CPU offloading technique that reduces memory consumption to less than 3GB by moving weights between CPU and GPU as needed during forward pass.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\npipe.enable_sequential_cpu_offload()\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Computing FID Score\nDESCRIPTION: Calculates the Fréchet Inception Distance between real and generated images using torchmetrics.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom torchmetrics.image.fid import FrechetInceptionDistance\n\nfid = FrechetInceptionDistance(normalize=True)\nfid.update(real_images, real=True)\nfid.update(fake_images, real=False)\n\nprint(f\"FID: {float(fid.compute())}\")\n```\n\n----------------------------------------\n\nTITLE: Launching Textual Inversion with Custom Gradient Accumulation\nDESCRIPTION: Example command showing how to launch the textual inversion script with custom gradient accumulation steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch textual_inversion.py \\\n  --gradient_accumulation_steps=4\n```\n\n----------------------------------------\n\nTITLE: Applying Group Offloading in Diffusers (PyTorch)\nDESCRIPTION: Demonstrates how to use `apply_group_offloading` to offload parts of a Diffusers pipeline to the CPU or disk to reduce GPU memory usage.  This function supports block-level and leaf-level offloading, configurable by the `offload_type` parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n\"apply_group_offloading(pipe.text_encoder, onload_device=onload_device, offload_type=\\\"block_level\\\", num_blocks_per_group=2)\napply_group_offloading(pipe.vae, onload_device=onload_device, offload_type=\\\"leaf_level\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Using Imagic Stable Diffusion for Image Editing in Python\nDESCRIPTION: Demonstrates Imagic Stable Diffusion which allows for editing an existing image using text prompts. The example shows how to load an image, train the model on it, and generate variations with different alpha values.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport torch\nimport os\nfrom diffusers import DiffusionPipeline, DDIMScheduler\n\nhas_cuda = torch.cuda.is_available()\ndevice = torch.device('cpu' if not has_cuda else 'cuda')\npipe = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    safety_checker=None,\n    custom_pipeline=\"imagic_stable_diffusion\",\n    scheduler=DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n).to(device)\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\nseed = 0\nprompt = \"A photo of Barack Obama smiling with a big grin\"\nurl = 'https://www.dropbox.com/s/6tlwzr73jd1r9yk/obama.png?dl=1'\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((512, 512))\nres = pipe.train(\n    prompt,\n    image=init_image,\n    generator=generator)\nres = pipe(alpha=1, guidance_scale=7.5, num_inference_steps=50)\nos.makedirs(\"imagic\", exist_ok=True)\nimage = res.images[0]\nimage.save('./imagic/imagic_image_alpha_1.png')\nres = pipe(alpha=1.5, guidance_scale=7.5, num_inference_steps=50)\nimage = res.images[0]\nimage.save('./imagic/imagic_image_alpha_1_5.png')\nres = pipe(alpha=2, guidance_scale=7.5, num_inference_steps=50)\nimage = res.images[0]\nimage.save('./imagic/imagic_image_alpha_2.png')\n```\n\n----------------------------------------\n\nTITLE: Setting Source and Target Prompts for DiffEdit\nDESCRIPTION: Defining the source prompt describing the current image content and target prompt describing the desired edit.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsource_prompt = \"a bowl of fruits\"\ntarget_prompt = \"a bowl of pears\"\n```\n\n----------------------------------------\n\nTITLE: Inpainting Pipeline Setup\nDESCRIPTION: Initial setup code for the IF inpainting pipeline, showing how to load images and initialize the first stage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import IFInpaintingPipeline, IFInpaintingSuperResolutionPipeline, DiffusionPipeline\nfrom diffusers.utils import pt_to_pil, load_image, make_image_grid\nimport torch\n\n# download image\nurl = \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/if/person.png\"\noriginal_image = load_image(url)\n\n# download mask\nurl = \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/if/glasses_mask.png\"\nmask_image = load_image(url)\n\n# stage 1\nstage_1 = IFInpaintingPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Performing Scheduler Step\nDESCRIPTION: Demonstrates the use of the step function in schedulers, which takes the predicted model output and current sample to produce the next, slightly more denoised sample in the diffusion process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md#2025-04-11_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nscheduler.step(model_output, current_sample)\n```\n\n----------------------------------------\n\nTITLE: Predicting Noise Residual with Diffusion Model\nDESCRIPTION: Passes a noisy sample to the model along with a timestep to predict the noise residual. The model uses the timestep to determine its position in the diffusion process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> with torch.no_grad():\n...     noisy_residual = model(sample=noisy_sample, timestep=2).sample\n```\n\n----------------------------------------\n\nTITLE: Creating Image Display Function\nDESCRIPTION: Defines a utility function to process tensor data into viewable images, converting the tensor to a PIL Image for display.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> import PIL.Image\n>>> import numpy as np\n\n\n>>> def display_sample(sample, i):\n...     image_processed = sample.cpu().permute(0, 2, 3, 1)\n...     image_processed = (image_processed + 1.0) * 127.5\n...     image_processed = image_processed.numpy().astype(np.uint8)\n\n...     image_pil = PIL.Image.fromarray(image_processed[0])\n...     display(f\"Image at step {i}\")\n...     display(image_pil)\n```\n\n----------------------------------------\n\nTITLE: Using Stable Diffusion with K Diffusion in Python\nDESCRIPTION: This code demonstrates how to use the Stable Diffusion pipeline with K Diffusion. It sets up the pipeline, generates an image from a prompt, and saves the result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", custom_pipeline=\"sd_text2img_k_diffusion\")\npipe = pipe.to(\"cuda\")\n\nprompt = \"an astronaut riding a horse on mars\"\npipe.set_scheduler(\"sample_heun\")\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\nimage = pipe(prompt, generator=generator, num_inference_steps=20).images[0]\n\nimage.save(\"./astronaut_heun_k_diffusion.png\")\n```\n\n----------------------------------------\n\nTITLE: Launching the Diffusers Server\nDESCRIPTION: Command to start the server application that handles image generation requests.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/server/README.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython server.py\n```\n\n----------------------------------------\n\nTITLE: Unfusing LoRA Weights\nDESCRIPTION: Shows how to unfuse LoRA weights to restore the original model weights, which only works with single LoRA fusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipeline.unfuse_lora()\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion Mega Pipeline Implementation\nDESCRIPTION: Unified pipeline that combines text-to-image, image-to-image, and inpainting capabilities in a single class. Provides a comprehensive interface for various Stable Diffusion operations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python3\nfrom diffusers import DiffusionPipeline\nimport PIL\nimport requests\nfrom io import BytesIO\nimport torch\n\n\ndef download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", custom_pipeline=\"stable_diffusion_mega\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\npipe.enable_attention_slicing()\n\n\n### Text-to-Image\nimages = pipe.text2img(\"An astronaut riding a horse\").images\n\n### Image-to-Image\ninit_image = download_image(\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\")\n\nprompt = \"A fantasy landscape, trending on artstation\"\n\nimages = pipe.img2img(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n\n### Inpainting\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\ninit_image = download_image(img_url).resize((512, 512))\nmask_image = download_image(mask_url).resize((512, 512))\n\nprompt = \"a cat sitting on a bench\"\nimages = pipe.inpaint(prompt=prompt, image=init_image, mask_image=mask_image, strength=0.75).images\n```\n\n----------------------------------------\n\nTITLE: Inference with Checkpoint During Training\nDESCRIPTION: Code for running inference with a partially trained DreamBooth model checkpoint. Loads components individually from the checkpoint directory and creates a pipeline for generating images before training completes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel\nfrom transformers import CLIPTextModel\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\"path/to/model/checkpoint-100/unet\")\n\n# if you have trained with `--args.train_text_encoder` make sure to also load the text encoder\ntext_encoder = CLIPTextModel.from_pretrained(\"path/to/model/checkpoint-100/checkpoint-100/text_encoder\")\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", unet=unet, text_encoder=text_encoder, dtype=torch.float16,\n).to(\"cuda\")\n\nimage = pipeline(\"A photo of sks dog in a bucket\", num_inference_steps=50, guidance_scale=7.5).images[0]\nimage.save(\"dog-bucket.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading Pipeline for Embedding Generation\nDESCRIPTION: Initializing the StableDiffusionDiffEditPipeline specifically for computing text embeddings from the generated prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionDiffEditPipeline\n\npipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16, use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\npipeline.enable_vae_slicing()\n\n@torch.no_grad()\ndef embed_prompts(sentences, tokenizer, text_encoder, device=\"cuda\"):\n    embeddings = []\n    for sent in sentences:\n        text_inputs = tokenizer(\n            sent,\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=None)[0]\n        embeddings.append(prompt_embeds)\n    return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)\n\nsource_embeds = embed_prompts(source_prompts, pipeline.tokenizer, pipeline.text_encoder)\ntarget_embeds = embed_prompts(target_prompts, pipeline.tokenizer, pipeline.text_encoder)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion Reference Pipeline in Python\nDESCRIPTION: Sets up a Stable Diffusion pipeline with reference control using UniPCMultistepScheduler. Configures both attention and AdaIN reference controls with float16 precision on CUDA.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import UniPCMultistepScheduler\nfrom diffusers.utils import load_image\n\ninput_image = load_image(\"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\")\n\npipe = StableDiffusionReferencePipeline.from_pretrained(\n       \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n       safety_checker=None,\n       torch_dtype=torch.float16\n       ).to('cuda:0')\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\nresult_img = pipe(ref_image=input_image,\n      prompt=\"1girl\",\n      num_inference_steps=20,\n      reference_attn=True,\n      reference_adain=True).images[0]\n```\n\n----------------------------------------\n\nTITLE: Optimizing DeepFloyd IF Pipeline for Speed in Python\nDESCRIPTION: Demonstrates techniques for improving inference speed with DeepFloyd IF, including moving all model components to GPU, reducing inference steps, using predefined timestep schedules, and applying torch.compile for additional performance gains.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n```\n\nLANGUAGE: python\nCODE:\n```\npipe(\"<prompt>\", num_inference_steps=30)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.deepfloyd_if import fast27_timesteps\n\npipe(\"<prompt>\", timesteps=fast27_timesteps)\n```\n\nLANGUAGE: python\nCODE:\n```\npipe = IFImg2ImgPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nimage = pipe(image=image, prompt=\"<prompt>\", strength=0.3).images\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\npipe.text_encoder = torch.compile(pipe.text_encoder, mode=\"reduce-overhead\", fullgraph=True)\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Basic Image Inpainting with Memory Optimization\nDESCRIPTION: Initial example showing basic image inpainting setup with xFormers memory optimization and image loading. Uses a base image and mask to generate an elven castle concept art.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\ngenerator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Loading LTXVideoTransformer3DModel from Pretrained Weights\nDESCRIPTION: Code snippet demonstrating how to load the LTXVideoTransformer3DModel from pretrained weights. The model is loaded with bfloat16 precision and moved to CUDA device for GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/ltx_video_transformer3d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import LTXVideoTransformer3DModel\n\ntransformer = LTXVideoTransformer3DModel.from_pretrained(\"Lightricks/LTX-Video\", subfolder=\"transformer\", torch_dtype=torch.bfloat16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Adjusting Strength Parameter in Inpainting Pipeline\nDESCRIPTION: This snippet demonstrates how to adjust the 'strength' parameter in the inpainting pipeline, which controls the amount of noise added to the base image and influences the similarity between the output and the base image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, strength=0.6).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Diffusers\nDESCRIPTION: This snippet clones the latest Diffusers library from Huggingface GitHub and installs its dependencies necessary for the project.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n%cd /content\n\n!git clone https://github.com/huggingface/diffusers.git\n!pip install -q /content/diffusers\n\n!pip install -q datasets transformers\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with UniDiffuser\nDESCRIPTION: Shows how to use UniDiffuser for text-to-image generation, where the model generates an image conditioned on a provided text prompt. This demonstrates sampling from the conditional image distribution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unidiffuser.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Text-to-image generation\nprompt = \"an elephant under the sea\"\n\nsample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)\nt2i_image = sample.images[0]\nt2i_image\n```\n\n----------------------------------------\n\nTITLE: Initial Outpainting Pipeline Setup and Generation\nDESCRIPTION: Setup for the SDXL ControlNet pipeline and function to generate the initial outpainted image using multiple control models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/advanced_inference/outpaint.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncontrolnets = [\n    ControlNetModel.from_pretrained(\n        \"destitech/controlnet-inpaint-dreamer-sdxl\", torch_dtype=torch.float16, variant=\"fp16\"\n    ),\n    ControlNetModel.from_pretrained(\n        \"diffusers/controlnet-zoe-depth-sdxl-1.0\", torch_dtype=torch.float16\n    ),\n]\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(\"cuda\")\npipeline = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"SG161222/RealVisXL_V4.0\", torch_dtype=torch.float16, variant=\"fp16\", controlnet=controlnets, vae=vae\n).to(\"cuda\")\n\ndef generate_image(prompt, negative_prompt, inpaint_image, zoe_image, seed: int = None):\n    if seed is None:\n        seed = random.randint(0, 2**32 - 1)\n\n    generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n\n    image = pipeline(\n        prompt,\n        negative_prompt=negative_prompt,\n        image=[inpaint_image, zoe_image],\n        guidance_scale=6.5,\n        num_inference_steps=25,\n        generator=generator,\n        controlnet_conditioning_scale=[0.5, 0.8],\n        control_guidance_end=[0.9, 0.6],\n    ).images[0]\n\n    return image\n\nprompt = \"nike air jordans on a basketball court\"\nnegative_prompt = \"\"\n\ntemp_image = generate_image(prompt, negative_prompt, white_bg_image, image_zoe, 908097)\n```\n\n----------------------------------------\n\nTITLE: Loading DDPM model and scheduler separately in Python\nDESCRIPTION: Shows how to load the UNet2DModel and DDPMScheduler components of a DDPM pipeline individually.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import DDPMScheduler, UNet2DModel\n\n>>> scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")\n>>> model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Converting Model Parameters to BFloat16\nDESCRIPTION: Code to convert model parameters to bfloat16 precision for faster computation while preserving scheduler state in original precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nscheduler_state = params.pop(\"scheduler\")\nparams = jax.tree_util.tree_map(lambda x: x.astype(jnp.bfloat16), params)\nparams[\"scheduler\"] = scheduler_state\n```\n\n----------------------------------------\n\nTITLE: Adding Seamless Tiling to Stable Diffusion for Texture Generation\nDESCRIPTION: This script adds asymmetric tiling functionality to Stable Diffusion pipelines, allowing for the generation of seamless textures. It works by modifying the convolution layers' padding mode to use circular padding, which creates textures that can be tiled without visible seams. The implementation supports toggling x and y axis tiling independently.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README_community_scripts.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import Optional\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.models.lora import LoRACompatibleConv\n\ndef seamless_tiling(pipeline, x_axis, y_axis):\n    def asymmetric_conv2d_convforward(self, input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None):\n        self.paddingX = (self._reversed_padding_repeated_twice[0], self._reversed_padding_repeated_twice[1], 0, 0)\n        self.paddingY = (0, 0, self._reversed_padding_repeated_twice[2], self._reversed_padding_repeated_twice[3])\n        working = torch.nn.functional.pad(input, self.paddingX, mode=x_mode)\n        working = torch.nn.functional.pad(working, self.paddingY, mode=y_mode)\n        return torch.nn.functional.conv2d(working, weight, bias, self.stride, torch.nn.modules.utils._pair(0), self.dilation, self.groups)\n    x_mode = 'circular' if x_axis else 'constant'\n    y_mode = 'circular' if y_axis else 'constant'\n    targets = [pipeline.vae, pipeline.text_encoder, pipeline.unet]\n    convolution_layers = []\n    for target in targets:\n        for module in target.modules():\n            if isinstance(module, torch.nn.Conv2d):\n                convolution_layers.append(module)\n    for layer in convolution_layers:\n        if isinstance(layer, LoRACompatibleConv) and layer.lora_layer is None:\n            layer.lora_layer = lambda * x: 0\n        layer._conv_forward = asymmetric_conv2d_convforward.__get__(layer, torch.nn.Conv2d)\n    return pipeline\n\npipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True)\npipeline.enable_model_cpu_offload()\nprompt = [\"texture of a red brick wall\"]\nseed = 123456\ngenerator = torch.Generator(device='cuda').manual_seed(seed)\n\npipeline = seamless_tiling(pipeline=pipeline, x_axis=True, y_axis=True)\nimage = pipeline(\n    prompt=prompt,\n    width=512,\n    height=512,\n    num_inference_steps=20,\n    guidance_scale=7,\n    num_images_per_prompt=1,\n    generator=generator\n).images[0]\nseamless_tiling(pipeline=pipeline, x_axis=False, y_axis=False)\n\ntorch.cuda.empty_cache()\nimage.save('image.png')\n```\n\n----------------------------------------\n\nTITLE: Implementing IP Adapter Cutoff Callback in Diffusers\nDESCRIPTION: Sets up and applies the IPAdapterScaleCutoffCallback to disable the IP Adapter after a specified number of steps. The callback is configured to cut off at step 5 and is used during image generation with a tiger prompt and reference image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.callbacks import IPAdapterScaleCutoffCallback\nfrom diffusers.utils import load_image\nimport torch\n \n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", \n    torch_dtype=torch.float16\n).to(\"cuda\")\n\n\npipeline.load_ip_adapter(\n    \"h94/IP-Adapter\", \n    subfolder=\"sdxl_models\", \n    weight_name=\"ip-adapter_sdxl.bin\"\n)\n\npipeline.set_ip_adapter_scale(0.6)\n\n\ncallback = IPAdapterScaleCutoffCallback(\n    cutoff_step_ratio=None, \n    cutoff_step_index=5\n)\n\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\"\n)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(2628670641)\n\nimages = pipeline(\n    prompt=\"a tiger sitting in a chair drinking orange juice\",\n    ip_adapter_image=image,\n    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n    generator=generator,\n    num_inference_steps=50,\n    callback_on_step_end=callback,\n).images\n\nimages[0].save(\"custom_callback_img.png\")\n```\n\n----------------------------------------\n\nTITLE: Comparing Diffusers and K Diffusion Results in Python\nDESCRIPTION: This code compares the results of using Stable Diffusion with both Diffusers and K Diffusion. It sets up pipelines for both methods and generates images using the same prompt and seed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\n\nseed = 33\n\npipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\nimage = pipe(prompt, generator=generator, num_inference_steps=50).images[0]\n\n# K Diffusion\npipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", custom_pipeline=\"sd_text2img_k_diffusion\")\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\npipe.set_scheduler(\"sample_euler\")\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\nimage = pipe(prompt, generator=generator, num_inference_steps=50).images[0]\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Default PLMS Scheduler in Python\nDESCRIPTION: This code snippet shows how to perform text-to-image generation using the Stable Diffusion pipeline with the default PLMS scheduler. It requires logging in to Hugging Face CLI.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# make sure you're logged in with `huggingface-cli login`\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"astronaut_rides_horse.png\")\n```\n\n----------------------------------------\n\nTITLE: Downloading and Resizing Image for Kandinsky Image-to-Image\nDESCRIPTION: Downloads an image from a URL and resizes it to 768x512 pixels for use in image-to-image generation with Kandinsky models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\noriginal_image = load_image(url)\noriginal_image = original_image.resize((768, 512))\n```\n\n----------------------------------------\n\nTITLE: Initializing Matryoshka Diffusion Pipeline in Python\nDESCRIPTION: Sets up a Matryoshka Diffusion pipeline for multi-resolution image generation. Supports different nesting levels (0: 64x64, 1: 256x256, 2: 1024x1024) with recommended inference steps for each resolution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_108\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import make_image_grid\n\n# nesting_level=0 -> 64x64; nesting_level=1 -> 256x256 - 64x64; nesting_level=2 -> 1024x1024 - 256x256 - 64x64\npipe = DiffusionPipeline.from_pretrained(\"tolgacangoz/matryoshka-diffusion-models\",\n                                         nesting_level=0,\n                                         trust_remote_code=False,  # One needs to give permission for this code to run\n                                         ).to(\"cuda\")\n\nprompt0 = \"a blue jay stops on the top of a helmet of Japanese samurai, background with sakura tree\"\nprompt = f\"breathtaking {prompt0}. award-winning, professional, highly detailed\"\nimage = pipe(prompt, num_inference_steps=50).images\nmake_image_grid(image, rows=1, cols=len(image))\n\n# pipe.change_nesting_level(<int>)  # 0, 1, or 2\n# 50+, 100+, and 250+ num_inference_steps are recommended for nesting levels 0, 1, and 2 respectively.\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion Prompt Scheduling Callback\nDESCRIPTION: Defines a callback class for scheduling prompts during Stable Diffusion generation. The callback allows changing prompts at specific steps during the generation process by manipulating prompt embeddings. Includes pipeline setup and example usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README_community_scripts.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.callbacks import PipelineCallback, MultiPipelineCallbacks\nfrom diffusers.configuration_utils import register_to_config\nimport torch\nfrom typing import Any, Dict, Tuple, Union\n\n\nclass SDPromptSchedulingCallback(PipelineCallback):\n    @register_to_config\n    def __init__(\n        self,\n        encoded_prompt: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n        cutoff_step_ratio=None,\n        cutoff_step_index=None,\n    ):\n        super().__init__(\n            cutoff_step_ratio=cutoff_step_ratio, cutoff_step_index=cutoff_step_index\n        )\n\n    tensor_inputs = [\"prompt_embeds\"]\n\n    def callback_fn(\n        self, pipeline, step_index, timestep, callback_kwargs\n    ) -> Dict[str, Any]:\n        cutoff_step_ratio = self.config.cutoff_step_ratio\n        cutoff_step_index = self.config.cutoff_step_index\n        if isinstance(self.config.encoded_prompt, tuple):\n            prompt_embeds, negative_prompt_embeds = self.config.encoded_prompt\n        else:\n            prompt_embeds = self.config.encoded_prompt\n\n        # Use cutoff_step_index if it's not None, otherwise use cutoff_step_ratio\n        cutoff_step = (\n            cutoff_step_index\n            if cutoff_step_index is not None\n            else int(pipeline.num_timesteps * cutoff_step_ratio)\n        )\n\n        if step_index == cutoff_step:\n            if pipeline.do_classifier_free_guidance:\n                prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n            callback_kwargs[self.tensor_inputs[0]] = prompt_embeds\n        return callback_kwargs\n\n\npipeline: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n).to(\"cuda\")\npipeline.safety_checker = None\npipeline.requires_safety_checker = False\n\ncallback = MultiPipelineCallbacks(\n    [\n        SDPromptSchedulingCallback(\n            encoded_prompt=pipeline.encode_prompt(\n                prompt=f\"prompt {index}\",\n                negative_prompt=f\"negative prompt {index}\",\n                device=pipeline._execution_device,\n                num_images_per_prompt=1,\n                # pipeline.do_classifier_free_guidance can't be accessed until after pipeline is ran\n                do_classifier_free_guidance=True,\n            ),\n            cutoff_step_index=index,\n        ) for index in range(1, 20)\n    ]\n)\n\nimage = pipeline(\n    prompt=\"prompt\"\n    negative_prompt=\"negative prompt\",\n    callback_on_step_end=callback,\n    callback_on_step_end_tensor_inputs=[\"prompt_embeds\"],\n).images[0]\ntorch.cuda.empty_cache()\nimage.save('image.png')\n```\n\n----------------------------------------\n\nTITLE: Replicating Parameters and Sharding Inputs for TPU\nDESCRIPTION: Replicates model parameters and shards input data across TPU devices for parallel processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\np_params = replicate(params)\nprompt_ids = shard(prompt_ids)\nprompt_ids.shape\n```\n\n----------------------------------------\n\nTITLE: Setting Up Generator for Reproducibility\nDESCRIPTION: Creates a PyTorch Generator with a fixed seed to ensure reproducible results across multiple runs of the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion MoD ControlNet Tile SR Pipeline for SDXL\nDESCRIPTION: This code snippet sets up and uses a Stable Diffusion MoD (Mixture-of-Diffusers) ControlNet Tile SR Pipeline for SDXL. It combines tiled diffusion with SDXL's ControlNet Tile process to generate super-resolution images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, ControlNetUnionModel, AutoencoderKL, UniPCMultistepScheduler, UNet2DConditionModel\nfrom diffusers.utils import load_image\nfrom PIL import Image\n\ndevice = \"cuda\"\n\n# Initialize the models and pipeline\ncontrolnet = ControlNetUnionModel.from_pretrained(\n    \"brad-twinkl/controlnet-union-sdxl-1.0-promax\", torch_dtype=torch.float16\n).to(device=device)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(device=device)\n\nmodel_id = \"SG161222/RealVisXL_V5.0\"\npipe = DiffusionPipeline.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    vae=vae,\n    controlnet=controlnet,\n    custom_pipeline=\"mod_controlnet_tile_sr_sdxl\",    \n    use_safetensors=True,\n    variant=\"fp16\",\n).to(device)\n\nunet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", variant=\"fp16\", use_safetensors=True)\n\n#pipe.enable_model_cpu_offload()  # << Enable this if you have limited VRAM\npipe.enable_vae_tiling() # << Enable this if you have limited VRAM\npipe.enable_vae_slicing() # << Enable this if you have limited VRAM\n\n# Set selected scheduler\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\n# Load image\ncontrol_image = load_image(\"https://huggingface.co/datasets/DEVAIEXP/assets/resolve/main/1.jpg\")\noriginal_height = control_image.height\noriginal_width = control_image.width\nprint(f\"Current resolution: H:{original_height} x W:{original_width}\")\n\n# Pre-upscale image for tiling\nresolution = 4096\ntile_gaussian_sigma = 0.3\nmax_tile_size = 1024 # or 1280\n\ncurrent_size = max(control_image.size)\nscale_factor = max(2, resolution / current_size)\nnew_size = (int(control_image.width * scale_factor), int(control_image.height * scale_factor))\nimage = control_image.resize(new_size, Image.LANCZOS)\n\n# Update target height and width\ntarget_height = image.height\ntarget_width = image.width\nprint(f\"Target resolution: H:{target_height} x W:{target_width}\")\n\n# Calculate overlap size\nnormal_tile_overlap, border_tile_overlap = pipe.calculate_overlap(target_width, target_height)\n\n# Set other params\ntile_weighting_method = pipe.TileWeightingMethod.COSINE.value\nguidance_scale = 4\nnum_inference_steps = 35\ndenoising_strenght = 0.65\ncontrolnet_strength = 1.0\nprompt = \"high-quality, noise-free edges, high quality, 4k, hd, 8k\"\nnegative_prompt = \"blurry, pixelated, noisy, low resolution, artifacts, poor details\"\n\n# Image generation\ngenerated_image = pipe(\n    image=image,\n    control_image=control_image,\n    control_mode=[6],\n    controlnet_conditioning_scale=float(controlnet_strength),\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    normal_tile_overlap=normal_tile_overlap,\n    border_tile_overlap=border_tile_overlap,\n    height=target_height,\n    width=target_width,\n    original_size=(original_width, original_height),\n    target_size=(target_width, target_height),\n    guidance_scale=guidance_scale,        \n    strength=float(denoising_strenght),\n    tile_weighting_method=tile_weighting_method,\n    max_tile_size=max_tile_size,\n    tile_gaussian_sigma=float(tile_gaussian_sigma),\n    num_inference_steps=num_inference_steps,\n)[\"images\"][0]\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Trained FLUX.1 Model\nDESCRIPTION: Performs inference with the trained FLUX.1 model, using the custom style tokens to generate an image with the learned style applied to a new concept.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninstance_token = \"<s0><s1>\"\nprompt = f\"a {instance_token} icon of an orange llama eating ramen, in the style of {instance_token}\"\n\nimage = pipe(prompt=prompt, num_inference_steps=25, cross_attention_kwargs={\"scale\": 1.0}).images[0]\nimage.save(\"llama.png\")\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Video Generation with Mochi Pipeline\nDESCRIPTION: Shows how to split the Mochi transformer across multiple GPUs using device mapping and memory allocation configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/mochi.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MochiPipeline, MochiTransformer3DModel\nfrom diffusers.utils import export_to_video\n\nmodel_id = \"genmo/mochi-1-preview\"\ntransformer = MochiTransformer3DModel.from_pretrained(\n    model_id,\n    subfolder=\"transformer\",\n    device_map=\"auto\",\n    max_memory={0: \"24GB\", 1: \"24GB\"}\n)\n\npipe = MochiPipeline.from_pretrained(model_id,  transformer=transformer)\npipe.enable_model_cpu_offload()\npipe.enable_vae_tiling()\n\nwith torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, cache_enabled=False):\n    frames = pipe(\n        prompt=\"Close-up of a chameleon's eye, with its scaly skin changing color. Ultra high resolution 4k.\",\n        negative_prompt=\"\",\n        height=480,\n        width=848,\n        num_frames=85,\n        num_inference_steps=50,\n        guidance_scale=4.5,\n        num_videos_per_prompt=1,\n        generator=torch.Generator(device=\"cuda\").manual_seed(0),\n        max_sequence_length=256,\n        output_type=\"pil\",\n    ).frames[0]\n\nexport_to_video(frames, \"output.mp4\", fps=30)\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with Stable Diffusion ONNX Pipeline\nDESCRIPTION: Demonstrates how to load a Stable Diffusion model, convert it to ONNX format, and run inference using the ORTStableDiffusionPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/onnx.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.onnxruntime import ORTStableDiffusionPipeline\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipeline = ORTStableDiffusionPipeline.from_pretrained(model_id, export=True)\nprompt = \"sailing ship in storm by Leonardo da Vinci\"\nimage = pipeline(prompt).images[0]\npipeline.save_pretrained(\"./onnx-stable-diffusion-v1-5\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DiffusionPipeline from Pretrained Model in Python\nDESCRIPTION: Creates a DiffusionPipeline instance by loading a pre-trained DDPM model that generates butterfly images. This code downloads all necessary modeling, tokenization, and scheduling components from the specified model checkpoint.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/unconditional_image_generation.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import DiffusionPipeline\n\n>>> generator = DiffusionPipeline.from_pretrained(\"anton-l/ddpm-butterflies-128\")\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Weights for Inference with FLUX.1\nDESCRIPTION: Loads the trained LoRA weights from Hugging Face Hub into a FLUX.1 pipeline for inference. This is the first step in the inference process when using pivotal tuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom huggingface_hub import hf_hub_download, upload_file\nfrom diffusers import AutoPipelineForText2Image\nfrom safetensors.torch import load_file\n\nusername = \"linoyts\"\nrepo_id = f\"{username}/3d-icon-Flux-LoRA\"\n\npipe = AutoPipelineForText2Image.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to('cuda')\n\n\npipe.load_lora_weights(repo_id, weight_name=\"pytorch_lora_weights.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: Loading BLIP Model and Processor for Image Captioning in Python\nDESCRIPTION: Imports and initializes the BLIP model and processor from Hugging Face Transformers library for image captioning. The model is loaded with float16 precision to reduce memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import BlipForConditionalGeneration, BlipProcessor\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n```\n\n----------------------------------------\n\nTITLE: Passing T2I-Adapter Conditioning in Training Loop\nDESCRIPTION: This Python code shows how the T2I-Adapter's conditioning image and text embeddings are passed to the UNet during the training loop. The T2I-Adapter processes the conditioning image, and the resulting residuals are used in the UNet to guide image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"t2iadapter_image = batch[\\\"conditioning_pixel_values\\\"].to(dtype=weight_dtype)\ndown_block_additional_residuals = t2iadapter(t2iadapter_image)\ndown_block_additional_residuals = [\n    sample.to(dtype=weight_dtype) for sample in down_block_additional_residuals\n]\n\nmodel_pred = unet(\n    inp_noisy_latents,\n    timesteps,\n    encoder_hidden_states=batch[\\\"prompt_ids\\\"],\n    added_cond_kwargs=batch[\\\"unet_added_conditions\\\"],\n    down_block_additional_residuals=down_block_additional_residuals,\n).sample\"\n```\n\n----------------------------------------\n\nTITLE: Image-to-3D Generation with ShapEImg2ImgPipeline\nDESCRIPTION: Code for converting 2D images to 3D objects using the ShapEImg2ImgPipeline. This example loads a pre-existing image, processes it through the pipeline, and exports the result as an animated GIF.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom diffusers import ShapEImg2ImgPipeline\nfrom diffusers.utils import export_to_gif\n\npipe = ShapEImg2ImgPipeline.from_pretrained(\"openai/shap-e-img2img\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nguidance_scale = 3.0\nimage = Image.open(\"burger.png\").resize((256, 256))\n\nimages = pipe(\n    image,\n    guidance_scale=guidance_scale,\n    num_inference_steps=64,\n    frame_size=256,\n).images\n\ngif_path = export_to_gif(images[0], \"burger_3d.gif\")\n```\n\n----------------------------------------\n\nTITLE: Enabling CPU Offloading for Kandinsky\nDESCRIPTION: Shows how to enable model CPU offloading to avoid out-of-memory errors when working with memory-intensive Kandinsky models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Loading ControlNetUnionModel and StableDiffusionXLControlNetUnionPipeline in Python\nDESCRIPTION: This snippet demonstrates how to load the ControlNetUnionModel and use it with the StableDiffusionXLControlNetUnionPipeline. It loads the pretrained ControlNetUnionModel and initializes the pipeline with it.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/controlnet_union.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLControlNetUnionPipeline, ControlNetUnionModel\n\ncontrolnet = ControlNetUnionModel.from_pretrained(\"xinsir/controlnet-union-sdxl-1.0\")\npipe = StableDiffusionXLControlNetUnionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet)\n```\n\n----------------------------------------\n\nTITLE: Applying FreeU to Stable Diffusion XL in Python\nDESCRIPTION: This snippet shows how to use FreeU with the Stable Diffusion XL model. It sets up the pipeline, enables FreeU with specific parameters, and generates an image of a squirrel eating a burger.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/image_quality.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16,\n).to(\"cuda\")\npipeline.enable_freeu(s1=0.9, s2=0.2, b1=1.3, b2=1.4)\ngenerator = torch.Generator(device=\"cpu\").manual_seed(13)\nprompt = \"A squirrel eating a burger\"\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Refining Inpainted Image Using SDXL Refiner in Python\nDESCRIPTION: This snippet applies the Stable Diffusion XL refiner to enhance the inpainted image. It keeps the output in latent space (setting output_type=\"latent\") to avoid unnecessary decode-encode operations when chaining with other pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\nimage = pipeline(prompt=prompt, image=image_inpainting, mask_image=mask_image, output_type=\"latent\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for SDXL Training\nDESCRIPTION: Sets up environment variables for the model name and dataset identifier used in InstructPix2Pix SDXL training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README_sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport DATASET_ID=\"fusing/instructpix2pix-1000-samples\"\n```\n\n----------------------------------------\n\nTITLE: MarigoldDepthPipeline API Call Example\nDESCRIPTION: Example of the MarigoldDepthPipeline __call__ method signature showing depth prediction pipeline usage\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nMarigoldDepthPipeline.__call__(\n    image,\n    num_inference_steps=None,\n    generator=None,\n    output_type=\"pil\",\n    return_dict=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading DDIM Scheduler\nDESCRIPTION: Demonstrates loading a DDIM scheduler from pretrained configuration and integrating it into the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/schedulers.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMScheduler, DiffusionPipeline\n\nddim = DDIMScheduler.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n```\n\n----------------------------------------\n\nTITLE: Generating Initial Random Noise\nDESCRIPTION: Creates the initial random noise tensor for the diffusion process, with dimensions adjusted for the VAE model's downsampling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlatents = torch.randn(\n    (batch_size, unet.config.in_channels, height // 8, width // 8),\n    generator=generator,\n    device=torch_device,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding ControlNet to Pipeline\nDESCRIPTION: Shows how to integrate a ControlNet model into a StableDiffusion pipeline configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n\nckpt_path = \"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.safetensors\"\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_canny\")\npipeline = StableDiffusionControlNetPipeline.from_single_file(ckpt_path, controlnet=controlnet)\n```\n\n----------------------------------------\n\nTITLE: Loading a UNet2D Model in Python\nDESCRIPTION: Loads a pre-trained UNet2D diffusion model from the Google repository for generating cat images. The model is loaded using the from_pretrained method with safetensors enabled.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import UNet2DModel\n\n>>> repo_id = \"google/ddpm-cat-256\"\n>>> model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Converting SDXL Latents to RGB Images in Diffusers\nDESCRIPTION: Implements a function to convert the 4-channel latent space of SDXL to RGB tensors. This function transforms the latent representation into viewable images that can be displayed during the generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef latents_to_rgb(latents):\n    weights = (\n        (60, -60, 25, -70),\n        (60,  -5, 15, -50),\n        (60,  10, -5, -35),\n    )\n\n    weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))\n    biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(latents.device)\n    rgb_tensor = torch.einsum(\"...lxy,lr -> ...rxy\", latents, weights_tensor) + biases_tensor.unsqueeze(-1).unsqueeze(-1)\n    image_array = rgb_tensor.clamp(0, 255).byte().cpu().numpy().transpose(1, 2, 0)\n\n    return Image.fromarray(image_array)\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion v1.5 Image Generation with Remote VAE Decoding in Python\nDESCRIPTION: Illustrates a complete pipeline for generating an image using Stable Diffusion v1.5 with remote VAE decoding. It includes setting up the pipeline, generating latents, and decoding them remotely.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_decode.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    vae=None,\n).to(\"cuda\")\n\nprompt = \"Strawberry ice cream, in a stylish modern glass, coconut, splashing milk cream and honey, in a gradient purple background, fluid motion, dynamic movement, cinematic lighting, Mysterious\"\n\nlatent = pipe(\n    prompt=prompt,\n    output_type=\"latent\",\n).images\nimage = remote_decode(\n    endpoint=\"https://q1bj3bpq6kzilnsu.us-east-1.aws.endpoints.huggingface.cloud/\",\n    tensor=latent,\n    scaling_factor=0.18215,\n)\nimage.save(\"test.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Configuration for 🤗 Accelerate in Python\nDESCRIPTION: This Python code snippet demonstrates how to programmatically create a basic configuration for 🤗 Accelerate, which is useful particularly in environments that do not support interactive shell input.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Initialize Accelerate Environment using Bash\nDESCRIPTION: This snippet demonstrates how to initialize the Accelerate environment for PyTorch using its interactive configuration command or set up a default configuration if environment questions are not desired.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/realfill/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Launching Training Script with Mixed Precision\nDESCRIPTION: This command shows how to launch the SDXL training script with the mixed precision option set to bf16, which can help improve training speed by allowing for lower memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_text_to_image_sdxl.py \\n  --mixed_precision=\"bf16\"\n```\n\n----------------------------------------\n\nTITLE: Importing LuminaNextDiT2DModel in Python\nDESCRIPTION: This code snippet demonstrates how to import the LuminaNextDiT2DModel class. The actual implementation details are not provided in the given content, but this import statement would typically be used to access the model in a Python environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/lumina_nextdit2d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import LuminaNextDiT2DModel\n```\n\n----------------------------------------\n\nTITLE: Training FLUX.1 with Pure Textual Inversion\nDESCRIPTION: Configures a training run for pure textual inversion by setting train_transformer_frac to 0, optimizing only the text embeddings without training transformer LoRA layers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"black-forest-labs/FLUX.1-dev\"\nexport DATASET_NAME=\"./3d_icon\"\nexport OUTPUT_DIR=\"3d-icon-Flux-LoRA\"\n\naccelerate launch train_dreambooth_lora_flux_advanced.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --instance_prompt=\"3d icon in the style of TOK\" \\\n  --output_dir=$OUTPUT_DIR \\\n  --caption_column=\"prompt\" \\\n  --mixed_precision=\"bf16\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --repeats=1 \\\n  --report_to=\"wandb\"\\\n  --gradient_accumulation_steps=1 \\\n  --gradient_checkpointing \\\n  --learning_rate=1.0 \\\n  --text_encoder_lr=1.0 \\\n  --optimizer=\"prodigy\"\\\n  --train_text_encoder_ti\\\n  --enable_t5_ti\\\n  --train_text_encoder_ti_frac=0.5\\\n  --train_transformer_frac=0\\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --rank=8 \\\n  --max_train_steps=700 \\\n  --checkpointing_steps=2000 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Replacing Scheduler for Kandinsky Performance Tuning\nDESCRIPTION: Demonstrates how to replace the default DDIMScheduler with a DDPMScheduler to explore tradeoffs between inference speed and image quality for Kandinsky models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDPMScheduler\nfrom diffusers import DiffusionPipeline\n\nscheduler = DDPMScheduler.from_pretrained(\"kandinsky-community/kandinsky-2-1\", subfolder=\"ddpm_scheduler\")\npipe = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", scheduler=scheduler, torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Classifier-Free Guidance Callback Function in Python\nDESCRIPTION: Custom callback function definition that disables classifier-free guidance after 40% of inference steps to save computation with minimal impact on quality. The function modifies the pipeline's guidance scale and adjusts prompt embeddings accordingly.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef callback_dynamic_cfg(pipe, step_index, timestep, callback_kwargs):\n        # adjust the batch_size of prompt_embeds according to guidance_scale\n        if step_index == int(pipeline.num_timesteps * 0.4):\n                prompt_embeds = callback_kwargs[\"prompt_embeds\"]\n                prompt_embeds = prompt_embeds.chunk(2)[-1]\n\n                # update guidance_scale and prompt_embeds\n                pipeline._guidance_scale = 0.0\n                callback_kwargs[\"prompt_embeds\"] = prompt_embeds\n        return callback_kwargs\n```\n\n----------------------------------------\n\nTITLE: Installing Necessary Dependencies in Python\nDESCRIPTION: This snippet provides the command to install the required libraries for running the DiffusionPipeline. It ensures that the necessary versions of diffusers, accelerate, and transformers are available.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install --upgrade diffusers accelerate transformers\n```\n\n----------------------------------------\n\nTITLE: Loading a Pipeline from Local Weights\nDESCRIPTION: This code demonstrates how to load a diffusion pipeline from locally saved weights, which is useful for offline work or when you need to use a locally modified model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> pipeline = DiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\", use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Overriding Model Configuration\nDESCRIPTION: Shows how to override default model configuration parameters when loading.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel\n\nckpt_path = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0_0.9vae.safetensors\"\nmodel = UNet2DConditionModel.from_single_file(ckpt_path, upcast_attention=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Latents with PixArt-α\nDESCRIPTION: Creates image latents using the pre-computed prompt embeddings\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = PixArtAlphaPipeline.from_pretrained(\n    \"PixArt-alpha/PixArt-XL-2-1024-MS\",\n    text_encoder=None,\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\nlatents = pipe(\n    negative_prompt=None,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    prompt_attention_mask=prompt_attention_mask,\n    negative_prompt_attention_mask=negative_prompt_attention_mask,\n    num_images_per_prompt=1,\n    output_type=\"latent\",\n).images\n\ndel pipe.transformer\nflush()\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Configuration\nDESCRIPTION: Command for launching distributed training across multiple GPUs using Accelerate. Includes additional parameters like EMA and increased resolution compared to the single-GPU version.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu train_instruct_pix2pix.py \\\n --pretrained_model_name_or_path=stable-diffusion-v1-5/stable-diffusion-v1-5 \\\n --dataset_name=sayakpaul/instructpix2pix-1000-samples \\\n --use_ema \\\n --enable_xformers_memory_efficient_attention \\\n --resolution=512 --random_flip \\\n --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n --max_train_steps=15000 \\\n --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n --learning_rate=5e-05 --lr_warmup_steps=0 \\\n --conditioning_dropout_prob=0.05 \\\n --mixed_precision=fp16 \\\n --seed=42 \\\n --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Importing FlaxUNet2DConditionModel in Python\nDESCRIPTION: This code imports the FlaxUNet2DConditionModel, which is the Flax implementation of the UNet2DConditionModel. Flax is a neural network library for JAX, providing a version of the model optimized for TPUs and GPUs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet2d-cond.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.models.unets.unet_2d_condition_flax import FlaxUNet2DConditionModel\n```\n\n----------------------------------------\n\nTITLE: Applying First Block Cache to HunyuanVideo Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to apply First Block Cache optimization to the HunyuanVideo generation pipeline. It uses the apply_cache_on_pipe function with a residual difference threshold to accelerate video generation while preserving quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/para_attn.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport torch\nfrom diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\nfrom diffusers.utils import export_to_video\n\nmodel_id = \"tencent/HunyuanVideo\"\ntransformer = HunyuanVideoTransformer3DModel.from_pretrained(\n    model_id,\n    subfolder=\"transformer\",\n    torch_dtype=torch.bfloat16,\n    revision=\"refs/pr/18\",\n)\npipe = HunyuanVideoPipeline.from_pretrained(\n    model_id,\n    transformer=transformer,\n    torch_dtype=torch.float16,\n    revision=\"refs/pr/18\",\n).to(\"cuda\")\n\nfrom para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\n\napply_cache_on_pipe(pipe, residual_diff_threshold=0.6)\n\npipe.vae.enable_tiling()\n\nbegin = time.time()\noutput = pipe(\n    prompt=\"A cat walks on the grass, realistic\",\n    height=720,\n    width=1280,\n    num_frames=129,\n    num_inference_steps=30,\n).frames[0]\nend = time.time()\nprint(f\"Time: {end - begin:.2f}s\")\n\nprint(\"Saving video to hunyuan_video.mp4\")\nexport_to_video(output, \"hunyuan_video.mp4\", fps=15)\n```\n\n----------------------------------------\n\nTITLE: Training Amused 256 with 8-bit Adam Optimizer\nDESCRIPTION: Script for training Amused-256 using 8-bit Adam optimizer. Uses batch size 16 and learning rate 2e-5, achieving results in ~750 steps. Note that this configuration may diverge in longer training sessions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --train_batch_size <batch size> \\\n    --gradient_accumulation_steps <gradient accumulation steps> \\\n    --learning_rate 2e-5 \\\n    --use_8bit_adam \\\n    --pretrained_model_name_or_path amused/amused-256 \\\n    --instance_data_dataset  'm1guelpf/nouns' \\\n    --image_key image \\\n    --prompt_key text \\\n    --resolution 256 \\\n    --mixed_precision fp16 \\\n    --lr_scheduler constant \\\n    --validation_prompts \\\n        'a pixel art character with square red glasses, a baseball-shaped head and a orange-colored body on a dark background' \\\n        'a pixel art character with square orange glasses, a lips-shaped head and a red-colored body on a light background' \\\n        'a pixel art character with square blue glasses, a microwave-shaped head and a purple-colored body on a sunny background' \\\n        'a pixel art character with square red glasses, a baseball-shaped head and a blue-colored body on an orange background' \\\n        'a pixel art character with square red glasses' \\\n        'a pixel art character' \\\n        'square red glasses on a pixel art character' \\\n        'square red glasses on a pixel art character with a baseball-shaped head' \\\n    --max_train_steps 10000 \\\n    --checkpointing_steps 500 \\\n    --validation_steps 250 \\\n    --gradient_checkpointing\n```\n\n----------------------------------------\n\nTITLE: Loading CogVideoXTransformer3DModel with Pre-trained Weights in Python\nDESCRIPTION: This code snippet demonstrates how to load the CogVideoXTransformer3DModel with pre-trained weights from THUDM/CogVideoX-2b. The model is loaded with float16 precision and moved to a CUDA device for efficient computation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/cogvideox_transformer3d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import CogVideoXTransformer3DModel\n\ntransformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-2b\", subfolder=\"transformer\", torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running Inference\nDESCRIPTION: Demonstrates compiling the transformer and VAE components with torch.compile and running inference with a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/lumina.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline.transformer = torch.compile(pipeline.transformer, mode=\"max-autotune\", fullgraph=True)\npipeline.vae.decode = torch.compile(pipeline.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\nimage = pipeline(prompt=\"Upper body of a young woman in a Victorian-era outfit with brass goggles and leather straps. Background shows an industrial revolution cityscape with smoky skies and tall, metal structures\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22InpaintPipeline in Python\nDESCRIPTION: This snippet shows the class definition for KandinskyV22InpaintPipeline, which is used for inpainting tasks in Kandinsky 2.2. It includes the __call__ method for generating inpainted images based on input images, masks, and text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22InpaintPipeline\n\t- all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionAttendAndExcitePipeline in Python\nDESCRIPTION: This code snippet shows how to import the StableDiffusionAttendAndExcitePipeline class, which is the main implementation of the Attend-and-Excite technique for Stable Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/attend_and_excite.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionAttendAndExcitePipeline\n```\n\n----------------------------------------\n\nTITLE: Loading PixArt-Σ Pipeline with 8-bit Text Encoder\nDESCRIPTION: Initializes the PixArt-Σ pipeline with the text encoder loaded in 8-bit precision to reduce memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart_sigma.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import T5EncoderModel\nfrom diffusers import PixArtSigmaPipeline\nimport torch\n\ntext_encoder = T5EncoderModel.from_pretrained(\n    \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\",\n    subfolder=\"text_encoder\",\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\npipe = PixArtSigmaPipeline.from_pretrained(\n    \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\",\n    text_encoder=text_encoder,\n    transformer=None,\n    device_map=\"balanced\"\n)\n```\n\n----------------------------------------\n\nTITLE: Applying ToMe to StableDiffusionPipeline in Python\nDESCRIPTION: This snippet demonstrates how to use the tomesd library to apply Token Merging to a StableDiffusionPipeline, potentially reducing inference time.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/tome.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport tomesd\n\npipeline = StableDiffusionPipeline.from_pretrained(\n      \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16\n).to(\"cuda\")\ntomesd.apply_patch(pipeline, ratio=0.5)\n\nimage = pipeline(\"a photo of an astronaut riding a horse on mars\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading Source Image for ControlNet Processing\nDESCRIPTION: Loads and resizes an image to be used as the base for ControlNet conditioning. This is the first step in the ControlNet process, which involves preparing the source image that will guide the generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image\n\nimg = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png\"\n).resize((768, 768))\nimg\n```\n\n----------------------------------------\n\nTITLE: Checking VAE Downsampling Layers\nDESCRIPTION: Verifies the number of downsampling layers in the VAE model by checking if 2 raised to the power of (number of blocks - 1) equals 8.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n2 ** (len(vae.config.block_out_channels) - 1) == 8\n```\n\n----------------------------------------\n\nTITLE: Implementing Marigold Depth Estimation Pipeline in Python\nDESCRIPTION: Code for initializing and running the Marigold depth estimation pipeline, which transforms an input image into a colorized depth map. Uses the DiffusionPipeline.from_pretrained method with a custom pipeline parameter and applies various parameters such as denoising steps and ensemble size.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom PIL import Image\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"prs-eth/marigold-lcm-v1-0\",\n    custom_pipeline=\"marigold_depth_estimation\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n)\n\npipeline.to(\"cuda\")\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/community-marigold.png\")\noutput = pipeline(\n    image,\n    denoising_steps=4,\n    ensemble_size=5,\n    processing_res=768,\n    match_input_res=True,\n    batch_size=0,\n    seed=33,\n    color_map=\"Spectral\",\n    show_progress_bar=True,\n)\ndepth_colored: Image.Image = output.depth_colored\ndepth_colored.save(\"./depth_colored.png\")\n```\n\n----------------------------------------\n\nTITLE: Reducing Memory Usage with Attention Slicing\nDESCRIPTION: Implements a function to generate image batches and uses attention slicing to reduce memory consumption, allowing larger batch sizes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/stable_diffusion.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_inputs(batch_size=1):\n    generator = [torch.Generator(\"cuda\").manual_seed(i) for i in range(batch_size)]\n    prompts = batch_size * [prompt]\n    num_inference_steps = 20\n    return {\"prompt\": prompts, \"generator\": generator, \"num_inference_steps\": num_inference_steps}\n\npipeline.enable_attention_slicing()\nimages = pipeline(**get_inputs(batch_size=8)).images\n```\n\n----------------------------------------\n\nTITLE: Loading a Community Pipeline from a Local Directory\nDESCRIPTION: This example demonstrates loading a community pipeline from a local directory that contains a pipeline.py file with the pipeline class implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    custom_pipeline=\"./path/to/pipeline_directory/\",\n    clip_model=clip_model,\n    feature_extractor=feature_extractor,\n    use_safetensors=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Quantitative Evaluation of Marigold Depth Model\nDESCRIPTION: Demonstrates how to load and run inference with the Marigold depth model for quantitative evaluation on standard benchmarks. Shows setup of key parameters like inference steps and ensemble size for reproducible results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\n\ndevice = \"cuda\"\nseed = 2024\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\npipe = diffusers.MarigoldDepthPipeline.from_pretrained(\"prs-eth/marigold-depth-v1-1\").to(device)\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\ndepth = pipe(\n    image, \n    num_inference_steps=4,  # set according to the evaluation protocol from the paper\n    ensemble_size=10,       # set according to the evaluation protocol from the paper\n    generator=generator,\n)\n\n# evaluate metrics\n```\n\n----------------------------------------\n\nTITLE: Loading ControlNet Model from Original Format in Python\nDESCRIPTION: Example showing how to load a ControlNetModel and StableDiffusionControlNetPipeline from original model files. The code demonstrates loading both the controlnet and base model from either URLs or local paths.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/controlnet.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n\nurl = \"https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11p_sd15_canny.pth\"  # can also be a local path\ncontrolnet = ControlNetModel.from_single_file(url)\n\nurl = \"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/blob/main/v1-5-pruned.safetensors\"  # can also be a local path\npipe = StableDiffusionControlNetPipeline.from_single_file(url, controlnet=controlnet)\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Pipeline\nDESCRIPTION: Code to initialize the Stable Diffusion pipeline with a specific model checkpoint using PyTorch\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_ckpt = \"CompVis/stable-diffusion-v1-4\"\nsd_pipeline = StableDiffusionPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Images for FID Calculation\nDESCRIPTION: Applies preprocessing transforms to prepare images for FID calculation, including normalization and center cropping.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision.transforms import functional as F\nimport torch\n\ndef preprocess_image(image):\n    image = torch.tensor(image).unsqueeze(0)\n    image = image.permute(0, 3, 1, 2) / 255.0\n    return F.center_crop(image, (256, 256))\n\nreal_images = torch.cat([preprocess_image(image) for image in real_images])\nprint(real_images.shape)\n```\n\n----------------------------------------\n\nTITLE: Computing Text Embeddings with Bash\nDESCRIPTION: This command is used to compute and serialize text embeddings which are crucial for the training process. It generates an output file named 'embeddings.parquet'.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython compute_embeddings.py\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for PyTorch Implementation\nDESCRIPTION: Commands to navigate to the textual inversion example folder and install the required dependencies for the PyTorch implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/textual_inversion\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Up JAX Environment for SDXL\nDESCRIPTION: Initial setup for the JAX environment, including importing libraries, initializing the compilation cache, and determining the number of available TPU devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom diffusers import FlaxStableDiffusionXLPipeline\n\nfrom jax.experimental.compilation_cache import compilation_cache as cc\ncc.initialize_cache(\"/tmp/sdxl_cache\")\nimport time\n\nNUM_DEVICES = jax.device_count()\n```\n\n----------------------------------------\n\nTITLE: Modifying Dataset Preparation for ControlNet Flux Training\nDESCRIPTION: Patch to modify the dataset preparation workflow, reordering operations and reducing batch size to manage GPU memory usage for VAE encoding during preprocessing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_9\n\nLANGUAGE: diff\nCODE:\n```\n+train_dataset = prepare_train_dataset(train_dataset, accelerator)\nwith accelerator.main_process_first():\n    from datasets.fingerprint import Hasher\n\n    # fingerprint used by the cache for the other processes to load the result\n    # details: https://github.com/huggingface/diffusers/pull/4038#discussion_r1266078401\n    new_fingerprint = Hasher.hash(args)\n    train_dataset = train_dataset.map(\n-        compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint, batch_size=100\n+        compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint, batch_size=10\n    )\n\ndel text_encoders, tokenizers\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Then get the training dataset ready to be passed to the dataloader.\n-train_dataset = prepare_train_dataset(train_dataset, accelerator)\n```\n\n----------------------------------------\n\nTITLE: Cloning the Diffusers Repository\nDESCRIPTION: This snippet clones the diffusers GitHub repository and navigates to the specific example directory for SD3 LoRA Colab. Requires git command-line tool.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!git clone https://github.com/huggingface/diffusers\n%cd diffusers/examples/research_projects/sd3_lora_colab\n```\n\n----------------------------------------\n\nTITLE: Implementing Group Offloading for Layer Management\nDESCRIPTION: Demonstrates group offloading which handles internal layer groups, offering a balance between sequential and model offloading approaches for memory optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.hooks import apply_group_offloading\nfrom diffusers.utils import export_to_video\n\n# Load the pipeline\nonload_device = torch.device(\"cuda\")\noffload_device = torch.device(\"cpu\")\npipe = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-5b\", torch_dtype=torch.bfloat16)\n\n# We can utilize the enable_group_offload method for Diffusers model implementations\npipe.transformer.enable_group_offload(onload_device=onload_device, offload_device=offload_device, offload_type=\"leaf_level\", use_stream=True)\n```\n\n----------------------------------------\n\nTITLE: Training DDPM UNet on Pokemon Dataset\nDESCRIPTION: Command to train a DDPM UNet model on the Pokemon dataset using Accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_unconditional.py \\\n  --dataset_name=\"huggan/pokemon\" \\\n  --resolution=64 --center_crop --random_flip \\\n  --output_dir=\"ddpm-ema-pokemon-64\" \\\n  --train_batch_size=16 \\\n  --num_epochs=100 \\\n  --gradient_accumulation_steps=1 \\\n  --use_ema \\\n  --learning_rate=1e-4 \\\n  --lr_warmup_steps=500 \\\n  --mixed_precision=no \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Flushing GPU Memory\nDESCRIPTION: Clears CUDA memory and releases unreferenced memory to potentially enhance performance during training or inference. Uses PyTorch and the garbage collector.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport gc\n\ndef flush():\n    torch.cuda.empty_cache()\n    gc.collect()\n\nflush()\n```\n\n----------------------------------------\n\nTITLE: Training Kandinsky 2.2 Prior with LoRA\nDESCRIPTION: Script for training the Kandinsky 2.2 prior model using LoRA adaptation. Similar configuration to decoder training but specific to the prior model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image_prior_lora.py \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --num_train_epochs=100 --checkpointing_steps=5000 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --seed=42 \\\n  --rank=4 \\\n  --output_dir=\"kandi22-prior-naruto-lora\" \\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n  --push_to_hub \\\n```\n\n----------------------------------------\n\nTITLE: Cloning a Model Repository with Git LFS\nDESCRIPTION: Clone a diffusion model repository to local disk using Git LFS, which is required to download the large model weight files properly.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5\n```\n\n----------------------------------------\n\nTITLE: Launching InstructPix2Pix SDXL Training\nDESCRIPTION: Initiates the training process for InstructPix2Pix using SDXL. It specifies various parameters such as model path, dataset, batch size, and learning rate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README_sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_instruct_pix2pix_sdxl.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --seed=42 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Implementing ToMe with StableDiffusionPipeline\nDESCRIPTION: Code diff showing how to apply ToMe optimization to a Stable Diffusion pipeline. The patch is applied with a merge ratio of 0.5 to balance speed and quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md#2025-04-11_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n  from diffusers import StableDiffusionPipeline\n  import torch\n  import tomesd\n\n  pipeline = StableDiffusionPipeline.from_pretrained(\n        \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True,\n  ).to(\"cuda\")\n+ tomesd.apply_patch(pipeline, ratio=0.5)\n\n  image = pipeline(\"a photo of an astronaut riding a horse on mars\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Training Images\nDESCRIPTION: Command to collect regularization images using CLIP retrieval for training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install clip-retrieval\npython retrieve.py --class_prompt cat --class_data_dir real_reg/samples_cat --num_class_images 200\n```\n\n----------------------------------------\n\nTITLE: Loading Lumina2Transformer2DModel from Pretrained Weights\nDESCRIPTION: Demonstrates how to load the pretrained Lumina2Transformer2DModel from the Alpha-VLLM/Lumina-Image-2.0 repository using the diffusers library. The model is loaded with bfloat16 precision and specifically from the transformer subfolder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/lumina2_transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import Lumina2Transformer2DModel\n\ntransformer = Lumina2Transformer2DModel.from_pretrained(\"Alpha-VLLM/Lumina-Image-2.0\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderDC Model with Custom Config in Python\nDESCRIPTION: This snippet demonstrates loading an AutoencoderDC model from a single file checkpoint while specifying a custom configuration. This is useful for 'in' variant checkpoints that require a specific config.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoder_dc.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderDC\n\nckpt_path = \"https://huggingface.co/mit-han-lab/dc-ae-f128c512-in-1.0/blob/main/model.safetensors\"\nmodel = AutoencoderDC.from_single_file(ckpt_path, config=\"mit-han-lab/dc-ae-f128c512-in-1.0-diffusers\")\n```\n\n----------------------------------------\n\nTITLE: UNet Input Generation for Stable Diffusion with PyTorch\nDESCRIPTION: This function generates random input tensors suitable for the UNet model in a Stable Diffusion pipeline. It creates a random sample, timestep, and encoder hidden states, all on the CUDA device with float16 data type. The function is used to provide inputs for tracing and benchmarking the UNet model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef generate_inputs():\n    sample = torch.randn((2, 4, 64, 64), device=\"cuda\", dtype=torch.float16)\n    timestep = torch.rand(1, device=\"cuda\", dtype=torch.float16) * 999\n    encoder_hidden_states = torch.randn((2, 77, 768), device=\"cuda\", dtype=torch.float16)\n    return sample, timestep, encoder_hidden_states\n```\n\n----------------------------------------\n\nTITLE: Accelerating Image Generation with GPU and float16 Precision\nDESCRIPTION: Moves the pipeline to GPU, sets up a random generator, and generates an image. Then reloads the model in float16 precision for faster generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/stable_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\npipeline = pipeline.to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\nimage = pipeline(prompt, generator=generator).images[0]\n\n# Reload in float16\npipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True)\npipeline = pipeline.to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\nimage = pipeline(prompt, generator=generator).images[0]\n```\n\n----------------------------------------\n\nTITLE: Using AutoPipelineForText2Image with Kandinsky 2.2\nDESCRIPTION: Shows how to use the AutoPipelineForText2Image class to set up and use the combined Kandinsky 2.2 pipeline for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, guidance_scale=4.0, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantization Filter Functions\nDESCRIPTION: Defines filter functions to identify which linear layers and convolution layers should be quantized for optimal performance\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef dynamic_quant_filter_fn(mod, *args):\n    return (\n        isinstance(mod, torch.nn.Linear)\n        and mod.in_features > 16\n        and (mod.in_features, mod.out_features)\n        not in [\n            (1280, 640),\n            (1920, 1280),\n            (1920, 640),\n            (2048, 1280),\n            (2048, 2560),\n            (2560, 1280),\n            (256, 128),\n            (2816, 1280),\n            (320, 640),\n            (512, 1536),\n            (512, 256),\n            (512, 512),\n            (640, 1280),\n            (640, 1920),\n            (640, 320),\n            (640, 5120),\n            (640, 640),\n            (960, 320),\n            (960, 640),\n        ]\n    )\n\n\ndef conv_filter_fn(mod, *args):\n    return (\n        isinstance(mod, torch.nn.Conv2d) and mod.kernel_size == (1, 1) and 128 in [mod.in_channels, mod.out_channels]\n    )\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Stable Diffusion Text-to-Image\nDESCRIPTION: Python code for benchmarking Stable Diffusion text-to-image generation with PyTorch 2.0 optimizations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npath = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\n\npipe = DiffusionPipeline.from_pretrained(path, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    images = pipe(prompt=prompt).images\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training with Accelerate\nDESCRIPTION: Command for distributed training of a DDPM UNet model on multiple GPUs using Accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu train_unconditional.py \\\n  --dataset_name=\"huggan/pokemon\" \\\n  --resolution=64 --center_crop --random_flip \\\n  --output_dir=\"ddpm-ema-pokemon-64\" \\\n  --train_batch_size=16 \\\n  --num_epochs=100 \\\n  --gradient_accumulation_steps=1 \\\n  --use_ema \\\n  --learning_rate=1e-4 \\\n  --lr_warmup_steps=500 \\\n  --mixed_precision=\"fp16\" \\\n  --logger=\"wandb\"\n```\n\n----------------------------------------\n\nTITLE: FP8 Layerwise Weight-Casting with Diffusers (PyTorch)\nDESCRIPTION: Demonstrates how to use layerwise weight-casting to reduce the memory footprint of a Diffusers model.  It loads a model with `torch.bfloat16` and then enables layerwise casting to `torch.float8_e4m3fn` for weight storage, while keeping computations in `torch.bfloat16`.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n\"import torch\nfrom diffusers import CogVideoXPipeline, CogVideoXTransformer3DModel\nfrom diffusers.utils import export_to_video\n\nmodel_id = \\\"THUDM/CogVideoX-5b\\\"\\n\n# Load the model in bfloat16 and enable layerwise casting\ntransformer = CogVideoXTransformer3DModel.from_pretrained(model_id, subfolder=\\\"transformer\\\", torch_dtype=torch.bfloat16)\ntransformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)\n\n# Load the pipeline\npipe = CogVideoXPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)\npipe.to(\\\"cuda\\\")\n\nprompt = (\\n    \\\"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. \\\"\\n    \\\"The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other \\\"\\n    \\\"pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, \\\"\\n    \\\"casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. \\\"\\n    \\\"The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical \\\"\\n    \\\"atmosphere of this unique musical performance.\\\"\\n)\nvideo = pipe(prompt=prompt, guidance_scale=6, num_inference_steps=50).frames[0]\nexport_to_video(video, \\\"output.mp4\\\", fps=8)\"\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22CombinedPipeline in Python\nDESCRIPTION: This snippet shows the class definition for KandinskyV22CombinedPipeline, which combines the prior and main pipelines for a more streamlined text-to-image generation process in Kandinsky 2.2.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22CombinedPipeline\n\t- all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Loading Sharded SDXL Pipeline\nDESCRIPTION: Shows how to load a sharded UNet model into the StableDiffusionXLPipeline and generate images using half precision (float16).\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/inference_with_big_models.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel, StableDiffusionXLPipeline\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"sayakpaul/sdxl-unet-sharded\", torch_dtype=torch.float16\n)\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16\n).to(\"cuda\")\n\nimage = pipeline(\"a cute dog running on the grass\", num_inference_steps=30).images[0]\nimage.save(\"dog.png\")\n```\n\n----------------------------------------\n\nTITLE: Training Custom Diffusion with Single Concept in Bash\nDESCRIPTION: This script launches training for a Custom Diffusion model with a single concept (cat). It uses accelerate for distributed training, incorporates prior preservation with regularization, and includes parameters for learning rate, batch size, and other training configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport OUTPUT_DIR=\"path-to-save-model\"\nexport INSTANCE_DIR=\"./data/cat\"\n\naccelerate launch train_custom_diffusion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --class_data_dir=./real_reg/samples_cat/ \\\n  --with_prior_preservation \\\n  --real_prior \\\n  --prior_loss_weight=1.0 \\\n  --class_prompt=\"cat\" \\\n  --num_class_images=200 \\\n  --instance_prompt=\"photo of a <new1> cat\"  \\\n  --resolution=512  \\\n  --train_batch_size=2  \\\n  --learning_rate=1e-5  \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=250 \\\n  --scale_lr \\\n  --hflip  \\\n  --modifier_token \"<new1>\" \\\n  --validation_prompt=\"<new1> cat sitting in a bucket\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Saving a Pipeline as an FP16 Variant\nDESCRIPTION: Save a diffusion pipeline as a half-precision (fp16) variant in the same directory as the original checkpoint, allowing both variants to be loaded from the same location.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\n# save as fp16 variant\nstable_diffusion.save_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", variant=\"fp16\")\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Pipeline for Generating Initial Image in Python\nDESCRIPTION: This code initializes a text-to-image pipeline using Stable Diffusion v1.5 to generate a concept art digital painting of an elven castle. It enables model CPU offload and memory-efficient attention for better performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\ntext2image = pipeline(\"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Moving Model and Input to GPU for Faster Processing\nDESCRIPTION: Transfers the model and input tensor to CUDA-enabled GPU to accelerate the denoising process. This optimization is helpful for faster inference with diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> model.to(\"cuda\")\n>>> noisy_sample = noisy_sample.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Motion LoRA Animation Generation in Python\nDESCRIPTION: Implements animation generation using Motion LoRAs with the AnimateDiff pipeline. Uses a specific motion adapter and LoRA weights to create animations with predetermined motion patterns.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\n# Load the motion adapter\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n# load SD 1.5 based finetuned model\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\npipe.load_lora_weights(\n    \"guoyww/animatediff-motion-lora-zoom-out\", adapter_name=\"zoom-out\"\n)\n\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    beta_schedule=\"linear\",\n    timestep_spacing=\"linspace\",\n    steps_offset=1,\n)\npipe.scheduler = scheduler\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\noutput = pipe(\n    prompt=(\n        \"masterpiece, bestquality, highlydetailed, ultradetailed, sunset, \"\n        \"orange sky, warm lighting, fishing boats, ocean waves seagulls, \"\n        \"rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, \"\n        \"golden hour, coastal landscape, seaside scenery\"\n    ),\n    negative_prompt=\"bad quality, worse quality\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=25,\n    generator=torch.Generator(\"cpu\").manual_seed(42),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Installing Additional Dependencies for Training\nDESCRIPTION: This snippet provides the commands to clone the diffusers repository and install required dependencies from the requirements file. Ensure to run it from the correct directory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/training/text_to_image/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\\n--project=${PROJECT_ID} --zone=${ZONE} --worker=all \\\n--command='git clone https://github.com/huggingface/diffusers.git\ncd diffusers\ngit checkout main\ncd examples/research_projects/pytorch_xla\npip3 install -r requirements.txt\npip3 install pillow --upgrade\ncd ../../..\npip3 install .'\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Stable Diffusion Textual Inversion\nDESCRIPTION: Imports the necessary libraries for working with Stable Diffusion and textual inversion, including the StableDiffusionPipeline and utility functions for image visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/textual_inversion_inference.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```\n\n----------------------------------------\n\nTITLE: Cloning the diffusers Repository and Installing Dependencies\nDESCRIPTION: This snippet clones the Hugging Face diffusers repository and installs the necessary Python dependencies for the project, preparing the environment for further execution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/inference/flux/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers.git\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd diffusers\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers accelerate sentencepiece structlog\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install .\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/research_projects/pytorch_xla/inference/flux/\n```\n\n----------------------------------------\n\nTITLE: Running VAE Roundtrip Script with Pre-trained Model in Bash\nDESCRIPTION: Command to execute the vae_roundtrip.py script from the Diffusers library. It loads a pre-trained VAE model from Stable Diffusion v1-5, encodes an input image, decodes it back, and displays both side by side for comparison.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/vae/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/research_projects/vae\npython vae_roundtrip.py \\\n    --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\" \\\n    --subfolder=\"vae\" \\\n    --input_image=\"/path/to/your/input.png\"\n```\n\n----------------------------------------\n\nTITLE: Loading IP-Adapter FaceID Plus Model\nDESCRIPTION: This code shows how to load an IP-Adapter FaceID Plus model which uses both 'insightface' and CLIP image embeddings for better realism in facial representation. It requires loading both the CLIP image encoder and the FaceID adapter weights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import CLIPVisionModelWithProjection\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n    torch_dtype=torch.float16,\n)\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    image_encoder=image_encoder,\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter-FaceID\", subfolder=None, weight_name=\"ip-adapter-faceid-plus_sd15.bin\")\n```\n\n----------------------------------------\n\nTITLE: Creating a DDPM Noise Scheduler and Adding Noise to Images\nDESCRIPTION: Initializes a DDPMScheduler and demonstrates how to add noise to an image at a specific timestep. This process is fundamental to the diffusion model training approach.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from PIL import Image\n>>> from diffusers import DDPMScheduler\n\n>>> noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n>>> noise = torch.randn(sample_image.shape)\n>>> timesteps = torch.LongTensor([50])\n>>> noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n\n>>> Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])\n```\n\n----------------------------------------\n\nTITLE: Initializing Stable Diffusion Pipeline without Hub Login in Python\nDESCRIPTION: This snippet demonstrates how to initialize a Stable Diffusion pipeline without being logged into the Hugging Face Hub by downloading the weights locally and passing the local path to from_pretrained.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\")\n```\n\n----------------------------------------\n\nTITLE: Loading SparseControlNetModel in Python\nDESCRIPTION: This snippet demonstrates how to load pre-trained SparseControlNetModel variants for scribble and RGB inputs using the Hugging Face Diffusers library. It shows loading both fp32 and fp16 variants.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/controlnet_sparsectrl.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import SparseControlNetModel\n\n# fp32 variant in float16\n# 1. Scribble checkpoint\ncontrolnet = SparseControlNetModel.from_pretrained(\"guoyww/animatediff-sparsectrl-scribble\", torch_dtype=torch.float16)\n\n# 2. RGB checkpoint\ncontrolnet = SparseControlNetModel.from_pretrained(\"guoyww/animatediff-sparsectrl-rgb\", torch_dtype=torch.float16)\n\n# For loading fp16 variant, pass `variant=\"fp16\"` as an additional parameter\n```\n\n----------------------------------------\n\nTITLE: Calculating CLIP Score for Generated Images\nDESCRIPTION: Implements a function to calculate the CLIP score for generated images and their corresponding prompts. CLIP score measures the compatibility between image-caption pairs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/evaluation.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torchmetrics.functional.multimodal import clip_score\nfrom functools import partial\n\nclip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n\ndef calculate_clip_score(images, prompts):\n    images_int = (images * 255).astype(\"uint8\")\n    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()\n    return round(float(clip_score), 4)\n\nsd_clip_score = calculate_clip_score(images, prompts)\nprint(f\"CLIP score: {sd_clip_score}\")\n# CLIP score: 35.7038\n```\n\n----------------------------------------\n\nTITLE: Calculating CLIP Score\nDESCRIPTION: Code to calculate CLIP scores for generated images using the torchmetrics library\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torchmetrics.functional.multimodal import clip_score\nfrom functools import partial\n\nclip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n\ndef calculate_clip_score(images, prompts):\n    images_int = (images * 255).astype(\"uint8\")\n    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()\n    return round(float(clip_score), 4)\n\nsd_clip_score = calculate_clip_score(images, prompts)\nprint(f\"CLIP score: {sd_clip_score}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing T-GATE with PixArt Pipeline\nDESCRIPTION: Example of accelerating PixArtAlphaPipeline using T-GATE with specified gate step and inference steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/tgate.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import PixArtAlphaPipeline\nfrom tgate import TgatePixArtLoader\n\npipe = PixArtAlphaPipeline.from_pretrained(\"PixArt-alpha/PixArt-XL-2-1024-MS\", torch_dtype=torch.float16)\n\ngate_step = 8\ninference_step = 25\npipe = TgatePixArtLoader(\n       pipe,\n       gate_step=gate_step,\n       num_inference_steps=inference_step,\n).to(\"cuda\")\n\nimage = pipe.tgate(\n       \"An alpaca made of colorful building blocks, cyberpunk.\",\n       gate_step=gate_step,\n       num_inference_steps=inference_step,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Verifying PyTorch and PyTorch/XLA Installation\nDESCRIPTION: This snippet verifies the successful installation of PyTorch and PyTorch/XLA by attempting to import them in a Python environment. It ensures that both libraries are correctly set up for use in the project.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/inference/flux/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -c \"import torch; import torch_xla;\"\n```\n\n----------------------------------------\n\nTITLE: Loading a Pretrained UNet Model for Image Generation\nDESCRIPTION: This code demonstrates how to load a pretrained UNet2DModel from Hugging Face Hub. The example uses a model trained on cat images that will be used for generation in subsequent steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import UNet2DModel\n\n>>> repo_id = \"google/ddpm-cat-256\"\n>>> model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Loading or Initializing T2I-Adapter\nDESCRIPTION: This Python code snippet checks if a pretrained T2I-Adapter model is specified. If so, it loads the weights; otherwise, it initializes a new T2I-Adapter with specified architecture parameters. This determines how the T2I-Adapter is initialized before training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"if args.adapter_model_name_or_path:\n    logger.info(\"Loading existing adapter weights.\")\n    t2iadapter = T2IAdapter.from_pretrained(args.adapter_model_name_or_path)\nelse:\n    logger.info(\"Initializing t2iadapter weights.\")\n    t2iadapter = T2IAdapter(\n        in_channels=3,\n        channels=(320, 640, 1280, 1280),\n        num_res_blocks=2,\n        downscale_factor=16,\n        adapter_type=\\\"full_adapter_xl\\\",\n    )\"\n```\n\n----------------------------------------\n\nTITLE: FLUX FP16 Inference Implementation\nDESCRIPTION: Shows how to run FLUX with FP16 precision for accelerated inference on specific GPU architectures. Includes memory optimization techniques for low VRAM scenarios.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPipeline\n\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\n\npipe.to(torch.float16)\n\nprompt = \"A cat holding a sign that says hello world\"\nout = pipe(\n    prompt=prompt,\n    guidance_scale=0.,\n    height=768,\n    width=1360,\n    num_inference_steps=4,\n    max_sequence_length=256,\n).images[0]\nout.save(\"image.png\")\n```\n\n----------------------------------------\n\nTITLE: Replacing Default Scheduler in StableDiffusionPipeline with Python\nDESCRIPTION: Example showing how to replace the default PNDMScheduler with EulerDiscreteScheduler in a StableDiffusionPipeline for improved quality or speed. The code demonstrates two approaches: modifying the existing scheduler or loading a new scheduler during pipeline initialization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/overview.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n\npipeline = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\npipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\n# or\neuler_scheduler = EulerDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\npipeline = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", scheduler=euler_scheduler)\n```\n\n----------------------------------------\n\nTITLE: Enabling torch.compile for Kandinsky Optimization\nDESCRIPTION: Shows how to enable torch.compile for PyTorch 2.0+ to automatically use scaled dot-product attention (SDPA) for faster inference with Kandinsky models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_34\n\nLANGUAGE: python\nCODE:\n```\npipe.unet.to(memory_format=torch.channels_last)\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Parallel Inference with torchrun\nDESCRIPTION: This bash command snippet illustrates how to launch the xDiT framework using torchrun for multi-node, multi-GPU capabilities. It shows a specific command for running inference with 8 GPUs and various configuration parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/xdit.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node=8 ./inference.py --model models/FLUX.1-dev --data_parallel_degree 2 --ulysses_degree 2 --ring_degree 2 --prompt \"A snowy mountain\" \"A small dog\" --num_inference_steps 50\n```\n\n----------------------------------------\n\nTITLE: Implementing Directional Similarity for CLIP Evaluation in Python\nDESCRIPTION: This snippet defines a PyTorch module for computing directional similarity using CLIP embeddings. It encodes images and text, and calculates the cosine similarity between the differences in image and text embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DirectionalSimilarity(nn.Module):\n    def __init__(self, tokenizer, text_encoder, image_processor, image_encoder):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.text_encoder = text_encoder\n        self.image_processor = image_processor\n        self.image_encoder = image_encoder\n\n    def preprocess_image(self, image):\n        image = self.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n        return {\"pixel_values\": image.to(\"cuda\")}\n\n    def tokenize_text(self, text):\n        inputs = self.tokenizer(\n            text,\n            max_length=self.tokenizer.model_max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        return {\"input_ids\": inputs.input_ids.to(\"cuda\")}\n\n    def encode_image(self, image):\n        preprocessed_image = self.preprocess_image(image)\n        image_features = self.image_encoder(**preprocessed_image).image_embeds\n        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n        return image_features\n\n    def encode_text(self, text):\n        tokenized_text = self.tokenize_text(text)\n        text_features = self.text_encoder(**tokenized_text).text_embeds\n        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n        return text_features\n\n    def compute_directional_similarity(self, img_feat_one, img_feat_two, text_feat_one, text_feat_two):\n        sim_direction = F.cosine_similarity(img_feat_two - img_feat_one, text_feat_two - text_feat_one)\n        return sim_direction\n\n    def forward(self, image_one, image_two, caption_one, caption_two):\n        img_feat_one = self.encode_image(image_one)\n        img_feat_two = self.encode_image(image_two)\n        text_feat_one = self.encode_text(caption_one)\n        text_feat_two = self.encode_text(caption_two)\n        directional_similarity = self.compute_directional_similarity(\n            img_feat_one, img_feat_two, text_feat_one, text_feat_two\n        )\n        return directional_similarity\n```\n\n----------------------------------------\n\nTITLE: Combined Motion LoRAs with PEFT in Python\nDESCRIPTION: Demonstrates how to combine multiple Motion LoRAs using PEFT backend to create complex animations with multiple motion patterns.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\n# Load the motion adapter\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n# load SD 1.5 based finetuned model\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\n\npipe.load_lora_weights(\n    \"diffusers/animatediff-motion-lora-zoom-out\", adapter_name=\"zoom-out\",\n)\npipe.load_lora_weights(\n    \"diffusers/animatediff-motion-lora-pan-left\", adapter_name=\"pan-left\",\n)\npipe.set_adapters([\"zoom-out\", \"pan-left\"], adapter_weights=[1.0, 1.0])\n\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipe.scheduler = scheduler\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\noutput = pipe(\n    prompt=(\n        \"masterpiece, bestquality, highlydetailed, ultradetailed, sunset, \"\n        \"orange sky, warm lighting, fishing boats, ocean waves seagulls, \"\n        \"rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, \"\n        \"golden hour, coastal landscape, seaside scenery\"\n    ),\n    negative_prompt=\"bad quality, worse quality\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=25,\n    generator=torch.Generator(\"cpu\").manual_seed(42),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Impacted Changes - Python Bash\nDESCRIPTION: Runs the test suite for the affected code to ensure features work correctly after modifications. This step is crucial for validating changes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest tests/<TEST_TO_RUN>.py\n```\n\n----------------------------------------\n\nTITLE: Resetting Pipeline Device Map\nDESCRIPTION: Demonstrates how to reset the device mapping of a pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/inference_with_big_models.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline.reset_device_map()\n```\n\n----------------------------------------\n\nTITLE: SDXL Inference with PyTorch XLA\nDESCRIPTION: Implementation of SDXL inference using PyTorch XLA for accelerated performance on specialized hardware. Includes timing measurements to demonstrate compilation and inference times.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\nimport torch_xla.core.xla_model as xm\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipe = DiffusionPipeline.from_pretrained(model_id)\n\ndevice = xm.xla_device()\npipe.to(device)\n\nprompt = \"A naruto with green eyes and red legs.\"\nstart = time()\nimage = pipe(prompt, num_inference_steps=inference_steps).images[0]\nprint(f'Compilation time is {time()-start} sec')\nimage.save(\"naruto.png\")\n\nstart = time()\nimage = pipe(prompt, num_inference_steps=inference_steps).images[0]\nprint(f'Inference time is {time()-start} sec after compilation')\n```\n\n----------------------------------------\n\nTITLE: Installing ControlNet Auxiliary Dependencies\nDESCRIPTION: Installation command for the controlnet_aux library required for using the ZoeDepth estimator.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/advanced_inference/outpaint.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q controlnet_aux\n```\n\n----------------------------------------\n\nTITLE: Using LoRA with Kohya Checkpoints\nDESCRIPTION: This snippet shows how to load a LoRA trained with Kohya's script. It uses the AutoPipelineForText2Image to load the base Stable Diffusion model and then loads a Kohya LoRA checkpoint. The bl3uprint trigger word is added to the prompt to activate the LoRA effect.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# LoRA를 트리거하기 위해 bl3uprint를 프롬프트에 사용\nprompt = \"bl3uprint, a highly detailed blueprint of the eiffel tower, explaining how to build all parts, many txt, blueprint grid backdrop\"\nimage = pipeline(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Memory Optimization by Removing Components After Precomputation\nDESCRIPTION: After precomputing embeddings, this code frees memory by deleting the text encoders, tokenizers, and VAE which are no longer needed during the training loop.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndel text_encoders, tokenizers, vae\ngc.collect()\ntorch.cuda.empty_cache()\n```\n\n----------------------------------------\n\nTITLE: Loading and Inferencing with Textual Inversion Model in PyTorch\nDESCRIPTION: Load a trained textual inversion model and perform inference using StableDiffusionPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text_inversion.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"cat-backpack.png\")\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for DreamBooth Training Script\nDESCRIPTION: This code snippet shows how to navigate to the DreamBooth training script folder and install the required packages listed in the requirements.txt file. It also includes an additional command for installing specific dependencies for the SDXL model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/overview.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/dreambooth\npip install -r requirements.txt\n# to train SDXL with DreamBooth\npip install -r requirements_sdxl.txt\n```\n\n----------------------------------------\n\nTITLE: Loading Kandinsky 2.2 Prior Pipeline for Interpolation\nDESCRIPTION: Initializes the Kandinsky 2.2 Prior Pipeline and loads two sample images to be used for interpolation. The code prepares the environment and displays the source images using make_image_grid.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\nfrom diffusers.utils import load_image, make_image_grid\nimport torch\n\nprior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\nimg_1 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\nimg_2 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg\")\nmake_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing Optimizer for LoRA Layers\nDESCRIPTION: Configure the optimizer to only update the trainable LoRA layers during training\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lora.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optimizer_cls(\n    lora_layers,\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing UNet3DConditionOutput in Python\nDESCRIPTION: This snippet demonstrates how to import the UNet3DConditionOutput class from the diffusers library. This class is likely used to represent the output of a 3D conditional UNet model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet-motion.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.models.unets.unet_3d_condition import UNet3DConditionOutput\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample ImageNet Dataset\nDESCRIPTION: Downloads and extracts a sample dataset of ImageNet images for evaluation purposes using requests and zipfile libraries.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom zipfile import ZipFile\nimport requests\n\ndef download(url, local_filepath):\n    r = requests.get(url)\n    with open(local_filepath, \"wb\") as f:\n        f.write(r.content)\n    return local_filepath\n\ndummy_dataset_url = \"https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/sample-imagenet-images.zip\"\nlocal_filepath = download(dummy_dataset_url, dummy_dataset_url.split(\"/\")[-1])\n\nwith ZipFile(local_filepath, \"r\") as zipper:\n    zipper.extractall(\".\")\n```\n\n----------------------------------------\n\nTITLE: Cloning Diffusers Repository\nDESCRIPTION: This command clones the Hugging Face Diffusers repository from GitHub, allowing access to the example scripts and source code.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/text_to_image/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"git clone https://github.com/huggingface/diffusers\ncd diffusers\"\n```\n\n----------------------------------------\n\nTITLE: Importing FluxTransformer2DModel in Markdown\nDESCRIPTION: This snippet demonstrates how to include auto-generated documentation for the FluxTransformer2DModel class using the autodoc feature. It's typically used in documentation systems that support automatic API documentation generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/flux_transformer.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[[autodoc]] FluxTransformer2DModel\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Bash\nDESCRIPTION: This command launches the training process for the Flux.1 Dev model using the precomputed embeddings and various training parameters. The command uses the Accelerate library for optimized multi-GPU training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file=accelerate.yaml \\\n  train_dreambooth_lora_flux_miniature.py \\\n  --pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\" \\\n  --data_df_path=\"embeddings.parquet\" \\\n  --output_dir=\"yarn_art_lora_flux_nf4\" \\\n  --mixed_precision=\"fp16\" \\\n  --use_8bit_adam \\\n  --weighting_scheme=\"none\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --repeats=1 \\\n  --learning_rate=1e-4 \\\n  --guidance_scale=1 \\\n  --report_to=\"wandb\" \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --cache_latents \\\n  --rank=4 \\\n  --max_train_steps=700 \\\n  --seed=\"0\"\n```\n\n----------------------------------------\n\nTITLE: Implementing CLIP Guided Img2Img Stable Diffusion with CompVis Model\nDESCRIPTION: This code demonstrates how to use CLIP guided Img2Img stable diffusion to generate more realistic images from an initial image. It uses a CLIP model to guide the stable diffusion model at every denoising step, producing higher quality results. This implementation requires approximately 12GB of GPU RAM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\nimport requests\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom PIL import Image\nfrom transformers import CLIPImageProcessor, CLIPModel\n\n# Load CLIP model and feature extractor\nfeature_extractor = CLIPImageProcessor.from_pretrained(\n    \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n)\nclip_model = CLIPModel.from_pretrained(\n    \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", torch_dtype=torch.float16\n)\n\n# Load guided pipeline\nguided_pipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"clip_guided_stable_diffusion_img2img\",\n    clip_model=clip_model,\n    feature_extractor=feature_extractor,\n    torch_dtype=torch.float16,\n)\nguided_pipeline.enable_attention_slicing()\nguided_pipeline = guided_pipeline.to(\"cuda\")\n\n# Define prompt and fetch image\nprompt = \"fantasy book cover, full moon, fantasy forest landscape, golden vector elements, fantasy magic, dark light night, intricate, elegant, sharp focus, illustration, highly detailed, digital painting, concept art, matte, art by WLOP and Artgerm and Albert Bierstadt, masterpiece\"\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\nresponse = requests.get(url)\nedit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet lists the required Python packages and their minimum versions (where specified) for the Hugging Face Diffusers project. It includes accelerate for optimization, torchvision for computer vision tasks, transformers for NLP models, ftfy for text processing, tensorboard for visualization, and Jinja2 for templating.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/consistency_training/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\n```\n\n----------------------------------------\n\nTITLE: Defining RMSE Loss Function for Diffusion Posterior Sampling in Python\nDESCRIPTION: Implements the Root Mean Square Error (RMSE) loss function used in the Diffusion Posterior Sampling Pipeline for comparing generated and target images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_85\n\nLANGUAGE: python\nCODE:\n```\ndef RMSELoss(yhat, y):\n    return torch.sqrt(torch.sum((yhat-y)**2))\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Diffusers from Source\nDESCRIPTION: This code snippet demonstrates how to clone the Diffusers repository from GitHub and install it in a new virtual environment. This ensures that you have the latest version of the library and any example-specific requirements.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Loading a Diffusion Pipeline from the Hub\nDESCRIPTION: Load a pre-trained diffusion model pipeline from the Hugging Face Hub using the DiffusionPipeline class, which automatically detects the appropriate pipeline type and downloads the necessary components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = DiffusionPipeline.from_pretrained(repo_id)\n```\n\n----------------------------------------\n\nTITLE: Inferencing with CogVideoX LoRA in Python\nDESCRIPTION: Python code for loading a trained LoRA model with CogVideoX for video generation. It demonstrates how to load the LoRA weights and generate a video from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.utils import export_to_video\n\npipe = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-2b\", torch_dtype=torch.float16)\n# pipe.load_lora_weights(\"/path/to/lora/weights\", adapter_name=\"cogvideox-lora\") # Or,\npipe.load_lora_weights(\"my-awesome-hf-username/my-awesome-lora-name\", adapter_name=\"cogvideox-lora\") # If loading from the HF Hub\npipe.to(\"cuda\")\n\n# Assuming lora_alpha=32 and rank=64 for training. If different, set accordingly\npipe.set_adapters([\"cogvideox-lora\"], [32 / 64])\n\nprompt = \"A vast, shimmering ocean flows gracefully under a twilight sky, its waves undulating in a mesmerizing dance of blues and greens. The surface glints with the last rays of the setting sun, casting golden highlights that ripple across the water. Seagulls soar above, their cries blending with the gentle roar of the waves. The horizon stretches infinitely, where the ocean meets the sky in a seamless blend of hues. Close-ups reveal the intricate patterns of the waves, capturing the fluidity and dynamic beauty of the sea in motion.\"\nframes = pipe(prompt, guidance_scale=6, use_dynamic_cfg=True).frames[0]\nexport_to_video(frames, \"output.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Weights in Python\nDESCRIPTION: Shows how to load and use LoRA weights with a base model for generating stylized images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"ostris/super-cereal-sdxl-lora\", weight_name=\"cereal_box_sdxl_v1.safetensors\")\nprompt = \"bears, pizza bites\"\nimage = pipeline(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL Pipeline with Multiple LoRAs\nDESCRIPTION: Initializes a Stable Diffusion XL pipeline and loads two LoRA models with custom adapter names for merging.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\", weight_name=\"ikea_instructions_xl_v1_5.safetensors\", adapter_name=\"ikea\")\npipeline.load_lora_weights(\"lordjia/by-feng-zikai\", weight_name=\"fengzikai_v1.0_XL.safetensors\", adapter_name=\"feng\")\n```\n\n----------------------------------------\n\nTITLE: Quantized Video Generation with Mochi Pipeline using BitsAndBytes\nDESCRIPTION: Demonstrates loading and using a quantized Mochi model with 8-bit precision using BitsAndBytes configuration for reduced memory usage while generating videos.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/mochi.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, MochiTransformer3DModel, MochiPipeline\nfrom diffusers.utils import export_to_video\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"genmo/mochi-1-preview\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = MochiTransformer3DModel.from_pretrained(\n    \"genmo/mochi-1-preview\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = MochiPipeline.from_pretrained(\n    \"genmo/mochi-1-preview\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nvideo = pipeline(\n  \"Close-up of a cats eye, with the galaxy reflected in the cats eye. Ultra high resolution 4k.\",\n  num_inference_steps=28,\n  guidance_scale=3.5\n).frames[0]\nexport_to_video(video, \"cat.mp4\")\n```\n\n----------------------------------------\n\nTITLE: SDXL Inference with Fine-tuned Model\nDESCRIPTION: Python code for running inference with a fine-tuned SDXL model, including loading the model with proper data types and generating an image from a prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nmodel_path = \"you-model-id-goes-here\" # <-- change this\npipe = DiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nprompt = \"A naruto with green eyes and red legs.\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing FlaxStableDiffusionControlNetPipelineOutput in Python\nDESCRIPTION: This snippet shows the import statement for the FlaxStableDiffusionControlNetPipelineOutput class, which represents the output of the Flax implementation of the ControlNet pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Loading EasyAnimateTransformer3DModel in Python\nDESCRIPTION: This code snippet demonstrates how to load the EasyAnimateTransformer3DModel from a pre-trained checkpoint. It uses the 'from_pretrained' method to load the model, specifying the model path, subfolder, and torch data type.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/easyanimate_transformer3d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import EasyAnimateTransformer3DModel\n\ntransformer = EasyAnimateTransformer3DModel.from_pretrained(\"alibaba-pai/EasyAnimateV5.1-12b-zh\", subfolder=\"transformer\", torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading a LoRA Adapter with a Custom Name\nDESCRIPTION: Loads the toy-face LoRA adapter from the CiroN2022/toy-face repository with a custom adapter name 'toy' to allow easy switching between adapters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe.load_lora_weights(\"CiroN2022/toy-face\", weight_name=\"toy_face_sdxl.safetensors\", adapter_name=\"toy\")\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum-Quanto Package\nDESCRIPTION: Command to install the optimum-quanto package required for FP8 inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/flux.md#2025-04-11_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npip install optimum-quanto\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Generation with Kandinsky 2.2\nDESCRIPTION: Performs image-to-image generation using Kandinsky 2.2 model, incorporating the original image and embeddings. It also creates a grid to compare the original and generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import make_image_grid\n\nimage = pipeline(image=original_image, image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768, strength=0.3).images[0]\nmake_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with DiT Pipeline\nDESCRIPTION: Uses the DiT Pipeline to generate synthetic images conditioned on specific ImageNet classes for comparison.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiTPipeline, DPMSolverMultistepScheduler\n\ndit_pipeline = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-256\", torch_dtype=torch.float16)\ndit_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(dit_pipeline.scheduler.config)\ndit_pipeline = dit_pipeline.to(\"cuda\")\n\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nwords = [\n    \"cassette player\",\n    \"chainsaw\",\n    \"chainsaw\",\n    \"church\",\n    \"gas pump\",\n    \"gas pump\",\n    \"gas pump\",\n    \"parachute\",\n    \"parachute\",\n    \"tench\",\n]\n\nclass_ids = dit_pipeline.get_label_ids(words)\noutput = dit_pipeline(class_labels=class_ids, generator=generator, output_type=\"np\")\n\nfake_images = output.images\nfake_images = torch.tensor(fake_images)\nfake_images = fake_images.permute(0, 3, 1, 2)\nprint(fake_images.shape)\n```\n\n----------------------------------------\n\nTITLE: Creating PEFT Model from LoRA\nDESCRIPTION: Creates a PEFT model by combining SDXL UNet with LoRA weights and configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom peft import get_peft_model, LoraConfig\nimport copy\n\nsdxl_unet = copy.deepcopy(unet)\nikea_peft_model = get_peft_model(\n    sdxl_unet,\n    pipeline.unet.peft_config[\"ikea\"],\n    adapter_name=\"ikea\"\n)\n\noriginal_state_dict = {f\"base_model.model.{k}\": v for k, v in pipeline.unet.state_dict().items()}\nikea_peft_model.load_state_dict(original_state_dict, strict=True)\n```\n\n----------------------------------------\n\nTITLE: Launching Diffusers Server\nDESCRIPTION: Command to start the Diffusers pipeline server on localhost.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/create_a_server.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython server.py\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and PyTorch/XLA on TPU\nDESCRIPTION: This snippet demonstrates how to SSH into the TPU VM and install the nightly versions of PyTorch and PyTorch/XLA. Ensure that all commands are executed correctly to get the proper environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/training/text_to_image/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\\n--project=${PROJECT_ID} --zone=${ZONE} --worker=all \\\n--command='pip3 install --pre torch==2.6.0.dev20241031+cpu torchvision --index-url https://download.pytorch.org/whl/nightly/cpu\npip3 install \"torch_xla[tpu] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.6.0.dev20241031.cxx11-cp310-cp310-linux_x86_64.whl\" -f https://storage.googleapis.com/libtpu-releases/index.html\npip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html'\n```\n\n----------------------------------------\n\nTITLE: Saving Diffusion Pipeline to Multi-folder Layout\nDESCRIPTION: Demonstrates how to save a diffusion pipeline to the multi-folder layout using the save_pretrained method, which creates a directory structure and saves files in safetensors format by default.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\n\npipeline = StableDiffusionXLPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors\",\n)\npipeline.save_pretrained()\n```\n\n----------------------------------------\n\nTITLE: Accelerating AnimateDiff with TCD Scheduler\nDESCRIPTION: This example demonstrates integrating the TCD Scheduler with AnimateDiff to generate high-quality animated sequences using fewer inference steps. It combines TCD-LoRA with a motion adapter LoRA to generate smooth animated sequences in just 5 inference steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_tcd_lora.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler\nfrom scheduling_tcd import TCDScheduler\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5\")\npipe = AnimateDiffPipeline.from_pretrained(\n    \"frankjoshua/toonyou_beta6\",\n    motion_adapter=adapter,\n).to(\"cuda\")\n\n# set TCDScheduler\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n\n# load TCD LoRA\npipe.load_lora_weights(\"h1t/TCD-SD15-LoRA\", adapter_name=\"tcd\")\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", weight_name=\"diffusion_pytorch_model.safetensors\", adapter_name=\"motion-lora\")\n\npipe.set_adapters([\"tcd\", \"motion-lora\"], adapter_weights=[1.0, 1.2])\n\nprompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\ngenerator = torch.manual_seed(0)\nframes = pipe(\n    prompt=prompt,\n    num_inference_steps=5,\n    guidance_scale=0,\n    cross_attention_kwargs={\"scale\": 1},\n    num_frames=24,\n    eta=0.3,\n    generator=generator\n).frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Training FP32 Textual Inversion Model\nDESCRIPTION: Script to fine-tune Stable Diffusion model for Textual Inversion. Sets up model parameters, training configuration, and generates a FP32 model output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"./dicoo\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"dicoo_model\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for Pipeline Testing Command\nDESCRIPTION: Example command for running slow tests for new model implementations using pytest.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nRUN_SLOW=1 python -m pytest tests/test_my_new_model.py\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Stage 2 for ControlNet Training in YAML\nDESCRIPTION: YAML configuration for DeepSpeed stage 2 with CPU offloading for optimizers and parameters. This configuration helps reduce VRAM usage by offloading tensors to CPU, requiring approximately 25GB of RAM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  gradient_accumulation_steps: 4\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: false\n  zero_stage: 2\ndistributed_type: DEEPSPEED\n```\n\n----------------------------------------\n\nTITLE: TPU Setup and Verification in JAX\nDESCRIPTION: Code to setup and verify TPU configuration in Colab environment and check device count and type\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.tools.colab_tpu\njax.tools.colab_tpu.setup_tpu()\n\nnum_devices = jax.device_count()\ndevice_type = jax.devices()[0].device_kind\n\nprint(f\"Found {num_devices} JAX devices of type {device_type}.\")\nassert (\n    \"TPU\" in device_type,\n    \"Available device is not a TPU, please select TPU from Runtime > Change runtime type > Hardware accelerator\"\n)\n```\n\n----------------------------------------\n\nTITLE: Training FLUX.1 with Pivotal Tuning, LoRA, and T5 Optimization\nDESCRIPTION: Extends the previous training command by adding T5 textual inversion optimization. This setup optimizes both CLIP and T5 embeddings for the custom style tokens.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"black-forest-labs/FLUX.1-dev\"\nexport DATASET_NAME=\"./3d_icon\"\nexport OUTPUT_DIR=\"3d-icon-Flux-LoRA\"\n\naccelerate launch train_dreambooth_lora_flux_advanced.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --instance_prompt=\"3d icon in the style of TOK\" \\\n  --output_dir=$OUTPUT_DIR \\\n  --caption_column=\"prompt\" \\\n  --mixed_precision=\"bf16\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --repeats=1 \\\n  --report_to=\"wandb\"\\\n  --gradient_accumulation_steps=1 \\\n  --gradient_checkpointing \\\n  --learning_rate=1.0 \\\n  --text_encoder_lr=1.0 \\\n  --optimizer=\"prodigy\"\\\n  --train_text_encoder_ti\\\n  --enable_t5_ti\\\n  --train_text_encoder_ti_frac=0.5\\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --rank=8 \\\n  --max_train_steps=700 \\\n  --checkpointing_steps=2000 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: SchedulerMixin Class Documentation\nDESCRIPTION: Markdown documentation showing the base scheduler mixin class that implements common utilities shared across all schedulers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/overview.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## SchedulerMixin\\n[[autodoc]] SchedulerMixin\n```\n\n----------------------------------------\n\nTITLE: Training Flux Control LoRA\nDESCRIPTION: Accelerate launch command for training Flux Control with LoRA using pose conditions\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/flux-control/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_control_lora_flux.py \\\n  --pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\" \\\n  --dataset_name=\"raulc0399/open_pose_controlnet\" \\\n  --output_dir=\"pose-control-lora\" \\\n  --mixed_precision=\"bf16\" \\\n  --train_batch_size=1 \\\n  --rank=64 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=5000 \\\n  --validation_image=\"openpose.png\" \\\n  --validation_prompt=\"A couple, 4k photo, highly detailed\" \\\n  --offload \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Retrieving Active LoRA Adapters in Diffusers Pipeline\nDESCRIPTION: This snippet demonstrates how to use the get_active_adapters() method to check the list of active LoRA adapters attached to a Diffusers pipeline. It returns a list of adapter names.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nactive_adapters = pipe.get_active_adapters()\nactive_adapters\n[\"toy\", \"pixel\"]\n```\n\n----------------------------------------\n\nTITLE: Loading HunyuanVideoTransformer3DModel in Python\nDESCRIPTION: This code snippet demonstrates how to load the HunyuanVideoTransformer3DModel from a pre-trained checkpoint. It uses the 'from_pretrained' method to load the model with specific parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/hunyuan_video_transformer_3d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import HunyuanVideoTransformer3DModel\n\ntransformer = HunyuanVideoTransformer3DModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Encoding Edited Images into Latent Space\nDESCRIPTION: This code snippet encodes the edited images into latent space during the training loop, applying scaling factors as per the VAE configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlatents = vae.encode(batch[\"edited_pixel_values\"].to(weight_dtype)).latent_dist.sample()\nlatents = latents * vae.config.scaling_factor\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet enumerates the required Python packages for the Diffusers project. It includes specific version requirements for some packages to ensure compatibility.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running Inference\nDESCRIPTION: Compiles the optimized models and performs inference with the specified prompt\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npipe.unet = torch.compile(pipe.unet, mode=\"max-autotune\", fullgraph=True)\npipe.vae.decode = torch.compile(pipe.vae.decode, mode=\"max-autotune\", fullgraph=True)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```\n\n----------------------------------------\n\nTITLE: Inference with Fine-tuned Decoder LoRA\nDESCRIPTION: Python code for running inference using a fine-tuned Kandinsky decoder model with LoRA weights. Demonstrates loading the model and generating images from text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipe.unet.load_attn_procs(output_dir)\npipe.enable_model_cpu_offload()\n\nprompt='A robot naruto, 4k photo'\nimage = pipe(prompt=prompt).images[0]\nimage.save(\"robot_naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Initialize Accelerate Environment with Python\nDESCRIPTION: Use Python to configure a non-interactive Accelerate environment, especially useful in environments that do not support an interactive shell, such as Jupyter notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/realfill/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Setting up IP-Adapter FaceID Plus\nDESCRIPTION: Configures IP-Adapter FaceID Plus with both InsightFace and CLIP embeddings for improved photorealism.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import CLIPVisionModelWithProjection\n\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(\n    \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n    torch_dtype=torch.float16,\n)\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    image_encoder=image_encoder,\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter-FaceID\", subfolder=None, weight_name=\"ip-adapter-faceid-plus_sd15.bin\")\n```\n\n----------------------------------------\n\nTITLE: Improving Text-to-Video Quality with Full Precision Decoding\nDESCRIPTION: Shows how to improve the quality of generated videos by running the decoding step in full precision. This involves loading the VAE in float32 precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import WanPipeline, AutoencoderKLWan\nfrom diffusers.utils import export_to_video\n\nmodel_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\npipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)\n\npipe.enable_model_cpu_offload()\n\nprompt = \"A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.\"\nnegative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\nnum_frames = 33\n\nframes = pipe(prompt=prompt, num_frames=num_frames).frames[0]\nexport_to_video(frames, \"wan-t2v.mp4\", fps=16)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Stable Diffusion Inpainting Pipeline\nDESCRIPTION: This Python example outlines benchmarking for a Stable Diffusion inpainting pipeline, highlighting the use of `torch.compile` to optimize performance in filling missing parts of images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom diffusers.utils import load_image\nimport torch\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = load_image(img_url).resize((512, 512))\nmask_image = load_image(mask_url).resize((512, 512))\n\npath = \"runwayml/stable-diffusion-inpainting\"\n\nrun_compile = True  # Set True / False\n\npipe = StableDiffusionInpaintPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Command to initialize the Accelerate environment for distributed training setup.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/vqgan/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Performing the Guided Image Edit with DiffEdit\nDESCRIPTION: Running the DiffEdit pipeline with the mask, inverted latents, and text prompts to generate the edited image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noutput_image = pipeline(\n    prompt=target_prompt,\n    mask_image=mask_image,\n    image_latents=inv_latents,\n    negative_prompt=source_prompt,\n).images[0]\nmask_image = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\").resize((768, 768))\nmake_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Downloading Training Dataset\nDESCRIPTION: Download a sample dataset for textual inversion training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text_inversion.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./cat\"\nsnapshot_download(\n    \"diffusers/cat_toy_example\", local_dir=local_dir, repo_type=\"dataset\", ignore_patterns=\".gitattributes\"\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Activating a Second LoRA Adapter\nDESCRIPTION: Loads a pixel-art LoRA adapter and activates it using the set_adapters method, replacing the previously active toy-face adapter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe.load_lora_weights(\"nerijs/pixel-art-xl\", weight_name=\"pixel-art-xl.safetensors\", adapter_name=\"pixel\")\npipe.set_adapters(\"pixel\")\n```\n\n----------------------------------------\n\nTITLE: Launching Diffusion Model Training with Accelerate\nDESCRIPTION: Uses Accelerate's notebook_launcher function to start the training process with the specified number of processes, enabling distributed training across multiple GPUs if available.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import notebook_launcher\n\nargs = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\nnotebook_launcher(train_loop, args, num_processes=1)\n```\n\n----------------------------------------\n\nTITLE: Importing FluxControlNetPipeline in Python\nDESCRIPTION: This code snippet shows how to import the FluxControlNetPipeline class, which is the main implementation of ControlNet for Flux.1. It allows users to generate images with additional control inputs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet_flux.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxControlNetPipeline\n```\n\n----------------------------------------\n\nTITLE: Inpainting Setup with Kandinsky 2.1\nDESCRIPTION: Sets up the prior and inpainting pipelines for Kandinsky 2.1, loads an initial image, and creates a mask for inpainting.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline\nfrom diffusers.utils import load_image, make_image_grid\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = KandinskyInpaintPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-inpaint\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\nmask = np.zeros((768, 768), dtype=np.float32)\n# mask area above cat's head\nmask[:250, 250:-250] = 1\n```\n\n----------------------------------------\n\nTITLE: Generating Keyframes with Diffusion Pipeline\nDESCRIPTION: Generates 8 video frames at 2fps using a diffusion pipeline with specific prompt embeddings. Configures frame dimensions of 64x40 pixels with customized inference steps and guidance scale.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nvideo_frames = pipeline(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    num_frames=8,\n    height=40,\n    width=64,\n    num_inference_steps=2,\n    guidance_scale=9.0,\n    output_type=\"pt\"\n).frames\n```\n\n----------------------------------------\n\nTITLE: Merging LoRAs with add_weighted_adapter\nDESCRIPTION: Demonstrates merging LoRAs using the DARE linear method and generating an image with the merged model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel.add_weighted_adapter(\n    adapters=[\"ikea\", \"feng\"],\n    weights=[1.0, 1.0],\n    combination_type=\"dare_linear\",\n    adapter_name=\"ikea-feng\"\n)\nmodel.set_adapters(\"ikea-feng\")\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for Custom Diffusion\nDESCRIPTION: Commands to install the necessary dependencies for running custom diffusion training, including the clip-retrieval package for image collection.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npip install clip-retrieval\n```\n\n----------------------------------------\n\nTITLE: Configuring DDIMScheduler with Zero SNR Rescaling in Python\nDESCRIPTION: This snippet demonstrates how to configure the DDIMScheduler with zero SNR rescaling, which is proposed to improve inference results according to recent research.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/ddim.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, rescale_betas_zero_snr=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Stable Diffusion Pipeline\nDESCRIPTION: Uses the Stable Diffusion pipeline to generate images based on the sample prompts. This demonstrates how to use a diffusion model for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/evaluation.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nimages = sd_pipeline(sample_prompts, num_images_per_prompt=1, generator=generator).images\n```\n\n----------------------------------------\n\nTITLE: Documenting MarigoldImageProcessor visualize_intrinsics Method\nDESCRIPTION: Autodoc directive for generating documentation for the visualize_intrinsics method of the MarigoldImageProcessor class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n[[autodoc]] pipelines.marigold.marigold_image_processing.MarigoldImageProcessor.visualize_intrinsics\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusion3ControlNetInpaintingPipeline in Python\nDESCRIPTION: This snippet demonstrates how to import the StableDiffusion3ControlNetInpaintingPipeline class. This pipeline is specifically designed for inpainting tasks using ControlNet with Stable Diffusion 3.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet_sd3.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.controlnet_sd3.pipeline_stable_diffusion_3_controlnet_inpainting import StableDiffusion3ControlNetInpaintingPipeline\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Training Data\nDESCRIPTION: This Python function defines how to preprocess images and edit prompts for the training dataset, reshaping the images and tokenizing the captions to prepare them for model ingestion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_train(examples):\n    preprocessed_images = preprocess_images(examples)\n\n    original_images, edited_images = preprocessed_images.chunk(2)\n    original_images = original_images.reshape(-1, 3, args.resolution, args.resolution)\n    edited_images = edited_images.reshape(-1, 3, args.resolution, args.resolution)\n\n    examples[\"original_pixel_values\"] = original_images\n    examples[\"edited_pixel_values\"] = edited_images\n\n    captions = list(examples[edit_prompt_column])\n    examples[\"input_ids\"] = tokenize_captions(captions)\n    return examples\n```\n\n----------------------------------------\n\nTITLE: Training T2I-Adapter with SDXL Base Model\nDESCRIPTION: Script for training the T2I-Adapter using the Stable Diffusion XL base model. It sets up environment variables, configures training parameters such as learning rate and batch size, and includes validation steps to monitor progress.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_t2i_adapter_sdxl.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --mixed_precision=\"fp16\" \\\n --resolution=1024 \\\n --learning_rate=1e-5 \\\n --max_train_steps=15000 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --validation_steps=100 \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --report_to=\"wandb\" \\\n --seed=42 \\\n --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving Model Variants in Diffusers\nDESCRIPTION: Demonstrates loading a specific model variant and then saving it locally. This example loads the 'non_ema' variant of the UNet from stable-diffusion-v1-5 and saves it to a local directory, preserving the variant information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/schedulers.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"unet\", variant=\"non_ema\", use_safetensors=True\n)\nunet.save_pretrained(\"./local-unet\", variant=\"non_ema\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Dog Dataset for DreamBooth Training\nDESCRIPTION: Downloads a sample dog dataset from the Hugging Face Hub for use in DreamBooth training, setting up the instance directory for the training process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir,\n    repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an Image Grid Helper Function for Visualization\nDESCRIPTION: Python helper function to arrange multiple generated images into a grid for easy visualization and comparison of diffusion model outputs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\n\ndef image_grid(imgs, rows=2, cols=2):\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid\n\n\nimage_grid(images)\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderKLHunyuanVideo model from pretrained weights\nDESCRIPTION: This code snippet demonstrates how to load the pretrained HunyuanVideo VAE model from the Hugging Face model hub. It initializes the model with float16 precision for better performance on compatible hardware.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoder_kl_hunyuan_video.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKLHunyuanVideo\n\nvae = AutoencoderKLHunyuanVideo.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder=\"vae\", torch_dtype=torch.float16)\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Configuration for Accelerate in Non-Interactive Environments\nDESCRIPTION: In this snippet, the necessary function imports from the Accelerate library to swiftly write a basic configuration when the environment does not support an interactive shell.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Loading Textual Inversion Embeddings in Diffusers Pipeline\nDESCRIPTION: Loads textual inversion embeddings from a state dictionary into the secondary text encoder (CLIP ViT-G/14) of a diffusion pipeline, assigning them to specific tokens.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# load embeddings of text_encoder 2 (CLIP ViT-G/14)\npipe.load_textual_inversion(state_dict[\"clip_g\"], token=[\"<s0>\", \"<s1>\"], text_encoder=pipe.text_encoder_2, tokenizer=pipe.tokenizer_2)\n```\n\n----------------------------------------\n\nTITLE: Importing DiTTransformer2DModel in Python\nDESCRIPTION: This code snippet demonstrates how to import the DiTTransformer2DModel class from the HuggingFace diffusers library. It's a placeholder for the actual implementation details that would be provided by the autodoc functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/dit_transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiTTransformer2DModel\n```\n\n----------------------------------------\n\nTITLE: Generating Identity-Preserving Videos with ConsisID\nDESCRIPTION: Code for processing face images to extract identity embeddings and generating identity-preserving videos from text prompts. The example demonstrates how to prepare inputs, generate the video, and export it to a file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/consisid.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import export_to_video\n\nprompt = \"The video captures a boy walking along a city street, filmed in black and white on a classic 35mm camera. His expression is thoughtful, his brow slightly furrowed as if he's lost in contemplation. The film grain adds a textured, timeless quality to the image, evoking a sense of nostalgia. Around him, the cityscape is filled with vintage buildings, cobblestone sidewalks, and softly blurred figures passing by, their outlines faint and indistinct. Streetlights cast a gentle glow, while shadows play across the boy's path, adding depth to the scene. The lighting highlights the boy's subtle smile, hinting at a fleeting moment of curiosity. The overall cinematic atmosphere, complete with classic film still aesthetics and dramatic contrasts, gives the scene an evocative and introspective feel.\"\nimage = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/consisid/consisid_input.png?download=true\"\n\nid_cond, id_vit_hidden, image, face_kps = process_face_embeddings_infer(face_helper_1, face_clip_model, face_helper_2, eva_transform_mean, eva_transform_std, face_main_model, \"cuda\", torch.bfloat16, image, is_align_face=True)\n\nvideo = pipe(image=image, prompt=prompt, num_inference_steps=50, guidance_scale=6.0, use_dynamic_cfg=False, id_vit_hidden=id_vit_hidden, id_cond=id_cond, kps_cond=face_kps, generator=torch.Generator(\"cuda\").manual_seed(42))\nexport_to_video(video.frames[0], \"output.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Launching ControlNet Training with Mixed Precision\nDESCRIPTION: This bash script launches the ControlNet training script with mixed precision enabled using the `accelerate launch` command.  The `--mixed_precision` parameter specifies the use of fp16 format for faster training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate launch train_controlnet.py \\\n  --mixed_precision=\\\"fp16\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion XL Pipeline\nDESCRIPTION: Initializes a Stable Diffusion XL pipeline using the stabilityai/stable-diffusion-xl-base-1.0 checkpoint, with the model loaded in float16 precision on a CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipe = DiffusionPipeline.from_pretrained(pipe_id, torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing with torch.compile\nDESCRIPTION: Complete example showing how to load, fuse LoRA weights, and optimize the pipeline using torch.compile for faster inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\n# load base model and LoRAs\npipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\", weight_name=\"ikea_instructions_xl_v1_5.safetensors\", adapter_name=\"ikea\")\npipeline.load_lora_weights(\"lordjia/by-feng-zikai\", weight_name=\"fengzikai_v1.0_XL.safetensors\", adapter_name=\"feng\")\n\n# activate both LoRAs and set adapter weights\npipeline.set_adapters([\"ikea\", \"feng\"], adapter_weights=[0.7, 0.8])\n\n# fuse LoRAs and unload weights\npipeline.fuse_lora(adapter_names=[\"ikea\", \"feng\"], lora_scale=1.0)\npipeline.unload_lora_weights()\n\n# torch.compile\npipeline.unet.to(memory_format=torch.channels_last)\npipeline.unet = torch.compile(pipeline.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nimage = pipeline(\"A bowl of ramen shaped like a cute kawaii bear, by Feng Zikai\", generator=torch.manual_seed(0)).images[0]\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance of Original and IPEX-Optimized SDXL Pipelines in Python\nDESCRIPTION: This extensive code snippet compares the performance of the original Stable Diffusion XL pipeline with the IPEX-optimized version. It includes initialization, performance measurement, and comparison for both BFloat16 and Float32 data types.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom pipeline_stable_diffusion_xl_ipex import StableDiffusionXLPipelineIpex\nimport time\n\nprompt = \"sailing ship in storm by Rembrandt\"\nmodel_id = \"stabilityai/sdxl-turbo\"\nsteps = 4\n\n# Helper function for time evaluation\ndef elapsed_time(pipeline, nb_pass=3, num_inference_steps=1):\n    # warmup\n    for _ in range(2):\n        images = pipeline(prompt, num_inference_steps=num_inference_steps, height=512, width=512, guidance_scale=0.0).images\n    # time evaluation\n    start = time.time()\n    for _ in range(nb_pass):\n        pipeline(prompt, num_inference_steps=num_inference_steps, height=512, width=512, guidance_scale=0.0)\n    end = time.time()\n    return (end - start) / nb_pass\n\n##############     bf16 inference performance    ###############\n\n# 1. IPEX Pipeline initialization\npipe = StableDiffusionXLPipelineIpex.from_pretrained(model_id, low_cpu_mem_usage=True, use_safetensors=True)\npipe.prepare_for_ipex(torch.bfloat16, prompt, height=512, width=512)\n\n# 2. Original Pipeline initialization\npipe2 = StableDiffusionXLPipeline.from_pretrained(model_id, low_cpu_mem_usage=True, use_safetensors=True)\n\n# 3. Compare performance between Original Pipeline and IPEX Pipeline\nwith torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n    latency = elapsed_time(pipe, num_inference_steps=steps)\n    print(\"Latency of StableDiffusionXLPipelineIpex--bf16\", latency, \"s for total\", steps, \"steps\")\n    latency = elapsed_time(pipe2, num_inference_steps=steps)\n    print(\"Latency of StableDiffusionXLPipeline--bf16\", latency, \"s for total\", steps, \"steps\")\n\n##############     fp32 inference performance    ###############\n\n# 1. IPEX Pipeline initialization\npipe3 = StableDiffusionXLPipelineIpex.from_pretrained(model_id, low_cpu_mem_usage=True, use_safetensors=True)\npipe3.prepare_for_ipex(torch.float32, prompt, height=512, width=512)\n\n# 2. Original Pipeline initialization\npipe4 = StableDiffusionXLPipeline.from_pretrained(model_id, low_cpu_mem_usage=True, use_safetensors=True)\n\n# 3. Compare performance between Original Pipeline and IPEX Pipeline\nlatency = elapsed_time(pipe3, num_inference_steps=steps)\nprint(\"Latency of StableDiffusionXLPipelineIpex--fp32\", latency, \"s for total\", steps, \"steps\")\nlatency = elapsed_time(pipe4, num_inference_steps=steps)\nprint(\"Latency of StableDiffusionXLPipeline--fp32\", latency, \"s for total\", steps, \"steps\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Multiple Prompts for Evaluation\nDESCRIPTION: Generates images using the Stable Diffusion pipeline with multiple prompts. This is used to create a set of images for quantitative evaluation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/evaluation.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"a photo of an astronaut riding a horse on mars\",\n    \"A high tech solarpunk utopia in the Amazon rainforest\",\n    \"A pikachu fine dining with a view to the Eiffel Tower\",\n    \"A mecha robot in a favela in expressionist style\",\n    \"an insect robot preparing a delicious meal\",\n    \"A small cabin on top of a snowy mountain in the style of Disney, artstation\",\n]\n\nimages = sd_pipeline(prompts, num_images_per_prompt=1, output_type=\"np\").images\n\nprint(images.shape)\n# (6, 512, 512, 3)\n```\n\n----------------------------------------\n\nTITLE: Installing and Running CLIP Retrieval for Multiple Concepts\nDESCRIPTION: This script installs the clip-retrieval package and runs the retrieve.py script to collect real images for regularization during multi-concept training. These images will be used as part of the prior preservation mechanism.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install clip-retrieval\npython retrieve.py --class_prompt {} --class_data_dir {} --num_class_images 200\n```\n\n----------------------------------------\n\nTITLE: Defining Conditioning Image Transformations\nDESCRIPTION: This Python code defines a series of image transformations to be applied to the conditioning images. It resizes, crops, and converts the images to tensors, ensuring they are in the correct format for the T2I-Adapter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"conditioning_image_transforms = transforms.Compose(\n    [\n        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n        transforms.CenterCrop(args.resolution),\n        transforms.ToTensor(),\n    ]\n)\"\n```\n\n----------------------------------------\n\nTITLE: Installing and Updating Required Libraries for Diffusion Model Acceleration\nDESCRIPTION: Commands for installing the latest versions of diffusers and related libraries, plus the PyTorch nightly build to access the latest optimization features.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U diffusers\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -U transformers accelerate peft\n```\n\nLANGUAGE: bash\nCODE:\n```\npip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121\n```\n\n----------------------------------------\n\nTITLE: Loading a Diffusion Pipeline in Python\nDESCRIPTION: Initializes a DiffusionPipeline by loading the stable-diffusion-v1-5 model. This is the first step in setting up an image generation workflow.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipeline = DiffusionPipeline.from_pretrained(model_id)\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusion3ControlNetPipeline in Python\nDESCRIPTION: This snippet shows how to import the StableDiffusion3ControlNetPipeline class from the Hugging Face Diffusers library. This pipeline allows for using ControlNet with Stable Diffusion 3 for controlled image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet_sd3.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusion3ControlNetPipeline\n```\n\n----------------------------------------\n\nTITLE: Generating Images with 4-bit Quantized Models\nDESCRIPTION: Shows how to use the 4-bit quantized models to generate images using the FluxPipeline. It also demonstrates setting device_map to \"auto\" for automatic memory management.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    transformer=transformer_4bit,\n    text_encoder_2=text_encoder_2_4bit,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\npipe_kwargs = {\n    \"prompt\": \"A cat holding a sign that says hello world\",\n    \"height\": 1024,\n    \"width\": 1024,\n    \"guidance_scale\": 3.5,\n    \"num_inference_steps\": 50,\n    \"max_sequence_length\": 512,\n}\n\nimage = pipe(**pipe_kwargs, generator=torch.manual_seed(0),).images[0]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for SDXL in Python\nDESCRIPTION: Installs the necessary libraries for using Stable Diffusion XL, including diffusers, transformers, accelerate, and invisible-watermark.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate invisible-watermark>=0.2.0\n```\n\n----------------------------------------\n\nTITLE: Loading Flan-T5 for Automatic Prompt Generation\nDESCRIPTION: Initializing the Flan-T5 model and tokenizer to automatically generate source and target text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Training\nDESCRIPTION: Bash commands to configure environment variables for the training script, including the base model checkpoint and dataset identifier.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport DATASET_ID=\"fusing/instructpix2pix-1000-samples\"\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained LoRA Weights\nDESCRIPTION: This Python script shows how to use the StableDiffusionPipeline to perform inference with a model fine-tuned using LoRA weights. The script demonstrates loading fine-tuned weights and generating an image from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/lora/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_path = \"sayakpaul/sd-model-finetuned-lora-t4\"\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\npipe.unet.load_attn_procs(model_path)\npipe.to(\"cuda\")\n\nprompt = \"A naruto with green eyes and red legs.\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating Class Images for DreamBooth Training\nDESCRIPTION: Generate class images for DreamBooth using the PromptDataset class, essential for prior preservation loss. This step preprocesses input data to facilitate model training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_7\n\nLANGUAGE: py\nCODE:\n```\nsample_dataset = PromptDataset(args.class_prompt, num_new_images)\nsample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n\nsample_dataloader = accelerator.prepare(sample_dataloader)\npipeline.to(accelerator.device)\n\nfor example in tqdm(\n    sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n):\n    images = pipeline(example[\"prompt\"]).images\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment for CogVideoX Training\nDESCRIPTION: Commands for configuring the Accelerate environment for distributed training of CogVideoX models with LoRA.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Inference with PyTorch-trained Stable Diffusion Model\nDESCRIPTION: Python code for generating images with a trained Stable Diffusion model using PyTorch. The snippet loads the model with half-precision floating point (float16) for efficient GPU inference and generates an image from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\npipeline = StableDiffusionPipeline.from_pretrained(\"path/to/saved_model\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\nimage = pipeline(prompt=\"yoda\").images[0]\nimage.save(\"yoda-naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Creating Random Number Generator for JAX\nDESCRIPTION: Helper function to create reproducible random number generators for JAX operations\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_key(seed=0):\n    return jax.random.PRNGKey(seed)\n```\n\n----------------------------------------\n\nTITLE: PAG Layer Specification Example in RegEx\nDESCRIPTION: Examples of how to specify layers for PAG using different formats including full identifiers, regular expressions, and lists of identifiers. Shows proper syntax for targeting specific attention layers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pag.md#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndown_blocks.2.attentions.0.transformer_blocks.0.attn1.processor\ndown_blocks.2.(attentions|motion_modules).0.transformer_blocks.0.attn1.processor\ndown_blocks.2\nattn1\n[\"blocks.1\", \"blocks.(14|20)\", r\"down_blocks\\.(2,3)\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing Marigold Depth Estimation Pipeline in Python\nDESCRIPTION: This code snippet shows how to use the Marigold Depth Estimation pipeline for monocular depth estimation. It includes both the original DDIM version and the faster LCM version, demonstrating how to load the model, process an input image, and save the depth map results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\n# Original DDIM version (higher quality)\npipe = DiffusionPipeline.from_pretrained(\n    \"prs-eth/marigold-v1-0\",\n    custom_pipeline=\"marigold_depth_estimation\"\n    # torch_dtype=torch.float16,                # (optional) Run with half-precision (16-bit float).\n    # variant=\"fp16\",                           # (optional) Use with `torch_dtype=torch.float16`, to directly load fp16 checkpoint\n)\n\n# (New) LCM version (faster speed)\npipe = DiffusionPipeline.from_pretrained(\n    \"prs-eth/marigold-depth-lcm-v1-0\",\n    custom_pipeline=\"marigold_depth_estimation\"\n    # torch_dtype=torch.float16,                # (optional) Run with half-precision (16-bit float).\n    # variant=\"fp16\",                           # (optional) Use with `torch_dtype=torch.float16`, to directly load fp16 checkpoint\n)\n\npipe.to(\"cuda\")\n\nimg_path_or_url = \"https://share.phys.ethz.ch/~pf/bingkedata/marigold/pipeline_example.jpg\"\nimage: Image.Image = load_image(img_path_or_url)\n\npipeline_output = pipe(\n    image,                    # Input image.\n    # ----- recommended setting for DDIM version -----\n    # denoising_steps=10,     # (optional) Number of denoising steps of each inference pass. Default: 10.\n    # ensemble_size=10,       # (optional) Number of inference passes in the ensemble. Default: 10.\n    # ------------------------------------------------\n\n    # ----- recommended setting for LCM version ------\n    # denoising_steps=4,\n    # ensemble_size=5,\n    # -------------------------------------------------\n\n    # processing_res=768,     # (optional) Maximum resolution of processing. If set to 0: will not resize at all. Defaults to 768.\n    # match_input_res=True,   # (optional) Resize depth prediction to match input resolution.\n    # batch_size=0,           # (optional) Inference batch size, no bigger than `num_ensemble`. If set to 0, the script will automatically decide the proper batch size. Defaults to 0.\n    # seed=2024,              # (optional) Random seed can be set to ensure additional reproducibility. Default: None (unseeded). Note: forcing --batch_size 1 helps to increase reproducibility. To ensure full reproducibility, deterministic mode needs to be used.\n    # color_map=\"Spectral\",   # (optional) Colormap used to colorize the depth map. Defaults to \"Spectral\". Set to `None` to skip colormap generation.\n    # show_progress_bar=True, # (optional) If true, will show progress bars of the inference progress.\n)\n\ndepth: np.ndarray = pipeline_output.depth_np                    # Predicted depth map\ndepth_colored: Image.Image = pipeline_output.depth_colored      # Colorized prediction\n\n# Save as uint16 PNG\ndepth_uint16 = (depth * 65535.0).astype(np.uint16)\nImage.fromarray(depth_uint16).save(\"./depth_map.png\", mode=\"I;16\")\n\n# Save colorized depth map\ndepth_colored.save(\"./depth_colored.png\")\n```\n\n----------------------------------------\n\nTITLE: Memory Cleanup Functions\nDESCRIPTION: Utility functions to free GPU memory and clear unused components\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gc\n\ndef flush():\n    gc.collect()\n    torch.cuda.empty_cache()\n\ndel text_encoder\ndel pipe\nflush()\n```\n\n----------------------------------------\n\nTITLE: Training DreamBooth with Prior Preservation Loss in Flax\nDESCRIPTION: This bash script demonstrates how to train a Stable Diffusion model with DreamBooth using prior preservation loss in Flax. It requires a pre-trained model, instance images directory, and class images directory. The script handles preservation of class concepts while learning new instances.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\npython train_dreambooth_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-6 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Specifying Hugging Face Hub Dependency Version\nDESCRIPTION: Defines the minimum required version of huggingface-hub package as 0.26.2 or higher for the Diffusers project\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/model_search/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhuggingface-hub>=0.26.2\n```\n\n----------------------------------------\n\nTITLE: Loading Multifolder Safetensors Model\nDESCRIPTION: Demonstrates how to load a diffusion model with safetensors files stored in multiple folders using the DiffusionPipeline.from_pretrained method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    use_safetensors=True\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing MochiTransformer3D Model from Pretrained Weights\nDESCRIPTION: Code snippet demonstrating how to load and initialize a pretrained MochiTransformer3D model from the Genmo Mochi-1 Preview weights. The model is loaded in float16 precision and moved to CUDA device for GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/mochi_transformer3d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import MochiTransformer3DModel\n\ntransformer = MochiTransformer3DModel.from_pretrained(\"genmo/mochi-1-preview\", subfolder=\"transformer\", torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for DreamBooth\nDESCRIPTION: Install required dependencies for training DreamBooth models using PyTorch or Flax. Dependencies are essential for ensuring the training script operates correctly.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/dreambooth\npip install -r requirements.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/dreambooth\npip install -r requirements_flax.txt\n```\n\n----------------------------------------\n\nTITLE: Editing Images with StableDiffusionInstructPix2PixPipeline in Python\nDESCRIPTION: This snippet shows how to use the StableDiffusionInstructPix2PixPipeline to edit images based on given instructions. It processes a dataset of images and captions, applying edits to each image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\nimport numpy as np\n\ninstruct_pix2pix_pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n    \"timbrooks/instruct-pix2pix\", torch_dtype=torch.float16\n).to(\"cuda\")\n\ndef edit_image(input_image, instruction):\n    image = instruct_pix2pix_pipeline(\n        instruction,\n        image=input_image,\n        output_type=\"np\",\n        generator=generator,\n    ).images[0]\n    return image\n\ninput_images = []\noriginal_captions = []\nmodified_captions = []\nedited_images = []\n\nfor idx in range(len(dataset)):\n    input_image = dataset[idx][\"image\"]\n    edit_instruction = dataset[idx][\"edit\"]\n    edited_image = edit_image(input_image, edit_instruction)\n\n    input_images.append(np.array(input_image))\n    original_captions.append(dataset[idx][\"input\"])\n    modified_captions.append(dataset[idx][\"output\"])\n    edited_images.append(edited_image)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Kandinsky 2.1 for Image-to-3D Input\nDESCRIPTION: Code for generating custom images using Kandinsky 2.1 diffusion model. These generated images will be used as input for the image-to-3D generation process with Shap-E.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nprior_pipeline = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\nprompt = \"A cheeseburger, white background\"\n\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, guidance_scale=1.0).to_tuple()\nimage = pipeline(\n    prompt,\n    image_embeds=image_embeds,\n    negative_image_embeds=negative_image_embeds,\n).images[0]\n\nimage.save(\"burger.png\")\n```\n\n----------------------------------------\n\nTITLE: Training Kandinsky Prior Model with Accelerate\nDESCRIPTION: Bash command to launch the training script for the Kandinsky prior model using the Accelerate library. It configures training parameters like batch size, learning rate, and validation prompts for generating Naruto-style images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image_prior.py \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot naruto, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi2-prior-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Loading DreamBooth Checkpoint with Older Accelerate Versions\nDESCRIPTION: Python code snippet for loading a saved DreamBooth checkpoint with older versions of Accelerate (< 0.16.0). This shows how to prepare the UNet and text encoder for inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom diffusers import DiffusionPipeline\n\n# 학습에 사용된 것과 동일한 인수(model, revision)로 파이프라인을 불러옵니다.\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\npipeline = DiffusionPipeline.from_pretrained(model_id)\n\naccelerator = Accelerator()\n\n# 초기 학습에 `--train_text_encoder`가 사용된 경우 text_encoder를 사용합니다.\nunet, text_encoder = accelerator.prepare(pipeline.unet, pipeline.text_encoder)\n```\n\n----------------------------------------\n\nTITLE: Using Textual Inversion with Stable Diffusion XL\nDESCRIPTION: Initializes the SDXL pipeline and loads the dual textual inversion embeddings (one for each text encoder), then generates an image using the concept as a negative prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/textual_inversion_inference.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\npipe.load_textual_inversion(state_dict[\"clip_g\"], token=\"unaestheticXLv31\", text_encoder=pipe.text_encoder_2, tokenizer=pipe.tokenizer_2)\npipe.load_textual_inversion(state_dict[\"clip_l\"], token=\"unaestheticXLv31\", text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)\n\n# the embedding should be used as a negative embedding, so we pass it as a negative prompt\ngenerator = torch.Generator().manual_seed(33)\nimage = pipe(\"a woman standing in front of a mountain\", negative_prompt=\"unaestheticXLv31\", generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Setting Up TPU for JAX\nDESCRIPTION: Configures JAX to use TPU hardware and verifies the available devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax.tools.colab_tpu\n\njax.tools.colab_tpu.setup_tpu()\nimport jax\n\nnum_devices = jax.device_count()\ndevice_type = jax.devices()[0].device_kind\n\nprint(f\"Found {num_devices} JAX devices of type {device_type}.\")\nassert (\n    \"TPU\" in device_type\n), \"Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with Selective Component Reuse\nDESCRIPTION: Create a new pipeline by selectively reusing specific components from another pipeline while excluding others, such as omitting the safety checker and feature extractor.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nstable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(model_id)\nstable_diffusion_img2img = StableDiffusionImg2ImgPipeline(\n    vae=stable_diffusion_txt2img.vae,\n    text_encoder=stable_diffusion_txt2img.text_encoder,\n    tokenizer=stable_diffusion_txt2img.tokenizer,\n    unet=stable_diffusion_txt2img.unet,\n    scheduler=stable_diffusion_txt2img.scheduler,\n    safety_checker=None,\n    feature_extractor=None,\n    requires_safety_checker=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing EulerAncestralDiscreteSchedulerOutput in Python\nDESCRIPTION: This code snippet shows how to import the EulerAncestralDiscreteSchedulerOutput class from the HuggingFace Diffusers library. This class represents the output of the EulerAncestralDiscreteScheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers.scheduling_euler_ancestral_discrete import EulerAncestralDiscreteSchedulerOutput\n```\n\n----------------------------------------\n\nTITLE: Authenticating with HuggingFace Hub\nDESCRIPTION: Logs into HuggingFace Hub using notebook authentication to enable pushing content.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/push_to_hub.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Verifying Model Input and Output Shapes\nDESCRIPTION: Performs a quick check to ensure that the model can process the sample images correctly by verifying that the input and output shapes match as expected.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> sample_image = dataset[0][\"images\"].unsqueeze(0)\n>>> print(\"Input shape:\", sample_image.shape)\nInput shape: torch.Size([1, 3, 128, 128])\n\n>>> print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)\nOutput shape: torch.Size([1, 3, 128, 128])\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers with PyTorch using pip\nDESCRIPTION: Installs Diffusers with PyTorch support using pip package manager\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install diffusers[\"torch\"] transformers\n```\n\n----------------------------------------\n\nTITLE: Initializing 🤗 Accelerate Environment\nDESCRIPTION: This command sets up an 🤗 Accelerate environment which simplifies the process of configuring training across various hardware setups. It enhances the training experience by automatically adapting to the available resources.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Using Generator for Reproducible Image Generation\nDESCRIPTION: Demonstrates how to use torch.Generator to create reproducible image generations with a fixed seed. This enables deterministic outputs for iterative improvements or consistent batch generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16\n).to(\"cuda\")\ngenerator = torch.Generator(device=\"cuda\").manual_seed(30)\nimage = pipeline(\n\t\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n\tgenerator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepFloyd IF Text-to-Image Pipeline with Upscaling in Python\nDESCRIPTION: A complete implementation of the DeepFloyd IF text-to-image generation pipeline with three stages: initial image generation, refinement, and upscaling. The code demonstrates loading three different models, optimizing them with torch.channels_last memory format, conditionally applying torch.compile for acceleration, and generating images through the complete pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nrun_compile = True  # Set True / False\n\npipe_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-M-v1.0\", variant=\"fp16\", text_encoder=None, torch_dtype=torch.float16, use_safetensors=True)\npipe_1.to(\"cuda\")\npipe_2 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-II-M-v1.0\", variant=\"fp16\", text_encoder=None, torch_dtype=torch.float16, use_safetensors=True)\npipe_2.to(\"cuda\")\npipe_3 = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", torch_dtype=torch.float16, use_safetensors=True)\npipe_3.to(\"cuda\")\n\n\npipe_1.unet.to(memory_format=torch.channels_last)\npipe_2.unet.to(memory_format=torch.channels_last)\npipe_3.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    pipe_1.unet = torch.compile(pipe_1.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe_2.unet = torch.compile(pipe_2.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe_3.unet = torch.compile(pipe_3.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"the blue hulk\"\n\nprompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)\nneg_prompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)\n\nfor _ in range(3):\n    image_1 = pipe_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, output_type=\"pt\").images\n    image_2 = pipe_2(image=image_1, prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, output_type=\"pt\").images\n    image_3 = pipe_3(prompt=prompt, image=image_1, noise_level=100).images\n```\n\n----------------------------------------\n\nTITLE: Loading a GitHub Community Pipeline with Custom Components\nDESCRIPTION: This code shows how to load a CLIP Guided Stable Diffusion pipeline from GitHub with custom CLIP model components required for its functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nfrom transformers import CLIPImageProcessor, CLIPModel\n\nclip_model_id = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n\nfeature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)\nclip_model = CLIPModel.from_pretrained(clip_model_id)\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    custom_pipeline=\"clip_guided_stable_diffusion\",\n    clip_model=clip_model,\n    feature_extractor=feature_extractor,\n    use_safetensors=True,\n)\n```\n\n----------------------------------------\n\nTITLE: MarigoldDepthOutput Data Structure\nDESCRIPTION: Data structure defining the output format for depth predictions from the Marigold pipeline\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMarigoldDepthOutput(\n    depth,\n    depth_colored,\n    disparity,\n    disparity_colored\n)\n```\n\n----------------------------------------\n\nTITLE: Importing KDPM2AncestralDiscreteScheduler in Python\nDESCRIPTION: This code snippet demonstrates how to import the KDPM2AncestralDiscreteScheduler class from the HuggingFace Diffusers library. It's inferred from the documentation structure.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/dpm_discrete_ancestral.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KDPM2AncestralDiscreteScheduler\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Text Encoder with UNet in Flax for DreamBooth\nDESCRIPTION: This script shows how to fine-tune both the text encoder and UNet components for DreamBooth in Flax. The approach includes training the text encoder alongside the UNet which can improve the model's ability to understand custom concepts. It uses a lower learning rate compared to UNet-only training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\npython train_dreambooth_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --train_text_encoder \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=2e-6 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Displaying 3D Molecule Visualization\nDESCRIPTION: Renders the selected generated molecule in 3D using nglview, allowing for interactive examination of the molecular structure.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# new molecule\nshow(mols_gen[idx])\n```\n\n----------------------------------------\n\nTITLE: Setting Up the Example Environment\nDESCRIPTION: Commands to navigate to the Würstchen example directory and install the required dependencies for the text-to-image fine-tuning process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/wuerstchen/text_to_image/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/wuerstchen/text_to_image\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Preparing Source and Corrupted Images for Diffusion Posterior Sampling in Python\nDESCRIPTION: Shows how to load a source image, apply the Gaussian blur operator to create a corrupted image, and save both images for use in the Diffusion Posterior Sampling Pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_84\n\nLANGUAGE: python\nCODE:\n```\n# set up source image\nsrc = Image.open('sample.png')\n# read image into [1,3,H,W]\nsrc = torch.from_numpy(np.array(src, dtype=np.float32)).permute(2,0,1)[None]\n# normalize image to [-1,1]\nsrc = (src / 127.5) - 1.0\nsrc = src.to(\"cuda\")\n\n# set up operator and measurement\noperator = GaussialBlurOperator(kernel_size=61, intensity=3.0).to(\"cuda\")\nmeasurement = operator(src)\n\n# save the source and corrupted images\nsave_image((src+1.0)/2.0, \"dps_src.png\")\nsave_image((measurement+1.0)/2.0, \"dps_mea.png\")\n```\n\n----------------------------------------\n\nTITLE: Installing Custom Diffusion Dependencies\nDESCRIPTION: Navigate to the custom diffusion example folder and install required dependencies for training, including additional tools like clip-retrieval\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/custom_diffusion\npip install -r requirements.txt\npip install clip-retrieval\n```\n\n----------------------------------------\n\nTITLE: Specifying Required Dependencies for Hugging Face Diffusers\nDESCRIPTION: This requirements file lists all the essential packages needed to run the Diffusers library. It includes accelerate for optimization, torchvision for image processing, transformers for model handling, ftfy for text cleaning, tensorboard for visualization, Jinja2 for templating, and webdataset for efficient dataset handling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\nwebdataset\n```\n\n----------------------------------------\n\nTITLE: Fine-Grained Control of Multiple LoRA Adapters\nDESCRIPTION: Demonstrates advanced control of adapter strength at block and transformer level, applying different scales to specific parts of the model architecture for both toy-face and pixel-art adapters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nadapter_weight_scales_toy = 0.5\nadapter_weight_scales_pixel = {\n    \"unet\": {\n        \"down\": 0.9,  # all transformers in the down-part will use scale 0.9\n        # \"mid\"  # because, in this example, \"mid\" is not given, all transformers in the mid part will use the default scale 1.0\n        \"up\": {\n            \"block_0\": 0.6,  # all 3 transformers in the 0th block in the up-part will use scale 0.6\n            \"block_1\": [0.4, 0.8, 1.0],  # the 3 transformers in the 1st block in the up-part will use scales 0.4, 0.8 and 1.0 respectively\n        }\n    }\n}\npipe.set_adapters([\"toy\", \"pixel\"], [adapter_weight_scales_toy, adapter_weight_scales_pixel])\nimage = pipe(prompt, num_inference_steps=30, generator=torch.manual_seed(0)).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Inference with Fine-tuned Stable Diffusion Model using Flax\nDESCRIPTION: Python code to load a fine-tuned Stable Diffusion model and generate an image using Flax.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text2image.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline\n\nmodel_path = \"path_to_saved_model\"\npipe, params = FlaxStableDiffusionPipeline.from_pretrained(model_path, dtype=jax.numpy.bfloat16)\n\nprompt = \"yoda naruto\"\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\nimage.save(\"yoda-naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Core ML Model Checkpoints from Hugging Face Hub\nDESCRIPTION: Python script to download a Core ML model variant from the Hugging Face Hub for inference. This example downloads the 'original' attention variant, which is optimized for CPU and GPU performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/coreml.md#2025-04-11_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/packages\"\n\nmodel_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\nsnapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\nprint(f\"Model downloaded at {model_path}\")\n```\n\n----------------------------------------\n\nTITLE: Loading AuraFlowPipeline with GGUF Quantization in Python\nDESCRIPTION: This snippet shows how to load an AuraFlowPipeline using a GGUF (GGML Universal Format) checkpoint for quantization. It uses a pre-quantized model file and sets up the pipeline with bfloat16 precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/aura_flow.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import (\n    AuraFlowPipeline,\n    GGUFQuantizationConfig,\n    AuraFlowTransformer2DModel,\n)\n\ntransformer = AuraFlowTransformer2DModel.from_single_file(\n    \"https://huggingface.co/city96/AuraFlow-v0.3-gguf/blob/main/aura_flow_0.3-Q2_K.gguf\",\n    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n    torch_dtype=torch.bfloat16,\n)\n\npipeline = AuraFlowPipeline.from_pretrained(\n    \"fal/AuraFlow-v0.3\",\n    transformer=transformer,\n    torch_dtype=torch.bfloat16,\n)\n\nprompt = \"a cute pony in a field of flowers\"\nimage = pipeline(prompt).images[0]\nimage.save(\"auraflow.png\")\n```\n\n----------------------------------------\n\nTITLE: Launching Unconditional Training\nDESCRIPTION: This bash command shows how to launch a training script for unconditional image generation using the `accelerate` launcher.  It uses the `--train_data_dir` argument to specify the path to the directory containing the image dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/create_dataset.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate launch train_unconditional.py \\\n    --train_data_dir <path-to-train-directory> \\\n    <other-arguments>\"\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Pure Textual Inversion FLUX.1 Model\nDESCRIPTION: Generates images using the FLUX.1 model with pure textual inversion embeddings. This demonstrates how to apply the learned style to new concepts using only textual inversion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ninstance_token = \"<s0><s1>\"\nprompt = f\"a {instance_token} icon of an orange llama eating ramen, in the style of {instance_token}\"\n\nimage = pipe(prompt=prompt, num_inference_steps=25, cross_attention_kwargs={\"scale\": 1.0}).images[0]\nimage.save(\"llama.png\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Super-Resolution with Stable Diffusion X4 Upscaler\nDESCRIPTION: This code demonstrates how to upscale a low-resolution image using Stable Diffusion's X4 upscaler model. It loads a low-resolution image, initializes the upscaler pipeline, and generates a higher resolution version guided by a prompt description of the image content.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_2.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionUpscalePipeline\nfrom diffusers.utils import load_image, make_image_grid\nimport torch\n\n# load model and scheduler\nmodel_id = \"stabilityai/stable-diffusion-x4-upscaler\"\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\n\n# let's download an  image\nurl = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\"\nlow_res_img = load_image(url)\nlow_res_img = low_res_img.resize((128, 128))\nprompt = \"a white cat\"\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\nmake_image_grid([low_res_img.resize((512, 512)), upscaled_image.resize((512, 512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Loading IP-Adapter Image Embeddings for Generation\nDESCRIPTION: Shows how to load saved image embeddings and use them in the generation pipeline by passing them to the ip_adapter_image_embeds parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimage_embeds = torch.load(\"image_embeds.ipadpt\")\nimages = pipeline(\n    prompt=\"a polar bear sitting in a chair drinking a milkshake\",\n    ip_adapter_image_embeds=image_embeds,\n    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n    num_inference_steps=100,\n    generator=generator,\n).images\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Images and Text for Prior Model Training\nDESCRIPTION: Defines a preprocessing function that converts images to RGB, processes them with the CLIP image processor, and tokenizes the caption text. This prepares the data for training the prior model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_train(examples):\n    images = [image.convert(\"RGB\") for image in examples[image_column]]\n    examples[\"clip_pixel_values\"] = image_processor(images, return_tensors=\"pt\").pixel_values\n    examples[\"text_input_ids\"], examples[\"text_mask\"] = tokenize_captions(examples)\n    return examples\n```\n\n----------------------------------------\n\nTITLE: Replacing VAE in Diffusion Pipeline\nDESCRIPTION: Demonstrates how to replace a specific component (VAE) in a diffusion pipeline with a different version, showcasing the flexibility of the multi-folder layout.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    vae=vae,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Combining Prompt with Style Template in Python\nDESCRIPTION: A simple example of formatting a base prompt with a predefined style template. This demonstrates how to convert a basic prompt into a styled prompt for better image generation results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a cat basking in the sun on a roof in Turkey\"\nstyle = \"cinematic\"\n\nprompt = styles[style].format(prompt=prompt)\nprompt\n\"cinematic film still of a cat basking in the sun on a roof in Turkey, highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain\"\n```\n\n----------------------------------------\n\nTITLE: Loading and Using MusicLDM Pipeline in Markdown\nDESCRIPTION: Documentation block showing how to initialize and use the MusicLDM pipeline for text-to-music generation. Includes guidance on prompt construction, inference parameters, and best practices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/musicldm.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# MusicLDM\n\nMusicLDM was proposed in [MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies](https://huggingface.co/papers/2308.01546) by Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov.\nMusicLDM takes a text prompt as input and predicts the corresponding music sample.\n\nInspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview) and [AudioLDM](https://huggingface.co/docs/diffusers/api/pipelines/audioldm),\nMusicLDM is a text-to-music _latent diffusion model (LDM)_ that learns continuous audio representations from [CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)\nlatents.\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple IP-Adapter Images with Masks\nDESCRIPTION: Shows how to load and configure multiple IP-Adapter images with corresponding masks and scales for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=[\"ip-adapter-plus-face_sdxl_vit-h.safetensors\"])\npipeline.set_ip_adapter_scale([[0.7, 0.7]])  # one scale for each image-mask pair\n\nface_image1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl1.png\")\nface_image2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl2.png\")\n\nip_images = [[face_image1, face_image2]]\n\nmasks = [masks.reshape(1, masks.shape[0], masks.shape[2], masks.shape[3])]\n```\n\n----------------------------------------\n\nTITLE: Generating Videos with Mochi-1 Model\nDESCRIPTION: This code demonstrates how to use the Mochi-1 model to generate videos with high-quality motion dynamics. The implementation includes memory optimization techniques like model CPU offloading and VAE tiling to handle the 10B parameter model efficiently.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MochiPipeline\nfrom diffusers.utils import export_to_video\n\npipe = MochiPipeline.from_pretrained(\"genmo/mochi-1-preview\", variant=\"bf16\", torch_dtype=torch.bfloat16)\n\n# reduce memory requirements\npipe.enable_model_cpu_offload()\npipe.enable_vae_tiling()\n\nprompt = \"Close-up of a chameleon's eye, with its scaly skin changing color. Ultra high resolution 4k.\"\nvideo = pipe(prompt, num_frames=84).frames[0]\nexport_to_video(video, \"output.mp4\", fps=30)\n```\n\n----------------------------------------\n\nTITLE: Marginal Generation with UniDiffuser (Image-only and Text-only)\nDESCRIPTION: Demonstrates marginal generation using UniDiffuser to produce either images or text alone. This approach samples from the marginal distribution of images or text without classifier-free guidance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unidiffuser.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Unlike other generation tasks, image-only and text-only generation don't use classifier-free guidance\n# Image-only generation\npipe.set_image_mode()\nsample_image = pipe(num_inference_steps=20).images[0]\n# Text-only generation\npipe.set_text_mode()\nsample_text = pipe(num_inference_steps=20).text[0]\n```\n\n----------------------------------------\n\nTITLE: Defining SDXL Badge Layout\nDESCRIPTION: HTML markup for displaying LoRA and MPS compatibility badges in the documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"flex flex-wrap space-x-1\">\n  <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n  <img alt=\"MPS\" src=\"https://img.shields.io/badge/MPS-000000?style=flat&logo=apple&logoColor=white%22\">\n</div>\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure for Attention Processors\nDESCRIPTION: Structured documentation layout describing different attention processor implementations including basic processors, specialized variants for different models, and various optimization approaches.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/attnprocessor.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Attention Processor\n\nAn attention processor is a class for applying different types of attention mechanisms.\n\n## AttnProcessor\n\n[[autodoc]] models.attention_processor.AttnProcessor\n\n[[autodoc]] models.attention_processor.AttnProcessor2_0\n\n[[autodoc]] models.attention_processor.AttnAddedKVProcessor\n\n[[autodoc]] models.attention_processor.AttnAddedKVProcessor2_0\n\n[[autodoc]] models.attention_processor.AttnProcessorNPU\n\n[[autodoc]] models.attention_processor.FusedAttnProcessor2_0\n```\n\n----------------------------------------\n\nTITLE: Processing Generated Prompt in Python\nDESCRIPTION: Processes the output from the GPT2 model by separating the input prompt from the generated content, identifying word pairs, and formatting the enhanced prompt for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noutput_tokens = [tokenizer.decode(generated_id, skip_special_tokens=True) for generated_id in generated_ids]\ninput_part, generated_part = output_tokens[0][: len(prompt)], output_tokens[0][len(prompt) :]\npairs, words = find_and_order_pairs(generated_part, word_pairs)\nformatted_generated_part = pairs + \", \" + words\nenhanced_prompt = input_part + \", \" + formatted_generated_part\nenhanced_prompt\n[\"cinematic film still of a cat basking in the sun on a roof in Turkey, highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain quality sharp focus beautiful detailed intricate stunning amazing epic\"]\n```\n\n----------------------------------------\n\nTITLE: Optimizing Video Generation Pipeline\nDESCRIPTION: This diff snippet shows optimizations for reducing memory usage in video generation, including CPU offloading, forward chunking, and chunked decoding.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_16\n\nLANGUAGE: diff\nCODE:\n```\n- pipeline.enable_model_cpu_offload()\n- frames = pipeline(image, decode_chunk_size=8, generator=generator).frames[0]\n+ pipeline.enable_model_cpu_offload()\n+ pipeline.unet.enable_forward_chunking()\n+ frames = pipeline(image, decode_chunk_size=2, generator=generator, num_frames=25).frames[0]\n```\n\n----------------------------------------\n\nTITLE: Denoising a Sample with DDPM Scheduler Step\nDESCRIPTION: Uses the scheduler's step method to compute a less noisy sample from the model's prediction, current timestep, and input sample. This is a single step in the progressive denoising process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample).prev_sample\n>>> less_noisy_sample.shape\n```\n\n----------------------------------------\n\nTITLE: Converting to PIL Image\nDESCRIPTION: Processes the decoded tensor and converts it to a PIL Image for visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimage = (image / 2 + 0.5).clamp(0, 1).squeeze()\nimage = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\nimage = Image.fromarray(image)\nimage\n```\n\n----------------------------------------\n\nTITLE: Installing T2I-Adapter Requirements\nDESCRIPTION: Command to install the specific requirements for T2I-Adapter training from the requirements.txt file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Diffusion Posterior Sampling Pipeline in Python\nDESCRIPTION: Shows how to initialize and run the Diffusion Posterior Sampling Pipeline using the prepared components, including the measurement, operator, loss function, and hyperparameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_87\n\nLANGUAGE: python\nCODE:\n```\n# finally, the pipeline\ndpspipe = DPSPipeline(model, scheduler)\nimage = dpspipe(\n    measurement=measurement,\n    operator=operator,\n    loss_fn=RMSELoss,\n    zeta=1.0,\n).images[0]\nimage.save(\"dps_generated_image.png\")\n```\n\n----------------------------------------\n\nTITLE: Using Trained Model for Inference\nDESCRIPTION: Python code demonstrating how to load a trained unconditional diffusion model and generate images using the DiffusionPipeline class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"anton-l/ddpm-butterflies-128\").to(\"cuda\")\nimage = pipeline().images[0]\n```\n\n----------------------------------------\n\nTITLE: Installing Training Dependencies for Diffusers\nDESCRIPTION: Commands to install the Diffusers library and its training dependencies, and initialize the Accelerate environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text2image.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/diffusers.git\npip install -U -r requirements.txt\nacccelerate config\n```\n\n----------------------------------------\n\nTITLE: Prior Model Prediction Step\nDESCRIPTION: Performs the forward pass in the prior model during training. The model takes noisy latents, timesteps, embeddings, and text encoder outputs to predict image embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel_pred = prior(\n    noisy_latents,\n    timestep=timesteps,\n    proj_embedding=prompt_embeds,\n    encoder_hidden_states=text_encoder_hidden_states,\n    attention_mask=text_mask,\n).predicted_image_embedding\n```\n\n----------------------------------------\n\nTITLE: Saving Sharded SDXL UNet Checkpoint\nDESCRIPTION: Demonstrates how to load and save a sharded version of the SDXL UNet model with a maximum shard size of 5GB.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/inference_with_big_models.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"unet\"\n)\nunet.save_pretrained(\"sdxl-unet-sharded\", max_shard_size=\"5GB\")\n```\n\n----------------------------------------\n\nTITLE: Launching ControlNet SDXL Training (Bash)\nDESCRIPTION: Command to start the ControlNet SDXL training process using Accelerate, specifying various parameters such as model paths, dataset, learning rate, and validation settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_controlnet_sdxl.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --mixed_precision=\"fp16\" \\\n --resolution=1024 \\\n --learning_rate=1e-5 \\\n --max_train_steps=15000 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --validation_steps=100 \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --report_to=\"wandb\" \\\n --seed=42 \\\n --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment for Distributed Training\nDESCRIPTION: Command to initialize and configure the Accelerate environment for distributed training across multiple GPUs or devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Full model LCM distillation training command\nDESCRIPTION: Complete accelerate launch command for training a full Latent Consistency Model by distilling stable-diffusion-v1.5 using the CC12M dataset, with parameters for mixed precision, resolution, learning rate, and various optimization settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path/to/saved/model\"\n\naccelerate launch train_lcm_distill_sd_wds.py \\\n    --pretrained_teacher_model=$MODEL_NAME \\\n    --output_dir=$OUTPUT_DIR \\\n    --mixed_precision=fp16 \\\n    --resolution=512 \\\n    --learning_rate=1e-6 --loss_type=\"huber\" --ema_decay=0.95 --adam_weight_decay=0.0 \\\n    --max_train_steps=1000 \\\n    --max_train_samples=4000000 \\\n    --dataloader_num_workers=8 \\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\n    --validation_steps=200 \\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\n    --train_batch_size=12 \\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n    --gradient_accumulation_steps=1 \\\n    --use_8bit_adam \\\n    --resume_from_checkpoint=latest \\\n    --report_to=wandb \\\n    --seed=453645634 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Using default Accelerate configuration\nDESCRIPTION: Command to set up Accelerate with default configuration options without interactive prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Running Inference with UNet Model for Image Generation\nDESCRIPTION: This snippet demonstrates how to pass a noisy image sample and timestep to the UNet model to get a prediction. The model predicts the residual noise at the specified timestep in the diffusion process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> with torch.no_grad():\n...     noisy_residual = model(sample=noisy_sample, timestep=2).sample\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Parameters Table in Markdown\nDESCRIPTION: A markdown table showing the parameter counts for two aMUSEd models: amused-256 and amused-512.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/amused.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Model | Params |\n|-------|--------|\n| [amused-256](https://huggingface.co/amused/amused-256) | 603M |\n| [amused-512](https://huggingface.co/amused/amused-512) | 608M |\n```\n\n----------------------------------------\n\nTITLE: Launching Training on Single GPU\nDESCRIPTION: Command to launch the unconditional image generation training script on a single GPU using Accelerate, with options to specify dataset, output directory, and mixed precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_unconditional.py \\\n  --dataset_name=\"huggan/flowers-102-categories\" \\\n  --output_dir=\"ddpm-ema-flowers-64\" \\\n  --mixed_precision=\"fp16\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for Training\nDESCRIPTION: Commands to clone the diffusers repository and install it in development mode to ensure compatibility with the latest example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Using torch.compile with Quanto in Python\nDESCRIPTION: Explains the use of torch.compile to optimize quantized models with prepared configurations, specifically quantization to int8. It requires optimum-quanto and PyTorch, compiling and caching the model functions to improve performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/quanto.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, QuantoConfig\n\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\nquantization_config = QuantoConfig(weights_dtype=\"int8\")\ntransformer = FluxTransformer2DModel.from_pretrained(\n    model_id,\n    subfolder=\"transformer\",\n    quantization_config=quantization_config,\n    torch_dtype=torch.bfloat16,\n)\ntransformer = torch.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n\npipe = FluxPipeline.from_pretrained(\n    model_id, transformer=transformer, torch_dtype=torch_dtype\n)\npipe.to(\"cuda\")\nimages = pipe(\"A cat holding a sign that says hello\").images[0]\nimages.save(\"flux-quanto-compile.png\")\n```\n\n----------------------------------------\n\nTITLE: Using torch.compile with Diffusers\nDESCRIPTION: Python code showing how to use torch.compile to further optimize the UNet in Diffusers pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\nimages = pipe(prompt, num_inference_steps=steps, num_images_per_prompt=batch_size).images\n```\n\n----------------------------------------\n\nTITLE: Loading and Initializing UNet Models for LCM Distillation\nDESCRIPTION: Code that loads the teacher UNet model from a pretrained checkpoint and initializes a student UNet model by copying the teacher's weights. The student UNet will be updated during training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nteacher_unet = UNet2DConditionModel.from_pretrained(\n    args.pretrained_teacher_model, subfolder=\"unet\", revision=args.teacher_revision\n)\n\nunet = UNet2DConditionModel(**teacher_unet.config)\nunet.load_state_dict(teacher_unet.state_dict(), strict=False)\nunet.train()\n```\n\n----------------------------------------\n\nTITLE: Customizing Pipeline with DDIM Scheduler\nDESCRIPTION: Example of customizing a StableDiffusionXL pipeline by using a different scheduler component.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline, DDIMScheduler\n\nckpt_path = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0_0.9vae.safetensors\"\nscheduler = DDIMScheduler()\npipeline = StableDiffusionXLPipeline.from_single_file(ckpt_path, scheduler=scheduler)\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Generation with ControlNet and Kandinsky 2.2\nDESCRIPTION: Sets up the Kandinsky 2.2 prior and ControlNet image-to-image pipelines, generates image embeddings from a prompt and initial image, and creates a new image using the ControlNet pipeline with a depth map hint.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\n\nprior_pipeline = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\npipeline = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n).to(\"cuda\")\n\nprompt = \"A robot, 4k photo\"\nnegative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(43)\n\nimg_emb = prior_pipeline(prompt=prompt, image=img, strength=0.85, generator=generator)\nnegative_emb = prior_pipeline(prompt=negative_prior_prompt, image=img, strength=1, generator=generator)\n\nimage = pipeline(image=img, strength=0.5, image_embeds=img_emb.image_embeds, negative_image_embeds=negative_emb.image_embeds, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]\nmake_image_grid([img.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Adapter at Model Level in Python\nDESCRIPTION: Demonstrates loading a LoRA adapter directly at the model level using PeftAdapterMixin.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.unet.load_lora_adapter(\"jbilcke-hf/sdxl-cinematic-1\", weight_name=\"pytorch_lora_weights.safetensors\", prefix=\"unet\")\n\n# use cnmt in the prompt to trigger the LoRA\nprompt = \"A cute cnmt eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\nimage = pipeline(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Modifying UNet Parameters for InstructPix2Pix\nDESCRIPTION: This Python code snippet modifies the number of input channels in the UNet model's first convolutional layer to adapt to InstructPix2Pix's satellite image processing needs, using PyTorch functions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nin_channels = 8\nout_channels = unet.conv_in.out_channels\nunet.register_to_config(in_channels=in_channels)\n\nwith torch.no_grad():\n    new_conv_in = nn.Conv2d(\n        in_channels, out_channels, unet.conv_in.kernel_size, unet.conv_in.stride, unet.conv_in.padding\n    )\n    new_conv_in.weight.zero_()\n    new_conv_in.weight[:, :4, :, :].copy_(unet.conv_in.weight)\n    unet.conv_in = new_conv_in\n```\n\n----------------------------------------\n\nTITLE: GeoDiff Project Header Documentation\nDESCRIPTION: Markdown documentation header describing the experimental nature of the GeoDiff notebook and providing attribution to the maintainer natolambert.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/README.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# GeoDiff\n\n> [!TIP]\n> This notebook is not actively maintained by the Diffusers team. For any questions or comments, please contact [natolambert](https://twitter.com/natolambert).\n\nThis is an experimental research notebook demonstrating how to generate stable 3D structures of molecules with [GeoDiff](https://github.com/MinkaiXu/GeoDiff) and Diffusers.\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using FluxControlInpaintPipeline in Python\nDESCRIPTION: Example demonstrating how to set up and use the FluxControlInpaintPipeline for image inpainting. Shows pipeline initialization, model loading with memory optimizations, mask creation, and inference with depth control. Includes options for handling GPU constraints through model quantization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/control_flux_inpaint.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxControlInpaintPipeline\nfrom diffusers.models.transformers import FluxTransformer2DModel\nfrom transformers import T5EncoderModel\nfrom diffusers.utils import load_image, make_image_grid\nfrom image_gen_aux import DepthPreprocessor # https://github.com/huggingface/image_gen_aux\nfrom PIL import Image\nimport numpy as np\n\npipe = FluxControlInpaintPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-Depth-dev\",\n    torch_dtype=torch.bfloat16,\n)\n# use following lines if you have GPU constraints\n# ---------------------------------------------------------------\ntransformer = FluxTransformer2DModel.from_pretrained(\n    \"sayakpaul/FLUX.1-Depth-dev-nf4\", subfolder=\"transformer\", torch_dtype=torch.bfloat16\n)\ntext_encoder_2 = T5EncoderModel.from_pretrained(\n    \"sayakpaul/FLUX.1-Depth-dev-nf4\", subfolder=\"text_encoder_2\", torch_dtype=torch.bfloat16\n)\npipe.transformer = transformer\npipe.text_encoder_2 = text_encoder_2\npipe.enable_model_cpu_offload()\n# ---------------------------------------------------------------\npipe.to(\"cuda\")\n\nprompt = \"a blue robot singing opera with human-like expressions\"\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\n\nhead_mask = np.zeros_like(image)\nhead_mask[65:580,300:642] = 255\nmask_image = Image.fromarray(head_mask)\n\nprocessor = DepthPreprocessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\ncontrol_image = processor(image)[0].convert(\"RGB\")\n\noutput = pipe(\n    prompt=prompt,\n    image=image,\n    control_image=control_image,\n    mask_image=mask_image,\n    num_inference_steps=30,\n    strength=0.9,\n    guidance_scale=10.0,\n    generator=torch.Generator().manual_seed(42),\n).images[0]\nmake_image_grid([image, control_image, mask_image, output.resize(image.size)], rows=1, cols=4).save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Optimizer for ControlNet Training\nDESCRIPTION: This python code sets up the optimizer for training the ControlNet model using the specified learning rate and other Adam optimizer parameters.  It focuses the optimization process specifically on the ControlNet's parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"params_to_optimize = controlnet.parameters()\noptimizer = optimizer_class(\n    params_to_optimize,\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\"\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training on 16GB GPU\nDESCRIPTION: Command for training on a 16GB GPU using gradient checkpointing and 8-bit optimizer from bitsandbytes for memory efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=2 --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Subsequent Inference with AOT Compiled Pipeline\nDESCRIPTION: Shows subsequent image generation which should have faster inference time due to pre-compilation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\nprompt = \"photo of a rhino dressed suit and tie sitting at a table in a bar with a bar stools, award winning photography, Elke vogelsang\"\nneg_prompt = \"cartoon, illustration, animation. face. male, female\"\nimages = generate(prompt, neg_prompt)\nprint(f\"Inference in {time.time() - start}\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Setting Up GPT2 for Prompt Enhancement in Python\nDESCRIPTION: Configures a pre-trained GPT2 model specifically trained on Stable Diffusion prompts for generating enhanced text. Sets up generation configuration and tokenizer for prompt expansion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = GPT2Tokenizer.from_pretrained(\"Gustavosta/MagicPrompt-Stable-Diffusion\")\nmodel = GPT2LMHeadModel.from_pretrained(\"Gustavosta/MagicPrompt-Stable-Diffusion\", torch_dtype=torch.float16).to(\n    \"cuda\"\n)\nmodel.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\ntoken_count = inputs[\"input_ids\"].shape[1]\nmax_new_tokens = 50 - token_count\n\ngeneration_config = GenerationConfig(\n    penalty_alpha=0.7,\n    top_k=50,\n    eos_token_id=model.config.eos_token_id,\n    pad_token_id=model.config.eos_token_id,\n    pad_token=model.config.pad_token_id,\n    do_sample=True,\n)\n\nwith torch.no_grad():\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_new_tokens,\n        generation_config=generation_config,\n        logits_processor=proccesor_list,\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderKLWan Model in Python\nDESCRIPTION: Code snippet that demonstrates how to load the pretrained AutoencoderKLWan model from the Wan-AI/Wan2.1-T2V-1.3B-Diffusers repository. It specifies the subfolder 'vae' and sets the torch data type to float32.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoder_kl_wan.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKLWan\n\nvae = AutoencoderKLWan.from_pretrained(\"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", subfolder=\"vae\", torch_dtype=torch.float32)\n```\n\n----------------------------------------\n\nTITLE: Merging LoRAs with set_adapters\nDESCRIPTION: Demonstrates merging LoRAs using set_adapters method with weighted scaling and generating an image with the merged model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline.set_adapters([\"ikea\", \"feng\"], adapter_weights=[0.7, 0.8])\n\ngenerator = torch.manual_seed(0)\nprompt = \"A bowl of ramen shaped like a cute kawaii bear, by Feng Zikai\"\nimage = pipeline(prompt, generator=generator, cross_attention_kwargs={\"scale\": 1.0}).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Python Core ML Inference Command\nDESCRIPTION: Shell command to run Stable Diffusion inference using Python with Core ML models, specifying compute units and model parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/coreml.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython -m python_coreml_stable_diffusion.pipeline --prompt \"a photo of an astronaut riding a horse on mars\" -i ./models/coreml-stable-diffusion-v1-4_original_packages/original/packages -o </path/to/output/image> --compute-unit CPU_AND_GPU --seed 93\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Textual Inversion on SDXL using Shell\nDESCRIPTION: This shell script sets environment variables and executes a script to fine-tune a model for textual inversion using SDXL. It specifies training parameters such as model path, training data directory, learnable property, tokenization settings, training steps, and output directory. The model's configuration allows for mixed precision to optimize performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/README_sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport DATA_DIR=\"./cat\"\n\naccelerate launch textual_inversion_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" \\\n  --initializer_token=\"toy\" \\\n  --mixed_precision=\"bf16\" \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=500 \\\n  --learning_rate=5.0e-04 \\\n  --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --save_as_full_pipeline \\\n  --output_dir=\"./textual_inversion_cat_sdxl\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for CogVideoX LoRA Training\nDESCRIPTION: Bash commands for setting up the development environment by installing necessary packages for LoRA training with CogVideoX models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Optimizing Memory Layout\nDESCRIPTION: Shows how to optimize the memory layout of transformer and VAE components using channels_last format.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/lumina.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline.transformer.to(memory_format=torch.channels_last)\npipeline.vae.to(memory_format=torch.channels_last)\n```\n\n----------------------------------------\n\nTITLE: Loading Models from Civitai\nDESCRIPTION: Python code demonstrating how to load Text-to-Image, Image-to-Image, and Inpainting models from Civitai using the EasyPipeline classes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/model_search/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pipeline_easy import (\n    EasyPipelineForText2Image,\n    EasyPipelineForImage2Image,\n    EasyPipelineForInpainting,\n)\n\n# Text-to-Image\npipeline = EasyPipelineForText2Image.from_civitai(\n    \"search_word\",\n    base_model=\"SD 1.5\",\n).to(\"cuda\")\n\n\n# Image-to-Image\npipeline = EasyPipelineForImage2Image.from_civitai(\n    \"search_word\",\n    base_model=\"SD 1.5\",\n).to(\"cuda\")\n\n\n# Inpainting\npipeline = EasyPipelineForInpainting.from_civitai(\n    \"search_word\",\n    base_model=\"SD 1.5\",\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Initialize and configure the Hugging Face Accelerate environment for distributed training\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lora.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: HTML Documentation Layout for Navigation Cards\nDESCRIPTION: HTML structure implementing a responsive grid layout of four navigation cards that direct users to different sections of the documentation: tutorials, guides, conceptual guides, and references.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/index.md#2025-04-11_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5\">\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./tutorials/tutorial_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">チュートリアル</div>\n      <p class=\"text-gray-700\">出力の生成、独自の拡散システムの構築、拡散モデルのトレーニングを開始するために必要な基本的なスキルを学ぶことができます。初めて 🤗Diffusersを使用する場合は、ここから始めることをおすすめします！</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./using-diffusers/loading_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">ガイド</div>\n      <p class=\"text-gray-700\">パイプライン、モデル、スケジューラの読み込みに役立つ実践的なガイドです。また、特定のタスクにパイプラインを使用する方法、出力の生成方法を制御する方法、生成速度を最適化する方法、さまざまなトレーニング手法についても学ぶことができます。</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./conceptual/philosophy\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Conceptual guides</div>\n      <p class=\"text-gray-700\">ライブラリがなぜこのように設計されたのかを理解し、ライブラリを利用する際の倫理的ガイドラインや安全対策について詳しく学べます。</p>\n   </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./api/models/overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">リファレンス</div>\n      <p class=\"text-gray-700\">🤗 Diffusersのクラスとメソッドがどのように機能するかについての技術的な説明です。</p>\n    </a>\n  </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Training RealFill on Low-Memory GPU with Bash\nDESCRIPTION: Utilizes specific optimizations such as gradient checkpointing, 8-bit optimizers, and xformers to efficiently run RealFill training on GPUs with limited memory. Instructions include setting various script parameters and environment variables.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/realfill/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-2-inpainting\"\nexport TRAIN_DIR=\"data/flowerwoman\"\nexport OUTPUT_DIR=\"flowerwoman-model\"\n\naccelerate launch train_realfill.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$TRAIN_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --resolution=512 \\\n  --train_batch_size=16 \\\n  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --enable_xformers_memory_efficient_attention \\\n  --set_grads_to_none \\\n  --unet_learning_rate=2e-4 \\\n  --text_encoder_learning_rate=4e-5 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=100 \\\n  --max_train_steps=2000 \\\n  --lora_rank=8 \\\n  --lora_dropout=0.1 \\\n  --lora_alpha=16 \\\n```\n\n----------------------------------------\n\nTITLE: Replacing a Pipeline's Scheduler\nDESCRIPTION: Load a pipeline with a custom scheduler by instantiating the scheduler first and then passing it to the pipeline's from_pretrained method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler, DPMSolverMultistepScheduler\n\nrepo_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n\nscheduler = EulerDiscreteScheduler.from_pretrained(repo_id, subfolder=\"scheduler\")\n\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, scheduler=scheduler)\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Configuration File\nDESCRIPTION: YAML configuration for DeepSpeed distributed training with specific optimization settings for GPU memory management and mixed precision training\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndebug: true\ndeepspeed_config:\n  gradient_accumulation_steps: 1\n  gradient_clipping: 1.0\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: false\n  zero_stage: 2\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Removing Cache Files in Data Directory\nDESCRIPTION: This snippet removes the '.cache' directory from the 'dog' directory to clear any unnecessary cache files.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!rm -rf dog/.cache\n```\n\n----------------------------------------\n\nTITLE: Optimizing Memory Layout for Transformer and VAE\nDESCRIPTION: Configures the memory layout of pipeline components to channels_last format for improved performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latte.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline.transformer.to(memory_format=torch.channels_last)\npipeline.vae.to(memory_format=torch.channels_last)\n```\n\n----------------------------------------\n\nTITLE: Creating Prompts File in Python\nDESCRIPTION: This snippet demonstrates creating a 'prompts.txt' file, where each line is a video generation prompt used by CogVideoX. The file is part of a dataset preparation process where paths to video files are configured relative to the data root.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\nA black and white animated sequence featuring a rabbit, named Rabbity Ribfried, and an anthropomorphic goat in a musical, playful environment, showcasing their evolving interaction.\nA black and white animated sequence on a ship's deck features a bulldog character, named Bully Bulldoger, showcasing exaggerated facial expressions and body language. The character progresses from confident to focused, then to strained and distressed, displaying a range of emotions as it navigates challenges. The ship's interior remains static in the background, with minimalistic details such as a bell and open door. The character's dynamic movements and changing expressions drive the narrative, with no camera movement to distract from its evolving reactions and physical gestures.\n...\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Training DreamBooth LoRA with Text Encoder in FLUX.1\nDESCRIPTION: Script for launching DreamBooth LoRA training with text encoder fine-tuning enabled. Uses FLUX.1 model with specific hyperparameters for training configuration including mixed precision, batch size, and learning rate settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_flux.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"black-forest-labs/FLUX.1-dev\"\nexport OUTPUT_DIR=\"trained-flux-dev-dreambooth-lora\"\n\naccelerate launch train_dreambooth_lora_flux.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"bf16\" \\\n  --train_text_encoder\\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --guidance_scale=1 \\\n  --gradient_accumulation_steps=4 \\\n  --optimizer=\"prodigy\" \\\n  --learning_rate=1. \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Implementing DDIM Noise Comparative Analysis Pipeline with Google's DDPM Model\nDESCRIPTION: This code demonstrates how to use a custom DDIM noise comparative analysis pipeline to investigate what visual concepts diffusion models learn at different noise levels. The pipeline applies varying strengths of noise to an input image and then reconstructs it, showing how each noise level contributes to the final sample.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom PIL import Image\nimport numpy as np\n\nimage_path = \"path/to/your/image\"  # images from CelebA-HQ might be better\nimage_pil = Image.open(image_path)\nimage_name = image_path.split(\"/\")[-1].split(\".\")[0]\n\ndevice = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\npipe = DiffusionPipeline.from_pretrained(\n    \"google/ddpm-ema-celebahq-256\",\n    custom_pipeline=\"ddim_noise_comparative_analysis\",\n)\npipe = pipe.to(device)\n\nfor strength in np.linspace(0.1, 1, 25):\n    denoised_image, latent_timestep = pipe(\n        image_pil, strength=strength, return_dict=False\n    )\n    denoised_image = denoised_image[0]\n    denoised_image.save(\n        f\"noise_comparative_analysis_{image_name}_{latent_timestep}.png\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Inference with Checkpoint-Loaded Kandinsky2.2 Decoder\nDESCRIPTION: Python code to load a specific checkpoint of a fine-tuned Kandinsky2.2 decoder model for inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image, UNet2DConditionModel\n\nmodel_path = \"path_to_saved_model\"\n\nunet = UNet2DConditionModel.from_pretrained(model_path + \"/checkpoint-<N>/unet\")\n\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", unet=unet, torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nimage = pipe(prompt=\"A robot naruto, 4k photo\").images[0]\nimage.save(\"robot-naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Memory Optimization with Group Offloading for Wan 2.1\nDESCRIPTION: Demonstrates how to apply group offloading to reduce VRAM usage when working with large Wan 2.1 models. This technique offloads model components to CPU and loads them back to GPU as needed during inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom diffusers import AutoencoderKLWan, WanTransformer3DModel, WanImageToVideoPipeline\nfrom diffusers.hooks.group_offloading import apply_group_offloading\nfrom diffusers.utils import export_to_video, load_image\nfrom transformers import UMT5EncoderModel, CLIPVisionModel\n```\n\n----------------------------------------\n\nTITLE: Creating Canny Image with OpenCV for Stable Diffusion 1.5\nDESCRIPTION: Code to generate a canny edge detection image using OpenCV, which will be used as a control image for the T2I-Adapter. The process involves loading an image, applying the Canny edge detection algorithm, and converting it back to a PIL Image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom diffusers.utils import load_image\n\nimage = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = Image.fromarray(image)\n```\n\n----------------------------------------\n\nTITLE: Setting up TensorRT Inpainting Stable Diffusion Pipeline\nDESCRIPTION: This snippet demonstrates how to set up a TensorRT Inpainting Stable Diffusion Pipeline for accelerated inference. It uses the PNDMScheduler and loads a pre-trained model for inpainting tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nimport torch\nfrom diffusers import PNDMScheduler\nfrom diffusers.pipelines import DiffusionPipeline\n\n# Use the PNDMScheduler scheduler here instead\nscheduler = PNDMScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-inpainting\", subfolder=\"scheduler\")\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-inpainting\",\n    custom_pipeline=\"stable_diffusion_tensorrt_inpaint\",\n    variant='fp16',\n    torch_dtype=torch.float16,\n    scheduler=scheduler,\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing Training Script Dependencies for CogVideoX\nDESCRIPTION: This snippet demonstrates how to navigate to a specific folder and install Python dependencies required for running training scripts using CogVideoX in PyTorch.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n# PyTorch\ncd examples/cogvideo\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-grounded Diffusion with LLM Integration in Python\nDESCRIPTION: This code snippet demonstrates how to use the LLM-grounded Diffusion pipeline with LLM integration. It shows how to initialize the pipeline, parse LLM responses, and generate images based on text prompts and layout information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\n    \"longlian/lmd_plus\",\n    custom_pipeline=\"llm_grounded_diffusion\",\n    custom_revision=\"main\",\n    variant=\"fp16\", torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\n# Generate directly from a text prompt and an LLM response\nprompt = \"a waterfall and a modern high speed train in a beautiful forest with fall foliage\"\nphrases, boxes, bg_prompt, neg_prompt = pipe.parse_llm_response(\"\"\"\n[('a waterfall', [71, 105, 148, 258]), ('a modern high speed train', [255, 223, 181, 149])]\nBackground prompt: A beautiful forest with fall foliage\nNegative prompt:\n\"\"\")\n\nimages = pipe(\n    prompt=prompt,\n    negative_prompt=neg_prompt,\n    phrases=phrases,\n    boxes=boxes,\n    gligen_scheduled_sampling_beta=0.4,\n    output_type=\"pil\",\n    num_inference_steps=50,\n    lmd_guidance_kwargs={}\n).images\n\nimages[0].save(\"./lmd_plus_generation.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Pipeline with trust_remote_code in Python\nDESCRIPTION: Code for loading a custom community pipeline from the Hugging Face Hub with the trust_remote_code flag enabled. This is necessary for custom code execution and is recommended to be used with a specific revision for security reasons.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"<change-username>/<change-id>\", trust_remote_code=True, torch_dtype=torch.float16\n).to(\"cuda\")\n\nprompt = \"hello\"\n\n# Text embeds\nprompt_embeds, negative_embeds = pipeline.encode_prompt(prompt)\n```\n\n----------------------------------------\n\nTITLE: Main Function for PyTorch Distributed Inference Execution\nDESCRIPTION: Executes the run_inference function on multiple GPUs using multiprocessing. Calls the mp.spawn method to create parallel processes, each running the run_inference method across the specified world_size number of GPUs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef main():\n    world_size = 2\n    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dataset Images with Matplotlib\nDESCRIPTION: Creates a visualization of the first four images from the dataset using matplotlib. This helps to inspect the training data before preprocessing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import matplotlib.pyplot as plt\n\n>>> fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n>>> for i, image in enumerate(dataset[:4][\"image\"]):\n...     axs[i].imshow(image)\n...     axs[i].set_axis_off()\n>>> fig.show()\n```\n\n----------------------------------------\n\nTITLE: Loading Custom UNet for Community Pipeline in Python\nDESCRIPTION: Fourth step in building a custom community pipeline, which imports and initializes a custom UNet model (ShowOneUNet3DConditionModel) specifically designed for the Show-1 text-to-video model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom showone_unet_3d_condition import ShowOneUNet3DConditionModel\n\nunet = ShowOneUNet3DConditionModel.from_pretrained(pipe_id, subfolder=\"unet\")\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving an Image from Text\nDESCRIPTION: This snippet demonstrates how to generate an image from a text prompt using the Stable Diffusion pipeline and how to save the resulting image. The output is a PIL.Image object.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> image = pipeline(\"An image of a squirrel in Picasso style\").images[0]\n>>> image\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> image.save(\"image_of_squirrel_painting.png\")\n```\n\n----------------------------------------\n\nTITLE: Advanced Optimization with Attention Processor\nDESCRIPTION: Advanced optimization technique using an efficient attention processor to speed up self-attention operations in both VAE and UNet models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\nfrom diffusers.models.attention_processor import AttnProcessor2_0\n\npipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n    \"prs-eth/marigold-depth-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(\"cuda\")\n\npipe.vae.set_attn_processor(AttnProcessor2_0()) \npipe.unet.set_attn_processor(AttnProcessor2_0())\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\ndepth = pipe(image, num_inference_steps=1)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Fused Pipeline\nDESCRIPTION: Demonstrates loading a previously fused pipeline and using it for inference without needing to load LoRA adapters separately.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipeline = DiffusionPipeline.from_pretrained(\n    \"username/fused-ikea-feng\", torch_dtype=torch.float16,\n).to(\"cuda\")\n\nimage = pipeline(\"A bowl of ramen shaped like a cute kawaii bear, by Feng Zikai\", generator=torch.manual_seed(0)).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Setting Model and Dataset Environment Variables\nDESCRIPTION: Configure environment variables for specifying the base model and training dataset for Stable Diffusion\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Accelerate Configuration\nDESCRIPTION: Commands to initialize an Accelerate environment for distributed training. Includes options for interactive configuration, default setup, or programmatic configuration in environments without interactive shells.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_flux.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Running InstructPix2Pix Video Pipeline\nDESCRIPTION: Initializes and runs the StableDiffusionInstructPix2PixPipeline with custom attention processor for text-guided video editing. Processes multiple frames and saves the result as a video.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video_zero.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\nfrom diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\nmodel_id = \"timbrooks/instruct-pix2pix\"\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\npipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))\n\nprompt = \"make it Van Gogh Starry Night style\"\nresult = pipe(prompt=[prompt] * len(video), image=video).images\nimageio.mimsave(\"edited_video.mp4\", result, fps=4)\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained Model\nDESCRIPTION: Code example showing how to use the trained Wuerstchen model for inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/wuerstchen.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"path/to/saved/model\", torch_dtype=torch.float16).to(\"cuda\")\n\ncaption = \"A cute bird naruto holding a shield\"\nimages = pipeline(\n    caption,\n    width=1024,\n    height=1536,\n    prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,\n    prior_guidance_scale=4.0,\n    num_images_per_prompt=2,\n).images\n```\n\n----------------------------------------\n\nTITLE: Inference with Intermediate Decoder Checkpoint\nDESCRIPTION: Code for performing inference with a specific checkpoint from decoder model training. This loads only the UNet from a checkpoint and combines it with the standard Kandinsky decoder components for inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image, UNet2DConditionModel\n\nunet = UNet2DConditionModel.from_pretrained(\"path/to/saved/model\" + \"/checkpoint-<N>/unet\")\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", unet=unet, torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n\nimage = pipeline(prompt=\"A robot naruto, 4k photo\").images[0]\n```\n\n----------------------------------------\n\nTITLE: Launching RealFill Training with Accelerate and Bash\nDESCRIPTION: Demonstrates how to launch the RealFill training procedure using the accelerate command. It involves setting environment variables for model, training, and output directories, and executing the script with specific training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/realfill/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-2-inpainting\"\nexport TRAIN_DIR=\"data/flowerwoman\"\nexport OUTPUT_DIR=\"flowerwoman-model\"\n\naccelerate launch train_realfill.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$TRAIN_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --resolution=512 \\\n  --train_batch_size=16 \\\n  --gradient_accumulation_steps=1 \\\n  --unet_learning_rate=2e-4 \\\n  --text_encoder_learning_rate=4e-5 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=100 \\\n  --max_train_steps=2000 \\\n  --lora_rank=8 \\\n  --lora_dropout=0.1 \\\n  --lora_alpha=16 \\\n```\n\n----------------------------------------\n\nTITLE: Initializing Training with Python and Accelerate\nDESCRIPTION: Launches a training script for a Flux ControlNet model using CUDA, with specific configuration parameters for batch size, learning rate, and precision\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python3 train_controlnet_flux.py \n\naccelerate  launch  --config_file \"./accelerate_config_zero3.yaml\" train_controlnet_flux.py \\\n    --pretrained_model_name_or_path=$pretrained_model_name_or_path \\\n    --jsonl_for_train=$TRAIN_JSON_FILE \\\n    --conditioning_image_column=$CONTROL_TYPE \\\n    --image_column=image \\\n    --caption_column=$CAPTION_COLUMN\\\n    --cache_dir=$CACHE_DIR \\\n    --tracker_project_name=$MODEL_TYPE \\\n    --output_dir=$OUTPUT_DIR \\\n    --max_train_steps=500000 \\\n    --mixed_precision bf16 \\\n    --checkpointing_steps=1000 \\\n    --gradient_accumulation_steps=8 \\\n    --resolution=512 \\\n    --train_batch_size=1 \\\n    --learning_rate=1e-5 \\\n    --num_double_layers=4 \\\n    --num_single_layers=0 \\\n    --gradient_checkpointing \\\n    --resume_from_checkpoint=\"latest\"\n```\n\n----------------------------------------\n\nTITLE: SchedulerOutput Class Documentation\nDESCRIPTION: Documentation for the SchedulerOutput utility class used to define scheduler outputs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/overview.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput\n```\n\n----------------------------------------\n\nTITLE: Loading Image Processor for Community Pipeline in Python\nDESCRIPTION: Third step in building a custom community pipeline, which loads the CLIPImageProcessor from Transformers to handle image preprocessing for the Show-1 model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import CLIPImageProcessor\n\nfeature_extractor = CLIPImageProcessor.from_pretrained(pipe_id, subfolder=\"feature_extractor\")\n```\n\n----------------------------------------\n\nTITLE: Implementing HD-Painter for Text-Guided Image Inpainting in Python\nDESCRIPTION: This code snippet demonstrates how to use the HD-Painter pipeline for high-resolution text-guided image inpainting. It initializes the pipeline, loads input images, and generates an inpainted image based on a given prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, DDIMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-inpainting\",\n    custom_pipeline=\"hd_painter\"\n)\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"wooden boat\"\ninit_image = load_image(\"https://raw.githubusercontent.com/Picsart-AI-Research/HD-Painter/main/__assets__/samples/images/2.jpg\")\nmask_image = load_image(\"https://raw.githubusercontent.com/Picsart-AI-Research/HD-Painter/main/__assets__/samples/masks/2.png\")\n\nimage = pipe(prompt, init_image, mask_image, use_rasg=True, use_painta=True, generator=torch.manual_seed(12345)).images[0]\n\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Example Code Snippet Placeholder for Diffusion Pipeline\nDESCRIPTION: A placeholder code block intended to be replaced with an actual example showing how to use the diffusion pipeline. This serves as a reminder for model authors to include usage examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/utils/model_card_template.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# TODO: add an example code snippet for running this diffusion pipeline\n```\n\n----------------------------------------\n\nTITLE: Loading IP-Adapter Weights\nDESCRIPTION: This snippet demonstrates loading IP-Adapter weights into a diffusion pipeline. The IP-Adapter allows using image prompts alongside text prompts to guide the image generation process. The weights are loaded from the 'h94/IP-Adapter' repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n```\n\n----------------------------------------\n\nTITLE: Documenting MarigoldNormalsPipeline __call__ Method\nDESCRIPTION: Autodoc directive for generating documentation for the __call__ method of the MarigoldNormalsPipeline class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n[[autodoc]] MarigoldNormalsPipeline\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment - Bash\nDESCRIPTION: This snippet provides commands to initialize the 🤗 Accelerate library's environment, which is optimized for training on multiple GPUs or TPUs. It includes options for both interactive and non-interactive shell setups.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Training DeepFloyd IF Stage 1 with LoRA and DreamBooth\nDESCRIPTION: This script demonstrates how to train the first stage of DeepFloyd IF with LoRA and DreamBooth. It uses a small 64x64 resolution, pre-computed text embeddings, and specific tokenizer settings for the T5 text encoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"DeepFloyd/IF-I-XL-v1.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"dreambooth_dog_lora\"\n\naccelerate launch train_dreambooth_lora.py \\\n  --report_to wandb \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a sks dog\" \\\n  --resolution=64 \\\n  --train_batch_size=4 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --scale_lr \\\n  --max_train_steps=1200 \\\n  --validation_prompt=\"a sks dog\" \\\n  --validation_epochs=25 \\\n  --checkpointing_steps=100 \\\n  --pre_compute_text_embeddings \\\n  --tokenizer_max_length=77 \\\n  --text_encoder_use_attention_mask\n```\n\n----------------------------------------\n\nTITLE: Initializing Scheduler and Optimizer\nDESCRIPTION: Code snippet showing how the diffusion scheduler and optimizer are initialized in the training script, with support for different scheduler configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the scheduler\naccepts_prediction_type = \"prediction_type\" in set(inspect.signature(DDPMScheduler.__init__).parameters.keys())\nif accepts_prediction_type:\n    noise_scheduler = DDPMScheduler(\n        num_train_timesteps=args.ddpm_num_steps,\n        beta_schedule=args.ddpm_beta_schedule,\n        prediction_type=args.prediction_type,\n    )\nelse:\n    noise_scheduler = DDPMScheduler(num_train_timesteps=args.ddpm_num_steps, beta_schedule=args.ddpm_beta_schedule)\n\n# Initialize the optimizer\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with Merged LoRA Adapters\nDESCRIPTION: Uses the merged pixel-art and toy-face adapters to generate an image that combines characteristics from both styles.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"toy_face of a hacker with a hoodie, pixel art\"\nimage = pipe(\n    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": 1.0}, generator=torch.manual_seed(0)\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Full SDXL Fine-tuning Training Command\nDESCRIPTION: Complete training command for fine-tuning SDXL models, including environment variables setup and training script with hyperparameters for the Naruto dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch train_text_to_image_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --enable_xformers_memory_efficient_attention \\\n  --resolution=512 --center_crop --random_flip \\\n  --proportion_empty_prompts=0.2 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 --gradient_checkpointing \\\n  --max_train_steps=10000 \\\n  --use_8bit_adam \\\n  --learning_rate=1e-06 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --mixed_precision=\"fp16\" \\\n  --report_to=\"wandb\" \\\n  --validation_prompt=\"a cute Sundar Pichai creature\" --validation_epochs 5 \\\n  --checkpointing_steps=5000 \\\n  --output_dir=\"sdxl-naruto-model\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Loading a Task-Specific Pipeline for a Multi-Task Model\nDESCRIPTION: Load a checkpoint that supports multiple tasks with a specific pipeline class for the desired task, such as image-to-image generation with Stable Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nrepo_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(repo_id)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using ConsisIDPipeline in Python\nDESCRIPTION: This snippet shows how to import and use the ConsisIDPipeline class for identity-preserving text-to-video generation. It includes placeholders for the pipeline initialization and generation call.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/consisid.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import ConsisIDPipeline\n\n# Initialize the pipeline\npipeline = ConsisIDPipeline.from_pretrained(\"BestWishYsh/ConsisID-preview\")\n\n# Generate video\noutput = pipeline(\n    prompt=\"Your text prompt here\",\n    reference_image=\"path/to/reference/image.jpg\",\n    num_inference_steps=50,\n    num_frames=24\n)\n\n# Process output\n# ...\n```\n\n----------------------------------------\n\nTITLE: Basic Remote VAE Decoding with Random Tensors in Python\nDESCRIPTION: Demonstrates how to use the remote VAE on random tensors for Stable Diffusion v1. It uses the remote_decode function with a specific endpoint and tensor parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_decode.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimage = remote_decode(\n    endpoint=\"https://q1bj3bpq6kzilnsu.us-east-1.aws.endpoints.huggingface.cloud/\",\n    tensor=torch.randn([1, 4, 64, 64], dtype=torch.float16),\n    scaling_factor=0.18215,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Training Requirements\nDESCRIPTION: Command to install the specific requirements for the advanced diffusion training examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Random Noise for Diffusion Input\nDESCRIPTION: Generates random Gaussian noise with the correct shape to use as input for the diffusion model, setting a seed for reproducibility.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n\n>>> torch.manual_seed(0)\n\n>>> noisy_sample = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)\n>>> noisy_sample.shape\n```\n\n----------------------------------------\n\nTITLE: Generating ControlNet-Conditioned Image\nDESCRIPTION: Uses the Kandinsky 2.2 ControlNet Pipeline to generate an image conditioned on both the text embeddings and the depth map. This final step in the ControlNet process combines the textual concepts with the structural information from the depth map.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimage = pipeline(image_embeds=image_emb, negative_image_embeds=zero_image_emb, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Programmatic Accelerate Configuration\nDESCRIPTION: Python code to configure Accelerate in non-interactive environments like notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Using Default Accelerate Configuration\nDESCRIPTION: Command to use the default accelerate configuration without interactive prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Predictive Uncertainty Estimation with Marigold\nDESCRIPTION: Shows how to enable and visualize epistemic uncertainty estimation in Marigold predictions using ensemble predictions. The code loads a half-precision model and generates uncertainty maps that can be saved as images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\n\npipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n    \"prs-eth/marigold-depth-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(\"cuda\")\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\ndepth = pipe(\n\timage,\n\tensemble_size=10,  # any number >= 3\n\toutput_uncertainty=True,\n)\n\nuncertainty = pipe.image_processor.visualize_uncertainty(depth.uncertainty)\nuncertainty[0].save(\"einstein_depth_uncertainty.png\")\n```\n\n----------------------------------------\n\nTITLE: Uploading a LoRA Model to Hugging Face Hub\nDESCRIPTION: Command to upload a trained LoRA model to the Hugging Face Hub using the huggingface-cli.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# huggingface-cli upload my-cool-account-name/my-cool-lora-name /path/to/awesome/lora\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Launch Command\nDESCRIPTION: Accelerate launch command for training IP Adapter using multiple GPUs with mixed precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/ip_adapter/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --num_processes 8 --multi_gpu --mixed_precision \"fp16\" \\\n  tutorial_train_ip-adapter.py \\\n  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5/\" \\\n  --image_encoder_path=\"{image_encoder_path}\" \\\n  --data_json_file=\"{data.json}\" \\\n  --data_root_path=\"{image_path}\" \\\n  --mixed_precision=\"fp16\" \\\n  --resolution=512 \\\n  --train_batch_size=8 \\\n  --dataloader_num_workers=4 \\\n  --learning_rate=1e-04 \\\n  --weight_decay=0.01 \\\n  --output_dir=\"{output_dir}\" \\\n  --save_steps=10000\n```\n\n----------------------------------------\n\nTITLE: Installing JAX 0.4.5\nDESCRIPTION: This bash script installs JAX version 0.4.5 with TPU support using pip, specifying a custom repository for finding the JAX wheels.  This is required when running Flax on TPUs because it ensures compatibility between JAX and the TPU hardware.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install \\\"jax[tpu]==0.4.5\\\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Real Images for Person Regularization\nDESCRIPTION: Command to collect real person images using clip-retrieval for regularization when training on human faces.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install clip-retrieval\npython retrieve.py --class_prompt person --class_data_dir real_reg/samples_person --num_class_images 200\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository and Installing Dependencies - Bash\nDESCRIPTION: This snippet guides users on cloning the diffusers repository and installing the required Python package. It ensures that all necessary dependencies are in place for running the training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/consistency_distillation\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training with Prior-Preservation Loss\nDESCRIPTION: Extended training command including prior-preservation loss to avoid overfitting and language drift, with class image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Stable Diffusion on Custom Dataset\nDESCRIPTION: Bash commands to fine-tune a Stable Diffusion model on your custom dataset stored in a local directory. Uses the same optimization techniques as the Naruto example.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport TRAIN_DIR=\"path_to_your_dataset\"\n\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$TRAIN_DIR \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing HuggingFace Diffusers Repository\nDESCRIPTION: This snippet demonstrates how to clone the HuggingFace Diffusers repository from GitHub and install the library from source. It is essential for setting up the environment to run the training scripts effectively.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/overview.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Applying Rotation Transformation to 3D Meshes\nDESCRIPTION: Code that demonstrates how to apply rotation transformations to the generated 3D meshes using trimesh library. This allows for adjusting the default viewpoint of the mesh before exporting to GLB format.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport trimesh\nimport numpy as np\n\nmesh = trimesh.load(\"3d_cake.ply\")\nrot = trimesh.transformations.rotation_matrix(-np.pi / 2, [1, 0, 0])\nmesh = mesh.apply_transform(rot)\nmesh_export = mesh.export(\"3d_cake.glb\", file_type=\"glb\")\n```\n\n----------------------------------------\n\nTITLE: Documenting MarigoldImageProcessor visualize_normals Method\nDESCRIPTION: Autodoc directive for generating documentation for the visualize_normals method of the MarigoldImageProcessor class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n[[autodoc]] pipelines.marigold.marigold_image_processing.MarigoldImageProcessor.visualize_normals\n```\n\n----------------------------------------\n\nTITLE: Setting Default Accelerate Config (Bash)\nDESCRIPTION: This command sets up a default 🤗 Accelerate environment without requiring any user configurations. This is useful for quickly setting up a basic training environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: LCM-LoRA Training with Datasets Library\nDESCRIPTION: Alternative training script for LCM-LoRA using the Datasets library and PEFT best practices, demonstrated with the Narutos dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\nexport VAE_PATH=\"madebyollin/sdxl-vae-fp16-fix\"\n\naccelerate launch train_lcm_distill_lora_sdxl.py \\\n  --pretrained_teacher_model=${MODEL_NAME}  \\\n  --pretrained_vae_model_name_or_path=${VAE_PATH} \\\n  --output_dir=\"narutos-lora-lcm-sdxl\" \\\n  --mixed_precision=\"fp16\" \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=1024 \\\n  --train_batch_size=24 \\\n  --gradient_accumulation_steps=1 \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --lora_rank=64 \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=3000 \\\n  --checkpointing_steps=500 \\\n  --validation_steps=50 \\\n  --seed=\"0\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Stable Diffusion Inpainting\nDESCRIPTION: Python code for benchmarking Stable Diffusion inpainting with PyTorch 2.0 optimizations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom diffusers import StableDiffusionInpaintPipeline\nimport requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\ndef download_image(url):\n    response = requests.get(url)\n    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = download_image(img_url).resize((512, 512))\nmask_image = download_image(mask_url).resize((512, 512))\n\npath = \"runwayml/stable-diffusion-inpainting\"\n\nrun_compile = True  # Set True / False\n\npipe = StableDiffusionInpaintPipeline.from_pretrained(path, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: Attention Slicing Configuration\nDESCRIPTION: Demonstrates how to enable attention slicing to optimize memory usage and improve performance on Apple silicon devices, especially for systems with less than 64GB RAM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/mps.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True).to(\"mps\")\npipeline.enable_attention_slicing()\n```\n\n----------------------------------------\n\nTITLE: Importing DDIMPipeline in Python\nDESCRIPTION: This snippet demonstrates how to import the DDIMPipeline class from the HuggingFace Diffusers library. The DDIMPipeline is used for generating images using the DDIM model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ddim.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMPipeline\n```\n\n----------------------------------------\n\nTITLE: Loading Base SDXL UNet Model\nDESCRIPTION: Initializes the base SDXL UNet model that corresponds to the LoRA checkpoints.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n    subfolder=\"unet\",\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Schedulers in Wan Pipeline - Python\nDESCRIPTION: This snippet shows how to configure the WanPipeline with custom schedulers such as FlowMatchEulerDiscreteScheduler and UniPCMultistepScheduler. It highlights where to replace with custom scheduler instances. Dependencies include the diffusers library and specific scheduler classes. The code allows setting up the WanPipeline with a scheduler that fits the needs of specific performance characteristics or quality preferences.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FlowMatchEulerDiscreteScheduler, UniPCMultistepScheduler, WanPipeline\n\nscheduler_a = FlowMatchEulerDiscreteScheduler(shift=5.0)\nscheduler_b = UniPCMultistepScheduler(prediction_type=\"flow_prediction\", use_flow_sigmas=True, flow_shift=4.0)\n\npipe = WanPipeline.from_pretrained(\"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", scheduler=<CUSTOM_SCHEDULER_HERE>)\n\n# or,\npipe.scheduler = <CUSTOM_SCHEDULER_HERE>\n```\n\n----------------------------------------\n\nTITLE: Installing Training Dependencies\nDESCRIPTION: Install example-specific requirements by navigating to the example folder and running pip install with the requirements file. This prepares the environment for training models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/unconditional_image_generation/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Inference with InstantStyle for Style and Layout Transfer\nDESCRIPTION: Generates an image using InstantStyle configuration that follows both the style and layout of the reference image. The specific layer activations allow the text prompt to control content while the reference image controls style and composition.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nstyle_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(26)\nimage = pipeline(\n    prompt=\"a cat, masterpiece, best quality, high quality\",\n    ip_adapter_image=style_image,\n    negative_prompt=\"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n    guidance_scale=5,\n    num_inference_steps=30,\n    generator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Video-to-Video Generation with ControlNet Integration\nDESCRIPTION: Implements video-to-video generation using AnimateDiff with ControlNet for enhanced control over spatial information preservation. Uses OpenPose for pose detection and LCM scheduler for faster inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nfrom controlnet_aux.processor import OpenposeDetector\nfrom diffusers import AnimateDiffVideoToVideoControlNetPipeline\nfrom diffusers.utils import export_to_gif, load_video\nfrom diffusers import AutoencoderKL, ControlNetModel, MotionAdapter, LCMScheduler\n\n# Load the ControlNet\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16)\n# Load the motion adapter\nmotion_adapter = MotionAdapter.from_pretrained(\"wangfuyun/AnimateLCM\")\n# Load SD 1.5 based finetuned model\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16)\npipe = AnimateDiffVideoToVideoControlNetPipeline.from_pretrained(\n    \"SG161222/Realistic_Vision_V5.1_noVAE\",\n    motion_adapter=motion_adapter,\n    controlnet=controlnet,\n    vae=vae,\n).to(device=\"cuda\", dtype=torch.float16)\n\n# Enable LCM to speed up inference\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule=\"linear\")\npipe.load_lora_weights(\"wangfuyun/AnimateLCM\", weight_name=\"AnimateLCM_sd15_t2v_lora.safetensors\", adapter_name=\"lcm-lora\")\npipe.set_adapters([\"lcm-lora\"], [0.8])\n\nvideo = load_video(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/dance.gif\")\nvideo = [frame.convert(\"RGB\") for frame in video]\n\nprompt = \"astronaut in space, dancing\"\nnegative_prompt = \"bad quality, worst quality, jpeg artifacts, ugly\"\n```\n\n----------------------------------------\n\nTITLE: End-to-End Image-to-Image with Kandinsky 2.1 Combined Pipeline\nDESCRIPTION: Uses the AutoPipelineForImage2Image to perform end-to-end image-to-image generation with Kandinsky 2.1, including model loading, image preparation, and generation in a single pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\nimport torch\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True)\npipeline.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\noriginal_image = load_image(url)\n\noriginal_image.thumbnail((768, 768))\n\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=original_image, strength=0.3).images[0]\nmake_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Write Basic Configuration with Accelerate - Python\nDESCRIPTION: Use Accelerate's utility to write a basic configuration file, especially useful in non-interactive environments like Jupyter Notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing Script Dependencies (PyTorch)\nDESCRIPTION: Install required dependencies for the text-to-image LoRA training script using PyTorch\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lora.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/text_to_image\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Final Outpainting Pipeline and Mask Generation\nDESCRIPTION: Setup for the final outpainting pipeline using SDXL Inpainting model and creation of blurred mask for smooth transitions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/advanced_inference/outpaint.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = StableDiffusionXLInpaintPipeline.from_pretrained(\n    \"OzzyGT/RealVisXL_V4.0_inpainting\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    vae=vae,\n).to(\"cuda\")\n\nmask = Image.new(\"L\", temp_image.size)\nmask.paste(resized_img.split()[3], (x, y))\nmask = ImageOps.invert(mask)\nfinal_mask = mask.point(lambda p: p > 128 and 255)\nmask_blurred = pipeline.mask_processor.blur(final_mask, blur_factor=20)\nmask_blurred\n```\n\n----------------------------------------\n\nTITLE: Changing Scheduler for Reduced Inference Steps in Python\nDESCRIPTION: Replaces the default scheduler with DPMSolverMultistepScheduler which can produce quality results with fewer inference steps. This further increases generation speed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DPMSolverMultistepScheduler\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: Documenting MarigoldIntrinsicsPipeline __call__ Method\nDESCRIPTION: Autodoc directive for generating documentation for the __call__ method of the MarigoldIntrinsicsPipeline class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n[[autodoc]] MarigoldIntrinsicsPipeline\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Training Custom Diffusion - Single Concept\nDESCRIPTION: Launch script for training Custom Diffusion on a single concept with all necessary parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport OUTPUT_DIR=\"path-to-save-model\"\nexport INSTANCE_DIR=\"./data/cat\"\n\naccelerate launch train_custom_diffusion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --class_data_dir=./real_reg/samples_cat/ \\\n  --with_prior_preservation --real_prior --prior_loss_weight=1.0 \\\n  --class_prompt=\"cat\" --num_class_images=200 \\\n  --instance_prompt=\"photo of a <new1> cat\"  \\\n  --resolution=512  \\\n  --train_batch_size=2  \\\n  --learning_rate=1e-5  \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=250 \\\n  --scale_lr --hflip  \\\n  --modifier_token \"<new1>\"\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Transformation with SDXL Refiner\nDESCRIPTION: Example of using SDXL's refiner model for image-to-image transformation. Loads an initial image and transforms it based on a text prompt using StableDiffusionXLImg2ImgPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLImg2ImgPipeline\nfrom diffusers.utils import load_image\n\npipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipe = pipe.to(\"cuda\")\nurl = \"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png\"\n\ninit_image = load_image(url).convert(\"RGB\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt, image=init_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Diffusers Training\nDESCRIPTION: Installs the diffusers library with training dependencies required for training a diffusion model. This code is commented out and needs to be uncommented to execute in Colab.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install diffusers[training]\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet enumerates the essential Python packages required to run the Diffusers project. It includes libraries for acceleration, computer vision, transformers, datasets, image processing, numerical operations, progress tracking, and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/vqgan/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\ndatasets\ntimm\nnumpy\ntqdm\ntensorboard\n```\n\n----------------------------------------\n\nTITLE: Initializing Text-to-Image Pipeline with PAG\nDESCRIPTION: Sets up a Stable Diffusion XL pipeline for text-to-image generation with PAG enabled. Configures the pipeline with mid-layer PAG application and float16 precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    enable_pag=True,\n    pag_applied_layers=[\"mid\"],\n    torch_dtype=torch.float16\n)\npipeline.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Installing Compel Library\nDESCRIPTION: Shell command to install the Compel library for prompt weighting functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/weighted_prompts.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install compel\n```\n\n----------------------------------------\n\nTITLE: Creating and Connecting to a Google Cloud TPU VM\nDESCRIPTION: This bash script creates a Google Cloud TPU VM and connects to it using SSH. The script takes zone, TPU type, and VM name as parameters. This is necessary to execute the Flax training script on a TPU.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n\"ZONE=us-central2-b\nTPU_TYPE=v4-8\nVM_NAME=hg_flax\n\ngcloud alpha compute tpus tpu-vm create $VM_NAME \\\n --zone $ZONE \\\n --accelerator-type $TPU_TYPE \\\n --version  tpu-vm-v4-base\n\ngcloud alpha compute tpus tpu-vm ssh $VM_NAME --zone $ZONE -- \\\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Commands to configure the Accelerate environment for distributed training, with options for interactive, default, or programmatic configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Kandinsky 2.2\nDESCRIPTION: Sets up the prior pipeline and main pipeline for Kandinsky 2.2, generates image embeddings, and creates an image from the embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\nimport torch\n\nprior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16).to(\"cuda\")\npipeline = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\"\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, guidance_scale=1.0).to_tuple()\n\nimage = pipeline(image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Exporting 3D Meshes to PLY Format\nDESCRIPTION: Code that demonstrates how to export the generated 3D mesh to PLY file format using the export_to_ply utility function from the diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import export_to_ply\n\nply_path = export_to_ply(images[0], \"3d_cake.ply\")\nprint(f\"Saved to folder: {ply_path}\")\n```\n\n----------------------------------------\n\nTITLE: Maximum Optimization with torch.compile\nDESCRIPTION: Final optimization step using torch.compile to further enhance performance on compatible hardware, best suited for repeated pipeline calls due to initial compilation overhead.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/marigold_usage.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nimport torch\nfrom diffusers.models.attention_processor import AttnProcessor2_0\n\npipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n    \"prs-eth/marigold-depth-v1-1\", variant=\"fp16\", torch_dtype=torch.float16\n).to(\"cuda\")\n\npipe.vae.set_attn_processor(AttnProcessor2_0()) \npipe.unet.set_attn_processor(AttnProcessor2_0())\n\npipe.vae = torch.compile(pipe.vae, mode=\"reduce-overhead\", fullgraph=True)\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nimage = diffusers.utils.load_image(\"https://marigoldmonodepth.github.io/images/einstein.jpg\")\n\ndepth = pipe(image, num_inference_steps=1)\n```\n\n----------------------------------------\n\nTITLE: Importing PeftAdapterMixin in Python\nDESCRIPTION: This code snippet shows how to import the PeftAdapterMixin class from the Diffusers library. This mixin class allows Diffusers models to work with PEFT adapters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/loaders/peft.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.loaders.peft import PeftAdapterMixin\n```\n\n----------------------------------------\n\nTITLE: Using Negative Prompts with AnimateDiffPipeline for Video Generation in Python\nDESCRIPTION: This snippet shows how to use negative prompts with the AnimateDiffPipeline to improve video generation quality by deterring unwanted features in the output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n\npipeline = AnimateDiffPipeline.from_pretrained(\"emilianJR/epiCRealism\", motion_adapter=adapter, torch_dtype=torch.float16)\nscheduler = DDIMScheduler.from_pretrained(\n    \"emilianJR/epiCRealism\",\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipeline.scheduler = scheduler\npipeline.enable_vae_slicing()\npipeline.enable_model_cpu_offload()\n\noutput = pipeline(\n    prompt=\"360 camera shot of a sushi roll in a restaurant\",\n    negative_prompt=\"Distorted, discontinuous, ugly, blurry, low resolution, motionless, static\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=50,\n    generator=torch.Generator(\"cpu\").manual_seed(0),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained Prior Model\nDESCRIPTION: Code for performing inference with a trained Kandinsky prior model. It loads the trained model, combines it with a decoder model, and generates an image from a text prompt with CPU offloading for memory efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image, DiffusionPipeline\nimport torch\n\nprior_pipeline = DiffusionPipeline.from_pretrained(output_dir, torch_dtype=torch.float16)\nprior_components = {\"prior_\" + k: v for k,v in prior_pipeline.components.items()}\npipeline = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", **prior_components, torch_dtype=torch.float16)\n\npipe.enable_model_cpu_offload()\nprompt=\"A robot naruto, 4k photo\"\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: UFOGen Scheduler Implementation in Python\nDESCRIPTION: Implementation of UFOGen scheduler for fast one-step text-to-image generation using diffusion models. Includes both one-step and multi-step sampling examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_102\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nfrom scheduling_ufogen import UFOGenScheduler\n\nufogen_model_id_or_path = \"/path/to/ufogen/model\"\npipe = StableDiffusionPipeline(\n    ufogen_model_id_or_path,\n    torch_dtype=torch.float16,\n)\n\npipe.scheduler = UFOGenScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"Three cats having dinner at a table at new years eve, cinematic shot, 8k.\"\n\nonestep_image = pipe(prompt, num_inference_steps=1).images[0]\n\nmultistep_image = pipe(prompt, num_inference_steps=4).images[0]\n```\n\n----------------------------------------\n\nTITLE: LCM-LoRA distillation training command\nDESCRIPTION: Complete accelerate launch command for training a LoRA adapter for Latent Consistency Models that can be injected into any stable-diffusion-v1.5 model. Includes parameters for LoRA rank, mixed precision, resolution, and various optimization settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path/to/saved/model\"\n\naccelerate launch train_lcm_distill_lora_sd_wds.py \\\n    --pretrained_teacher_model=$MODEL_NAME \\\n    --output_dir=$OUTPUT_DIR \\\n    --mixed_precision=fp16 \\\n    --resolution=512 \\\n    --lora_rank=64 \\\n    --learning_rate=1e-4 --loss_type=\"huber\" --adam_weight_decay=0.0 \\\n    --max_train_steps=1000 \\\n    --max_train_samples=4000000 \\\n    --dataloader_num_workers=8 \\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\n    --validation_steps=200 \\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\n    --train_batch_size=12 \\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n    --gradient_accumulation_steps=1 \\\n    --use_8bit_adam \\\n    --resume_from_checkpoint=latest \\\n    --report_to=wandb \\\n    --seed=453645634 \\\n    --push_to_hub \\\n```\n\n----------------------------------------\n\nTITLE: Implementing DDPM denoising loop in Python\nDESCRIPTION: Demonstrates the core denoising loop for a DDPM model, iterating over timesteps to progressively denoise the image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> input = noise\n\n>>> for t in scheduler.timesteps:\n...     with torch.no_grad():\n...         noisy_residual = model(input, t).sample\n...     previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n...     input = previous_noisy_sample\n```\n\n----------------------------------------\n\nTITLE: Loading IP-Adapter Weights\nDESCRIPTION: Loads IP-Adapter weights from the Hugging Face hub and adds them to the pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Serialized Quantized Model\nDESCRIPTION: Demonstrates how to load a serialized quantized model and use it in a pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/torchao.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPipeline, FluxTransformer2DModel\n\ntransformer = FluxTransformer2DModel.from_pretrained(\"/path/to/flux_int8wo\", torch_dtype=torch.bfloat16, use_safetensors=False)\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/Flux.1-Dev\", transformer=transformer, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n\nprompt = \"A cat holding a sign that says hello world\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.0).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Performing Inpainting with SDXL in Python\nDESCRIPTION: Demonstrates how to use SDXL for inpainting by providing an original image, a mask, and a prompt to describe the desired replacement for the masked area.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\n# use from_pipe to avoid consuming additional memory when loading a checkpoint\npipeline = AutoPipelineForInpainting.from_pipe(pipeline_text2image).to(\"cuda\")\n\nimg_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\nmask_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-inpaint-mask.png\"\n\ninit_image = load_image(img_url)\nmask_image = load_image(mask_url)\n\nprompt = \"A deep sea diver floating\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, strength=0.85, guidance_scale=12.5).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Enhancing Results with Prompt Engineering\nDESCRIPTION: Demonstrates how to improve image generation results by refining the prompt with more specific details and style instructions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/stable_diffusion.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompt += \", tribal panther make up, blue on red, side profile, looking away, serious eyes\"\nprompt += \" 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\"\n\nimages = pipeline(**get_inputs(batch_size=8)).images\n```\n\n----------------------------------------\n\nTITLE: Single Node Training for Textual Inversion with Stable Diffusion\nDESCRIPTION: Command for running textual inversion training on a single node using the CompVis/stable-diffusion-v1-4 model. Sets up training parameters including learning rate, batch size, and token configuration for fine-tuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\npython textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub\nDESCRIPTION: Command to log in to Hugging Face Hub for pushing trained models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Loading a Model from Local Files in Diffusers Pipeline in Python\nDESCRIPTION: This snippet shows how to load a StableDiffusionXLPipeline from locally downloaded files using the from_single_file method. It uses the local paths for the model checkpoint and configuration that were downloaded without symlinks in the previous step.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npipeline = StableDiffusionXLPipeline.from_single_file(my_local_checkpoint_path, config=my_local_config_path, local_files_only=True)\n```\n\n----------------------------------------\n\nTITLE: Initializing Kandinsky Prior Model Components\nDESCRIPTION: Loads the necessary components for training a Kandinsky 2.2 prior model including scheduler, image processor, tokenizer, image encoder, and text encoder. The prior model requires both text and image processing capabilities.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnoise_scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", prediction_type=\"sample\")\nimage_processor = CLIPImageProcessor.from_pretrained(\n    args.pretrained_prior_model_name_or_path, subfolder=\"image_processor\"\n)\ntokenizer = CLIPTokenizer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder=\"tokenizer\")\n\nwith ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n        args.pretrained_prior_model_name_or_path, subfolder=\"image_encoder\", torch_dtype=weight_dtype\n    ).eval()\n    text_encoder = CLIPTextModelWithProjection.from_pretrained(\n        args.pretrained_prior_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=weight_dtype\n    ).eval()\n```\n\n----------------------------------------\n\nTITLE: Loading CogView3PlusTransformer2DModel from Pre-trained Checkpoint in Python\nDESCRIPTION: Demonstrates how to load a pre-trained CogView3PlusTransformer2DModel from the THUDM/CogView3Plus-3b repository. The code loads the model with bfloat16 precision and moves it to a CUDA device for faster processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/cogview3plus_transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import CogView3PlusTransformer2DModel\n\ntransformer = CogView3PlusTransformer2DModel.from_pretrained(\"THUDM/CogView3Plus-3b\", subfolder=\"transformer\", torch_dtype=torch.bfloat16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Cloning a Repository with Git LFS for Checkpoint Conversion\nDESCRIPTION: Commands to install Git LFS and clone a repository containing a .ckpt file that needs to be converted to Diffusers format.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/CiaraRowles/TemporalNet\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for DreamBooth Training\nDESCRIPTION: Commands to set up Accelerate configuration for optimized training, including options for interactive and non-interactive environments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sana.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Training the Text Encoder in DreamBooth\nDESCRIPTION: Enable training of the text encoder alongside the UNet to enhance output quality. This requires substantial GPU memory of at least 24GB.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth.py \\\n  --train_text_encoder\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate in non-interactive environments\nDESCRIPTION: Python code snippet to create a basic Accelerate configuration in environments without interactive shell support, such as notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Exporting Stable Diffusion Model to ONNX Format\nDESCRIPTION: Uses the optimum-cli to export a Stable Diffusion model to ONNX format for offline use.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/onnx.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export onnx --model stable-diffusion-v1-5/stable-diffusion-v1-5 sd_v15_onnx/\n```\n\n----------------------------------------\n\nTITLE: Moving Pipeline to GPU in Python\nDESCRIPTION: Transfers the pipeline to the GPU for faster inference. This is essential for reasonable generation speeds with diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline = pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Installing TorchAO and PyTorch\nDESCRIPTION: Command to install the latest versions of PyTorch and TorchAO using pip.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/torchao.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U torch torchao\n```\n\n----------------------------------------\n\nTITLE: Importing LDMTextToImagePipeline in Python\nDESCRIPTION: This snippet shows how to import the LDMTextToImagePipeline class for text-to-image generation using Latent Diffusion Models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latent_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import LDMTextToImagePipeline\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Latent Consistency Model\nDESCRIPTION: This code shows how to generate images with the Latent Consistency Model using minimal steps (1-8 recommended). The model can produce high-quality results even with just 4 steps, making it significantly faster than traditional diffusion models that require 50+ steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n# Can be set to 1~50 steps. LCM supports fast inference even <= 4 steps. Recommend: 1~8 steps.\nnum_inference_steps = 4\n\nimages = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for LCM Training\nDESCRIPTION: Initializes the Adam optimizer for training the UNet model. It configures learning rate, betas, weight decay, and epsilon parameters based on command-line arguments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optimizer_class(\n    unet.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Pipeline in Python\nDESCRIPTION: This code snippet demonstrates how to load a Stable Diffusion pipeline using the Hugging Face Diffusers library. It includes logging in to Hugging Face Hub and moving the pipeline to a CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/schedulers.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import login\nfrom diffusers import DiffusionPipeline\nimport torch\n\n# first we need to login with our access token\nlogin()\n\n# Now we can download the pipeline\npipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n\npipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Early Interruption Callback for Diffusion Process in Python\nDESCRIPTION: Callback function that stops the diffusion process early by setting the pipeline's _interrupt attribute to True after a specified number of steps. This is particularly useful in UI applications where users might want to cancel generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\npipeline.enable_model_cpu_offload()\nnum_inference_steps = 50\n\ndef interrupt_callback(pipeline, i, t, callback_kwargs):\n    stop_idx = 10\n    if i == stop_idx:\n        pipeline._interrupt = True\n\n    return callback_kwargs\n\npipeline(\n    \"A photo of a cat\",\n    num_inference_steps=num_inference_steps,\n    callback_on_step_end=interrupt_callback,\n)\n```\n\n----------------------------------------\n\nTITLE: Skipping Quantization for Specific Modules in Python\nDESCRIPTION: Illustrates how to configure Quanto to skip quantization for specified model modules, which must match the keys in the model's state_dict. This example targets the \"proj_out\" module for skipping, using optimum-quanto and PyTorch.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/quanto.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxTransformer2DModel, QuantoConfig\n\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\nquantization_config = QuantoConfig(weights_dtype=\"float8\", modules_to_not_convert=[\"proj_out\"])\ntransformer = FluxTransformer2DModel.from_pretrained(\n      model_id,\n      subfolder=\"transformer\",\n      quantization_config=quantization_config,\n      torch_dtype=torch.bfloat16,\n)\n```\n\n----------------------------------------\n\nTITLE: Download Conditioning Images - Bash\nDESCRIPTION: Downloads two conditioning images from Hugging Face datasets, which will be used to condition the ControlNet training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n\"wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\"\n```\n\n----------------------------------------\n\nTITLE: Optimizing DeepFloyd IF Pipeline for Memory in Python\nDESCRIPTION: Shows memory optimization techniques for DeepFloyd IF, including CPU offloading strategies, loading text encoders in 8-bit precision, and manual component loading/unloading for systems with limited RAM like Google Colab free tier.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n```\n\nLANGUAGE: python\nCODE:\n```\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_sequential_cpu_offload()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import T5EncoderModel\n\ntext_encoder = T5EncoderModel.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", subfolder=\"text_encoder\", device_map=\"auto\", load_in_8bit=True, variant=\"8bit\"\n)\n\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\",\n    text_encoder=text_encoder,  # pass the previously instantiated 8bit text encoder\n    unet=None,\n    device_map=\"auto\",\n)\n\nprompt_embeds, negative_embeds = pipe.encode_prompt(\"<prompt>\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import IFPipeline, IFSuperResolutionPipeline\nimport torch\nimport gc\nfrom transformers import T5EncoderModel\nfrom diffusers.utils import pt_to_pil, make_image_grid\n\ntext_encoder = T5EncoderModel.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", subfolder=\"text_encoder\", device_map=\"auto\", load_in_8bit=True, variant=\"8bit\"\n)\n\n# text to image\npipe = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\",\n    text_encoder=text_encoder,  # pass the previously instantiated 8bit text encoder\n    unet=None,\n    device_map=\"auto\",\n)\n\nprompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"very deep learning\"'\nprompt_embeds, negative_embeds = pipe.encode_prompt(prompt)\n\n# Remove the pipeline so we can re-load the pipeline with the unet\ndel text_encoder\ndel pipe\ngc.collect()\ntorch.cuda.empty_cache()\n\npipe = IFPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n\ngenerator = torch.Generator().manual_seed(0)\nstage_1_output = pipe(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    generator=generator,\n).images\n\n#pt_to_pil(stage_1_output)[0].save(\"./if_stage_I.png\")\n\n# Remove the pipeline so we can load the super-resolution pipeline\ndel pipe\ngc.collect()\ntorch.cuda.empty_cache()\n\n# First super resolution\n\npipe = IFSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n\ngenerator = torch.Generator().manual_seed(0)\nstage_2_output = pipe(\n    image=stage_1_output,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    generator=generator,\n).images\n\n#pt_to_pil(stage_2_output)[0].save(\"./if_stage_II.png\")\nmake_image_grid([pt_to_pil(stage_1_output)[0], pt_to_pil(stage_2_output)[0]], rows=1, rows=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Collate Function for Preprocessed ControlNet Flux Data\nDESCRIPTION: A custom collate function for PyTorch DataLoader that properly formats and stacks preprocessed embeddings, latents, and condition data for ControlNet Flux training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n    conditioning_pixel_values = torch.stack([example[\"conditioning_pixel_values\"] for example in examples])\n    conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n\n    pixel_latents = torch.stack([torch.tensor(example[\"pixel_latents\"]) for example in examples])\n    pixel_latents = pixel_latents.to(memory_format=torch.contiguous_format).float()\n\n    control_latents = torch.stack([torch.tensor(example[\"control_latents\"]) for example in examples])\n    control_latents = control_latents.to(memory_format=torch.contiguous_format).float()\n    \n    latent_image_ids= torch.stack([torch.tensor(example[\"latent_image_ids\"]) for example in examples])\n    \n    prompt_ids = torch.stack([torch.tensor(example[\"prompt_embeds\"]) for example in examples])\n\n    pooled_prompt_embeds = torch.stack([torch.tensor(example[\"pooled_prompt_embeds\"]) for example in examples])\n    text_ids = torch.stack([torch.tensor(example[\"text_ids\"]) for example in examples])\n\n    return {\n        \"pixel_values\": pixel_values,\n        \"conditioning_pixel_values\": conditioning_pixel_values,\n        \"pixel_latents\": pixel_latents,\n        \"control_latents\": control_latents,\n        \"latent_image_ids\": latent_image_ids,\n        \"prompt_ids\": prompt_ids,\n        \"unet_added_conditions\": {\"pooled_prompt_embeds\": pooled_prompt_embeds, \"time_ids\": text_ids},\n    }\n```\n\n----------------------------------------\n\nTITLE: Installing Flax ControlNet Dependencies\nDESCRIPTION: This bash script navigates to the controlnet example directory and installs the required flax dependencies using pip and the requirements_flax.txt file. This ensures that all necessary libraries are available for the Flax training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n\"cd examples/controlnet\npip install -r requirements_flax.txt\"\n```\n\n----------------------------------------\n\nTITLE: Implementing bfloat16 Precision for Faster Inference\nDESCRIPTION: Enabling bfloat16 reduced precision to improve inference latency without affecting generation quality. This optimization significantly reduces computation time while maintaining result quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\n# Run the attention ops without SDPA.\npipe.unet.set_default_attn_processor()\npipe.vae.set_default_attn_processor()\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```\n\n----------------------------------------\n\nTITLE: Unloading LoRA Weights in Python\nDESCRIPTION: Shows how to unload LoRA weights and restore the model to its original state.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipeline.unload_lora_weights()\n```\n\n----------------------------------------\n\nTITLE: Referencing Diffusers Models Documentation in Markdown\nDESCRIPTION: This snippet provides a markdown link to the official Diffusers documentation for detailed information about the available models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/README.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Models\n\nFor more detail on the models, please refer to the [docs](https://huggingface.co/docs/diffusers/api/models/overview).\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: Standard Apache 2.0 license header comment for the Hugging Face Diffusers project\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Text-to-Video Generation with Custom Scheduler\nDESCRIPTION: Example showing how to replace the default scheduler with the DPMSolverMultistepScheduler for potentially better quality or faster generation of videos.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Spiderman is surfing\"\nvideo_frames = pipe(prompt, num_inference_steps=25).frames[0]\nvideo_path = export_to_video(video_frames)\nvideo_path\n```\n\n----------------------------------------\n\nTITLE: Setting Inference Steps for Scheduler\nDESCRIPTION: Illustrates the required method call to set the number of inference steps for a scheduler before beginning the denoising process. This must be called before using the step function.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md#2025-04-11_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nscheduler.set_num_inference_steps(...)\n```\n\n----------------------------------------\n\nTITLE: Installing Training Requirements for Advanced Diffusion\nDESCRIPTION: Command to install the specific requirements for advanced diffusion training examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Stable Diffusion on Habana Gaudi\nDESCRIPTION: Demonstrates how to generate multiple images in batches from text prompts using the Gaudi-optimized Stable Diffusion pipeline. This example generates 10 images per prompt with a batch size of 4, showing how to efficiently process multiple prompts in parallel.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/habana.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noutputs = pipeline(\n    prompt=[\n        \"High quality photo of an astronaut riding a horse in space\",\n        \"Face of a yellow cat, high resolution, sitting on a park bench\",\n    ],\n    num_images_per_prompt=10,\n    batch_size=4,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Weighted Text Embeddings for SDXL\nDESCRIPTION: Uses sd_embed to generate text embeddings for SDXL with weighted prompts. The function returns both prompt and negative prompt embeddings along with pooled embeddings required for SDXL models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n( \n  prompt_embeds,\n  prompt_neg_embeds,\n  pooled_prompt_embeds,\n  negative_pooled_prompt_embeds\n) = get_weighted_text_embeddings_sdxl(\n    pipe,\n    prompt=prompt,\n    neg_prompt=neg_prompt\n)\n\nimage = pipe(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=prompt_neg_embeds,\n    pooled_prompt_embeds=pooled_prompt_embeds,\n    negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n    num_inference_steps=30,\n    height=1024,\n    width=1024 + 512,\n    guidance_scale=4.0,\n    generator=torch.Generator(\"cuda\").manual_seed(2)\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Custom Diffusion System Implementation\nDESCRIPTION: Example demonstrating how to build a custom diffusion system using the DDPMScheduler and UNet2DModel components to generate images from noise.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDPMScheduler, UNet2DModel\nfrom PIL import Image\nimport torch\n\nscheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")\nmodel = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\").to(\"cuda\")\nscheduler.set_timesteps(50)\n\nsample_size = model.config.sample_size\nnoise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\ninput = noise\n\nfor t in scheduler.timesteps:\n    with torch.no_grad():\n        noisy_residual = model(input, t).sample\n        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n        input = prev_noisy_sample\n\nimage = (input / 2 + 0.5).clamp(0, 1)\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\nimage = Image.fromarray((image * 255).round().astype(\"uint8\"))\nimage\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Diffusers\nDESCRIPTION: Bash commands to create and configure a conda environment with Python 3.10 and install project requirements\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n diffusers python==3.10\nconda activate diffusers\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Optimizing Scheduler for Faster Inference\nDESCRIPTION: Switches to a more efficient DPMSolverMultistepScheduler and reduces inference steps to 20 for faster generation while maintaining quality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/stable_diffusion.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DPMSolverMultistepScheduler\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\nimage = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]\n```\n\n----------------------------------------\n\nTITLE: Installing GGUF Dependencies in Python Environment\nDESCRIPTION: Command to install the GGUF library required for quantized model loading and processing\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/gguf.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U gguf\n```\n\n----------------------------------------\n\nTITLE: Timestep Bias Strategy in SDXL Training Loop\nDESCRIPTION: This snippet demonstrates how timestep weights are calculated and used to add noise during the SDXL training process. The weights influence which timesteps are sampled during training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nweights = generate_timestep_weights(args, noise_scheduler.config.num_train_timesteps).to(\n        model_input.device\n    )\n    timesteps = torch.multinomial(weights, bsz, replacement=True).long()\n\nnoisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)\n```\n\n----------------------------------------\n\nTITLE: Ahead of Time Compilation Function for Stable Diffusion XL\nDESCRIPTION: Creates a compiled JAX function for text-to-image generation with static arguments for optimization and parallelization\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef aot_compile(\n        prompt=default_prompt,\n        negative_prompt=default_neg_prompt,\n        seed=default_seed,\n        guidance_scale=default_guidance_scale,\n        num_inference_steps=default_num_steps\n):\n    prompt_ids, neg_prompt_ids = tokenize_prompt(prompt, negative_prompt)\n    prompt_ids, neg_prompt_ids, rng = replicate_all(prompt_ids, neg_prompt_ids, seed)\n    g = jnp.array([guidance_scale] * prompt_ids.shape[0], dtype=jnp.float32)\n    g = g[:, None]\n\n    return pmap(\n        pipeline._generate,static_broadcasted_argnums=[3, 4, 5, 9]\n        ).lower(\n            prompt_ids,\n            p_params,\n            rng,\n            num_inference_steps, # num_inference_steps\n            height, # height\n            width, # width\n            g,\n            None,\n            neg_prompt_ids,\n            False # return_latents\n            ).compile()\n```\n\n----------------------------------------\n\nTITLE: Importing nglview for 3D Visualization\nDESCRIPTION: Imports the show_rdkit function from nglview, which is used to display interactive 3D visualizations of molecular structures.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom nglview import show_rdkit as show\n```\n\n----------------------------------------\n\nTITLE: Installing DreamBooth SDXL Requirements\nDESCRIPTION: Command to install the specific dependencies needed for DreamBooth training with SDXL.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements_sdxl.txt\n```\n\n----------------------------------------\n\nTITLE: Loading a Pre-trained Diffusion Pipeline\nDESCRIPTION: Loads a pre-trained Stable Diffusion model from the Hugging Face Hub for text-to-image generation using the DiffusionPipeline class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import DiffusionPipeline\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration\nDESCRIPTION: YAML configuration for training with DeepSpeed Zero2 optimization\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/flux-control/README.md#2025-04-11_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 1\n  gradient_clipping: 1.0\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: false\n  zero_stage: 2\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Loading ControlNetModel for Pose Estimation in Text2Video-Zero\nDESCRIPTION: This code loads a ControlNetModel for pose estimation and sets up the StableDiffusionControlNetPipeline with CrossFrameAttnProcessor for video generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16)\npipeline = StableDiffusionControlNetPipeline.from_pretrained(\n    model_id, controlnet=controlnet, torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\npipeline.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n```\n\n----------------------------------------\n\nTITLE: Configuring DDIMScheduler with Trailing Timestep Spacing in Python\nDESCRIPTION: This code configures the DDIMScheduler to always start from the last timestep, which is suggested to improve sampling results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/ddim.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n```\n\n----------------------------------------\n\nTITLE: Running inference with DDPMPipeline in Python\nDESCRIPTION: Demonstrates how to use the DDPMPipeline to generate an image in just a few lines of code.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import DDPMPipeline\n\n>>> ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n>>> image = ddpm(num_inference_steps=25).images[0]\n>>> image\n```\n\n----------------------------------------\n\nTITLE: Preprocess Dataset for Training - Python\nDESCRIPTION: Processes the image and text columns of the dataset. Converts images to RGB, applies transformations, and tokenizes captions, preparing the data for model training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_train(examples):\n    images = [image.convert(\"RGB\") for image in examples[image_column]]\n    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n    examples[\"input_ids\"] = tokenize_captions(examples)\n    return examples\n```\n\n----------------------------------------\n\nTITLE: Verifying PyTorch and PyTorch/XLA Installation\nDESCRIPTION: This snippet outlines the command to verify if PyTorch and PyTorch/XLA have been installed correctly on the TPU. It's essential to confirm that the libraries can be imported without issues.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/training/text_to_image/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\\n--project ${PROJECT_ID} --zone ${ZONE} --worker=all \\\n--command='python3 -c \"import torch; import torch_xla;\"'\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Configuration for Accelerate\nDESCRIPTION: Write basic configuration for Accelerate when using a non-interactive shell. This setup is necessary for environments that cannot support interactive configurations like notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_3\n\nLANGUAGE: py\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Stable Diffusion 3\nDESCRIPTION: This snippet installs necessary Python libraries and tools including diffusers, transformers, accelerate, and others needed to run Stable Diffusion 3 with DreamBooth LoRA training. It requires internet access and Python's pip package manager.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q -U git+https://github.com/huggingface/diffusers\n!pip install -q -U \\\n    transformers \\\n    accelerate \\\n    wandb \\\n    bitsandbytes \\\n    peft\n```\n\n----------------------------------------\n\nTITLE: Creating Image Processing and Display Function\nDESCRIPTION: Defines a utility function to process and display images during the denoising process. The function converts tensor outputs to PIL images after applying necessary transformations for visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> import PIL.Image\n>>> import numpy as np\n\n\n>>> def display_sample(sample, i):\n...     image_processed = sample.cpu().permute(0, 2, 3, 1)\n...     image_processed = (image_processed + 1.0) * 127.5\n...     image_processed = image_processed.numpy().astype(np.uint8)\n\n...     image_pil = PIL.Image.fromarray(image_processed[0])\n...     display(f\"Image at step {i}\")\n...     display(image_pil)\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionControlNetImg2ImgPipeline in Python\nDESCRIPTION: This snippet shows the import statement for the StableDiffusionControlNetImg2ImgPipeline class, which is used for image-to-image generation with ControlNet and Stable Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] StableDiffusionControlNetImg2ImgPipeline\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderKLLTXVideo Model in Python\nDESCRIPTION: This code snippet demonstrates how to load the AutoencoderKLLTXVideo model from the Lightricks/LTX-Video pretrained weights. It specifies the subfolder for the VAE, sets the torch data type to float32, and moves the model to CUDA.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoderkl_ltx_video.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKLLTXVideo\n\nvae = AutoencoderKLLTXVideo.from_pretrained(\"Lightricks/LTX-Video\", subfolder=\"vae\", torch_dtype=torch.float32).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment with Python\nDESCRIPTION: Creates and activates a Python virtual environment using built-in venv module\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv my-env\nsource my-env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Using CogVideoX DDIM Inversion Pipeline for Video Editing in Python\nDESCRIPTION: This code demonstrates how to use the CogVideoX DDIM Inversion Pipeline for video editing. It loads a pre-trained model, performs DDIM inversion on an input video, and generates both inverted and reconstructed versions of the video based on a provided prompt. The pipeline handles frame sampling and processing with configurable parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_112\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom examples.community.cogvideox_ddim_inversion import CogVideoXPipelineForDDIMInversion\n\n\n# Load pretrained pipeline\npipeline = CogVideoXPipelineForDDIMInversion.from_pretrained(\n    \"THUDM/CogVideoX1.5-5B\",\n    torch_dtype=torch.bfloat16,\n).to(\"cuda\")\n\n# Run DDIM inversion, and the videos will be generated in the output_path\noutput = pipeline_for_inversion(\n    prompt=\"prompt that describes the edited video\",\n    video_path=\"path/to/input.mp4\",\n    guidance_scale=6.0,\n    num_inference_steps=50,\n    skip_frames_start=0,\n    skip_frames_end=0,\n    frame_sample_step=None,\n    max_num_frames=81,\n    width=720,\n    height=480,\n    seed=42,\n)\npipeline.export_latents_to_video(output.inverse_latents[-1], \"path/to/inverse_video.mp4\", fps=8)\npipeline.export_latents_to_video(output.recon_latents[-1], \"path/to/recon_video.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Loading the SDXL Model in JAX\nDESCRIPTION: Code to download and load the pre-trained Stable Diffusion XL model using FlaxStableDiffusionXLPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline, params = FlaxStableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", revision=\"refs/pr/95\", split_head_dim=True\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Additional Dependencies\nDESCRIPTION: Commands to install required dependencies for training Custom Diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npip install clip-retrieval\n```\n\n----------------------------------------\n\nTITLE: Installing DeepCache Package\nDESCRIPTION: Command to install the DeepCache package using pip.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/deepcache.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install DeepCache\n```\n\n----------------------------------------\n\nTITLE: Quantizing Models to 8-bit Precision with bitsandbytes\nDESCRIPTION: Demonstrates how to quantize FluxTransformer2DModel and T5EncoderModel to 8-bit precision using BitsAndBytesConfig. It also shows how to adjust the torch_dtype for non-quantized modules.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\nfrom transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n\nfrom diffusers import FluxTransformer2DModel\nfrom transformers import T5EncoderModel\n\nquant_config = TransformersBitsAndBytesConfig(load_in_8bit=True,)\n\ntext_encoder_2_8bit = T5EncoderModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"text_encoder_2\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True,)\n\ntransformer_8bit = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Word Pair Recognition Function in Python\nDESCRIPTION: Implements a function to identify and order meaningful word pairs from generated text. This helps create more coherent and natural-sounding prompts by combining words that are more effective together.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nword_pairs = [\"highly detailed\", \"high quality\", \"enhanced quality\", \"perfect composition\", \"dynamic light\"]\n\ndef find_and_order_pairs(s, pairs):\n    words = s.split()\n    found_pairs = []\n    for pair in pairs:\n        pair_words = pair.split()\n        if pair_words[0] in words and pair_words[1] in words:\n            found_pairs.append(pair)\n            words.remove(pair_words[0])\n            words.remove(pair_words[1])\n\n    for word in words[:]:\n        for pair in pairs:\n            if word in pair.split():\n                words.remove(word)\n                break\n    ordered_pairs = \", \".join(found_pairs)\n    remaining_s = \", \".join(words)\n    return ordered_pairs, remaining_s\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderKLCogVideoX Model in Python\nDESCRIPTION: This snippet demonstrates how to load the AutoencoderKLCogVideoX model from a pretrained checkpoint. It uses the 'from_pretrained' method to load the model with specific parameters and moves it to the CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoderkl_cogvideox.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKLCogVideoX\n\nvae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-2b\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for DreamBooth Training\nDESCRIPTION: Installs the required dependencies for DreamBooth training from a requirements file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/colossalai/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: MoleculeGNN Model Implementation in Python\nDESCRIPTION: Main Graph Neural Network model for molecular diffusion tasks. It combines both local and global graph encoders to predict features necessary for molecular diffusion models, with separate pathways for processing chemical bonds and spatial proximities.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass MoleculeGNN(ModelMixin, ConfigMixin):\n    @register_to_config\n    def __init__(\n        self,\n        hidden_dim=128,\n        num_convs=6,\n        num_convs_local=4,\n        cutoff=10.0,\n        mlp_act=\"relu\",\n        edge_order=3,\n        edge_encoder=\"mlp\",\n        smooth_conv=True,\n    ):\n        super().__init__()\n        self.cutoff = cutoff\n        self.edge_encoder = edge_encoder\n        self.edge_order = edge_order\n\n        \"\"\"\n        edge_encoder: Takes both edge type and edge length as input and outputs a vector [Note]: node embedding is done\n        in SchNetEncoder\n        \"\"\"\n        self.edge_encoder_global = MLPEdgeEncoder(hidden_dim, mlp_act)  # get_edge_encoder(config)\n        self.edge_encoder_local = MLPEdgeEncoder(hidden_dim, mlp_act)  # get_edge_encoder(config)\n\n        \"\"\"\n        The graph neural network that extracts node-wise features.\n        \"\"\"\n        self.encoder_global = SchNetEncoder(\n            hidden_channels=hidden_dim,\n            num_filters=hidden_dim,\n            num_interactions=num_convs,\n            edge_channels=self.edge_encoder_global.out_channels,\n            cutoff=cutoff,\n            smooth=smooth_conv,\n        )\n        self.encoder_local = GINEncoder(\n            hidden_dim=hidden_dim,\n            num_convs=num_convs_local,\n        )\n\n        \"\"\"\n        `output_mlp` takes a mixture of two nodewise features and edge features as input and outputs\n            gradients w.r.t. edge_length (out_dim = 1).\n        \"\"\"\n        self.grad_global_dist_mlp = MultiLayerPerceptron(\n            2 * hidden_dim, [hidden_dim, hidden_dim // 2, 1], activation=mlp_act\n        )\n\n        self.grad_local_dist_mlp = MultiLayerPerceptron(\n            2 * hidden_dim, [hidden_dim, hidden_dim // 2, 1], activation=mlp_act\n        )\n\n        \"\"\"\n        Incorporate parameters together\n        \"\"\"\n        self.model_global = nn.ModuleList([self.edge_encoder_global, self.encoder_global, self.grad_global_dist_mlp])\n        self.model_local = nn.ModuleList([self.edge_encoder_local, self.encoder_local, self.grad_local_dist_mlp])\n\n    def _forward(\n        self,\n        atom_type,\n        pos,\n        bond_index,\n        bond_type,\n        batch,\n        time_step,  # NOTE, model trained without timestep performed best\n        edge_index=None,\n        edge_type=None,\n        edge_length=None,\n        return_edges=False,\n        extend_order=True,\n        extend_radius=True,\n        is_sidechain=None,\n    ):\n        \"\"\"\n        Args:\n            atom_type:  Types of atoms, (N, ).\n            bond_index: Indices of bonds (not extended, not radius-graph), (2, E).\n            bond_type:  Bond types, (E, ).\n            batch:      Node index to graph index, (N, ).\n        \"\"\"\n        N = atom_type.size(0)\n        if edge_index is None or edge_type is None or edge_length is None:\n            edge_index, edge_type = extend_graph_order_radius(\n                num_nodes=N,\n                pos=pos,\n                edge_index=bond_index,\n                edge_type=bond_type,\n                batch=batch,\n                order=self.edge_order,\n                cutoff=self.cutoff,\n                extend_order=extend_order,\n                extend_radius=extend_radius,\n                is_sidechain=is_sidechain,\n            )\n            edge_length = get_distance(pos, edge_index).unsqueeze(-1)  # (E, 1)\n        local_edge_mask = is_local_edge(edge_type)  # (E, )\n\n        # with the parameterization of NCSNv2\n        # DDPM loss implicit handle the noise variance scale conditioning\n        sigma_edge = torch.ones(size=(edge_index.size(1), 1), device=pos.device)  # (E, 1)\n\n        # Encoding global\n        edge_attr_global = self.edge_encoder_global(edge_length=edge_length, edge_type=edge_type)  # Embed edges\n\n        # Global\n        node_attr_global = self.encoder_global(\n```\n\n----------------------------------------\n\nTITLE: Loading Scheduler for Community Pipeline in Python\nDESCRIPTION: Second step in building a custom community pipeline, which loads the DPMSolverMultistepScheduler from the Diffusers library for the Show-1 model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DPMSolverMultistepScheduler\n\nscheduler = DPMSolverMultistepScheduler.from_pretrained(pipe_id, subfolder=\"scheduler\")\n```\n\n----------------------------------------\n\nTITLE: Launching InstructPix2Pix Training\nDESCRIPTION: Command to start the training process with Accelerate using mixed precision. Sets key parameters like model path, dataset, batch size, learning rate, and other training configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Running Full Test Suite - Python Bash\nDESCRIPTION: Executes the full test suite for the repository, which tests all functionalities of the library but may require significant resources and time.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ make test\n```\n\n----------------------------------------\n\nTITLE: Authenticating Hugging Face CLI with Bash\nDESCRIPTION: This snippet shows how to authenticate a user's access token using the Hugging Face CLI. The command is necessary to access certain models that require authentication. Users must have a registered account on Hugging Face and an access token.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/textual_inversion/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Creating a TPU on Google Cloud using gcloud\nDESCRIPTION: This snippet provides the environment variable settings and the command to create a TPU on Google Cloud. Make sure to replace placeholders with actual values for TPU configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/training/text_to_image/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TPU_NAME=<tpu-name>\nexport PROJECT_ID=<project-id>\nexport ZONE=<google-cloud-zone>\nexport ACCELERATOR_TYPE=<accelerator type like v5p-8>\nexport RUNTIME_VERSION=<runtime version like v2-alpha-tpuv5 for v5p>\n\ngcloud alpha compute tpus tpu-vm create ${TPU_NAME} --project ${PROJECT_ID} \\\n--zone ${ZONE} --accelerator-type ${ACCELERATOR_TYPE} --version ${RUNTIME_VERSION} \\\n--reserved\n```\n\n----------------------------------------\n\nTITLE: Chaining Text-to-Image and Image-to-Image Pipelines in Diffusers\nDESCRIPTION: This snippet shows how to chain a text-to-image pipeline with an image-to-image pipeline. It first generates an image from text using Stable Diffusion, then uses that image as input for further modification with another model like Kandinsky.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\nfrom diffusers.utils import make_image_grid\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\ntext2image = pipeline(\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\").images[0]\ntext2image\n```\n\n----------------------------------------\n\nTITLE: Setting Up Diffusers from Source using Bash\nDESCRIPTION: These Bash commands guide installing Diffusers from the source code, facilitating seamless updates for the latest training and modeling features. Cloning the repository and installing in editable mode ensures access to updated libraries and example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\n# Before running the script, make sure you install the library from source\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Generating Video from Image with Stable Video Diffusion\nDESCRIPTION: Implementation of the StableVideoDiffusionPipeline to transform a static image into a short video. This code loads the SVD-XT model, processes an input image, generates frames, and exports them as a video.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/svd.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import StableVideoDiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = StableVideoDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipe.enable_model_cpu_offload()\n\n# Conditioning 이미지 불러오기\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\nimage = image.resize((1024, 576))\n\ngenerator = torch.manual_seed(42)\nframes = pipe(image, decode_chunk_size=8, generator=generator).frames[0]\n\nexport_to_video(frames, \"generated.mp4\", fps=7)\n```\n\n----------------------------------------\n\nTITLE: Importing Remote VAE Utilities in Python\nDESCRIPTION: Imports the remote_encode utility from diffusers library for handling remote VAE operations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_encode.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils.remote_utils import remote_encode\n```\n\n----------------------------------------\n\nTITLE: Initializing Optimizer for Custom Diffusion Training\nDESCRIPTION: Configures the optimizer to update only the necessary parameters: the custom diffusion attention layers and optionally the text encoder's input embeddings if modifier tokens are used. This ensures training focuses only on the components needed for concept learning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optimizer_class(\n    itertools.chain(text_encoder.get_input_embeddings().parameters(), custom_diffusion_layers.parameters())\n    if args.modifier_token is not None\n    else custom_diffusion_layers.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Config for Accelerate - Python\nDESCRIPTION: This snippet shows how to write a basic configuration for the 🤗 Accelerate library programmatically. It's useful in environments where an interactive shell is not available.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for Text-to-Image Training\nDESCRIPTION: Command to install the necessary dependencies for text-to-image fine-tuning from the requirements file in the example folder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Inference Using StableDiffusionXLPipeline in Python\nDESCRIPTION: This Python snippet demonstrates how to load a fine-tuned model and generate images using the StableDiffusionXLPipeline. It imports necessary libraries, initializes the pipeline with the model, and generates images based on specific prompts. The code also includes saving the generated images to local files.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/README_sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\nmodel_id = \"./textual_inversion_cat_sdxl\"\npipe = StableDiffusionXLPipeline.from_pretrained(model_id,torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\nimage.save(\"cat-backpack.png\")\n\nimage = pipe(prompt=\"\", prompt_2=prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\nimage.save(\"cat-backpack-prompt_2.png\")\n```\n\n----------------------------------------\n\nTITLE: Generating Video with CogVideoX in Python\nDESCRIPTION: Compiles the transformer component and generates a video from a text prompt using the CogVideoX pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/cogvideox.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipe.transformer = torch.compile(pipeline.transformer, mode=\"max-autotune\", fullgraph=True)\n\n# CogVideoX works well with long and well-described prompts\nprompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\nvideo = pipe(prompt=prompt, guidance_scale=6, num_inference_steps=50).frames[0]\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderDC Model from Single File in Python\nDESCRIPTION: This snippet shows how to load an AutoencoderDC model from a single file checkpoint using the from_single_file method. It uses a direct URL to the model file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoder_dc.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom difusers import AutoencoderDC\n\nckpt_path = \"https://huggingface.co/mit-han-lab/dc-ae-f32c32-sana-1.0/blob/main/model.safetensors\"\nmodel = AutoencoderDC.from_single_file(ckpt_path)\n```\n\n----------------------------------------\n\nTITLE: Loading a Variant from a Custom Location\nDESCRIPTION: Load a variant of a diffusion model from a custom location, explicitly specifying the variant name to ensure proper loading when the original checkpoint isn't available.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# 👎 this won't work\nstable_diffusion = DiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\", torch_dtype=torch.float16)\n# 👍 this works\nstable_diffusion = DiffusionPipeline.from_pretrained(\n    \"./stable-diffusion-v1-5\", variant=\"fp16\", torch_dtype=torch.float16\n)\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion XL Long Weighted Prompt Pipeline in Python\nDESCRIPTION: This code demonstrates the use of a custom SDXL pipeline that supports unlimited length prompts and negative prompts, compatible with A1111 prompt weighted style. It includes examples of text-to-image, image-to-image, and inpainting tasks using this pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\"\n    , torch_dtype       = torch.float16\n    , use_safetensors   = True\n    , variant           = \"fp16\"\n    , custom_pipeline   = \"lpw_stable_diffusion_xl\",\n)\n\nprompt = \"photo of a cute (white) cat running on the grass\" * 20\nprompt2 = \"chasing (birds:1.5)\" * 20\nprompt = f\"{prompt},{prompt2}\"\nneg_prompt = \"blur, low quality, carton, animate\"\n\npipe.to(\"cuda\")\n\n# text2img\nt2i_images = pipe(\n    prompt=prompt,\n    negative_prompt=neg_prompt,\n).images  # alternatively, you can call the .text2img() function\n\n# img2img\ninput_image = load_image(\"/path/to/local/image.png\")  # or URL to your input image\ni2i_images = pipe.img2img(\n  prompt=prompt,\n  negative_prompt=neg_prompt,\n  image=input_image,\n  strength=0.8,  # higher strength will result in more variation compared to original image\n).images\n\n# inpaint\ninput_mask = load_image(\"/path/to/local/mask.png\")  # or URL to your input inpainting mask\ninpaint_images = pipe.inpaint(\n  prompt=\"photo of a cute (black) cat running on the grass\" * 20,\n  negative_prompt=neg_prompt,\n  image=input_image,\n  mask=input_mask,\n  strength=0.6,  # higher strength will result in more variation compared to original image\n).images\n\npipe.to(\"cpu\")\ntorch.cuda.empty_cache()\n\nfrom IPython.display import display  # assuming you are using this code in a notebook\ndisplay(t2i_images[0])\ndisplay(i2i_images[0])\ndisplay(inpaint_images[0])\n```\n\n----------------------------------------\n\nTITLE: First Inference with AOT Compiled Pipeline\nDESCRIPTION: Demonstrates the first image generation using the AOT compiled pipeline, which may have slightly longer execution time\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\nprompt = \"photo of a rhino dressed suit and tie sitting at a table in a bar with a bar stools, award winning photography, Elke vogelsang\"\nneg_prompt = \"cartoon, illustration, animation. face. male, female\"\nimages = generate(prompt, neg_prompt)\nprint(f\"First inference in {time.time() - start}\")\n```\n\n----------------------------------------\n\nTITLE: Cloning Diffusers Repository (Bash)\nDESCRIPTION: This command clones the Diffusers repository from GitHub to the local machine. It's a prerequisite step for accessing the training scripts and necessary dependencies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Downloading Training Dataset\nDESCRIPTION: Python code to download the example dog dataset from Hugging Face Hub using snapshot_download.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_lumina2.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing ToMe Package\nDESCRIPTION: Command to install the tomesd package via pip package manager\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install tomesd\n```\n\n----------------------------------------\n\nTITLE: Using CPU Generator for GPU Pipeline in Diffusers\nDESCRIPTION: This code demonstrates using a CPU-based Generator with torch.manual_seed for better reproducibility in GPU pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom diffusers import DDIMPipeline\n\nddim = DDIMPipeline.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)\nddim.to(\"cuda\")\ngenerator = torch.manual_seed(0)\nimage = ddim(num_inference_steps=2, output_type=\"np\", generator=generator).images\nprint(np.abs(image).sum())\n```\n\n----------------------------------------\n\nTITLE: Enabling Memory Optimizations in Diffusers Pipeline\nDESCRIPTION: Code snippet showing three key memory optimization techniques in the diffusers library: sequential CPU offload to move unused components to CPU, VAE slicing to process smaller chunks, and VAE tiling to reduce memory consumption during VAE encoding/decoding.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Quantization for Additional Memory Savings\nDESCRIPTION: Applying nested quantization to both text encoder and transformer components of a Flux.1 model. This technique performs a second quantization on already quantized weights to save an additional 0.4 bits per parameter without affecting performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\nfrom transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n\nfrom diffusers import FluxTransformer2DModel\nfrom transformers import T5EncoderModel\n\nquant_config = TransformersBitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n)\n\ntext_encoder_2_4bit = T5EncoderModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"text_encoder_2\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n)\n\ntransformer_4bit = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n```\n\n----------------------------------------\n\nTITLE: Cloning Apple's Stable Diffusion Repo\nDESCRIPTION: Commands to clone Apple's ml-stable-diffusion repository for Swift inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/coreml.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/apple/ml-stable-diffusion\ncd ml-stable-diffusion\n```\n\n----------------------------------------\n\nTITLE: Training Textual Inversion Model with PyTorch\nDESCRIPTION: Run the textual inversion training script using PyTorch implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text_inversion.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport DATA_DIR=\"./cat\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"textual_inversion_cat\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Downloading Example Dataset for DreamBooth Training\nDESCRIPTION: Python code to download the dog example dataset from Hugging Face Hub for use in DreamBooth training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Development Branch\nDESCRIPTION: Command to create and checkout a new branch for development changes\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout -b a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Flax Implementation\nDESCRIPTION: Commands to navigate to the textual inversion example folder and install the required dependencies for the Flax implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/textual_inversion\npip install -r requirements_flax.txt\n```\n\n----------------------------------------\n\nTITLE: Using Negative Prompts in Image-to-Image Generation with SDXL Refiner\nDESCRIPTION: This example demonstrates how to use negative prompts to condition the model to avoid certain elements in the generated image. Negative prompts can improve image quality by excluding unwanted features or modify an image by specifying things to exclude.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/img2img.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make_image_grid, load_image\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nnegative_prompt = \"ugly, deformed, disfigured, poor details, bad anatomy\"\n\n# pass prompt and image to pipeline\nimage = pipeline(prompt, negative_prompt=negative_prompt, image=init_image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Handling Unsupported Checkpoints with AutoPipeline\nDESCRIPTION: Demonstrates what happens when trying to use AutoPipeline with an unsupported checkpoint. This example shows the ValueError that occurs when attempting to load the ShapE image-to-image pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"openai/shap-e-img2img\", torch_dtype=torch.float16, use_safetensors=True\n)\n\"ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\"\n```\n\n----------------------------------------\n\nTITLE: Installing K Diffusion for Stable Diffusion in Python\nDESCRIPTION: This snippet shows how to install the K Diffusion library, which is required for using Stable Diffusion with K Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\npip install k-diffusion\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet enumerates the required Python packages and their versions for the Hugging Face Diffusers project. It includes libraries for acceleration, computer vision, transformers, text processing, visualization, templating, and parameter-efficient fine-tuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/requirements_sd3.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.31.0\ntorchvision\ntransformers>=4.41.2\nftfy\ntensorboard\nJinja2\npeft==0.11.1\nsentencepiece\n```\n\n----------------------------------------\n\nTITLE: Installing RDKit for Chemistry Visualization\nDESCRIPTION: Installs RDKit, a toolkit that provides functionalities for cheminformatics and molecule visualization in Python.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n!pip install rdkit\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for ControlNet\nDESCRIPTION: Commands to install the necessary Python libraries for working with ControlNet, including diffusers, transformers, accelerate, and OpenCV.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate opencv-python\n```\n\n----------------------------------------\n\nTITLE: Defining GNN Helper Classes in Python\nDESCRIPTION: These classes implement multi-layer perceptrons and convolution operations for use in graph neural network models. They are essential for building custom layers that process molecular data effectively.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass MoleculeGNNOutput(BaseOutput):\n    \"\"\"\n    Args:\n        sample (`torch.Tensor` of shape `(batch_size, num_channels, height, width)`):\n            Hidden states output. Output of last layer of model.\n    \"\"\"\n\n    sample: torch.Tensor\n```\n\nLANGUAGE: python\nCODE:\n```\nclass MultiLayerPerceptron(nn.Module):\n    \"\"\"\n    Multi-layer Perceptron. Note there is no activation or dropout in the last layer.\n    Args:\n        input_dim (int): input dimension\n        hidden_dim (list of int): hidden dimensions\n        activation (str or function, optional): activation function\n        dropout (float, optional): dropout rate\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dims, activation=\"relu\", dropout=0):\n        super(MultiLayerPerceptron, self).__init__()\n\n        self.dims = [input_dim] + hidden_dims\n        if isinstance(activation, str):\n            self.activation = getattr(F, activation)\n        else:\n            print(f\"Warning, activation passed {activation} is not string and ignored\")\n            self.activation = None\n        if dropout > 0:\n            self.dropout = nn.Dropout(dropout)\n        else:\n            self.dropout = None\n\n        self.layers = nn.ModuleList()\n        for i in range(len(self.dims) - 1):\n            self.layers.append(nn.Linear(self.dims[i], self.dims[i + 1]))\n\n    def forward(self, x):\n        \"\"\"\"\"\"\n        for i, layer in enumerate(self.layers):\n            x = layer(x)\n            if i < len(self.layers) - 1:\n                if self.activation:\n                    x = self.activation(x)\n                if self.dropout:\n                    x = self.dropout(x)\n        return x\n```\n\nLANGUAGE: python\nCODE:\n```\nclass ShiftedSoftplus(torch.nn.Module):\n    def __init__(self):\n        super(ShiftedSoftplus, self).__init__()\n        self.shift = torch.log(torch.tensor(2.0)).item()\n\n    def forward(self, x):\n        return F.softplus(x) - self.shift\n```\n\nLANGUAGE: python\nCODE:\n```\nclass CFConv(MessagePassing):\n    def __init__(self, in_channels, out_channels, num_filters, mlp, cutoff, smooth):\n        super(CFConv, self).__init__(aggr=\"add\")\n        self.lin1 = Linear(in_channels, num_filters, bias=False)\n        self.lin2 = Linear(num_filters, out_channels)\n        self.nn = mlp\n        self.cutoff = cutoff\n        self.smooth = smooth\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self.lin1.weight)\n        torch.nn.init.xavier_uniform_(self.lin2.weight)\n        self.lin2.bias.data.fill_(0)\n\n    def forward(self, x, edge_index, edge_length, edge_attr):\n        if self.smooth:\n            C = 0.5 * (torch.cos(edge_length * np.pi / self.cutoff) + 1.0)\n            C = C * (edge_length <= self.cutoff) * (edge_length >= 0.0)  # Modification: cutoff\n        else:\n            C = (edge_length <= self.cutoff).float()\n        W = self.nn(edge_attr) * C.view(-1, 1)\n\n        x = self.lin1(x)\n        x = self.propagate(edge_index, x=x, W=W)\n        x = self.lin2(x)\n        return x\n\n    def message(self, x_j: torch.Tensor, W) -> torch.Tensor:\n        return x_j * W\n```\n\nLANGUAGE: python\nCODE:\n```\nclass InteractionBlock(torch.nn.Module):\n    def __init__(self, hidden_channels, num_gaussians, num_filters, cutoff, smooth):\n        super(InteractionBlock, self).__init__()\n        mlp = Sequential(\n            Linear(num_gaussians, num_filters),\n            ShiftedSoftplus(),\n            Linear(num_filters, num_filters),\n        )\n        self.conv = CFConv(hidden_channels, hidden_channels, num_filters, mlp, cutoff, smooth)\n        self.act = ShiftedSoftplus()\n        self.lin = Linear(hidden_channels, hidden_channels)\n\n    def forward(self, x, edge_index, edge_length, edge_attr):\n        x = self.conv(x, edge_index, edge_length, edge_attr)\n        x = self.act(x)\n        x = self.lin(x)\n        return x\n```\n\nLANGUAGE: python\nCODE:\n```\nclass SchNetEncoder(Module):\n    def __init__(\n        self, hidden_channels=128, num_filters=128, num_interactions=6, edge_channels=100, cutoff=10.0, smooth=False\n    ):\n        super().__init__()\n\n        self.hidden_channels = hidden_channels\n        self.num_filters = num_filters\n        self.num_interactions = num_interactions\n        self.cutoff = cutoff\n\n        self.embedding = Embedding(100, hidden_channels, max_norm=10.0)\n\n        self.interactions = ModuleList()\n        for _ in range(num_interactions):\n            block = InteractionBlock(hidden_channels, edge_channels, num_filters, cutoff, smooth)\n            self.interactions.append(block)\n\n    def forward(self, z, edge_index, edge_length, edge_attr, embed_node=True):\n        if embed_node:\n            assert z.dim() == 1 and z.dtype == torch.long\n            h = self.embedding(z)\n        else:\n            h = z\n        for interaction in self.interactions:\n            h = h + interaction(h, edge_index, edge_length, edge_attr)\n\n        return h\n```\n\nLANGUAGE: python\nCODE:\n```\nclass GINEConv(MessagePassing):\n    \"\"\"\n    Custom class of the graph isomorphism operator from the \"How Powerful are Graph Neural Networks?\n    https://arxiv.org/abs/1810.00826 paper. Note that this implementation has the added option of a custom activation.\n    \"\"\"\n\n    def __init__(self, mlp: Callable, eps: float = 0.0, train_eps: bool = False, activation=\"softplus\", **kwargs):\n        super(GINEConv, self).__init__(aggr=\"add\", **kwargs)\n        self.nn = mlp\n        self.initial_eps = eps\n\n        if isinstance(activation, str):\n            self.activation = getattr(F, activation)\n        else:\n            self.activation = None\n\n        if train_eps:\n            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n        else:\n            self.register_buffer(\"eps\", torch.Tensor([eps]))\n\n    def forward(\n        self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: OptTensor = None, size: Size = None\n    ) -> torch.Tensor:\n        \"\"\"\"\"\"\n        if isinstance(x, torch.Tensor):\n            x: OptPairTensor = (x, x)\n\n        # Node and edge feature dimensionalites need to match.\n        if isinstance(edge_index, torch.Tensor):\n            assert edge_attr is not None\n            assert x[0].size(-1) == edge_attr.size(-1)\n        elif isinstance(edge_index, SparseTensor):\n            assert x[0].size(-1) == edge_index.size(-1)\n\n        # propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\n        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n\n        x_r = x[1]\n        if x_r is not None:\n            out += (1 + self.eps) * x_r\n\n        return self.nn(out)\n\n    def message(self, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n        if self.activation:\n            return self.activation(x_j + edge_attr)\n        else:\n            return x_j + edge_attr\n\n    def __repr__(self):\n        return \"{}(nn={})\".format(self.__class__.__name__, self.nn)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass GINEncoder(torch.nn.Module):\n    def __init__(self, hidden_dim, num_convs=3, activation=\"relu\", short_cut=True, concat_hidden=False):\n        super().__init__()\n\n        self.hidden_dim = hidden_dim\n        self.num_convs = num_convs\n        self.short_cut = short_cut\n        self.concat_hidden = concat_hidden\n        self.node_emb = nn.Embedding(100, hidden_dim)\n\n        if isinstance(activation, str):\n            self.activation = getattr(F, activation)\n        else:\n            self.activation = None\n\n        self.convs = nn.ModuleList()\n        for i in range(self.num_convs):\n            self.convs.append(\n                GINEConv(\n                    MultiLayerPerceptron(hidden_dim, [hidden_dim, hidden_dim], activation=activation),\n                    activation=activation,\n                )\n            )\n\n    def forward(self, z, edge_index, edge_attr):\n        \"\"\"\n        Input:\n            data: (torch_geometric.data.Data): batched graph edge_index: bond indices of the original graph (num_node,\n            hidden) edge_attr: edge feature tensor with shape (num_edge, hidden)\n        Output:\n            node_feature: graph feature\n        \"\"\"\n\n        node_attr = self.node_emb(z)  # (num_node, hidden)\n\n        hiddens = []\n        conv_input = node_attr  # (num_node, hidden)\n\n        for conv_idx, conv in enumerate(self.convs):\n            hidden = conv(conv_input, edge_index, edge_attr)\n            if conv_idx < len(self.convs) - 1 and self.activation is not None:\n                hidden = self.activation(hidden)\n            assert hidden.shape == conv_input.shape\n            if self.short_cut and hidden.shape == conv_input.shape:\n                hidden += conv_input\n\n            hiddens.append(hidden)\n            conv_input = hidden\n\n        if self.concat_hidden:\n            node_feature = torch.cat(hiddens, dim=-1)\n        else:\n            node_feature = hiddens[-1]\n\n        return node_feature\n```\n\nLANGUAGE: python\nCODE:\n```\nclass MLPEdgeEncoder(Module):\n    def __init__(self, hidden_dim=100, activation=\"relu\"):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.bond_emb = Embedding(100, embedding_dim=self.hidden_dim)\n        self.mlp = MultiLayerPerceptron(1, [self.hidden_dim, self.hidden_dim], activation=activation)\n\n    @property\n    def out_channels(self):\n        return self.hidden_dim\n\n    def forward(self, edge_length, edge_type):\n        \"\"\"\n        Input:\n\n```\n\n----------------------------------------\n\nTITLE: Remote VAE Decoding for Flux Model with Random Tensors in Python\nDESCRIPTION: Shows how to use remote VAE decoding specifically for the Flux model, which requires additional parameters like height and width due to packed latents.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_decode.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimage = remote_decode(\n    endpoint=\"https://whhx50ex1aryqvw6.us-east-1.aws.endpoints.huggingface.cloud/\",\n    tensor=torch.randn([1, 4096, 64], dtype=torch.float16),\n    height=1024,\n    width=1024,\n    scaling_factor=0.3611,\n    shift_factor=0.1159,\n)\n```\n\n----------------------------------------\n\nTITLE: Replicating Parameters across TPU Devices\nDESCRIPTION: Code to replicate model parameters across all available TPU devices for parallel processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\np_params = replicate(params)\n\ndef replicate_all(prompt_ids, neg_prompt_ids, seed):\n    ...\n```\n\n----------------------------------------\n\nTITLE: SD3 with CPU Offloading\nDESCRIPTION: Implementation of SD3 with model offloading to CPU for memory optimization during inference\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\n\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nimage = pipe(\n    prompt=\"a photo of a cat holding a sign that says hello world\",\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    height=1024,\n    width=1024,\n    guidance_scale=7.0,\n).images[0]\n\nimage.save(\"sd3_hello_world.png\")\n```\n\n----------------------------------------\n\nTITLE: SDXL Inference with PyTorch After Training\nDESCRIPTION: Python code for running inference with a trained SDXL model using PyTorch. This loads the model with half-precision floating-point format for efficient inference on GPU.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"path/to/your/model\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A naruto with green eyes and red legs.\"\nimage = pipeline(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate in Python Environment\nDESCRIPTION: Python code to write a basic Accelerate configuration for environments without an interactive shell.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sd3.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing Intel Extension for PyTorch (IPEX) via Shell Commands\nDESCRIPTION: Commands for installing IPEX package for PyTorch acceleration on Intel CPUs. Includes both latest version and specific version installation options.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_43\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install intel_extension_for_pytorch\n```\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n\n----------------------------------------\n\nTITLE: Pipeline Interface Pattern\nDESCRIPTION: Example showing the standard pipeline execution pattern through the __call__ method, which is consistent across all pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/philosophy.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline.__call__()\n```\n\n----------------------------------------\n\nTITLE: Enabling Full Determinism in Diffusers\nDESCRIPTION: This snippet shows how to enable full determinism in Diffusers, which uses deterministic algorithms for complete reproducibility at the cost of performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nenable_full_determinism()\n```\n\n----------------------------------------\n\nTITLE: Launching Consistency Model Training with Accelerate\nDESCRIPTION: This bash command launches the training script for a consistency model using the Accelerate library. It specifies various parameters including dataset, output directory, precision, resolution, training steps, batch size, learning rate, and model configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/consistency_training/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch examples/research_projects/consistency_training/train_cm_ct_unconditional.py \\\n    --dataset_name=\"cifar10\" \\\n    --dataset_image_column_name=\"img\" \\\n    --output_dir=\"/path/to/output/dir\" \\\n    --mixed_precision=fp16 \\\n    --resolution=32 \\\n    --max_train_steps=1000 --max_train_samples=10000 \\\n    --dataloader_num_workers=8 \\\n    --noise_precond_type=\"cm\" --input_precond_type=\"cm\" \\\n    --train_batch_size=4 \\\n    --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n    --use_8bit_adam \\\n    --use_ema \\\n    --validation_steps=100 --eval_batch_size=4 \\\n    --checkpointing_steps=100 --checkpoints_total_limit=10 \\\n    --class_conditional --num_classes=10 \\\n```\n\n----------------------------------------\n\nTITLE: Implementing EDICT Image Editing Pipeline in Python\nDESCRIPTION: This snippet shows the implementation of the EDICT (Exact Diffusion Inversion via Coupled Transformations) image editing pipeline. It uses a base prompt describing the current image and a target prompt for the desired edits. The pipeline is initialized with specific scheduler and text encoder configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, DDIMScheduler\nfrom transformers import CLIPTextModel\nimport torch, PIL, requests\nfrom io import BytesIO\nfrom IPython.display import display\n\ndef center_crop_and_resize(im):\n\n    width, height = im.size\n    d = min(width, height)\n    left = (width - d) / 2\n    upper = (height - d) / 2\n    right = (width + d) / 2\n    lower = (height + d) / 2\n\n    return im.crop((left, upper, right, lower)).resize((512, 512))\n\ntorch_dtype = torch.float16\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# scheduler and text_encoder param values as in the paper\nscheduler = DDIMScheduler(\n        num_train_timesteps=1000,\n        beta_start=0.00085,\n        beta_end=0.012,\n        beta_schedule=\"scaled_linear\",\n        set_alpha_to_one=False,\n        clip_sample=False,\n)\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    pretrained_model_name_or_path=\"openai/clip-vit-large-patch14\",\n    torch_dtype=torch_dtype,\n)\n\n# initialize pipeline\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path=\"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"edict_pipeline\",\n    variant=\"fp16\",\n    scheduler=scheduler,\n    text_encoder=text_encoder,\n    leapfrog_steps=True,\n    torch_dtype=torch_dtype,\n).to(device)\n\n# download image\nimage_url = \"https://huggingface.co/datasets/Joqsan/images/resolve/main/imagenet_dog_1.jpeg\"\nresponse = requests.get(image_url)\nimage = PIL.Image.open(BytesIO(response.content))\n\n# preprocess it\ncropped_image = center_crop_and_resize(image)\n\n# define the prompts\nbase_prompt = \"A dog\"\ntarget_prompt = \"A golden retriever\"\n\n# run the pipeline\nresult_image = pipeline(\n      base_prompt=base_prompt,\n      target_prompt=target_prompt,\n      image=cropped_image,\n)\n\ndisplay(result_image)\n```\n\n----------------------------------------\n\nTITLE: Merging LoRA Parameters in Python\nDESCRIPTION: This code snippet illustrates how to load a pretrained model and merge the LoRA weights into it, ensuring the proper handling of weights and the resulting model's state.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxPipeline \nimport torch \n\nckpt_id = \"black-forest-labs/FLUX.1-dev\"\npipeline = FluxPipeline.from_pretrained(\n    ckpt_id, text_encoder=None, text_encoder_2=None, torch_dtype=torch.float16\n)\npipeline.load_lora_weights(\"yarn_art_lora_flux_nf4\", weight_name=\"pytorch_lora_weights.safetensors\")\npipeline.fuse_lora()\npipeline.unload_lora_weights()\n\npipeline.transformer.save_pretrained(\"fused_transformer\")\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Accelerate Config (Python)\nDESCRIPTION: This Python code snippet writes a basic configuration for 🤗 Accelerate programmatically. It's useful for environments without an interactive shell, such as notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset for DreamBooth Training\nDESCRIPTION: Python code to download a sample dataset (dog images) from Hugging Face for use in DreamBooth training examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies for Diffusers Research Projects\nDESCRIPTION: Command to install the required dependencies for a specific research project using pip package manager. Must be executed from within the chosen project folder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/README.md#2025-04-11_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Loading SDXL ControlNet Pipeline - Python\nDESCRIPTION: Initializes the SDXL ControlNet pipeline with a canny edge detection model and VAE. Enables model CPU offloading to optimize memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-canny-sdxl-1.0\",\n    torch_dtype=torch.float16,\n    use_safetensors=True\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    vae=vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True\n)\npipe.enable_model_cpu_offload()\n```\n\n----------------------------------------\n\nTITLE: Enabling Scaled Dot Product Attention for Faster Attention Processing\nDESCRIPTION: Using PyTorch's scaled_dot_product_attention function to efficiently process attention blocks. This optimization is enabled by default in Diffusers and significantly improves processing speed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt, num_inference_steps=30).images[0]\n```\n\n----------------------------------------\n\nTITLE: Applying Initial SDXL Optimizations\nDESCRIPTION: Initializes SDXL pipeline with bfloat16, fuses QKV projections, and optimizes memory layout\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# SDPA + bfloat16.\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\n# Combine attention projection matrices.\npipe.fuse_qkv_projections()\n\n# Change the memory layout.\npipe.unet.to(memory_format=torch.channels_last)\npipe.vae.to(memory_format=torch.channels_last)\n```\n\n----------------------------------------\n\nTITLE: Prompting with LoRA and Embedding in AUTOMATIC1111/SD.Next\nDESCRIPTION: Example of how to use a trained LoRA and textual embedding in a prompt for AUTOMATIC1111 or SD.Next. The 'y2k_emb' token represents the embedding, while the LoRA is applied using the <lora:name:weight> syntax.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\na y2k_emb webpage about the movie Mean Girls <lora:y2k:0.9>\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face CLI\nDESCRIPTION: Command to log into Hugging Face CLI to access gated models like Stable Diffusion 3 and to enable pushing trained models to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Commands to clone the diffusers repository and install it in development mode for accessing the latest features and training dependencies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters for SDXL Ensemble Denoising\nDESCRIPTION: Defines the number of inference steps and noise fraction cutoff point for ensemble denoising with SDXL. The high_noise_frac parameter determines when to switch from base to refiner model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nn_steps = 40\nhigh_noise_frac = 0.8\n```\n\n----------------------------------------\n\nTITLE: Example GLIGEN Generation Prompt\nDESCRIPTION: Python code demonstrating how to specify a prompt, bounding boxes, and phrases for controlled image generation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprompt = 'A realistic image of landscape scene depicting a green car parking on the left of a blue truck, with a red air balloon and a bird in the sky'\nboxes = [[0.041015625, 0.548828125, 0.453125, 0.859375],\n         [0.525390625, 0.552734375, 0.93359375, 0.865234375],\n         [0.12890625, 0.015625, 0.412109375, 0.279296875],\n         [0.578125, 0.08203125, 0.857421875, 0.27734375]]\ngligen_phrases = ['a green car', 'a blue truck', 'a red air balloon', 'a bird']\n```\n\n----------------------------------------\n\nTITLE: License Header Comment Block in Markdown\nDESCRIPTION: Apache 2.0 license header for the Huggingface Diffusers project, specifying copyright and usage terms.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/loaders/unet.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Initializing DDIM Pipeline and Getting Outputs\nDESCRIPTION: Demonstrates how to initialize a DDIM pipeline from pretrained weights and generate outputs. Shows the basic pipeline instantiation and execution pattern.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/outputs.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMPipeline\n\npipeline = DDIMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\noutputs = pipeline()\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset Using Python and Hugging Face Hub\nDESCRIPTION: This Python code snippet uses the Hugging Face Hub library to download a dataset locally. The dataset contains images for training and the specified local directory will store the dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/textual_inversion/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./cat\"\nsnapshot_download(\"diffusers/cat_toy_example\", local_dir=local_dir, repo_type=\"dataset\", ignore_patterns=\".gitattributes\")\n```\n\n----------------------------------------\n\nTITLE: Image Scaling and Background Preparation Function\nDESCRIPTION: Function to scale images to SDXL-compatible dimensions (1024x1024) and paste them onto a white background while preserving aspect ratio. Includes imports and utility functions for image processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/advanced_inference/outpaint.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nimport requests\nimport torch\nfrom controlnet_aux import ZoeDetector\nfrom PIL import Image, ImageOps\n\nfrom diffusers import (\n    AutoencoderKL,\n    ControlNetModel,\n    StableDiffusionXLControlNetPipeline,\n    StableDiffusionXLInpaintPipeline,\n)\n\ndef scale_and_paste(original_image):\n    aspect_ratio = original_image.width / original_image.height\n\n    if original_image.width > original_image.height:\n        new_width = 1024\n        new_height = round(new_width / aspect_ratio)\n    else:\n        new_height = 1024\n        new_width = round(new_height * aspect_ratio)\n\n    resized_original = original_image.resize((new_width, new_height), Image.LANCZOS)\n    white_background = Image.new(\"RGBA\", (1024, 1024), \"white\")\n    x = (1024 - new_width) // 2\n    y = (1024 - new_height) // 2\n    white_background.paste(resized_original, (x, y), resized_original)\n\n    return resized_original, white_background\n\noriginal_image = Image.open(\n    requests.get(\n        \"https://huggingface.co/datasets/stevhliu/testing-images/resolve/main/no-background-jordan.png\",\n        stream=True,\n    ).raw\n).convert(\"RGBA\")\nresized_img, white_bg_image = scale_and_paste(original_image)\n```\n\n----------------------------------------\n\nTITLE: Importing DPMSolverSinglestepScheduler in Python\nDESCRIPTION: This code snippet shows how to import the DPMSolverSinglestepScheduler class from the diffusers library. It is typically used in diffusion model pipelines for efficient sampling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/singlestep_dpm_solver.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DPMSolverSinglestepScheduler\n```\n\n----------------------------------------\n\nTITLE: Training with Accelerate and DreamBooth LoRA\nDESCRIPTION: Starts training of the DreamBooth LoRA SD3 model on a dataset. Key parameters include model name, data directory, output location, and various model training configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!accelerate launch train_dreambooth_lora_sd3_miniature.py \\\n  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-3-medium-diffusers\"  \\\n  --instance_data_dir=\"dog\" \\\n  --data_df_path=\"sample_embeddings.parquet\" \\\n  --output_dir=\"trained-sd3-lora-miniature\" \\\n  --mixed_precision=\"fp16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --seed=\"0\"\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Quanto and Accelerate in Shell\nDESCRIPTION: Installs the \"optimum-quanto\" and \"accelerate\" packages, required for using Quanto for model quantization in Python. It is necessary to have Python and pip installed to execute this command.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/quanto.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install optimum-quanto accelerate\n```\n\n----------------------------------------\n\nTITLE: Writing a Basic 🤗 Accelerate Configuration in Python\nDESCRIPTION: This python script uses the `write_basic_config` function from `accelerate.utils` to generate a basic accelerate configuration. This is useful for non-interactive environments such as notebooks where command-line configuration is not possible.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"from accelerate.utils import write_basic_config\n\nwrite_basic_config()\"\n```\n\n----------------------------------------\n\nTITLE: Importing ImagePipelineOutput in Python\nDESCRIPTION: This snippet shows how to import the ImagePipelineOutput class from the HuggingFace Diffusers library. The ImagePipelineOutput is used to structure the output of image generation pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ddim.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines import ImagePipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Loading Negative Embeddings in Python\nDESCRIPTION: Demonstrates loading negative textual inversion embeddings to discourage certain traits in generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline.load_textual_inversion(\n    \"sayakpaul/EasyNegative-test\", weight_name=\"EasyNegative.safetensors\", token=\"EasyNegative\"\n)\n\nprompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, EasyNegative\"\nnegative_prompt = \"EasyNegative\"\n\nimage = pipeline(prompt, negative_prompt=negative_prompt, num_inference_steps=50).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Using from_single_file with Quanto in Python\nDESCRIPTION: Provides an example of using the from_single_file method to load a quantization configuration from a single file. This script uses optimum-quanto and PyTorch, loading a checkpoint for float8 quantization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/quanto.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxTransformer2DModel, QuantoConfig\n\nckpt_path = \"https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/flux1-dev.safetensors\"\nquantization_config = QuantoConfig(weights_dtype=\"float8\")\ntransformer = FluxTransformer2DModel.from_single_file(ckpt_path, quantization_config=quantization_config, torch_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Displaying Diffusers Logo in Markdown\nDESCRIPTION: This snippet shows how to center-align and display the Diffusers library logo using HTML within a Markdown file. It sets the image width and source URL.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/index.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg\" width=\"400\"/>\n    <br>\n</p>\n```\n\n----------------------------------------\n\nTITLE: GLIGEN Pipeline Setup\nDESCRIPTION: Initializes the StableDiffusionGLIGENPipeline with all required components and moves it to CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/demo.ipynb#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipe = StableDiffusionGLIGENPipeline(\n    vae,\n    text_encoder,\n    tokenizer,\n    unet,\n    noise_scheduler,\n    safety_checker=None,\n    feature_extractor=None,\n)\npipe = pipe.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading Python Extensions\nDESCRIPTION: Initializes Python autoreload extension and imports the StableDiffusionGLIGENPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/demo.ipynb#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nfrom diffusers import StableDiffusionGLIGENPipeline\n```\n\n----------------------------------------\n\nTITLE: Cloning Hugging Face Diffusers Repository and Installing\nDESCRIPTION: This snippet outlines the commands to clone the Hugging Face Diffusers repository and install the library from source. Make sure to run these commands from your terminal to be able to use the training scripts effectively.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers with Flax using pip\nDESCRIPTION: Installs Diffusers with Flax support using pip package manager\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install diffusers[\"flax\"] transformers\n```\n\n----------------------------------------\n\nTITLE: Installing T-GATE Dependencies\nDESCRIPTION: Commands to install T-GATE and its required dependencies including torch, diffusers, transformers, and DeepCache.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/tgate.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install tgate\npip install -U torch diffusers transformers accelerate DeepCache\n```\n\n----------------------------------------\n\nTITLE: Image Encoding and Decoding with Remote VAE\nDESCRIPTION: Complete example demonstrating how to load an image, encode it using a remote VAE endpoint, and decode it back. Uses specific scaling and shift factors for the VAE transformation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_encode.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image\nfrom diffusers.utils.remote_utils import remote_decode\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg?download=true\")\n\nlatent = remote_encode(\n    endpoint=\"https://ptccx55jz97f9zgo.us-east-1.aws.endpoints.huggingface.cloud/\",\n    scaling_factor=0.3611,\n    shift_factor=0.1159,\n)\n\ndecoded = remote_decode(\n    endpoint=\"https://whhx50ex1aryqvw6.us-east-1.aws.endpoints.huggingface.cloud/\",\n    tensor=latent,\n    scaling_factor=0.3611,\n    shift_factor=0.1159,\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Dynamic Quantization\nDESCRIPTION: Applies dynamic int8 quantization to the filtered layers in UNet and VAE models\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom torchao import apply_dynamic_quant\n\napply_dynamic_quant(pipe.unet, dynamic_quant_filter_fn)\napply_dynamic_quant(pipe.vae, dynamic_quant_filter_fn)\n```\n\n----------------------------------------\n\nTITLE: Importing LatentConsistencyModelImg2ImgPipeline in Python\nDESCRIPTION: This code snippet shows the class definition for the LatentConsistencyModelImg2ImgPipeline, which is used for image-to-image generation with Latent Consistency Models. It includes methods for enabling and disabling various features like FreeU, VAE slicing, and VAE tiling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latent_consistency_models.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] LatentConsistencyModelImg2ImgPipeline\n    - all\n    - __call__\n    - enable_freeu\n    - disable_freeu\n    - enable_vae_slicing\n    - disable_vae_slicing\n    - enable_vae_tiling\n    - disable_vae_tiling\n```\n\n----------------------------------------\n\nTITLE: Installing xFormers via pip\nDESCRIPTION: Installs xFormers, a framework recommended for optimizing the speed and memory usage during inference and training of Diffusers models. Requires a compatible version of PyTorch, ideally the latest version.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/xformers.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install xformers\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for Bfloat16 Inference\nDESCRIPTION: This snippet involves setting up the environment to accelerate the inference of Stable Diffusion models with Bfloat16 using Intel Extension for PyTorch. Required dependencies include `diffusers`, `transformers`, `accelerate`, `scipy`, and `safetensors`. Environment variables are configured to optimize the utilization of Intel OpenMP and `jemalloc` for memory usage. Executes the inference with options for different scheduling in the scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install diffusers transformers accelerate scipy safetensors\n\nexport KMP_BLOCKTIME=1\nexport KMP_SETTINGS=1\nexport KMP_AFFINITY=granularity=fine,compact,1,0\n\n# Intel OpenMP\nexport OMP_NUM_THREADS=< Cores to use >\nexport LD_PRELOAD=${LD_PRELOAD}:/path/to/lib/libiomp5.so\n# Jemalloc is a recommended malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.\nexport LD_PRELOAD=${LD_PRELOAD}:/path/to/lib/libjemalloc.so\nexport MALLOC_CONF=\"oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:-1,muzzy_decay_ms:9000000000\"\n\n# Launch with default DDIM\nnumactl --membind <node N> -C <cpu list> python python inference_bf16.py\n# Launch with DPMSolverMultistepScheduler\nnumactl --membind <node N> -C <cpu list> python python inference_bf16.py --dpm\n```\n\n----------------------------------------\n\nTITLE: Quantizing and Using Allegro Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to load a quantized AllegroPipeline for inference using bitsandbytes. It includes setting up quantization configurations, loading the text encoder and transformer models, and generating a video from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/allegro.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, AllegroTransformer3DModel, AllegroPipeline\nfrom diffusers.utils import export_to_video\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"rhymes-ai/Allegro\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = AllegroTransformer3DModel.from_pretrained(\n    \"rhymes-ai/Allegro\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = AllegroPipeline.from_pretrained(\n    \"rhymes-ai/Allegro\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = (\n    \"A seaside harbor with bright sunlight and sparkling seawater, with many boats in the water. From an aerial view, \"\n    \"the boats vary in size and color, some moving and some stationary. Fishing boats in the water suggest that this \"\n    \"location might be a popular spot for docking fishing boats.\"\n)\nvideo = pipeline(prompt, guidance_scale=7.5, max_sequence_length=512).frames[0]\nexport_to_video(video, \"harbor.mp4\", fps=15)\n```\n\n----------------------------------------\n\nTITLE: Documentation HTML Badge\nDESCRIPTION: HTML code for displaying a LoRA badge with styling\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"flex flex-wrap space-x-1\">\n  <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Importing RePaintSchedulerOutput in Python\nDESCRIPTION: This code snippet shows how to import the RePaintSchedulerOutput class. It's inferred from the context of the documentation, as no explicit code is provided.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/repaint.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers.scheduling_repaint import RePaintSchedulerOutput\n```\n\n----------------------------------------\n\nTITLE: Installing Updated Diffusers and PyTorch\nDESCRIPTION: This Bash command upgrades the PyTorch and 🤗 Diffusers packages to ensure compatibility with the latest enhancements in PyTorch 2.0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade torch diffusers\n```\n\n----------------------------------------\n\nTITLE: Loading UNet2DModel Directly from a Repository in Diffusers\nDESCRIPTION: Shows how to load a UNet2DModel directly from a Hugging Face repository without specifying a subfolder. This approach loads the complete model from the google/ddpm-cifar10-32 repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/schedulers.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DModel\n\nunet = UNet2DModel.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for FLUX Training\nDESCRIPTION: Command to install specific dependencies needed for training ControlNet with FLUX.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements_flux.txt\n```\n\n----------------------------------------\n\nTITLE: Downloading Conditioning Images\nDESCRIPTION: Commands to download sample conditioning images for validation during training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```\n\n----------------------------------------\n\nTITLE: Syncing Fork with Upstream - Git Bash\nDESCRIPTION: Pulls the latest changes from the upstream main branch to keep the local fork updated with the original repository's modifications.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n$ git pull upstream main\n```\n\n----------------------------------------\n\nTITLE: Resuming DreamBooth Training from a Checkpoint\nDESCRIPTION: Command parameter to resume training from a specific saved checkpoint, which allows for continuation of interrupted training or experimentation with different hyperparameters from a midpoint.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n  --resume_from_checkpoint=\"checkpoint-1500\"\n```\n\n----------------------------------------\n\nTITLE: HTML Badge Component\nDESCRIPTION: HTML markup for displaying a LoRA badge in the documentation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/text2img.md#2025-04-11_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"flex flex-wrap space-x-1\">\n  <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Scheduler and Model for Diffusion Posterior Sampling in Python\nDESCRIPTION: Demonstrates how to initialize the DDPM scheduler and load the pre-trained UNet2DModel for use in the Diffusion Posterior Sampling Pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_86\n\nLANGUAGE: python\nCODE:\n```\n# set up scheduler\nscheduler = DDPMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\nscheduler.set_timesteps(1000)\n\n# set up model\nmodel = UNet2DModel.from_pretrained(\"google/ddpm-celebahq-256\").to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Content\nDESCRIPTION: Markdown content describing the Paint by Example model, including paper abstract, implementation details, and usage tips.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/paint_by_example.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# Paint by Example\n\n[Paint by Example: Exemplar-based Image Editing with Diffusion Models](https://huggingface.co/papers/2211.13227) is by Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen.\n\nThe abstract from the paper is:\n\n*Language-guided image editing has achieved great success recently. In this paper, for the first time, we investigate exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.*\n```\n\n----------------------------------------\n\nTITLE: Updating Scheduler Config for IF Model LoRA Fine-tuning\nDESCRIPTION: This Python code demonstrates how to load a DeepFloyd IF model with LoRA weights and update the scheduler configuration to use fixed variance. This step is necessary when working with fine-tuned IF models, as the training scripts only train the predicted error component.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\")\n\npipe.load_lora_weights(\"<lora weights path>\")\n\n# Update scheduler config to fixed variance schedule\npipe.scheduler = pipe.scheduler.__class__.from_config(pipe.scheduler.config, variance_type=\"fixed_small\")\n```\n\n----------------------------------------\n\nTITLE: Creating Image Preprocessing Pipeline with torchvision Transforms\nDESCRIPTION: Defines a preprocessing pipeline using torchvision transforms that resizes images, applies data augmentation, and normalizes pixel values to the [-1, 1] range required by the diffusion model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchvision import transforms\n\n>>> preprocess = transforms.Compose(\n...     [\n...         transforms.Resize((config.image_size, config.image_size)),\n...         transforms.RandomHorizontalFlip(),\n...         transforms.ToTensor(),\n...         transforms.Normalize([0.5], [0.5]),\n...     ]\n... )\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Markdown structure defining the documentation sections for DDPM, including model description, pipeline documentation, and API references\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ddpm.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# DDPM\n\n[Denoising Diffusion Probabilistic Models](https://huggingface.co/papers/2006.11239) (DDPM) by Jonathan Ho, Ajay Jain and Pieter Abbeel proposes a diffusion based model of the same name. In the 🤗 Diffusers library, DDPM refers to the *discrete denoising scheduler* from the paper as well as the pipeline.\n\n# DDPMPipeline\n[[autodoc]] DDPMPipeline\n\t- all\n\t- __call__\n\n## ImagePipelineOutput\n[[autodoc]] pipelines.ImagePipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained PyTorch DreamBooth Model\nDESCRIPTION: Code for running inference with a trained DreamBooth model using PyTorch. Loads the trained pipeline, generates an image with a prompt containing the learned concept, and saves the result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"path_to_saved_model\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\nimage = pipeline(\"A photo of sks dog in a bucket\", num_inference_steps=50, guidance_scale=7.5).images[0]\nimage.save(\"dog-bucket.png\")\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Generation with Kandinsky 3\nDESCRIPTION: Performs image-to-image generation using Kandinsky 3 model, incorporating the prompt, negative prompt, and original image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimage = pipeline(prompt, negative_prompt=negative_prompt, image=image, strength=0.75, num_inference_steps=25).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Dreambooth for Inpainting Model\nDESCRIPTION: This script sets the environment for training Dreambooth using a pre-trained stable-diffusion-inpainting model. Necessary directories for instance data and output should be specified. The script runs with a specific instance prompt and customized parameters including resolution, learning rate, and maximum training steps. No dependencies are explicitly listed but `accelerate` must be installed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/dreambooth_inpaint/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"runwayml/stable-diffusion-inpainting\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth_inpaint.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=400\n```\n\n----------------------------------------\n\nTITLE: Markdown Tip Component\nDESCRIPTION: User tip about exploring scheduler options and pipeline component reuse\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/cogview4.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-a-pipeline) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n```\n\n----------------------------------------\n\nTITLE: Image Grid Creation\nDESCRIPTION: Creates a grid layout of the generated images for display.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/demo.ipynb#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndiffusers.utils.make_image_grid(images, 4, len(images) // 4)\n```\n\n----------------------------------------\n\nTITLE: Data Loading and Processing Helpers\nDESCRIPTION: Helper functions for data handling including repeating data samples and processing batches\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef repeat_data(data: Data, num_repeat) -> Batch:\n    datas = [copy.deepcopy(data) for i in range(num_repeat)]\n    return Batch.from_data_list(datas)\n\ndef repeat_batch(batch: Batch, num_repeat) -> Batch:\n    datas = batch.to_data_list()\n    new_data = []\n    for i in range(num_repeat):\n        new_data += copy.deepcopy(datas)\n    return Batch.from_data_list(new_data)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Training Script\nDESCRIPTION: This snippet includes commands necessary to navigate to the relevant examples directory and install the specific dependencies required for the SDXL training script. This step is crucial for ensuring that all necessary packages are available.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/text_to_image\npip install -r requirements_sdxl.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Steps to install Diffusers library from source code to ensure compatibility with example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Instructions for cloning the Diffusers repository and installing it from source to ensure compatibility with the latest example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/model_search/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Inference with Fine-tuned Kandinsky2.2 Decoder\nDESCRIPTION: Python code to load and use a fine-tuned Kandinsky2.2 decoder model for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(output_dir, torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt='A robot naruto, 4k photo'\nimages = pipe(prompt=prompt).images\nimages[0].save(\"robot-naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing UNet3DConditionModel in Python\nDESCRIPTION: This snippet shows how to import the UNet3DConditionModel class from the Hugging Face Diffusers library. It is used to create a 3D conditional UNet model for diffusion processes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet3d-cond.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet3DConditionModel\n```\n\n----------------------------------------\n\nTITLE: Configuring Safety Levels in Safe Stable Diffusion\nDESCRIPTION: Demonstrates how to use different safety configurations (WEAK, MEDIUM, STRONG, MAX) when generating images with the Safe Stable Diffusion pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_safe.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import StableDiffusionPipelineSafe\n>>> from diffusers.pipelines.stable_diffusion_safe import SafetyConfig\n\n>>> pipeline = StableDiffusionPipelineSafe.from_pretrained(\"AIML-TUDA/stable-diffusion-safe\")\n>>> prompt = \"the four horsewomen of the apocalypse, painting by tom of finland, gaston bussiere, craig mullins, j. c. leyendecker\"\n>>> out = pipeline(prompt=prompt, **SafetyConfig.MAX)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for SDXL Fine-tuning\nDESCRIPTION: Commands to clone the diffusers repository, install it in development mode, and set up required dependencies for SDXL fine-tuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Fine-tuning with Prior-Preservation Loss in Flax\nDESCRIPTION: Command to run DreamBooth training with prior-preservation loss using Flax, which helps prevent overfitting while still creating personalized generations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\npython train_dreambooth_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-6 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Installing requirements for LCM training\nDESCRIPTION: Command to install the necessary dependencies listed in the requirements.txt file for the Latent Consistency Distillation training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing xDiT using pip\nDESCRIPTION: This code snippet provides the command to install the xDiT library using pip, a package manager for Python. It enables users to access xDiT functionalities easily.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/xdit.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install xfuser\n```\n\n----------------------------------------\n\nTITLE: Setup Training Configuration Environment Variables in Bash\nDESCRIPTION: This Bash code snippet sets environment variables needed for configuring the InstructPix2Pix model training, including model ID, dataset ID, and training epochs. This setup is necessary before executing a training command.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/instructpix2pix_lora/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_ID=\"timbrooks/instruct-pix2pix\"\nDATASET_ID=\"instruction-tuning-sd/cartoonization\"\nTRAIN_EPOCHS=100\n```\n\n----------------------------------------\n\nTITLE: Kandinsky 3 HTML Badge Implementation\nDESCRIPTION: HTML code snippet for displaying the Kandinsky 3 LoRA badge with styling\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"flex flex-wrap space-x-1\">\n  <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Commands to install the required huggingface_hub package and login to access the model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface_hub --upgrade\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet lists the required Python packages for the Hugging Face Diffusers project. It includes specific version requirements for some packages and lists essential libraries for machine learning, data processing, and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\ndatasets\nftfy\ntensorboard\n```\n\n----------------------------------------\n\nTITLE: Generating Images with People from Multiple Source Images using OmniGen in Python\nDESCRIPTION: This code demonstrates using OmniGen to generate a new image of a man and woman sitting at a classroom desk, where the identities are preserved from two separate input images. It loads the OmniGen pipeline, processes multiple input images, and generates a new composite scene.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/omnigen.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenPipeline\nfrom diffusers.utils import load_image \n\npipe = OmniGenPipeline.from_pretrained(\n    \"Shitao/OmniGen-v1-diffusers\",\n    torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\n\nprompt=\"A man and a woman are sitting at a classroom desk. The man is the man with yellow hair in <img><|image_1|></img>. The woman is the woman on the left of <img><|image_2|></img>\"\ninput_image_1 = load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/3.png\")\ninput_image_2 = load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/4.png\")\ninput_images=[input_image_1, input_image_2]\nimage = pipe(\n    prompt=prompt, \n    input_images=input_images, \n    height=1024,\n    width=1024,\n    guidance_scale=2.5, \n    img_guidance_scale=1.6,\n    generator=torch.Generator(device=\"cpu\").manual_seed(666)\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Launching LoRA Fine-Tuning for Stable Diffusion\nDESCRIPTION: This Bash script runs the fine-tuning process for the Stable Diffusion model with LoRA using the Accelerate library. It sets various parameters like model path, dataset, resolution, epochs, and learning rate, and logs progress with Weights and Biases.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/lora/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=512 --random_flip \\\n  --train_batch_size=1 \\\n  --num_train_epochs=100 --checkpointing_steps=5000 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --seed=42 \\\n  --output_dir=\"sd-naruto-model-lora\" \\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\"\n  --use_peft \\\n  --lora_r=4 --lora_alpha=32 \\\n  --lora_text_encoder_r=4 --lora_text_encoder_alpha=32\n```\n\n----------------------------------------\n\nTITLE: SDXL Reference Pipeline\nDESCRIPTION: Configuration and usage of Stable Diffusion XL Reference pipeline for generating images while maintaining reference image characteristics.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers.schedulers import UniPCMultistepScheduler\n\nfrom .stable_diffusion_xl_reference import StableDiffusionXLReferencePipeline\n\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl_reference_input_cat.jpg\")\n\npipe = StableDiffusionXLReferencePipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\").to('cuda:0')\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\nresult_img = pipe(ref_image=input_image,\n      prompt=\"a dog\",\n      num_inference_steps=20,\n      reference_attn=True,\n      reference_adain=True).images[0]\n```\n\n----------------------------------------\n\nTITLE: Importing IPAdapterMixin in Python\nDESCRIPTION: This snippet shows the import statement for the IPAdapterMixin class from the loaders.ip_adapter module.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/loaders/ip_adapter.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom loaders.ip_adapter import IPAdapterMixin\n```\n\n----------------------------------------\n\nTITLE: Environment Setup for LoRA Training\nDESCRIPTION: Sets up environment variables for training configuration including model name, instance directory, and output directory paths.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Downsized Images for IF Stage II Validation\nDESCRIPTION: This Python script downloads a downsized version of training images to be used for validation during Stage II upscaling in DeepFloyd IF model training. The downsized images serve as inputs to the upscaler model during validation steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog_downsized\"\nsnapshot_download(\n    \"diffusers/dog-example-downsized\",\n    local_dir=local_dir,\n    repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Training InstructPix2Pix with SD-1.5/SDXL-0.9\nDESCRIPTION: Accelerate launch script for training InstructPix2Pix using either SD-1.5 or SDXL-0.9 as the base model. The script includes configuration for model training parameters, dataset settings, and validation specifications. It uses the fusing/instructpix2pix-1000-samples dataset with specific hyperparameters for training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README_sdxl.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\" or \"stabilityai/stable-diffusion-xl-base-0.9\"\nexport DATASET_ID=\"fusing/instructpix2pix-1000-samples\"\n\naccelerate launch train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --use_ema \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=512 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --seed=42 \\\n    --val_image_url=\"https://datasets-server.huggingface.co/assets/fusing/instructpix2pix-1000-samples/--/fusing--instructpix2pix-1000-samples/train/23/input_image/image.jpg\" \\\n    --validation_prompt=\"make it in Japan\" \\\n    --report_to=wandb \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Hub Authentication\nDESCRIPTION: Python code to login to Hugging Face Hub using an access token.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import login\n\nlogin()\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion RePaint Pipeline in Python\nDESCRIPTION: This code demonstrates the use of the RePaint logic with Stable Diffusion for image inpainting. It uses a custom RePaintScheduler and can work with models not specifically created for inpainting. The example downloads an image and a mask, then applies the inpainting process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\nimport requests\nimport torch\nfrom io import BytesIO\nfrom diffusers import StableDiffusionPipeline, RePaintScheduler\n\ndef download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\ninit_image = download_image(img_url).resize((512, 512))\nmask_image = download_image(mask_url).resize((512, 512))\nmask_image = PIL.ImageOps.invert(mask_image)\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16, custom_pipeline=\"stable_diffusion_repaint\",\n)\npipe.scheduler = RePaintScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\nprompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\nimage = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading Base and Mask Images for Inpainting\nDESCRIPTION: Loads the base image to be inpainted and the corresponding mask image that defines the areas to be inpainted.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for DreamBooth Training\nDESCRIPTION: Commands to clone the Diffusers repository, install it in editable mode, and install additional requirements for SANA DreamBooth training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sana.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\ncd examples/dreambooth\npip install -r requirements_sana.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring PAG Applied Layers for Image Generation in Python\nDESCRIPTION: This code demonstrates how to configure the layers where PAG is applied in the pipeline. It shows an example of generating images with different PAG layer configurations, allowing users to find the optimal settings for their specific use case.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/pag.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"an insect robot preparing a delicious meal, anime style\"\npipeline.set_pag_applied_layers(pag_layers)\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimages = pipeline(\n    prompt=prompt,\n    num_inference_steps=25,\n    guidance_scale=guidance_scale,\n    generator=generator,\n    pag_scale=pag_scale,\n).images\nimages[0]\n```\n\n----------------------------------------\n\nTITLE: Using Accelerated Transformers in Diffusers\nDESCRIPTION: Python code demonstrating how to use the accelerated transformer implementation in Diffusers with PyTorch 2.0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Norm Clipping Utility for Vectors in Python\nDESCRIPTION: A utility function that clips vector norms to a maximum value while preserving direction. This is commonly used in gradient-based methods to enhance training stability.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef clip_norm(vec, limit, p=2):\n    norm = torch.norm(vec, dim=-1, p=2, keepdim=True)\n    denom = torch.where(norm > limit, limit / norm, torch.ones_like(norm))\n    return vec * denom\n```\n\n----------------------------------------\n\nTITLE: Launching Textual Inversion Training in PyTorch\nDESCRIPTION: Complete bash command to launch Textual Inversion training using PyTorch. This script trains a model to learn embeddings for a cat toy concept, with parameters for model path, dataset location, training batch size, learning rate, and other hyperparameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport DATA_DIR=\"./cat\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" \\\n  --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 \\\n  --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"textual_inversion_cat\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Stable Diffusion with ONNXRuntime\nDESCRIPTION: This command fine-tunes a Stable Diffusion model on the Naruto dataset using ONNXRuntime for accelerated training. It sets environment variables for the model name and dataset name, and then launches the `train_text_to_image.py` script with various hyperparameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/text_to_image/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n\"export MODEL_NAME=\\\"CompVis/stable-diffusion-v1-4\\\"\nexport dataset_name=\\\"lambdalabs/naruto-blip-captions\\\"\naccelerate launch --mixed_precision=\\\"fp16\\\"  train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$dataset_name \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\\\"constant\\\" --lr_warmup_steps=0 \\\n  --output_dir=\\\"sd-naruto-model\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Pushing Private Model to Hub\nDESCRIPTION: Demonstrates pushing a model as a private repository to HuggingFace Hub for restricted access.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/push_to_hub.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncontrolnet.push_to_hub(\"my-controlnet-model-private\", private=True)\n```\n\n----------------------------------------\n\nTITLE: Lower Precision Video Generation with Mochi Pipeline\nDESCRIPTION: Demonstrates using bfloat16 precision for reduced memory usage (22GB VRAM) with slightly lower quality output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/mochi.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import MochiPipeline\nfrom diffusers.utils import export_to_video\n\npipe = MochiPipeline.from_pretrained(\"genmo/mochi-1-preview\", variant=\"bf16\", torch_dtype=torch.bfloat16)\n\n# Enable memory savings\npipe.enable_model_cpu_offload()\npipe.enable_vae_tiling()\n\nprompt = \"Close-up of a chameleon's eye, with its scaly skin changing color. Ultra high resolution 4k.\"\nframes = pipe(prompt, num_frames=85).frames[0]\n\nexport_to_video(frames, \"mochi.mp4\", fps=30)\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward Pass for One-Step UNet Pipeline\nDESCRIPTION: Complete implementation of the custom pipeline including the forward pass method that performs a single step of diffusion using the UNet and scheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nclass UnetSchedulerOneForwardPipeline(DiffusionPipeline):\n    def __init__(self, unet, scheduler):\n        super().__init__()\n\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    def __call__(self):\n        image = torch.randn(\n            (1, self.unet.config.in_channels, self.unet.config.sample_size, self.unet.config.sample_size),\n        )\n        timestep = 1\n\n        model_output = self.unet(image, timestep).sample\n        scheduler_output = self.scheduler.step(model_output, timestep, image).prev_sample\n\n        return scheduler_output\n```\n\n----------------------------------------\n\nTITLE: Listing Core Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet lists the required Python packages and their minimum versions needed to run the Hugging Face Diffusers library. It includes transformers for model definitions, accelerate for optimization, safetensors for secure tensor storage, datasets for data handling, torchvision for image processing, ftfy for text cleaning, and tensorboard and wandb for experiment tracking and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\ntransformers>=4.25.1\naccelerate>=0.16.0\nsafetensors\ndatasets\ntorchvision\nftfy\ntensorboard\nwandb\n```\n\n----------------------------------------\n\nTITLE: Image-to-Video Generation with CogVideoX in Python\nDESCRIPTION: Complete example for generating video from an image and text prompt using CogVideoX. Shows how to load the image-to-video pipeline, optimize VAE memory usage with tiling and slicing, set generation parameters, and export the result to an MP4 file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/cogvideox.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import CogVideoXImageToVideoPipeline\nfrom diffusers.utils import export_to_video, load_image\n\nprompt = \"A vast, shimmering ocean flows gracefully under a twilight sky, its waves undulating in a mesmerizing dance of blues and greens. The surface glints with the last rays of the setting sun, casting golden highlights that ripple across the water. Seagulls soar above, their cries blending with the gentle roar of the waves. The horizon stretches infinitely, where the ocean meets the sky in a seamless blend of hues. Close-ups reveal the intricate patterns of the waves, capturing the fluidity and dynamic beauty of the sea in motion.\"\nimage = load_image(image=\"cogvideox_rocket.png\")\npipe = CogVideoXImageToVideoPipeline.from_pretrained(\n    \"THUDM/CogVideoX-5b-I2V\",\n    torch_dtype=torch.bfloat16\n)\n \npipe.vae.enable_tiling()\npipe.vae.enable_slicing()\n\nvideo = pipe(\n    prompt=prompt,\n    image=image,\n    num_videos_per_prompt=1,\n    num_inference_steps=50,\n    num_frames=49,\n    guidance_scale=6,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\n\nexport_to_video(video, \"output.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Quantizing LTX Video Model with BitsAndBytes in Python\nDESCRIPTION: Demonstrates how to load a quantized LTX Video model using BitsAndBytes for 8-bit quantization and run inference to generate a video from a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ltx_video.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, LTXVideoTransformer3DModel, LTXPipeline\nfrom diffusers.utils import export_to_video\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"Lightricks/LTX-Video\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = LTXVideoTransformer3DModel.from_pretrained(\n    \"Lightricks/LTX-Video\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = LTXPipeline.from_pretrained(\n    \"Lightricks/LTX-Video\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting.\"\nvideo = pipeline(prompt=prompt, num_frames=161, num_inference_steps=50).frames[0]\nexport_to_video(video, \"ship.mp4\", fps=24)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for RL Diffusion Models\nDESCRIPTION: Commands to install required Python packages for running the diffusion-based RL examples. Includes PyTorch, MuJoCo, Gym, D4RL, and other dependencies needed for both Diffusion Policy and Diffuser implementations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/reinforcement_learning/README.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -f https://download.pytorch.org/whl/torch_stable.html \\\n                free-mujoco-py \\\n                einops \\\n                gym==0.24.1 \\\n                protobuf==3.20.1 \\\n                git+https://github.com/rail-berkeley/d4rl.git \\\n                mediapy \\\n                Pillow==9.0.0\n```\n\n----------------------------------------\n\nTITLE: Hugging Face CLI Login\nDESCRIPTION: Command to login to Hugging Face to access gated CogView4 model\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogview4-control/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Package Requirements List for Diffusers\nDESCRIPTION: Core dependency list for the Diffusers project, including deep learning frameworks and utility libraries. Lists essential packages like PyTorch, torchvision, transformers, and supporting packages for text processing and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/colossalai/requirement.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndiffusers\ntorch\ntorchvision\nftfy\ntensorboard\nJinja2\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Benchmarking ControlNet with Stable Diffusion\nDESCRIPTION: The script benchmarks the ControlNet integration with Stable Diffusion pipelines, utilizing `torch.compile` to achieve significant performance improvements in detailed image generation based on control inputs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.utils import load_image\nimport torch\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\ninit_image = load_image(url)\ninit_image = init_image.resize((512, 512))\n\npath = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    path, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n)\n\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\npipe.controlnet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe.controlnet = torch.compile(pipe.controlnet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers with PyTorch using UV\nDESCRIPTION: Installs Diffusers with PyTorch support using the UV package manager\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv install diffusers[\"torch\"] transformers\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Diffusers Library\nDESCRIPTION: This bash script clones the diffusers repository from GitHub, navigates into the repository directory, and installs the library using pip.  This is a prerequisite for using the training scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"git clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\"\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for Custom Diffusion\nDESCRIPTION: Commands for cloning the diffusers repository and installing it in development mode to ensure compatibility with the latest example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Dependencies\nDESCRIPTION: Command to install diffusers and related packages for using the model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -q diffusers accelerate transformers\n```\n\n----------------------------------------\n\nTITLE: HTML Comment for Copyright and License Information\nDESCRIPTION: This HTML comment block contains copyright information for The HuggingFace Team and the Apache License 2.0 terms under which the code is distributed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky.md#2025-04-11_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Null-Text Inversion Pipeline for Image Editing\nDESCRIPTION: Implementation of null-text inversion pipeline for editing real images with DDIM reconstruction. Supports optimization and reconstruction with or without null-text optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_95\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDIMScheduler\nfrom examples.community.pipeline_null_text_inversion import NullTextPipeline\nimport torch\n\ndevice = \"cuda\"\ninvert_prompt = \"A lying cat\"\ninput_image = \"siamese.jpg\"\nsteps = 50\nprompt = \"A lying dog\"\n\nmodel_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nscheduler = DDIMScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.0120, beta_schedule=\"scaled_linear\")\npipeline = NullTextPipeline.from_pretrained(model_path, scheduler=scheduler, torch_dtype=torch.float32).to(device)\n\ninverted_latent, uncond = pipeline.invert(input_image, invert_prompt, num_inner_steps=10, early_stop_epsilon=1e-5, num_inference_steps=steps)\npipeline(prompt, uncond, inverted_latent, guidance_scale=7.5, num_inference_steps=steps).images[0].save(input_image+\".output.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Downsized Validation Images for Stage 2 Training\nDESCRIPTION: This code shows how to download downsized validation images that are required for training the stage 2 upscaler model of DeepFloyd IF.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog_downsized\"\nsnapshot_download(\n    \"diffusers/dog-example-downsized\",\n    local_dir=local_dir,\n    repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Training Amused 256 with Full Finetuning\nDESCRIPTION: Script for full finetuning of Amused-256 model on nouns dataset with fp16 mixed precision and gradient checkpointing. Uses batch size 8 and learning rate 1e-4, targeting 750-1000 steps for decent results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --train_batch_size <batch size> \\\n    --gradient_accumulation_steps <gradient accumulation steps> \\\n    --learning_rate 1e-4 \\\n    --pretrained_model_name_or_path amused/amused-256 \\\n    --instance_data_dataset  'm1guelpf/nouns' \\\n    --image_key image \\\n    --prompt_key text \\\n    --resolution 256 \\\n    --mixed_precision fp16 \\\n    --lr_scheduler constant \\\n    --validation_prompts \\\n        'a pixel art character with square red glasses, a baseball-shaped head and a orange-colored body on a dark background' \\\n        'a pixel art character with square orange glasses, a lips-shaped head and a red-colored body on a light background' \\\n        'a pixel art character with square blue glasses, a microwave-shaped head and a purple-colored body on a sunny background' \\\n        'a pixel art character with square red glasses, a baseball-shaped head and a blue-colored body on an orange background' \\\n        'a pixel art character with square red glasses' \\\n        'a pixel art character' \\\n        'square red glasses on a pixel art character' \\\n        'square red glasses on a pixel art character with a baseball-shaped head' \\\n    --max_train_steps 10000 \\\n    --checkpointing_steps 500 \\\n    --validation_steps 250 \\\n    --gradient_checkpointing\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training with Mixed Precision\nDESCRIPTION: Advanced command example for training with multiple GPUs using Accelerate, including FP16 mixed precision, wandb logging, EMA model averaging, and other optimized settings for the Naruto dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu train_unconditional.py \\\n  --dataset_name=\"lambdalabs/naruto-blip-captions\" \\\n  --resolution=64 --center_crop --random_flip \\\n  --output_dir=\"ddpm-ema-naruto-64\" \\\n  --train_batch_size=16 \\\n  --num_epochs=100 \\\n  --gradient_accumulation_steps=1 \\\n  --use_ema \\\n  --learning_rate=1e-4 \\\n  --lr_warmup_steps=500 \\\n  --mixed_precision=\"fp16\" \\\n  --logger=\"wandb\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Mixed Precision - Bash\nDESCRIPTION: This snippet demonstrates the command to launch the training script with mixed precision enabled, which can improve training performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_lcm_distill_sd_wds.py \\n  --mixed_precision=\"fp16\"\n```\n\n----------------------------------------\n\nTITLE: Installing Watermarker Package for SDXL\nDESCRIPTION: Installation command for the invisible watermark library which is recommended for adding non-visible watermarks to images generated with SDXL.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install invisible-watermark>=0.2.0\n```\n\n----------------------------------------\n\nTITLE: Loading Diffusion Pipeline from DDUF Format\nDESCRIPTION: Demonstrates how to load a diffusion pipeline from the experimental DDUF format using the from_pretrained method with the dduf_file parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"DDUF/FLUX.1-dev-DDUF\", dduf_file=\"FLUX.1-dev.dduf\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\nimage = pipe(\n    \"photo a cat holding a sign that says Diffusers\", num_inference_steps=50, guidance_scale=3.5\n).images[0]\nimage.save(\"cat.png\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Interpolation Parameters\nDESCRIPTION: Sets up the images/texts to interpolate between and their corresponding weights for the interpolation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimages_texts = [\"a cat\", img_1, img_2]\nweights = [0.3, 0.3, 0.4]\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Pipeline with Custom Scheduler for LoRA\nDESCRIPTION: Code to load a Stable Diffusion pipeline with a custom scheduler for use with LoRA models, disabling the safety checker and enabling GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, UniPCMultistepScheduler\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"andite/anything-v4.0\", torch_dtype=torch.float16, safety_checker=None\n).to(\"cuda\")\npipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub\nDESCRIPTION: This command authenticates the user with the Hugging Face Hub, allowing access to models and datasets that require authentication.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/text_to_image/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n\"huggingface-cli login\"\n```\n\n----------------------------------------\n\nTITLE: GLIGEN Model Components Initialization\nDESCRIPTION: Loads and initializes various components required for the GLIGEN model including tokenizer, schedulers, text encoder, and VAE from a pretrained checkpoint.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/demo.ipynb#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nimport diffusers\nfrom diffusers import (\n    AutoencoderKL,\n    DDPMScheduler,\n    EulerDiscreteScheduler,\n    UNet2DConditionModel,\n)\n\n\n# pretrained_model_name_or_path = 'masterful/gligen-1-4-generation-text-box'\n\npretrained_model_name_or_path = \"/root/data/zhizhonghuang/checkpoints/models--masterful--gligen-1-4-generation-text-box/snapshots/d2820dc1e9ba6ca082051ce79cfd3eb468ae2c83\"\n\ntokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\nnoise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\ntext_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\")\nvae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n# unet = UNet2DConditionModel.from_pretrained(\n#     pretrained_model_name_or_path, subfolder=\"unet\"\n# )\n\nnoise_scheduler = EulerDiscreteScheduler.from_config(noise_scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: Installing NGLView\nDESCRIPTION: A shell command to install NGLView, a library for visualizing 3D molecular structures, which will be rendered later using its functionalities.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n!pip install nglview\n```\n\n----------------------------------------\n\nTITLE: Installing T2I-Adapter Dependencies\nDESCRIPTION: This command navigates to the t2i_adapter example directory and installs the required dependencies from the requirements.txt file. These dependencies are necessary to run the training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"cd examples/t2i_adapter\npip install -r requirements.txt\"\n```\n\n----------------------------------------\n\nTITLE: Installing Weights and Biases\nDESCRIPTION: Command to install Weights and Biases for experiment tracking and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install wandb\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Model in Flax\nDESCRIPTION: Initialize Stable Diffusion pipeline with bfloat16 precision for TPU optimization\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndtype = jnp.bfloat16\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    variant=\"bf16\",\n    dtype=dtype,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Processing 3D Molecular Structures\nDESCRIPTION: Sets up storage arrays for generated and original molecules, then processes each result by reshaping 3D position data and setting positions in RDKit molecule objects.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# init storage objects\nmols_gen = []\nmols_orig = []\nfor to_process in results:\n    # store the reference 3d position\n    to_process[\"pos_ref\"] = to_process[\"pos_ref\"].reshape(-1, to_process[\"rdmol\"].GetNumAtoms(), 3)\n\n    # store the generated 3d position\n    to_process[\"pos_gen\"] = to_process[\"pos_gen\"].reshape(-1, to_process[\"rdmol\"].GetNumAtoms(), 3)\n\n    # copy data to new object\n    new_mol = set_rdmol_positions(to_process.rdmol, to_process[\"pos_gen\"][0])\n\n    # append results\n    mols_gen.append(new_mol)\n    mols_orig.append(to_process.rdmol)\n\nprint(f\"collect {len(mols_gen)} generated molecules in `mols`\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Pre-trained Diffusion Pipeline\nDESCRIPTION: This snippet demonstrates how to load a pre-trained Stable Diffusion model using DiffusionPipeline. It downloads and caches all modeling, tokenization, and scheduling components needed for text-to-image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import DiffusionPipeline\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Customizing Training Resolution\nDESCRIPTION: Example of modifying the training script's image resolution parameter during the launch command, demonstrating flexibility in training configuration\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_custom_diffusion.py \\\n  --resolution=256\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Command\nDESCRIPTION: Command to train InstructPix2Pix model using multiple GPUs with distributed training setup.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/instructpix2pix.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu train_instruct_pix2pix.py \\\n --pretrained_model_name_or_path=stable-diffusion-v1-5/stable-diffusion-v1-5 \\\n --dataset_name=sayakpaul/instructpix2pix-1000-samples \\\n --use_ema \\\n --enable_xformers_memory_efficient_attention \\\n --resolution=512 --random_flip \\\n --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n --max_train_steps=15000 \\\n --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n --learning_rate=5e-05 --lr_warmup_steps=0 \\\n --conditioning_dropout_prob=0.05 \\\n --mixed_precision=fp16 \\\n --seed=42 \\\n --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face CLI for Checkpoint Conversion\nDESCRIPTION: Command to login to the Hugging Face CLI to enable pushing converted models to the hub and opening pull requests.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice HTML Comment\nDESCRIPTION: Copyright and license notice for the HuggingFace Team documentation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm2.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: UNet Model Loading\nDESCRIPTION: Loads the UNet model from a specific checkpoint path.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/demo.ipynb#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nunet = UNet2DConditionModel.from_pretrained(\"/root/data/zhizhonghuang/ckpt/GLIGEN_Text_Retrain_COCO\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Prior-Preservation Training in PyTorch\nDESCRIPTION: Commands to set environment variables for DreamBooth training with prior-preservation loss, including class image directory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"path_to_training_images\"\nexport CLASS_DIR=\"path_to_class_images\"\nexport OUTPUT_DIR=\"path_to_saved_model\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Pipeline Script\nDESCRIPTION: Command to download the pipeline_easy.py script from the auto_diffusers repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/model_search/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n!wget https://raw.githubusercontent.com/suzukimain/auto_diffusers/refs/heads/master/src/auto_diffusers/pipeline_easy.py\n```\n\n----------------------------------------\n\nTITLE: Setting Up StableDiffusionPipeline with Textual Inversion\nDESCRIPTION: Initializes the StableDiffusionPipeline and loads a textual inversion model for the 'midjourney-style' concept. This enables using the textual inversion concept in prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n  \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n  torch_dtype=torch.float16,\n).to(\"cuda\")\npipe.load_textual_inversion(\"sd-concepts-library/midjourney-style\")\n```\n\n----------------------------------------\n\nTITLE: Logging in to Hugging Face Hub for Model Sharing\nDESCRIPTION: Command for logging into the Hugging Face Hub to enable saving and sharing models with the community.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/lora.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Using Bit Diffusion for Discrete Data Generation\nDESCRIPTION: This code demonstrates how to use Bit Diffusion, a method for applying diffusion models to discrete data like discrete image data or DNA sequences. The implementation loads a pretrained DDPM model for CIFAR-10 and generates an unconditional discrete image using the bit diffusion custom pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"google/ddpm-cifar10-32\", custom_pipeline=\"bit_diffusion\")\nimage = pipe().images[0]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Batched Generation Function\nDESCRIPTION: Creates a helper function to generate inputs for batch processing of multiple images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/stable_diffusion.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_inputs(batch_size=1):\n    generator = [torch.Generator(\"cuda\").manual_seed(i) for i in range(batch_size)]\n    prompts = batch_size * [prompt]\n    num_inference_steps = 20\n\n    return {\"prompt\": prompts, \"generator\": generator, \"num_inference_steps\": num_inference_steps}\n```\n\n----------------------------------------\n\nTITLE: Running ControlNet Training with Accelerate CLI Command\nDESCRIPTION: Bash command for training ControlNet using Accelerate with DeepSpeed. Includes memory optimization techniques like gradient checkpointing, xformers efficient attention, and mixed-precision training with fp16.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_controlnet.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --gradient_checkpointing \\\n --enable_xformers_memory_efficient_attention \\\n --set_grads_to_none \\\n --mixed_precision fp16\n```\n\n----------------------------------------\n\nTITLE: Freezing Parameters in Text Encoder for Custom Diffusion\nDESCRIPTION: Freezes specific parameters in the text encoder to prevent them from being updated during training. This includes the encoder layers, final layer norm, and position embeddings, while keeping token embeddings trainable to learn the new concept associations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparams_to_freeze = itertools.chain(\n    text_encoder.text_model.encoder.parameters(),\n    text_encoder.text_model.final_layer_norm.parameters(),\n    text_encoder.text_model.embeddings.position_embedding.parameters(),\n)\nfreeze_params(params_to_freeze)\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment\nDESCRIPTION: Command to initialize an Accelerate environment for distributed training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Disabling Watermarker in SDXL Pipeline\nDESCRIPTION: Code snippet showing how to disable the watermarker when initializing an SDXL pipeline if different regulations for generating or safely distributing images apply.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe = StableDiffusionXLPipeline.from_pretrained(..., add_watermarker=False)\n```\n\n----------------------------------------\n\nTITLE: Downloading 3D Icon Dataset from Hugging Face Hub\nDESCRIPTION: Downloads a 3D icon dataset from the Hugging Face Hub to a local directory using the snapshot_download function, ignoring Git attributes files.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./3d_icon\"\nsnapshot_download(\n    \"LinoyTsaban/3d_icon\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: License Header in Markdown\nDESCRIPTION: Apache 2.0 license header for the DiT implementation documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/dit.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Generating and Decoding Images Remotely - Python\nDESCRIPTION: This code snippet imports necessary libraries, initializes a Stable Diffusion Img2Img pipeline, encodes an input image, generates a new image based on the provided prompt, and then decodes the image using a remote endpoint. It requires the Hugging Face Diffusers library and PyTorch, as well as network access to specific model endpoints.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_encode.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom diffusers.utils import load_image\nfrom diffusers.utils.remote_utils import remote_decode, remote_encode\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    vae=None,\n).to(\"cuda\")\n\ninit_image = load_image(\n    \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n)\ninit_image = init_image.resize((768, 512))\n\ninit_latent = remote_encode(\n    endpoint=\"https://qc6479g0aac6qwy9.us-east-1.aws.endpoints.huggingface.cloud/\",\n    image=init_image,\n    scaling_factor=0.18215,\n)\n\nprompt = \"A fantasy landscape, trending on artstation\"\nlatent = pipe(\n    prompt=prompt,\n    image=init_latent,\n    strength=0.75,\n    output_type=\"latent\",\n).images\n\nimage = remote_decode(\n    endpoint=\"https://q1bj3bpq6kzilnsu.us-east-1.aws.endpoints.huggingface.cloud/\",\n    tensor=latent,\n    scaling_factor=0.18215,\n)\nimage.save(\"fantasy_landscape.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Training SDXL Turbo with DPO\nDESCRIPTION: Command for training SDXL Turbo with Diffusion DPO using LoRA. Similar to SDXL but includes Turbo-specific settings and 512 resolution parameter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/diffusion_dpo/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_diffusion_dpo_sdxl.py \\\n  --pretrained_model_name_or_path=stabilityai/sdxl-turbo \\\n  --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\n  --output_dir=\"diffusion-sdxl-turbo-dpo\" \\\n  --mixed_precision=\"fp16\" \\\n  --dataset_name=kashif/pickascore \\\n  --train_batch_size=8 \\\n  --gradient_accumulation_steps=2 \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --rank=8 \\\n  --learning_rate=1e-5 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=2000 \\\n  --checkpointing_steps=500 \\\n  --run_validation --validation_steps=50 \\\n  --seed=\"0\" \\\n  --report_to=\"wandb\" \\\n  --is_turbo --resolution 512 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Installing Conda in Python Environment\nDESCRIPTION: This snippet demonstrates how to install Condacolab and setup Conda environments for handling complex dependencies, especially useful for geometric networks in PyTorch.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install -q condacolab\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport condacolab\n\ncondacolab.install()\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for ControlNet Training\nDESCRIPTION: Commands to clone the diffusers repository and install it in development mode to ensure compatibility with the latest example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Decoder Model Prediction Step\nDESCRIPTION: Performs the forward pass in the UNet model during decoder training. The model takes noisy latents and timesteps to predict the noise residual, focusing on the first 4 channels of the output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel_pred = unet(noisy_latents, timesteps, None, added_cond_kwargs=added_cond_kwargs).sample[:, :4]\n```\n\n----------------------------------------\n\nTITLE: Saving a Generated Image\nDESCRIPTION: This snippet shows how to save a generated image to disk as a PNG file using the PIL Image save method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> image.save(\"image_of_squirrel_painting.png\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Pipeline Outputs as Tuple\nDESCRIPTION: Demonstrates how to access pipeline outputs using tuple indexing, which returns only non-None attributes. Shows accessing the first element of the output tuple.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/outputs.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noutputs[:1]\n```\n\n----------------------------------------\n\nTITLE: Importing ScoreSdeVeScheduler in Python\nDESCRIPTION: This code snippet demonstrates how to import the ScoreSdeVeScheduler class. The scheduler is used for variance exploding stochastic differential equation (SDE) scheduling in generative modeling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/score_sde_ve.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import ScoreSdeVeScheduler\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for Diffusers Project\nDESCRIPTION: A requirements list specifying the necessary Python packages for the Hugging Face Diffusers library. It includes core libraries like accelerate, transformers, and torchvision with version constraints, as well as utilities like ftfy, tensorboard, and Jinja2.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\npeft==0.7.0\n```\n\n----------------------------------------\n\nTITLE: Model Component Extraction\nDESCRIPTION: Script to extract and save image projection and IP adapter components from a trained checkpoint.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/ip_adapter/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom safetensors.torch import load_file, save_file\n\n# Load the trained model checkpoint in safetensors format\nckpt = \"checkpoint-50000/pytorch_model.safetensors\"\nsd = load_file(ckpt)  # Using safetensors load function\n\n# Extract image projection and IP adapter components\nimage_proj_sd = {}\nip_sd = {}\n\nfor k in sd:\n    if k.startswith(\"unet\"):\n        pass  # Skip unet-related keys\n    elif k.startswith(\"image_proj_model\"):\n        image_proj_sd[k.replace(\"image_proj_model.\", \"\")] = sd[k]\n    elif k.startswith(\"adapter_modules\"):\n        ip_sd[k.replace(\"adapter_modules.\", \"\")] = sd[k]\n\n# Save the components into separate safetensors files\nsave_file(image_proj_sd, \"image_proj.safetensors\")\nsave_file(ip_sd, \"ip_adapter.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for JAX/Flax Stable Diffusion\nDESCRIPTION: Installs the required Python packages for running Stable Diffusion with JAX/Flax.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install jax==0.3.25 jaxlib==0.3.25 flax transformers ftfy\n!pip install diffusers\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Diffusers\nDESCRIPTION: This code installs the necessary libraries (diffusers, accelerate, and transformers) to work with diffusion models. The code is commented out and should be uncommented when running in environments like Google Colab.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install --upgrade diffusers accelerate transformers\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset for Textual Inversion (Python)\nDESCRIPTION: Python code to download a sample dataset (cat toy images) from Hugging Face for use in textual inversion training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/README.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./cat\"\nsnapshot_download(\"diffusers/cat_toy_example\", local_dir=local_dir, repo_type=\"dataset\", ignore_patterns=\".gitattributes\")\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Commands to clone the Diffusers repository, install it from source, and set up the training environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\ncd example_folder\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Committing Changes - Git Bash\nDESCRIPTION: Stages modified files and commits changes to the local repository, recording the developer's work with a descriptive message.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ git add modified_file.py\n$ git commit -m \"A descriptive message about your changes.\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Conditioning Images for ControlNet Training\nDESCRIPTION: Bash commands to download test conditioning images used in the training examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sd3.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\n\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```\n\n----------------------------------------\n\nTITLE: Setting Checkpoint Saving Frequency\nDESCRIPTION: Command line argument to save training checkpoints at regular intervals (every 500 steps in this example) to prevent training loss in case of interruptions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n--checkpointing_steps=500\n```\n\n----------------------------------------\n\nTITLE: Adjusting Embedding Weight in AUTOMATIC1111/SD.Next Prompt\nDESCRIPTION: Demonstrates how to increase the weight of a textual embedding in an AUTOMATIC1111 or SD.Next prompt. The embedding token is wrapped in parentheses with a weight value.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\n(y2k_emb:1.2)\n```\n\n----------------------------------------\n\nTITLE: Defining Conditioning Image Transforms in PyTorch\nDESCRIPTION: This python code defines a series of image transforms using `torchvision.transforms`. The transforms resize the image, perform a center crop, and convert the image to a tensor. These transforms are applied to the conditioning image during dataset preprocessing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"conditioning_image_transforms = transforms.Compose(\n    [\n        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n        transforms.CenterCrop(args.resolution),\n        transforms.ToTensor(),\n    ]\n)\"\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet lists the required Python packages for the Hugging Face Diffusers project. It specifies accelerate version 0.16.0 or higher, torchvision, and datasets as dependencies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ndatasets\n```\n\n----------------------------------------\n\nTITLE: Training Unconditional Image Generation on Oxford Flowers Dataset\nDESCRIPTION: Complete command to launch training of a DDPM model with EMA on the Oxford Flowers dataset, with specific parameters for image resolution, batch size, learning rate, and other training configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_unconditional.py \\\n  --dataset_name=\"huggan/flowers-102-categories\" \\\n  --resolution=64 \\\n  --output_dir=\"ddpm-ema-flowers-64\" \\\n  --train_batch_size=16 \\\n  --num_epochs=100 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-4 \\\n  --lr_warmup_steps=500 \\\n  --mixed_precision=no \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Launching DreamBooth Training with Flax in Terminal\nDESCRIPTION: Command-line instructions for starting DreamBooth training using Flax/JAX, specifying model paths, instance data, output directory, and key training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport INSTANCE_DIR=\"./dog\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\npython train_dreambooth_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-6 \\\n  --max_train_steps=400 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment\nDESCRIPTION: Command to initialize the Accelerate environment for distributed training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Importing LMSDiscreteSchedulerOutput in Python\nDESCRIPTION: This code snippet shows how to import the LMSDiscreteSchedulerOutput class from the Hugging Face Diffusers library. It's inferred from the context of the documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/lms_discrete.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers.scheduling_lms_discrete import LMSDiscreteSchedulerOutput\n```\n\n----------------------------------------\n\nTITLE: Using Negative Crop Conditioning in SDXL\nDESCRIPTION: This code demonstrates how to use negative crop conditioning in SDXL to steer generation away from specific cropping parameters. It combines negative settings for original size, crop coordinates, and target size in a single generation request.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(\n    prompt=prompt,\n    negative_original_size=(512, 512),\n    negative_crops_coords_top_left=(0, 0),\n    negative_target_size=(1024, 1024),\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Low-Memory Inference\nDESCRIPTION: Command to install the bitsandbytes library required for 8-bit precision loading of the text encoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart_sigma.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U bitsandbytes\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Stable Diffusion Image-to-Image\nDESCRIPTION: Python code for benchmarking Stable Diffusion image-to-image generation with PyTorch 2.0 optimizations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom diffusers import StableDiffusionImg2ImgPipeline\nimport requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((512, 512))\n\npath = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(path, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: Running DreamBooth Training for SD3\nDESCRIPTION: Bash script to run the DreamBooth training for Stable Diffusion 3, including setting environment variables and configuring training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-3-medium-diffusers\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"trained-sd3\"\n\naccelerate launch train_dreambooth_sd3.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"fp16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: PushToHubMixin Class Documentation\nDESCRIPTION: Documentation for the PushToHubMixin utility class that enables pushing models to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/overview.md#2025-04-11_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## PushToHubMixin\\n\\n[[autodoc]] utils.PushToHubMixin\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Commands to clone the Diffusers repository, install it in editable mode, and install additional requirements for the example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\ncd example_folder\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Enabling DoRA Training\nDESCRIPTION: Command line flag to enable DoRA training in the training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n--use_dora\n```\n\n----------------------------------------\n\nTITLE: Setting up InstructPix2Pix for Video Editing\nDESCRIPTION: This snippet demonstrates how to set up the StableDiffusionInstructPix2PixPipeline with CrossFrameAttnProcessor for text-based video editing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\nfrom diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\npipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\"timbrooks/instruct-pix2pix\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))\n```\n\n----------------------------------------\n\nTITLE: Loading Pipeline with Hub Configuration\nDESCRIPTION: Demonstrates loading a pipeline while explicitly specifying the configuration from a Hub repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\n\nckpt_path = \"https://huggingface.co/segmind/SSD-1B/blob/main/SSD-1B.safetensors\"\nrepo_id = \"segmind/SSD-1B\"\n\npipeline = StableDiffusionXLPipeline.from_single_file(ckpt_path, config=repo_id)\n```\n\n----------------------------------------\n\nTITLE: Importing FlaxUNet2DConditionOutput in Python\nDESCRIPTION: This snippet demonstrates importing the FlaxUNet2DConditionOutput class, which represents the output of the FlaxUNet2DConditionModel. This class is specific to the Flax implementation and structures the model's output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet2d-cond.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.models.unets.unet_2d_condition_flax import FlaxUNet2DConditionOutput\n```\n\n----------------------------------------\n\nTITLE: Resuming Training from a Checkpoint\nDESCRIPTION: Command line argument to resume training from a previously saved checkpoint, allowing continuation of interrupted training sessions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n--resume_from_checkpoint=\"checkpoint-1500\"\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Commands to clone the Diffusers repository, install the library from source, and set up training dependencies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\ncd example_folder\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Core dependencies required for the Diffusers library, including ML frameworks (transformers, torch, flax), image processing libraries (torchvision), and utility packages (ftfy, tensorboard, Jinja2). The transformers package requires version 4.25.1 or higher.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/requirements_flax.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers>=4.25.1\nflax\noptax\ntorch\ntorchvision\nftfy\ntensorboard\nJinja2\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with ControlNet and Kandinsky 2.2\nDESCRIPTION: Sets up the Kandinsky 2.2 prior and ControlNet pipelines, generates image embeddings from a prompt, and creates an image using the ControlNet pipeline with a depth map hint.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n\nprior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\npipeline = KandinskyV22ControlnetPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n).to(\"cuda\")\n\nprompt = \"A robot, 4k photo\"\nnegative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(43)\n\nimage_emb, zero_image_emb = prior_pipeline(\n    prompt=prompt, negative_prompt=negative_prior_prompt, generator=generator\n).to_tuple()\n\nimage = pipeline(image_embeds=image_emb, negative_image_embeds=zero_image_emb, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for T2I-Adapter\nDESCRIPTION: Command to install the necessary Python libraries (diffusers, accelerate, and controlnet-aux) for using T2I-Adapter with Stable Diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers accelerate controlnet-aux==0.0.7\n```\n\n----------------------------------------\n\nTITLE: Dependency Requirements Configuration for Diffusers\nDESCRIPTION: Specifies the core Python package dependencies needed to run the Diffusers project. Includes specific version requirements for transformers, accelerate, and peft packages, along with general requirements for wandb, torch, and torchvision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogview4-control/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers==4.47.0\nwandb\ntorch\ntorchvision\naccelerate==1.2.0\npeft>=0.14.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Environment Variables\nDESCRIPTION: Sets up essential environment variables for model training, including paths for pretrained models, training data, and output directories\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport pretrained_model_name_or_path='flux-dev-model-path'\nexport MODEL_TYPE='train_model_type'\nexport TRAIN_JSON_FILE=\"your_json_file\"\nexport CONTROL_TYPE='control_preprocessor_type'\nexport CAPTION_COLUMN='caption_column'\n\nexport CACHE_DIR=\"/data/train_csr/.cache/huggingface/\"\nexport OUTPUT_DIR='/data/train_csr/FLUX/MODEL_OUT/'$MODEL_TYPE\n```\n\n----------------------------------------\n\nTITLE: Setting Offline Mode Environment Variable\nDESCRIPTION: Configures Diffusers to run in offline mode using cached files only\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_HUB_OFFLINE=1\n```\n\n----------------------------------------\n\nTITLE: Creating DreamBooth Training Dataset and DataLoader\nDESCRIPTION: Sets up the training dataset using DreamBoothDataset class with instance and class data paths, prompts, and tokenizer. Creates a DataLoader with appropriate batch size, shuffling, and collation function.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataset = DreamBoothDataset(\n    instance_data_root=args.instance_data_dir,\n    instance_prompt=args.instance_prompt,\n    class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n    class_prompt=args.class_prompt,\n    class_num=args.num_class_images,\n    tokenizer=tokenizer,\n    size=args.resolution,\n    center_crop=args.center_crop,\n    encoder_hidden_states=pre_computed_encoder_hidden_states,\n    class_prompt_encoder_hidden_states=pre_computed_class_prompt_encoder_hidden_states,\n    tokenizer_max_length=args.tokenizer_max_length,\n)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=args.train_batch_size,\n    shuffle=True,\n    collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\n    num_workers=args.dataloader_num_workers,\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Diffusers CLI Environment Information in Bash\nDESCRIPTION: This snippet shows the output of the diffusers-cli command, which displays version information for various Python packages and libraries used in the Diffusers project, as well as system and GPU information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/tests/quantization/bnb/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n- 🤗 Diffusers version: 0.31.0.dev0\n- Platform: Linux-5.15.0-117-generic-x86_64-with-glibc2.35\n- Running on Google Colab?: No\n- Python version: 3.10.12\n- PyTorch version (GPU?): 2.5.0.dev20240818+cu124 (True)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Huggingface_hub version: 0.24.5\n- Transformers version: 4.44.2\n- Accelerate version: 0.34.0.dev0\n- PEFT version: 0.12.0\n- Bitsandbytes version: 0.43.3\n- Safetensors version: 0.4.4\n- xFormers version: not installed\n- Accelerator: NVIDIA GeForce RTX 4090, 24564 MiB\nNVIDIA GeForce RTX 4090, 24564 MiB\n- Using GPU in script?: Yes\n```\n\n----------------------------------------\n\nTITLE: Training Custom Diffusion for Human Faces\nDESCRIPTION: Training command optimized for human faces with adjusted hyperparameters like reduced learning rate, more training steps, and freezing cross-attention layers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport OUTPUT_DIR=\"path-to-save-model\"\nexport INSTANCE_DIR=\"path-to-images\"\n\naccelerate launch train_custom_diffusion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --class_data_dir=./real_reg/samples_person/ \\\n  --with_prior_preservation --real_prior --prior_loss_weight=1.0 \\\n  --class_prompt=\"person\" --num_class_images=200 \\\n  --instance_prompt=\"photo of a <new1> person\"  \\\n  --resolution=512  \\\n  --train_batch_size=2  \\\n  --learning_rate=5e-6  \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=1000 \\\n  --scale_lr --hflip --noaug \\\n  --freeze_model crossattn \\\n  --modifier_token \"<new1>\" \\\n  --enable_xformers_memory_efficient_attention \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Config (Bash)\nDESCRIPTION: This command initializes the 🤗 Accelerate environment. It prompts the user to configure the training setup based on their hardware and environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Enabling TensorFloat-32 for PyTorch matrix multiplications\nDESCRIPTION: Enables TensorFloat-32 (tf32) mode for matrix multiplications in PyTorch, which can significantly speed up computations with minimal loss in numerical accuracy on Ampere and later CUDA devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/fp16.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Copyright notice and Apache 2.0 license information for the HuggingFace Team\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ddpm.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Importing SchedulerOutput in Python\nDESCRIPTION: This code snippet demonstrates how to import the SchedulerOutput class from the diffusers library. SchedulerOutput is used to structure the output of scheduler steps in diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/singlestep_dpm_solver.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers.scheduling_utils import SchedulerOutput\n```\n\n----------------------------------------\n\nTITLE: Pushing ControlNet Model to Hub\nDESCRIPTION: Creates and pushes a ControlNet model configuration to HuggingFace Hub with custom architecture parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/push_to_hub.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import ControlNetModel\n\ncontrolnet = ControlNetModel(\n    block_out_channels=(32, 64),\n    layers_per_block=2,\n    in_channels=4,\n    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n    cross_attention_dim=32,\n    conditioning_embedding_out_channels=(16, 32),\n)\ncontrolnet.push_to_hub(\"my-controlnet-model\")\n```\n\n----------------------------------------\n\nTITLE: Importing LatteTransformer3DModel in Markdown\nDESCRIPTION: This code snippet demonstrates how to include autodoc-generated documentation for the LatteTransformer3DModel class in a Markdown file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/latte_transformer3d.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[[autodoc]] LatteTransformer3DModel\n```\n\n----------------------------------------\n\nTITLE: Launching Textual Inversion Training (Bash)\nDESCRIPTION: Bash command to start the textual inversion training process using Accelerate. It sets various parameters such as model name, data directory, and training hyperparameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport DATA_DIR=\"./cat\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" \\\n  --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 \\\n  --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --push_to_hub \\\n  --output_dir=\"textual_inversion_cat\"\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Commands to clone the diffusers repository and install it from source to ensure compatibility with the latest example scripts and features.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Importing SchedulerOutput in Python\nDESCRIPTION: This code snippet shows how to import the SchedulerOutput class from the scheduling_utils module in the HuggingFace Diffusers library. It's inferred from the documentation structure.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/dpm_discrete_ancestral.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers.scheduling_utils import SchedulerOutput\n```\n\n----------------------------------------\n\nTITLE: Running Specific Tests\nDESCRIPTION: Command to run specific test files during development\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest tests/<TEST_TO_RUN>.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Various methods to configure the Accelerate environment for training, including interactive, default, and programmatic approaches.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Using AsymmetricAutoencoderKL with StableDiffusionInpaintPipeline in Python\nDESCRIPTION: This snippet demonstrates how to use the AsymmetricAutoencoderKL model with the StableDiffusionInpaintPipeline for image inpainting. It loads the necessary models, prepares input images, and generates an inpainted image based on a given prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/asymmetricautoencoderkl.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AsymmetricAutoencoderKL, StableDiffusionInpaintPipeline\nfrom diffusers.utils import load_image, make_image_grid\n\n\nprompt = \"a photo of a person with beard\"\nimg_url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/repaint/celeba_hq_256.png\"\nmask_url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/repaint/mask_256.png\"\n\noriginal_image = load_image(img_url).resize((512, 512))\nmask_image = load_image(mask_url).resize((512, 512))\n\npipe = StableDiffusionInpaintPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\")\npipe.vae = AsymmetricAutoencoderKL.from_pretrained(\"cross-attention/asymmetric-autoencoder-kl-x-1-5\")\npipe.to(\"cuda\")\n\nimage = pipe(prompt=prompt, image=original_image, mask_image=mask_image).images[0]\nmake_image_grid([original_image, mask_image, image], rows=1, cols=3)\n```\n\n----------------------------------------\n\nTITLE: Importing CogView3PipelineOutput in Python\nDESCRIPTION: This snippet demonstrates how to import the CogView3PipelineOutput class from the diffusers library. This class represents the output of the CogView3Plus pipeline, which includes the generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/cogview3.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.cogview3.pipeline_output import CogView3PipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Chunk-based Long Video Generation\nDESCRIPTION: Shows how to generate longer videos by processing them in chunks. This approach helps manage memory usage while maintaining temporal consistency across the entire video.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video_zero.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import TextToVideoZeroPipeline\nimport numpy as np\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\nseed = 0\nvideo_length = 24  #24 ÷ 4fps = 6 seconds\nchunk_size = 8\nprompt = \"A panda is playing guitar on times square\"\n\n# Generate the video chunk-by-chunk\nresult = []\nchunk_ids = np.arange(0, video_length, chunk_size - 1)\ngenerator = torch.Generator(device=\"cuda\")\nfor i in range(len(chunk_ids)):\n    print(f\"Processing chunk {i + 1} / {len(chunk_ids)}\")\n    ch_start = chunk_ids[i]\n    ch_end = video_length if i == len(chunk_ids) - 1 else chunk_ids[i + 1]\n    # Attach the first frame for Cross Frame Attention\n    frame_ids = [0] + list(range(ch_start, ch_end))\n    # Fix the seed for the temporal consistency\n    generator.manual_seed(seed)\n    output = pipe(prompt=prompt, video_length=len(frame_ids), generator=generator, frame_ids=frame_ids)\n    result.append(output.images[1:])\n\n# Concatenate chunks and save\nresult = np.concatenate(result)\nresult = [(r * 255).astype(\"uint8\") for r in result]\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```\n\n----------------------------------------\n\nTITLE: Updating Diffusers Installation\nDESCRIPTION: Updates a local installation of Diffusers to the latest version\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/diffusers/\ngit pull\n```\n\n----------------------------------------\n\nTITLE: Installing Training Dependencies\nDESCRIPTION: Command to install additional requirements for training from the requirements.txt file in the example folder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/vqgan/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Unittest\nDESCRIPTION: Commands to run tests using unittest for both main tests and examples\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m unittest discover -s tests -t . -v\n$ python -m unittest discover -s examples -t examples -v\n```\n\n----------------------------------------\n\nTITLE: Displaying NVIDIA GPU Information using nvidia-smi\nDESCRIPTION: This bash command shows the output of nvidia-smi, providing details about the NVIDIA H100 80GB HBM3 GPU used for benchmarking, including its memory usage, temperature, and power consumption.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/tests/quantization/torchao/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------|\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA H100 80GB HBM3          On  | 00000000:53:00.0 Off |                    0 |\n| N/A   34C    P0              69W / 700W |      2MiB / 81559MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Account\nDESCRIPTION: Command to log into Hugging Face account for automatic model upload after training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for DiffEdit\nDESCRIPTION: Installation commands for the necessary libraries to use DiffEdit functionality in a Colab environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate\n```\n\n----------------------------------------\n\nTITLE: Configuring 🤗Accelerate\nDESCRIPTION: This command initializes an 🤗Accelerate environment, which is used for distributed training and mixed precision support.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/text_to_image/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate config\"\n```\n\n----------------------------------------\n\nTITLE: Loading Image Datasets with Datasets Library\nDESCRIPTION: These Python snippets demonstrate how to load an image dataset using the `load_dataset` function from the `datasets` library. The snippets show loading datasets from local folders, local files (archives), remote files (archives), and providing splits by mapping specific files.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/create_dataset.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"from datasets import load_dataset\n\n# example 1: local folder\ndataset = load_dataset(\"imagefolder\", data_dir=\"path_to_your_folder\")\n\n# example 2: local files (supported formats are tar, gzip, zip, xz, rar, zstd)\ndataset = load_dataset(\"imagefolder\", data_files=\"path_to_zip_file\")\n\n# example 3: remote files (supported formats are tar, gzip, zip, xz, rar, zstd)\ndataset = load_dataset(\n    \"imagefolder\",\n    data_files=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\",\n)\n\n# example 4: providing several splits\ndataset = load_dataset(\n    \"imagefolder\", data_files={\"train\": [\"path/to/file1\", \"path/to/file2\"], \"test\": [\"path/to/file3\", \"path/to/file4\"]}\n)\"\n```\n\n----------------------------------------\n\nTITLE: CLIP-based Inference Implementation\nDESCRIPTION: Complete inference implementation using CLIP model and trained IP Adapter components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/ip_adapter/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom safetensors.torch import load_file\nfrom transformers import CLIPProcessor, CLIPModel  # Using the Hugging Face CLIP model \n\n# Load model components from safetensors\nimage_proj_ckpt = \"image_proj.safetensors\"\nip_adapter_ckpt = \"ip_adapter.safetensors\"\n\n# Load the saved weights\nimage_proj_sd = load_file(image_proj_ckpt)\nip_adapter_sd = load_file(ip_adapter_ckpt)\n\n# Define the model Parameters\nclass ImageProjectionModel(torch.nn.Module):\n    def __init__(self, input_dim=768, output_dim=512):  # CLIP's default embedding size is 768\n        super().__init__()\n        self.model = torch.nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass IPAdapterModel(torch.nn.Module):\n    def __init__(self, input_dim=512, output_dim=10):  # Example for 10 classes\n        super().__init__()\n        self.model = torch.nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.model(x)\n\n# Initialize models\nimage_proj_model = ImageProjectionModel()\nip_adapter_model = IPAdapterModel()\n\n# Load weights into models\nimage_proj_model.load_state_dict(image_proj_sd)\nip_adapter_model.load_state_dict(ip_adapter_sd)\n\n# Set models to evaluation mode\nimage_proj_model.eval()\nip_adapter_model.eval()\n\n#Inference pipeline\ndef inference(image_tensor):\n    \"\"\"\n    Run inference using the loaded models.\n\n    Args:\n        image_tensor: Preprocessed image tensor from CLIPProcessor\n\n    Returns:\n        Final inference results\n    \"\"\"\n    with torch.no_grad():\n        # Step 1: Project the image features\n        image_proj = image_proj_model(image_tensor)\n\n        # Step 2: Pass the projected features through the IP Adapter\n        result = ip_adapter_model(image_proj)\n\n    return result\n\n# Using CLIP for image preprocessing\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n#Image file path\nimage_path = \"path/to/image.jpg\"\n\n# Preprocess the image\ninputs = processor(images=image_path, return_tensors=\"pt\")\nimage_features = clip_model.get_image_features(inputs[\"pixel_values\"])\n\n# Normalize the image features as per CLIP's recommendations\nimage_features = image_features / image_features.norm(dim=-1, keepdim=True)\n\n# Run inference\noutput = inference(image_features)\nprint(\"Inference output:\", output)\n```\n\n----------------------------------------\n\nTITLE: Clearing GPU Memory After Text Encoding\nDESCRIPTION: Removes text encoders and related components from GPU memory after prompt encoding to make space for the diffusion transformer. Uses garbage collection and CUDA memory management functions to ensure memory is properly released.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport gc \n\ndef flush():\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.reset_peak_memory_stats()\n\ndel pipeline.text_encoder\ndel pipeline.text_encoder_2\ndel pipeline.tokenizer\ndel pipeline.tokenizer_2\ndel pipeline\n\nflush()\n```\n\n----------------------------------------\n\nTITLE: Syncing with Upstream\nDESCRIPTION: Command to sync local repository with upstream main branch\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ git pull upstream main\n```\n\n----------------------------------------\n\nTITLE: Importing RePaintScheduler in Python\nDESCRIPTION: This code snippet demonstrates how to import the RePaintScheduler class. It's inferred from the context of the documentation, as no explicit code is provided.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/repaint.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import RePaintScheduler\n```\n\n----------------------------------------\n\nTITLE: Setting Up Prompt and Generation Parameters\nDESCRIPTION: Creates a prompt incorporating the textual inversion concept using the special placeholder token <cat-toy> and defines the grid layout parameters for the generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/textual_inversion_inference.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a grafitti in a favela wall with a <cat-toy> on it\"\n\nnum_samples_per_row = 2\nnum_rows = 2\n```\n\n----------------------------------------\n\nTITLE: Importing PixArtTransformer2DModel in Python\nDESCRIPTION: This code snippet demonstrates how to import the PixArtTransformer2DModel class. It is inferred from the autodoc directive in the markdown, which suggests that the model can be directly imported and used in Python code.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/pixart_transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import PixArtTransformer2DModel\n```\n\n----------------------------------------\n\nTITLE: Styledrop Fine-tuning AmuSED at 256 Resolution\nDESCRIPTION: This script implements the Styledrop method for fine-tuning AmuSED at 256 resolution. It uses a learning rate of 4e-4 and expects decent results in 1500-2000 steps. The script includes various training parameters and validation prompts for style transfer.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --mixed_precision fp16 \\\n    --report_to wandb \\\n    --use_lora \\\n    --pretrained_model_name_or_path amused/amused-256 \\\n    --train_batch_size 1 \\\n    --lr_scheduler constant \\\n    --learning_rate 4e-4 \\\n    --validation_prompts \\\n        'A chihuahua walking on the street in [V] style' \\\n        'A banana on the table in [V] style' \\\n        'A church on the street in [V] style' \\\n        'A tabby cat walking in the forest in [V] style' \\\n    --instance_data_image 'A mushroom in [V] style.png' \\\n    --max_train_steps 10000 \\\n    --checkpointing_steps 500 \\\n    --validation_steps 100 \\\n    --resolution 256\n```\n\n----------------------------------------\n\nTITLE: Dataset and Caption Column Arguments\nDESCRIPTION: These command-line arguments specify the dataset name and the column name in the metadata file that contains the caption for each image. They are used during the training process to load and process the dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_23\n\nLANGUAGE: text\nCODE:\n```\n--dataset_name=my_dataset/\n--caption_column=prompt\n```\n\n----------------------------------------\n\nTITLE: Importing DPMSolverMultistepScheduler in Python\nDESCRIPTION: This code snippet shows how to import the DPMSolverMultistepScheduler class from the diffusers library. It is inferred from the context of the documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/multistep_dpm_solver.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DPMSolverMultistepScheduler\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: This snippet lists the required Python packages and their specific versions for the Hugging Face Diffusers project. These dependencies are crucial for ensuring the correct installation and operation of the Diffusers library. The versions specified ensure compatibility and prevent potential conflicts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/realfill/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"diffusers==0.20.1\naccelerate==0.23.0\ntransformers==4.38.0\npeft==0.5.0\ntorch==2.2.0\ntorchvision>=0.16\nftfy==6.1.1\ntensorboard==2.14.0\nJinja2==3.1.6\"\n```\n\n----------------------------------------\n\nTITLE: 12GB GPU Optimized Training\nDESCRIPTION: Training configuration optimized for 12GB GPUs using additional memory optimizations including xformers and gradient optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_controlnet.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --gradient_checkpointing \\\n --use_8bit_adam \\\n --enable_xformers_memory_efficient_attention \\\n --set_grads_to_none\n```\n\n----------------------------------------\n\nTITLE: Processing and Extracting Depth Map for Kandinsky ControlNet\nDESCRIPTION: Sets up the environment and processes an initial cat image to extract a depth map using the Transformers depth-estimation pipeline. The depth map will be used as a control hint for the Kandinsky image-to-image generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\nfrom diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\nfrom diffusers.utils import load_image\nfrom transformers import pipeline\n\nimg = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png\"\n).resize((768, 768))\n\ndef make_hint(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    hint = detected_map.permute(2, 0, 1)\n    return hint\n\ndepth_estimator = pipeline(\"depth-estimation\")\nhint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Graph Field Network Transformation for Equivariant Diffusion in Python\nDESCRIPTION: Implements the roto-translational equivariant transformation required for diffusion models operating on 3D molecular structures. This ensures the model's predictions are invariant to rotation and translation of the molecule.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef graph_field_network(score_d, pos, edge_index, edge_length):\n    \"\"\"\n    Transformation to make the epsilon predicted from the diffusion model roto-translational equivariant. See equations\n    5-7 of the GeoDiff Paper https://arxiv.org/pdf/2203.02923.pdf\n    \"\"\"\n    N = pos.size(0)\n    dd_dr = (1.0 / edge_length) * (pos[edge_index[0]] - pos[edge_index[1]])  # (E, 3)\n    score_pos = scatter_add(dd_dr * score_d, edge_index[0], dim=0, dim_size=N) + scatter_add(\n        -dd_dr * score_d, edge_index[1], dim=0, dim_size=N\n    )  # (N, 3)\n    return score_pos\n```\n\n----------------------------------------\n\nTITLE: Verifying JAX TPU Installation\nDESCRIPTION: Python code to verify JAX installation by checking available TPU cores.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.device_count()\n```\n\n----------------------------------------\n\nTITLE: Training Amused 512 with Full Finetuning\nDESCRIPTION: Script for full finetuning of Amused-512 model on Minecraft dataset. Uses batch size 8 and learning rate 8e-5, achieving results in 500-1000 steps. Requires more memory due to higher resolution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --train_batch_size <batch size> \\\n    --gradient_accumulation_steps <gradient accumulation steps> \\\n    --learning_rate 8e-5 \\\n    --pretrained_model_name_or_path amused/amused-512 \\\n    --instance_data_dataset  'monadical-labs/minecraft-preview' \\\n    --prompt_prefix 'minecraft ' \\\n    --image_key image \\\n    --prompt_key text \\\n    --resolution 512 \\\n    --mixed_precision fp16 \\\n    --lr_scheduler constant \\\n    --validation_prompts \\\n        'minecraft Avatar' \\\n        'minecraft character' \\\n        'minecraft' \\\n        'minecraft president' \\\n        'minecraft pig' \\\n    --max_train_steps 10000 \\\n    --checkpointing_steps 500 \\\n    --validation_steps 250 \\\n    --gradient_checkpointing\n```\n\n----------------------------------------\n\nTITLE: Dependency Requirements List for Hugging Face Diffusers\nDESCRIPTION: A list of Python package dependencies required for the Diffusers library. It includes transformers for model support, torch and flax for deep learning backends, and various utility packages for data handling and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/requirements_flax.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers>=4.25.1\ndatasets\nflax\noptax\ntorch\ntorchvision\nftfy\ntensorboard\nJinja2\n```\n\n----------------------------------------\n\nTITLE: Launching Textual Inversion Training with ONNXRuntime via Bash\nDESCRIPTION: This bash command launches the textual inversion training using ONNXRuntime to accelerate the process. The script uses various parameters to configure training, such as model name, data directory, learning rate, and output directory. It requires setting environment variables for model and dataset paths.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/textual_inversion/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport DATA_DIR=\"path-to-dir-containing-images\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"textual_inversion_cat\"\n```\n\n----------------------------------------\n\nTITLE: Training AutoencoderKL on CIFAR10\nDESCRIPTION: Launch command for training AutoencoderKL on CIFAR10 dataset with specific hyperparameters and validation images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/autoencoderkl/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_autoencoderkl.py \\\n    --pretrained_model_name_or_path stabilityai/sd-vae-ft-mse \\\n    --dataset_name=cifar10 \\\n    --image_column=img \\\n    --validation_image images/bird.jpg images/car.jpg images/dog.jpg images/frog.jpg \\\n    --num_train_epochs 100 \\\n    --gradient_accumulation_steps 2 \\\n    --learning_rate 4.5e-6 \\\n    --lr_scheduler cosine \\\n    --report_to wandb \\\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Hugging Face Hub\nDESCRIPTION: This Python snippet shows how to upload a dataset to the Hugging Face Hub using the `push_to_hub` method. It assumes the user has already logged in using `huggingface-cli login` and demonstrates how to push to both public and private repositories.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/create_dataset.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"# assuming you have ran the huggingface-cli login command in a terminal\ndataset.push_to_hub(\"name_of_your_dataset\")\n\n# if you want to push to a private repo, simply pass private=True:\ndataset.push_to_hub(\"name_of_your_dataset\", private=True)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for PixArt-α\nDESCRIPTION: Command to install the bitsandbytes library needed for 8-bit precision text encoding\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U bitsandbytes\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Dataset\nDESCRIPTION: Code snippet showing how datasets are loaded and preprocessed in the training script, with various image transformations applied.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset = load_dataset(\"imagefolder\", data_dir=args.train_data_dir, cache_dir=args.cache_dir, split=\"train\")\n\naugmentations = transforms.Compose(\n    [\n        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n        transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n        transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Achieving Reproducibility on CPU with PyTorch Generator\nDESCRIPTION: This code demonstrates how to use a PyTorch Generator with a fixed seed to achieve reproducible results on CPU.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom diffusers import DDIMPipeline\n\nddim = DDIMPipeline.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimage = ddim(num_inference_steps=2, output_type=\"np\", generator=generator).images\nprint(np.abs(image).sum())\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Configuration Parameters\nDESCRIPTION: This snippet shows how to access the configuration parameters of a loaded model. These parameters define the model architecture and cannot be changed after model creation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> model.config\n```\n\n----------------------------------------\n\nTITLE: Visual Reasoning with OmniGen\nDESCRIPTION: Shows OmniGen's visual reasoning capabilities by asking it to identify and highlight a specific object in the image. The example demonstrates how the model can understand complex relationships between objects and concepts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/omnigen.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt=\"If the woman is thirsty, what should she take? Find it in the image and highlight it in blue. <img><|image_1|></img>\"\ninput_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/edit.png\")]\nimage = pipe(\n    prompt=prompt, \n    input_images=input_images, \n    guidance_scale=2, \n    img_guidance_scale=1.6,\n    use_input_image_size_as_output=True,\n    generator=torch.Generator(device=\"cpu\").manual_seed(0)\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Running Slow Quantization Tests for Diffusers\nDESCRIPTION: This bash command demonstrates how to run slow quantization tests for the Diffusers project using pytest. It enables HuggingFace transfer and sets the RUN_SLOW flag to execute the SlowTorchAoTests.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/tests/quantization/torchao/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nHF_HUB_ENABLE_HF_TRANSFER=1 RUN_SLOW=1 pytest -s tests/quantization/torchao/test_torchao.py::SlowTorchAoTests\n```\n\n----------------------------------------\n\nTITLE: Downloading Styledrop Example Image\nDESCRIPTION: This command downloads the example style image for use with the Styledrop method. The image is a mushroom in a specific style and is used for fine-tuning the model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nwget https://huggingface.co/datasets/diffusers/docs-images/resolve/main/amused/A%20mushroom%20in%20%5BV%5D%20style.png\n```\n\n----------------------------------------\n\nTITLE: IF Stage I Full DreamBooth Training\nDESCRIPTION: This script performs full model fine-tuning (not LoRA) on DeepFloyd IF Stage I model. It uses 8-bit Adam optimization to manage memory usage, an extremely low learning rate (1e-7), and skips saving the text encoder to conserve space. It requires approximately 48GB VRAM with the specified batch size of 4.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"DeepFloyd/IF-I-XL-v1.0\"\n\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"dreambooth_if\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=64 \\\n  --train_batch_size=4 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-7 \\\n  --max_train_steps=150 \\\n  --validation_prompt \"a photo of sks dog\" \\\n  --validation_steps 25 \\\n  --text_encoder_use_attention_mask \\\n  --tokenizer_max_length 77 \\\n  --pre_compute_text_embeddings \\\n  --use_8bit_adam \\\n  --set_grads_to_none \\\n  --skip_save_text_encoder \\\n  --validation_scheduler DDPMScheduler \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderKL from Original Format in Python\nDESCRIPTION: Example of loading an AutoencoderKL model from a single file (safetensors) URL or local file path using the from_single_file method instead of the default from_pretrained method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoderkl.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKL\n\nurl = \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors\"  # can also be a local file\nmodel = AutoencoderKL.from_single_file(url)\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub for Model Access\nDESCRIPTION: Command to authenticate with the Hugging Face Hub using your access token to download model weights that require license acceptance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: List of Python package dependencies required for the Diffusers project. Includes machine learning frameworks, utilities, and monitoring tools with specific version requirements where needed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/diffusion_dpo/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\npeft\nwandb\n```\n\n----------------------------------------\n\nTITLE: Loading Scheduler and Models for DreamBooth Training\nDESCRIPTION: Demonstrates how to load the necessary components for DreamBooth training including the DDPM scheduler, text encoder, VAE (if the model has one), and UNet model from a pretrained model path.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Load scheduler and models\nnoise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\ntext_encoder = text_encoder_cls.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n)\n\nif model_has_vae(args):\n    vae = AutoencoderKL.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision\n    )\nelse:\n    vae = None\n\nunet = UNet2DConditionModel.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n)\n```\n\n----------------------------------------\n\nTITLE: Removing Incorrect CUDA Pathspec\nDESCRIPTION: A command to remove the incorrect CUDA version path specification in Colab's conda configuration, ensuring correct version usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n!rm /usr/local/conda-meta/pinned\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Conditioning Images\nDESCRIPTION: Commands to download sample conditioning images used for validation during the ControlNet training process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```\n\n----------------------------------------\n\nTITLE: Setting Training Configuration Variables\nDESCRIPTION: Configures environment variables for model name, output directory, and dataset paths for multi-subject Dreambooth inpainting training\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_subject_dreambooth_inpainting/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"runwayml/stable-diffusion-inpainting\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\nexport DATASET_1=\"gzguevara/mr_potato_head_masked\"\nexport DATASET_2=\"gzguevara/cat_toy_masked\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Wan 2.1 Video Generation\nDESCRIPTION: Installs additional Python packages required for using the Wan 2.1 video generation models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -u ftfy imageio-ffmpeg imageio\n```\n\n----------------------------------------\n\nTITLE: Using Official SDXLCFGCutoffCallback with StableDiffusionXLPipeline in Python\nDESCRIPTION: Example showing how to use the official SDXLCFGCutoffCallback to disable classifier-free guidance after a certain number of steps in the SDXL pipeline. The callback can be configured with either cutoff_step_ratio or cutoff_step_index parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import DPMSolverMultistepScheduler, StableDiffusionXLPipeline\nfrom diffusers.callbacks import SDXLCFGCutoffCallback\n\n\ncallback = SDXLCFGCutoffCallback(cutoff_step_ratio=0.4)\n# can also be used with cutoff_step_index\n# callback = SDXLCFGCutoffCallback(cutoff_step_ratio=None, cutoff_step_index=10)\n\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, use_karras_sigmas=True)\n\nprompt = \"a sports car at the road, best quality, high quality, high detail, 8k resolution\"\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(2628670641)\n\nout = pipeline(\n    prompt=prompt,\n    negative_prompt=\"\",\n    guidance_scale=6.5,\n    num_inference_steps=25,\n    generator=generator,\n    callback_on_step_end=callback,\n)\n\nout.images[0].save(\"official_callback.png\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate environment for LCM training\nDESCRIPTION: Command to initialize an Accelerate environment with interactive configuration for distributed training setup.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Programmatically Creating Accelerate Configuration\nDESCRIPTION: Python code to create a basic Accelerate configuration programmatically, useful in notebook environments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Defining Styles and Keywords for Prompt Enhancement in Python\nDESCRIPTION: Sets up predefined style templates and a list of quality keywords that will be used to enhance basic prompts. The styles dictionary contains different aesthetic approaches that can be applied to prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import GenerationConfig, GPT2LMHeadModel, GPT2Tokenizer, LogitsProcessor, LogitsProcessorList\nfrom diffusers import StableDiffusionXLPipeline\n\nstyles = {\n    \"cinematic\": \"cinematic film still of {prompt}, highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain\",\n    \"anime\": \"anime artwork of {prompt}, anime style, key visual, vibrant, studio anime, highly detailed\",\n    \"photographic\": \"cinematic photo of {prompt}, 35mm photograph, film, professional, 4k, highly detailed\",\n    \"comic\": \"comic of {prompt}, graphic illustration, comic art, graphic novel art, vibrant, highly detailed\",\n    \"lineart\": \"line art drawing {prompt}, professional, sleek, modern, minimalist, graphic, line art, vector graphics\",\n    \"pixelart\": \" pixel-art {prompt}, low-res, blocky, pixel art style, 8-bit graphics\",\n}\n\nwords = [\n    \"aesthetic\", \"astonishing\", \"beautiful\", \"breathtaking\", \"composition\", \"contrasted\", \"epic\", \"moody\", \"enhanced\",\n    \"exceptional\", \"fascinating\", \"flawless\", \"glamorous\", \"glorious\", \"illumination\", \"impressive\", \"improved\",\n    \"inspirational\", \"magnificent\", \"majestic\", \"hyperrealistic\", \"smooth\", \"sharp\", \"focus\", \"stunning\", \"detailed\",\n    \"intricate\", \"dramatic\", \"high\", \"quality\", \"perfect\", \"light\", \"ultra\", \"highly\", \"radiant\", \"satisfying\",\n    \"soothing\", \"sophisticated\", \"stylish\", \"sublime\", \"terrific\", \"touching\", \"timeless\", \"wonderful\", \"unbelievable\",\n    \"elegant\", \"awesome\", \"amazing\", \"dynamic\", \"trendy\",\n]\n```\n\n----------------------------------------\n\nTITLE: Creating Training Data with Distributed Processing\nDESCRIPTION: Bash command to generate training data using torchrun across 8 GPUs with specified paths and model checkpoint\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --master_port 17673 --nproc_per_node=8 make_datasets.py \\\n    --data_root /mnt/workspace/workgroup/zhizhonghuang/dataset/COCO/train2017 \\\n    --save_root /root/gligen_data \\\n    --ram_checkpoint /root/.cache/huggingface/hub/models--xinyu1205--recognize_anything_model/snapshots/ebc52dc741e86466202a5ab8ab22eae6e7d48bf1/ram_swin_large_14m.pth\n```\n\n----------------------------------------\n\nTITLE: Running the Training Job for Stable Diffusion\nDESCRIPTION: This snippet describes how to start the training job for the Stable Diffusion model by executing a command on the TPU VM. It includes necessary environmental variables and command-line arguments for the training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/training/text_to_image/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\\n--project=${PROJECT_ID} --zone=${ZONE} --worker=all \\\n--command='\\nexport XLA_DISABLE_FUNCTIONALIZATION=0\\nexport PROFILE_DIR=/tmp/\\nexport CACHE_DIR=/tmp/\\nexport DATASET_NAME=lambdalabs/naruto-blip-captions\\nexport PER_HOST_BATCH_SIZE=32 # This is known to work on TPU v4. Can set this to 64 for TPU v5p\\nexport TRAIN_STEPS=50\\nexport OUTPUT_DIR=/tmp/trained-model/\\npython diffusers/examples/research_projects/pytorch_xla/train_text_to_image_xla.py --pretrained_model_name_or_path=stabilityai/stable-diffusion-2-base --dataset_name=$DATASET_NAME --resolution=512 --center_crop --random_flip --train_batch_size=$PER_HOST_BATCH_SIZE  --max_train_steps=$TRAIN_STEPS --learning_rate=1e-06 --mixed_precision=bf16 --profile_duration=80000 --output_dir=$OUTPUT_DIR --dataloader_num_workers=8 --loader_prefetch_size=4 --device_prefetch_size=4'\n```\n\n----------------------------------------\n\nTITLE: Importing UNet3DConditionOutput in Python\nDESCRIPTION: This snippet demonstrates how to import the UNet3DConditionOutput class from the Hugging Face Diffusers library. This class represents the output of a UNet3DConditionModel.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet3d-cond.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.models.unets.unet_3d_condition import UNet3DConditionOutput\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local Pipeline with Weights\nDESCRIPTION: This snippet outlines the commands to set up a local pipeline for image generation by cloning a pretrained model repository and loading the weights for inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\n!git lfs install\n!git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5\n```\n\nLANGUAGE: Python\nCODE:\n```\n>>> pipeline = DiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\", use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Training Command for SDXL LoRA\nDESCRIPTION: Bash script for training a Stable Diffusion XL model using DeepSpeed with specific model, dataset, and training parameters\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\nexport ACCELERATE_CONFIG_FILE=\"your accelerate_config.yaml\"\n\naccelerate launch  --config_file $ACCELERATE_CONFIG_FILE train_text_to_image_lora_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=1024  \\\n  --train_batch_size=1 \\\n  --num_train_epochs=2 \\\n  --checkpointing_steps=2 \\\n  --learning_rate=1e-04 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --mixed_precision=\"fp16\" \\\n  --max_train_steps=20 \\\n  --validation_epochs=20 \\\n  --seed=1234 \\\n  --output_dir=\"sd-naruto-model-lora-sdxl\" \\\n  --validation_prompt=\"cute dragon creature\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Commands to initialize and configure an 🤗Accelerate environment for distributed training optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Inference with Custom Diffusion - Single Concept\nDESCRIPTION: Python code for running inference using a trained Custom Diffusion model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n).to(\"cuda\")\npipe.unet.load_attn_procs(\n    \"path-to-save-model\", weight_name=\"pytorch_custom_diffusion_weights.bin\"\n)\npipe.load_textual_inversion(\"path-to-save-model\", weight_name=\"<new1>.bin\")\n\nimage = pipe(\n    \"<new1> cat sitting in a bucket\",\n    num_inference_steps=100,\n    guidance_scale=6.0,\n    eta=1.0,\n).images[0]\nimage.save(\"cat.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading SanaTransformer2DModel in Python\nDESCRIPTION: This code snippet demonstrates how to load the SanaTransformer2DModel from a pre-trained checkpoint using the Diffusers library. It specifies the model path, subfolder, and sets the torch data type to bfloat16.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/sana_transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import SanaTransformer2DModel\n\ntransformer = SanaTransformer2DModel.from_pretrained(\"Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained Decoder Model\nDESCRIPTION: Code for performing inference with a trained Kandinsky decoder model. It loads the trained model and generates an image from a text prompt with CPU offloading for memory efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"path/to/saved/model\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n\nprompt=\"A robot naruto, 4k photo\"\nimage = pipeline(prompt=prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Clearing Transformer from Memory\nDESCRIPTION: Removes the transformer and pipeline from GPU memory after denoising to prepare for loading the VAE. This ensures maximum available memory for the VAE decoding stage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndel pipeline.transformer\ndel pipeline\n\nflush()\n```\n\n----------------------------------------\n\nTITLE: Training DreamBooth with Min-SNR Weighting and Prior Preservation Loss\nDESCRIPTION: Use Min-SNR weighting for faster convergence and prior preservation loss for generating diverse images during training. Adjust settings like SNR gamma and prior loss weight as needed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth.py \\\n  --snr_gamma=5.0\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth.py \\\n  --with_prior_preservation \\\n  --prior_loss_weight=1.0 \\\n  --class_data_dir=\"path/to/class/images\" \\\n  --class_prompt=\"text prompt describing class\"\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Commands to clone and install the Diffusers library from source code to ensure compatibility with example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Implementing Masked Image-to-Image Pipeline for Stable Diffusion XL\nDESCRIPTION: This code demonstrates the Masked Image-to-Image Pipeline for Stable Diffusion XL models. It computes a mask from the difference between two images and applies the inpainting process, with the latent code initialized from the masked image affecting the final result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_66\n\nLANGUAGE: python\nCODE:\n```\nimg = PIL.Image.open(\"./mech.png\")\n# read image with mask painted over\nimg_paint = PIL.Image.open(\"./mech_painted.png\")\n\npipeline = MaskedStableDiffusionXLImg2ImgPipeline.from_pretrained(\"frankjoshua/juggernautXL_v8Rundiffusion\", dtype=torch.float16)\n\npipeline.to('cuda')\npipeline.enable_xformers_memory_efficient_attention()\n\nprompt = \"a mech warrior wearing a mask\"\nseed = 8348273636437\nfor i in range(10):\n    generator = torch.Generator(device=\"cuda\").manual_seed(seed + i)\n    print(seed + i)\n    result = pipeline(prompt=prompt, blur=48, image=img_paint, original_image=img, strength=0.9,\n                          generator=generator, num_inference_steps=60, num_images_per_prompt=1)\n    im = result.images[0]\n    im.save(f\"result{i}.png\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Configuration\nDESCRIPTION: Set up the Hugging Face Accelerate environment for distributed or mixed-precision training, with options for default or interactive configuration\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: GLIGEN Training Configuration\nDESCRIPTION: Accelerate launch command for training GLIGEN model with specified hyperparameters and data paths\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_gligen_text.py \\\n    --data_path /root/data/zhizhonghuang/coco_train2017.pth \\\n    --image_path /mnt/workspace/workgroup/zhizhonghuang/dataset/COCO/train2017 \\\n    --train_batch_size 8 \\\n    --max_train_steps 100000 \\\n    --checkpointing_steps 1000 \\\n    --checkpoints_total_limit 10 \\\n    --learning_rate 5e-5 \\\n    --dataloader_num_workers 16 \\\n    --mixed_precision fp16 \\\n    --report_to wandb \\\n    --tracker_project_name gligen \\\n    --output_dir /root/data/zhizhonghuang/ckpt/GLIGEN_Text_Retrain_COCO\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Configuration\nDESCRIPTION: Set up the Accelerate environment for distributed training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text_inversion.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Checking Input Channels for Inpainting with Stable Diffusion\nDESCRIPTION: Shows how to load a pretrained Stable Diffusion inpainting pipeline and verify the number of input channels. This step ensures the correct setup for inpainting tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/adapt_a_model.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\", use_safetensors=True)\npipeline.unet.config[\"in_channels\"]\n9\n```\n\n----------------------------------------\n\nTITLE: Improving Image Quality with Better VAE\nDESCRIPTION: Loads a more advanced VAE model from Stability AI to enhance the quality of generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/stable_diffusion.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.vae = vae\nimages = pipeline(**get_inputs(batch_size=8)).images\n```\n\n----------------------------------------\n\nTITLE: Training with Validation Monitoring via Weights & Biases\nDESCRIPTION: Enhanced training command that enables validation inference during training to monitor progress with Weights and Biases integration. Includes parameters for validation image and prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --val_image_url=\"https://hf.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png\" \\\n    --validation_prompt=\"make the mountains snowy\" \\\n    --seed=42 \\\n    --report_to=wandb \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Dataset with Hugging Face Hub\nDESCRIPTION: Downloads a sample dataset of cat toy images from the Hugging Face Hub and stores them in a local directory. This code uses the snapshot_download function to retrieve the dataset while ignoring git attribute files.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./cat\"\nsnapshot_download(\n    \"diffusers/cat_toy_example\", local_dir=local_dir, repo_type=\"dataset\", ignore_patterns=\".gitattributes\"\n)\n```\n\n----------------------------------------\n\nTITLE: IADB Pipeline Image Generation\nDESCRIPTION: Implementation of the α-(de)Blending diffusion model showing how to generate images using a pretrained CelebA-HQ model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_57\n\nLANGUAGE: python\nCODE:\n```\npipeline_iadb = DiffusionPipeline.from_pretrained(\"thomasc4/iadb-celebahq-256\", custom_pipeline='iadb')\n\npipeline_iadb = pipeline_iadb.to('cuda')\n\noutput = pipeline_iadb(batch_size=4, num_inference_steps=128)\nfor i in range(len(output[0])):\n    plt.imshow(output[0][i])\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Stable Diffusion Pipeline with Basic Prompt\nDESCRIPTION: Sets up a Stable Diffusion pipeline using the v1-4 model and generates an image using a basic prompt without weighting.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/weighted_prompts.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"a red cat playing with a ball\"\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\n\nimage = pipe(prompt, generator=generator, num_inference_steps=20).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Listing Package Dependencies for Diffusers Library\nDESCRIPTION: A requirements file listing the Python packages required for the Diffusers library. It specifies minimum versions for some packages (accelerate and transformers) and includes packages for model training, visualization, and data processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/requirements_flux.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\ndatasets\nwandb\nSentencePiece\n```\n\n----------------------------------------\n\nTITLE: Loading Pivotal Tuning Embeddings for Inference\nDESCRIPTION: Python code to load the pivotal tuning embeddings created during training for use in inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntext_encoders = [pipe.text_encoder, pipe.text_encoder_2]\ntokenizers = [pipe.tokenizer, pipe.tokenizer_2]\n\nembedding_path = hf_hub_download(repo_id=repo_id, filename=\"3d-icon-SDXL-LoRA_emb.safetensors\", repo_type=\"model\")\n\nstate_dict = load_file(embedding_path)\n# load embeddings of text_encoder 1 (CLIP ViT-L/14)\npipe.load_textual_inversion(state_dict[\"clip_l\"], token=[\"<s0>\", \"<s1>\"], text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)\n```\n\n----------------------------------------\n\nTITLE: Loading Basic Diffusion Pipeline with CUDA\nDESCRIPTION: Initializes a basic diffusion pipeline using the stable-diffusion-v1-5 model with float16 precision on CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/schedulers.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Setting Default Accelerate Configuration\nDESCRIPTION: Command to set up a default Accelerate configuration without interactive prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Implementing IP Adapter with Negative Noise in Stable Diffusion XL\nDESCRIPTION: This script demonstrates how to implement negative image prompting with IP-Adapter in Diffusers pipelines. It overrides the default zero-filled negative tensor with actual negative noise to improve generation quality while preserving image composition. The implementation includes custom encoding functions and pipeline setup with IP-Adapter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README_community_scripts.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import AutoencoderKL, DPMSolverMultistepScheduler, StableDiffusionXLPipeline\nfrom diffusers.models import ImageProjection\nfrom diffusers.utils import load_image\n\n\ndef encode_image(\n    image_encoder,\n    feature_extractor,\n    image,\n    device,\n    num_images_per_prompt,\n    output_hidden_states=None,\n    negative_image=None,\n):\n    dtype = next(image_encoder.parameters()).dtype\n\n    if not isinstance(image, torch.Tensor):\n        image = feature_extractor(image, return_tensors=\"pt\").pixel_values\n\n    image = image.to(device=device, dtype=dtype)\n    if output_hidden_states:\n        image_enc_hidden_states = image_encoder(image, output_hidden_states=True).hidden_states[-2]\n        image_enc_hidden_states = image_enc_hidden_states.repeat_interleave(num_images_per_prompt, dim=0)\n\n        if negative_image is None:\n            uncond_image_enc_hidden_states = image_encoder(\n                torch.zeros_like(image), output_hidden_states=True\n            ).hidden_states[-2]\n        else:\n            if not isinstance(negative_image, torch.Tensor):\n                negative_image = feature_extractor(negative_image, return_tensors=\"pt\").pixel_values\n            negative_image = negative_image.to(device=device, dtype=dtype)\n            uncond_image_enc_hidden_states = image_encoder(negative_image, output_hidden_states=True).hidden_states[-2]\n\n        uncond_image_enc_hidden_states = uncond_image_enc_hidden_states.repeat_interleave(num_images_per_prompt, dim=0)\n        return image_enc_hidden_states, uncond_image_enc_hidden_states\n    else:\n        image_embeds = image_encoder(image).image_embeds\n        image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n        uncond_image_embeds = torch.zeros_like(image_embeds)\n\n        return image_embeds, uncond_image_embeds\n\n\n@torch.no_grad()\ndef prepare_ip_adapter_image_embeds(\n    unet,\n    image_encoder,\n    feature_extractor,\n    ip_adapter_image,\n    do_classifier_free_guidance,\n    device,\n    num_images_per_prompt,\n    ip_adapter_negative_image=None,\n):\n    if not isinstance(ip_adapter_image, list):\n        ip_adapter_image = [ip_adapter_image]\n\n    if len(ip_adapter_image) != len(unet.encoder_hid_proj.image_projection_layers):\n        raise ValueError(\n            f\"`ip_adapter_image` must have same length as the number of IP Adapters. Got {len(ip_adapter_image)} images and {len(unet.encoder_hid_proj.image_projection_layers)} IP Adapters.\"\n        )\n\n    image_embeds = []\n    for single_ip_adapter_image, image_proj_layer in zip(\n        ip_adapter_image, unet.encoder_hid_proj.image_projection_layers\n    ):\n        output_hidden_state = not isinstance(image_proj_layer, ImageProjection)\n        single_image_embeds, single_negative_image_embeds = encode_image(\n            image_encoder,\n            feature_extractor,\n            single_ip_adapter_image,\n            device,\n            1,\n            output_hidden_state,\n            negative_image=ip_adapter_negative_image,\n        )\n        single_image_embeds = torch.stack([single_image_embeds] * num_images_per_prompt, dim=0)\n        single_negative_image_embeds = torch.stack([single_negative_image_embeds] * num_images_per_prompt, dim=0)\n\n        if do_classifier_free_guidance:\n            single_image_embeds = torch.cat([single_negative_image_embeds, single_image_embeds])\n            single_image_embeds = single_image_embeds.to(device)\n\n        image_embeds.append(single_image_embeds)\n\n    return image_embeds\n\n\nvae = AutoencoderKL.from_pretrained(\n    \"madebyollin/sdxl-vae-fp16-fix\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"RunDiffusion/Juggernaut-XL-v9\",\n    torch_dtype=torch.float16,\n    vae=vae,\n    variant=\"fp16\",\n).to(\"cuda\")\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\npipeline.scheduler.config.use_karras_sigmas = True\n\npipeline.load_ip_adapter(\n    \"h94/IP-Adapter\",\n    subfolder=\"sdxl_models\",\n    weight_name=\"ip-adapter-plus_sdxl_vit-h.safetensors\",\n    image_encoder_folder=\"models/image_encoder\",\n)\npipeline.set_ip_adapter_scale(0.7)\n\nip_image = load_image(\"source.png\")\nnegative_ip_image = load_image(\"noise.png\")\n\nimage_embeds = prepare_ip_adapter_image_embeds(\n    unet=pipeline.unet,\n    image_encoder=pipeline.image_encoder,\n    feature_extractor=pipeline.feature_extractor,\n    ip_adapter_image=[[ip_image]],\n    do_classifier_free_guidance=True,\n    device=\"cuda\",\n    num_images_per_prompt=1,\n    ip_adapter_negative_image=negative_ip_image,\n)\n\n\nprompt = \"cinematic photo of a cyborg in the city, 4k, high quality, intricate, highly detailed\"\nnegative_prompt = \"blurry, smooth, plastic\"\n\nimage = pipeline(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    ip_adapter_image_embeds=image_embeds,\n    guidance_scale=6.0,\n    num_inference_steps=25,\n    generator=torch.Generator(device=\"cpu\").manual_seed(1556265306),\n).images[0]\n\nimage.save(\"result.png\")\n```\n\n----------------------------------------\n\nTITLE: LoRA Documentation Headers\nDESCRIPTION: License and copyright header documentation for the LoRA loader implementation in Huggingface Diffusers\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/loaders/lora.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Applying Code Style Corrections - Python Bash\nDESCRIPTION: Uses the `make style` command to enforce coding standards through automatic style corrections and verifications of the source code.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ make style\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for Training\nDESCRIPTION: Commands to set up Accelerate for distributed training, including options for default configuration and notebook environments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Hub\nDESCRIPTION: Command to log into the Hugging Face Hub to enable automatic uploading of trained LoRA models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: SDXL ControlNet Reference Pipeline\nDESCRIPTION: Integration of ControlNet with SDXL Reference pipeline for controlled image generation while maintaining reference characteristics.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import ControlNetModel, AutoencoderKL\nfrom diffusers.schedulers import UniPCMultistepScheduler\nfrom diffusers.utils import load_image\nimport numpy as np\nimport torch\nimport cv2\nfrom PIL import Image\n\nfrom .stable_diffusion_xl_controlnet_reference import StableDiffusionXLControlNetReferencePipeline\n\ncanny_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl_reference_input_cat.jpg\"\n)\n\nref_image = load_image(\n    \"https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\"\n)\n\ncontrolnet_conditioning_scale = 0.5\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetReferencePipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, vae=vae, torch_dtype=torch.float16\n).to(\"cuda:0\")\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\nimage = np.array(canny_image)\nimage = cv2.Canny(image, 100, 200)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with DiffusionPipeline in Python\nDESCRIPTION: Generates an image using the diffusion model and extracts the first image from the returned object. The output is wrapped in a PIL.Image object by default.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/unconditional_image_generation.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> image = generator().images[0]\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Accelerate Environment\nDESCRIPTION: This command sets up a default 🤗 Accelerate environment without requiring user input. This is a quick way to configure Accelerate with standard settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate config default\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dependencies with uv\nDESCRIPTION: Command to compile and update the requirements file using the uv package manager.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/server/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip compile requirements.in -o requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Optimizing VAE Component\nDESCRIPTION: Replaces the default VAE with an optimized version from Stability AI.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/stable_diffusion.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.vae = vae\n```\n\n----------------------------------------\n\nTITLE: Setting Up Model and Textual Inversion Concept\nDESCRIPTION: Specifies the Stable Diffusion model checkpoint and the pre-learned concept repository to use for textual inversion from the Stable Diffusion Conceptualizer.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/textual_inversion_inference.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npretrained_model_name_or_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nrepo_id_embeds = \"sd-concepts-library/cat-toy\"\n```\n\n----------------------------------------\n\nTITLE: Forward Pass Implementation in MoleculeGNN\nDESCRIPTION: Core implementation of the forward pass in a molecular GNN model that processes atom types, positions, and bond information to generate molecular conformations. Includes both global and local feature processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, sample, timestep: Union[torch.Tensor, float, int], return_dict: bool = True, sigma=1.0, global_start_sigma=0.5, w_global=1.0, extend_order=False, extend_radius=True, clip_local=None, clip_global=1000.0) -> Union[MoleculeGNNOutput, Tuple]:\n    atom_type = sample.atom_type\n    bond_index = sample.edge_index\n    bond_type = sample.edge_type\n    num_graphs = sample.num_graphs\n    pos = sample.pos\n\n    timesteps = torch.full(size=(num_graphs,), fill_value=timestep, dtype=torch.long, device=pos.device)\n\n    edge_inv_global, edge_inv_local, edge_index, edge_type, edge_length, local_edge_mask = self._forward(\n        atom_type=atom_type,\n        pos=sample.pos,\n        bond_index=bond_index,\n        bond_type=bond_type,\n        batch=sample.batch,\n        time_step=timesteps,\n        return_edges=True,\n        extend_order=extend_order,\n        extend_radius=extend_radius,\n    )\n\n    node_eq_local = graph_field_network(\n        edge_inv_local, pos, edge_index[:, local_edge_mask], edge_length[local_edge_mask]\n    )\n    if clip_local is not None:\n        node_eq_local = clip_norm(node_eq_local, limit=clip_local)\n\n    if sigma < global_start_sigma:\n        edge_inv_global = edge_inv_global * (1 - local_edge_mask.view(-1, 1).float())\n        node_eq_global = graph_field_network(edge_inv_global, pos, edge_index, edge_length)\n        node_eq_global = clip_norm(node_eq_global, limit=clip_global)\n    else:\n        node_eq_global = 0\n\n    eps_pos = node_eq_local + node_eq_global * w_global\n\n    if not return_dict:\n        return (-eps_pos,)\n\n    return MoleculeGNNOutput(sample=torch.Tensor(-eps_pos).to(pos.device))\n```\n\n----------------------------------------\n\nTITLE: Using a Custom VAE for ControlNet SDXL Inference (Python)\nDESCRIPTION: Python code snippet showing how to specify a custom VAE when loading the SDXL ControlNet pipeline for inference, addressing potential numerical instability issues.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nvae = AutoencoderKL.from_pretrained(vae_path_or_repo_id, torch_dtype=torch.float16)\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    base_model_path, controlnet=controlnet, torch_dtype=torch.float16,\n    vae=vae,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing SD3 Requirements\nDESCRIPTION: Command to install the specific requirements for Stable Diffusion 3 training from the requirements file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements_sd3.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies for Hugging Face Diffusers\nDESCRIPTION: A requirements file listing the necessary Python packages for the Diffusers library. It includes accelerate for distributed training, torchvision for computer vision utilities, transformers for models, ftfy for text processing, tensorboard and wandb for experiment tracking, and other supporting libraries.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/requirements_sd3.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\ndatasets\nwandb\n```\n\n----------------------------------------\n\nTITLE: Loading Trained LoRA Weights for Inference\nDESCRIPTION: Python code to load the trained UNet LoRA weights for inference with Stable Diffusion XL.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom huggingface_hub import hf_hub_download, upload_file\nfrom diffusers import DiffusionPipeline\nfrom diffusers.models import AutoencoderKL\nfrom safetensors.torch import load_file\n\nusername = \"linoyts\"\nrepo_id = f\"{username}/3d-icon-SDXL-LoRA\"\n\npipe = DiffusionPipeline.from_pretrained(\n        \"stabilityai/stable-diffusion-xl-base-1.0\",\n        torch_dtype=torch.float16,\n        variant=\"fp16\",\n).to(\"cuda\")\n\n\npipe.load_lora_weights(repo_id, weight_name=\"pytorch_lora_weights.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: Decoding Latents to Image\nDESCRIPTION: Converts generated latents into a final PIL image using the VAE decoder\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\nimage = pipe.image_processor.postprocess(image, output_type=\"pil\")[0]\nimage.save(\"cat.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with Stable Diffusion XL ONNX Pipeline\nDESCRIPTION: Shows how to load a Stable Diffusion XL model and run inference using the ORTStableDiffusionXLPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/onnx.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.onnxruntime import ORTStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"sailing ship in storm by Leonardo da Vinci\"\nimage = pipeline(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Copying English Documentation Files to New Language Directory in Bash\nDESCRIPTION: Commands to navigate to the docs directory and copy the English documentation files to a new directory for your target language. <LANG-ID> should be replaced with the appropriate ISO language code.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/TRANSLATING.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/path/to/diffusers/docs\ncp -r source/en source/<LANG-ID>\n```\n\n----------------------------------------\n\nTITLE: Importing LEditsPPDiffusionPipelineOutput in Python\nDESCRIPTION: This code imports the LEditsPPDiffusionPipelineOutput class, which represents the output of the LEDITS++ diffusion pipeline. It includes all associated methods and properties.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ledits_pp.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.ledits_pp.pipeline_output import LEditsPPDiffusionPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Initializing Distributed Inference with Accelerate in Python\nDESCRIPTION: Sets up distributed inference using the Hugging Face's Accelerate library, which allows inference across multiple GPUs by automatically distributing the prompts. It initializes a DiffusionPipeline and assigns a GPU to each process, using contexts to manage distributed states. Accelerate must be installed and the script is executed using the accelerate command with a specific number of processes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom accelerate import PartialState\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n)\ndistributed_state = PartialState()\npipeline.to(distributed_state.device)\n\nwith distributed_state.split_between_processes([\"a dog\", \"a cat\"]) as prompt:\n    result = pipeline(prompt).images[0]\n    result.save(f\"result_{distributed_state.process_index}.png\")\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Min-SNR Weighting (Bash)\nDESCRIPTION: This command launches the training script for the Kandinsky 2.2 prior model with Min-SNR weighting enabled, using a gamma value of 5.0. Min-SNR weighting rebalances the loss to achieve faster convergence during training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_text_to_image_prior.py \\\n  --snr_gamma=5.0\n```\n\n----------------------------------------\n\nTITLE: Loading Local Pipeline Weights\nDESCRIPTION: Demonstrates how to load a pipeline from locally downloaded weights instead of downloading from Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> pipeline = DiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment\nDESCRIPTION: Command to configure the Accelerate environment for distributed training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/autoencoderkl/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Converting Convolutions to Linear Layers\nDESCRIPTION: Converts appropriate pointwise convolution layers to linear layers to maximize quantization benefits\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom torchao import swap_conv2d_1x1_to_linear\n\nswap_conv2d_1x1_to_linear(pipe.unet, conv_filter_fn)\nswap_conv2d_1x1_to_linear(pipe.vae, conv_filter_fn)\n```\n\n----------------------------------------\n\nTITLE: Modified VQGAN Configuration with Fewer Layers\nDESCRIPTION: JSON configuration for a VQGAN model with reduced layers, demonstrating how to modify the architecture by adjusting block types and channels.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/vqgan/README.md#2025-04-11_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"_class_name\": \"VQModel\",\n  \"_diffusers_version\": \"0.17.0.dev0\",\n  \"act_fn\": \"silu\",\n  \"block_out_channels\": [\n    128,\n    256,\n    256\n  ],\n  \"down_block_types\": [\n    \"DownEncoderBlock2D\",\n    \"DownEncoderBlock2D\",\n    \"DownEncoderBlock2D\"\n  ],\n  \"in_channels\": 3,\n  \"latent_channels\": 4,\n  \"layers_per_block\": 2,\n  \"norm_num_groups\": 32,\n  \"norm_type\": \"spatial\",\n  \"num_vq_embeddings\": 16384,\n  \"out_channels\": 3,\n  \"sample_size\": 32,\n  \"scaling_factor\": 0.18215,\n  \"up_block_types\": [\n    \"UpDecoderBlock2D\",\n    \"UpDecoderBlock2D\",\n    \"UpDecoderBlock2D\"\n  ],\n  \"vq_embed_dim\": 4\n}\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Text Encoder with UNet in Dreambooth\nDESCRIPTION: Command for training Dreambooth with both text encoder and UNet fine-tuning, which produces better results especially for faces. This requires at least 24GB VRAM and cannot fit on 16GB GPUs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_subject_dreambooth/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --train_text_encoder \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --use_8bit_adam \\\n  --gradient_checkpointing \\\n  --learning_rate=2e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Extending Molecular Graph with Radius-Based Connections in Python\nDESCRIPTION: Creates additional connections between atoms that are within a specified cutoff distance. This enables the model to capture non-bonded interactions like hydrogen bonds, π-stacking, or van der Waals forces that are important for molecular structure.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _extend_to_radius_graph(pos, edge_index, edge_type, cutoff, batch, unspecified_type_number=0, is_sidechain=None):\n    assert edge_type.dim() == 1\n    N = pos.size(0)\n\n    bgraph_adj = torch.sparse.LongTensor(edge_index, edge_type, torch.Size([N, N]))\n\n    if is_sidechain is None:\n        rgraph_edge_index = radius_graph(pos, r=cutoff, batch=batch)  # (2, E_r)\n    else:\n        # fetch sidechain and its batch index\n        is_sidechain = is_sidechain.bool()\n        dummy_index = torch.arange(pos.size(0), device=pos.device)\n        sidechain_pos = pos[is_sidechain]\n        sidechain_index = dummy_index[is_sidechain]\n        sidechain_batch = batch[is_sidechain]\n\n        assign_index = radius(x=pos, y=sidechain_pos, r=cutoff, batch_x=batch, batch_y=sidechain_batch)\n        r_edge_index_x = assign_index[1]\n        r_edge_index_y = assign_index[0]\n        r_edge_index_y = sidechain_index[r_edge_index_y]\n\n        rgraph_edge_index1 = torch.stack((r_edge_index_x, r_edge_index_y))  # (2, E)\n        rgraph_edge_index2 = torch.stack((r_edge_index_y, r_edge_index_x))  # (2, E)\n        rgraph_edge_index = torch.cat((rgraph_edge_index1, rgraph_edge_index2), dim=-1)  # (2, 2E)\n        # delete self loop\n        rgraph_edge_index = rgraph_edge_index[:, (rgraph_edge_index[0] != rgraph_edge_index[1])]\n\n    rgraph_adj = torch.sparse.LongTensor(\n        rgraph_edge_index,\n        torch.ones(rgraph_edge_index.size(1)).long().to(pos.device) * unspecified_type_number,\n        torch.Size([N, N]),\n    )\n\n    composed_adj = (bgraph_adj + rgraph_adj).coalesce()  # Sparse (N, N, T)\n\n    new_edge_index = composed_adj.indices()\n    new_edge_type = composed_adj.values().long()\n\n    return new_edge_index, new_edge_type\n```\n\n----------------------------------------\n\nTITLE: Decoding Latents to Image\nDESCRIPTION: Converts the generated latents into a final image using the VAE decoder and saves it.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart_sigma.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\nimage = pipe.image_processor.postprocess(image, output_type=\"pil\")[0]\nimage.save(\"cat.png\")\n```\n\n----------------------------------------\n\nTITLE: Distillation for Quantization Training\nDESCRIPTION: Command to perform distillation and quantization-aware training on the FP32 model to generate an INT8 model. Includes configuration for knowledge transfer and model optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport FP32_MODEL_NAME=\"./dicoo_model\"\nexport DATA_DIR=\"./dicoo\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$FP32_MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --use_ema --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=300 \\\n  --learning_rate=5.0e-04 --max_grad_norm=3 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"int8_model\" \\\n  --do_quantization --do_distillation --verify_loading\n```\n\n----------------------------------------\n\nTITLE: Running Stable Diffusion Inference with Core ML in Python\nDESCRIPTION: Command to run text-to-image generation using the Apple Core ML implementation of Stable Diffusion in Python. This example uses CPU and GPU for computation with a specified seed for reproducibility.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/coreml.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m python_coreml_stable_diffusion.pipeline --prompt \"a photo of an astronaut riding a horse on mars\" -i models/coreml-stable-diffusion-v1-4_original_packages -o </path/to/output/image> --compute-unit CPU_AND_GPU --seed 93\n```\n\n----------------------------------------\n\nTITLE: Installing Datasets Library for Custom Captioning\nDESCRIPTION: Installs the datasets library using pip, which is required for working with custom captions in the training process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets\n```\n\n----------------------------------------\n\nTITLE: Video-to-Video Generation with ControlNet in Python\nDESCRIPTION: Creates animated videos using ControlNet with OpenPose for motion control. Processes video frames through a pose detector and generates new frames while maintaining the original motion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Create controlnet preprocessor\nopen_pose = OpenposeDetector.from_pretrained(\"lllyasviel/Annotators\").to(\"cuda\")\n\n# Preprocess controlnet images\nconditioning_frames = []\nfor frame in tqdm(video):\n    conditioning_frames.append(open_pose(frame))\n\nstrength = 0.8\nwith torch.inference_mode():\n    video = pipe(\n        video=video,\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=10,\n        guidance_scale=2.0,\n        controlnet_conditioning_scale=0.75,\n        conditioning_frames=conditioning_frames,\n        strength=strength,\n        generator=torch.Generator().manual_seed(42),\n    ).frames[0]\n\nvideo = [frame.resize(conditioning_frames[0].size) for frame in video]\nexport_to_gif(video, f\"animatediff_vid2vid_controlnet.gif\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Configuring Stable Diffusion Inpainting with TensorRT\nDESCRIPTION: Example showing how to set up and use Stable Diffusion 2 inpainting model with TensorRT optimization to generate an inpainted image of a mecha robot.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_56\n\nLANGUAGE: python\nCODE:\n```\npipe.set_cached_folder(\"stabilityai/stable-diffusion-2-inpainting\", variant='fp16',)\n\npipe = pipe.to(\"cuda\")\n\nurl = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nresponse = requests.get(url)\ninput_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\nresponse = requests.get(mask_url)\nmask_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\nprompt = \"a mecha robot sitting on a bench\"\nimage = pipe(prompt, image=input_image, mask_image=mask_image, strength=0.75,).images[0]\nimage.save('tensorrt_inpaint_mecha_robot.png')\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Instructions for cloning and installing the Diffusers library from source code to ensure compatibility with example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_lumina2.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Downloading Model Weights with Git LFS\nDESCRIPTION: This bash command sequence installs Git LFS and clones the Stable Diffusion v1.5 model repository from Hugging Face, allowing for local usage of the model without downloading via the API.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n!git lfs install\n!git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5\n```\n\n----------------------------------------\n\nTITLE: Hugging Face CLI Login\nDESCRIPTION: Command to login to Hugging Face CLI for accessing gated models\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/flux-control/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Memory Optimization and StyleAligned Pipeline Configuration in Python\nDESCRIPTION: Configures memory saving techniques and StyleAligned settings for image generation using Diffusers. Includes prompt generation, inference settings, and pipeline configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_98\n\nLANGUAGE: python\nCODE:\n```\n# Enable memory saving techniques\npipe.enable_vae_slicing()\npipe.enable_vae_tiling()\n\nprompt = [\n  \"a toy train. macro photo. 3d game asset\",\n  \"a toy airplane. macro photo. 3d game asset\",\n  \"a toy bicycle. macro photo. 3d game asset\",\n  \"a toy car. macro photo. 3d game asset\",\n]\nnegative_prompt = \"low quality, worst quality, \"\n\n# Enable StyleAligned\npipe.enable_style_aligned(\n    share_group_norm=False,\n    share_layer_norm=False,\n    share_attention=True,\n    adain_queries=True,\n    adain_keys=True,\n    adain_values=False,\n    full_attention_share=False,\n    shared_score_scale=1.0,\n    shared_score_shift=0.0,\n    only_self_level=0.0,\n)\n\n# Run inference\nimages = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=2,\n    height=1024,\n    width=1024,\n    num_inference_steps=10,\n    generator=torch.Generator().manual_seed(42),\n).images\n\n# Disable StyleAligned if you do not wish to use it anymore\npipe.disable_style_aligned()\n```\n\n----------------------------------------\n\nTITLE: Adding cross-file section redirect in Markdown\nDESCRIPTION: Markdown syntax for preserving old section links when moving sections to another file, using relative paths to maintain compatibility with versioned documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\nSections that were moved:\n\n[ <a href=\"../new-file#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\n\n----------------------------------------\n\nTITLE: Training Amused 256 with LoRA\nDESCRIPTION: Script for training Amused-256 using LoRA optimization. Uses batch size 16 and learning rate 8e-4, requiring 1000-1250 steps for decent results. Significantly reduces memory usage compared to full finetuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --train_batch_size <batch size> \\\n    --gradient_accumulation_steps <gradient accumulation steps> \\\n    --learning_rate 8e-4 \\\n    --use_lora \\\n    --pretrained_model_name_or_path amused/amused-256 \\\n    --instance_data_dataset  'm1guelpf/nouns' \\\n    --image_key image \\\n    --prompt_key text \\\n    --resolution 256 \\\n    --mixed_precision fp16 \\\n    --lr_scheduler constant \\\n    --validation_prompts \\\n        'a pixel art character with square red glasses, a baseball-shaped head and a orange-colored body on a dark background' \\\n        'a pixel art character with square orange glasses, a lips-shaped head and a red-colored body on a light background' \\\n        'a pixel art character with square blue glasses, a microwave-shaped head and a purple-colored body on a sunny background' \\\n        'a pixel art character with square red glasses, a baseball-shaped head and a blue-colored body on an orange background' \\\n        'a pixel art character with square red glasses' \\\n        'a pixel art character' \\\n        'square red glasses on a pixel art character' \\\n        'square red glasses on a pixel art character with a baseball-shaped head' \\\n    --max_train_steps 10000 \\\n    --checkpointing_steps 500 \\\n    --validation_steps 250 \\\n    --gradient_checkpointing\n```\n\n----------------------------------------\n\nTITLE: Cloning Apple's Stable Diffusion Repository for Swift Inference\nDESCRIPTION: Command to clone Apple's ml-stable-diffusion repository, which provides Swift code for running Stable Diffusion inference on Apple devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/coreml.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/apple/ml-stable-diffusion\ncd ml-stable-diffusion\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained Flax DreamBooth Model\nDESCRIPTION: Code for running inference with a trained DreamBooth model using Flax/JAX. Loads the trained pipeline, handles device sharding, generates an image with the learned concept, and saves the result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\"path-to-your-trained-model\", dtype=jax.numpy.bfloat16)\n\nprompt = \"A photo of sks dog in a bucket\"\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\nimage.save(\"dog-bucket.png\")\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes\nDESCRIPTION: Command to push local changes to remote fork\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ git push -u origin a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Complete Training Loop for Diffusion Model with Accelerate\nDESCRIPTION: Implements a comprehensive training loop using 🤗 Accelerate for distributed training, mixed precision, gradient accumulation, and TensorBoard logging. The function also handles model checkpointing and optional pushing to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom huggingface_hub import create_repo, upload_folder\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport os\n\ndef train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n    # Initialize accelerator and tensorboard logging\n    accelerator = Accelerator(\n        mixed_precision=config.mixed_precision,\n        gradient_accumulation_steps=config.gradient_accumulation_steps,\n        log_with=\"tensorboard\",\n        project_dir=os.path.join(config.output_dir, \"logs\"),\n    )\n    if accelerator.is_main_process:\n        if config.output_dir is not None:\n            os.makedirs(config.output_dir, exist_ok=True)\n        if config.push_to_hub:\n            repo_id = create_repo(\n                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n            ).repo_id\n        accelerator.init_trackers(\"train_example\")\n\n    # Prepare everything\n    # There is no specific order to remember, you just need to unpack the\n    # objects in the same order you gave them to the prepare method.\n    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, lr_scheduler\n    )\n\n    global_step = 0\n\n    # Now you train the model\n    for epoch in range(config.num_epochs):\n        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n        progress_bar.set_description(f\"Epoch {epoch}\")\n\n        for step, batch in enumerate(train_dataloader):\n            clean_images = batch[\"images\"]\n            # Sample noise to add to the images\n            noise = torch.randn(clean_images.shape, device=clean_images.device)\n            bs = clean_images.shape[0]\n\n            # Sample a random timestep for each image\n            timesteps = torch.randint(\n                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n                dtype=torch.int64\n            )\n\n            # Add noise to the clean images according to the noise magnitude at each timestep\n            # (this is the forward diffusion process)\n            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n\n            with accelerator.accumulate(model):\n                # Predict the noise residual\n                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n                loss = F.mse_loss(noise_pred, noise)\n                accelerator.backward(loss)\n\n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            progress_bar.update(1)\n            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n            progress_bar.set_postfix(**logs)\n            accelerator.log(logs, step=global_step)\n            global_step += 1\n\n        # After each epoch you optionally sample some demo images with evaluate() and save the model\n        if accelerator.is_main_process:\n            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n\n            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n                evaluate(config, epoch, pipeline)\n\n            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n                if config.push_to_hub:\n                    upload_folder(\n                        repo_id=repo_id,\n                        folder_path=config.output_dir,\n                        commit_message=f\"Epoch {epoch}\",\n                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n                    )\n                else:\n                    pipeline.save_pretrained(config.output_dir)\n```\n\n----------------------------------------\n\nTITLE: Applying torch.compile to a Quantized Model\nDESCRIPTION: Demonstrates how to use torch.compile with a TorchAO quantized model for further optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/torchao.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransformer = torch.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Pushing FP16 Model Variant\nDESCRIPTION: Demonstrates pushing a model with fp16 weight variant to the Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/push_to_hub.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncontrolnet.push_to_hub(\"my-controlnet-model\", variant=\"fp16\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Instance Data Images\nDESCRIPTION: This code downloads the instance data images from the Hugging Face hub to a local directory named 'dog'. It uses the huggingface_hub library and downloads a specified dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Memory Management Functions\nDESCRIPTION: Utility functions to clear GPU memory and delete unused components.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart_sigma.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gc\n\ndef flush():\n    gc.collect()\n    torch.cuda.empty_cache()\n\ndel text_encoder\ndel pipe\nflush()\n```\n\n----------------------------------------\n\nTITLE: Using a Quantized Checkpoint with Bash\nDESCRIPTION: This snippet shows how to include an optional argument for specifying a quantized model path when launching training, which can help in faster execution by using a pretrained quantized model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n+ --quantized_model_path=\"hf-internal-testing/flux.1-dev-nf4-pkg\"\n```\n\n----------------------------------------\n\nTITLE: Loading Unconditional UNet Model\nDESCRIPTION: Loads a pre-trained UNet2DModel for unconditional image generation, specifically trained on cat images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import UNet2DModel\n\n>>> repo_id = \"google/ddpm-cat-256\"\n>>> model = UNet2DModel.from_pretrained(repo_id)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Motion LoRAs with AnimateLCM in Python\nDESCRIPTION: This snippet extends the AnimateLCM setup to include multiple Motion LoRAs. It initializes the AnimateDiffPipeline and sets up different adapters for enhanced animation quality. It similarly configures parameters and generates an animated video based on a specific prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AnimateDiffPipeline, LCMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"wangfuyun/AnimateLCM\")\npipe = AnimateDiffPipeline.from_pretrained(\"emilianJR/epiCRealism\", motion_adapter=adapter)\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule=\"linear\")\n\npipe.load_lora_weights(\"wangfuyun/AnimateLCM\", weight_name=\"sd15_lora_beta.safetensors\", adapter_name=\"lcm-lora\")\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-tilt-up\", adapter_name=\"tilt-up\")\n\npipe.set_adapters([\"lcm-lora\", \"tilt-up\"], [1.0, 0.8])\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\noutput = pipe(\n    prompt=\"A space rocket with trails of smoke behind it launching into space from the desert, 4k, high resolution\",\n    negative_prompt=\"bad quality, worse quality, low resolution\",\n    num_frames=16,\n    guidance_scale=1.5,\n    num_inference_steps=6,\n    generator=torch.Generator(\"cpu\").manual_seed(0),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animatelcm-motion-lora.gif\")\n```\n\n----------------------------------------\n\nTITLE: Converting Stable Diffusion Checkpoint to Diffusers Format\nDESCRIPTION: Python command to run the conversion script that transforms a .ckpt file to the Diffusers format, specifying the checkpoint path, configuration file, output location, and indicating it's a ControlNet model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ../diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path temporalnetv3.ckpt --original_config_file cldm_v15.yaml --dump_path ./ --controlnet\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers with PyTorch Support\nDESCRIPTION: Commands for installing the Diffusers library with PyTorch support using pip and conda package managers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade diffusers[torch]\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge diffusers\n```\n\n----------------------------------------\n\nTITLE: HTML Badge Implementation for LoRA\nDESCRIPTION: HTML code snippet showing the implementation of a LoRA badge using flex styling and custom styling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet_sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"flex flex-wrap space-x-1\">\n  <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Defining Stream CSV Format for Video Data\nDESCRIPTION: This code specifies the structure of a CSV file containing video paths and corresponding captions for streaming format data preparation. It ensures videos are aligned with text descriptions to leverage CogVideoX's capabilities in large-scale training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\n<CAPTION_COLUMN>,<PATH_TO_VIDEO_COLUMN>\n\"A black and white animated sequence featuring a rabbit, named Rabbity Ribfried, and an anthropomorphic goat in a musical, playful environment, showcasing their evolving interaction.\",\"00000.mp4\"\n\"A black and white animated sequence on a ship's deck features a bulldog character, named Bully Bulldoger, showcasing exaggerated facial expressions and body language. The character progresses from confident to focused, then to strained and distressed, displaying a range of emotions as it navigates challenges. The ship's interior remains static in the background, with minimalistic details such as a bell and open door. The character's dynamic movements and changing expressions drive the narrative, with no camera movement to distract from its evolving reactions and physical gestures.\",\"00001.mp4\"\n...\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Environment Information Command\nDESCRIPTION: Shell command to display environment information for bug reports using the Diffusers CLI tool.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndiffusers-cli env\n```\n\n----------------------------------------\n\nTITLE: Running Environment Information Command in Shell\nDESCRIPTION: Command to display necessary environment information for bug reports in the Diffusers project.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndiffusers-cli env\n```\n\n----------------------------------------\n\nTITLE: Launching Textual Inversion Training in Flax\nDESCRIPTION: Complete bash command to launch Textual Inversion training using Flax (JAX). This script trains a model to learn embeddings for a cat toy concept, with parameters for model path, dataset location, training batch size, learning rate, and other hyperparameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport DATA_DIR=\"./cat\"\n\npython textual_inversion_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" \\\n  --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 \\\n  --scale_lr \\\n  --output_dir=\"textual_inversion_cat\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Loading Text Encoders with Balanced Device Mapping for Prompt Encoding\nDESCRIPTION: Initializes FluxPipeline with only text encoders distributed evenly across two GPUs using balanced device mapping. The diffusion transformer and VAE are set to None to preserve memory while encoding the prompt text into embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import FluxPipeline\nimport torch\n\nprompt = \"a photo of a dog with cat-like look\"\n\npipeline = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    transformer=None,\n    vae=None,\n    device_map=\"balanced\",\n    max_memory={0: \"16GB\", 1: \"16GB\"},\n    torch_dtype=torch.bfloat16\n)\nwith torch.no_grad():\n    print(\"Encoding prompts.\")\n    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(\n        prompt=prompt, prompt_2=None, max_sequence_length=512\n    )\n```\n\n----------------------------------------\n\nTITLE: Applying Float8 Quantization with Diffusers in Python\nDESCRIPTION: Demonstrates how to use Quanto to apply float8 quantization to a Diffusers model, requiring the optimum-quanto and PyTorch. The script loads a model, uses a quantization configuration to set weights to float8, processes an input prompt, and generates an image output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/quanto.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxTransformer2DModel, QuantoConfig\n\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\nquantization_config = QuantoConfig(weights_dtype=\"float8\")\ntransformer = FluxTransformer2DModel.from_pretrained(\n      model_id,\n      subfolder=\"transformer\",\n      quantization_config=quantization_config,\n      torch_dtype=torch.bfloat16,\n)\n\npipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch_dtype)\npipe.to(\"cuda\")\n\nprompt = \"A cat holding a sign that says hello world\"\nimage = pipe(\n    prompt, num_inference_steps=50, guidance_scale=4.5, max_sequence_length=512\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with a Trained Model (PyTorch)\nDESCRIPTION: This code snippet shows how to perform inference with a trained Stable Diffusion model using the `StableDiffusionPipeline`. It loads the trained model, creates a prompt including the `placeholder_token`, and generates an image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"path-to-your-trained-model\"\npipe = StableDiffusionPipeline.from_pretrained(model_id,torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n\nimage.save(\"cat-backpack.png\")\n```\n\n----------------------------------------\n\nTITLE: Image Folder Directory Structure\nDESCRIPTION: This code snippet illustrates the expected directory structure for a folder of images used as a dataset for unconditional image generation.  Each image file (e.g., .png) resides directly within the specified `data_dir`.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/create_dataset.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"data_dir/xxx.png\ndata_dir/xxy.png\ndata_dir/[...]/xxz.png\"\n```\n\n----------------------------------------\n\nTITLE: Installing ColossalAI from PyPI\nDESCRIPTION: Installs the ColossalAI library from PyPI using pip.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/colossalai/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install colossalai\n```\n\n----------------------------------------\n\nTITLE: Loading Scheduler Configuration\nDESCRIPTION: Shows how to load a scheduler configuration using the ConfigMixin.from_config method, which allows for easy swapping of schedulers in the diffusion process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md#2025-04-11_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nConfigMixin.from_config()\n```\n\n----------------------------------------\n\nTITLE: Loading Image and Extracting Depth Map for ControlNet\nDESCRIPTION: Loads an image and uses a depth estimation pipeline to extract a depth map for use with ControlNet.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image\nimport torch\nimport numpy as np\nfrom transformers import pipeline\n\nimg = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png\"\n).resize((768, 768))\n\ndef make_hint(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    hint = detected_map.permute(2, 0, 1)\n    return hint\n\ndepth_estimator = pipeline(\"depth-estimation\")\nhint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: IADB Training Loop\nDESCRIPTION: Basic training loop implementation for the IADB diffusion model showing noise sampling and optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nwhile True:\n    x0 = sample_noise()\n    x1 = sample_dataset()\n\n    alpha = torch.rand(batch_size)\n\n    # Blend\n    x_alpha = (1-alpha) * x0 + alpha * x1\n\n    # Loss\n    loss = torch.sum((D(x_alpha, alpha)- (x1-x0))**2)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Configuring AltDiffusion Models for Dreambooth Training\nDESCRIPTION: Example of how to change the model name to use AltDiffusion instead of standard Stable Diffusion for Dreambooth training. This demonstrates how to adapt the training script for alternative diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_subject_dreambooth/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\" --> export MODEL_NAME=\"BAAI/AltDiffusion-m9\"\nor\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\" --> export MODEL_NAME=\"BAAI/AltDiffusion\"\n```\n\n----------------------------------------\n\nTITLE: Decoding and Saving Latent Tensors in Diffusers Callback\nDESCRIPTION: Creates a callback function to decode and save latent representations into viewable images at each step of the generation process. The function saves each step as a PNG file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/callback.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef decode_tensors(pipe, step, timestep, callback_kwargs):\n    latents = callback_kwargs[\"latents\"]\n\n    image = latents_to_rgb(latents[0])\n    image.save(f\"{step}.png\")\n\n    return callback_kwargs\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with Text Prompt\nDESCRIPTION: Generates an image using the loaded pipeline with a text prompt describing an image of a squirrel in Picasso style.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> image = pipeline(\"An image of a squirrel in Picasso style\").images[0]\n>>> image\n```\n\n----------------------------------------\n\nTITLE: Swapping Scheduler in DiffusionPipeline\nDESCRIPTION: This snippet demonstrates how to replace the default scheduler in the DiffusionPipeline with an alternative scheduler for potentially different denoising speeds and qualities in image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n>>> from diffusers import EulerDiscreteScheduler\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\n>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: Citation Information for Würstchen\nDESCRIPTION: BibTeX citation information for referencing the Würstchen paper in academic work.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wuerstchen.md#2025-04-11_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{pernias2023wuerstchen,\n      title={Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models},\n      author={Pablo Pernias and Dominic Rampas and Mats L. Richter and Christopher J. Pal and Marc Aubreville},\n      year={2023},\n      eprint={2306.00637},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: PEFT Installation Command\nDESCRIPTION: Shell command to install the PEFT library for advanced LoRA combinations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npip install peft\n```\n\n----------------------------------------\n\nTITLE: Documenting MarigoldNormalsOutput Class\nDESCRIPTION: Autodoc directive for generating documentation for the MarigoldNormalsOutput class in the marigold normals pipeline module.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n[[autodoc]] pipelines.marigold.pipeline_marigold_normals.MarigoldNormalsOutput\n```\n\n----------------------------------------\n\nTITLE: Decoding Latents with VAE to Generate Final Image\nDESCRIPTION: Loads the VAE model on a single GPU to decode the latent representation into the final image. Processes the latents according to VAE specifications and saves the resulting image to disk.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKL\nfrom diffusers.image_processor import VaeImageProcessor\nimport torch \n\nvae = AutoencoderKL.from_pretrained(ckpt_id, subfolder=\"vae\", torch_dtype=torch.bfloat16).to(\"cuda\")\nvae_scale_factor = 2 ** (len(vae.config.block_out_channels))\nimage_processor = VaeImageProcessor(vae_scale_factor=vae_scale_factor)\n\nwith torch.no_grad():\n    print(\"Running decoding.\")\n    latents = FluxPipeline._unpack_latents(latents, height, width, vae_scale_factor)\n    latents = (latents / vae.config.scaling_factor) + vae.config.shift_factor\n\n    image = vae.decode(latents, return_dict=False)[0]\n    image = image_processor.postprocess(image, output_type=\"pil\")\n    image[0].save(\"split_transformer.png\")\n```\n\n----------------------------------------\n\nTITLE: Controlling Video Generation with Micro-conditioning in SVD\nDESCRIPTION: Implementation that demonstrates how to use micro-conditioning parameters to control the generated video's motion characteristics. This example increases motion by adjusting the motion_bucket_id and noise_aug_strength parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/svd.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import StableVideoDiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = StableVideoDiffusionPipeline.from_pretrained(\n  \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n)\npipe.enable_model_cpu_offload()\n\n# Conditioning 이미지 불러오기\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\")\nimage = image.resize((1024, 576))\n\ngenerator = torch.manual_seed(42)\nframes = pipe(image, decode_chunk_size=8, generator=generator, motion_bucket_id=180, noise_aug_strength=0.1).frames[0]\nexport_to_video(frames, \"generated.mp4\", fps=7)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Custom Textual Inversion Tokens\nDESCRIPTION: Creates images using previously loaded textual inversion tokens in a prompt. The example generates an image of an orange llama eating ramen using custom style tokens and saves the result as a PNG file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninstance_token = \"<s0><s1>\"\nprompt = f\"a {instance_token} icon of an orange llama eating ramen, in the style of {instance_token}\"\n\nimage = pipe(prompt=prompt, num_inference_steps=25, cross_attention_kwargs={\"scale\": 1.0}).images[0]\nimage.save(\"llama.png\")\n```\n\n----------------------------------------\n\nTITLE: Creating Documentation Navigation Cards in HTML/Markdown\nDESCRIPTION: This code creates a responsive grid of navigation cards for different sections of the Diffusers documentation. Each card includes a title, description, and link styled with CSS classes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/index.md#2025-04-11_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5\">\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./tutorials/tutorial_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Tutorials</div>\n      <p class=\"text-gray-700\">Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using 🤗 Diffusers for the first time!</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./using-diffusers/loading_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">How-to guides</div>\n      <p class=\"text-gray-700\">Practical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./conceptual/philosophy\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Conceptual guides</div>\n      <p class=\"text-gray-700\">Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.</p>\n   </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./api/models/overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Reference</div>\n      <p class=\"text-gray-700\">Technical descriptions of how 🤗 Diffusers classes and methods work.</p>\n    </a>\n  </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Textual Inversion Manager\nDESCRIPTION: Initializes a DiffusersTextualInversionManager for handling textual inversions with Compel.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/weighted_prompts.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntextual_inversion_manager = DiffusersTextualInversionManager(pipe)\ncompel = Compel(\n    tokenizer=pipe.tokenizer,\n    text_encoder=pipe.text_encoder,\n    textual_inversion_manager=textual_inversion_manager)\n```\n\n----------------------------------------\n\nTITLE: Calculating MSE Loss Between Predicted and Actual Noise\nDESCRIPTION: Demonstrates the calculation of the mean squared error loss between the noise predicted by the model and the actual noise added to the image. This is the training objective for the diffusion model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch.nn.functional as F\n\n>>> noise_pred = model(noisy_image, timesteps).sample\n>>> loss = F.mse_loss(noise_pred, noise)\n```\n\n----------------------------------------\n\nTITLE: IADB Sampling Implementation\nDESCRIPTION: Core sampling algorithm implementation for the IADB (α-(de)Blending) diffusion model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndef sample_iadb(model, x0, nb_step):\n    x_alpha = x0\n    for t in range(nb_step):\n        alpha = (t/nb_step)\n        alpha_next =((t+1)/nb_step)\n\n        d = model(x_alpha, torch.tensor(alpha, device=x_alpha.device))['sample']\n        x_alpha = x_alpha + (alpha_next-alpha)*d\n\n    return x_alpha\n```\n\n----------------------------------------\n\nTITLE: Loading a Specific Pipeline Class from the Hub\nDESCRIPTION: Explicitly load a specific pipeline class for a pre-trained model, which produces the same instance as using the generic DiffusionPipeline class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\nrepo_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(repo_id)\n```\n\n----------------------------------------\n\nTITLE: Sending Requests to Diffusers Server\nDESCRIPTION: cURL command to send image generation requests to the Diffusers server API.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/create_a_server.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H \"Content-Type: application/json\" --data '{\"model\": \"something\", \"prompt\": \"a kitten in front of a fireplace\"}' http://localhost:8000/v1/images/generations\n```\n\n----------------------------------------\n\nTITLE: Single GPU Training Command\nDESCRIPTION: Command to train InstructPix2Pix model on a single GPU with mixed precision and various training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/instructpix2pix.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Block Level Group Offloading with Memory Management\nDESCRIPTION: Implements group offloading for text and transformer models using CUDA devices, enabling efficient memory usage during image-to-video generation. Supports offloading between CPU and CUDA with configurable block-level strategies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wan.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom diffusers import AutoencoderKLWan, WanTransformer3DModel, WanImageToVideoPipeline\nfrom diffusers.hooks.group_offloading import apply_group_offloading\nfrom diffusers.utils import export_to_video, load_image\nfrom transformers import UMT5EncoderModel, CLIPVisionModel\n\nmodel_id = \"Wan-AI/Wan2.1-I2V-14B-720P-Diffusers\"\nimage_encoder = CLIPVisionModel.from_pretrained(\n    model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32\n)\n\ntext_encoder = UMT5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\ntransformer = WanTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n\nonload_device = torch.device(\"cuda\")\noffload_device = torch.device(\"cpu\")\n\napply_group_offloading(text_encoder,\n    onload_device=onload_device,\n    offload_device=offload_device,\n    offload_type=\"block_level\",\n    num_blocks_per_group=4\n)\n\ntransformer.enable_group_offload(\n    onload_device=onload_device,\n    offload_device=offload_device,\n    offload_type=\"leaf_level\",\n    use_stream=True\n)\n```\n\n----------------------------------------\n\nTITLE: Long Prompt Weighting Stable Diffusion - PyTorch Implementation\nDESCRIPTION: Implements long prompt weighting for Stable Diffusion with support for emphasis control through special syntax. Allows for prompts beyond the 77 token limit and precise weight control for prompt parts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    'hakurei/waifu-diffusion',\n    custom_pipeline=\"lpw_stable_diffusion\",\n    torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nprompt = \"best_quality (1girl:1.3) bow bride brown_hair closed_mouth frilled_bow frilled_hair_tubes frills (full_body:1.3) fox_ear hair_bow hair_tubes happy hood japanese_clothes kimono long_sleeves red_bow smile solo tabi uchikake white_kimono wide_sleeves cherry_blossoms\"\nneg_prompt = \"lowres, bad_anatomy, error_body, error_hair, error_arm, error_hands, bad_hands, error_fingers, bad_fingers, missing_fingers, error_legs, bad_legs, multiple_legs, missing_legs, error_lighting, error_shadow, error_reflection, text, error, extra_digit, fewer_digits, cropped, worst_quality, low_quality, normal_quality, jpeg_artifacts, signature, watermark, username, blurry\"\n\npipe.text2img(prompt, negative_prompt=neg_prompt, width=512, height=512, max_embeddings_multiples=3).images[0]\n```\n\n----------------------------------------\n\nTITLE: Training ControlNet for Stable Diffusion 3/3.5\nDESCRIPTION: Bash script to launch the ControlNet training process using Accelerate, specifying model paths, training parameters, and validation settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sd3.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stabilityai/stable-diffusion-3-medium-diffusers\"\nexport OUTPUT_DIR=\"sd3-controlnet-out\"\n\naccelerate launch train_controlnet_sd3.py \\\n    --pretrained_model_name_or_path=$MODEL_DIR \\\n    --output_dir=$OUTPUT_DIR \\\n    --train_data_dir=\"fill50k\" \\\n    --resolution=1024 \\\n    --learning_rate=1e-5 \\\n    --max_train_steps=15000 \\\n    --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n    --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n    --validation_steps=100 \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=4\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for Training\nDESCRIPTION: Commands to initialize and configure Accelerate for distributed training of diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Using Multiple LoRA Adapters with Different Weights\nDESCRIPTION: This snippet demonstrates how to load multiple LoRA weights and combine them using different adapter weights. It loads two LoRA adapters ('dreamtraps' and 'base') and sets different weights for each to control their influence on the final image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline.load_lora_weights(\n    \"multilorahubtesting/SDXL-DreamTraps\",\n    weight_name=\"manifest.safetensors\",\n    adapter_name=\"dreamtraps\"\n)\n\npipeline.load_lora_weights(\n    \"multilorahubtesting/SDXL_CONCEPT_FEMALE\",\n    weight_name=\"manifest.safetensors\",\n    adapter_name=\"base\"\n)\n\npipeline.set_adapters(\n    [\"dreamtraps\", \"base\"],\n    adapter_weights=[0.7, 0.3]\n)\n\nprompt = \"A cinematic shot of a female space warrior standing on mars\"\nimage = pipeline(prompt=prompt, num_inference_steps=30).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Header with License\nDESCRIPTION: Contains the Apache 2.0 license header and introduces the IPNDMScheduler documentation with a reference to its original implementation source.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/ipndm.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# IPNDMScheduler\n\n`IPNDMScheduler` is a fourth-order Improved Pseudo Linear Multistep scheduler. The original implementation can be found at [crowsonkb/v-diffusion-pytorch](https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296).\n\n## IPNDMScheduler\n[[autodoc]] IPNDMScheduler\n\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput\n```\n\n----------------------------------------\n\nTITLE: HTML License Header Comment\nDESCRIPTION: Apache 2.0 license header comment for the CogView4 documentation file\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/cogview4.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Inference with Custom Diffusion for Multiple Concepts\nDESCRIPTION: Python code for generating images that combine multiple learned concepts (cat and wooden pot) from a model hosted on Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom huggingface_hub.repocard import RepoCard\nfrom diffusers import DiffusionPipeline\n\nmodel_id = \"sayakpaul/custom-diffusion-cat-wooden-pot\"\ncard = RepoCard.load(model_id)\nbase_model_id = card.data.to_dict()[\"base_model\"]\n\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16).to(\"cuda\")\npipe.unet.load_attn_procs(model_id, weight_name=\"pytorch_custom_diffusion_weights.bin\")\npipe.load_textual_inversion(model_id, weight_name=\"<new1>.bin\")\npipe.load_textual_inversion(model_id, weight_name=\"<new2>.bin\")\n\nimage = pipe(\n    \"the <new1> cat sculpture in the style of a <new2> wooden pot\",\n    num_inference_steps=100,\n    guidance_scale=6.0,\n    eta=1.0,\n).images[0]\nimage.save(\"multi-subject.png\")\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Accelerate Config in Python\nDESCRIPTION: Python code to write a basic Accelerate configuration programmatically, useful in notebook environments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Creating an Image Grid Display Function in Python\nDESCRIPTION: Implements a utility function to arrange multiple generated images into a grid for easy visualization and comparison.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\n\ndef image_grid(imgs, rows=2, cols=2):\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderDC Model from Pretrained Checkpoint in Python\nDESCRIPTION: This snippet demonstrates how to load a pretrained AutoencoderDC model using the from_pretrained method. It loads the model with float32 precision and moves it to the CUDA device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoder_dc.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderDC\n\nae = AutoencoderDC.from_pretrained(\"mit-han-lab/dc-ae-f32c32-sana-1.0-diffusers\", torch_dtype=torch.float32).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Stable Diffusion with PyTorch\nDESCRIPTION: Example command to fine-tune the Stable Diffusion model on the Naruto BLIP captions dataset using PyTorch and Accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text2image.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport dataset_name=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$dataset_name \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --mixed_precision=\"fp16\" \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Comparing CLIP Scores for Different Stable Diffusion Checkpoints\nDESCRIPTION: Generates images using two different Stable Diffusion checkpoints (v1-4 and v1-5) and compares their CLIP scores. This demonstrates how to quantitatively evaluate and compare different model versions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/evaluation.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nimages = sd_pipeline(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n\nmodel_ckpt_1_5 = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nsd_pipeline_1_5 = StableDiffusionPipeline.from_pretrained(model_ckpt_1_5, torch_dtype=weight_dtype).to(device)\n\nimages_1_5 = sd_pipeline_1_5(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n\nsd_clip_score_1_4 = calculate_clip_score(images, prompts)\nprint(f\"CLIP Score with v-1-4: {sd_clip_score_1_4}\")\n# CLIP Score with v-1-4: 34.9102\n\nsd_clip_score_1_5 = calculate_clip_score(images_1_5, prompts)\nprint(f\"CLIP Score with v-1-5: {sd_clip_score_1_5}\")\n# CLIP Score with v-1-5: 36.2137\n```\n\n----------------------------------------\n\nTITLE: Optimizing SVD with torch.compile\nDESCRIPTION: Code modification to improve inference speed by 20-25% using torch.compile on the UNet component. This approach increases memory usage slightly but provides significant performance improvement.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/svd.md#2025-04-11_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- pipe.enable_model_cpu_offload()\n+ pipe.to(\"cuda\")\n+ pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Login for Hub Integration\nDESCRIPTION: Command to log in to Hugging Face Hub for pushing trained models using the huggingface-cli tool.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Replacing VAE for Quality Improvement in Python\nDESCRIPTION: Loads a specialized autoencoder (VAE) component from Stability AI and integrates it into the pipeline to enhance the quality of generated images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.vae = vae\nimages = pipeline(**get_inputs(batch_size=8)).images\nimage_grid(images, rows=2, cols=4)\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with LTX Video GGUF Checkpoint in Python\nDESCRIPTION: Demonstrates how to load an LTX Video GGUF checkpoint and run inference to generate a video from a text prompt using the Diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ltx_video.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers.utils import export_to_video\nfrom diffusers import LTXPipeline, LTXVideoTransformer3DModel, GGUFQuantizationConfig\n\nckpt_path = (\n    \"https://huggingface.co/city96/LTX-Video-gguf/blob/main/ltx-video-2b-v0.9-Q3_K_S.gguf\"\n)\ntransformer = LTXVideoTransformer3DModel.from_single_file(\n    ckpt_path,\n    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n    torch_dtype=torch.bfloat16,\n)\npipe = LTXPipeline.from_pretrained(\n    \"Lightricks/LTX-Video\",\n    transformer=transformer,\n    torch_dtype=torch.bfloat16,\n)\npipe.enable_model_cpu_offload()\n\nprompt = \"A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage\"\nnegative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\n\nvideo = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=704,\n    height=480,\n    num_frames=161,\n    num_inference_steps=50,\n).frames[0]\nexport_to_video(video, \"output_gguf_ltx.mp4\", fps=24)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for Training\nDESCRIPTION: Commands to install Diffusers from source and set up the training environment with required dependencies for advanced diffusion training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to Fork - Git Bash\nDESCRIPTION: Pushes local changes from the new feature branch to the developer's GitHub fork, making them available for review via a pull request.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ git push -u origin a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Running DreamBooth LoRA Training for SDXL\nDESCRIPTION: Bash script to launch DreamBooth LoRA training for SDXL with recommended parameters, including environment variable setup and accelerate launch command.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"lora-trained-xl\"\nexport VAE_PATH=\"madebyollin/sdxl-vae-fp16-fix\"\n\naccelerate launch train_dreambooth_lora_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --pretrained_vae_model_name_or_path=$VAE_PATH \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"fp16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Quantized Pipeline Setup\nDESCRIPTION: Shows how to set up a quantized pipeline using 8-bit quantization with bitsandbytes for reduced memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/lumina.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, Transformer2DModel, LuminaPipeline\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"Alpha-VLLM/Lumina-Next-SFT-diffusers\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = Transformer2DModel.from_pretrained(\n    \"Alpha-VLLM/Lumina-Next-SFT-diffusers\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = LuminaPipeline.from_pretrained(\n    \"Alpha-VLLM/Lumina-Next-SFT-diffusers\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\nimage = pipeline(prompt).images[0]\nimage.save(\"lumina.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Weights from Civitai\nDESCRIPTION: This code shows how to load a Stable Diffusion v1.5 pipeline and apply LoRA weights downloaded from Civitai. The process involves downloading the safetensors file from Civitai, saving it locally, and then loading it using the load_lora_weights method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\npipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n\n# Load LoRA weights\npipeline.load_lora_weights(\"./\", weight_name=\"more_details.safetensors\")\n\n# Use the pipeline with LoRA weights\nprompt = \"a cute kitten playing with a ball, highly detailed\"\nimage = pipeline(prompt=prompt, num_inference_steps=25).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Initializing GaudiStableDiffusionPipeline with Optimized Scheduler for Habana Gaudi\nDESCRIPTION: Sets up a Stable Diffusion pipeline optimized for Habana Gaudi processors. This code creates a GaudiDDIMScheduler and GaudiStableDiffusionPipeline with HPU deployment and graph optimization enabled, specifying the pre-configured Gaudi configuration from Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/habana.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.habana import GaudiConfig\nfrom optimum.habana.diffusers import GaudiDDIMScheduler, GaudiStableDiffusionPipeline\n\nmodel_name = \"stabilityai/stable-diffusion-2-base\"\nscheduler = GaudiDDIMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\npipeline = GaudiStableDiffusionPipeline.from_pretrained(\n    model_name,\n    scheduler=scheduler,\n    use_habana=True,\n    use_hpu_graphs=True,\n    gaudi_config=\"Habana/stable-diffusion\",\n)\n```\n\n----------------------------------------\n\nTITLE: Launching DreamBooth LoRA Training\nDESCRIPTION: Complete command for launching a DreamBooth LoRA training process with Accelerate. Includes parameters for model configuration, learning rate, validation settings, and push to hub functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/lora.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --checkpointing_steps=100 \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=50 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Running DreamBooth with LoRA for SD3\nDESCRIPTION: Bash script to run DreamBooth training with LoRA (Low-Rank Adaptation) for Stable Diffusion 3, which requires fewer parameters to train.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-3-medium-diffusers\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"trained-sd3-lora\"\n\naccelerate launch train_dreambooth_lora_sd3.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"fp16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --learning_rate=4e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: HTML License Comment in Markdown\nDESCRIPTION: Apache 2.0 license header comment for the documentation file, specifying copyright and usage terms.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/k_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: CSV Format for CogVideoX Training Data\nDESCRIPTION: Alternative CSV file format for organizing training data with caption and video path columns that can be loaded using the datasets library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\n<CAPTION_COLUMN>,<PATH_TO_VIDEO_COLUMN>\n\"\"\"A black and white animated sequence featuring a rabbit, named Rabbity Ribfried, and an anthropomorphic goat in a musical, playful environment, showcasing their evolving interaction.\"\"\",\"\"\"00000.mp4\"\"\"\n\"\"\"A black and white animated sequence on a ship's deck features a bulldog character, named Bully Bulldoger, showcasing exaggerated facial expressions and body language. The character progresses from confident to focused, then to strained and distressed, displaying a range of emotions as it navigates challenges. The ship's interior remains static in the background, with minimalistic details such as a bell and open door. The character's dynamic movements and changing expressions drive the narrative, with no camera movement to distract from its evolving reactions and physical gestures.\"\"\",\"\"\"00001.mp4\"\"\"\n...\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Stable Diffusion with Custom Dataset using Flax\nDESCRIPTION: Example command to fine-tune the Stable Diffusion model on a custom local dataset using Flax.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text2image.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport TRAIN_DIR=\"path_to_your_dataset\"\n\npython train_text_to_image_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$TRAIN_DIR \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --mixed_precision=\"fp16\" \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up PyTorch and PyTorch/XLA on TPU\nDESCRIPTION: This code snippet installs the required packages for running PyTorch and PyTorch/XLA on TPU devices. Specific versions are targeted to ensure compatibility and optimal performance with TPU hardware.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/inference/flux/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install torch~=2.5.0 torch_xla[tpu]~=2.5.0 -f https://storage.googleapis.com/libtpu-releases/index.html -f https://storage.googleapis.com/libtpu-wheels/index.html\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Instructions for cloning and installing the Diffusers library from source code to ensure compatibility with example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/autoencoderkl/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Loading a Pipeline with Float16 Precision in Python\nDESCRIPTION: Initializes the diffusion pipeline with float16 precision for improved performance. This significantly reduces memory usage and increases generation speed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Zero3 Configuration for Distributed Training\nDESCRIPTION: YAML configuration for DeepSpeed Zero3 optimization, which enables training larger models across multiple GPUs with memory offloading.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 8\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 8\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Running FLUX.1 inference script\nDESCRIPTION: Command to execute the Python script that performs inference using the FLUX.1 diffusion model. The script loads text encoders on CPU and Flux transformer/VAE models on TPU.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/inference/flux/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython flux_inference.py\n```\n\n----------------------------------------\n\nTITLE: Additional Memory Optimization Flags for Low VRAM\nDESCRIPTION: Additional command-line arguments to enable training on GPUs with less than 16GB VRAM, utilizing memory optimization techniques like gradient checkpointing and 8-bit Adam.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n  --enable_xformers_memory_efficient_attention \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --mixed_precision=\"fp16\" \\\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Kandinsky 2.1\nDESCRIPTION: Sets up the prior pipeline and main pipeline for Kandinsky 2.1, encodes the prompt, generates image embeddings, and creates an image from the text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyPriorPipeline, KandinskyPipeline\nimport torch\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16).to(\"cuda\")\npipeline = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\"\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt, guidance_scale=1.0).to_tuple()\n\nimage = pipeline(prompt, image_embeds=image_embeds, negative_prompt=negative_prompt, negative_image_embeds=negative_image_embeds, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Text-to-3D Generation with ShapEPipeline\nDESCRIPTION: Code for generating 3D objects from text prompts using the ShapEPipeline. This example shows how to initialize the model, set parameters like guidance scale, and pass text prompts to generate image frames of 3D objects.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import ShapEPipeline\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = pipe.to(device)\n\nguidance_scale = 15.0\nprompt = [\"A firecracker\", \"A birthday cupcake\"]\n\nimages = pipe(\n    prompt,\n    guidance_scale=guidance_scale,\n    num_inference_steps=64,\n    frame_size=256,\n).images\n```\n\n----------------------------------------\n\nTITLE: Displaying NVIDIA GPU Information using nvidia-smi in Bash\nDESCRIPTION: This snippet shows the output of the nvidia-smi command, displaying information about the NVIDIA GPUs in the system, including their model, memory usage, and power consumption.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/tests/quantization/bnb/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n| 30%   55C    P0             61W /  450W |       1MiB /  24564MiB |      2%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 4090        Off |   00000000:13:00.0 Off |                  Off |\n| 30%   51C    P0             60W /  450W |       1MiB /  24564MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n```\n\n----------------------------------------\n\nTITLE: Loading Base Model and LoRA Adapters in Python\nDESCRIPTION: Shows how to load a base diffusion model and multiple LoRA adapters, then set adapter weights for combining them. Uses SDXL as the base model with two different LoRA adapters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\", weight_name=\"ikea_instructions_xl_v1_5.safetensors\", adapter_name=\"ikea\")\npipeline.load_lora_weights(\"lordjia/by-feng-zikai\", weight_name=\"fengzikai_v1.0_XL.safetensors\", adapter_name=\"feng\")\n\npipeline.set_adapters([\"ikea\", \"feng\"], adapter_weights=[0.7, 0.8])\n```\n\n----------------------------------------\n\nTITLE: Loading TheLastBen-Trained LoRA Models in Diffusers\nDESCRIPTION: This snippet demonstrates how to load a LoRA checkpoint trained with TheLastBen's tools. It shows how to load a William Eggleston style LoRA from Hugging Face and use it with Stable Diffusion XL for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_lora_weights(\"TheLastBen/William_Eggleston_Style_SDXL\", weight_name=\"wegg.safetensors\")\n\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \"a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\"\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Commands to initialize and configure the Accelerate environment for distributed training setup.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: In-painting using Stable Diffusion in Python\nDESCRIPTION: This example demonstrates how to use StableDiffusionInpaintPipeline for in-painting. It loads the pipeline, downloads an initial image and a mask, and then generates a new image by in-painting a specific part of the image based on a text prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport PIL\nimport requests\nimport torch\nfrom io import BytesIO\n\nfrom diffusers import StableDiffusionInpaintPipeline\n\ndef download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = download_image(img_url).resize((512, 512))\nmask_image = download_image(mask_url).resize((512, 512))\n\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    torch_dtype=torch.float16,\n)\npipe = pipe.to(\"cuda\")\n\nprompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\nimage = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: UNet Model Implementation Reference\nDESCRIPTION: Reference documentation for UNet2DModel and UNet2DOutput classes from the Diffusers library. These components implement the 2D UNet architecture for image processing tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet2d.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[[autodoc]] UNet2DModel\\n\\n## UNet2DOutput\\n[[autodoc]] models.unets.unet_2d.UNet2DOutput\n```\n\n----------------------------------------\n\nTITLE: Basic DreamBooth Fine-tuning with PyTorch\nDESCRIPTION: Command to launch DreamBooth fine-tuning with basic parameters using the Accelerate library. This trains only the UNet without prior preservation loss.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=400\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Images for Decoder Model Training\nDESCRIPTION: Defines a preprocessing function for the decoder model that converts images to RGB and applies both standard transforms and CLIP image processing. This prepares the data for training the decoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_train(examples):\n    images = [image.convert(\"RGB\") for image in examples[image_column]]\n    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n    examples[\"clip_pixel_values\"] = image_processor(images, return_tensors=\"pt\").pixel_values\n    return examples\n```\n\n----------------------------------------\n\nTITLE: Importing LEditsPPPipelineStableDiffusion in Python\nDESCRIPTION: This snippet shows how to import the LEditsPPPipelineStableDiffusion class, which is the implementation of LEDITS++ for Stable Diffusion models. It includes all methods and the __call__ and invert functions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ledits_pp.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.ledits_pp import LEditsPPPipelineStableDiffusion\n```\n\n----------------------------------------\n\nTITLE: Configuring LoRA U-net Blocks for Training\nDESCRIPTION: Command line flag to specify which U-net blocks should be trained during LoRA fine-tuning. This allows targeted training of specific attention blocks within the U-net architecture.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n--lora_unet_blocks=\"unet.up_blocks.0.attentions.0,unet.up_blocks.0.attentions.1\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Generator with Seed in Python\nDESCRIPTION: Initializes a PyTorch generator with a fixed seed for reproducible image generation. This allows comparing results across different optimization techniques.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n```\n\n----------------------------------------\n\nTITLE: Installing PEFT for DoRA Training\nDESCRIPTION: Command to upgrade PEFT library installation for enabling DoRA training functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install -U peft\n```\n\n----------------------------------------\n\nTITLE: Sample output from FLUX.1 inference\nDESCRIPTION: Example console output showing the loading and compilation process for the FLUX.1 model. The output demonstrates the expected progress bars, compilation steps, and timing information for inference runs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/inference/flux/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.06it/s]\nLoading pipeline components...:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                | 3/5 [00:00<00:00,  6.80it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.28it/s]\n2025-03-14 21:17:53 [info     ] loading flux from black-forest-labs/FLUX.1-dev\n2025-03-14 21:17:53 [info     ] loading flux from black-forest-labs/FLUX.1-dev\nLoading pipeline components...:   0%|                                                                                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]2025-03-14 21:17:53 [info     ] loading flux from black-forest-labs/FLUX.1-dev\n2025-03-14 21:17:53 [info     ] loading flux from black-forest-labs/FLUX.1-dev\nLoading pipeline components...:   0%|                                                                                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]2025-03-14 21:17:54 [info     ] loading flux from black-forest-labs/FLUX.1-dev\n2025-03-14 21:17:54 [info     ] loading flux from black-forest-labs/FLUX.1-dev\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.66it/s]\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.48it/s]\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.32it/s]\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.69it/s]\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.74it/s]\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.10it/s]\n2025-03-14 21:17:56 [info     ] loading flux from black-forest-labs/FLUX.1-dev\nLoading pipeline components...:   0%|                                                                                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]2025-03-14 21:17:56 [info     ] loading flux from black-forest-labs/FLUX.1-dev\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.55it/s]\nLoading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]\n2025-03-14 21:18:34 [info     ] starting compilation run...   \n2025-03-14 21:18:37 [info     ] starting compilation run...   \n2025-03-14 21:18:38 [info     ] starting compilation run...   \n2025-03-14 21:18:39 [info     ] starting compilation run...   \n2025-03-14 21:18:41 [info     ] starting compilation run...   \n2025-03-14 21:18:41 [info     ] starting compilation run...   \n2025-03-14 21:18:42 [info     ] starting compilation run...   \n2025-03-14 21:18:43 [info     ] starting compilation run...   \n 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                | 23/28 [13:35<03:04, 36.80s/it]2025-03-14 21:33:42.057559: W torch_xla/csrc/runtime/pjrt_computation_client.cc:667] Failed to deserialize executable: INTERNAL: TfrtTpuExecutable proto deserialization failed while parsing core program!\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [16:27<00:00, 35.28s/it]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [16:27<00:00, 35.26s/it]\n2025-03-14 21:36:38 [info     ] compilation took 1079.3314765350078 sec.\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [16:12<00:00, 34.73s/it]\n2025-03-14 21:36:38 [info     ] starting inference run...     \n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [16:12<00:00, 34.73s/it]\n2025-03-14 21:36:38 [info     ] compilation took 1081.89390801001 sec.\n2025-03-14 21:36:39 [info     ] starting inference run...     \n2025-03-14 21:36:39 [info     ] compilation took 1077.1543154849933 sec.\n2025-03-14 21:36:39 [info     ] compilation took 1075.7239800530078 sec.\n2025-03-14 21:36:39 [info     ] starting inference run...     \n2025-03-14 21:36:40 [info     ] starting inference run...     \n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [16:22<00:00, 35.10s/it]\n2025-03-14 21:36:50 [info     ] compilation took 1088.1632604240003 sec.\n2025-03-14 21:36:50 [info     ] starting inference run...     \n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [16:28<00:00, 35.32s/it]\n2025-03-14 21:36:55 [info     ] compilation took 1096.8027802760043 sec.\n2025-03-14 21:36:56 [info     ] starting inference run...     \n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [16:59<00:00, 36.40s/it]\n```\n\n----------------------------------------\n\nTITLE: DreamBooth LoRA with Text Encoder Training\nDESCRIPTION: Bash script to run DreamBooth LoRA training with text encoder fine-tuning enabled, which adapts both the UNet and CLIP text encoders for improved concept learning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stabilityai/stable-diffusion-3-medium-diffusers\"\nexport OUTPUT_DIR=\"trained-sd3-lora\"\n\naccelerate launch train_dreambooth_lora_sd3.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --output_dir=$OUTPUT_DIR \\\n  --dataset_name=\"Norod78/Yarn-art-style\" \\\n  --instance_prompt=\"a photo of TOK yarn art dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --train_text_encoder\\\n  --gradient_accumulation_steps=1 \\\n  --optimizer=\"prodigy\"\\\n  --learning_rate=1.0 \\\n  --text_encoder_lr=1.0 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=1500 \\\n  --rank=32 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Checking CUDA Version in Python\nDESCRIPTION: This snippet runs a shell command in a Python environment to check the installed version of CUDA, which is a prerequisite for GPU accelerated tasks in PyTorch.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!nvcc --version\n```\n\n----------------------------------------\n\nTITLE: Generating Image from Text using SDXL Turbo\nDESCRIPTION: This code snippet demonstrates how to use SDXL Turbo for text-to-image generation. It sets up the pipeline, defines a prompt, and generates an image with specific parameters like guidance scale and inference steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline_text2image = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline_text2image = pipeline_text2image.to(\"cuda\")\n\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n\nimage = pipeline_text2image(prompt=prompt, guidance_scale=0.0, num_inference_steps=1).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Disabling Advisory Warnings Using Environment Variables\nDESCRIPTION: Example of how to disable advisory warnings by setting the DIFFUSERS_NO_ADVISORY_WARNINGS environment variable.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/logging.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDIFFUSERS_NO_ADVISORY_WARNINGS=1 ./myprogram.py\n```\n\n----------------------------------------\n\nTITLE: Creating Accelerate Config Programmatically\nDESCRIPTION: Python code to create a basic Accelerate configuration in environments that don't support interactive shells, such as notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Displaying Community Pipeline Examples Table in Markdown\nDESCRIPTION: This code snippet shows a markdown table listing various community pipeline examples for the Hugging Face Diffusers library. It includes columns for the example name, description, code example reference, Colab notebook link, and author information.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Example | Description | Code Example | Colab | Author |\n|:--------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------:|\n|Spatiotemporal Skip Guidance (STG)|[Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling](https://arxiv.org/abs/2411.18664) (CVPR 2025) enhances video diffusion models by generating a weaker model through layer skipping and using it as guidance, improving fidelity in models like HunyuanVideo, LTXVideo, and Mochi.|[Spatiotemporal Skip Guidance](#spatiotemporal-skip-guidance)|-|[Junha Hyung](https://junhahyung.github.io/), [Kinam Kim](https://kinam0252.github.io/), and [Ednaordinary](https://github.com/Ednaordinary)|\n|Adaptive Mask Inpainting|Adaptive Mask Inpainting algorithm from [Beyond the Contact: Discovering Comprehensive Affordance for 3D Objects from Pre-trained 2D Diffusion Models](https://github.com/snuvclab/coma) (ECCV '24, Oral) provides a way to insert human inside the scene image without altering the background, by inpainting with adapting mask.|[Adaptive Mask Inpainting](#adaptive-mask-inpainting)|-|[Hyeonwoo Kim](https://sshowbiz.xyz),[Sookwan Han](https://jellyheadandrew.github.io)|\n|Flux with CFG|[Flux with CFG](https://github.com/ToTheBeginning/PuLID/blob/main/docs/pulid_for_flux.md) provides an implementation of using CFG in [Flux](https://blackforestlabs.ai/announcing-black-forest-labs/).|[Flux with CFG](#flux-with-cfg)|[Notebook](https://github.com/huggingface/notebooks/blob/main/diffusers/flux_with_cfg.ipynb)|[Linoy Tsaban](https://github.com/linoytsaban), [Apolinário](https://github.com/apolinario), and [Sayak Paul](https://github.com/sayakpaul)|\n|Differential Diffusion|[Differential Diffusion](https://github.com/exx8/differential-diffusion) modifies an image according to a text prompt, and according to a map that specifies the amount of change in each region.|[Differential Diffusion](#differential-diffusion)|[![Hugging Face Space](https://img.shields.io/badge/🤗%20Hugging%20Face-Space-yellow)](https://huggingface.co/spaces/exx8/differential-diffusion) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/exx8/differential-diffusion/blob/main/examples/SD2.ipynb)|[Eran Levin](https://github.com/exx8) and [Ohad Fried](https://www.ohadf.com/)|\n| HD-Painter | [HD-Painter](https://github.com/Picsart-AI-Research/HD-Painter) enables prompt-faithfull and high resolution (up to 2k) image inpainting upon any diffusion-based image inpainting method. | [HD-Painter](#hd-painter) | [![Hugging Face Space](https://img.shields.io/badge/🤗%20Hugging%20Face-Space-yellow)](https://huggingface.co/spaces/PAIR/HD-Painter) | [Manukyan Hayk](https://github.com/haikmanukyan) and [Sargsyan Andranik](https://github.com/AndranikSargsyan) |\n| Marigold Monocular Depth Estimation | A universal monocular depth estimator, utilizing Stable Diffusion, delivering sharp predictions in the wild. (See the [project page](https://marigoldmonodepth.github.io) and [full codebase](https://github.com/prs-eth/marigold) for more details.) | [Marigold Depth Estimation](#marigold-depth-estimation) | [![Hugging Face Space](https://img.shields.io/badge/🤗%20Hugging%20Face-Space-yellow)](https://huggingface.co/spaces/toshas/marigold) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/12G8reD13DdpMie5ZQlaFNo2WCGeNUH-u?usp=sharing) | [Bingxin Ke](https://github.com/markkua) and [Anton Obukhov](https://github.com/toshas) |\n| LLM-grounded Diffusion (LMD+) | LMD greatly improves the prompt following ability of text-to-image generation models by introducing an LLM as a front-end prompt parser and layout planner. [Project page.](https://llm-grounded-diffusion.github.io/) [See our full codebase (also with diffusers).](https://github.com/TonyLianLong/LLM-groundedDiffusion) | [LLM-grounded Diffusion (LMD+)](#llm-grounded-diffusion) | [Huggingface Demo](https://huggingface.co/spaces/longlian/llm-grounded-diffusion) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SXzMSeAB-LJYISb2yrUOdypLz4OYWUKj) | [Long (Tony) Lian](https://tonylian.com/) |\n| CLIP Guided Stable Diffusion | Doing CLIP guidance for text to image generation with Stable Diffusion | [CLIP Guided Stable Diffusion](#clip-guided-stable-diffusion) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/CLIP_Guided_Stable_diffusion_with_diffusers.ipynb) | [Suraj Patil](https://github.com/patil-suraj/) |\n```\n\n----------------------------------------\n\nTITLE: Loading Pipeline from Local Files with Hub Cache\nDESCRIPTION: Demonstrates loading a pipeline from locally cached files downloaded from the Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\nmy_local_checkpoint_path = hf_hub_download(\n    repo_id=\"segmind/SSD-1B\",\n    filename=\"SSD-1B.safetensors\"\n)\n\nmy_local_config_path = snapshot_download(\n    repo_id=\"segmind/SSD-1B\",\n    allow_patterns=[\"*.json\", \"**/*.json\", \"*.txt\", \"**/*.txt\"]\n)\n\npipeline = StableDiffusionXLPipeline.from_single_file(my_local_checkpoint_path, config=my_local_config_path, local_files_only=True)\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22ControlnetImg2ImgPipeline in Python\nDESCRIPTION: This snippet defines the KandinskyV22ControlnetImg2ImgPipeline class, which incorporates ControlNet functionality into the Kandinsky 2.2 image-to-image pipeline for more controlled image transformations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22ControlnetImg2ImgPipeline\n\t- all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Setting Up Prior Model and Optimizer\nDESCRIPTION: Initializes the PriorTransformer model and configures its optimizer with learning parameters. The prior model is responsible for generating image embeddings in the Kandinsky pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprior = PriorTransformer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder=\"prior\")\nprior.train()\noptimizer = optimizer_cls(\n    prior.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Compiler Settings\nDESCRIPTION: Sets up compiler configuration flags for optimizing model performance including conv_1x1_as_mm and coordinate descent tuning\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\n# Notice the two new flags at the end.\ntorch._inductor.config.conv_1x1_as_mm = True\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.epilogue_fusion = False\ntorch._inductor.config.coordinate_descent_check_all_directions = True\ntorch._inductor.config.force_fuse_int_mm_with_mul = True\ntorch._inductor.config.use_mixed_mm = True\n```\n\n----------------------------------------\n\nTITLE: Installing Virtual Environment with UV\nDESCRIPTION: Creates and activates a Python virtual environment using the UV package manager\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv venv my-env\nsource my-env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Installing Testing Dependencies - Python Bash\nDESCRIPTION: Installs additional dependencies required for running tests, ensuring that all necessary packages for testing are available.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -e \".[test]\"\n```\n\n----------------------------------------\n\nTITLE: Loading CogView4Transformer2DModel in Python\nDESCRIPTION: This snippet demonstrates how to load the CogView4Transformer2DModel from a pretrained checkpoint. It uses the 'from_pretrained' method to load the model, specifying the model name, subfolder, and data type.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/cogview4_transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import CogView4Transformer2DModel\n\ntransformer = CogView4Transformer2DModel.from_pretrained(\"THUDM/CogView4-6B\", subfolder=\"transformer\", torch_dtype=torch.bfloat16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Diffusers\nDESCRIPTION: Lists required Python packages and their minimum versions for running the Diffusers library. Includes core ML dependencies like accelerate, transformers, and PyTorch extensions, along with utilities for text processing and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.21.0\nftfy\ntensorboard\nJinja2\nintel_extension_for_pytorch>=1.13\n```\n\n----------------------------------------\n\nTITLE: Targeting Specific Blocks with LoRA Training\nDESCRIPTION: Command-line flags to specify which transformer blocks to target during LoRA training, allowing for fine-grained control over which parts of the model are adapted.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n--lora_blocks \"12,13,14,15,16,17,18,19,20,21,22,23,24,30,31,32,33,34,35,36,37\"\n```\n\n----------------------------------------\n\nTITLE: Running Slow Tests\nDESCRIPTION: Command to run slow tests with environment variable setting\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./tests/\n```\n\n----------------------------------------\n\nTITLE: Generating Interpolated Image with Kandinsky 2.2\nDESCRIPTION: Performs the interpolation process with Kandinsky 2.2 by generating embeddings from the prior pipeline and then using them with the main pipeline to create the final image. The approach is similar to Kandinsky 2.1 but uses the updated model architecture.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# prompt can be left empty\nprompt = \"\"\nprior_out = prior_pipeline.interpolate(images_texts, weights)\n\npipeline = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\nimage = pipeline(prompt, **prior_out, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading and Using IP-Adapter\nDESCRIPTION: This snippet shows how to load a Stable Diffusion model and add IP-Adapter weights to guide image generation with both text and image prompts. The IP-Adapter is a lightweight adapter that allows image prompts with diffusion models by separating cross-attention layers for image and text features.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading_adapters.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\nfrom diffusers.utils import load_image\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Combined ControlNet and IP-Adapter\nDESCRIPTION: Performs image generation using both ControlNet and IP-Adapter guidance. The code passes both the depth map for structural control and the reference image for appearance guidance to produce an image that combines aspects of both inputs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/ip_adapter.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ngenerator = torch.Generator(device=\"cpu\").manual_seed(33)\nimage = pipeline(\n    prompt=\"best quality, high quality\",\n    image=depth_map,\n    ip_adapter_image=ip_adapter_image,\n    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n    num_inference_steps=50,\n    generator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Skipping Specific Modules in 8-bit Quantization for Stable Diffusion 3\nDESCRIPTION: Example of loading a Stable Diffusion 3 model in 8-bit precision while skipping the quantization of specific modules (proj_out) that might cause instability when quantized. This selective approach can improve model performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import SD3Transformer2DModel, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True, llm_int8_skip_modules=[\"proj_out\"],\n)\n\nmodel_8bit = SD3Transformer2DModel.from_pretrained(\n    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n    subfolder=\"transformer\",\n    quantization_config=quantization_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with T2I-Adapter and Stable Diffusion 1.5\nDESCRIPTION: Code to generate an image using the StableDiffusionAdapterPipeline with T2I-Adapter. The pipeline takes a text prompt and the canny edge image as inputs, and generates an image that follows both the textual description and the structural guidance from the canny image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/t2i_adapter.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\nimage = pipeline(\n    prompt=\"cinematic photo of a plush and soft midcentury style rug on a wooden floor, 35mm photograph, film, professional, 4k, highly detailed\",\n    image=image,\n    generator=generator,\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Fine-tuning with Prior-Preservation Loss in PyTorch\nDESCRIPTION: Command to launch DreamBooth training with prior-preservation loss to prevent overfitting and language drift. Uses class images that match the subject category.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements\nDESCRIPTION: Command to install additional dependencies from requirements.txt file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Committing Changes\nDESCRIPTION: Commands to stage and commit changes to the local repository\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ git add modified_file.py\n$ git commit -m \"A descriptive message about your changes.\"\n```\n\n----------------------------------------\n\nTITLE: Training LoRA Control Model\nDESCRIPTION: Command for launching fine-tuning with pose conditions using accelerate\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogview4-control/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_control_lora_cogview4.py \\\n  --pretrained_model_name_or_path=\"THUDM/CogView4-6B\" \\\n  --dataset_name=\"raulc0399/open_pose_controlnet\" \\\n  --output_dir=\"pose-control-lora\" \\\n  --mixed_precision=\"bf16\" \\\n  --train_batch_size=1 \\\n  --rank=64 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=5000 \\\n  --validation_image=\"openpose.png\" \\\n  --validation_prompt=\"A couple, 4k photo, highly detailed\" \\\n  --offload \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Overriding Pipeline Configuration\nDESCRIPTION: Example of overriding default pipeline configuration parameters during initialization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLInstructPix2PixPipeline\n\nckpt_path = \"https://huggingface.co/stabilityai/cosxl/blob/main/cosxl_edit.safetensors\"\npipeline = StableDiffusionXLInstructPix2PixPipeline.from_single_file(ckpt_path, config=\"diffusers/sdxl-instructpix2pix-768\", is_cosxl_edit=True)\n```\n\n----------------------------------------\n\nTITLE: Enabling xFormers Optimization for Kandinsky\nDESCRIPTION: Demonstrates how to enable xFormers memory efficient attention for improved inference performance with Kandinsky models when using PyTorch versions below 2.0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipe.enable_xformers_memory_efficient_attention()\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion XL Prompt Scheduling Callback\nDESCRIPTION: Defines a callback class for scheduling prompts during Stable Diffusion XL generation. Extends the base callback functionality with additional SDXL-specific features like text embeddings and time IDs. Includes pipeline setup and example usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README_community_scripts.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers.callbacks import PipelineCallback, MultiPipelineCallbacks\nfrom diffusers.configuration_utils import register_to_config\nimport torch\nfrom typing import Any, Dict, Tuple, Union\n\n\nclass SDXLPromptSchedulingCallback(PipelineCallback):\n    @register_to_config\n    def __init__(\n        self,\n        encoded_prompt: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n        add_text_embeds: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n        add_time_ids: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n        cutoff_step_ratio=None,\n        cutoff_step_index=None,\n    ):\n        super().__init__(\n            cutoff_step_ratio=cutoff_step_ratio, cutoff_step_index=cutoff_step_index\n        )\n\n    tensor_inputs = [\"prompt_embeds\", \"add_text_embeds\", \"add_time_ids\"]\n\n    def callback_fn(\n        self, pipeline, step_index, timestep, callback_kwargs\n    ) -> Dict[str, Any]:\n        cutoff_step_ratio = self.config.cutoff_step_ratio\n        cutoff_step_index = self.config.cutoff_step_index\n        if isinstance(self.config.encoded_prompt, tuple):\n            prompt_embeds, negative_prompt_embeds = self.config.encoded_prompt\n        else:\n            prompt_embeds = self.config.encoded_prompt\n        if isinstance(self.config.add_text_embeds, tuple):\n            add_text_embeds, negative_add_text_embeds = self.config.add_text_embeds\n        else:\n            add_text_embeds = self.config.add_text_embeds\n        if isinstance(self.config.add_time_ids, tuple):\n            add_time_ids, negative_add_time_ids = self.config.add_time_ids\n        else:\n            add_time_ids = self.config.add_time_ids\n\n        # Use cutoff_step_index if it's not None, otherwise use cutoff_step_ratio\n        cutoff_step = (\n            cutoff_step_index\n            if cutoff_step_index is not None\n            else int(pipeline.num_timesteps * cutoff_step_ratio)\n        )\n\n        if step_index == cutoff_step:\n            if pipeline.do_classifier_free_guidance:\n                prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n                add_text_embeds = torch.cat([negative_add_text_embeds, add_text_embeds])\n                add_time_ids = torch.cat([negative_add_time_ids, add_time_ids])\n            callback_kwargs[self.tensor_inputs[0]] = prompt_embeds\n            callback_kwargs[self.tensor_inputs[1]] = add_text_embeds\n            callback_kwargs[self.tensor_inputs[2]] = add_time_ids\n        return callback_kwargs\n\n\npipeline: StableDiffusionXLPipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n).to(\"cuda\")\n\ncallbacks = []\nfor index in range(1, 20):\n    (\n        prompt_embeds,\n        negative_prompt_embeds,\n        pooled_prompt_embeds,\n        negative_pooled_prompt_embeds,\n    ) = pipeline.encode_prompt(\n        prompt=f\"prompt {index}\",\n        negative_prompt=f\"prompt {index}\",\n        device=pipeline._execution_device,\n        num_images_per_prompt=1,\n        # pipeline.do_classifier_free_guidance can't be accessed until after pipeline is ran\n        do_classifier_free_guidance=True,\n    )\n    text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n    add_time_ids = pipeline._get_add_time_ids(\n        (1024, 1024),\n        (0, 0),\n        (1024, 1024),\n        dtype=prompt_embeds.dtype,\n        text_encoder_projection_dim=text_encoder_projection_dim,\n    )\n    negative_add_time_ids = pipeline._get_add_time_ids(\n        (1024, 1024),\n        (0, 0),\n        (1024, 1024),\n        dtype=prompt_embeds.dtype,\n        text_encoder_projection_dim=text_encoder_projection_dim,\n    )\n    callbacks.append(\n        SDXLPromptSchedulingCallback(\n            encoded_prompt=(prompt_embeds, negative_prompt_embeds),\n            add_text_embeds=(pooled_prompt_embeds, negative_pooled_prompt_embeds),\n            add_time_ids=(add_time_ids, negative_add_time_ids),\n            cutoff_step_index=index,\n        )\n    )\n\n\ncallback = MultiPipelineCallbacks(callbacks)\n\nimage = pipeline(\n    prompt=\"prompt\",\n    negative_prompt=\"negative prompt\",\n    callback_on_step_end=callback,\n    callback_on_step_end_tensor_inputs=[\n        \"prompt_embeds\",\n        \"add_text_embeds\",\n        \"add_time_ids\",\n    ],\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Prior-Preservation Loss\nDESCRIPTION: This bash script runs training on the Dreambooth model utilizing prior-preservation to prevent overfitting. Clas and instance images directories must be set, and training parameters include prior-preservation loss weight and prior image generation. Input parameters include specific prompts for instances and classes, and the script expects around 200-300 class images.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/dreambooth_inpaint/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"runwayml/stable-diffusion-inpainting\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth_inpaint.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements\nDESCRIPTION: Installing necessary dependencies from requirements file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running SDXL Ensemble Denoising Pipeline\nDESCRIPTION: Complete example of executing the SDXL ensemble denoising process, where the base model handles high noise removal and the refiner handles low noise refinement. Uses denoising_end and denoising_start parameters to coordinate the handoff.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A majestic lion jumping from a big stone at night\"\n\nimage = base(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_end=high_noise_frac,\n    output_type=\"latent\",\n).images\nimage = refiner(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_start=high_noise_frac,\n    image=image,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Cloning the Diffusers Repository (Bash)\nDESCRIPTION: Command to clone the Hugging Face Diffusers repository, which is required to access and use the training examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\n```\n\n----------------------------------------\n\nTITLE: StyleAligned Image Generation Pipeline\nDESCRIPTION: Implementation of style-aligned image generation using shared attention. Enables consistent style across multiple generated images without requiring fine-tuning or manual intervention.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_97\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom PIL import Image\n\nmodel_id = \"a-r-r-o-w/dreamshaper-xl-turbo\"\npipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\", custom_pipeline=\"pipeline_sdxl_style_aligned\")\npipe = pipe.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Reverting to Default Attention Processor\nDESCRIPTION: Python code demonstrating how to revert to the default attention processor in Diffusers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.models.attention_processor import AttnProcessor\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\npipe.unet.set_default_attn_processor()\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Enabling EDM Training Configuration\nDESCRIPTION: Simple diff showing how to enable EDM-style training by adding a command line flag for SDXL model training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_9\n\nLANGUAGE: diff\nCODE:\n```\n+  --do_edm_style_training \\\n```\n\n----------------------------------------\n\nTITLE: Initializing Stable Diffusion Pipeline on MPS Device\nDESCRIPTION: Demonstrates how to load and run a Stable Diffusion pipeline on Apple Silicon using PyTorch MPS backend. Includes warm-up pass recommendation for PyTorch 1.13 and attention slicing for better memory management.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/mps.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\npipe = pipe.to(\"mps\")\n\n# Recommended when computer has less than 64GB RAM\npipe.enable_attention_slicing()\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n\n# First \"warmup\" pass (see explanation above)\n_ = pipe(prompt, num_inference_steps=1)\n\n# Results will match CPU device after warmup pass\nimage = pipe(prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading PartiPrompts Dataset for Diffusion Model Evaluation\nDESCRIPTION: Loads the PartiPrompts dataset and selects sample prompts for evaluating diffusion models. This dataset provides diverse prompts for qualitative benchmarking of image generation models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/evaluation.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\n# prompts = load_dataset(\"nateraw/parti-prompts\", split=\"train\")\n# prompts = prompts.shuffle()\n# sample_prompts = [prompts[i][\"Prompt\"] for i in range(5)]\n\n# Fixing these sample prompts in the interest of reproducibility.\nsample_prompts = [\n    \"a corgi\",\n    \"a hot air balloon with a yin-yang symbol, with the moon visible in the daytime sky\",\n    \"a car with no windows\",\n    \"a cube made of porcupine\",\n    'The saying \"BE EXCELLENT TO EACH OTHER\" written on a red brick wall with a graffiti image of a green alien wearing a tuxedo. A yellow fire hydrant is on a sidewalk in the foreground.',\n]\n```\n\n----------------------------------------\n\nTITLE: Inference with Custom Diffusion from Hugging Face Hub\nDESCRIPTION: Python code for loading a Custom Diffusion model directly from the Hugging Face Hub and generating an image using the learned concept.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom huggingface_hub.repocard import RepoCard\nfrom diffusers import DiffusionPipeline\n\nmodel_id = \"sayakpaul/custom-diffusion-cat\"\ncard = RepoCard.load(model_id)\nbase_model_id = card.data.to_dict()[\"base_model\"]\n\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16).to(\"cuda\")\npipe.unet.load_attn_procs(model_id, weight_name=\"pytorch_custom_diffusion_weights.bin\")\npipe.load_textual_inversion(model_id, weight_name=\"<new1>.bin\")\n\nimage = pipe(\n    \"<new1> cat sitting in a bucket\",\n    num_inference_steps=100,\n    guidance_scale=6.0,\n    eta=1.0,\n).images[0]\nimage.save(\"cat.png\")\n```\n\n----------------------------------------\n\nTITLE: DreamBooth LoRA Training Command\nDESCRIPTION: Launches DreamBooth training with LoRA adaptation using accelerate. Includes configuration for learning rate, batch size, validation, and model checkpointing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_dreambooth_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --checkpointing_steps=100 \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=50 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Basic ControlNet Training\nDESCRIPTION: Standard training configuration requiring ~38GB VRAM, using the fill50k dataset and Stable Diffusion 1.5 model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_controlnet.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --train_batch_size=4\n```\n\n----------------------------------------\n\nTITLE: JSON metadata.jsonl example\nDESCRIPTION: A simple example of the metadata.jsonl\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\"file_name\": \"01.jpg\", \"prompt\": \"prompt 01\"}\n{\"file_name\": \"02.jpg\", \"prompt\": \"prompt 02\"}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Generated Images in a Grid\nDESCRIPTION: Creates a helper function to display the generated images in a grid layout.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef image_grid(imgs, rows, cols):\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid\n\nimage_grid(images, 2, 4)\n```\n\n----------------------------------------\n\nTITLE: Math Formula Showing Denoising Distribution\nDESCRIPTION: LaTeX formula representing the denoising distribution function that models use to denoise input samples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/overview.md#2025-04-11_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\np_{\\theta}(x_{t-1}|x_{t})\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install or upgrade the latest stable versions of Diffusers and PEFT libraries.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/merge_loras.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -U diffusers peft\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub using Notebook Login\nDESCRIPTION: Authenticates with the Hugging Face Hub from a notebook environment to enable model sharing. This requires the user to have a Hugging Face account and token with write permissions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Environment - Python Bash\nDESCRIPTION: Installs the necessary development dependencies in a virtual environment to ensure that developers have the required packages for feature development.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Quantizing and Using a Flux Model with TorchAO\nDESCRIPTION: Example of quantizing a Flux model using TorchAO, loading it into a pipeline, and generating an image. This snippet demonstrates weight-only int8 quantization and includes a memory usage comparison.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/torchao.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, TorchAoConfig\n\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\ndtype = torch.bfloat16\n\nquantization_config = TorchAoConfig(\"int8wo\")\ntransformer = FluxTransformer2DModel.from_pretrained(\n    model_id,\n    subfolder=\"transformer\",\n    quantization_config=quantization_config,\n    torch_dtype=dtype,\n)\npipe = FluxPipeline.from_pretrained(\n    model_id,\n    transformer=transformer,\n    torch_dtype=dtype,\n)\npipe.to(\"cuda\")\n\n# Without quantization: ~31.447 GB\n# With quantization: ~20.40 GB\nprint(f\"Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB\")\n\nprompt = \"A cat holding a sign that says hello world\"\nimage = pipe(\n    prompt, num_inference_steps=50, guidance_scale=4.5, max_sequence_length=512\n).images[0]\nimage.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Running Quality Checks\nDESCRIPTION: Command to run code quality checks using ruff and custom scripts\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ make quality\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Neuron on AWS Inf2 Instance\nDESCRIPTION: This command installs the Optimum Neuron package with the neuronx option using pip. It's required to set up the environment for using Diffusers on AWS Inf2 instances.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/neuron.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install --upgrade-strategy eager optimum[neuronx]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Default 🤗 Accelerate Configuration\nDESCRIPTION: This command allows for the setup of a default 🤗 Accelerate environment without interactive prompts. It's useful for those who may be using a non-interactive shell environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/sdxl.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Training T2I-Adapter for SDXL\nDESCRIPTION: Complete accelerate launch command for training a T2I-Adapter model with SDXL. Includes parameters for dataset, learning rate, validation, and other training configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_t2i_adapter_sdxl.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --mixed_precision=\"fp16\" \\\n --resolution=1024 \\\n --learning_rate=1e-5 \\\n --max_train_steps=15000 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --validation_steps=100 \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --report_to=\"wandb\" \\\n --seed=42 \\\n --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Loading Kandinsky 2.2 ControlNet Pipelines\nDESCRIPTION: Initializes both the Kandinsky 2.2 Prior Pipeline and the ControlNet Pipeline with depth conditioning. The pipelines are loaded with half-precision floating point (float16) to optimize memory usage and performance on GPU.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n\nprior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\npipeline = KandinskyV22ControlnetPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Generating Images with 8-bit Quantized Models\nDESCRIPTION: Shows how to use the quantized models to generate images using the FluxPipeline. It also demonstrates setting device_map to \"auto\" for automatic memory management.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    transformer=transformer_8bit,\n    text_encoder_2=text_encoder_2_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\npipe_kwargs = {\n    \"prompt\": \"A cat holding a sign that says hello world\",\n    \"height\": 1024,\n    \"width\": 1024,\n    \"guidance_scale\": 3.5,\n    \"num_inference_steps\": 50,\n    \"max_sequence_length\": 512,\n}\n\nimage = pipe(**pipe_kwargs, generator=torch.manual_seed(0),).images[0]\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Trained Textual Inversion Model in PyTorch\nDESCRIPTION: Python code to load a trained Textual Inversion model and generate images using the learned embedding. The code initializes a StableDiffusionPipeline, loads the textual inversion embedding, and generates an image with the custom token.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\npipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\npipeline.load_textual_inversion(\"sd-concepts-library/cat-toy\")\nimage = pipeline(\"A <cat-toy> train\", num_inference_steps=50).images[0]\nimage.save(\"cat-train.png\")\n```\n\n----------------------------------------\n\nTITLE: Creating Inpainting Control Condition\nDESCRIPTION: Function to prepare the control image by combining the initial and mask images into a tensor, marking masked pixels with -1.0 value.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\n\ndef make_inpaint_condition(image, image_mask):\n    image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n    image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n\n    assert image.shape[0:1] == image_mask.shape[0:1]\n    image[image_mask > 0.5] = -1.0  # set as masked pixel\n    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return image\n\ncontrol_image = make_inpaint_condition(init_image, mask_image)\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with K-LMS Scheduler in Python\nDESCRIPTION: This code example shows text-to-image generation using the Stable Diffusion pipeline with the K-LMS scheduler. It requires logging in to Hugging Face CLI and uses a custom scheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# make sure you're logged in with `huggingface-cli login`\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n\nlms = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    scheduler=lms,\n).to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"astronaut_rides_horse.png\")\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Commands to clone and install the Diffusers library from source code to ensure compatibility with latest example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: One Step Unet Pipeline Example\nDESCRIPTION: Demonstrates the implementation of a simple one-step Unet pipeline using the DDPM CIFAR10 model. This is primarily an example implementation for educational purposes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"google/ddpm-cifar10-32\", custom_pipeline=\"one_step_unet\")\npipe()\n```\n\n----------------------------------------\n\nTITLE: Deleting LoRA Adapters in Diffusers Pipeline\nDESCRIPTION: This snippet illustrates how to use the delete_adapters() function to remove a specific LoRA adapter from a Diffusers pipeline. It deletes the 'toy' adapter and then checks the remaining active adapters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npipe.delete_adapters(\"toy\")\npipe.get_active_adapters()\n[\"pixel\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing Optimizer for T2I-Adapter\nDESCRIPTION: This Python code initializes the optimizer for the T2I-Adapter parameters. The optimizer is configured with the specified learning rate, betas, weight decay, and epsilon values to update the adapter's weights during training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"params_to_optimize = t2iadapter.parameters()\noptimizer = optimizer_class(\n    params_to_optimize,\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\"\n```\n\n----------------------------------------\n\nTITLE: Setting Prediction Type for DDIM Training in Bash\nDESCRIPTION: This bash command shows how to set the prediction type to 'v_prediction' when training a model, which is recommended for improved results with DDIM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/ddim.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--prediction_type=\"v_prediction\"\n```\n\n----------------------------------------\n\nTITLE: Updating Training Loop for ControlNet Flux with Preprocessed Data\nDESCRIPTION: Training loop code that uses the preprocessed embeddings and latents to efficiently train a ControlNet Flux model with flow matching loss calculation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor epoch in range(first_epoch, args.num_train_epochs):\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(flux_controlnet):\n            # Convert images to latent space\n            pixel_latents = batch[\"pixel_latents\"].to(dtype=weight_dtype)\n            control_image = batch[\"control_latents\"].to(dtype=weight_dtype)\n            latent_image_ids = batch[\"latent_image_ids\"].to(dtype=weight_dtype)\n\n            # Sample noise that we'll add to the latents\n            noise = torch.randn_like(pixel_latents).to(accelerator.device).to(dtype=weight_dtype)\n            bsz = pixel_latents.shape[0]\n\n            # Sample a random timestep for each image\n            t = torch.sigmoid(torch.randn((bsz,), device=accelerator.device, dtype=weight_dtype))\n\n            # apply flow matching\n            noisy_latents = (\n                1 - t.unsqueeze(1).unsqueeze(2).repeat(1, pixel_latents.shape[1], pixel_latents.shape[2])\n            ) * pixel_latents + t.unsqueeze(1).unsqueeze(2).repeat(\n                1, pixel_latents.shape[1], pixel_latents.shape[2]\n            ) * noise\n\n            guidance_vec = torch.full(\n                (noisy_latents.shape[0],), 3.5, device=noisy_latents.device, dtype=weight_dtype\n            )\n\n            controlnet_block_samples, controlnet_single_block_samples = flux_controlnet(\n                hidden_states=noisy_latents,\n                controlnet_cond=control_image,\n                timestep=t,\n                guidance=guidance_vec,\n                pooled_projections=batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(dtype=weight_dtype),\n                encoder_hidden_states=batch[\"prompt_ids\"].to(dtype=weight_dtype),\n                txt_ids=batch[\"unet_added_conditions\"][\"time_ids\"][0].to(dtype=weight_dtype),\n                img_ids=latent_image_ids[0],\n                return_dict=False,\n            )\n\n            noise_pred = flux_transformer(\n                hidden_states=noisy_latents,\n                timestep=t,\n                guidance=guidance_vec,\n                pooled_projections=batch[\"unet_added_conditions\"][\"pooled_prompt_embeds\"].to(dtype=weight_dtype),\n                encoder_hidden_states=batch[\"prompt_ids\"].to(dtype=weight_dtype),\n                controlnet_block_samples=[sample.to(dtype=weight_dtype) for sample in controlnet_block_samples]\n                if controlnet_block_samples is not None\n                else None,\n                controlnet_single_block_samples=[\n                    sample.to(dtype=weight_dtype) for sample in controlnet_single_block_samples\n                ]\n                if controlnet_single_block_samples is not None\n                else None,\n                txt_ids=batch[\"unet_added_conditions\"][\"time_ids\"][0].to(dtype=weight_dtype),\n                img_ids=latent_image_ids[0],\n                return_dict=False,\n            )[0]\n```\n\n----------------------------------------\n\nTITLE: Installing bitsandbytes Library for Quantization in Python\nDESCRIPTION: This snippet shows how to install the bitsandbytes library using pip, which is required for quantizing the T5 text encoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install bitsandbytes\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest\nDESCRIPTION: Command to run tests using pytest with parallel execution\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m pytest -n auto --dist=loadfile -s -v ./tests/\n```\n\n----------------------------------------\n\nTITLE: Activating Only Mid Block for LoRA Adapter\nDESCRIPTION: Customizes the pixel-art adapter to only affect the mid block of the UNet while turning off its effect on down and up blocks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nadapter_weight_scales = { \"unet\": { \"down\": 0, \"mid\": 1, \"up\": 0} }\npipe.set_adapters(\"pixel\", adapter_weight_scales)\nimage = pipe(prompt, num_inference_steps=30, generator=torch.manual_seed(0)).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading and Inference with a Fine-tuned Stable Diffusion Model\nDESCRIPTION: Python code to load a fine-tuned Stable Diffusion model from a saved directory and generate images with custom prompts. Uses half-precision (float16) for efficient inference on GPU.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_path = \"path_to_saved_model\"\npipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nimage = pipe(prompt=\"yoda\").images[0]\nimage.save(\"yoda-naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Running ControlNet Training\nDESCRIPTION: Command to start ControlNet training with specified parameters and configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython3 train_controlnet_flax.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --validation_steps=1000 \\\n --train_batch_size=2 \\\n --revision=\"non-ema\" \\\n --from_pt \\\n --report_to=\"wandb\" \\\n --tracker_project_name=$HUB_MODEL_ID \\\n --num_train_epochs=11 \\\n --push_to_hub \\\n --hub_model_id=$HUB_MODEL_ID\n```\n\n----------------------------------------\n\nTITLE: Generating Images with LoRA-Augmented Diffusion Pipeline\nDESCRIPTION: Code to generate images using a Stable Diffusion pipeline enhanced with loaded LoRA weights, including prompt engineering and parameter configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"masterpiece, illustration, ultra-detailed, cityscape, san francisco, golden gate bridge, california, bay area, in the snow, beautiful detailed starry sky\"\nnegative_prompt = \"lowres, cropped, worst quality, low quality, normal quality, artifacts, signature, watermark, username, blurry, more than one bridge, bad architecture\"\n\nimages = pipeline(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=512,\n    height=512,\n    num_inference_steps=25,\n    num_images_per_prompt=4,\n    generator=torch.manual_seed(0),\n).images\n```\n\n----------------------------------------\n\nTITLE: Executing ControlNet Image-to-Image Generation\nDESCRIPTION: Runs the Kandinsky ControlNet Image-to-Image pipeline to generate the final transformed image. The function uses the initial image, depth hint, and both positive and negative embeddings to generate a robot version of the cat image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nimage = pipeline(image=img, strength=0.5, image_embeds=img_emb.image_embeds, negative_image_embeds=negative_emb.image_embeds, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]\nmake_image_grid([img.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Complete Example of DDIM Pipeline Configuration and Image Generation in Python\nDESCRIPTION: This comprehensive example shows how to load a pre-trained model, configure the DDIMScheduler with recommended settings, and generate an image using the optimized pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/ddim.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, DDIMScheduler\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"ptx0/pseudo-journey-v2\", torch_dtype=torch.float16)\npipe.scheduler = DDIMScheduler.from_config(\n    pipe.scheduler.config, rescale_betas_zero_snr=True, timestep_spacing=\"trailing\"\n)\npipe.to(\"cuda\")\n\nprompt = \"A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k\"\nimage = pipe(prompt, guidance_rescale=0.7).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Using FreeNoise Split Inference for Memory Savings in Python\nDESCRIPTION: This snippet illustrates how to implement FreeNoise split inference to optimize memory usage during video generation. It introduces the `enable_free_noise_split_inference` and enables forward chunking within the U-Net model to handle larger batch sizes efficiently.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Load pipeline and adapters\n# ...\n+ pipe.enable_free_noise_split_inference()\n+ pipe.unet.enable_forward_chunking(16)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for ControlNet Training\nDESCRIPTION: Commands to clone the diffusers repository, install it in editable mode, and install additional requirements for Stable Diffusion 3 training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sd3.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\ncd examples/controlnet\npip install -r requirements_sd3.txt\n```\n\n----------------------------------------\n\nTITLE: Accessing Safety Concept in Safe Stable Diffusion Pipeline\nDESCRIPTION: Shows how to initialize the Safe Stable Diffusion pipeline and access its safety concept property which defines the types of content to be filtered.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_safe.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import StableDiffusionPipelineSafe\n\n>>> pipeline = StableDiffusionPipelineSafe.from_pretrained(\"AIML-TUDA/stable-diffusion-safe\")\n>>> pipeline.safety_concept\n```\n\n----------------------------------------\n\nTITLE: Loading Model from Hub\nDESCRIPTION: Shows how to load a previously pushed model from HuggingFace Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/push_to_hub.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = ControlNetModel.from_pretrained(\"your-namespace/my-controlnet-model\")\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Accelerate Configuration\nDESCRIPTION: This Python code snippet programmatically creates a basic 🤗 Accelerate configuration file.  This is useful in environments where an interactive shell is not available (e.g., notebooks).\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"from accelerate.utils import write_basic_config\n\nwrite_basic_config()\"\n```\n\n----------------------------------------\n\nTITLE: Basic DreamBooth Training Command\nDESCRIPTION: Bash command to launch DreamBooth training using Accelerate, specifying model, data directories, and training parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=400 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for JAX/Flax\nDESCRIPTION: Installation commands for required libraries including JAX, Flax, transformers, and diffusers\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install -q jax==0.3.25 jaxlib==0.3.25 flax transformers ftfy\n#!pip install -q diffusers\n```\n\n----------------------------------------\n\nTITLE: Using Copied From Mechanism in Pipeline Output Class\nDESCRIPTION: This code snippet demonstrates the use of the `# Copied from` mechanism to maintain consistency across the codebase by allowing a new class to precisely mirror the structure of an existing class while modifying its naming. This ensures that updates to the original class can be easily propagated.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Copied from diffusers.pipelines.stable_diffusion.pipeline_output.StableDiffusionPipelineOutput with Stable->Alt\nclass AltDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Alt Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or NumPy array of shape `(batch_size, height, width,\n            num_channels)`.\n        nsfw_content_detected (`List[bool]`)\n            List indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content or\n            `None` if safety checking could not be performed.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Importing VQDiffusionSchedulerOutput in Python\nDESCRIPTION: This code snippet shows how to import the VQDiffusionSchedulerOutput class from the diffusers library. It is inferred from the context of the documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/vq_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers.scheduling_vq_diffusion import VQDiffusionSchedulerOutput\n```\n\n----------------------------------------\n\nTITLE: Syncing Forked Repository\nDESCRIPTION: Commands to sync forked repository with upstream main while avoiding reference notes\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout -b your-branch-for-syncing\n$ git pull --squash --no-commit upstream main\n$ git commit -m '<your message without GitHub references>'\n$ git push --set-upstream origin your-branch-for-syncing\n```\n\n----------------------------------------\n\nTITLE: Customizing LoRA Adapter Strength for UNet Components\nDESCRIPTION: Enables the pixel-art adapter with customized strength values for different parts of the UNet, activating only the down blocks while disabling mid and up blocks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipe.enable_lora()  # enable lora again, after we disabled it above\nprompt = \"toy_face of a hacker with a hoodie, pixel art\"\nadapter_weight_scales = { \"unet\": { \"down\": 1, \"mid\": 0, \"up\": 0} }\npipe.set_adapters(\"pixel\", adapter_weight_scales)\nimage = pipe(prompt, num_inference_steps=30, generator=torch.manual_seed(0)).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained ControlNet for Stable Diffusion 3/3.5\nDESCRIPTION: Python code demonstrating how to load a trained ControlNet model and perform inference using the StableDiffusion3ControlNetPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sd3.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusion3ControlNetPipeline, SD3ControlNetModel\nfrom diffusers.utils import load_image\nimport torch\n\nbase_model_path = \"stabilityai/stable-diffusion-3-medium-diffusers\"\ncontrolnet_path = \"DavyMorgan/sd3-controlnet-out\"\n\ncontrolnet = SD3ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\npipe = StableDiffusion3ControlNetPipeline.from_pretrained(\n    base_model_path, controlnet=controlnet\n)\npipe.to(\"cuda\", torch.float16)\n\n\ncontrol_image = load_image(\"./conditioning_image_1.png\").resize((1024, 1024))\nprompt = \"pale golden rod circle with old lace background\"\n\n# generate image\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt, num_inference_steps=20, generator=generator, control_image=control_image\n).images[0]\nimage.save(\"./output.png\")\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training with Text Encoder Fine-tuning\nDESCRIPTION: Command to fine-tune both the UNet and text encoder, which requires more VRAM (at least 24GB) but can provide better results, especially for faces.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --train_text_encoder \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --use_8bit_adam \\\n  --gradient_checkpointing \\\n  --learning_rate=2e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Generate Function for Image Generation with AOT Compilation\nDESCRIPTION: Implements a generate function that uses the pre-compiled JAX function for text-to-image generation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef generate(\n    prompt,\n    negative_prompt,\n    seed=default_seed,\n    guidance_scale=default_guidance_scale\n):\n    prompt_ids, neg_prompt_ids = tokenize_prompt(prompt, negative_prompt)\n    prompt_ids, neg_prompt_ids, rng = replicate_all(prompt_ids, neg_prompt_ids, seed)\n    g = jnp.array([guidance_scale] * prompt_ids.shape[0], dtype=jnp.float32)\n    g = g[:, None]\n    images = p_generate(\n        prompt_ids,\n        p_params,\n        rng,\n        g,\n        None,\n        neg_prompt_ids)\n\n    # convert the images to PIL\n    images = images.reshape((images.shape[0] * images.shape[1], ) + images.shape[-3:])\n    return pipeline.numpy_to_pil(np.array(images))\n```\n\n----------------------------------------\n\nTITLE: Additional Prompt2Prompt Editing Techniques in Diffusers\nDESCRIPTION: This code snippet demonstrates additional editing techniques available in the Prompt2Prompt pipeline, including ReplaceEdit with local blend, RefineEdit, RefineEdit with local blend, and ReweightEdit for modulating word importance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\"A turtle playing with a ball\",\n           \"A monkey playing with a ball\"]\n\ncross_attention_kwargs = {\n    \"edit_type\": \"replace\",\n    \"cross_replace_steps\": 0.4,\n    \"self_replace_steps\": 0.4,\n    \"local_blend_words\": [\"turtle\", \"monkey\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training of Kandinsky2.2 Decoder\nDESCRIPTION: Bash script to fine-tune the Kandinsky2.2 decoder model using multiple GPUs with Accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu  train_text_to_image_decoder.py \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot naruto, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi2-decoder-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Pipeline Execution\nDESCRIPTION: Code snippet showing how the pipeline execution is offloaded to a new thread using asyncio's run_in_executor method, allowing the main thread to handle other requests while image generation is in progress.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/create_a_server.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noutput = await loop.run_in_executor(None, lambda: pipeline(image_input.prompt, generator = generator))\n```\n\n----------------------------------------\n\nTITLE: Loading and Exploring InstructPix2Pix Dataset in Python\nDESCRIPTION: This snippet demonstrates how to load a custom dataset for evaluating image-conditioned text-to-image generation. It shows the structure of the dataset and how to access its features.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sayakpaul/instructpix2pix-demo\", split=\"train\")\ndataset.features\n```\n\n----------------------------------------\n\nTITLE: Launch ControlNet Training (16GB GPU) - Bash\nDESCRIPTION: Launches the ControlNet training script with gradient checkpointing and the 8-bit Adam optimizer enabled. These optimizations are suitable for GPUs with 16GB of memory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate launch train_controlnet.py \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \"\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training on 8GB GPU with DeepSpeed\nDESCRIPTION: Command for training on an 8GB GPU using DeepSpeed for tensor offloading and mixed precision training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch --mixed_precision=\"fp16\" train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --sample_batch_size=1 \\\n  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Running DreamBooth LoRA Training\nDESCRIPTION: Complete training command with detailed parameters for fine-tuning Lumina2 model using DreamBooth and LoRA.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_lumina2.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"Alpha-VLLM/Lumina-Image-2.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"trained-lumina2-lora\"\n\naccelerate launch train_dreambooth_lora_lumina2.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"bf16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --use_8bit_adam \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Setting Up Default Accelerate Configuration for CogVideoX\nDESCRIPTION: Command for creating a default Accelerate configuration without interactive prompts for environments that don't support interactive shells.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: SDXL Video Generation\nDESCRIPTION: Implementation example using SDXL model for video generation through the TextToVideoZeroSDXLPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/text_to_video_zero.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import TextToVideoZeroSDXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipe = TextToVideoZeroSDXLPipeline.from_pretrained(\n    model_id, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Installing Example Requirements\nDESCRIPTION: Terminal command to install the specific requirements for an example after navigating to its directory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Benchmarking IPEX vs Standard Stable Diffusion Pipeline in Python\nDESCRIPTION: Complete benchmark implementation comparing performance between standard and IPEX-optimized Stable Diffusion pipelines in both FP32 and BF16 precision modes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport intel_extension_for_pytorch as ipex\nfrom diffusers import StableDiffusionPipeline\nimport time\n\nprompt = \"sailing ship in storm by Rembrandt\"\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n# Helper function for time evaluation\ndef elapsed_time(pipeline, nb_pass=3, num_inference_steps=20):\n    # warmup\n    for _ in range(2):\n        images = pipeline(prompt, num_inference_steps=num_inference_steps, height=512, width=512).images\n    # time evaluation\n    start = time.time()\n    for _ in range(nb_pass):\n        pipeline(prompt, num_inference_steps=num_inference_steps, height=512, width=512)\n    end = time.time()\n    return (end - start) / nb_pass\n\n##############     bf16 inference performance    ###############\n\n# 1. IPEX Pipeline initialization\npipe = DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"stable_diffusion_ipex\")\npipe.prepare_for_ipex(prompt, dtype=torch.bfloat16, height=512, width=512)\n\n# 2. Original Pipeline initialization\npipe2 = StableDiffusionPipeline.from_pretrained(model_id)\n\n# 3. Compare performance between Original Pipeline and IPEX Pipeline\nwith torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n    latency = elapsed_time(pipe)\n    print(\"Latency of StableDiffusionIPEXPipeline--bf16\", latency)\n    latency = elapsed_time(pipe2)\n    print(\"Latency of StableDiffusionPipeline--bf16\", latency)\n\n##############     fp32 inference performance    ###############\n\n# 1. IPEX Pipeline initialization\npipe3 = DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"stable_diffusion_ipex\")\npipe3.prepare_for_ipex(prompt, dtype=torch.float32, height=512, width=512)\n\n# 2. Original Pipeline initialization\npipe4 = StableDiffusionPipeline.from_pretrained(model_id)\n\n# 3. Compare performance between Original Pipeline and IPEX Pipeline\nlatency = elapsed_time(pipe3)\nprint(\"Latency of StableDiffusionIPEXPipeline--fp32\", latency)\nlatency = elapsed_time(pipe4)\nprint(\"Latency of StableDiffusionPipeline--fp32\", latency)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers and Dependencies\nDESCRIPTION: Commands to clone and install Diffusers repository and its dependencies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -U -r requirements_flax.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install wandb\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Stable Diffusion on Naruto Dataset\nDESCRIPTION: Bash commands to fine-tune a Stable Diffusion model on the Naruto dataset using mixed precision training, gradient checkpointing, and EMA. This configuration is optimized for single GPU training with limited VRAM.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with SDXL\nDESCRIPTION: Complete example of loading the SDXL base model and generating an image from a text prompt. Uses the StableDiffusionXLPipeline with FP16 precision for better performance.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipe.to(\"cuda\")\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt=prompt).images[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Flax Schedulers with TPU Support\nDESCRIPTION: Shows implementation of Flax schedulers with TPU compatibility, including parameter replication and input sharding.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/schedulers.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline, FlaxDPMSolverMultistepScheduler\n\nscheduler, scheduler_state = FlaxDPMSolverMultistepScheduler.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    subfolder=\"scheduler\"\n)\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    scheduler=scheduler,\n    variant=\"bf16\",\n    dtype=jax.numpy.bfloat16,\n)\nparams[\"scheduler\"] = scheduler_state\n```\n\n----------------------------------------\n\nTITLE: Assembling Atom Pair Features in Python\nDESCRIPTION: Combines node attributes and edge attributes for atom pairs in a molecular graph. It multiplies node attributes of connected nodes and concatenates the result with edge attributes to create pairwise features.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef assemble_atom_pair_feature(node_attr, edge_index, edge_attr):\n    h_row, h_col = node_attr[edge_index[0]], node_attr[edge_index[1]]\n    h_pair = torch.cat([h_row * h_col, edge_attr], dim=-1)  # (E, 2H)\n    return h_pair\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Hugging Face Diffusers\nDESCRIPTION: Enumerates the required Python packages and their versions for the Hugging Face Diffusers project. This includes libraries for acceleration, computer vision, transformers, text processing, visualization, templating, dataset handling, and experiment tracking.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/requirements_sdxl.txt#2025-04-11_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\ndatasets\nwandb\n```\n\n----------------------------------------\n\nTITLE: Inference with Fine-tuned Prior LoRA\nDESCRIPTION: Python code for running inference using a fine-tuned Kandinsky prior model with LoRA weights. Shows how to load and use the prior model for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipe.prior_prior.load_attn_procs(output_dir)\npipe.enable_model_cpu_offload()\n\nprompt='A robot naruto, 4k photo'\nimage = pipe(prompt=prompt).images[0]\nimage.save(\"robot_naruto.png\")\nimage\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment\nDESCRIPTION: Command to initialize an Accelerate environment for distributed training setup, which allows configuring device placement and mixed precision settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/wuerstchen/text_to_image/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Downloading 3D Icon Dataset\nDESCRIPTION: Python code to download a 3D icon dataset from Hugging Face Hub to use for training the LoRA model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./3d_icon\"\nsnapshot_download(\n    \"LinoyTsaban/3d_icon\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Kandinsky2.2 Decoder with Custom Dataset\nDESCRIPTION: Bash script to fine-tune the Kandinsky2.2 decoder model using a custom dataset directory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport TRAIN_DIR=\"path_to_your_dataset\"\n\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image_decoder.py \\\n  --train_data_dir=$TRAIN_DIR \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot naruto, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi22-decoder-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Launching Text-to-Image Training with Hub Dataset\nDESCRIPTION: This bash command illustrates launching a text-to-image training script using a dataset hosted on the Hugging Face Hub. It uses the `--dataset_name` argument to specify the name of the dataset repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/create_dataset.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate launch --mixed_precision=\\\"fp16\\\"  train_text_to_image.py \\\n  --pretrained_model_name_or_path=\\\"stable-diffusion-v1-5/stable-diffusion-v1-5\\\" \\\n  --dataset_name=\\\"name_of_your_dataset\\\" \\\n  <other-arguments>\"\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for Fine-tuning\nDESCRIPTION: Commands to install the Diffusers library from source to ensure compatibility with the example scripts and install required dependencies for text-to-image fine-tuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Using Long Prompts with T5 Text Encoder in Stable Diffusion 3 Pipeline\nDESCRIPTION: This snippet demonstrates how to adjust the maximum sequence length for the T5 Text Encoder to accept longer prompts in the Stable Diffusion 3 pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_3.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature's body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight\"\n\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    guidance_scale=4.5,\n    max_sequence_length=512,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Loading and Inference from a Checkpoint Using UNet\nDESCRIPTION: Python code to load a specific checkpoint from training and perform inference. This approach loads only the UNet component rather than the entire pipeline, which is useful when checkpoints only save the UNet weights.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline, UNet2DConditionModel\n\nmodel_path = \"path_to_saved_model\"\nunet = UNet2DConditionModel.from_pretrained(model_path + \"/checkpoint-<N>/unet\", torch_dtype=torch.float16)\n\npipe = StableDiffusionPipeline.from_pretrained(\"<initial model>\", unet=unet, torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nimage = pipe(prompt=\"yoda\").images[0]\nimage.save(\"yoda-naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading AutoencoderKLMochi Model in Python\nDESCRIPTION: This snippet demonstrates how to load the AutoencoderKLMochi model from a pre-trained checkpoint. It uses the 'from_pretrained' method to load the model, specifying the model name, subfolder, and device.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/autoencoderkl_mochi.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import AutoencoderKLMochi\n\nvae = AutoencoderKLMochi.from_pretrained(\"genmo/mochi-1-preview\", subfolder=\"vae\", torch_dtype=torch.float32).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Inferencing with Textual Inversion Model in Flax\nDESCRIPTION: Load a trained textual inversion model and perform inference using FlaxStableDiffusionPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text_inversion.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline\n\nmodel_path = \"path-to-your-trained-model\"\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(model_path, dtype=jax.numpy.bfloat16)\n\nprompt = \"A <cat-toy> backpack\"\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\nimage.save(\"cat-backpack.png\")\n```\n\n----------------------------------------\n\nTITLE: Training with Gradient Checkpointing and 8-bit Optimizer\nDESCRIPTION: This script sets up Dreambooth training with gradient checkpointing and 8-bit optimizer capabilities to run on 16GB GPU. It requires bitsandbytes library installation for the optimizer function. Instance and class data paths must be configured, and the script introduces gradient accumulation and specific training parameters to optimize GPU utilization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/dreambooth_inpaint/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"runwayml/stable-diffusion-inpainting\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth_inpaint.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=2 --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Running Streaming Training Example\nDESCRIPTION: Example command for training with streaming data from Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"runs/uncanny-faces-{timestamp}\"\nexport HUB_MODEL_ID=\"controlnet-uncanny-faces\"\n\npython3 train_controlnet_flax.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=multimodalart/facesyntheticsspigacaptioned \\\n --streaming \\\n --conditioning_image_column=spiga_seg \\\n --image_column=image \\\n --caption_column=image_caption \\\n --resolution=512 \\\n --max_train_samples 100000 \\\n --learning_rate=1e-5 \\\n --train_batch_size=1 \\\n --revision=\"flax\" \\\n --report_to=\"wandb\" \\\n --tracker_project_name=$HUB_MODEL_ID\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for Diffusers Project\nDESCRIPTION: Lists required Python packages with their minimum version constraints for the Diffusers project. Includes core ML libraries like accelerate and transformers, along with utility packages for image processing and tokenization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.31.0\ntorchvision\ntransformers>=4.41.2\nftfy\ntensorboard\nJinja2\npeft>=0.11.1\nsentencepiece\ndecord>=0.6.0\nimageio-ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Combined Graph Extension Function for Molecules in Python\nDESCRIPTION: A utility function that combines both order-based and radius-based graph extension methods for molecular structures. It first extends the graph based on multi-hop connections, then adds radius-based edges.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef extend_graph_order_radius(\n    num_nodes,\n    pos,\n    edge_index,\n    edge_type,\n    batch,\n    order=3,\n    cutoff=10.0,\n    extend_order=True,\n    extend_radius=True,\n    is_sidechain=None,\n):\n    if extend_order:\n        edge_index, edge_type = _extend_graph_order(\n            num_nodes=num_nodes, edge_index=edge_index, edge_type=edge_type, order=order\n        )\n\n    if extend_radius:\n        edge_index, edge_type = _extend_to_radius_graph(\n            pos=pos, edge_index=edge_index, edge_type=edge_type, cutoff=cutoff, batch=batch, is_sidechain=is_sidechain\n        )\n\n    return edge_index, edge_type\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset for DreamBooth Training\nDESCRIPTION: Python code to download a sample dataset (dog images) from Hugging Face Hub for use in DreamBooth training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sana.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Compilation and Timing of AOT Compiled Function\nDESCRIPTION: Demonstrates how to compile the generate function and measure compilation time\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\nprint(\"Compiling ...\")\np_generate = aot_compile()\nprint(f\"Compiled in {time.time() - start}\")\n```\n\n----------------------------------------\n\nTITLE: Training with Prior Preservation and Regularization\nDESCRIPTION: Launch the Custom Diffusion training script with prior preservation loss and regularization parameters to improve model generalization\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_custom_diffusion.py \\\n  --with_prior_preservation \\\n  --prior_loss_weight=1.0 \\\n  --class_data_dir=\"./real_reg/samples_cat\" \\\n  --class_prompt=\"cat\" \\\n  --real_prior=True\n```\n\n----------------------------------------\n\nTITLE: Configuring UNet2DConditionModel for Inpainting in Python\nDESCRIPTION: Illustrates how to initialize a UNet2DConditionModel with altered input channels for inpainting tasks. This includes setting ignore_mismatched_sizes and low_cpu_mem_usage to handle size mismatch issues during adaptation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/adapt_a_model.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nunet = UNet2DConditionModel.from_pretrained(\n    model_id,\n    subfolder=\"unet\",\n    in_channels=9,\n    low_cpu_mem_usage=False,\n    ignore_mismatched_sizes=True,\n    use_safetensors=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers with Flax using UV\nDESCRIPTION: Installs Diffusers with Flax support using the UV package manager\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install diffusers[\"flax\"] transformers\n```\n\n----------------------------------------\n\nTITLE: Creating Weighted Prompts with Textual Inversion for SD 1.5\nDESCRIPTION: Defines weighted prompts for Stable Diffusion 1.5 that include a textual inversion concept '<midjourney-style>'. This combines prompt weighting with textual inversion techniques.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom sd_embed.embedding_funcs import get_weighted_text_embeddings_sd15\n\nprompt = \"\"\"<midjourney-style> A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. \nThis imaginative creature features the distinctive, bulky body of a hippo, \nbut with a texture and appearance resembling a golden-brown, crispy waffle. \nThe creature might have elements like waffle squares across its skin and a syrup-like sheen. \nIt's set in a surreal environment that playfully combines a natural water habitat of a hippo with elements of a breakfast table setting, \npossibly including oversized utensils or plates in the background. \nThe image should evoke a sense of playful absurdity and culinary fantasy.\n\"\"\"\n\nneg_prompt = \"\"\"\\\nskin spots,acnes,skin blemishes,age spot,(ugly:1.2),(duplicate:1.2),(morbid:1.21),(mutilated:1.2),\\\n(tranny:1.2),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.2),(bad proportions:1.3),\\\nextra limbs,(disfigured:1.2),(missing arms:1.2),(extra legs:1.2),(fused fingers:1.5),\\\n(too many fingers:1.5),(unclear eyes:1.2),lowers,bad hands,missing fingers,extra digit,\\\nbad hands,missing fingers,(extra arms and legs),(worst quality:2),(low quality:2),\\\n(normal quality:2),lowres,((monochrome)),((grayscale))\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generating Image Latents\nDESCRIPTION: Generates image latents using the computed text embeddings with reduced memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart_sigma.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = PixArtSigmaPipeline.from_pretrained(\n    \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\",\n    text_encoder=None,\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\nlatents = pipe(\n    negative_prompt=None,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    prompt_attention_mask=prompt_attention_mask,\n    negative_prompt_attention_mask=negative_prompt_attention_mask,\n    num_images_per_prompt=1,\n    output_type=\"latent\",\n).images\n\ndel pipe.transformer\nflush()\n```\n\n----------------------------------------\n\nTITLE: SDXL LoRA Fine-tuning Training Command\nDESCRIPTION: Complete command for LoRA fine-tuning of SDXL models with appropriate hyperparameters, validation, and logging settings for efficient training on consumer GPUs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_text_to_image_lora_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=1024 --random_flip \\\n  --train_batch_size=1 \\\n  --num_train_epochs=2 --checkpointing_steps=500 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --mixed_precision=\"fp16\" \\\n  --seed=42 \\\n  --output_dir=\"sd-naruto-model-lora-sdxl\" \\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Processing Continuous Inputs in Transformer2DModel\nDESCRIPTION: Describes the steps for processing continuous inputs in the Transformer2DModel. It involves projecting and reshaping the input, applying Transformer blocks, and reshaping the output to an image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. Project the input and reshape it to `(batch_size, sequence_length, feature_dimension)`.\n2. Apply the Transformer blocks in the standard way.\n3. Reshape to image.\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Gaudi Pipeline\nDESCRIPTION: Example of generating multiple images using the Gaudi-optimized Stable Diffusion pipeline with batch processing\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/habana.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noutputs = pipeline(\n    prompt=[\n        \"High quality photo of an astronaut riding a horse in space\",\n        \"Face of a yellow cat, high resolution, sitting on a park bench\",\n    ],\n    num_images_per_prompt=10,\n    batch_size=4,\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Accelerate for DeepSpeed - Bash\nDESCRIPTION: This command is used to configure the Accelerate environment to use DeepSpeed, particularly stage 2, for training on GPUs with 8GB of memory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate config\"\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22PriorEmb2EmbPipeline in Python\nDESCRIPTION: This snippet shows the class definition for KandinskyV22PriorEmb2EmbPipeline, which is used for embedding-to-embedding transformations in the Kandinsky 2.2 prior model. It includes methods for calling the pipeline and interpolating between embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22PriorEmb2EmbPipeline\n\t- all\n\t- __call__\n\t- interpolate\n```\n\n----------------------------------------\n\nTITLE: Generating Interpolated Image with Kandinsky 2.1\nDESCRIPTION: Uses the Kandinsky 2.1 prior pipeline to interpolate between the specified images/texts and generates a new image based on the interpolation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\"\nprior_out = prior_pipeline.interpolate(images_texts, weights)\n\npipeline = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n\nimage = pipeline(prompt, **prior_out, height=768, width=768).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Apache License Header\nDESCRIPTION: Copyright and license information for the Intel Labs Team Authors and HuggingFace Team implementation of LDM3D.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/ldm3d_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The Intel Labs Team Authors and HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Converting Image Frames to GIF for 3D Visualization\nDESCRIPTION: Code that demonstrates how to convert the generated image frames into animated GIFs for visualizing the 3D objects. This uses the export_to_gif utility function from the diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import export_to_gif\n\nexport_to_gif(images[0], \"firecracker_3d.gif\")\nexport_to_gif(images[1], \"cake_3d.gif\")\n```\n\n----------------------------------------\n\nTITLE: 16GB GPU Optimized Training\nDESCRIPTION: Training configuration optimized for 16GB GPUs using gradient checkpointing and 8-bit optimizer.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_controlnet.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --gradient_checkpointing \\\n --use_8bit_adam\n```\n\n----------------------------------------\n\nTITLE: Loading WanTransformer3D Model from Pretrained Weights\nDESCRIPTION: Demonstrates how to initialize a WanTransformer3DModel from pretrained weights using the Diffusers library. The model is loaded with bfloat16 precision and uses the transformer subfolder from the Wan2.1-T2V-1.3B-Diffusers pretrained model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/wan_transformer_3d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import WanTransformer3DModel\n\ntransformer = WanTransformer3DModel.from_pretrained(\"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Configuring torch.compile Compiler Flags for Maximum Performance\nDESCRIPTION: Setting up torch.compile configuration flags to optimize compilation behavior for diffusion models, focusing on performance-critical settings that affect inference speed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/fast_diffusion.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\ntorch._inductor.config.conv_1x1_as_mm = True\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.epilogue_fusion = False\ntorch._inductor.config.coordinate_descent_check_all_directions = True\n```\n\n----------------------------------------\n\nTITLE: Creating Random Noise for Diffusion Generation\nDESCRIPTION: This code creates a random normal distribution tensor with the appropriate dimensions for the loaded UNet model. This will serve as the starting point for the diffusion process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n\n>>> torch.manual_seed(0)\n\n>>> noisy_sample = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)\n>>> noisy_sample.shape\ntorch.Size([1, 3, 256, 256])\n```\n\n----------------------------------------\n\nTITLE: Local Edge Check Function for Molecular Graphs in Python\nDESCRIPTION: Determines whether an edge in a molecular graph represents a local (chemical bond) connection. Local edges have positive edge types as opposed to radius-graph added edges.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef is_local_edge(edge_type):\n    return edge_type > 0\n```\n\n----------------------------------------\n\nTITLE: Exporting and Converting Mesh Files\nDESCRIPTION: Demonstrates exporting the mesh to PLY format and converting it to GLB format using trimesh, with optional viewpoint rotation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import export_to_ply\nimport trimesh\nimport numpy as np\n\nply_path = export_to_ply(images[0], \"3d_cake.ply\")\nprint(f\"Saved to folder: {ply_path}\")\n\nmesh = trimesh.load(\"3d_cake.ply\")\nrot = trimesh.transformations.rotation_matrix(-np.pi / 2, [1, 0, 0])\nmesh = mesh.apply_transform(rot)\nmesh_export = mesh.export(\"3d_cake.glb\", file_type=\"glb\")\n```\n\n----------------------------------------\n\nTITLE: Setting Verbosity Level Using Environment Variables\nDESCRIPTION: Example of how to override the default verbosity level using the DIFFUSERS_VERBOSITY environment variable.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/logging.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nDIFFUSERS_VERBOSITY=error ./myprogram.py\n```\n\n----------------------------------------\n\nTITLE: Downloading Conditioning Test Images\nDESCRIPTION: Commands to download sample conditioning images for testing the T2I-Adapter during training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\n\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```\n\n----------------------------------------\n\nTITLE: Training Stable Diffusion with PyTorch\nDESCRIPTION: Bash script for training a Stable Diffusion model on the Naruto BLIP captions dataset using PyTorch and Accelerate. The script sets environment variables for the base model and dataset, and launches the training with various parameters including mixed precision, EMA, resolution settings, and gradient accumulation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport dataset_name=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$dataset_name \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --enable_xformers_memory_efficient_attention \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-naruto-model\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Using the One-Step UNet Pipeline\nDESCRIPTION: Example showing how to initialize and use the custom pipeline with both new and pretrained models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DDPMScheduler, UNet2DModel\n\nscheduler = DDPMScheduler()\nunet = UNet2DModel()\n\npipeline = UnetSchedulerOneForwardPipeline(unet=unet, scheduler=scheduler)\noutput = pipeline()\n# load pretrained weights\npipeline = UnetSchedulerOneForwardPipeline.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)\noutput = pipeline()\n```\n\n----------------------------------------\n\nTITLE: Initializing Stable Diffusion Pipeline\nDESCRIPTION: Loads the stable-diffusion-v1-5 model using DiffusionPipeline with safetensors enabled.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/stable_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipeline = DiffusionPipeline.from_pretrained(model_id, use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Loading LTX Video Pipeline from Single File in Python\nDESCRIPTION: Shows how to load the entire LTX Video pipeline from a single file, including the text encoder and tokenizer, using the Diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/ltx_video.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import LTXImageToVideoPipeline\nfrom transformers import T5EncoderModel, T5Tokenizer\n\nsingle_file_url = \"https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors\"\ntext_encoder = T5EncoderModel.from_pretrained(\n  \"Lightricks/LTX-Video\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16\n)\ntokenizer = T5Tokenizer.from_pretrained(\n  \"Lightricks/LTX-Video\", subfolder=\"tokenizer\", torch_dtype=torch.bfloat16\n)\npipe = LTXImageToVideoPipeline.from_single_file(\n  single_file_url, text_encoder=text_encoder, tokenizer=tokenizer, torch_dtype=torch.bfloat16\n)\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training on 12GB GPU\nDESCRIPTION: Optimized command for training on a 12GB GPU using gradient checkpointing, 8-bit optimizer, xformers, and setting grads to none.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"dog\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --enable_xformers_memory_efficient_attention \\\n  --set_grads_to_none \\\n  --learning_rate=2e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800 \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Model and Scheduler Setup\nDESCRIPTION: Code for loading the pretrained molecular GNN model and setting up the DDPM scheduler for the diffusion process\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nDEVICE = \"cuda\"\nmodel = MoleculeGNN.from_pretrained(\"fusing/gfn-molecule-gen-drugs\").to(DEVICE)\n```\n\nLANGUAGE: python\nCODE:\n```\nnum_timesteps = 1000\nscheduler = DDPMScheduler(\n    num_train_timesteps=num_timesteps, beta_schedule=\"sigmoid\", beta_start=1e-7, beta_end=2e-3, clip_sample=False\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing 🤗 Accelerate Configuration\nDESCRIPTION: This bash script initializes an 🤗 Accelerate environment. This prepares the training environment for distributed training and mixed-precision execution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate config\"\n```\n\n----------------------------------------\n\nTITLE: Launching T2I-Adapter Training\nDESCRIPTION: This command launches the T2I-Adapter training script using 🤗 Accelerate, enabling distributed training and mixed-precision.  It includes an example of passing the `--gradient_accumulation_steps` parameter to control gradient accumulation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate launch train_t2i_adapter_sdxl.py \\\n  ----gradient_accumulation_steps=4\"\n```\n\n----------------------------------------\n\nTITLE: Creating Random Gaussian Noise for Diffusion Input\nDESCRIPTION: Generates random Gaussian noise with the appropriate shape for the diffusion model input. The shape includes batch size, number of channels, and image dimensions specified in the model configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n\n>>> torch.manual_seed(0)\n\n>>> noisy_sample = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)\n>>> noisy_sample.shape\ntorch.Size([1, 3, 256, 256])\n```\n\n----------------------------------------\n\nTITLE: Training Command with Parameters\nDESCRIPTION: Full training command with recommended parameters for Wuerstchen model training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/wuerstchen.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch  train_text_to_image_prior.py \\\n  --mixed_precision=\"fp16\" \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=4 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --dataloader_num_workers=4 \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot naruto, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"wuerstchen-prior-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Setting Compute Data Type for 4-bit Quantization\nDESCRIPTION: Configuring BitsAndBytesConfig to use bfloat16 as the compute data type for 4-bit quantized models. This speeds up computation while maintaining precision benefits of the quantization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Optimizing SDXL Turbo Inference Speed\nDESCRIPTION: These code snippets provide tips for improving the inference speed of SDXL Turbo. They include compiling the UNet for faster execution and upcasting the VAE to float32 to avoid costly dtype conversions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\nLANGUAGE: python\nCODE:\n```\npipe.upcast_vae()\n```\n\n----------------------------------------\n\nTITLE: Initializing Japanese Stable Diffusion XL Pipeline\nDESCRIPTION: Sets up a DiffusionPipeline using the Japanese Stable Diffusion XL model with remote code trust enabled. Moves the pipeline to CUDA device for GPU acceleration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md#2025-04-11_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/japanese-stable-diffusion-xl\", trust_remote_code=True\n)\npipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Image Processing Warning in Diffusers\nDESCRIPTION: Runtime warning message from the Diffusers image processor when casting image values. The warning indicates potential data loss during uint8 conversion of normalized image values.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/inference/flux/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimages = (images * 255).round().astype(\"uint8\")\n```\n\n----------------------------------------\n\nTITLE: Converting PromptDiffusion model\nDESCRIPTION: This bash script converts a PromptDiffusion model checkpoint to a Diffusers compatible format. It uses the `convert_original_promptdiffusion_to_diffusers.py` script, specifying the paths to the original checkpoint, the configuration file, and the desired output directory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/promptdiffusion/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n```bash\npython convert_original_promptdiffusion_to_diffusers.py --checkpoint_path path-to-network-step04999.ckpt --original_config_file path-to-cldm_v15.yaml --dump_path path-to-output-directory\n```\n```\n\n----------------------------------------\n\nTITLE: Loading Diffusion Scheduler\nDESCRIPTION: Initializes a DDPMScheduler from the model repository configuration to manage the denoising process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import DDPMScheduler\n\n>>> scheduler = DDPMScheduler.from_config(repo_id)\n>>> scheduler\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Configuration\nDESCRIPTION: Configuration for distributed training across multiple GPUs with mixed precision and WandB logging.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch --mixed_precision=\"fp16\" --multi_gpu train_controlnet.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --train_batch_size=4 \\\n --mixed_precision=\"fp16\" \\\n --tracker_project_name=\"controlnet-demo\" \\\n --report_to=wandb\n```\n\n----------------------------------------\n\nTITLE: Launch Training with Mixed Precision - Bash\nDESCRIPTION: Execute the training script using the Accelerate command with mixed precision (fp16) to speed up the training process and reduce memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_text_to_image.py \\\n  --mixed_precision=\"fp16\"\n```\n\n----------------------------------------\n\nTITLE: Loading Pipeline with Original Configuration\nDESCRIPTION: Shows how to load a pipeline using the original configuration file from an external source.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLPipeline\n\nckpt_path = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0_0.9vae.safetensors\"\noriginal_config = \"https://raw.githubusercontent.com/Stability-AI/generative-models/main/configs/inference/sd_xl_base.yaml\"\n\npipeline = StableDiffusionXLPipeline.from_single_file(ckpt_path, original_config=original_config)\n```\n\n----------------------------------------\n\nTITLE: Extended autodoc syntax for pipeline documentation\nDESCRIPTION: Extended Markdown syntax using the autodoc feature to document pipeline classes with additional specific methods that may not be documented by default.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n[[autodoc]] XXXPipeline\n    - all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n    - enable_xformers_memory_efficient_attention\n    - disable_xformers_memory_efficient_attention\n```\n\n----------------------------------------\n\nTITLE: HTML License Header\nDESCRIPTION: HTML comment block containing the Apache 2.0 license information for the HuggingFace Team.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/i2vgenxl.md#2025-04-11_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: LoRA Inference Setup and Generation\nDESCRIPTION: Shows how to load and use LoRA weights for inference, including loading the base model and adapter layers, and generating images with the adapted model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\npipe = DiffusionPipeline.from_pretrained(\"base-model-name\").to(\"cuda\")\n```\n\nLANGUAGE: python\nCODE:\n```\npipe.load_lora_weights(\"path-to-the-lora-checkpoint\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimage = pipe(\"A picture of a sks dog in a bucket\", num_inference_steps=25).images[0]\n```\n\n----------------------------------------\n\nTITLE: Installing Core ML Dependencies\nDESCRIPTION: Commands to install required Python packages for Core ML inference, including huggingface_hub and Apple's ml-stable-diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/coreml.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface_hub\npip install git+https://github.com/apple/ml-stable-diffusion\n```\n\n----------------------------------------\n\nTITLE: Running Quality Control Checks - Python Bash\nDESCRIPTION: Runs quality control checks using `ruff` and custom scripts to identify coding mistakes in the codebase before submission.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ make quality\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub\nDESCRIPTION: This code snippet shows how to authenticate with the Hugging Face Hub using your access token. This is required to download models and datasets that require authentication.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Listing Project Dependencies\nDESCRIPTION: This snippet defines the dependencies required by the diffusers project.  It includes versions and specific git repository references for packages like peft. These dependencies need to be installed for the project to function correctly.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/lora/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: TEXT\nCODE:\n```\n\"accelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\ndatasets\nftfy\ntensorboard\nJinja2\ngit+https://github.com/huggingface/peft.git\"\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Pose Images for Text2Video-Zero\nDESCRIPTION: This snippet demonstrates how to download a video and extract pose images from it for use with Text2Video-Zero.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/text-img2vid.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download\nfrom PIL import Image\nimport imageio\n\nfilename = \"__assets__/poses_skeleton_gifs/dance1_corr.mp4\"\nrepo_id = \"PAIR/Text2Video-Zero\"\nvideo_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n\nreader = imageio.get_reader(video_path, \"ffmpeg\")\nframe_count = 8\npose_images = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n```\n\n----------------------------------------\n\nTITLE: Image Generation with GLIGEN\nDESCRIPTION: Generates multiple images using the configured pipeline with specified prompt, bounding boxes, and generation parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/demo.ipynb#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimages = pipe(\n    prompt=prompt,\n    gligen_phrases=gligen_phrases,\n    gligen_boxes=boxes,\n    gligen_scheduled_sampling_beta=1.0,\n    output_type=\"pil\",\n    num_inference_steps=50,\n    negative_prompt=\"artifacts, blurry, smooth texture, bad quality, distortions, unrealistic, distorted image, bad proportions, duplicate\",\n    num_images_per_prompt=16,\n).images\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Shap-E Model\nDESCRIPTION: Installation commands for the necessary libraries to use Shap-E models. This includes diffusers, transformers, accelerate, and trimesh packages which are required for 3D asset generation and manipulation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Colab에서 필요한 라이브러리를 설치하기 위해 주석을 제외하세요\n#!pip install -q diffusers transformers accelerate trimesh\n```\n\n----------------------------------------\n\nTITLE: Creating Initial Text for Prompt Generation\nDESCRIPTION: Setting up the initial text instructions to guide Flan-T5 in generating appropriate source and target prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsource_concept = \"bowl\"\ntarget_concept = \"basket\"\n\nsource_text = f\"Provide a caption for images containing a {source_concept}. \"\n\"The captions should be in English and should be no longer than 150 characters.\"\n\ntarget_text = f\"Provide a caption for images containing a {target_concept}. \"\n\"The captions should be in English and should be no longer than 150 characters.\"\n```\n\n----------------------------------------\n\nTITLE: License Header in Markdown\nDESCRIPTION: Apache 2.0 license header defining usage terms and conditions for the KarrasVeScheduler implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/stochastic_karras_ve.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Cloning Diffusers Repository and Installing Dependencies with Bash\nDESCRIPTION: This snippet provides the bash commands to clone the Diffusers GitHub repository and install the necessary dependencies for running training scripts. It is recommended to install from source to keep the scripts updated. The commands should be executed in a new virtual environment to avoid conflicts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/textual_inversion/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Inference with Quantized Model\nDESCRIPTION: Script to generate images using the trained INT8 model. Uses text2images.py with a specific prompt including the placeholder token.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport INT8_MODEL_NAME=\"./int8_model\"\n\npython text2images.py \\\n  --pretrained_model_name_or_path=$INT8_MODEL_NAME \\\n  --caption \"a lovely <dicoo> in red dress and hat, in the snowly and brightly night, with many brighly buildings.\" \\\n  --images_num 4\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Configuration Parameters\nDESCRIPTION: Shows how to access the configuration parameters of a loaded diffusion model. The configuration contains important architectural parameters that define the model's structure.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> model.config\n```\n\n----------------------------------------\n\nTITLE: Preparing Prompts and Video Paths Files for CogVideoX Training\nDESCRIPTION: Example of the prompts.txt file format for LoRA finetuning, containing line-separated descriptive text prompts that will be paired with corresponding video files.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nA black and white animated sequence featuring a rabbit, named Rabbity Ribfried, and an anthropomorphic goat in a musical, playful environment, showcasing their evolving interaction.\nA black and white animated sequence on a ship's deck features a bulldog character, named Bully Bulldoger, showcasing exaggerated facial expressions and body language. The character progresses from confident to focused, then to strained and distressed, displaying a range of emotions as it navigates challenges. The ship's interior remains static in the background, with minimalistic details such as a bell and open door. The character's dynamic movements and changing expressions drive the narrative, with no camera movement to distract from its evolving reactions and physical gestures.\n...\n```\n\n----------------------------------------\n\nTITLE: Launch ControlNet Training (PyTorch) - Bash\nDESCRIPTION: Launches the ControlNet training script using PyTorch with specific configurations, including dataset, resolution, learning rate, validation images and prompts, batch size, gradient accumulation steps, and enabling push to hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n\"export MODEL_DIR=\\\"stable-diffusion-v1-5/stable-diffusion-v1-5\\\"\nexport OUTPUT_DIR=\\\"path/to/save/model\\\"\\n\naccelerate launch train_controlnet.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --validation_image \\\"./conditioning_image_1.png\\\" \\\"./conditioning_image_2.png\\\" \\\n --validation_prompt \\\"red circle with blue background\\\" \\\"cyan circle with brown floral background\\\" \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --push_to_hub\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Pipeline Documentation Table\nDESCRIPTION: A structured table documenting various AI pipelines, their implementations, and creators, including links to papers, notebooks, and demos.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|   Zero1to3 Pipeline                                                                                                    | Implementation of [Zero-1-to-3: Zero-shot One Image to 3D Object](https://arxiv.org/abs/2303.11328)                                                                                                                                                                                                                                                                                                                                                                                                                                      | [Zero1to3 Pipeline](#zero1to3-pipeline)      | - |              [Xin Kong](https://github.com/kxhit) |\n| Stable Diffusion XL Long Weighted Prompt Pipeline | A pipeline support unlimited length of prompt and negative prompt, use A1111 style of prompt weighting | [Stable Diffusion XL Long Weighted Prompt Pipeline](#stable-diffusion-xl-long-weighted-prompt-pipeline) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1LsqilswLR40XLLcp6XFOl5nKb_wOe26W?usp=sharing) | [Andrew Zhu](https://xhinker.medium.com/) |\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inpainting Pipeline in Python\nDESCRIPTION: This snippet loads base and mask images, then uses a Stable Diffusion inpainting pipeline to generate a new image based on a prompt. It demonstrates basic usage of the pipeline and image manipulation functions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/road-mask.png\")\n\nimage = pipeline(prompt=\"road\", image=init_image, mask_image=mask_image).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```\n\n----------------------------------------\n\nTITLE: Enabling Forward Chunking for Memory Optimization in Python\nDESCRIPTION: Demonstrates how to enable forward chunking on the transformer to reduce memory usage during inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/hunyuandit.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline.transformer.enable_forward_chunking(chunk_size=1, dim=1)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for Training\nDESCRIPTION: Commands to clone the diffusers repository and install it from source to ensure compatibility with the latest example scripts. This approach is recommended for having the most up-to-date implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_flux.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Quantizing CogVideoX Pipeline in Python\nDESCRIPTION: Demonstrates how to load a quantized CogVideoX pipeline for reduced memory usage, using the bitsandbytes library for 8-bit quantization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/cogvideox.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, CogVideoXTransformer3DModel, CogVideoXPipeline\nfrom diffusers.utils import export_to_video\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"THUDM/CogVideoX-2b\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = CogVideoXTransformer3DModel.from_pretrained(\n    \"THUDM/CogVideoX-2b\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = CogVideoXPipeline.from_pretrained(\n    \"THUDM/CogVideoX-2b\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting.\"\nvideo = pipeline(prompt=prompt, guidance_scale=6, num_inference_steps=50).frames[0]\nexport_to_video(video, \"ship.mp4\", fps=8)\n```\n\n----------------------------------------\n\nTITLE: Documenting MarigoldIntrinsicsOutput Class\nDESCRIPTION: Autodoc directive for generating documentation for the MarigoldIntrinsicsOutput class in the marigold intrinsics pipeline module.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/marigold.md#2025-04-11_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n[[autodoc]] pipelines.marigold.pipeline_marigold_intrinsics.MarigoldIntrinsicsOutput\n```\n\n----------------------------------------\n\nTITLE: Accelerate Configuration\nDESCRIPTION: Various methods to configure Accelerate for distributed training setup.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/ip_adapter/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with Conda\nDESCRIPTION: This command installs PyTorch, TorchVision, and Torchaudio with CUDA 11.1 support via Conda, which is necessary for running models on the GPU.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n!conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia\n```\n\n----------------------------------------\n\nTITLE: Compiling the JAX Pipeline\nDESCRIPTION: Code to perform the initial compilation of the generation function, which will be slow during the first run but enables fast subsequent inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\nprint(f\"Compiling ...\")\ngenerate(default_prompt, default_neg_prompt)\nprint(f\"Compiled in {time.time() - start}\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Checkpoint Saving During DreamBooth Training\nDESCRIPTION: Command parameter to enable periodic checkpoint saving during training, which helps prevent losing work and allows selection of the best model before overfitting occurs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n  --checkpointing_steps=500\n```\n\n----------------------------------------\n\nTITLE: Extracting Components from a Pipeline for Reuse\nDESCRIPTION: Extract all components from one pipeline to reuse them in another pipeline without duplicating model weights in memory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nstable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(model_id)\n\ncomponents = stable_diffusion_txt2img.components\n```\n\n----------------------------------------\n\nTITLE: Example Structure of _toctree.yml File in YAML\nDESCRIPTION: Example of the _toctree.yml file structure that needs to be translated. This file defines the table of contents for the documentation website, showing the hierarchy and organization of documentation pages.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/TRANSLATING.md#2025-04-11_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- sections:\n  - local: pipeline_tutorial # Do not change this! Use the same name for your .md file\n    title: Pipelines for inference # Translate this!\n    ...\n  title: Tutorials # Translate this!\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Clone the Hugging Face Diffusers repository and install the library in a new virtual environment. This ensures access to the latest example scripts and dependencies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/unconditional_image_generation/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Syncing Forked Repository - Git Bash\nDESCRIPTION: Detailed steps on syncing a forked repository's main branch with the upstream main branch, ensuring a clean commit history.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout -b your-branch-for-syncing\n$ git pull --squash --no-commit upstream main\n$ git commit -m '<your message without GitHub references>'\n$ git push --set-upstream origin your-branch-for-syncing\n```\n\n----------------------------------------\n\nTITLE: Text Encoder Fine-tuning Training Command\nDESCRIPTION: Bash script for training a Stable Diffusion XL model with text encoder fine-tuning, adding additional memory requirements\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_text_to_image_lora_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=1024 --random_flip \\\n  --train_batch_size=1 \\\n  --num_train_epochs=2 --checkpointing_steps=500 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --seed=42 \\\n  --output_dir=\"sd-naruto-model-lora-sdxl-txt\" \\\n  --train_text_encoder \\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Benchmarking ControlNet\nDESCRIPTION: Python code for benchmarking ControlNet with PyTorch 2.0 optimizations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nimport requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((512, 512))\n\npath = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n\nrun_compile = True  # Set True / False\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    path, controlnet=controlnet, torch_dtype=torch.float16\n)\n\npipe = pipe.to(\"cuda\")\npipe.unet.to(memory_format=torch.channels_last)\npipe.controlnet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe.controlnet = torch.compile(pipe.controlnet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"ghibli style, a fantasy landscape with castles\"\n\nfor _ in range(3):\n    image = pipe(prompt=prompt, image=init_image).images[0]\n```\n\n----------------------------------------\n\nTITLE: LoRA Training Command with Accelerate\nDESCRIPTION: Execute LoRA training for Stable Diffusion using accelerate, with mixed precision and specific hyperparameters for text-to-image generation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=512 --random_flip \\\n  --train_batch_size=1 \\\n  --num_train_epochs=100 --checkpointing_steps=5000 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --seed=42 \\\n  --output_dir=\"sd-naruto-model-lora\" \\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\"\n```\n\n----------------------------------------\n\nTITLE: Alternative Accelerate Configurations\nDESCRIPTION: Different methods to configure Accelerate, including default configuration and programmatic setup in notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_lumina2.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Loading Pipeline from Specific Local Directory\nDESCRIPTION: Shows how to load a pipeline from files downloaded to a specific local directory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\nmy_local_checkpoint_path = hf_hub_download(\n    repo_id=\"segmind/SSD-1B\",\n    filename=\"SSD-1B.safetensors\"\n    local_dir=\"my_local_checkpoints\"\n)\n\nmy_local_config_path = snapshot_download(\n    repo_id=\"segmind/SSD-1B\",\n    allow_patterns=[\"*.json\", \"**/*.json\", \"*.txt\", \"**/*.txt\"]\n    local_dir=\"my_local_config\"\n)\n\npipeline = StableDiffusionXLPipeline.from_single_file(my_local_checkpoint_path, config=my_local_config_path, local_files_only=True)\n```\n\n----------------------------------------\n\nTITLE: Loading AnimateDiff Checkpoints from a Single File\nDESCRIPTION: This snippet demonstrates how to load AnimateDiff checkpoints directly into the MotionAdapter from a single file URL, allowing for easier integration and model setup for animation tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/animatediff.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import MotionAdapter\n\nckpt_path = \"https://huggingface.co/Lightricks/LongAnimateDiff/blob/main/lt_long_mm_32_frames.ckpt\"\n\nadapter = MotionAdapter.from_single_file(ckpt_path, torch_dtype=torch.float16)\npipeline = AnimateDiffPipeline.from_pretrained(\"emilianJR/epiCRealism\", motion_adapter=adapter)\n```\n\n----------------------------------------\n\nTITLE: Downloading Dog Example Dataset from Hugging Face\nDESCRIPTION: Python code to download a sample dog dataset from Hugging Face Hub for use in the DreamBooth training example. The dataset is stored locally for training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_flux.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving Image Grids from DemoFusion Output\nDESCRIPTION: This utility function creates an image grid from multiple generated images and optionally saves each individual image. It's designed to display and save the high-resolution outputs from the DemoFusion pipeline, arranging them in a horizontal grid.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_91\n\nLANGUAGE: python\nCODE:\n```\ndef image_grid(imgs, save_path=None):\n\n    w = 0\n    for i, img in enumerate(imgs):\n        h_, w_ = imgs[i].size\n        w += w_\n    h = h_\n    grid = Image.new('RGB', size=(w, h))\n    grid_w, grid_h = grid.size\n\n    w = 0\n    for i, img in enumerate(imgs):\n        h_, w_ = imgs[i].size\n        grid.paste(img, box=(w, h - h_))\n        if save_path != None:\n            img.save(save_path + \"/img_{}.jpg\".format((i + 1) * 1024))\n        w += w_\n\n    return grid\n\nimage_grid(images, save_path=\"./outputs/\")\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained Textual Inversion Model (Python)\nDESCRIPTION: Python code demonstrating how to use a trained textual inversion model for inference with the StableDiffusionPipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"path-to-your-trained-model\"\npipe = StableDiffusionPipeline.from_pretrained(model_id,torch_dtype=torch.float16).to(\"cuda\")\n\nrepo_id_embeds = \"path-to-your-learned-embeds\"\npipe.load_textual_inversion(repo_id_embeds)\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n\nimage.save(\"cat-backpack.png\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Real Images for Cat Regularization\nDESCRIPTION: Command to collect real cat images using clip-retrieval for regularization during training to prevent overfitting.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install clip-retrieval\npython retrieve.py --class_prompt cat --class_data_dir real_reg/samples_cat --num_class_images 200\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Pipeline for Evaluation\nDESCRIPTION: Initializes the Stable Diffusion pipeline using a specific checkpoint for evaluation purposes. This sets up the model for generating images from text prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/evaluation.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_ckpt = \"CompVis/stable-diffusion-v1-4\"\nsd_pipeline = StableDiffusionPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: HTML License Comment\nDESCRIPTION: Copyright and Apache 2.0 license notice for the Hugging Face Diffusers library\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/overview.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Image Generation with Time Measurement\nDESCRIPTION: Demonstrates parallel image generation with time measurement using block_until_ready() to ensure accurate timing of asynchronous JAX operations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\nimages = p_generate(prompt_ids, p_params, rng)\nimages = images.block_until_ready()\n\n# CPU times: user 1min 15s, sys: 18.2 s, total: 1min 34s\n# Wall time: 1min 15s\n```\n\n----------------------------------------\n\nTITLE: Importing FluxPipelineOutput in Python\nDESCRIPTION: This code snippet demonstrates how to import the FluxPipelineOutput class, which represents the output of the FluxControlNetPipeline. It likely contains the generated image and possibly additional metadata.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/controlnet_flux.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines.flux.pipeline_output import FluxPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Converting PLY to GLB Format for 3D Visualization\nDESCRIPTION: Code for converting PLY mesh files to GLB format using the trimesh library. GLB is a common format for 3D model visualization in various platforms and viewers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/shap-e.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport trimesh\n\nmesh = trimesh.load(\"3d_cake.ply\")\nmesh_export = mesh.export(\"3d_cake.glb\", file_type=\"glb\")\n```\n\n----------------------------------------\n\nTITLE: Installing RAM Model Dependencies\nDESCRIPTION: Bash command to install the Recognize Anything Model (RAM) from GitHub repository\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/xinyu1205/recognize-anything.git --no-deps\n```\n\n----------------------------------------\n\nTITLE: Installing documentation dependencies for Diffusers\nDESCRIPTION: Command to install the necessary packages to build the documentation for the Diffusers library, using the docs extra requirements.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[docs]\"\n```\n\n----------------------------------------\n\nTITLE: Using Default Accelerate Configuration\nDESCRIPTION: Command to set up Accelerate with default configuration without requiring interactive input from the user.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Install TensorFlow and Tensorboard Profile Plugin - Bash\nDESCRIPTION: Installs TensorFlow and the Tensorboard profile plugin via pip.  These are dependencies for profiling Flax training runs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install tensorflow tensorboard-plugin-profile\ntensorboard --logdir runs/fill-circle-100steps-20230411_165612/\"\n```\n\n----------------------------------------\n\nTITLE: Inference with Fine-tuned Stable Diffusion Model using PyTorch\nDESCRIPTION: Python code to load a fine-tuned Stable Diffusion model and generate an image using PyTorch.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text2image.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\n\nmodel_path = \"path_to_saved_model\"\npipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nimage = pipe(prompt=\"yoda\").images[0]\nimage.save(\"yoda-naruto.png\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment Configuration\nDESCRIPTION: Command to initialize the Accelerate environment configuration which will prompt the user through a series of questions to set up their training environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Initializing UNet2DModel in Training Script\nDESCRIPTION: Code snippet showing how the UNet2DModel is initialized and configured in the training script, allowing customization of model architecture parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = UNet2DModel(\n    sample_size=args.resolution,\n    in_channels=3,\n    out_channels=3,\n    layers_per_block=2,\n    block_out_channels=(128, 128, 256, 256, 512, 512),\n    down_block_types=(\n        \"DownBlock2D\",\n        \"DownBlock2D\",\n        \"DownBlock2D\",\n        \"DownBlock2D\",\n        \"AttnDownBlock2D\",\n        \"DownBlock2D\",\n    ),\n    up_block_types=(\n        \"UpBlock2D\",\n        \"AttnUpBlock2D\",\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: This code snippet shows how to install the necessary Python dependencies for the textual inversion example. It assumes you are in the `examples` directory of the Diffusers repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Downloading Training Dataset\nDESCRIPTION: Python code to download a sample dataset of dog images from Hugging Face Hub for use in the DreamBooth training example.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sd3.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./dog\"\nsnapshot_download(\n    \"diffusers/dog-example\",\n    local_dir=local_dir, repo_type=\"dataset\",\n    ignore_patterns=\".gitattributes\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Diffusion Weights to Attention Layers\nDESCRIPTION: Sets up the custom attention processors for the UNet by correctly configuring cross-attention dimensions and hidden sizes for each block. This creates customized attention layers with appropriate weights and configurations for the attention mechanisms.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nst = unet.state_dict()\nfor name, _ in unet.attn_processors.items():\n    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n    if name.startswith(\"mid_block\"):\n        hidden_size = unet.config.block_out_channels[-1]\n    elif name.startswith(\"up_blocks\"):\n        block_id = int(name[len(\"up_blocks.\")])\n        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n    elif name.startswith(\"down_blocks\"):\n        block_id = int(name[len(\"down_blocks.\")])\n        hidden_size = unet.config.block_out_channels[block_id]\n    layer_name = name.split(\".processor\")[0]\n    weights = {\n        \"to_k_custom_diffusion.weight\": st[layer_name + \".to_k.weight\"],\n        \"to_v_custom_diffusion.weight\": st[layer_name + \".to_v.weight\"],\n    }\n    if train_q_out:\n        weights[\"to_q_custom_diffusion.weight\"] = st[layer_name + \".to_q.weight\"]\n        weights[\"to_out_custom_diffusion.0.weight\"] = st[layer_name + \".to_out.0.weight\"]\n        weights[\"to_out_custom_diffusion.0.bias\"] = st[layer_name + \".to_out.0.bias\"]\n    if cross_attention_dim is not None:\n        custom_diffusion_attn_procs[name] = attention_class(\n            train_kv=train_kv,\n            train_q_out=train_q_out,\n            hidden_size=hidden_size,\n            cross_attention_dim=cross_attention_dim,\n        ).to(unet.device)\n        custom_diffusion_attn_procs[name].load_state_dict(weights)\n    else:\n        custom_diffusion_attn_procs[name] = attention_class(\n            train_kv=False,\n            train_q_out=False,\n            hidden_size=hidden_size,\n            cross_attention_dim=cross_attention_dim,\n        )\ndel st\nunet.set_attn_processor(custom_diffusion_attn_procs)\ncustom_diffusion_layers = AttnProcsLayers(unet.attn_processors)\n```\n\n----------------------------------------\n\nTITLE: Adding section redirect in Markdown\nDESCRIPTION: Markdown syntax for preserving old section links when renaming or moving sections, which helps maintain backwards compatibility with existing references.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\nSections that were moved:\n\n[ <a href=\"#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\n\n----------------------------------------\n\nTITLE: Launching DreamBooth Training for SANA with LoRA\nDESCRIPTION: Bash command to start DreamBooth training using LoRA for SANA models, including various parameters for model configuration, training settings, and output options.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sana.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"trained-sana-lora\"\n\naccelerate launch train_dreambooth_lora_sana.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"bf16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --use_8bit_adam \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Moving DiffusionPipeline to CUDA Device in Python\nDESCRIPTION: Transfers the generator object to a CUDA-enabled GPU for faster inference. This is recommended as the model has around 1.4 billion parameters.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/unconditional_image_generation.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> generator.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: HTML License Header\nDESCRIPTION: Copyright and Apache 2.0 license header for the documentation file\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/unipc.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Defining Pipeline Default Parameters\nDESCRIPTION: Setting default parameters for the text-to-image generation pipeline, including prompt, negative prompt, seed, guidance scale, and number of steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndefault_prompt = ...\ndefault_neg_prompt = ...\ndefault_seed = 33\ndefault_guidance_scale = 5.0\ndefault_num_steps = 25\n```\n\n----------------------------------------\n\nTITLE: Saving a Pipeline as a Non-EMA Variant\nDESCRIPTION: Save a diffusion pipeline as a non-EMA variant in the same directory as the original checkpoint, which is useful for fine-tuning workflows.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# save as non-ema variant\nstable_diffusion.save_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", variant=\"non_ema\")\n```\n\n----------------------------------------\n\nTITLE: Disabling All LoRA Adapters\nDESCRIPTION: Disables all LoRA adapters to generate an image using only the base model without any adapter modifications.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipe.disable_lora()\n\nprompt = \"toy_face of a hacker with a hoodie\"\nimage = pipe(prompt, num_inference_steps=30, generator=torch.manual_seed(0)).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Accessing Pipeline Components\nDESCRIPTION: Shows how to access individual components of a diffusion pipeline, such as models and schedulers, which are documented in the model_index.json file and accessible as attributes of the pipeline object.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md#2025-04-11_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npipeline.components\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for HuggingFace Diffusers\nDESCRIPTION: This requirements list defines the necessary Python packages for the Hugging Face Diffusers project. It includes specific version constraints for transformers (4.47.0), accelerate (1.2.0), and peft (>=0.14.0), while leaving torch, torchvision, and wandb without version specifications.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/flux-control/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers==4.47.0\nwandb\ntorch\ntorchvision\naccelerate==1.2.0\npeft>=0.14.0\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Shap-E\nDESCRIPTION: Installation commands for the necessary Python libraries to use Shap-E.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate trimesh\n```\n\n----------------------------------------\n\nTITLE: Configuring 🤗Accelerate Environment\nDESCRIPTION: This code snippet demonstrates how to initialize an 🤗Accelerate environment. 🤗Accelerate is a library that simplifies distributed training. This command prompts you to configure the environment settings interactively.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Full Fine-tuning Command\nDESCRIPTION: Command for full model fine-tuning without LoRA\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogview4-control/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file=accelerate_ds2.yaml train_control_cogview4.py \\\n  --pretrained_model_name_or_path=\"THUDM/CogView4-6B\" \\\n  --dataset_name=\"raulc0399/open_pose_controlnet\" \\\n  --output_dir=\"pose-control\" \\\n  --mixed_precision=\"bf16\" \\\n  --train_batch_size=2 \\\n  --dataloader_num_workers=4 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --proportion_empty_prompts=0.2 \\\n  --learning_rate=5e-5 \\\n  --adam_weight_decay=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"cosine\" \\\n  --lr_warmup_steps=1000 \\\n  --checkpointing_steps=1000 \\\n  --max_train_steps=10000 \\\n  --validation_steps=200 \\\n  --validation_image \"2_pose_1024.jpg\" \"3_pose_1024.jpg\" \\\n  --validation_prompt \"two friends sitting by each other enjoying a day at the park, full hd, cinematic\" \"person enjoying a day at the park, full hd, cinematic\" \\\n  --offload \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Training ControlNet with FLUX - Full Command\nDESCRIPTION: Complete command to train a ControlNet model with FLUX using Accelerate. Includes parameters for dataset configuration, model architecture, training hyperparameters, and validation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_controlnet_flux.py \\\n    --pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\" \\\n    --dataset_name=fusing/fill50k \\\n    --conditioning_image_column=conditioning_image \\\n    --image_column=image \\\n    --caption_column=text \\\n    --output_dir=\"path to save model\" \\\n    --mixed_precision=\"bf16\" \\\n    --resolution=512 \\\n    --learning_rate=1e-5 \\\n    --max_train_steps=15000 \\\n    --validation_steps=100 \\\n    --checkpointing_steps=200 \\\n    --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n    --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=4 \\\n    --report_to=\"wandb\" \\\n    --num_double_layers=4 \\\n    --num_single_layers=0 \\\n    --seed=42 \\\n    --push_to_hub \\\n```\n\n----------------------------------------\n\nTITLE: Installing Training Example Dependencies\nDESCRIPTION: Bash commands for cloning the Diffusers repository and installing example-specific dependencies.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -r examples/<your-example-folder>/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Gradio for Stable Diffusion Web Demos\nDESCRIPTION: This command installs or updates Gradio, which is required to create web demos for Stable Diffusion pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/overview.md#2025-04-11_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -U gradio\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained T2I-Adapter for SDXL\nDESCRIPTION: Python code for running inference with a trained T2I-Adapter model. Includes loading the model, optimizing for speed and memory usage, and generating a conditional image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteSchedulerTest\nfrom diffusers.utils import load_image\nimport torch\n\nbase_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\nadapter_path = \"path to adapter\"\n\nadapter = T2IAdapter.from_pretrained(adapter_path, torch_dtype=torch.float16)\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    base_model_path, adapter=adapter, torch_dtype=torch.float16\n)\n\n# speed up diffusion process with faster scheduler and memory optimization\npipe.scheduler = EulerAncestralDiscreteSchedulerTest.from_config(pipe.scheduler.config)\n# remove following line if xformers is not installed or when using Torch 2.0.\npipe.enable_xformers_memory_efficient_attention()\n# memory optimization.\npipe.enable_model_cpu_offload()\n\ncontrol_image = load_image(\"./conditioning_image_1.png\")\nprompt = \"pale golden rod circle with old lace background\"\n\n# generate image\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt, num_inference_steps=20, generator=generator, image=control_image\n).images[0]\nimage.save(\"./output.png\")\n```\n\n----------------------------------------\n\nTITLE: Training DreamBooth with Prior-Preservation Loss\nDESCRIPTION: Runs the DreamBooth training script with prior-preservation loss to avoid overfitting and language-drift. It includes additional parameters for class images and prior loss weight.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/colossalai/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\ntorchrun --nproc_per_node 2 train_dreambooth_colossalai.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=800 \\\n  --placement=\"cuda\"\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Habana Package\nDESCRIPTION: Command to install the Optimum Habana package using pip with eager upgrade strategy\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/habana.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install --upgrade-strategy eager optimum[habana]\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Diffusers Project\nDESCRIPTION: This snippet lists the required Python packages and their versions for the Hugging Face Diffusers project. It includes libraries for acceleration, model training, dataset handling, and image processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/autoencoderkl/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\naccelerate>=0.16.0\nbitsandbytes\ndatasets\nhuggingface_hub\nlpips\nnumpy\npackaging\nPillow\ntaming_transformers\ntorch\ntorchvision\ntqdm\ntransformers\nwandb\nxformers\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Different ways to configure an Accelerate environment for distributed training, including interactive setup, default configuration, or programmatic configuration in a notebook.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Diffusers Training\nDESCRIPTION: Command to install necessary Python packages for training unconditional image generation models, including diffusers with training components, accelerate for distributed training, and datasets for data handling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install diffusers[training] accelerate datasets\n```\n\n----------------------------------------\n\nTITLE: Loading a Diffusion Pipeline from Local Disk\nDESCRIPTION: Load a previously downloaded diffusion model pipeline from a local path instead of downloading it from the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/loading.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"./stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id)\n```\n\n----------------------------------------\n\nTITLE: Serializing a Quantized Model\nDESCRIPTION: Example of how to serialize a quantized model using the save_pretrained method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/torchao.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import FluxTransformer2DModel, TorchAoConfig\n\nquantization_config = TorchAoConfig(\"int8wo\")\ntransformer = FluxTransformer2DModel.from_pretrained(\n    \"black-forest-labs/Flux.1-Dev\",\n    subfolder=\"transformer\",\n    quantization_config=quantization_config,\n    torch_dtype=torch.bfloat16,\n)\ntransformer.save_pretrained(\"/path/to/flux_int8wo\", safe_serialization=False)\n```\n\n----------------------------------------\n\nTITLE: Initializing OmniGenTransformer2DModel in Python\nDESCRIPTION: Demonstrates how to load a pretrained OmniGenTransformer2DModel from the Hugging Face model hub with specific configurations for bfloat16 precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/omnigen_transformer.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import OmniGenTransformer2DModel\n\ntransformer = OmniGenTransformer2DModel.from_pretrained(\"Shitao/OmniGen-v1-diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Geometric\nDESCRIPTION: Command for installing the PyTorch Geometric library via Conda, which is required for handling geometric data in machine learning tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n!conda install -c rusty1s pytorch-geometric=1.7.2\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for ControlNet SDXL Training (Bash)\nDESCRIPTION: Commands to clone the diffusers repository, install it in editable mode, and install additional requirements for SDXL training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\ncd examples/controlnet\npip install -r requirements_sdxl.txt\n```\n\n----------------------------------------\n\nTITLE: Running Stable Diffusion Inference on TPU\nDESCRIPTION: Executes the Stable Diffusion pipeline on TPU devices in parallel, generating multiple images simultaneously.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrng = create_key(0)\nrng = jax.random.split(rng, jax.device_count())\n\nimages = pipeline(prompt_ids, p_params, rng, jit=True)[0]\n\nimages = images.reshape((images.shape[0] * images.shape[1],) + images.shape[-3:])\nimages = pipeline.numpy_to_pil(images)\n```\n\n----------------------------------------\n\nTITLE: Importing PriorTransformer in Python\nDESCRIPTION: Import statement for the PriorTransformer class from the diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/prior_transformer.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import PriorTransformer\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source for DreamBooth Training\nDESCRIPTION: Commands to clone the diffusers repository and install it in development mode to ensure compatibility with the latest example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Configuration\nDESCRIPTION: YAML configuration for training with DeepSpeed Zero2 optimization\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogview4-control/README.md#2025-04-11_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 1\n  gradient_clipping: 1.0\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: false\n  zero_stage: 2\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Verifying PyTorch Installation and GPU Utilization\nDESCRIPTION: This snippet checks whether PyTorch can successfully access the GPU and prints its version for verification purposes.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\n\nprint(torch.cuda.is_available())\ntorch.__version__\n```\n\n----------------------------------------\n\nTITLE: Installing Training Script Dependencies\nDESCRIPTION: Commands to navigate to the examples directory and install the required dependencies for the unconditional image generation training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/unconditional_image_generation\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Loading Diffusion Pipeline from Single-file Layout\nDESCRIPTION: Shows how to load a diffusion pipeline from a single-file layout using the from_single_file method. This example loads the Stable Diffusion XL base model from a safetensors file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\n\npipeline = StableDiffusionXLPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Accelerate Config in Python\nDESCRIPTION: Python code snippet to create a basic accelerate configuration in environments without interactive shell support.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Multi-node Distributed Training for Textual Inversion\nDESCRIPTION: Commands for setting up and running distributed training across multiple nodes using Intel Extension for PyTorch. Configures environment variables and launches training with specified node distribution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\noneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bindings_for_pytorch import cwd; print(cwd)\")\nsource $oneccl_bindings_for_pytorch_path/env/setvars.sh\n\npython -m intel_extension_for_pytorch.cpu.launch --distributed \\\n  --hostfile hostfile --nnodes 2 --nproc_per_node 2 textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=750 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\n\n----------------------------------------\n\nTITLE: Installing SDXL-specific Requirements\nDESCRIPTION: Command to install the specific dependencies required for SDXL training from the requirements file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements_sdxl.txt\n```\n\n----------------------------------------\n\nTITLE: Converting denoised output to image in Python\nDESCRIPTION: Processes the final denoised output from the DDPM model into a displayable image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from PIL import Image\n>>> import numpy as np\n\n>>> image = (input / 2 + 0.5).clamp(0, 1).squeeze()\n>>> image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n>>> image = Image.fromarray(image)\n>>> image\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch ControlNet Dependencies\nDESCRIPTION: This bash script navigates to the controlnet example directory and installs the required python dependencies using pip and the requirements.txt file. This ensures that all necessary libraries are available for the PyTorch training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"cd examples/controlnet\npip install -r requirements.txt\"\n```\n\n----------------------------------------\n\nTITLE: Installing Example Dependencies (Bash)\nDESCRIPTION: Command to install all required dependencies for a specific training example using pip and the requirements.txt file from the example directory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd diffusers\npip install -r examples/<your-example-folder>/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting up a Default 🤗 Accelerate Environment\nDESCRIPTION: This bash script sets up a default 🤗 Accelerate environment without choosing any configurations. This is a simplified way to initialize accelerate, suitable for basic configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate config default\"\n```\n\n----------------------------------------\n\nTITLE: Checking Generated Image Dimensions\nDESCRIPTION: Verifies the shape of the generated images array after parallel processing, showing the batch size and image dimensions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimages.shape\n# (8, 1, 512, 512, 3)\n```\n\n----------------------------------------\n\nTITLE: Initializing Parallel Generation with JAX pmap\nDESCRIPTION: Creates a parallelized version of the pipeline's _generate method using jax.pmap for distributed execution across devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/stable_diffusion_jax_how_to.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\np_generate = pmap(pipeline._generate)\n```\n\n----------------------------------------\n\nTITLE: Importing ScoreSdeVpScheduler in Python\nDESCRIPTION: This code snippet shows how to import the ScoreSdeVpScheduler class from the deprecated schedulers module in the diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/score_sde_vp.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.schedulers.deprecated.scheduling_sde_vp import ScoreSdeVpScheduler\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Commands to clone the Diffusers GitHub repository and install it locally for using the Textual Inversion functionality.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Training Textual Inversion Model with Flax\nDESCRIPTION: Run the textual inversion training script using Flax implementation for faster training on TPUs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text_inversion.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install -U -r requirements_flax.txt\n\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport DATA_DIR=\"./cat\"\n\npython textual_inversion_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 --scale_lr \\\n  --output_dir=\"textual_inversion_cat\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Styledrop Fine-tuning AmuSED at 512 Resolution\nDESCRIPTION: This script implements the Styledrop method for fine-tuning AmuSED at 512 resolution. It uses a learning rate of 1e-3 and a LoRA alpha of 1, expecting decent results in 1500-2000 steps. The script includes various training parameters and validation prompts for style transfer at higher resolution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --mixed_precision fp16 \\\n    --report_to wandb \\\n    --use_lora \\\n    --pretrained_model_name_or_path amused/amused-512 \\\n    --train_batch_size 1 \\\n    --lr_scheduler constant \\\n    --learning_rate 1e-3 \\\n    --validation_prompts \\\n        'A chihuahua walking on the street in [V] style' \\\n        'A banana on the table in [V] style' \\\n        'A church on the street in [V] style' \\\n        'A tabby cat walking in the forest in [V] style' \\\n    --instance_data_image 'A mushroom in [V] style.png' \\\n    --max_train_steps 100000 \\\n    --checkpointing_steps 500 \\\n    --validation_steps 100 \\\n    --resolution 512 \\\n    --lora_alpha 1\n```\n\n----------------------------------------\n\nTITLE: Running Full Test Suite\nDESCRIPTION: Command to run the complete test suite\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ make test\n```\n\n----------------------------------------\n\nTITLE: Rendering 2D Chemical Diagram from SMILES\nDESCRIPTION: Creates a 2D visualization of the selected molecule using RDKit's drawing capabilities, which represents the input to the model. The function converts SMILES notation to a molecule object and renders it as an SVG.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nmc = Chem.MolFromSmiles(dataset[0][\"smiles\"])\nmolSize = (450, 300)\ndrawer = MD2.MolDraw2DSVG(molSize[0], molSize[1])\ndrawer.DrawMolecule(mc)\ndrawer.FinishDrawing()\nsvg = drawer.GetDrawingText()\ndisplay(SVG(svg.replace(\"svg:\", \"\")))\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for SDXL Training\nDESCRIPTION: Commands to set up the Accelerate environment for distributed training, with options for interactive, default, or programmatic configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: LoRA Prior Model Fine-Tuning\nDESCRIPTION: Command for fine-tuning the Würstchen prior model using LoRA (Low-Rank Adaptation), which adds trainable rank decomposition matrices to frozen model weights. This approach is more parameter-efficient and helps prevent catastrophic forgetting.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/wuerstchen/text_to_image/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch train_text_to_image_lora_prior.py \\\n  --mixed_precision=\"fp16\" \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=768 \\\n  --train_batch_size=8 \\\n  --num_train_epochs=100 --checkpointing_steps=5000 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --seed=42 \\\n  --rank=4 \\\n  --validation_prompt=\"cute dragon creature\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"wuerstchen-prior-naruto-lora\"\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Kandinsky2.2 Decoder\nDESCRIPTION: Bash script to fine-tune the Kandinsky2.2 decoder model using Accelerate and a specified dataset.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image_decoder.py \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot naruto, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi2-decoder-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Regularization Images\nDESCRIPTION: Download 200 real images using clip-retrieval for a specific class, which helps prevent overfitting during custom model training\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython retrieve.py --class_prompt cat --class_data_dir real_reg/samples_cat --num_class_images 200\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LoRA Training\nDESCRIPTION: Bash commands to set up environment variables required for the LoRA fine-tuning process, including model name, output directory, hub model ID, and dataset name.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/lora.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"/sddata/finetune/lora/naruto\"\nexport HUB_MODEL_ID=\"naruto-lora\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing the Input Image\nDESCRIPTION: Loading the image to be edited from a URL and resizing it to the desired dimensions.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image, make_image_grid\n\nimg_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\nraw_image = load_image(img_url).resize((768, 768))\nraw_image\n```\n\n----------------------------------------\n\nTITLE: Installing Script Dependencies - Bash\nDESCRIPTION: Navigate to the example folder and install necessary Python dependencies from requirements files to run the training script efficiently.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/text_to_image\npip install -r requirements.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/text_to_image\npip install -r requirements_flax.txt\n```\n\n----------------------------------------\n\nTITLE: Saving Generated Image to File in Python\nDESCRIPTION: Saves the generated image to a PNG file using the PIL.Image save method. This allows for persistent storage of the generated image.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/unconditional_image_generation.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> image.save(\"generated_image.png\")\n```\n\n----------------------------------------\n\nTITLE: Model Components Access Pattern\nDESCRIPTION: Example demonstrating how pipeline components can be accessed and shared between pipelines using the DiffusionPipeline.components interface.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/philosophy.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDiffusionPipeline.components()\n```\n\n----------------------------------------\n\nTITLE: Loading Pipeline Example\nDESCRIPTION: Example showing the key pattern of loading a diffusion pipeline through the DiffusionPipeline.from_pretrained method, which is a core interface design pattern.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/philosophy.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDiffusionPipeline.from_pretrained(\"model_name\")\n```\n\n----------------------------------------\n\nTITLE: Initializing IP-Adapter FaceID\nDESCRIPTION: Sets up IP-Adapter FaceID for face-specific image generation using InsightFace embeddings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\npipeline.load_ip_adapter(\"h94/IP-Adapter-FaceID\", subfolder=None, weight_name=\"ip-adapter-faceid_sdxl.bin\", image_encoder_folder=None)\n```\n\n----------------------------------------\n\nTITLE: Importing LMSDiscreteScheduler in Python\nDESCRIPTION: This code snippet demonstrates how to import the LMSDiscreteScheduler class from the Hugging Face Diffusers library. It's inferred from the context of the documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/lms_discrete.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import LMSDiscreteScheduler\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies from Source\nDESCRIPTION: Commands to clone the diffusers repository and install required dependencies for training InstructPix2Pix.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/instructpix2pix.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\ncd examples/instruct_pix2pix\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Textual Inversion in a Converted KerasCV Model\nDESCRIPTION: Python code to generate images using a textual inversion model that was converted from KerasCV, using a placeholder token to personalize the output.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\"sayakpaul/textual-inversion-cat-kerascv_sd_diffusers_pipeline\")\npipeline.to(\"cuda\")\n\nplaceholder_token = \"<my-funny-cat-token>\"\nprompt = f\"two {placeholder_token} getting married, photorealistic, high quality\"\nimage = pipeline(prompt, num_inference_steps=50).images[0]\n```\n\n----------------------------------------\n\nTITLE: HTML Comment Block - License Information\nDESCRIPTION: Apache 2.0 license header comment block detailing copyright and usage terms for the HuggingFace Team codebase.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/latent_upscale.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: This command installs the Diffusers library from the cloned repository, ensuring you have the latest version and any example-specific requirements.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/text_to_image/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install .\"\n```\n\n----------------------------------------\n\nTITLE: Generating Control Frames with Canny Edge Detection in Python\nDESCRIPTION: This snippet processes input video frames to create control images using Canny edge detection. These control images will be used to guide the video frame generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_105\n\nLANGUAGE: python\nCODE:\n```\ncontrol_frames = []\n# get canny image\nfor frame in frames:\n    image = cv2.Canny(frame, 50, 100)\n    np_image = np.array(image)\n    np_image = np_image[:, :, None]\n    np_image = np.concatenate([np_image, np_image, np_image], axis=2)\n    canny_image = Image.fromarray(np_image)\n    control_frames.append(canny_image)\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face CLI for Model Deployment\nDESCRIPTION: This command logs the user into their Hugging Face account via CLI, facilitating seamless model uploads to the Hub after training. This step is crucial for managing and sharing trained models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\n# Log in to Hugging Face CLI to push model to the Hub\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Setting Default Accelerate Configuration\nDESCRIPTION: Command to set up a default Accelerate configuration without interactive prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Training Custom Diffusion with Weights & Biases Logging\nDESCRIPTION: Training command with additional parameters to enable tracking experiments and saving intermediate results using Weights & Biases (wandb).\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_custom_diffusion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --class_data_dir=./real_reg/samples_cat/ \\\n  --with_prior_preservation --real_prior --prior_loss_weight=1.0 \\\n  --class_prompt=\"cat\" --num_class_images=200 \\\n  --instance_prompt=\"photo of a <new1> cat\"  \\\n  --resolution=512  \\\n  --train_batch_size=2  \\\n  --learning_rate=1e-5  \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=250 \\\n  --scale_lr --hflip  \\\n  --modifier_token \"<new1>\" \\\n  --validation_prompt=\"<new1> cat sitting in a bucket\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Loading and Uploading Custom Datasets\nDESCRIPTION: Python code to load custom image datasets using ImageFolder and upload them to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\n# example 1: local folder\ndataset = load_dataset(\"imagefolder\", data_dir=\"path_to_your_folder\")\n\n# example 2: local files (supported formats are tar, gzip, zip, xz, rar, zstd)\ndataset = load_dataset(\"imagefolder\", data_files=\"path_to_zip_file\")\n\n# example 3: remote files (supported formats are tar, gzip, zip, xz, rar, zstd)\ndataset = load_dataset(\"imagefolder\", data_files=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\")\n\n# example 4: providing several splits\ndataset = load_dataset(\"imagefolder\", data_files={\"train\": [\"path/to/file1\", \"path/to/file2\"], \"test\": [\"path/to/file3\", \"path/to/file4\"]})\n\n# Push to hub\ndataset.push_to_hub(\"name_of_your_dataset\")\n\n# Push to private repo\ndataset.push_to_hub(\"name_of_your_dataset\", private=True)\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Text Prompts for SDXL\nDESCRIPTION: Function to tokenize the input text prompts and negative prompts for processing by the SDXL text encoders.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef tokenize_prompt(prompt, neg_prompt):\n    prompt_ids = pipeline.prepare_inputs(prompt)\n    neg_prompt_ids = pipeline.prepare_inputs(neg_prompt)\n    return prompt_ids, neg_prompt_ids\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Pipeline Execution\nDESCRIPTION: Key line that demonstrates how to execute the pipeline function asynchronously by running it in a separate executor, allowing the main thread to handle other requests while waiting for the result.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/server/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noutput = await loop.run_in_executor(None, lambda: pipeline(image_input.prompt, generator = generator))\n```\n\n----------------------------------------\n\nTITLE: IF Stage II LoRA DreamBooth Training\nDESCRIPTION: This script trains a LoRA for DeepFloyd IF Stage II upscaler model. It requires validation images for upscaling during training, uses timestep conditioning, and operates at a higher resolution (256px) with a lower learning rate than Stage I. The upscaler fine-tuning is particularly important for preserving fine-grained details.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"DeepFloyd/IF-II-L-v1.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"dreambooth_dog_upscale\"\nexport VALIDATION_IMAGES=\"dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png\"\n\npython train_dreambooth_lora.py \\\n    --report_to wandb \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --instance_data_dir=$INSTANCE_DIR \\\n    --output_dir=$OUTPUT_DIR \\\n    --instance_prompt=\"a sks dog\" \\\n    --resolution=256 \\\n    --train_batch_size=4 \\\n    --gradient_accumulation_steps=1 \\\n    --learning_rate=1e-6 \\\n    --max_train_steps=2000 \\\n    --validation_prompt=\"a sks dog\" \\\n    --validation_epochs=100 \\\n    --checkpointing_steps=500 \\\n    --pre_compute_text_embeddings \\\n    --tokenizer_max_length=77 \\\n    --text_encoder_use_attention_mask \\\n    --validation_images $VALIDATION_IMAGES \\\n    --class_labels_conditioning=timesteps\n```\n\n----------------------------------------\n\nTITLE: Training Script with Text Encoder Fine-tuning\nDESCRIPTION: Demonstrates training configuration that includes fine-tuning the text encoder, which requires more GPU memory and potentially improves results for face-related tasks\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_subject_dreambooth_inpainting/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_multi_subject_dreambooth_inpaint.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir $DATASET_1 $DATASET_2 \\\n  --output_dir=$OUTPUT_DIR \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=2 \\\n  --learning_rate=2e-6 \\\n  --max_train_steps=500 \\\n  --report_to_wandb \\\n  --train_text_encoder\n```\n\n----------------------------------------\n\nTITLE: Initializing UNet2DModel from Pretrained Checkpoint\nDESCRIPTION: Load a pretrained UNet2DModel for unconditional image generation, specifically trained on cat images\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quicktour.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNet2DModel\n\nrepo_id = \"google/ddpm-cat-256\"\nmodel = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for Training\nDESCRIPTION: Different ways to configure Accelerate for distributed training, including interactive configuration, default configuration, and programmatic configuration in Python.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Importing Remote Decode Function for Hybrid Inference in Python\nDESCRIPTION: Imports the remote_decode function from diffusers.utils.remote_utils to simplify interaction with Hybrid Inference for VAE decoding.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/hybrid_inference/vae_decode.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils.remote_utils import remote_decode\n```\n\n----------------------------------------\n\nTITLE: Downloading a LoRA Model from Civitai\nDESCRIPTION: Wget command to download a LoRA model file from Civitai for use with Diffusers, saving it with a descriptive filename.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n!wget https://civitai.com/api/download/models/19998 -O howls_moving_castle.safetensors\n```\n\n----------------------------------------\n\nTITLE: Importing ImagePipelineOutput in Python\nDESCRIPTION: This snippet shows how to import the ImagePipelineOutput class, which is used to represent the output of image generation pipelines.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latent_diffusion.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.pipelines import ImagePipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for InstructPix2Pix Training\nDESCRIPTION: Command to install the necessary dependencies for the InstructPix2Pix training example from the requirements file in the example folder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Inference Function in PyTorch\nDESCRIPTION: Configures and runs a function for distributed inference using PyTorch's distributed library. The init_process_group function creates the necessary environment, and each process is assigned a GPU to handle a specific prompt, with results saved as images. Requires PyTorch and diffusers libraries.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef run_inference(rank, world_size):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n    sd.to(rank)\n\n    if torch.distributed.get_rank() == 0:\n        prompt = \"a dog\"\n    elif torch.distributed.get_rank() == 1:\n        prompt = \"a cat\"\n\n    image = sd(prompt).images[0]\n    image.save(f\"./{'_'.join(prompt)}.png\")\n```\n\n----------------------------------------\n\nTITLE: Initializing One-Step UNet Pipeline in Python\nDESCRIPTION: Basic implementation of a custom pipeline class that inherits from DiffusionPipeline and registers UNet and scheduler modules.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nclass UnetSchedulerOneForwardPipeline(DiffusionPipeline):\n    def __init__(self, unet, scheduler):\n        super().__init__()\n\n        self.register_modules(unet=unet, scheduler=scheduler)\n```\n\n----------------------------------------\n\nTITLE: Swift Core ML Model Download\nDESCRIPTION: Python script to download compiled Core ML model variants for Swift inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/coreml.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/compiled\"\n\nmodel_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\nsnapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\nprint(f\"Model downloaded at {model_path}\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Tensorboard for Profiling\nDESCRIPTION: Commands to install and run Tensorboard with profiling plugin.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npip install tensorflow tensorboard-plugin-profile\ntensorboard --logdir runs/fill-circle-100steps-20230411_165612/\n```\n\n----------------------------------------\n\nTITLE: Generating requirements.txt using uv pip compile\nDESCRIPTION: Command used to autogenerate the requirements.txt file from requirements.in using the uv package management tool.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/server/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip compile requirements.in -o requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Launching InstructPix2Pix Training with LoRA in Bash\nDESCRIPTION: The script launches the training process for the InstructPix2Pix model with LoRA using Accelerate. It passes various parameters such as the model ID, dataset ID, resolution, and more to configure the training run. It also sets up reporting to Weights & Biases and pushing to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/instructpix2pix_lora/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_ID=\"timbrooks/instruct-pix2pix\"\nexport DATASET_ID=\"instruction-tuning-sd/cartoonization\"\nexport OUTPUT_DIR=\"instructPix2Pix-cartoonization\"\n\naccelerate launch train_instruct_pix2pix_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_ID \\\n  --dataset_name=$DATASET_ID \\\n  --enable_xformers_memory_efficient_attention \\\n  --resolution=256 --random_flip \\\n  --train_batch_size=2 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n  --learning_rate=5e-05 --lr_warmup_steps=0 \\\n  --val_image_url=\"https://hf.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png\" \\\n  --validation_prompt=\"Generate a cartoonized version of the natural image\" \\\n  --seed=42 \\\n  --rank=4 \\\n  --output_dir=$OUTPUT_DIR \\\n  --report_to=wandb \\\n  --push_to_hub \\\n  --original_image_column=\"original_image\" \\\n  --edited_image_column=\"cartoonized_image\" \\\n  --edit_prompt_column=\"edit_prompt\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Images\nDESCRIPTION: Commands to download test conditioning images used for validation during training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\n\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```\n\n----------------------------------------\n\nTITLE: Generating a Caption for an Input Image with BLIP in Python\nDESCRIPTION: Loads an image from a URL, resizes it, and generates a caption using the previously defined generate_caption function. The caption can then be used in the StableDiffusionDiffEditPipeline's invert function.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/diffedit.md#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.utils import load_image\n\nimg_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\nraw_image = load_image(img_url).resize((768, 768))\ncaption = generate_caption(raw_image, model, processor)\n```\n\n----------------------------------------\n\nTITLE: A100 GPU Training Configuration\nDESCRIPTION: Demonstrates a high-performance training configuration used on an A100 GPU with larger batch size and text encoder fine-tuning\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_subject_dreambooth_inpainting/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_multi_subject_dreambooth_inpaint.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir $DATASET_1 $DATASET_2 \\\n  --output_dir=$OUTPUT_DIR \\\n  --resolution=512 \\\n  --train_batch_size=10 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-6 \\\n  --max_train_steps=500 \\\n  --report_to_wandb \\\n  --train_text_encoder\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Configuration\nDESCRIPTION: Commands for setting up 🤗Accelerate for distributed training with different options depending on the environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Disabling FreeU in Diffusion Pipeline\nDESCRIPTION: This snippet shows how to disable the FreeU technique in a diffusion pipeline after it has been enabled.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/image_quality.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline.disable_freeu()\n```\n\n----------------------------------------\n\nTITLE: Running Stable Diffusion Inference in Swift\nDESCRIPTION: Command to run text-to-image generation using Swift and Core ML. This example uses the Swift Package Manager to execute the StableDiffusionSample program with the compiled Core ML model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/coreml.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nswift run StableDiffusionSample --resource-path models/coreml-stable-diffusion-v1-4_original_compiled --compute-units all \"a photo of an astronaut riding a horse on mars\"\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Login Command\nDESCRIPTION: Command to login to Hugging Face Hub for model pushing capabilities.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_lumina2.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Steps to clone the diffusers repository from GitHub and install it in a new virtual environment, which is recommended to ensure compatibility with the latest example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/wuerstchen/text_to_image/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Navigating to Custom Diffusion Example Directory\nDESCRIPTION: Command to change to the custom diffusion example directory in the diffusers repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/custom_diffusion\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements (Bash)\nDESCRIPTION: This command navigates to the Kandinsky 2.2 example folder and installs the required dependencies from the `requirements.txt` file. This ensures that all necessary packages for the training script are installed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/kandinsky2_2/text_to_image\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Git LFS for Large File Storage\nDESCRIPTION: Installs Git Large File Storage (LFS) to handle versioning of large model checkpoint files. This is necessary when pushing diffusion models to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n!sudo apt -qq install git-lfs\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Extending Graph Order for Molecular Structures in Python\nDESCRIPTION: Creates higher-order connections in molecular graphs by computing powers of the adjacency matrix. This allows the model to capture interactions between atoms that are not directly bonded but connected through intermediate atoms.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _extend_graph_order(num_nodes, edge_index, edge_type, order=3):\n    \"\"\"\n    Args:\n        num_nodes:  Number of atoms.\n        edge_index: Bond indices of the original graph.\n        edge_type:  Bond types of the original graph.\n        order:  Extension order.\n    Returns:\n        new_edge_index: Extended edge indices. new_edge_type: Extended edge types.\n    \"\"\"\n\n    def binarize(x):\n        return torch.where(x > 0, torch.ones_like(x), torch.zeros_like(x))\n\n    def get_higher_order_adj_matrix(adj, order):\n        \"\"\"\n        Args:\n            adj:        (N, N)\n            type_mat:   (N, N)\n        Returns:\n            Following attributes will be updated:\n              - edge_index\n              - edge_type\n            Following attributes will be added to the data object:\n              - bond_edge_index: Original edge_index.\n        \"\"\"\n        adj_mats = [\n            torch.eye(adj.size(0), dtype=torch.long, device=adj.device),\n            binarize(adj + torch.eye(adj.size(0), dtype=torch.long, device=adj.device)),\n        ]\n\n        for i in range(2, order + 1):\n            adj_mats.append(binarize(adj_mats[i - 1] @ adj_mats[1]))\n        order_mat = torch.zeros_like(adj)\n\n        for i in range(1, order + 1):\n            order_mat += (adj_mats[i] - adj_mats[i - 1]) * i\n\n        return order_mat\n\n    num_types = 22\n    # given from len(BOND_TYPES), where BOND_TYPES = {t: i for i, t in enumerate(BT.names.values())}\n    # from rdkit.Chem.rdchem import BondType as BT\n    N = num_nodes\n    adj = to_dense_adj(edge_index).squeeze(0)\n    adj_order = get_higher_order_adj_matrix(adj, order)  # (N, N)\n\n    type_mat = to_dense_adj(edge_index, edge_attr=edge_type).squeeze(0)  # (N, N)\n    type_highorder = torch.where(adj_order > 1, num_types + adj_order - 1, torch.zeros_like(adj_order))\n    assert (type_mat * type_highorder == 0).all()\n    type_new = type_mat + type_highorder\n\n    new_edge_index, new_edge_type = dense_to_sparse(type_new)\n    _, edge_order = dense_to_sparse(adj_order)\n\n    # data.bond_edge_index = data.edge_index  # Save original edges\n    new_edge_index, new_edge_type = coalesce(new_edge_index, new_edge_type.long(), N, N)  # modify data\n\n    return new_edge_index, new_edge_type\n```\n\n----------------------------------------\n\nTITLE: Defining Gaussian Blur Operator for Diffusion Posterior Sampling in Python\nDESCRIPTION: Implements a Gaussian blur operator as a PyTorch module for use in the Diffusion Posterior Sampling Pipeline. This operator is used to generate corrupted images for reconstruction tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_83\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn.functional as F\nimport scipy\nfrom torch import nn\n\n# define the Gaussian blurring operator first\nclass GaussialBlurOperator(nn.Module):\n    def __init__(self, kernel_size, intensity):\n        super().__init__()\n\n        class Blurkernel(nn.Module):\n            def __init__(self, blur_type='gaussian', kernel_size=31, std=3.0):\n                super().__init__()\n                self.blur_type = blur_type\n                self.kernel_size = kernel_size\n                self.std = std\n                self.seq = nn.Sequential(\n                    nn.ReflectionPad2d(self.kernel_size//2),\n                    nn.Conv2d(3, 3, self.kernel_size, stride=1, padding=0, bias=False, groups=3)\n                )\n                self.weights_init()\n\n            def forward(self, x):\n                return self.seq(x)\n\n            def weights_init(self):\n                if self.blur_type == \"gaussian\":\n                    n = np.zeros((self.kernel_size, self.kernel_size))\n                    n[self.kernel_size // 2, self.kernel_size // 2] = 1\n                    k = scipy.ndimage.gaussian_filter(n, sigma=self.std)\n                    k = torch.from_numpy(k)\n                    self.k = k\n                    for name, f in self.named_parameters():\n                        f.data.copy_(k)\n                elif self.blur_type == \"motion\":\n                    k = Kernel(size=(self.kernel_size, self.kernel_size), intensity=self.std).kernelMatrix\n                    k = torch.from_numpy(k)\n                    self.k = k\n                    for name, f in self.named_parameters():\n                        f.data.copy_(k)\n\n            def update_weights(self, k):\n                if not torch.is_tensor(k):\n                    k = torch.from_numpy(k)\n                for name, f in self.named_parameters():\n                    f.data.copy_(k)\n\n            def get_kernel(self):\n                return self.k\n\n        self.kernel_size = kernel_size\n        self.conv = Blurkernel(blur_type='gaussian',\n                            kernel_size=kernel_size,\n                            std=intensity)\n        self.kernel = self.conv.get_kernel()\n        self.conv.update_weights(self.kernel.type(torch.float32))\n\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, data, **kwargs):\n        return self.conv(data)\n\n    def transpose(self, data, **kwargs):\n        return data\n\n    def get_kernel(self):\n        return self.kernel.view(1, 1, self.kernel_size, self.kernel_size)\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Mixed Precision (Bash)\nDESCRIPTION: This command launches the training script for the Kandinsky 2.2 prior model with mixed precision (fp16).  It speeds up training by using lower precision floating-point numbers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/kandinsky.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_text_to_image_prior.py \\\n  --mixed_precision=\"fp16\"\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment Variables\nDESCRIPTION: Commands to set required environment variables for model training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"runs/fill-circle-{timestamp}\"\nexport HUB_MODEL_ID=\"controlnet-fill-circle\"\n```\n\n----------------------------------------\n\nTITLE: Checking Memory Footprint of Quantized Models\nDESCRIPTION: Demonstrates how to check the memory footprint of a quantized model using the get_memory_footprint method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(model.get_memory_footprint())\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install Flax and Diffusers libraries required for the SDXL pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install flax diffusers transformers\n```\n\n----------------------------------------\n\nTITLE: Badge Display HTML\nDESCRIPTION: HTML markup for displaying the LoRA badge in the documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/ldm3d_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"flex flex-wrap space-x-1\">\n  <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Reduced Inference Steps in Python\nDESCRIPTION: Generates an image using only 20 inference steps instead of the default 50, resulting in much faster generation without significant quality loss.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\nimage = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements and Configuring Accelerate\nDESCRIPTION: Commands for installing project requirements and setting up Accelerate configuration for training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_lumina2.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements_sana.txt\nacccelerate config\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Installs the latest development version of Diffusers directly from GitHub\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\n----------------------------------------\n\nTITLE: Launching Training Script without Text Encoder Fine-tuning\nDESCRIPTION: Initiates the multi-subject Dreambooth inpainting training using Hugging Face Accelerate, with default configuration and multiple dataset inputs\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_subject_dreambooth_inpainting/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_multi_subject_dreambooth_inpaint.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir $DATASET_1 $DATASET_2 \\\n  --output_dir=$OUTPUT_DIR \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=2 \\\n  --learning_rate=3e-6 \\\n  --max_train_steps=500 \\\n  --report_to_wandb\n```\n\n----------------------------------------\n\nTITLE: Training Textual Inversion with Flax/JAX (Bash)\nDESCRIPTION: Bash command to run textual inversion training using the Flax/JAX implementation, which is faster on TPUs and GPUs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport DATA_DIR=\"path-to-dir-containing-images\"\n\npython textual_inversion_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" \\\n  --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 \\\n  --scale_lr \\\n  --output_dir=\"textual_inversion_cat\"\n```\n\n----------------------------------------\n\nTITLE: License Header Comment in Markdown\nDESCRIPTION: Apache 2.0 license notice and copyright information for the StableCascadeUNet implementation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/stable_cascade_unet.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepFloyd IF Text-to-Image with Upscaling using PyTorch Compile\nDESCRIPTION: This code demonstrates how to set up a three-stage pipeline combining DeepFloyd IF models (stages I and II) with Stable Diffusion upscaler for high-quality image generation. It applies torch.compile() optimization to significantly improve inference speed on CUDA devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\nrun_compile = True  # Set True / False\n\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-M-v1.0\", variant=\"fp16\", text_encoder=None, torch_dtype=torch.float16)\npipe.to(\"cuda\")\npipe_2 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-II-M-v1.0\", variant=\"fp16\", text_encoder=None, torch_dtype=torch.float16)\npipe_2.to(\"cuda\")\npipe_3 = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", torch_dtype=torch.float16)\npipe_3.to(\"cuda\")\n\n\npipe.unet.to(memory_format=torch.channels_last)\npipe_2.unet.to(memory_format=torch.channels_last)\npipe_3.unet.to(memory_format=torch.channels_last)\n\nif run_compile:\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe_2.unet = torch.compile(pipe_2.unet, mode=\"reduce-overhead\", fullgraph=True)\n    pipe_3.unet = torch.compile(pipe_3.unet, mode=\"reduce-overhead\", fullgraph=True)\n\nprompt = \"the blue hulk\"\n\nprompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)\nneg_prompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)\n\nfor _ in range(3):\n    image = pipe(prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, output_type=\"pt\").images\n    image_2 = pipe_2(image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, output_type=\"pt\").images\n    image_3 = pipe_3(prompt=prompt, image=image, noise_level=100).images\n```\n\n----------------------------------------\n\nTITLE: Importing RDKit Visualization Tools\nDESCRIPTION: Imports necessary libraries from IPython, RDKit, and RDKit's drawing module to visualize 2D chemical diagrams of molecules.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import SVG, display\nfrom rdkit import Chem\nfrom rdkit.Chem.Draw import rdMolDraw2D as MD2\n```\n\n----------------------------------------\n\nTITLE: Accelerate Environment Initialization (Default Configuration)\nDESCRIPTION: This code snippet initializes an Accelerate environment to facilitate training on multiple GPUs/TPUs with default configurations. It is especially useful in setups that allow for interactive shell.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Running Textual Inversion Training with Flax/JAX\nDESCRIPTION: This code snippet demonstrates how to launch a textual inversion training run using the `textual_inversion_flax.py` script. It sets environment variables for the model name and data directory, and then runs the training script with various hyperparameters using python command directly.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/README.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport DATA_DIR=\"path-to-dir-containing-images\"\n\npython textual_inversion_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 --scale_lr \\\n  --output_dir=\"textual_inversion_cat\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Connecting to TPU VM\nDESCRIPTION: Commands to create a TPUv4-8 VM instance on Google Cloud and establish an SSH connection.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nZONE=us-central2-b\nTPU_TYPE=v4-8\nVM_NAME=hg_flax\n\ngcloud alpha compute tpus tpu-vm create $VM_NAME \\\n --zone $ZONE \\\n --accelerator-type $TPU_TYPE \\\n --version  tpu-vm-v4-base\n\ngcloud alpha compute tpus tpu-vm ssh $VM_NAME --zone $ZONE --\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Python methods to configure the Hugging Face Accelerate environment for distributed training\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/README.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Example of previewing Diffusers documentation\nDESCRIPTION: Specific example command for previewing the Diffusers documentation in English, which starts a local server at http://localhost:3000.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndoc-builder preview diffusers docs/source/en\n```\n\n----------------------------------------\n\nTITLE: Accelerate Configuration Setup\nDESCRIPTION: Commands to initialize Accelerate environment for training. Includes both CLI and Python approaches.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/instructpix2pix.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Creating a Batch Input Function in Python\nDESCRIPTION: Defines a helper function that prepares inputs for batch generation. It creates multiple generators with different seeds and replicates the prompt for the specified batch size.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_inputs(batch_size=1):\n    generator = [torch.Generator(\"cuda\").manual_seed(i) for i in range(batch_size)]\n    prompts = batch_size * [prompt]\n    num_inference_steps = 20\n\n    return {\"prompt\": prompts, \"generator\": generator, \"num_inference_steps\": num_inference_steps}\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Stable Diffusion\nDESCRIPTION: This Bash snippet sets up environment variables that specify the model and dataset names. These variables are essential for training the Stable Diffusion model using specified datasets and pretrained models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/lora/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n```\n\n----------------------------------------\n\nTITLE: Cloning Diffusers Repository\nDESCRIPTION: Clones the Diffusers repository for editable installation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers.git\ncd diffusers\n```\n\n----------------------------------------\n\nTITLE: Installing Training Dependencies\nDESCRIPTION: This command installs the required dependencies for running the training scripts, as specified in the `requirements.txt` file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/text_to_image/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install -r requirements.txt\"\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements\nDESCRIPTION: Installing required dependencies from requirements.txt file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/ip_adapter/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Pushing the Converted Model to a Pull Request\nDESCRIPTION: Git command to push the converted model back to the original repository as a pull request for testing and integration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit push origin pr/13:refs/pr/13\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion Pipeline with Traced UNet Integration\nDESCRIPTION: This code snippet demonstrates how to load a previously traced UNet model and integrate it into the Stable Diffusion pipeline. It defines a `TracedUNet` class that wraps the traced model and overrides the `forward` method to return a `UNet2DConditionOutput` object, ensuring compatibility with the pipeline's expected output format.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/memory.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableDiffusionPipeline\nimport torch\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass UNet2DConditionOutput:\n    sample: torch.Tensor\n\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n).to(\"cuda\")\n\n# use jitted unet\nunet_traced = torch.jit.load(\"unet_traced.pt\")\n\n\n# del pipe.unet\nclass TracedUNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.in_channels = pipe.unet.config.in_channels\n        self.device = pipe.unet.device\n\n    def forward(self, latent_model_input, t, encoder_hidden_states):\n        sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]\n        return UNet2DConditionOutput(sample=sample)\n\n\npipe.unet = TracedUNet()\n\nwith torch.inference_mode():\n    image = pipe([prompt] * 1, num_inference_steps=50).images[0]\n```\n\n----------------------------------------\n\nTITLE: Full Fine-tuning AmuSED with LoRA\nDESCRIPTION: This script fine-tunes the AmuSED model using LoRA (Low-Rank Adaptation). It uses a learning rate of 1e-4 and expects decent results in 500-1000 steps. The script includes options for batch size, gradient accumulation, and various training parameters, with the addition of the LoRA flag.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/amused/README.md#2025-04-11_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --train_batch_size <batch size> \\\n    --gradient_accumulation_steps <gradient accumulation steps> \\\n    --learning_rate 1e-4 \\\n    --use_lora \\\n    --pretrained_model_name_or_path amused/amused-512 \\\n    --instance_data_dataset  'monadical-labs/minecraft-preview' \\\n    --prompt_prefix 'minecraft ' \\\n    --image_key image \\\n    --prompt_key text \\\n    --resolution 512 \\\n    --mixed_precision fp16 \\\n    --lr_scheduler constant \\\n    --validation_prompts \\\n        'minecraft Avatar' \\\n        'minecraft character' \\\n        'minecraft' \\\n        'minecraft president' \\\n        'minecraft pig' \\\n    --max_train_steps 10000 \\\n    --checkpointing_steps 500 \\\n    --validation_steps 250 \\\n    --gradient_checkpointing\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for RealFill with Bash\nDESCRIPTION: This snippet shows how to install necessary training dependencies for the RealFill script by navigating to the designated folder and using pip to install the packages listed in 'requirements.txt'.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/realfill/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd realfill\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: VAE Image Processor Documentation Header\nDESCRIPTION: License and copyright header for the VAE Image Processor documentation file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/image_processor.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Install bitsandbytes - Python\nDESCRIPTION: Installs the bitsandbytes library using pip, which is required for using the 8-bit Adam optimizer. This is a dependency for optimizing training on GPUs with limited memory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n\"pip install bitsandbytes\"\n```\n\n----------------------------------------\n\nTITLE: Launching Accelerate Script for Distributed Inference\nDESCRIPTION: Runs the Python script using the accelerate launch command with the specified number of GPU processes. The num_processes parameter determines the number of GPUs utilized during execution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/distributed_inference.md#2025-04-11_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\naccelerate launch run_distributed.py --num_processes=2\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Model with DiffusionPipeline\nDESCRIPTION: Loads the stable-diffusion-v1-5 model using DiffusionPipeline and sets up the prompt for image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/stable_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipeline = DiffusionPipeline.from_pretrained(model_id, use_safetensors=True)\n\nprompt = \"portrait photo of a old warrior chief\"\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum with ONNX Runtime Support\nDESCRIPTION: Installs the Optimum library with ONNX Runtime support using pip.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/onnx.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -q optimum[\"onnxruntime\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Flax/JAX Dependencies\nDESCRIPTION: This code snippet shows how to install the necessary Python dependencies for the Flax/JAX textual inversion example. This command installs dependencies listed in `requirements_flax.txt`.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -U -r requirements_flax.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment\nDESCRIPTION: Configure the Hugging Face Accelerate environment, which provides optimization and distributed training capabilities for machine learning models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/unconditional_image_generation/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Accelerate Config in Python\nDESCRIPTION: Initialize Accelerate configuration programmatically in Python.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text_inversion.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing ColossalAI from Source\nDESCRIPTION: Clones the ColossalAI repository and installs it from source.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/colossalai/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# install colossalai\npip install .\n```\n\n----------------------------------------\n\nTITLE: Implementing FABRIC Pipeline for Feedback-based Image Generation in Diffusers\nDESCRIPTION: This code demonstrates how to use the FABRIC pipeline to generate images with optional feedback. It loads a pre-trained model, generates an image from a prompt, and shows how to incorporate feedback images to influence the generation process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom diffusers import DiffusionPipeline\n\n# load the pipeline\n# make sure you're logged in with `huggingface-cli login`\nmodel_id_or_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n# can also be used with dreamlike-art/dreamlike-photoreal-2.0\npipe = DiffusionPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16, custom_pipeline=\"pipeline_fabric\").to(\"cuda\")\n\n# let's specify a prompt\nprompt = \"An astronaut riding an elephant\"\nnegative_prompt = \"lowres, cropped\"\n\n# call the pipeline\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_inference_steps=20,\n    generator=torch.manual_seed(12)\n).images[0]\n\nimage.save(\"horse_to_elephant.jpg\")\n\n# let's try another example with feedback\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png\"\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\nprompt = \"photo, A blue colored car, fish eye\"\nliked = [init_image]\n## same goes with disliked\n\n# call the pipeline\ntorch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    liked=liked,\n    num_inference_steps=20,\n).images[0]\n\nimage.save(\"black_to_blue.png\")\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Community Projects List\nDESCRIPTION: HTML table markup that displays a list of community projects with their names and descriptions. Each project includes a link to its GitHub repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/community_projects.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table>\n    <tr>\n        <th>Project Name</th>\n        <th>Description</th>\n    </tr>\n  <tr style=\"border-top: 2px solid black\">\n    <td><a href=\"https://github.com/carson-katri/dream-textures\"> dream-textures </a></td>\n    <td>Stable Diffusion built-in to Blender</td>\n  </tr>\n  <tr style=\"border-top: 2px solid black\">\n    <td><a href=\"https://github.com/megvii-research/HiDiffusion\"> HiDiffusion </a></td>\n    <td>Increases the resolution and speed of your diffusion model by only adding a single line of code</td>\n  </tr>\n  <!-- Additional rows omitted for brevity -->\n</table>\n```\n\n----------------------------------------\n\nTITLE: Training Stable Diffusion 1.5 with DPO\nDESCRIPTION: Command for training SD 1.5 with Diffusion DPO using LoRA. Uses 8-bit Adam optimizer, gradient checkpointing, and fp16 precision. Includes validation and wandb logging.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/diffusion_dpo/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_diffusion_dpo.py \\\n  --pretrained_model_name_or_path=stable-diffusion-v1-5/stable-diffusion-v1-5  \\\n  --output_dir=\"diffusion-dpo\" \\\n  --mixed_precision=\"fp16\" \\\n  --dataset_name=kashif/pickascore \\\n  --resolution=512 \\\n  --train_batch_size=16 \\\n  --gradient_accumulation_steps=2 \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --rank=8 \\\n  --learning_rate=1e-5 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=10000 \\\n  --checkpointing_steps=2000 \\\n  --run_validation --validation_steps=200 \\\n  --seed=\"0\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Downloading Conditioning Images for Training T2I-Adapter\nDESCRIPTION: Commands to download sample conditioning images that will be used during the training process. These images provide the visual cues that the T2I-Adapter will learn to respond to.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Images After Training\nDESCRIPTION: Loads and displays the final generated image samples created by the trained diffusion model to visualize the training results.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport glob\n\nsample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\nImage.open(sample_images[-1])\n```\n\n----------------------------------------\n\nTITLE: Creating Accelerate Configuration in Python\nDESCRIPTION: Python code to create a basic Accelerate configuration when working in environments that don't support interactive shells, such as notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch 2.0 and Diffusers\nDESCRIPTION: Command to upgrade PyTorch and Diffusers to versions compatible with PyTorch 2.0 optimizations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/torch2.0.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade torch diffusers\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dependencies with uv\nDESCRIPTION: Command to upgrade the server dependencies using the uv package manager.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/create_a_server.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip compile requirements.in -o requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry on Windows\nDESCRIPTION: Disables telemetry collection for Diffusers on Windows systems\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nset HF_HUB_DISABLE_TELEMETRY=1\n```\n\n----------------------------------------\n\nTITLE: Installing watchdog for live documentation preview\nDESCRIPTION: Command to install the watchdog module, which is required for previewing documentation changes in real-time.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install watchdog\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Stable Diffusion XL\nDESCRIPTION: Installation commands for required dependencies including transformers, accelerate, safetensors, and invisible-watermark packages to use SDXL properly.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/api/pipelines/stable_diffusion/stable_diffusion_xl.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install transformers\npip install accelerate\npip install safetensors\npip install invisible-watermark>=0.2.0\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Accelerate Config in Python\nDESCRIPTION: Python code for creating a basic accelerate configuration in non-interactive environments such as Jupyter notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/custom_diffusion.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Checking Out a Pull Request for Model Conversion\nDESCRIPTION: Commands to navigate to the repository directory, fetch a specific pull request, and check it out for the conversion process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/other-formats.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd TemporalNet && git fetch origin refs/pr/13:pr/13\ngit checkout pr/13\n```\n\n----------------------------------------\n\nTITLE: Implementing Attention Slicing for Memory Efficiency\nDESCRIPTION: Enables attention slicing in the pipeline to reduce memory consumption, allowing for larger batch sizes during inference.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/stable_diffusion.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline.enable_attention_slicing()\n```\n\n----------------------------------------\n\nTITLE: Running Slow Tests with Pytest - Python Bash\nDESCRIPTION: Runs slower tests, which are skipped by default. Set the environment variable `RUN_SLOW` to 'yes' to include them in the test execution.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./tests/\n```\n\n----------------------------------------\n\nTITLE: Installing diffusers from source for LCM training\nDESCRIPTION: Instructions for cloning the diffusers repository and installing it in development mode to ensure compatibility with the latest example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Predicting Noise Residual with UNet Model\nDESCRIPTION: Performs inference with the UNet model to predict the noise residual for a given timestep in the diffusion process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> with torch.no_grad():\n...     noisy_residual = model(sample=noisy_sample, timestep=2).sample\n```\n\n----------------------------------------\n\nTITLE: Loading Pivotal Tuning Embeddings for Inference\nDESCRIPTION: Loads the trained pivotal tuning embeddings for both CLIP and T5 text encoders. This is the second step in the inference process when using pivotal tuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntext_encoders = [pipe.text_encoder, pipe.text_encoder_2]\ntokenizers = [pipe.tokenizer, pipe.tokenizer_2]\n\nembedding_path = hf_hub_download(repo_id=repo_id, filename=\"3d-icon-Flux-LoRA_emb.safetensors\", repo_type=\"model\")\n\nstate_dict = load_file(embedding_path)\n# load embeddings of text_encoder 1 (CLIP ViT-L/14)\npipe.load_textual_inversion(state_dict[\"clip_l\"], token=[\"<s0>\", \"<s1>\"], text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)\n# load embeddings of text_encoder 2 (T5 XXL) - ignore this line if you didn't enable `--enable_t5_ti`\npipe.load_textual_inversion(state_dict[\"t5\"], token=[\"<s0>\", \"<s1>\"], text_encoder=pipe.text_encoder_2, tokenizer=pipe.tokenizer_2)\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Account for Model Pushing\nDESCRIPTION: Command to log into Hugging Face account, necessary for pushing trained models to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sana.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Textual Inversion\nDESCRIPTION: Command to install required dependencies for training Textual Inversion models\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Diffusers\nDESCRIPTION: Install the necessary libraries for using the Diffusers library, including Accelerate for hardware acceleration and Transformers for supporting popular diffusion models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade diffusers accelerate transformers\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Steps to clone and install the Diffusers library from source code for training IP Adapter.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/ip_adapter/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry on Linux/MacOS\nDESCRIPTION: Disables telemetry collection for Diffusers on Unix-based systems\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_HUB_DISABLE_TELEMETRY=1\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face CLI for gated models\nDESCRIPTION: Command to log in to Hugging Face to access gated models like FLUX.1. This is required after accepting the usage terms on the model's page.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/inference/flux/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Community Scripts Overview Table in Markdown\nDESCRIPTION: A markdown table listing community examples with their descriptions, code references, notebook links and authors. Includes entries for IP-Adapter Negative Noise, Asymmetric Tiling, and Prompt Scheduling Callback.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README_community_scripts.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Example                                                                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Code Example                                                                              | Colab                                                                                                                                                                                                              |                                                        Author |\n|:--------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------:|\n| Using IP-Adapter with Negative Noise                                                                                                  | Using negative noise with IP-adapter to better control the generation (see the [original post](https://github.com/huggingface/diffusers/discussions/7167) on the forum for more details)                                                                                                                                                                                                                                    | [IP-Adapter Negative Noise](#ip-adapter-negative-noise)                                   |[Notebook](https://github.com/huggingface/notebooks/blob/main/diffusers/ip_adapter_negative_noise.ipynb) | [Álvaro Somoza](https://github.com/asomoza)|\n| Asymmetric Tiling                                                                                                  |configure seamless image tiling independently for the X and Y axes                                                                                                                                                                                                      | [Asymmetric Tiling](#Asymmetric-Tiling )                                   |[Notebook](https://github.com/huggingface/notebooks/blob/main/diffusers/asymetric_tiling.ipynb) | [alexisrolland](https://github.com/alexisrolland)|\n| Prompt Scheduling Callback                                                                                                  |Allows changing prompts during a generation                                                                                                                                                                                                      | [Prompt Scheduling-Callback](#Prompt-Scheduling-Callback )                                   |[Notebook](https://github.com/huggingface/notebooks/blob/main/diffusers/prompt_scheduling_callback.ipynb) | [hlky](https://github.com/hlky)|\n```\n\n----------------------------------------\n\nTITLE: Decoding Latent Image\nDESCRIPTION: Decodes the final latent representation into an image using the VAE decoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/write_own_pipeline.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized AuraFlowPipeline with BitsAndBytes in Python\nDESCRIPTION: This snippet demonstrates how to load a quantized AuraFlowPipeline for inference using the bitsandbytes library. It quantizes both the text encoder and transformer models to 8-bit precision to reduce memory requirements.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/aura_flow.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, AuraFlowTransformer2DModel, AuraFlowPipeline\nfrom transformers import BitsAndBytesConfig as BitsAndBytesConfig, T5EncoderModel\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\ntext_encoder_8bit = T5EncoderModel.from_pretrained(\n    \"fal/AuraFlow\",\n    subfolder=\"text_encoder\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\nquant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\ntransformer_8bit = AuraFlowTransformer2DModel.from_pretrained(\n    \"fal/AuraFlow\",\n    subfolder=\"transformer\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n\npipeline = AuraFlowPipeline.from_pretrained(\n    \"fal/AuraFlow\",\n    text_encoder=text_encoder_8bit,\n    transformer=transformer_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"balanced\",\n)\n\nprompt = \"a tiny astronaut hatching from an egg on the moon\"\nimage = pipeline(prompt).images[0]\nimage.save(\"auraflow.png\")\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest - Python Bash\nDESCRIPTION: Runs the test suite using `pytest` for the entire tests directory, leveraging parallel execution for efficiency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m pytest -n auto --dist=loadfile -s -v ./tests/\n```\n\n----------------------------------------\n\nTITLE: Full Model Inference Script\nDESCRIPTION: Python script for inference using fully fine-tuned model with pose control\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogview4-control/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers import CogView4ControlPipeline, CogView4Transformer2DModel\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport numpy as np\nimport torch \n\ntransformer = CogView4Transformer2DModel.from_pretrained(\"...\") # change this.\npipe = CogView4ControlPipeline.from_pretrained(\n  \"THUDM/CogView4-6B\",  transformer=transformer, torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\nopen_pose = OpenposeDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n# prepare pose condition.\nurl = \"https://huggingface.co/Adapter/t2iadapter/resolve/main/people.jpg\"\nimage = load_image(url)\nimage = open_pose(image, detect_resolution=512, image_resolution=1024)\nimage = np.array(image)[:, :, ::-1]           \nimage = Image.fromarray(np.uint8(image))\n\nprompt = \"A couple, 4k photo, highly detailed\"\n\ngen_images = pipe(\n  prompt=prompt,\n  control_image=image,\n  num_inference_steps=50,\n  guidance_scale=25., \n).images[0]\ngen_images.save(\"output.png\")\n```\n\n----------------------------------------\n\nTITLE: Training VQGAN on CIFAR10 Dataset\nDESCRIPTION: Command to launch VQGAN training on the CIFAR10 dataset using Accelerate, specifying dataset, image column, validation images, resolution, batch size, and logging options.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/vqgan/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_vqgan.py \\\n  --dataset_name=cifar10 \\\n  --image_column=img \\\n  --validation_images images/bird.jpg images/car.jpg images/dog.jpg images/frog.jpg \\\n  --resolution=128 \\\n  --train_batch_size=2 \\\n  --gradient_accumulation_steps=8 \\\n  --report_to=wandb\n```\n\n----------------------------------------\n\nTITLE: Importing FlowMatchHeunDiscreteScheduler in Markdown\nDESCRIPTION: This code snippet demonstrates how to include auto-generated documentation for the FlowMatchHeunDiscreteScheduler class using a special Markdown syntax. It's typically used in documentation build systems to automatically insert class documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/flow_match_heun_discrete.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[[autodoc]] FlowMatchHeunDiscreteScheduler\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub via CLI\nDESCRIPTION: Alternative method to authenticate with the Hugging Face Hub using the command-line interface. This approach is useful when working outside of a notebook environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements\nDESCRIPTION: Command to install additional dependencies from requirements file.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/autoencoderkl/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for DreamBooth Training in PyTorch\nDESCRIPTION: Commands to set environment variables for the DreamBooth training script, including model name, data directories, and output path.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"path_to_training_images\"\nexport OUTPUT_DIR=\"path_to_saved_model\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained Models\nDESCRIPTION: Hugging Face CLI commands to download pre-trained models required for GLIGEN training\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/gligen/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli download --resume-download xinyu1205/recognize_anything_model ram_swin_large_14m.pth\nhuggingface-cli download --resume-download IDEA-Research/grounding-dino-base\nhuggingface-cli download --resume-download Salesforce/blip2-flan-t5-xxl\n```\n\n----------------------------------------\n\nTITLE: Setting the Generation Prompt in Python\nDESCRIPTION: Defines the text prompt that will be used to generate an image. This example uses a prompt describing an old warrior chief portrait.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/stable_diffusion.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"portrait photo of a old warrior chief\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for Core ML Inference\nDESCRIPTION: Commands to install the necessary Python libraries for running Core ML inference, including huggingface_hub and Apple's ml-stable-diffusion repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/optimization/coreml.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface_hub\npip install git+https://github.com/apple/ml-stable-diffusion\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Clone and install the Hugging Face Diffusers library directly from the GitHub repository, setting up the development environment for Custom Diffusion training\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/custom_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Complete Denoising Loop for Image Generation\nDESCRIPTION: Implements a full denoising loop that iterates through all timesteps to progressively reduce noise in the sample. The loop predicts noise residuals, applies the scheduler step, and optionally displays intermediate results every 50 steps.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> import tqdm\n\n>>> sample = noisy_sample\n\n>>> for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n...     # 1. predict noise residual\n...     with torch.no_grad():\n...         residual = model(sample, t).sample\n\n...     # 2. compute less noisy image and set x_t -> x_t-1\n...     sample = scheduler.step(residual, t, sample).prev_sample\n\n...     # 3. optionally look at image\n...     if (i + 1) % 50 == 0:\n...         display_sample(sample, i + 1)\n```\n\n----------------------------------------\n\nTITLE: Loading Diffusers Pipeline from Multi-folder Layout\nDESCRIPTION: Demonstrates how to load a Diffusers pipeline from the default multi-folder layout using the from_pretrained method. This example loads the Stable Diffusion XL base model with specific configuration options.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Custom Sigmas Implementation in Diffusion Pipeline\nDESCRIPTION: Shows how to use custom sigma values in a diffusion pipeline, which controls noise levels at each timestep.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/scheduler_features.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipeline = DiffusionPipeline.from_pretrained(\n  \"stabilityai/stable-diffusion-xl-base-1.0\",\n  torch_dtype=torch.float16,\n  variant=\"fp16\",\n).to(\"cuda\")\npipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\nsigmas = [14.615, 6.315, 3.771, 2.181, 1.342, 0.862, 0.555, 0.380, 0.234, 0.113, 0.0]\nprompt = \"anthropomorphic capybara wearing a suit and working with a computer\"\ngenerator = torch.Generator(device='cuda').manual_seed(123)\nimage = pipeline(\n    prompt=prompt,\n    num_inference_steps=10,\n    sigmas=sigmas,\n    generator=generator\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Three different methods to configure an Accelerate environment for distributed training - interactive, default, or programmatic configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Importing KandinskyV22ControlnetPipeline in Python\nDESCRIPTION: This snippet defines the KandinskyV22ControlnetPipeline class, which incorporates ControlNet functionality into the Kandinsky 2.2 model for more controlled image generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] KandinskyV22ControlnetPipeline\n\t- all\n\t- __call__\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Diffusers\nDESCRIPTION: Lists required Python packages and their version constraints for the Diffusers project. Includes core ML libraries like Accelerate for optimization, TorchVision for computer vision tasks, Transformers for model support, and IP-Adapter for image processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/ip_adapter/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate\ntorchvision\ntransformers>=4.25.1\nip_adapter\n```\n\n----------------------------------------\n\nTITLE: Running DreamBooth with LoRA on FLUX.1\nDESCRIPTION: Bash command for DreamBooth training using LoRA (Low-Rank Adaptation) on the FLUX.1 model. This approach requires significantly less memory while maintaining quality, utilizing the PEFT library as a backend.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_flux.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"black-forest-labs/FLUX.1-dev\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"trained-flux-lora\"\n\naccelerate launch train_dreambooth_lora_flux.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"bf16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --guidance_scale=1 \\\n  --gradient_accumulation_steps=4 \\\n  --optimizer=\"prodigy\" \\\n  --learning_rate=1. \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Installing Accelerate Dependency\nDESCRIPTION: Installs the Accelerate library required for source installation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install accelerate\n```\n\n----------------------------------------\n\nTITLE: Setting Default Accelerate Configuration\nDESCRIPTION: Command to initialize Accelerate with default settings, which is useful when you don't need custom configuration for your training environment.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Saving Generated Image\nDESCRIPTION: Saves the generated image to a file using the PIL Image save method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> image.save(\"image_of_squirrel_painting.png\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries with Bash\nDESCRIPTION: This snippet demonstrates how to install the necessary Python libraries for fine-tuning the model. It ensures that the user has the required packages before proceeding with the training process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U transformers accelerate bitsandbytes peft datasets \npip install git+https://github.com/huggingface/diffusers -U\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Commands to clone the Diffusers repository and install it from source, ensuring the latest version is used for training examples.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/vqgan/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Achieving Reproducibility on GPU with PyTorch Generator\nDESCRIPTION: This snippet shows how to use a PyTorch Generator for GPU to attempt reproducibility, though results may vary across hardware.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reusing_seeds.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom diffusers import DDIMPipeline\n\nddim = DDIMPipeline.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)\nddim.to(\"cuda\")\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\nimage = ddim(num_inference_steps=2, output_type=\"np\", generator=generator).images\nprint(np.abs(image).sum())\n```\n\n----------------------------------------\n\nTITLE: Training CogVideoX LoRA with Bash Script\nDESCRIPTION: Bash script for launching a CogVideoX LoRA training job with accelerate. It configures training parameters including model, dataset, validation, and optimization settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\nGPU_IDS=\"0\"\n\naccelerate launch --gpu_ids $GPU_IDS examples/cogvideo/train_cogvideox_lora.py \\\n  --pretrained_model_name_or_path THUDM/CogVideoX-2b \\\n  --cache_dir <CACHE_DIR> \\\n  --instance_data_root <PATH_TO_WHERE_VIDEO_FILES_ARE_STORED> \\\n  --dataset_name my-awesome-name/my-awesome-dataset \\\n  --caption_column <CAPTION_COLUMN> \\\n  --video_column <PATH_TO_VIDEO_COLUMN> \\\n  --id_token <ID_TOKEN> \\\n  --validation_prompt \"<ID_TOKEN> Spiderman swinging over buildings:::A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance\" \\\n  --validation_prompt_separator ::: \\\n  --num_validation_videos 1 \\\n  --validation_epochs 10 \\\n  --seed 42 \\\n  --rank 64 \\\n  --lora_alpha 64 \\\n  --mixed_precision fp16 \\\n  --output_dir /raid/aryan/cogvideox-lora \\\n  --height 480 --width 720 --fps 8 --max_num_frames 49 --skip_frames_start 0 --skip_frames_end 0 \\\n  --train_batch_size 1 \\\n  --num_train_epochs 30 \\\n  --checkpointing_steps 1000 \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate 1e-3 \\\n  --lr_scheduler cosine_with_restarts \\\n  --lr_warmup_steps 200 \\\n  --lr_num_cycles 1 \\\n  --enable_slicing \\\n  --enable_tiling \\\n  --optimizer Adam \\\n  --adam_beta1 0.9 \\\n  --adam_beta2 0.95 \\\n  --max_grad_norm 1.0 \\\n  --report_to wandb\n```\n\n----------------------------------------\n\nTITLE: Loading a Mask Image for Inpainting in Python\nDESCRIPTION: This simple code snippet loads a mask image from a URL to be used with inpainting. The mask defines the area to be modified in the subsequent inpainting operation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md#2025-04-11_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_text-chain-mask.png\")\n```\n\n----------------------------------------\n\nTITLE: Training FLUX.1 with Pivotal Tuning and LoRA\nDESCRIPTION: A complete command for training FLUX.1 using the Dreambooth LoRA approach with pivotal tuning. Includes configurations for dataset, learning rates, optimizers, and tracking with Weights & Biases.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"black-forest-labs/FLUX.1-dev\"\nexport DATASET_NAME=\"./3d_icon\"\nexport OUTPUT_DIR=\"3d-icon-Flux-LoRA\"\n\naccelerate launch train_dreambooth_lora_flux_advanced.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --instance_prompt=\"3d icon in the style of TOK\" \\\n  --output_dir=$OUTPUT_DIR \\\n  --caption_column=\"prompt\" \\\n  --mixed_precision=\"bf16\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --repeats=1 \\\n  --report_to=\"wandb\"\\\n  --gradient_accumulation_steps=1 \\\n  --gradient_checkpointing \\\n  --learning_rate=1.0 \\\n  --text_encoder_lr=1.0 \\\n  --optimizer=\"prodigy\"\\\n  --train_text_encoder_ti\\\n  --train_text_encoder_ti_frac=0.5\\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --rank=8 \\\n  --max_train_steps=700 \\\n  --checkpointing_steps=2000 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: ZoeDepth Estimation Processing\nDESCRIPTION: Code to apply ZoeDepth estimation to the prepared image for better guidance during generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/advanced_inference/outpaint.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nzoe = ZoeDetector.from_pretrained(\"lllyasviel/Annotators\")\nimage_zoe = zoe(white_bg_image, detect_resolution=512, image_resolution=1024)\nimage_zoe\n```\n\n----------------------------------------\n\nTITLE: Applying Dropout for Conditioning in Training Loop\nDESCRIPTION: This Python snippet applies dropout to original image embeddings and edit instruction embeddings, allowing for dynamic modulation of edit prompts in the training process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nencoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\noriginal_image_embeds = vae.encode(batch[\"original_pixel_values\"].to(weight_dtype)).latent_dist.mode()\n\nif args.conditioning_dropout_prob is not None:\n    random_p = torch.rand(bsz, device=latents.device, generator=generator)\n    prompt_mask = random_p < 2 * args.conditioning_dropout_prob\n    prompt_mask = prompt_mask.reshape(bsz, 1, 1)\n    null_conditioning = text_encoder(tokenize_captions([\" \"]).to(accelerator.device))[0]\n    encoder_hidden_states = torch.where(prompt_mask, null_conditioning, encoder_hidden_states)\n\n    image_mask_dtype = original_image_embeds.dtype\n    image_mask = 1 - ((random_p >= args.conditioning_dropout_prob).to(image_mask_dtype) * (random_p < 3 * args.conditioning_dropout_prob).to(image_mask_dtype))\n    image_mask = image_mask.reshape(bsz, 1, 1, 1)\n    original_image_embeds = image_mask * original_image_embeds\n```\n\n----------------------------------------\n\nTITLE: Installing FLUX-specific Requirements\nDESCRIPTION: Command to install the FLUX-specific dependencies required for DreamBooth training with the FLUX.1 model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_flux.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements_flux.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Clone the Hugging Face Diffusers repository and install the library locally\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lora.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Cloning the Forked Diffusers Repository in Bash\nDESCRIPTION: Command to clone your forked version of the Diffusers repository to your local machine for editing. This is the first step after forking the repository.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/TRANSLATING.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<YOUR-USERNAME>/diffusers.git\n```\n\n----------------------------------------\n\nTITLE: HTML Image and Layout Implementation for Diffusers Library\nDESCRIPTION: HTML code that renders the Diffusers library logo centered at the top of the page, using embedded styling for alignment and sizing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/index.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<p align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg\" width=\"400\"/>\n    <br>\n</p>\n```\n\n----------------------------------------\n\nTITLE: Installing Intel PyTorch Dependencies\nDESCRIPTION: Command to install the required Intel Extension for PyTorch dependencies for distributed training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install oneccl_bind_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Training Script for IF Stage II\nDESCRIPTION: This script launches the DreamBooth training process for IF Stage II.  It sets various parameters, including the model name, instance data directory, output directory, instance prompt, resolution, batch size, learning rate, and maximum training steps. It uses accelerate to distribute the training across multiple devices.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\nexport MODEL_NAME=\"DeepFloyd/IF-II-L-v1.0\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"dreambooth_dog_upscale\"\nexport VALIDATION_IMAGES=\"dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png\"\n\naccelerate launch train_dreambooth.py \\\n  --report_to wandb \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a sks dog\" \\\n  --resolution=256 \\\n  --train_batch_size=2 \\\n  --gradient_accumulation_steps=6 \\\n  --learning_rate=5e-6 \\\n  --max_train_steps=2000 \\\n  --validation_prompt=\"a sks dog\" \\\n  --validation_steps=150 \\\n  --checkpointing_steps=500 \\\n  --pre_compute_text_embeddings \\\n  --tokenizer_max_length=77 \\\n  --text_encoder_use_attention_mask \\\n  --validation_images $VALIDATION_IMAGES \\\n  --class_labels_conditioning timesteps \\\n  --validation_scheduler DDPMScheduler\\\n  --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with DDIM Scheduler in Python\nDESCRIPTION: This snippet demonstrates text-to-image generation using the Stable Diffusion pipeline with the DDIM scheduler. It requires logging in to Hugging Face CLI and uses a custom scheduler.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# make sure you're logged in with `huggingface-cli login`\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\n\nscheduler =  DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    scheduler=scheduler,\n).to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"astronaut_rides_horse.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading ConsisID Model and Face Helper Models\nDESCRIPTION: Code for installing required packages, downloading model checkpoints, loading face helper models for preprocessing input face images, and initializing the ConsisID pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/consisid.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# !pip install consisid_eva_clip insightface facexlib\nimport torch\nfrom diffusers import ConsisIDPipeline\nfrom diffusers.pipelines.consisid.consisid_utils import prepare_face_models\nfrom huggingface_hub import snapshot_download\n\n# Download ckpts\nsnapshot_download(repo_id=\"BestWishYsh/ConsisID-preview\", local_dir=\"BestWishYsh/ConsisID-preview\")\n\n# Load face helper model to preprocess input face image\nface_helper_1, face_helper_2, face_clip_model, face_main_model, eva_transform_mean, eva_transform_std = prepare_face_models(\"BestWishYsh/ConsisID-preview\", device=\"cuda\", dtype=torch.bfloat16)\n\n# Load consisid base model\npipe = ConsisIDPipeline.from_pretrained(\"BestWishYsh/ConsisID-preview\", torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Generating an Image with the Pixel Art Adapter\nDESCRIPTION: Performs inference using the pixel-art LoRA adapter to generate a pixel art image of a hacker with a hoodie.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"a hacker with a hoodie, pixel art\"\nimage = pipe(\n    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=torch.manual_seed(0)\n).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Loading Textual Inversion Embeddings for Pure Textual Inversion Inference\nDESCRIPTION: Loads the textual inversion embeddings for inference with a pure textual inversion approach (without LoRA). This is the setup for using the model trained with train_transformer_frac=0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/README_flux.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom huggingface_hub import hf_hub_download, upload_file\nfrom diffusers import AutoPipelineForText2Image\nfrom safetensors.torch import load_file\n\nusername = \"linoyts\"\nrepo_id = f\"{username}/3d-icon-Flux-LoRA\"\n\npipe = AutoPipelineForText2Image.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to('cuda')\n\ntext_encoders = [pipe.text_encoder, pipe.text_encoder_2]\ntokenizers = [pipe.tokenizer, pipe.tokenizer_2]\n\nembedding_path = hf_hub_download(repo_id=repo_id, filename=\"3d-icon-Flux-LoRA_emb.safetensors\", repo_type=\"model\")\n\nstate_dict = load_file(embedding_path)\n# load embeddings of text_encoder 1 (CLIP ViT-L/14)\npipe.load_textual_inversion(state_dict[\"clip_l\"], token=[\"<s0>\", \"<s1>\"], text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)\n# load embeddings of text_encoder 2 (T5 XXL) - ignore this line if you didn't enable `--enable_t5_ti`\npipe.load_textual_inversion(state_dict[\"t5\"], token=[\"<s0>\", \"<s1>\"], text_encoder=pipe.text_encoder_2, tokenizer=pipe.tokenizer_2)\n```\n\n----------------------------------------\n\nTITLE: Importing StableDiffusionPipelineOutput in Python\nDESCRIPTION: This code snippet shows the import statement for the StableDiffusionPipelineOutput class, which is used to represent the output of Stable Diffusion pipelines, including Latent Consistency Models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latent_consistency_models.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput\n```\n\n----------------------------------------\n\nTITLE: Changing Diffusion Scheduler\nDESCRIPTION: Shows how to replace the default PNDMScheduler with an EulerDiscreteScheduler to adjust the denoising quality and speed trade-off.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import EulerDiscreteScheduler\n\n>>> pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed with Bash\nDESCRIPTION: This command installs DeepSpeed, which can be used as an alternative training optimizer to enhance performance during the fine-tuning of the model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -Uq deepspeed\n```\n\n----------------------------------------\n\nTITLE: Setting Default Accelerate Configuration\nDESCRIPTION: Command to set a default accelerate configuration without requiring interactive input.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/t2i_adapter/README_sdxl.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Importing PriorTransformerOutput in Python\nDESCRIPTION: Import statement for the PriorTransformerOutput class from the diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/prior_transformer.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.models.transformers.prior_transformer import PriorTransformerOutput\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face CLI\nDESCRIPTION: This snippet logs the user into the Hugging Face CLI after they have accepted the gate to use Stable Diffusion 3. Successful login is required before using diffusers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/sd3_dreambooth_lora_16gb.ipynb#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!huggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: HTML Image Display with Caption\nDESCRIPTION: HTML table structure displaying an example GIF output from I2VGen-XL with a library caption.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/i2vgenxl.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table>\n    <tr>\n        <td><center>\n        library.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/i2vgen-xl-example.gif\"\n            alt=\"library\"\n            style=\"width: 300px;\" />\n        </center></td>\n    </tr>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Distance Calculation for Molecular Graphs in Python\nDESCRIPTION: Computes the Euclidean distances between connected atoms in a molecular graph. These distances are essential for geometric features used in molecular modeling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef get_distance(pos, edge_index):\n    return (pos[edge_index[0]] - pos[edge_index[1]]).norm(dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Launch ControlNet Training (12GB GPU) - Bash\nDESCRIPTION: Launches the ControlNet training script with 8-bit Adam, gradient checkpointing, xFormers memory efficient attention, and sets gradients to None. This configuration is intended for GPUs with 12GB of memory to reduce memory usage.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n\"accelerate launch train_controlnet.py \\\n  --use_8bit_adam \\\n  --gradient_checkpointing \\\n  --enable_xformers_memory_efficient_attention \\\n  --set_grads_to_none \"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Diffusers with PEFT\nDESCRIPTION: Installs the necessary Python packages including transformers, accelerate, peft, and diffusers to enable working with adapters in Diffusers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/using_peft_for_inference.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -q transformers accelerate peft diffusers\n```\n\n----------------------------------------\n\nTITLE: InstantID Pipeline Implementation in Python\nDESCRIPTION: Implementation of InstantID for ID-preserving image generation using face analysis and controlnet models. Includes model setup, face embedding generation, and image processing.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#2025-04-11_snippet_101\n\nLANGUAGE: python\nCODE:\n```\nimport diffusers\nfrom diffusers.utils import load_image\nfrom diffusers import ControlNetModel\n\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\napp = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\nfrom huggingface_hub import hf_hub_download\n\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/config.json\", local_dir=\"./checkpoints\")\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/diffusion_pytorch_model.safetensors\", local_dir=\"./checkpoints\")\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ip-adapter.bin\", local_dir=\"./checkpoints\")\n\nface_adapter = './checkpoints/ip-adapter.bin'\ncontrolnet_path = './checkpoints/ControlNetModel'\n\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n\nbase_model = 'wangqixun/YamerMIX_v8'\npipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n    base_model,\n    controlnet=controlnet,\n    torch_dtype=torch.float16\n)\npipe.to(\"cuda\")\n\npipe.load_ip_adapter_instantid(face_adapter)\n\nface_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ai_face2.png\")\n\nface_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\nface_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*x['bbox'][3]-x['bbox'][1])[-1]\nface_emb = face_info['embedding']\nface_kps = draw_kps(face_image, face_info['kps'])\n\nprompt = \"film noir style, ink sketch|vector, male man, highly detailed, sharp focus, ultra sharpness, monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic\"\nnegative_prompt = \"ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, vibrant, colorful\"\n\npipe.set_ip_adapter_scale(0.8)\nimage = pipe(\n    prompt,\n    image_embeds=face_emb,\n    image=face_kps,\n    controlnet_conditioning_scale=0.8,\n).images[0]\n```\n\n----------------------------------------\n\nTITLE: Initializing Compel Processor\nDESCRIPTION: Creates a Compel processor object using the pipeline's tokenizer and text encoder.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/weighted_prompts.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom compel import Compel\n\ncompel_proc = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n```\n\n----------------------------------------\n\nTITLE: ControlNet-FLUX Inference Example\nDESCRIPTION: Python code demonstrating how to load a trained ControlNet-FLUX model and perform inference with a conditioning image and prompt.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_flux.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers.pipelines.flux.pipeline_flux_controlnet import FluxControlNetPipeline\nfrom diffusers.models.controlnet_flux import FluxControlNetModel\n\nbase_model = 'black-forest-labs/FLUX.1-dev'\ncontrolnet_model = 'promeai/FLUX.1-controlnet-lineart-promeai'\ncontrolnet = FluxControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\npipe = FluxControlNetPipeline.from_pretrained(\n    base_model, \n    controlnet=controlnet, \n    torch_dtype=torch.bfloat16\n)\n# enable memory optimizations   \npipe.enable_model_cpu_offload()\n\ncontrol_image = load_image(\"https://huggingface.co/promeai/FLUX.1-controlnet-lineart-promeai/resolve/main/images/example-control.jpg\")resize((1024, 1024))\nprompt = \"cute anime girl with massive fluffy fennec ears and a big fluffy tail blonde messy long hair blue eyes wearing a maid outfit with a long black gold leaf pattern dress and a white apron mouth open holding a fancy black forest cake with candles on top in the kitchen of an old dark Victorian mansion lit by candlelight with a bright window to the foggy forest and very expensive stuff everywhere\"\n\nimage = pipe(\n    prompt, \n    control_image=control_image,\n    controlnet_conditioning_scale=0.6,\n    num_inference_steps=28, \n    guidance_scale=3.5,\n).images[0]\nimage.save(\"./output.png\")\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: List of Python package dependencies required for the Hugging Face Diffusers project. Includes core machine learning libraries like accelerate, transformers, and peft with specific version constraints.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/requirements_flux.txt#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\naccelerate>=0.31.0\ntorchvision\ntransformers>=4.41.2\nftfy\ntensorboard\nJinja2\npeft>=0.11.1\nsentencepiece\n```\n\n----------------------------------------\n\nTITLE: Implementing AltDiffusionPipelineOutput Class in Python\nDESCRIPTION: A Python class for handling outputs from Alt Diffusion pipelines. The class inherits from BaseOutput and stores generated images and NSFW content detection flags. This is copied from StableDiffusionPipelineOutput with 'Stable' replaced by 'Alt'.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/conceptual/contribution.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Copied from diffusers.pipelines.stable_diffusion.pipeline_output.StableDiffusionPipelineOutput with Stable->Alt\nclass AltDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Alt Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or NumPy array of shape `(batch_size, height, width,\n            num_channels)`.\n        nsfw_content_detected (`List[bool]`)\n            List indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content or\n            `None` if safety checking could not be performed.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for SVD\nDESCRIPTION: Installation command for required dependencies including diffusers, transformers, and accelerate libraries.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/svd.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q -U diffusers transformers accelerate\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Dependencies in Colab\nDESCRIPTION: Code snippet for installing the necessary Python libraries in Google Colab to use Kandinsky models, including diffusers, transformers, and accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate\n```\n\n----------------------------------------\n\nTITLE: Importing CogVideoXDPMScheduler in Markdown\nDESCRIPTION: This markdown snippet uses an autodoc directive to automatically generate documentation for the CogVideoXDPMScheduler class.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/multistep_dpm_solver_cogvideox.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[[autodoc]] CogVideoXDPMScheduler\n```\n\n----------------------------------------\n\nTITLE: Selecting a Molecule for Visualization\nDESCRIPTION: Sets an index to select a specific molecule from the results and verifies that the selection is valid.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/geodiff/geodiff_molecule_conformation.ipynb#2025-04-11_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nidx = 0\nassert idx < len(results), \"selected molecule that was not generated\"\n```\n\n----------------------------------------\n\nTITLE: Loading Wuerstchen Prior Model\nDESCRIPTION: Code to load and configure the Wuerstchen Prior model and optimizer.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/wuerstchen.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprior = WuerstchenPrior.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder=\"prior\")\n\noptimizer = optimizer_cls(\n    prior.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Training Configuration Class with Dataclass\nDESCRIPTION: Defines a dataclass containing all the hyperparameters and configuration options for training the diffusion model. This includes image size, batch sizes, learning rate, and model output settings.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/basic_training.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from dataclasses import dataclass\n\n>>> @dataclass\n... class TrainingConfig:\n...     image_size = 128  # the generated image resolution\n...     train_batch_size = 16\n...     eval_batch_size = 16  # how many images to sample during evaluation\n...     num_epochs = 50\n...     gradient_accumulation_steps = 1\n...     learning_rate = 1e-4\n...     lr_warmup_steps = 500\n...     save_image_epochs = 10\n...     save_model_epochs = 30\n...     mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n...     output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub\n\n...     push_to_hub = True  # whether to upload the saved model to the HF Hub\n...     hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n...     hub_private_repo = None\n...     overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n...     seed = 0\n\n\n>>> config = TrainingConfig()\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet enumerates the required Python packages and their minimum versions (where specified) for the Diffusers project. It includes accelerate for optimization, torchvision for computer vision tasks, transformers for NLP models, ftfy for text processing, tensorboard for visualization, and datasets for data handling.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\ndatasets\n```\n\n----------------------------------------\n\nTITLE: Cloning Diffusers Repository\nDESCRIPTION: This command clones the diffusers repository from GitHub. It's the first step to access the training scripts and related code.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/t2i_adapters.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"git clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\"\n```\n\n----------------------------------------\n\nTITLE: Training DreamBooth with ColossalAI\nDESCRIPTION: Runs the DreamBooth training script using ColossalAI. It sets environment variables and uses torchrun for distributed training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/colossalai/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\ntorchrun --nproc_per_node 2 train_dreambooth_colossalai.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=400 \\\n  --placement=\"cuda\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Core ML Model Checkpoints\nDESCRIPTION: Python script to download Core ML model variants from Hugging Face Hub using snapshot_download function.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/coreml.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/packages\"\n\nmodel_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\nsnapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\nprint(f\"Model downloaded at {model_path}\")\n```\n\n----------------------------------------\n\nTITLE: Running Optimized HunyuanVideo Inference with Torchrun on Multiple GPUs\nDESCRIPTION: A Bash command illustrating the execution of the `run_hunyuan_video.py` script using `torchrun` to leverage multiple GPUs for improved inference speed. It specifies the number of GPUs to be used with the `--nproc_per_node` argument.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/para_attn.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n\"torchrun --nproc_per_node=8 run_hunyuan_video.py\"\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Diffusers Project\nDESCRIPTION: A list of Python package requirements with version constraints for the Hugging Face Diffusers project. It specifies core dependencies including Accelerate for optimization, torchvision for computer vision tasks, transformers for language models, and additional utilities like tensorboard for visualization and PEFT for parameter-efficient fine-tuning.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/requirements_sdxl.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.22.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\ndatasets\npeft==0.7.0\n```\n\n----------------------------------------\n\nTITLE: Installing JAX for TPU\nDESCRIPTION: Command to install a TPU-compatible version of JAX on a TPU VM (version 3 or higher).\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sdxl_flax/README.md#2025-04-11_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n```\n\n----------------------------------------\n\nTITLE: Installing Training Dependencies\nDESCRIPTION: Commands to install required dependencies for the Wuerstchen training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/wuerstchen.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/wuerstchen/text_to_image\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: HTML License Comment Block\nDESCRIPTION: Copyright and Apache 2.0 license declaration for the HuggingFace Diffusers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/paint_by_example.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Running Unittest Tests - Python Bash\nDESCRIPTION: Utilizes Python's built-in unittest framework to discover and run tests located in the tests and examples directories.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md#2025-04-11_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m unittest discover -s tests -t . -v\n$ python -m unittest discover -s examples -t examples -v\n```\n\n----------------------------------------\n\nTITLE: Encoding Prompt with PixArt-α\nDESCRIPTION: Generates text embeddings from a prompt using the initialized pipeline\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    prompt = \"cute cat\"\n    prompt_embeds, prompt_attention_mask, negative_embeds, negative_prompt_attention_mask = pipe.encode_prompt(prompt)\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Commands to clone the diffusers repository and install it from source, which is required before running the unconditional image generation training script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/unconditional_training.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Training CogVideoX LoRA with Bash Script\nDESCRIPTION: Bash script for launching text-to-video LoRA training using accelerate. Includes comprehensive training parameters like batch size, learning rate, validation settings, and model architecture configurations.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\nGPU_IDS=\"0\"\n\naccelerate launch --gpu_ids $GPU_IDS examples/cogvideo/train_cogvideox_lora.py \\\n  --pretrained_model_name_or_path THUDM/CogVideoX-2b \\\n  --cache_dir <CACHE_DIR> \\\n  --instance_data_root <PATH_TO_WHERE_VIDEO_FILES_ARE_STORED> \\\n  --dataset_name my-awesome-name/my-awesome-dataset \\\n  --caption_column <CAPTION_COLUMN> \\\n  --video_column <PATH_TO_VIDEO_COLUMN> \\\n  --id_token <ID_TOKEN> \\\n  --validation_prompt \"<ID_TOKEN> Spiderman swinging over buildings:::A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance\" \\\n  --validation_prompt_separator ::: \\\n  --num_validation_videos 1 \\\n  --validation_epochs 10 \\\n  --seed 42 \\\n  --rank 64 \\\n  --lora_alpha 64 \\\n  --mixed_precision fp16 \\\n  --output_dir /raid/aryan/cogvideox-lora \\\n  --height 480 --width 720 --fps 8 --max_num_frames 49 --skip_frames_start 0 --skip_frames_end 0 \\\n  --train_batch_size 1 \\\n  --num_train_epochs 30 \\\n  --checkpointing_steps 1000 \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate 1e-3 \\\n  --lr_scheduler cosine_with_restarts \\\n  --lr_warmup_steps 200 \\\n  --lr_num_cycles 1 \\\n  --enable_slicing \\\n  --enable_tiling \\\n  --optimizer Adam \\\n  --adam_beta1 0.9 \\\n  --adam_beta2 0.95 \\\n  --max_grad_norm 1.0 \\\n  --report_to wandb\n```\n\n----------------------------------------\n\nTITLE: Importing SD3Transformer2DModel in Python\nDESCRIPTION: This code snippet demonstrates how to import the SD3Transformer2DModel class. The model is part of the Stable Diffusion 3 project and features the novel MMDiT transformer block.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/sd3_transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[[autodoc]] SD3Transformer2DModel\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate Environment\nDESCRIPTION: Different methods to initialize and configure the Accelerate environment for training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/wuerstchen.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers with conda\nDESCRIPTION: Installs Diffusers using conda package manager from conda-forge channel\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge diffusers\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for Hugging Face Diffusers Project\nDESCRIPTION: A requirements specification listing all necessary Python packages for the Diffusers project. It includes core dependencies like accelerate, torchvision, transformers, and PEFT with specific version constraints to ensure compatibility.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\ndatasets>=2.19.1\nftfy\ntensorboard\nJinja2\npeft==0.7.0\n```\n\n----------------------------------------\n\nTITLE: Safe Stable Diffusion Reference Link\nDESCRIPTION: Markdown link to the Safe Stable Diffusion documentation that describes mitigation strategies for inappropriate content generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe)\n```\n\n----------------------------------------\n\nTITLE: Importing VideoProcessor Class in Markdown\nDESCRIPTION: This snippet shows how to import and document the VideoProcessor class and its methods using Markdown and autodoc directives.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/video_processor.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Video Processor\n\nThe [`VideoProcessor`] provides a unified API for video pipelines to prepare inputs for VAE encoding and post-processing outputs once they're decoded. The class inherits [`VaeImageProcessor`] so it includes transformations such as resizing, normalization, and conversion between PIL Image, PyTorch, and NumPy arrays.\n\n## VideoProcessor\n\n[[autodoc]] video_processor.VideoProcessor.preprocess_video\n\n[[autodoc]] video_processor.VideoProcessor.postprocess_video\n```\n\n----------------------------------------\n\nTITLE: Training Dreambooth on 16GB GPU with 8-bit Adam Optimizer\nDESCRIPTION: Command for training Dreambooth on a 16GB GPU using gradient checkpointing and the 8-bit optimizer from bitsandbytes. This setup enables running on smaller memory GPUs with optimization techniques.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_subject_dreambooth/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=2 --gradient_checkpointing \\\n  --use_8bit_adam \\\n  --learning_rate=5e-6 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --max_train_steps=800\n```\n\n----------------------------------------\n\nTITLE: Viewing Pipeline Components Structure\nDESCRIPTION: This code shows the structure of a loaded Stable Diffusion pipeline, displaying its components including the UNet2DConditionModel, PNDMScheduler, and other essential parts that make up the complete pipeline.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/pt/quicktour.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> pipeline\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.13.1\",\n  ...,\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  ...,\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing HuggingFace Diffusers from Source - Bash\nDESCRIPTION: Set up the HuggingFace Diffusers library by cloning the repository and installing it from the local source, ensuring the environment is ready for training scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Setting Up Default Accelerate Configuration\nDESCRIPTION: Command to set up a default Accelerate configuration without interactive prompts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Diffusers\nDESCRIPTION: This code snippet shows how to install the necessary libraries to work with Diffusers, including diffusers itself, accelerate for faster model loading, and transformers which is required for models like Stable Diffusion.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install --upgrade diffusers accelerate transformers\n```\n\n----------------------------------------\n\nTITLE: Importing UNetMotionModel in Python\nDESCRIPTION: This snippet shows how to import the UNetMotionModel class from the diffusers library. The UNetMotionModel is a 2D UNet variant used in diffusion systems for image processing tasks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/unet-motion.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import UNetMotionModel\n```\n\n----------------------------------------\n\nTITLE: Installing Safetensors Library\nDESCRIPTION: Installs the Safetensors library, which is required for working with safetensors files in Diffusers.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install safetensors\n```\n\n----------------------------------------\n\nTITLE: Initializing Local Repository Setup for Diffusers\nDESCRIPTION: Commands to clone the forked repository and set up the upstream remote for contribution\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone git@github.com:<your GitHub handle>/diffusers.git\n$ cd diffusers\n$ git remote add upstream https://github.com/huggingface/diffusers.git\n```\n\n----------------------------------------\n\nTITLE: Listing Dependencies for Hugging Face Diffusers Project\nDESCRIPTION: This snippet enumerates the required Python packages for the Diffusers project. It includes machine learning frameworks like transformers, flax, and torch, as well as utility libraries for data processing and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/requirements_flax.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers>=4.25.1\nflax\noptax\ntorch\ntorchvision\nftfy\ntensorboard\nJinja2\n```\n\n----------------------------------------\n\nTITLE: Safety Checker Component Link\nDESCRIPTION: Markdown link to the Safety Checker implementation in the Diffusers repository that handles content filtering.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py)\n```\n\n----------------------------------------\n\nTITLE: Importing CogVideoXDDIMScheduler in Markdown\nDESCRIPTION: This code snippet demonstrates how to include an autodoc reference for the CogVideoXDDIMScheduler class in a Markdown document. It uses the [[autodoc]] syntax specific to the documentation system being used.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/ddim_cogvideox.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[[autodoc]] CogVideoXDDIMScheduler\n```\n\n----------------------------------------\n\nTITLE: Adding Hub Upload Parameter to Training Script\nDESCRIPTION: Command line argument to enable pushing the trained model to the Hugging Face Hub during or after training.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/unconditional_training.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n--push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Initializing 🤗 Accelerate Environment for CogVideoX Training\nDESCRIPTION: Provides commands to configure the Hugging Face Accelerate environment needed for progressive and optimized model training using CogVideoX. Offers options for default and interactive configuration setups compatible with notebooks.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/cogvideox.md#2025-04-11_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n# Initialize an Accelerate environment\naccelerate config\n\n# For a default accelerate configuration without answering questions\naccelerate config default\n\n# If your environment doesn't support an interactive shell\n\"\"\"python\nfrom accelerate.utils import write_basic_config\nwrite_basic_config()\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Flax Training Configuration\nDESCRIPTION: Demonstrates how to set up and run DreamBooth training using Flax/JAX without prior preservation loss.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\npython train_dreambooth_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-6 \\\n  --max_train_steps=400\n```\n\n----------------------------------------\n\nTITLE: Switching Schedulers in a Diffusion Pipeline\nDESCRIPTION: This snippet shows how to replace the default scheduler in a pipeline with a different one. In this case, it replaces the PNDMScheduler with EulerDiscreteScheduler while preserving the configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ja/quicktour.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from diffusers import EulerDiscreteScheduler\n\n>>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\n>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n```\n\n----------------------------------------\n\nTITLE: License Header Comment in Markdown\nDESCRIPTION: Apache 2.0 license header comment defining usage rights and conditions for the AuraFlowTransformer2DModel documentation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/models/aura_flow_transformer2d.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Diffusers\nDESCRIPTION: List of required Python packages and their minimum version requirements needed to run the Diffusers project. Includes core ML libraries like accelerate, transformers, and torchvision along with utility packages.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\naccelerate>=0.31.0\ntorchvision\ntransformers>=4.41.2\nftfy\ntensorboard\nJinja2\npeft>=0.11.1\nsentencepiece\n```\n\n----------------------------------------\n\nTITLE: Listing Diffusers Dependencies\nDESCRIPTION: This snippet lists the dependencies required for the Diffusers project. These packages must be installed to use the library. Dependencies include transformers, flax, optax, torch, torchvision, ftfy, tensorboard, and Jinja2.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/multi_token_textual_inversion/requirements_flax.txt#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"transformers>=4.25.1\nflax\noptax\ntorch\ntorchvision\nftfy\ntensorboard\nJinja2\"\n```\n\n----------------------------------------\n\nTITLE: Package Requirements for Diffusers\nDESCRIPTION: Specifies the necessary Python package dependencies for the Hugging Face Diffusers project, including version constraints for machine learning and deep learning libraries\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/training/text_to_image/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: requirements.txt\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\ndatasets>=2.19.1\nftfy\ntensorboard\nJinja2\npeft==0.7.0\n```\n\n----------------------------------------\n\nTITLE: HTML License Header\nDESCRIPTION: Copyright and Apache 2.0 license notice for the HuggingFace Diffusers project\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md#2025-04-11_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Library from Source\nDESCRIPTION: Commands to clone and install the Diffusers library from source code.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/wuerstchen.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Diffusers Library\nDESCRIPTION: This requirements file lists the necessary Python packages for running the Hugging Face Diffusers library. It specifies minimum versions for accelerate (>=0.16.0), transformers (>=4.25.1), and peft (>=0.6.0), while listing other dependencies such as torchvision, wandb, bitsandbytes, and deepspeed without version constraints.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/wuerstchen/text_to_image/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nwandb\nbitsandbytes\ndeepspeed\npeft>=0.6.0\n```\n\n----------------------------------------\n\nTITLE: Validation Parameters for Textual Inversion Training\nDESCRIPTION: Command line parameters to enable validation during Textual Inversion training. These parameters periodically save generated images to track training progress, specifying the validation prompt, number of validation images, and validation frequency.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text_inversion.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n--validation_prompt=\"A <cat-toy> train\"\n--num_validation_images=4\n--validation_steps=100\n```\n\n----------------------------------------\n\nTITLE: Inference Using the Fine-Tuned Model\nDESCRIPTION: This Python snippet demonstrates how to load the fine-tuned model and generate images based on input prompts. It initializes the model, compiles the graph, and measures the time for generation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/pytorch_xla/training/text_to_image/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport os\nimport sys\nimport  numpy as np\n\nimport torch_xla.core.xla_model as xm\nfrom time import time\nfrom diffusers import StableDiffusionPipeline\nimport torch_xla.runtime as xr\n\nCACHE_DIR = os.environ.get(\"CACHE_DIR\", None)\nif CACHE_DIR:\n    xr.initialize_cache(CACHE_DIR, readonly=False)\n\ndef main():\n    device = xm.xla_device()\n    model_path = \"jffacevedo/pxla_trained_model\"\n    pipe = StableDiffusionPipeline.from_pretrained(\n        model_path, \n        torch_dtype=torch.bfloat16\n    )\n    pipe.to(device)\n    prompt = [\"A naruto with green eyes and red legs.\"]\n    start = time()\n    print(\"compiling...\")\n    image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n    print(f\"compile time: {time() - start}\")\n    print(\"generate...\")\n    start = time()\n    image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n    print(f\"generation time (after compile) : {time() - start}\")\n    image.save(\"naruto.png\")\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Kandinsky 3\nDESCRIPTION: Loads the Kandinsky 3 pipeline and generates an image directly from a text prompt without needing a prior model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import Kandinsky3Pipeline\nimport torch\n\npipeline = Kandinsky3Pipeline.from_pretrained(\"kandinsky-community/kandinsky-3\", variant=\"fp16\", torch_dtype=torch.float16)\npipeline.enable_model_cpu_offload()\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nimage = pipeline(prompt).images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing Function\nDESCRIPTION: Function to preprocess training data including image transforms and caption tokenization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/wuerstchen.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_train(examples):\n    images = [image.convert(\"RGB\") for image in examples[image_column]]\n    examples[\"effnet_pixel_values\"] = [effnet_transforms(image) for image in images]\n    examples[\"text_input_ids\"], examples[\"text_mask\"] = tokenize_captions(examples)\n    return examples\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install development dependencies in a virtual environment\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: List of required Python packages for the Hugging Face Diffusers project, including dependencies for deep learning, data processing, and experiment tracking.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/diffusion_orpo/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndatasets\naccelerate\ntransformers\ntorchvision\nwandb\npeft\nwebdataset\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for InstructPix2Pix\nDESCRIPTION: This snippet outlines the commands needed to install the necessary dependencies for running the InstructPix2Pix training script. Ensure the library is cloned from the HuggingFace repository and required packages are installed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/instructpix2pix.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/instruct_pix2pix\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet enumerates the Python packages required for the Hugging Face Diffusers project. It includes specific version requirements for some packages and general dependencies for others.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\ndatasets\nftfy\ntensorboard\nJinja2\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Diffusers Server\nDESCRIPTION: Commands to install the server dependencies for running a Diffusers pipeline server.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/create_a_server.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install .\npip install -f requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Training AutoencoderKL on ImageNet\nDESCRIPTION: Launch command for training AutoencoderKL on ImageNet dataset with mixed precision and decoder-only configuration.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/autoencoderkl/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train_autoencoderkl.py \\\n    --pretrained_model_name_or_path stabilityai/sd-vae-ft-mse \\\n    --num_train_epochs 100 \\\n    --gradient_accumulation_steps 2 \\\n    --learning_rate 4.5e-6 \\\n    --lr_scheduler cosine \\\n    --report_to wandb \\\n    --mixed_precision bf16 \\\n    --train_data_dir /path/to/ImageNet/train \\\n    --validation_image ./image.png \\\n    --decoder_only\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face CLI for Model Access\nDESCRIPTION: This Bash command logs you into the Hugging Face CLI to gain access to gated content such as the Stable Diffusion 3 model. Required prior to pushing trained models to the Hugging Face Hub. Ensure you have an account and have shared your contact information with Hugging Face.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/sd3_lora_colab/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Balanced Device Mapping for Multi-GPU Inference\nDESCRIPTION: Example of using balanced device mapping strategy to distribute model components across multiple GPUs.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/tutorials/inference_with_big_models.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True, device_map=\"balanced\"\n)\nimage = pipeline(\"a dog\").images[0]\nimage\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Textual Inversion\nDESCRIPTION: Install the required libraries for training a textual inversion model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text_inversion.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install diffusers accelerate transformers\n```\n\n----------------------------------------\n\nTITLE: Launching LCM Training Script with Accelerate\nDESCRIPTION: Bash command to launch the LCM distillation training script using Accelerate. It configures model directories, training parameters, dataset paths, optimization settings, and various training options like checkpointing and logging.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/lcm_distill.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path/to/saved/model\"\n\naccelerate launch train_lcm_distill_sd_wds.py \\\n    --pretrained_teacher_model=$MODEL_DIR \\\n    --output_dir=$OUTPUT_DIR \\\n    --mixed_precision=fp16 \\\n    --resolution=512 \\\n    --learning_rate=1e-6 --loss_type=\"huber\" --ema_decay=0.95 --adam_weight_decay=0.0 \\\n    --max_train_steps=1000 \\\n    --max_train_samples=4000000 \\\n    --dataloader_num_workers=8 \\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\n    --validation_steps=200 \\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\n    --train_batch_size=12 \\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n    --gradient_accumulation_steps=1 \\\n    --use_8bit_adam \\\n    --resume_from_checkpoint=latest \\\n    --report_to=wandb \\\n    --seed=453645634 \\\n    --push_to_hub\n```\n\n----------------------------------------\n\nTITLE: Formatting Video Paths File for CogVideoX Training\nDESCRIPTION: Example of the videos.txt file format containing line-separated paths to video files relative to the instance_data_root directory.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/README.md#2025-04-11_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nvideos/00000.mp4\nvideos/00001.mp4\n...\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for Hugging Face Diffusers\nDESCRIPTION: This snippet lists the required Python packages for the Hugging Face Diffusers library. It includes core dependencies like transformers, PyTorch, and Flax, along with utilities for data processing and visualization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/requirements_flax.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers>=4.25.1\ndatasets\nflax\noptax\ntorch\ntorchvision\nftfy\ntensorboard\nJinja2\n```\n\n----------------------------------------\n\nTITLE: Setting AttnAddedKVProcessor for Kandinsky Optimization\nDESCRIPTION: Demonstrates an alternative approach to explicitly set the attention processor to use AttnAddedKVProcessor2_0 for improved performance with Kandinsky models.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md#2025-04-11_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers.models.attention_processor import AttnAddedKVProcessor2_0\n\npipe.unet.set_attn_processor(AttnAddedKVProcessor2_0())\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Diffusers\nDESCRIPTION: A list of Python package dependencies required for the Diffusers library. It specifies minimum version requirements for some packages like accelerate (>=0.31.0), transformers (>=4.41.2), and peft (>=0.11.1).\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/requirements_flux.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.31.0\ntorchvision\ntransformers>=4.41.2\nftfy\ntensorboard\nJinja2\npeft>=0.11.1\nsentencepiece\n```\n\n----------------------------------------\n\nTITLE: Introducing Experimental Features for Diffusers Library in Markdown\nDESCRIPTION: This markdown snippet outlines the purpose of the experimental code additions to the Diffusers library. It specifically mentions support for reinforcement learning through the implementation of the Diffuser model.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/experimental/README.md#2025-04-11_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# 🧨 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in Markdown\nDESCRIPTION: Copyright and Apache License 2.0 notice for the HuggingFace Team documentation\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/dance_diffusion.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment with LoRA Dependencies in Bash\nDESCRIPTION: This Bash script clones the Hugging Face diffusers repository and installs the required dependencies for running InstructPix2Pix with LoRA extensions. It configures an Accelerate environment and ensures that the PEFT library is correctly installed.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/instructpix2pix_lora/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\ncd examples\npip install -r requirements.txt\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate Environment for DreamBooth Training\nDESCRIPTION: Commands to configure HuggingFace Accelerate for distributed training with DreamBooth. Provides options for interactive and non-interactive environments.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```\n\n----------------------------------------\n\nTITLE: Initialize 🤗 Accelerate Environment - Bash\nDESCRIPTION: Configure the Accelerate library settings to optimize the training process for your hardware. This step is crucial for setting up multi-GPU or TPU trainings with mixed precision.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/text2image.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for SDXL Turbo\nDESCRIPTION: Installs the necessary libraries (diffusers, transformers, accelerate) for using SDXL Turbo. This step is typically needed in environments like Google Colab.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/sdxl_turbo.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers with Flax Support\nDESCRIPTION: Command for installing the Diffusers library with Flax support using pip.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade diffusers[flax]\n```\n\n----------------------------------------\n\nTITLE: Displaying Diffusers CLI Version and Dependency Information\nDESCRIPTION: This bash output shows the version information for Diffusers and its dependencies, including the Python version, PyTorch version, and various related libraries. It provides a snapshot of the development environment used for testing and benchmarking.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/tests/quantization/torchao/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n- 🤗 Diffusers version: 0.32.0.dev0\n- Platform: Linux-5.15.0-1049-aws-x86_64-with-glibc2.31\n- Running on Google Colab?: No\n- Python version: 3.10.14\n- PyTorch version (GPU?): 2.6.0.dev20241112+cu121 (False)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Huggingface_hub version: 0.26.2\n- Transformers version: 4.46.3\n- Accelerate version: 1.1.1\n- PEFT version: not installed\n- Bitsandbytes version: not installed\n- Safetensors version: 0.4.5\n- xFormers version: not installed\n```\n\n----------------------------------------\n\nTITLE: Loading StableCascade UNet Model\nDESCRIPTION: Shows how to load a StableCascade UNet model from a checkpoint file using the from_single_file method.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/using-diffusers/other-formats.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import StableCascadeUNet\n\nckpt_path = \"https://huggingface.co/stabilityai/stable-cascade/blob/main/stage_b_lite.safetensors\"\nmodel = StableCascadeUNet.from_single_file(ckpt_path)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Kandinsky\nDESCRIPTION: Pip install command to install the necessary libraries (diffusers, transformers, accelerate) for using Kandinsky models. This step is typically needed when running in Colab.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/using-diffusers/kandinsky.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!pip install -q diffusers transformers accelerate\n```\n\n----------------------------------------\n\nTITLE: Basic DreamBooth Fine-tuning with Flax\nDESCRIPTION: Command to run DreamBooth training using Flax for TPUs or faster training on high-memory GPUs. Requires at least 30GB VRAM as it doesn't support gradient checkpointing or accumulation.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\npython train_dreambooth_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --learning_rate=5e-6 \\\n  --max_train_steps=400\n```\n\n----------------------------------------\n\nTITLE: Launch ControlNet Training (Flax) - Bash\nDESCRIPTION: Launches the ControlNet training script using Flax with specified parameters, including model path, output directory, dataset, learning rate, validation details, and push to hub configurations.  This command runs the `train_controlnet_flax.py` script.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/training/controlnet.md#2025-04-11_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n\"python3 train_controlnet_flax.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --validation_image \\\".conditioning_image_1.png\\\" \\\".conditioning_image_2.png\\\" \\\n --validation_prompt \\\"red circle with blue background\\\" \\\"cyan circle with brown floral background\\\" \\\n --validation_steps=1000 \\\n --train_batch_size=2 \\\n --revision=\\\"non-ema\\\" \\\n --from_pt \\\n --report_to=\\\"wandb\\\" \\\n --tracker_project_name=$HUB_MODEL_ID \\\n --num_train_epochs=11 \\\n --push_to_hub \\\n --hub_model_id=$HUB_MODEL_ID\"\n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers from Source\nDESCRIPTION: Terminal commands to install the Diffusers library from source to run the latest versions of the example scripts.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\n----------------------------------------\n\nTITLE: Converting Image-Text Pairs to ImageFolder Format\nDESCRIPTION: This script converts a directory of image-text pairs into the ImageFolder format by creating a `metadata.jsonl` file.  It uses `convert_to_imagefolder.py` to automate this process given a directory containing images and corresponding text files.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README.md#2025-04-11_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\npython convert_to_imagefolder.py --path my_dataset/\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements\nDESCRIPTION: List of required Python packages and their versions for the Diffusers project. Includes ML frameworks like accelerate and transformers, along with utility packages for image processing and model optimization.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate\ntorchvision\ntransformers>=4.25.0\nftfy\ntensorboard\nmodelcards\nneural-compressor\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference from a DreamBooth Checkpoint with Accelerate >=0.16.0\nDESCRIPTION: Python code to load a saved DreamBooth checkpoint and create an inference pipeline with it. This works with newer versions of Accelerate (>=0.16.0) and can handle models with fine-tuned text encoders.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/dreambooth.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel\nfrom transformers import CLIPTextModel\nimport torch\n\n# 학습에 사용된 것과 동일한 인수(model, revision)로 파이프라인을 불러옵니다.\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\n\nunet = UNet2DConditionModel.from_pretrained(\"/sddata/dreambooth/daruma-v2-1/checkpoint-100/unet\")\n\n# `args.train_text_encoder`로 학습한 경우면 텍스트 인코더를 꼭 불러오세요\ntext_encoder = CLIPTextModel.from_pretrained(\"/sddata/dreambooth/daruma-v2-1/checkpoint-100/text_encoder\")\n\npipeline = DiffusionPipeline.from_pretrained(model_id, unet=unet, text_encoder=text_encoder, dtype=torch.float16)\npipeline.to(\"cuda\")\n\n# 추론을 수행하거나 저장하거나, 허브에 푸시합니다.\npipeline.save_pretrained(\"dreambooth-pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Diffusers Project\nDESCRIPTION: A requirements.txt style file that specifies the necessary Python packages for the Hugging Face Diffusers project. It includes version constraints for certain packages, such as Accelerate version 0.16.0 or higher, Transformers version 4.25.1 or higher, and PEFT locked to exactly version 0.7.0.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/requirements_sdxl.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.16.0\ntorchvision\ntransformers>=4.25.1\nftfy\ntensorboard\nJinja2\npeft==0.7.0\n```\n\n----------------------------------------\n\nTITLE: Installing Test Dependencies\nDESCRIPTION: Command to install testing dependencies\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -e \".[test]\"\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Stable Diffusion with Custom Dataset using PyTorch\nDESCRIPTION: Example command to fine-tune the Stable Diffusion model on a custom local dataset using PyTorch and Accelerate.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text2image.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport TRAIN_DIR=\"path_to_your_dataset\"\nexport OUTPUT_DIR=\"path_to_save_model\"\n\naccelerate launch train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$TRAIN_DIR \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --mixed_precision=\"fp16\" \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: Installing JAX TPU Dependencies\nDESCRIPTION: Command to install JAX version 0.4.5 with TPU support.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README.md#2025-04-11_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\npip install \"jax[tpu]==0.4.5\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n```\n\n----------------------------------------\n\nTITLE: Checking Diffusers Version - Python Command\nDESCRIPTION: A Python command to verify the installed version of the Diffusers library, used to ensure compatibility before reporting issues.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -c \"import diffusers; print(diffusers.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Stable Diffusion with Flax\nDESCRIPTION: Example command to fine-tune the Stable Diffusion model on the Naruto BLIP captions dataset using Flax.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/ko/training/text2image.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\nexport dataset_name=\"lambdalabs/naruto-blip-captions\"\n\npython train_text_to_image_flax.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$dataset_name \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n----------------------------------------\n\nTITLE: Moving Model and Data to GPU\nDESCRIPTION: Transfers the UNet model and input noise to GPU to accelerate the denoising process.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/zh/quicktour.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> model.to(\"cuda\")\n>>> noisy_sample = noisy_sample.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel with OpenVINO\nDESCRIPTION: Command to install the Optimum Intel package with OpenVINO support using pip, ensuring the latest version through the eager upgrade strategy.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\n----------------------------------------\n\nTITLE: Referencing Scheduler Documentation in Markdown\nDESCRIPTION: This markdown snippet directs users to the official Diffusers documentation for detailed information about schedulers. It uses a hyperlink to guide readers to the specific API documentation page.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/README.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Schedulers\n\nFor more information on the schedulers, please refer to the [docs](https://huggingface.co/docs/diffusers/api/schedulers/overview).\n```\n\n----------------------------------------\n\nTITLE: Editable Install with PyTorch\nDESCRIPTION: Performs an editable installation of Diffusers with PyTorch support\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/source/en/installation.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[torch]\"\n```\n\n----------------------------------------\n\nTITLE: Running Automatic Styling Script in Markdown\nDESCRIPTION: This snippet shows the command to run an automatic styling script for docstrings. The script ensures that docstrings use the full line width and that code examples are formatted using black, consistent with the Transformers library.\nSOURCE: https://github.com/huggingface/diffusers/blob/main/docs/README.md#2025-04-11_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n```\nmake style\n```\n```"
  }
]
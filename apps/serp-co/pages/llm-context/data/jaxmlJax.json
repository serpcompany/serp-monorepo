[
  {
    "owner": "jax-ml",
    "repo": "jax",
    "content": "TITLE: Applying JIT Compilation to a JAX Function\nDESCRIPTION: Demonstrates how to use jax.jit to compile a simple SELU function, both as a standalone function and using decorator syntax.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/key-concepts.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nselu_jit = jax.jit(selu)\nprint(selu_jit(1.0))\n```\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n```\n\n----------------------------------------\n\nTITLE: Installing JAX\nDESCRIPTION: Commands for installing JAX via pip for CPU or NVIDIA GPU usage.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/quickstart.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install jax\npip install -U \"jax[cuda12]\"\n```\n\n----------------------------------------\n\nTITLE: Automatic Differentiation Example\nDESCRIPTION: Implementation of gradient computation using jax.grad with a logistic function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/quickstart.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad\n\ndef sum_logistic(x):\n  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\nx_small = jnp.arange(3.)\nderivative_fn = grad(sum_logistic)\nprint(derivative_fn(x_small))\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop\nDESCRIPTION: Executes the main training loop, iterating over epochs and batches. Updates model parameters, and computes and prints training and test accuracies after each epoch.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfor epoch in range(num_epochs):\n  start_time = time.time()\n  for x, y in training_generator:\n    y = one_hot(y, n_targets)\n    params = update(params, x, y)\n  epoch_time = time.time() - start_time\n\n  train_acc = accuracy(params, train_images, train_labels)\n  test_acc = accuracy(params, test_images, test_labels)\n  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n  print(\"Training set accuracy {}\".format(train_acc))\n  print(\"Test set accuracy {}\".format(test_acc))\n```\n\n----------------------------------------\n\nTITLE: Automatic Differentiation with grad in JAX\nDESCRIPTION: Shows how to use JAX's grad function to compute derivatives of arbitrary order on a tanh function, including handling of control flow constructs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad\nimport jax.numpy as jnp\n\ndef tanh(x):  # Define a function\n  y = jnp.exp(-2.0 * x)\n  return (1.0 - y) / (1.0 + y)\n\ngrad_tanh = grad(tanh)  # Obtain its gradient function\nprint(grad_tanh(1.0))   # Evaluate it at x = 1.0\n# prints 0.4199743\n```\n\n----------------------------------------\n\nTITLE: Basic JAX Neural Network Implementation with Transformations\nDESCRIPTION: Demonstrates a simple neural network implementation using JAX, showcasing the use of jit, grad, and vmap transformations for efficient computation and differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\n\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.tanh(outputs)  # inputs to the next layer\n  return outputs                # no activation on last layer\n\ndef loss(params, inputs, targets):\n  preds = predict(params, inputs)\n  return jnp.sum((preds - targets)**2)\n\ngrad_loss = jit(grad(loss))  # compiled gradient evaluation function\nperex_grads = jit(vmap(grad_loss, in_axes=(None, 0, 0)))  # fast per-example grads\n```\n\n----------------------------------------\n\nTITLE: Hessian-Vector Products using Forward-over-Reverse Mode\nDESCRIPTION: An improved implementation of Hessian-vector products using a combination of forward and reverse mode differentiation. This approach is more memory-efficient and works with arrays of any shape and arbitrary container types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jvp, grad\n\n# forward-over-reverse\ndef hvp(f, primals, tangents):\n  return jvp(grad(f), primals, tangents)[1]\n```\n\n----------------------------------------\n\nTITLE: Implementing Gradient Descent with JAX\nDESCRIPTION: Runs 20 iterations of gradient descent optimization using JAX's automatic differentiation to compute gradients and update parameters.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nstep_size = 1e-2\n\nfor _ in range(20):\n  grads = grad(loss)(params, batch)\n  params = [(W - step_size * dW, b - step_size * db)\n            for (W, b), (dW, db) in zip(params, grads)]\n```\n\n----------------------------------------\n\nTITLE: Using with_sharding_constraint to Replicate Array in JAX\nDESCRIPTION: Defines a JIT-compiled function that adds 1 to an input array and then applies a sharding constraint to fully replicate the array across all devices using an empty partition spec P().\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  x = x + 1\n  y = jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P()))\n  return y\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Prediction Function in JAX\nDESCRIPTION: Implements a forward pass through a neural network for a single image example, using JAX's numpy operations and special functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.scipy.special import logsumexp\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  # per-example predictions\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n\n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits - logsumexp(logits)\n```\n\n----------------------------------------\n\nTITLE: Defining a Function and its First Derivative using `jax.grad` in Python\nDESCRIPTION: This snippet defines a polynomial function `f` using a lambda expression. It then uses `jax.grad` to create a new function `dfdx` which computes the first derivative of `f` with respect to its input `x`. It requires JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nf = lambda x: x**3 + 2*x**2 - 3*x + 1\n\ndfdx = jax.grad(f)\n```\n\n----------------------------------------\n\nTITLE: Using jax.debug.print with jax.jit\nDESCRIPTION: Demonstrates how to use jax.debug.print to inspect runtime values within a jitted function, contrasting it with Python's print which only shows tracers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  jax.debug.print(\"jax.debug.print(x) -> {x}\", x=x)\n  y = jnp.sin(x)\n  jax.debug.print(\"jax.debug.print(y) -> {y}\", y=y)\n  return y\n\nresult = f(2.)\n```\n\n----------------------------------------\n\nTITLE: Using `static_argnums` for JIT Compilation with Conditionals in JAX Python\nDESCRIPTION: Demonstrates how to use the `static_argnums` argument in `jax.jit` to successfully JIT-compile the function `f` (which failed previously). By marking the first argument (index 0) as static, JAX treats its value as constant during tracing, allowing the conditional logic. A new compilation occurs for each distinct static value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nf_jit_correct = jax.jit(f, static_argnums=0)\nprint(f_jit_correct(10))\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop for Neural Network in JAX\nDESCRIPTION: Sets up a training loop that iterates over batches of MNIST data, updates model parameters, and evaluates accuracy on train and test sets using JAX operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\ndef get_train_batches():\n  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n  # You can build up an arbitrary tf.data input pipeline\n  ds = ds.batch(batch_size).prefetch(1)\n  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n  return tfds.as_numpy(ds)\n\nfor epoch in range(num_epochs):\n  start_time = time.time()\n  for x, y in get_train_batches():\n    x = jnp.reshape(x, (len(x), num_pixels))\n    y = one_hot(y, num_labels)\n    params = update(params, x, y)\n  epoch_time = time.time() - start_time\n\n  train_acc = accuracy(params, train_images, train_labels)\n  test_acc = accuracy(params, test_images, test_labels)\n  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n  print(\"Training set accuracy {}\".format(train_acc))\n  print(\"Test set accuracy {}\".format(test_acc))\n```\n\n----------------------------------------\n\nTITLE: Creating Higher-Order Derivative Functions using `jax.grad` in Python\nDESCRIPTION: This snippet demonstrates chaining `jax.grad` to obtain functions for higher-order derivatives. Starting from the first derivative function `dfdx` (defined previously), it creates functions `d2fdx`, `d3fdx`, and `d4fdx` representing the second, third, and fourth derivatives of the original function `f`, respectively.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nd2fdx = jax.grad(dfdx)\nd3fdx = jax.grad(d2fdx)\nd4fdx = jax.grad(d3fdx)\n```\n\n----------------------------------------\n\nTITLE: Applying and Benchmarking JAX JIT Compilation in Python\nDESCRIPTION: Applies the `jax.jit` transformation to the `selu` function to enable Just-In-Time compilation by XLA. It includes a warm-up call to trigger compilation before benchmarking the execution time of the compiled function using `%timeit`, demonstrating significant performance improvement.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nselu_jit = jax.jit(selu)\n\n# Pre-compile the function before timing...\nselu_jit(x).block_until_ready()\n\n%timeit selu_jit(x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Working with Pytrees in JAX\nDESCRIPTION: Demonstrates how JAX handles various pytree structures, including nested lists, dictionaries, and named tuples, using tree utilities.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/key-concepts.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# (nested) list of parameters\nparams = [1, 2, (jnp.arange(3), jnp.ones(2))]\n\nprint(jax.tree.structure(params))\nprint(jax.tree.leaves(params))\n```\n\nLANGUAGE: python\nCODE:\n```\n# Dictionary of parameters\nparams = {'n': 5, 'W': jnp.ones((2, 2)), 'b': jnp.zeros(2)}\n\nprint(jax.tree.structure(params))\nprint(jax.tree.leaves(params))\n```\n\nLANGUAGE: python\nCODE:\n```\n# Named tuple of parameters\nfrom typing import NamedTuple\n\nclass Params(NamedTuple):\n  a: int\n  b: float\n\nparams = Params(1, 5.0)\nprint(jax.tree.structure(params))\nprint(jax.tree.leaves(params))\n```\n\n----------------------------------------\n\nTITLE: Using linearize with jit for function differentiation\nDESCRIPTION: Example showing how to compose the linearize transformation with JIT compilation to compute function values and their derivatives efficiently.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_68\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\ny, f_lin = linearize(f, 3.)\ny_dot = f_lin(1.)\nprint(y, y_dot)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Function Value and Gradient using `jax.value_and_grad` (Python)\nDESCRIPTION: This snippet demonstrates the use of `jax.value_and_grad` to efficiently compute both the value of the `loss` function and its gradients with respect to specified arguments (`W` and `b` via `(0, 1)`) in a single pass. It prints the computed loss value and compares it to a direct evaluation of `loss(W, b)`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nloss_value, Wb_grad = jax.value_and_grad(loss, (0, 1))(W, b)\nprint('loss value', loss_value)\nprint('loss value', loss(W, b))\n```\n\n----------------------------------------\n\nTITLE: Testing Custom JVP with jvp and grad\nDESCRIPTION: Using JAX's jvp and grad functions to test a function with custom JVP rules, showing how the differentiation behaves with the custom rules.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jvp, grad\n\nprint(f(2., 3.))\ny, y_dot = jvp(f, (2., 3.), (1., 0.))\nprint(y)\nprint(y_dot)\nprint(grad(f)(2., 3.))\n```\n\n----------------------------------------\n\nTITLE: Defining JAX Primitives and Bindings\nDESCRIPTION: Implements primitive operations and their binding functions. Defines core mathematical operations like add, multiply, sin, cos etc. as primitive operations that can be intercepted and transformed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import NamedTuple\n\nclass Primitive(NamedTuple):\n  name: str\n\nadd_p = Primitive('add')\nmul_p = Primitive('mul')\nneg_p = Primitive(\"neg\")\nsin_p = Primitive(\"sin\")\ncos_p = Primitive(\"cos\")\nreduce_sum_p = Primitive(\"reduce_sum\")\ngreater_p = Primitive(\"greater\")\nless_p = Primitive(\"less\")\ntranspose_p = Primitive(\"transpose\")\nbroadcast_p = Primitive(\"broadcast\")\n\ndef add(x, y): return bind1(add_p, x, y)\ndef mul(x, y): return bind1(mul_p, x, y)\ndef neg(x): return bind1(neg_p, x)\ndef sin(x): return bind1(sin_p, x)\ndef cos(x): return bind1(cos_p, x)\ndef greater(x, y): return bind1(greater_p, x, y)\ndef less(x, y): return bind1(less_p, x, y)\ndef transpose(x, perm): return bind1(transpose_p, x, perm=perm)\ndef broadcast(x, shape, axes): return bind1(broadcast_p, x, shape=shape, axes=axes)\ndef reduce_sum(x, axis=None):\n  if axis is None:\n    axis = tuple(range(np.ndim(x)))\n  if type(axis) is int:\n    axis = (axis,)\n  return bind1(reduce_sum_p, x, axis=axis)\n\ndef bind1(prim, *args, **params):\n  out, = bind(prim, *args, **params)\n  return out\n```\n\n----------------------------------------\n\nTITLE: Vectorization with vmap\nDESCRIPTION: Demonstration of automatic vectorization using jax.vmap for efficient batch operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/quickstart.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import vmap\n\n@jit\ndef vmap_batched_apply_matrix(batched_x):\n  return vmap(apply_matrix)(batched_x)\n\nnp.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n                           vmap_batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\nprint('Auto-vectorized with vmap')\n%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Basic Parallel Mapping with pmap\nDESCRIPTION: Demonstrates using pmap to parallelize a simple squaring operation across multiple devices, distributing the array elements.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ny = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(y)\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Vectorization Module\nDESCRIPTION: Imports the vmap function from JAX, which is used for vectorizing operations across array dimensions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import vmap\n```\n\n----------------------------------------\n\nTITLE: Using jax.debug for Interactive Debugging with Print and Breakpoint\nDESCRIPTION: Demonstrates how to use jax.debug.print to print values and jax.debug.breakpoint to pause execution within jit-compiled JAX functions. This allows inspection of runtime values in compiled code that would otherwise be opaque.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef f(x):\n  jax.debug.print(\"🤯 {x} 🤯\", x=x)\n  y = jnp.sin(x)\n  jax.debug.breakpoint()\n  jax.debug.print(\"🤯 {y} 🤯\", y=y)\n  return y\n\nf(2.)\n# Prints:\n# 🤯 2.0 🤯\n# Enters breakpoint to inspect values!\n# 🤯 0.9092974662780762 🤯\n```\n\n----------------------------------------\n\nTITLE: Testing Linear Scaling of Residuals in Function Chains\nDESCRIPTION: Demonstrates how the number of stored residuals scales linearly with the length of a function chain when using standard differentiation. Shows the memory inefficiency of naive approach.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nf = chain_compose([jnp.sin] * 16)\nprint_saved_residuals(f, 3.)\n```\n\n----------------------------------------\n\nTITLE: Extracting and Printing jaxpr via jax.make_jaxpr in Python\nDESCRIPTION: Demonstrates how to use jax.make_jaxpr to trace a Python function, extracting its internal jaxpr intermediate representation. Requires JAX and jax.numpy; dependencies include 'jax' and 'jax.numpy'. The target function adds the result of element-wise sine computation multiplied by a constant to an input array, and sums the result. Inputs are two arrays; output is the printed jaxpr for inspection. Limitations: Only works for functions convertible by JAX tracing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import make_jaxpr\nimport jax.numpy as jnp\n\ndef func1(first, second):\n   temp = first + jnp.sin(second) * 3.\n   return jnp.sum(temp)\n\nprint(make_jaxpr(func1)(jnp.zeros(8), jnp.ones(8)))\n```\n\n----------------------------------------\n\nTITLE: Testing Numerically Stable log1pexp Gradient\nDESCRIPTION: Verifying that the custom JVP rule produces numerically stable gradient values for large inputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(log1pexp)(100.))\n```\n\n----------------------------------------\n\nTITLE: Training an MLP with JAX, Gradient Descent, and Pytrees - JAX - Python\nDESCRIPTION: Defines the full MLP forward pass, loss function, and parameter update routine using JAX. The update function is JIT-compiled and applies stochastic gradient descent to parameter pytrees using jax.tree.map, with gradients computed via jax.grad. Inputs include model parameters, data (x, y), and the learning rate. This code depends on JAX, works for arbitrary pytree-structured parameters, and demonstrates end-to-end integration of pytrees with ML training workflows.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define the forward pass.\\ndef forward(params, x):\\n  *hidden, last = params\\n  for layer in hidden:\\n    x = jax.nn.relu(x @ layer['weights'] + layer['biases'])\\n  return x @ last['weights'] + last['biases']\\n\\n# Define the loss function.\\ndef loss_fn(params, x, y):\\n  return jnp.mean((forward(params, x) - y) ** 2)\\n\\n# Set the learning rate.\\nLEARNING_RATE = 0.0001\\n\\n# Using the stochastic gradient descent, define the parameter update function.\\n# Apply `@jax.jit` for JIT compilation (speed).\\n@jax.jit\\ndef update(params, x, y):\\n  # Calculate the gradients with `jax.grad`.\\n  grads = jax.grad(loss_fn)(params, x, y)\\n  # Note that `grads` is a pytree with the same structure as `params`.\\n  # `jax.grad` is one of many JAX functions that has\\n  # built-in support for pytrees.\\n  # This is useful - you can apply the SGD update using JAX pytree utilities.\\n  return jax.tree.map(\\n      lambda p, g: p - LEARNING_RATE * g, params, grads\\n  )\n```\n\n----------------------------------------\n\nTITLE: Reusing Compiled JIT Function in Python with JAX\nDESCRIPTION: This snippet demonstrates how a JIT-compiled function can be reused efficiently without recompilation when called with inputs of the same shape and dtype.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nx2 = np.random.randn(3, 4)\ny2 = np.random.randn(4)\nf(x2, y2)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Array Mutability in NumPy vs Immutability in JAX\nDESCRIPTION: Illustrates a key difference between NumPy and JAX arrays: NumPy arrays are mutable (can be changed after creation), while JAX arrays are immutable (cannot be modified after creation).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# NumPy: mutable arrays\nx = np.arange(10)\nx[0] = 10\nprint(x)\n```\n\nLANGUAGE: python\nCODE:\n```\n%xmode minimal\n```\n\nLANGUAGE: python\nCODE:\n```\n# JAX: immutable arrays\nx = jnp.arange(10)\nx[0] = 10\n```\n\n----------------------------------------\n\nTITLE: Custom JVP with Python Control Flow\nDESCRIPTION: Shows how to use Python control flow within custom JVP rules. The example demonstrates defining different JVP behavior based on input conditions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x):\n  if x > 0:\n    return jnp.sin(x)\n  else:\n    return jnp.cos(x)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  ans = f(x)\n  if x > 0:\n    return ans, 2 * x_dot\n  else:\n    return ans, 3 * x_dot\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with JAX\nDESCRIPTION: Demonstrates computing gradients of the defined function for both positive and negative inputs, showing JAX's automatic differentiation capability.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nkey = random.key(0)\nx = random.normal(key, ())\n\nprint(grad(f)(x))\nprint(grad(f)(-x))\n```\n\n----------------------------------------\n\nTITLE: Neural Network Prediction Function in JAX\nDESCRIPTION: Defines an unbatched neural network prediction function that processes a single input vector through multiple layers with matrix multiplications and tanh activations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef predict(params, input_vec):\n  assert input_vec.ndim == 1\n  activations = input_vec\n  for W, b in params:\n    outputs = jnp.dot(W, activations) + b  # `activations` on the right-hand side!\n    activations = jnp.tanh(outputs)        # inputs to the next layer\n  return outputs                           # no activation on last layer\n```\n\n----------------------------------------\n\nTITLE: Implementing and JIT-Compiling a Row Normalization Function\nDESCRIPTION: Shows how to define a function for normalizing the rows of a matrix, then creates a JIT-compiled version using jax.jit. Demonstrates that the compiled version produces the same results with potentially much faster execution time.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n\ndef norm(X):\n  X = X - X.mean(0)\n  return X / X.std(0)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\nnorm_compiled = jit(norm)\n```\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1701)\nX = jnp.array(np.random.rand(10000, 10))\nnp.allclose(norm(X), norm_compiled(X), atol=1E-6)\n```\n\nLANGUAGE: python\nCODE:\n```\n%timeit norm(X).block_until_ready()\n%timeit norm_compiled(X).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Defining Utility and Loss Functions for Neural Network Training\nDESCRIPTION: Implements one-hot encoding, accuracy calculation, loss function, and parameter update function using JAX operations and transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef one_hot(x, k, dtype=jnp.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n\ndef accuracy(params, images, targets):\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\n\n@jit\ndef update(params, x, y):\n  grads = grad(loss)(params, x, y)\n  return [(w - step_size * dw, b - step_size * db)\n          for (w, b), (dw, db) in zip(params, grads)]\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobians with jacfwd and jacrev in JAX\nDESCRIPTION: This code demonstrates computing Jacobians using both forward-mode (jacfwd) and reverse-mode (jacrev) automatic differentiation on a prediction function, showing they compute the same values but with different implementation efficiencies.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nJ = jacfwd(f)(W)\nprint(\"jacfwd result, with shape\", J.shape)\nprint(J)\n\nJ = jacrev(f)(W)\nprint(\"jacrev result, with shape\", J.shape)\nprint(J)\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation Example\nDESCRIPTION: Demonstration of using jax.jit to compile and optimize function execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/quickstart.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\n\nselu_jit = jit(selu)\n_ = selu_jit(x)  # compiles on first call\n%timeit selu_jit(x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Composing JAX Transformations for Hessian Computation\nDESCRIPTION: Shows how to compose JAX transformations to create efficient higher-order operations like computing a Hessian matrix using forward and reverse-mode automatic differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit, jacfwd, jacrev\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\n```\n\n----------------------------------------\n\nTITLE: Automatic Differentiation with JAX\nDESCRIPTION: This snippet demonstrates automatic differentiation in JAX. It defines a piecewise function and computes its first, second, and third derivatives using the grad function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad\n\ndef f(x):\n  if x > 0:\n    return 2 * x ** 3\n  else:\n    return 3 * x\n\nkey = random.key(0)\nx = random.normal(key, ())\n\nprint(grad(f)(x))\nprint(grad(f)(-x))\n\nprint(grad(grad(f))(-x))\nprint(grad(grad(grad(f)))(-x))\n```\n\n----------------------------------------\n\nTITLE: Setting IPython Exception Mode in Python\nDESCRIPTION: Configures the IPython environment to display concise tracebacks for code cell exceptions. This is often used in notebooks for cleaner error output.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%xmode minimal\n```\n\n----------------------------------------\n\nTITLE: Computing Hessian Matrices in JAX\nDESCRIPTION: Shows how to compute a dense Hessian matrix by composing forward and reverse-mode automatic differentiation (jacfwd and jacrev) in JAX, explaining why forward-over-reverse is typically the most efficient approach.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef hessian(f):\n    return jacfwd(jacrev(f))\n\nH = hessian(f)(W)\nprint(\"hessian, with shape\", H.shape)\nprint(H)\n```\n\n----------------------------------------\n\nTITLE: Parallelization with pmap in JAX\nDESCRIPTION: This snippet demonstrates parallelization across multiple devices using pmap in JAX. It shows how to apply a function to multiple array elements in parallel and visualize the results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\njax.device_count()\n\nfrom jax import pmap\n\ny = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(y)\n\ny\n\nimport matplotlib.pyplot as plt\nplt.plot(y)\n```\n\n----------------------------------------\n\nTITLE: Using lax.fori_loop for indexed iterations\nDESCRIPTION: Shows how to use lax.fori_loop for creating JIT-compilable and potentially fully differentiable indexed loops.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninit_val = 0\nstart = 0\nstop = 10\nbody_fun = lambda i,x: x+i\nlax.fori_loop(start, stop, body_fun, init_val)\n# --> array(45, dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Automatic Differentiation with Control Flow in JAX\nDESCRIPTION: Demonstrates how JAX's automatic differentiation works with Python control flow structures like conditional statements.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef abs_val(x):\n  if x > 0:\n    return x\n  else:\n    return -x\n\nabs_val_grad = grad(abs_val)\nprint(abs_val_grad(1.0))   # prints 1.0\nprint(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n```\n\n----------------------------------------\n\nTITLE: Implementing Gradient Clipping with custom_vjp\nDESCRIPTION: Using custom_vjp to implement gradient clipping, a technique to limit gradient magnitudes during backpropagation without affecting the forward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom jax import custom_vjp\n\n@custom_vjp\ndef clip_gradient(lo, hi, x):\n  return x  # identity function\n\ndef clip_gradient_fwd(lo, hi, x):\n  return x, (lo, hi)  # save bounds as residuals\n\ndef clip_gradient_bwd(res, g):\n  lo, hi = res\n  return (None, None, jnp.clip(g, lo, hi))  # use None to indicate zero cotangents for lo and hi\n\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)\n```\n\n----------------------------------------\n\nTITLE: Differentiating w.r.t. Dictionary Parameters using `jax.grad` (Python)\nDESCRIPTION: This snippet shows how `jax.grad` seamlessly handles differentiation with respect to parameters stored in standard Python containers like dictionaries (PyTrees). It defines a new loss function `loss2` that accepts parameters `W` and `b` within a dictionary. It then computes and prints the gradient of `loss2` with respect to this dictionary.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef loss2(params_dict):\n    preds = predict(params_dict['W'], params_dict['b'], inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nprint(grad(loss2)({'W': W, 'b': b}))\n```\n\n----------------------------------------\n\nTITLE: JAX Random Number Generation\nDESCRIPTION: Example of generating random numbers and benchmarking SELU function execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/quickstart.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import random\n\nkey = random.key(1701)\nx = random.normal(key, (1_000_000,))\n%timeit selu(x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom VJP Function in JAX\nDESCRIPTION: Demonstrates defining a custom vector-Jacobian product (VJP) rule for a function. The example shows how to define the primal function and its corresponding forward and backward passes for reverse-mode differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import custom_vjp\nimport jax.numpy as jnp\n\n# f :: a -> b\n@custom_vjp\ndef f(x):\n  return jnp.sin(x)\n\n# f_fwd :: a -> (b, c)\ndef f_fwd(x):\n  return f(x), jnp.cos(x)\n\n# f_bwd :: (c, CT b) -> CT a\ndef f_bwd(cos_x, y_bar):\n  return (cos_x * y_bar,)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n----------------------------------------\n\nTITLE: Implementing log1pexp with defjvps Convenience Wrapper\nDESCRIPTION: Alternative implementation of numerically stable log1pexp using the defjvps convenience wrapper for simpler rule definition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef log1pexp(x):\n  return jnp.log(1. + jnp.exp(x))\n\nlog1pexp.defjvps(lambda t, ans, x: (1 - 1/(1 + jnp.exp(x))) * t)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiply-Add Using JAX LAX Primitives\nDESCRIPTION: A simple implementation of a multiply-add function (x*y+z) using existing JAX primitives from the lax module. This demonstrates how to create JAX-traceable functions that can be transformed with operations like grad.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\nfrom jax._src import api\n\ndef multiply_add_lax(x, y, z):\n  \"\"\"Implementation of multiply-add using the `jax.lax` primitives.\"\"\"\n  return lax.add(lax.mul(x, y), z)\n\n\ndef square_add_lax(a, b):\n  \"\"\"A square-add function using the newly defined multiply-add.\"\"\"\n  return multiply_add_lax(a, a, b)\n\nprint(\"square_add_lax = \", square_add_lax(2., 10.))\n# Differentiate w.r.t. the first argument\nprint(\"grad(square_add_lax) = \", api.grad(square_add_lax, argnums=0)(2.0, 10.))\n```\n\n----------------------------------------\n\nTITLE: Defining Logistic Regression Model Functions\nDESCRIPTION: Implements sigmoid activation, prediction and loss functions for logistic regression along with dataset initialization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12,  0.77],\n                   [0.88, -1.08, 0.15],\n                   [0.52, 0.06, -1.30],\n                   [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\n# Initialize random model coefficients\nkey, W_key, b_key = random.split(key, 3)\nW = random.normal(W_key, (3,))\nb = random.normal(b_key, ())\n```\n\n----------------------------------------\n\nTITLE: Performing Functional Array Updates in JAX using `.at[]`\nDESCRIPTION: This snippet demonstrates the JAX mechanism for performing functional array updates, which returns a *new* array with the specified element updated, leaving the original array unchanged. It uses the `.at` property followed by indexing and a method like `.set()`. This is the JAX alternative to NumPy's in-place update (`x[i] = y`).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.numpy.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nx.at[i].set(y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Efficient Neural Network Layer Iteration with JAX Checkpointing\nDESCRIPTION: This code snippet demonstrates how to efficiently implement a neural network with multiple layers using JAX's lax.scan and checkpoint functions. It shows both a basic implementation and an optimized version using checkpointing to control gradient computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nLayerParam = tuple[jnp.ndarray, jnp.ndarray]  # Weights-bias pair for a layer.\nParamsList = list[LayerParam]\n\ndef net(params: ParamsList, x: jnp.ndarray):\n  for W, b in params:\n    x = jnp.maximum(jnp.dot(x, W) + b, 0.)\n  return x\n\nparams = [(jnp.array([[0.5, 0.5], [1., 1.]]), jnp.array([0.5, 0.5])), \n          (jnp.array([[0.5, 0.5], [1., 1.]]), jnp.array([0.5, 0.5]))]\n\nall_weights = jnp.stack([W for W, _ in params])\nall_biases = jnp.stack([b for _, b in params])\n\ndef layer(x, W_b_pair):\n  W, b = W_b_pair\n  out = jnp.maximum(jnp.dot(x, W) + b, 0.)\n  return out, None\n\ndef net(all_weights, all_biases, x):\n  x, _ = jax.lax.scan(layer, x, (all_weights, all_biases))\n  return x\n\nfrom functools import partial\n\n@partial(jax.checkpoint,\n         policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\ndef layer(x, W_b_pair):\n  W, b = W_b_pair\n  out = jnp.maximum(jnp.dot(x, W) + b, 0.)\n  return out, None\n```\n\n----------------------------------------\n\nTITLE: Implementing Batching for Custom Primitive in JAX\nDESCRIPTION: Implementation of the batching logic for a custom multiply_add primitive, enabling vectorized execution across batched dimensions and registering with JAX's batching system.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.interpreters import batching\n\n@trace(\"multiply_add_batch\")\ndef multiply_add_batch(vector_arg_values, batch_axes):\n  \"\"\"Computes the batched version of the primitive.\n  \n  This must be a JAX-traceable function.\n  \n  Since the `multiply_add primitive` already operates point-wise on arbitrary\n  dimension tensors, to batch it you can use the primitive itself. This works as\n  long as both the inputs have the same dimensions and are batched along the\n  same axes. The result is batched along the axis that the inputs are batched.\n\n  Args:\n    vector_arg_values: A tuple of two arguments, each being a tensor of matching\n      shape.\n    batch_axes: The axes that are being batched. See vmap documentation.\n\n  Returns:\n    A tuple of the result, and the result axis that was batched. \n  \"\"\"\n  assert batch_axes[0] == batch_axes[1]\n  assert batch_axes[0] == batch_axes[2]\n  _trace(\"Using multiply_add to compute the batch:\")\n  res = multiply_add_prim(*vector_arg_values)\n  return res, batch_axes[0]\n\n\nbatching.primitive_batchers[multiply_add_p] = multiply_add_batch\n```\n\n----------------------------------------\n\nTITLE: Using jacrev with Container Types in JAX\nDESCRIPTION: Demonstrates how to use JAX's automatic differentiation with container types (dictionaries) by computing the Jacobian of a prediction function that accepts parameters as a dictionary with 'W' and 'b' keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef predict_dict(params, inputs):\n    return predict(params['W'], params['b'], inputs)\n\nJ_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\nfor k, v in J_dict.items():\n    print(\"Jacobian from {} to logits is\".format(k))\n    print(v)\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of checkify for Runtime Error Checking in JAX\nDESCRIPTION: Demonstrates how to use checkify.checkify transformation with the checkify.check function to add runtime checks to JAX code that can be jitted. The example shows checking for a negative index and handling the returned error object.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n  checkify.check(i >= 0, \"index needs to be non-negative, got {i}\", i=i)\n  y = x[i]\n  z = jnp.sin(y)\n  return z\n\njittable_f = checkify.checkify(f)\n\nerr, z = jax.jit(jittable_f)(jnp.ones((5,)), -2)\nprint(err.get())\n# >> index needs to be non-negative, got -2! (check failed at <...>:6 (f))\n```\n\n----------------------------------------\n\nTITLE: Comparing NumPy and JAX for Plotting Trigonometric Functions\nDESCRIPTION: Demonstrates how JAX provides NumPy-like functionality with its jax.numpy interface by calculating and plotting a trigonometric function. Shows how JAX arrays can often be used as drop-in replacements for NumPy arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_np = np.linspace(0, 10, 1000)\ny_np = 2 * np.sin(x_np) * np.cos(x_np)\nplt.plot(x_np, y_np);\n```\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n\nx_jnp = jnp.linspace(0, 10, 1000)\ny_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\nplt.plot(x_jnp, y_jnp);\n```\n\n----------------------------------------\n\nTITLE: Advanced Differentiation Operations\nDESCRIPTION: Examples of computing Jacobian and Hessian matrices using JAX's differentiation tools.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/quickstart.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jacfwd, jacrev\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\nprint(hessian(sum_logistic)(x_small))\n```\n\n----------------------------------------\n\nTITLE: Mapping N-ary Functions Over Pytrees - JAX - Python\nDESCRIPTION: This code demonstrates the use of jax.tree.map with a binary function (lambda x, y: x+y) across two identically structured pytrees. It adds corresponding leaves in two lists of lists, relying on JAX for deep traversal and element-wise computation. Both input pytrees must have the exact same structure; otherwise, a runtime error will occur.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nanother_list_of_lists = list_of_lists\\njax.tree.map(lambda x, y: x+y, list_of_lists, another_list_of_lists)\n```\n\n----------------------------------------\n\nTITLE: Defining and Invoking a Blocked Matrix Multiplication Kernel with Pallas in Python\nDESCRIPTION: This snippet illustrates how to define a simple block-wise matrix multiplication kernel, partition input matrices using BlockSpec, and launch the computation with pallas_call in JAX. It requires the 'jax', 'pl' (Pallas), 'numpy', and standard dependencies for running on compatible hardware such as GPUs/TPUs. The kernel matmul_kernel performs the matrix multiplication for each block, and matmul sets up the grid and block mappings; the output is validated for correctness. The inputs are two (1024, 1024) arrays, and the output is compared with standard matmul for verification.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_kernel(x_ref, y_ref, z_ref):\n  z_ref[...] = x_ref[...] @ y_ref[...]\n\ndef matmul(x: jax.Array, y: jax.Array):\n  return pl.pallas_call(\n    matmul_kernel,\n    out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),\n    grid=(2, 2),\n    in_specs=[\n        pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)),\n        pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))\n    ],\n    out_specs=pl.BlockSpec(\n        (x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j),\n    )\n  )(x, y)\nk1, k2 = jax.random.split(jax.random.key(0))\nx = jax.random.normal(k1, (1024, 1024))\ny = jax.random.normal(k2, (1024, 1024))\nz = matmul(x, y)\nnp.testing.assert_allclose(z, x @ y)\n\n```\n\n----------------------------------------\n\nTITLE: defjvps with Multiple Arguments\nDESCRIPTION: Demonstrates using defjvps with a function that has multiple arguments. The example shows how to define separate JVP rules for each argument.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x, y):\n  return x ** 2 * y\n\nf.defjvps(lambda x_dot, primal_out, x, y: 2 * x * y * x_dot,\n          lambda y_dot, primal_out, x, y: x ** 2 * y_dot)\n```\n\n----------------------------------------\n\nTITLE: Initializing Neural Network Parameters\nDESCRIPTION: Defines helper functions to randomly initialize weights and biases for a dense neural network layer and the full network. Sets hyperparameters for the model architecture and training process.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef random_layer_params(m, n, key, scale=1e-2):\n  w_key, b_key = random.split(key)\n  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n\ndef init_network_params(sizes, key):\n  keys = random.split(key, len(sizes))\n  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n\nlayer_sizes = [784, 512, 512, 10]\nstep_size = 0.01\nnum_epochs = 8\nbatch_size = 128\nn_targets = 10\nparams = init_network_params(layer_sizes, random.key(0))\n```\n\n----------------------------------------\n\nTITLE: Using `static_argnames` for JIT Compilation with Loops in JAX Python\nDESCRIPTION: Shows how to use the `static_argnames` argument in `jax.jit` to JIT-compile the function `g` (which failed previously). By specifying the argument name 'n' as static, its value is considered constant during tracing, enabling the `while` loop condition. Re-compilation happens for new values of `n`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ng_jit_correct = jax.jit(g, static_argnames=['n'])\nprint(g_jit_correct(10, 20))\n```\n\n----------------------------------------\n\nTITLE: Printing Traced Values in Compiled JAX Function\nDESCRIPTION: Demonstrates using jax.debug.print to print traced array values inside a jit-compiled function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/print_breakpoint.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef f(x):\n  jax.debug.print(\"🤯 {x} 🤯\", x=x)\n  y = jnp.sin(x)\n  jax.debug.print(\"🤯 {y} 🤯\", y=y)\n  return y\n\nf(2.)\n# Prints:\n# 🤯 2.0 🤯\n# 🤯 0.9092974662780762 🤯\n```\n\n----------------------------------------\n\nTITLE: Illustrating JIT Failure with Value-Based Conditionals in JAX Python\nDESCRIPTION: Demonstrates a limitation of `jax.jit`. Attempting to JIT-compile a function `f` that uses a standard Python `if` statement conditioned on the *value* of an input `x` results in a `ConcretizationTypeError` because JAX cannot trace control flow dependent on runtime values during compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Condition on value of x.\n\ndef f(x):\n  if x > 0:\n    return x\n  else:\n    return 2 * x\n\njax.jit(f)(10)  # Raises an error\n```\n\n----------------------------------------\n\nTITLE: Using JAX's Indexed Update Syntax for Immutable Arrays\nDESCRIPTION: Demonstrates JAX's approach to updating array elements using the .at[] syntax, which returns a new array with the updated value rather than modifying the original array in-place.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny = x.at[0].set(10)\nprint(x)\nprint(y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Model with JAX\nDESCRIPTION: Defines neural network prediction, loss function, and parameter initialization functions for a simple network, demonstrating JAX for machine learning.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.tanh(outputs)  # inputs to the next layer\n  return outputs                # no activation on last layer\n\ndef loss(params, batch):\n  inputs, targets = batch\n  predictions = predict(params, inputs)\n  return jnp.sum((predictions - targets)**2)\n\n\n\ndef init_layer(key, n_in, n_out):\n  k1, k2 = random.split(key)\n  W = random.normal(k1, (n_in, n_out))\n  b = random.normal(k2, (n_out,))\n  return W, b\n\nlayer_sizes = [5, 2, 3]\n\nkey = random.key(0)\nkey, *keys = random.split(key, len(layer_sizes))\nparams = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n\nkey, *keys = random.split(key, 3)\ninputs = random.normal(keys[0], (8, 5))\ntargets = random.normal(keys[1], (8, 3))\nbatch = (inputs, targets)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking a Standard JAX Function Execution in Python\nDESCRIPTION: Defines the Scaled Exponential Linear Unit (SELU) activation function using `jax.numpy`. It then benchmarks the execution time of this function on a large array using `%timeit`, ensuring completion with `block_until_ready()` due to JAX's asynchronous dispatch.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(1000000)\n%timeit selu(x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Using jax.checkpoint for Function Composition in Python\nDESCRIPTION: This snippet demonstrates the idiomatic way to achieve memory savings for function composition (`f(x) = h(g(x))`) using `jax.checkpoint`. By wrapping the inner function `g` with `jax.checkpoint`, its intermediate results (residuals) are not stored during the forward pass of `f_checkpoint`, and `g` is recomputed during the backward pass when differentiating `f_checkpoint`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef f_checkpoint(x):\n  y = jax.checkpoint(g)(x)\n  z = h(y)\n  return z\n```\n\n----------------------------------------\n\nTITLE: Using grad for Holomorphic Complex-to-Complex Functions - JAX - Python\nDESCRIPTION: Creates a holomorphic function f using jnp.sin, then computes its gradient at a complex number using grad(f, holomorphic=True). Specifying holomorphic=True is necessary for complex-valued outputs, and assumes responsibility to ensure holomorphicness. Demonstrates the interface and necessity for correct autodiff with complex outputs in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef f(z):\n  return jnp.sin(z)\n\nz = 3. + 4j\ngrad(f, holomorphic=True)(z)\n\n```\n\n----------------------------------------\n\nTITLE: Vectorizing Prediction Function with vmap\nDESCRIPTION: Uses JAX's vmap to automatically vectorize the prediction function, enabling it to handle batches of inputs efficiently without manual implementation of batching.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Make a batched version of the `predict` function\nbatched_predict = vmap(predict, in_axes=(None, 0))\n\n# `batched_predict` has the same call signature as `predict`\nbatched_preds = batched_predict(params, random_flattened_images)\nprint(batched_preds.shape)\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Data with TensorFlow Datasets\nDESCRIPTION: Uses TensorFlow Datasets to load and preprocess the MNIST dataset, converting it to NumPy arrays compatible with JAX operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\n# Ensure TF does not see GPU and grab all GPU memory.\ntf.config.set_visible_devices([], device_type='GPU')\n\nimport tensorflow_datasets as tfds\n\ndata_dir = '/tmp/tfds'\n\n# Fetch full datasets for evaluation\nmnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\nmnist_data = tfds.as_numpy(mnist_data)\ntrain_data, test_data = mnist_data['train'], mnist_data['test']\nnum_labels = info.features['label'].num_classes\nh, w, c = info.features['image'].shape\nnum_pixels = h * w * c\n\n# Full train set\ntrain_images, train_labels = train_data['image'], train_data['label']\ntrain_images = jnp.reshape(train_images, (len(train_images), num_pixels))\ntrain_labels = one_hot(train_labels, num_labels)\n\n# Full test set\ntest_images, test_labels = test_data['image'], test_data['label']\ntest_images = jnp.reshape(test_images, (len(test_images), num_pixels))\ntest_labels = one_hot(test_labels, num_labels)\n```\n\n----------------------------------------\n\nTITLE: Registering JVP (Forward Differentiation) Rule for JAX Primitive in Python\nDESCRIPTION: Defines and registers the forward-mode autodiff (JVP) rule for a custom JAX primitive. Computes both the primal output and its tangent (JVP) in a JAX-traceable way, handling ad.Zero values. Uses multiply_add_prim recursively for tangent computation. Requires JAX's ad, lax, and the primitive under test, as well as a correct environment for tracing and registration. Arguments and tangents must match in length and type.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.interpreters import ad\n\n@trace(\"multiply_add_value_and_jvp\")\ndef multiply_add_value_and_jvp(arg_values, arg_tangents):\n  \"\"\"Evaluates the primal output and the tangents (Jacobian-vector product).\n\n  Given values of the arguments and perturbation of the arguments (tangents), \n  compute the output of the primitive and the perturbation of the output.\n\n  This method must be JAX-traceable. JAX may invoke it with abstract values \n  for the arguments and tangents.\n\n  Args:\n    arg_values: A tuple of arguments\n    arg_tangents: A tuple with the tangents of the arguments. The tuple has \n      the same length as the arg_values. Some of the tangents may also be the \n      special value `ad.Zero` to specify a zero tangent\n\n  Returns:\n     A pair of the primal output and the tangent.\n  \"\"\"\n  x, y, z = arg_values\n  xt, yt, zt = arg_tangents\n  _trace(\"Primal evaluation:\")\n  # Now, you have a JAX-traceable computation of the output. \n  # Normally, you can use the multiply add (`ma`) primitive itself to compute the primal output. \n  primal_out = multiply_add_prim(x, y, z)\n\n  _trace(\"Tangent evaluation:\")\n  # You must use a JAX-traceable way to compute the tangent. It turns out that \n  # the output tangent can be computed as (xt * y + x * yt + zt),\n  # which you can implement in a JAX-traceable way using the same \"multiply_add_prim\" primitive.\n\n  # You do need to deal specially with `Zero`. Here, you just turn it into a \n  # proper tensor of 0s (of the same shape as 'x'). \n  # An alternative would be to check for `Zero` and perform algebraic \n  # simplification of the output tangent computation.\n  def make_zero(tan):\n    return lax.zeros_like_array(x) if type(tan) is ad.Zero else tan  \n\n  output_tangent = multiply_add_prim(make_zero(xt), y, multiply_add_prim(x, make_zero(yt), make_zero(zt)))\n  return (primal_out, output_tangent)\n\n# Register the forward differentiation rule with JAX:\nad.primitive_jvps[multiply_add_p] = multiply_add_value_and_jvp\n\n```\n\n----------------------------------------\n\nTITLE: Just-in-Time Compilation with jit in JAX\nDESCRIPTION: Shows how to use JAX's jit transformation to compile functions for improved performance, demonstrating significant speedup on element-wise operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = jnp.ones((5000, 5000))\nfast_f = jit(slow_f)\n%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X\n%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)\n```\n\n----------------------------------------\n\nTITLE: Implementing Numerically Stable log1pexp with custom_jvp\nDESCRIPTION: Using custom_jvp to define a numerically stable differentiation rule for log1pexp that avoids floating point issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import custom_jvp\n\n@custom_jvp\ndef log1pexp(x):\n  return jnp.log(1. + jnp.exp(x))\n\n@log1pexp.defjvp\ndef log1pexp_jvp(primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  ans = log1pexp(x)\n  ans_dot = (1 - 1/(1 + jnp.exp(x))) * x_dot\n  return ans, ans_dot\n```\n\n----------------------------------------\n\nTITLE: Comparing Convolution in jax.numpy vs jax.lax\nDESCRIPTION: Demonstrates the difference between the high-level convolution API in jax.numpy and the more powerful but verbose convolution API in jax.lax, showing how jax.numpy operations are expressed in terms of more fundamental lax operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.array([1, 2, 1])\ny = jnp.ones(10)\njnp.convolve(x, y)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\nresult = lax.conv_general_dilated(\n    x.reshape(1, 1, 3).astype(float),  # note: explicit promotion\n    y.reshape(1, 1, 10),\n    window_strides=(1,),\n    padding=[(len(y) - 1, len(y) - 1)])  # equivalent of padding='full' in NumPy\nresult[0, 0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Auto-batching with JAX's vmap\nDESCRIPTION: Uses JAX's vmap function to automatically vectorize the prediction function, enabling it to handle batches of images efficiently.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Make a batched version of the `predict` function\nbatched_predict = vmap(predict, in_axes=(None, 0))\n\n# `batched_predict` has the same call signature as `predict`\nbatched_preds = batched_predict(params, random_flattened_images)\nprint(batched_preds.shape)\n```\n\n----------------------------------------\n\nTITLE: Matrix-Jacobian Products with and without vmap\nDESCRIPTION: Implementations of Matrix-Jacobian products with and without JAX's vmap transformation. The vmap version efficiently vectorizes the computation for significant performance improvements without explicit loops.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\n# Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.\n# First, use a list comprehension to loop over rows in the matrix M.\ndef loop_mjp(f, x, M):\n    y, vjp_fun = vjp(f, x)\n    return jnp.vstack([vjp_fun(mi) for mi in M])\n\n# Now, use vmap to build a computation that does a single fast matrix-matrix\n# multiply, rather than an outer loop over vector-matrix multiplies.\ndef vmap_mjp(f, x, M):\n    y, vjp_fun = vjp(f, x)\n    outs, = vmap(vjp_fun)(M)\n    return outs\n\nkey = random.key(0)\nnum_covecs = 128\nU = random.normal(key, (num_covecs,) + y.shape)\n\nloop_vs = loop_mjp(f, W, M=U)\nprint('Non-vmapped Matrix-Jacobian product')\n%timeit -n10 -r3 loop_mjp(f, W, M=U)\n\nprint('\\nVmapped Matrix-Jacobian product')\nvmap_vs = vmap_mjp(f, W, M=U)\n%timeit -n10 -r3 vmap_mjp(f, W, M=U)\n\nassert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical'\n```\n\n----------------------------------------\n\nTITLE: Implementing Linear Regression with Explicit Parameter State in JAX (Python)\nDESCRIPTION: Defines the components for a JAX-based linear regression model using explicit state management for parameters. A `NamedTuple` `Params` holds the model's weight and bias. The `init` function initializes these parameters using `jax.random`. The `loss` function computes the mean squared error. Crucially, the `@jax.jit`-compiled `update` function takes the current `params` (state), computes gradients using `jax.grad`, performs an SGD update, and returns the *new* `params` state.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/stateful-computations.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import NamedTuple\n\nclass Params(NamedTuple):\n  weight: jnp.ndarray\n  bias: jnp.ndarray\n\n\ndef init(rng) -> Params:\n  \"\"\"Returns the initial model params.\"\"\"\n  weights_key, bias_key = jax.random.split(rng)\n  weight = jax.random.normal(weights_key, ())\n  bias = jax.random.normal(bias_key, ())\n  return Params(weight, bias)\n\n\ndef loss(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Computes the least squares error of the model's predictions on x against y.\"\"\"\n  pred = params.weight * x + params.bias\n  return jnp.mean((pred - y) ** 2)\n\n\nLEARNING_RATE = 0.005\n\n@jax.jit\ndef update(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> Params:\n  \"\"\"Performs one SGD update step on params using the given data.\"\"\"\n  grad = jax.grad(loss)(params, x, y)\n\n  # If we were using Adam or another stateful optimizer,\n  # we would also do something like\n  #\n  #   updates, new_optimizer_state = optimizer(grad, optimizer_state)\n  # \n  # and then use `updates` instead of `grad` to actually update the params.\n  # (And we'd include `new_optimizer_state` in the output, naturally.)\n\n  new_params = jax.tree.map(\n      lambda param, g: param - g * LEARNING_RATE, params, grad)\n\n  return new_params\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom JAX Primitive\nDESCRIPTION: This example shows how to create a new JAX primitive for the multiply-add operation. It demonstrates the setup of the primitive using core.Primitive and wrapping it in a traceable function, but doesn't yet implement the primitive's behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.extend import core\n\nmultiply_add_p = core.Primitive(\"multiply_add\")  # Create the primitive\n\n@trace(\"multiply_add_prim\")\ndef multiply_add_prim(x, y, z):\n  \"\"\"The JAX-traceable way to use the JAX primitive.\n  \n  Note that the traced arguments must be passed as positional arguments\n  to `bind`. \n  \"\"\"\n  return multiply_add_p.bind(x, y, z)\n\n@trace(\"square_add_prim\")\ndef square_add_prim(a, b):\n  \"\"\"A square-add function implemented using the new JAX-primitive.\"\"\"\n  return multiply_add_prim(a, a, b)\n```\n\n----------------------------------------\n\nTITLE: Applying JIT Selectively within a Loop in JAX Python\nDESCRIPTION: Presents a workaround for JIT limitations with dynamic loops. Instead of JIT-compiling the entire function `g_inner_jitted`, only the computationally intensive `loop_body` is JIT-compiled using the `@jax.jit` decorator. The outer Python loop still runs dynamically, calling the compiled body.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# While loop conditioned on x and n with a jitted body.\n\n@jax.jit\ndef loop_body(prev_i):\n  return prev_i + 1\n\ndef g_inner_jitted(x, n):\n  i = 0\n  while i < n:\n    i = loop_body(i)\n  return x + i\n\ng_inner_jitted(10, 20)\n```\n\n----------------------------------------\n\nTITLE: Using jax.tree.map on Nested Lists - JAX - Python\nDESCRIPTION: This snippet applies jax.tree.map to double each element in a nested list structure, similar to the Python map but recursive over all leaves. It operates only on the leaves, multiplying each leaf by 2, and requires the JAX library. The input is a list of lists containing integers; the output is a new pytree with the same structure but all values doubled.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlist_of_lists = [\\n    [1, 2, 3],\\n    [1, 2],\\n    [1, 2, 3, 4]\\n]\\n\\njax.tree.map(lambda x: x*2, list_of_lists)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hessian-Vector Product Function\nDESCRIPTION: Creates a function to efficiently compute Hessian-vector products without materializing the full Hessian matrix.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n```\n\n----------------------------------------\n\nTITLE: Installing JAX with CUDA 12 Support for NVIDIA GPUs using pip\nDESCRIPTION: This command installs the latest version of JAX along with the necessary bindings for NVIDIA GPU acceleration using CUDA 12. The `[cuda12]` extra specifies the CUDA version dependency. Requires compatible NVIDIA drivers and potentially the CUDA toolkit to be installed separately.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"jax[cuda12]\"\n```\n\n----------------------------------------\n\nTITLE: Automatic Vectorization using JAX's vmap\nDESCRIPTION: Uses JAX's vmap function to automatically vectorize the original convolve function. This approach maintains the simplicity of the original function while achieving efficient batched computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-vectorization.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nauto_batch_convolve = jax.vmap(convolve)\n\nauto_batch_convolve(xs, ws)\n```\n\n----------------------------------------\n\nTITLE: Performing First-Order Optimization with JAX Optimizers in Python\nDESCRIPTION: This snippet showcases how to use the JAX optimizers mini-library to train the parameters of a neural network (assumed to be defined by `net_apply` and initialized as `net_params` from the previous example). It defines a simple squared-error loss function, initializes a momentum optimizer (`optimizers.momentum`), and defines a JIT-compiled update step function (`step`) that computes gradients using `jax.grad` and applies the optimizer update. Finally, it runs a basic optimization loop over dummy data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/example_libraries/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.example_libraries import optimizers\nfrom jax import jit, grad\n\n# Define a simple squared-error loss\ndef loss(params, batch):\n  inputs, targets = batch\n  predictions = net_apply(params, inputs)\n  return jnp.sum((predictions - targets)**2)\n\n# Use optimizers to set optimizer initialization and update functions\nopt_init, opt_update, get_params = optimizers.momentum(step_size=1e-3, mass=0.9)\n\n# Define a compiled update step\n@jit\ndef step(i, opt_state, batch):\n  params = get_params(opt_state)\n  g = grad(loss)(params, batch)\n  return opt_update(i, g, opt_state)\n\n# Dummy input data stream\ndata_generator = ((jnp.zeros((128, 28, 28, 1)), jnp.zeros((128, 10)))\n                  for _ in range(10))\n\n# Optimize parameters in a loop\nopt_state = opt_init(net_params)\nfor i in range(10):\n  opt_state = step(i, opt_state, next(data_generator))\nnet_params = get_params(opt_state)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Primitive Transpose in JAX\nDESCRIPTION: Implementation of the transpose operation for a custom multiply_add primitive, handling different cases for constant and variable arguments and registering with JAX's autodiff system.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@trace(\"multiply_add_transpose\")\ndef multiply_add_transpose(ct, x, y, z):\n  \"\"\"Evaluates the transpose of a linear primitive.\n\n  This method is only used when computing the backward gradient following \n  `value_and_jvp`, and is only needed for primitives that are used in the JVP \n  calculation for some other primitive. You need a transposition for `multiply_add_prim`, \n  because you have used `multiply_add_prim` in the computation of the `output_tangent` in \n  `multiply_add_value_and_jvp`.\n\n  In this case, multiply_add is not a linear primitive. However, it is used linearly \n  w.r.t. tangents in `multiply_add_value_and_jvp`:\n       `output_tangent(xt, yt, zt) = multiply_add_prim(xt, y, multiply_add_prim(x, yt, zt))`.\n\n  Always one of the first two multiplicative arguments is a constant.\n\n  Args:\n      ct: The cotangent of the output of the primitive.\n      x, y, z: The values of the arguments. The arguments that are used linearly\n        get an ad.UndefinedPrimal value. The other arguments get a constant\n        value.\n\n  Returns:\n      A tuple with the cotangent of the inputs, with the value None\n      corresponding to the constant arguments.\n  \"\"\"\n  if not ad.is_undefined_primal(x):\n    # This use of multiply_add is with a constant \"x\".\n    assert ad.is_undefined_primal(y)\n    ct_y = ad.Zero(y.aval) if type(ct) is ad.Zero else multiply_add_prim(x, ct, lax.zeros_like_array(x))\n    res = None, ct_y, ct\n  else:\n    # This use of multiply_add is with a constant \"y\".\n    assert ad.is_undefined_primal(x)\n    ct_x = ad.Zero(x.aval) if type(ct) is ad.Zero else multiply_add_prim(ct, y, lax.zeros_like_array(y))\n    res = ct_x, None, ct\n  return res\n\nad.primitive_transposes[multiply_add_p] = multiply_add_transpose\n```\n\n----------------------------------------\n\nTITLE: Basic JAX NumPy Usage\nDESCRIPTION: Demonstration of JAX's NumPy-compatible API using a SELU activation function implementation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/quickstart.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n\ndef selu(x, alpha=1.67, lmbda=1.05):\n  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(5.0)\nprint(selu(x))\n```\n\n----------------------------------------\n\nTITLE: Efficient Vectorization with vmap in JAX\nDESCRIPTION: Demonstrates using JAX's vmap transformation to efficiently vectorize computations across a batch dimension, transforming the function to operate on batches without changing its core logic.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import vmap\npredictions = vmap(partial(predict, params))(input_batch)\n```\n\n----------------------------------------\n\nTITLE: Debugging JAX Computations Using debug.callback - Python\nDESCRIPTION: Uses debug.callback to insert arbitrary side-effectful function calls (e.g., logging) into JAX-jit compiled functions. No assumption of purity is required; debug.callback never returns values, only triggers side effects such as printing/logging. Useful for generalized tracing/debugging, compatible with jit and vmap.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import debug\n\ndef log_value(x):\n  # This could be an actual logging call; we'll use\n  # print() for demonstration\n  print(\"log:\", x)\n\n@jax.jit\ndef f(x):\n  debug.callback(log_value, x)\n  return x\n\nf(1.0);\n```\n\n----------------------------------------\n\nTITLE: Initializing Neural Network Parameters in JAX\nDESCRIPTION: Defines helper functions to randomly initialize weights and biases for dense neural network layers using JAX's random module.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef random_layer_params(m, n, key, scale=1e-2):\n  w_key, b_key = random.split(key)\n  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n\ndef init_network_params(sizes, key):\n  keys = random.split(key, len(sizes))\n  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n\nlayer_sizes = [784, 512, 512, 10]\nstep_size = 0.01\nnum_epochs = 10\nbatch_size = 128\nn_targets = 10\nparams = init_network_params(layer_sizes, random.key(0))\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Matrix Multiplication with Sharded Inputs in JAX\nDESCRIPTION: This code demonstrates how JAX chooses optimal output sharding for matrix multiplication when inputs are sharded across different dimensions. The left matrix is sharded across dimension 'a', the right matrix across dimension 'b', and the compiler chooses an output sharding that maximizes parallelism.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ny = jax.device_put(x, NamedSharding(mesh, P('a', None)))\nz = jax.device_put(x, NamedSharding(mesh, P(None, 'b')))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\n\nw = jnp.dot(y, z)\nprint('out sharding:')\njax.debug.visualize_array_sharding(w)\n```\n\n----------------------------------------\n\nTITLE: Basic VJP Implementation in JAX\nDESCRIPTION: Demonstrates how to use JAX's vjp function to compute Vector-Jacobian Products. This example shows pulling back a covector along a function evaluated at a specific point using random vectors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import vjp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\ny, vjp_fun = vjp(f, W)\n\nkey, subkey = random.split(key)\nu = random.normal(subkey, y.shape)\n\n# Pull back the covector `u` along `f` evaluated at `W`\nv = vjp_fun(u)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Conditional Logic in JAX Jaxpr Generation in Python\nDESCRIPTION: Demonstrates how `jax.make_jaxpr` handles Python conditionals. The generated jaxpr only captures the computational path taken based on the static properties (like `ndim`) of the input array during tracing, ignoring the untaken branch.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef log2_if_rank_2(x):\n  if x.ndim == 2:\n    ln_x = jnp.log(x)\n    ln_2 = jnp.log(2.0)\n    return ln_x / ln_2\n  else:\n    return x\n\nprint(jax.make_jaxpr(log2_if_rank_2)(jax.numpy.array([1, 2, 3])))\n```\n\n----------------------------------------\n\nTITLE: Installing JAX Nightly for NVIDIA GPU (CUDA 12) with Pip - Bash\nDESCRIPTION: Installs nightly pre-release versions of JAX, jaxlib, and CUDA 12-compatible plugins via pip for NVIDIA GPU acceleration. The command includes jax-cuda12-plugin (with optional CUDA extras) and jax-cuda12-pjrt to support CUDA 12 features. Prerequisite: NVIDIA CUDA 12 drivers and libraries must be installed. Input is the bash command; output is an updated JAX with CUDA-12 support. Pre-release versions may introduce instability.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install -U --pre jax jaxlib \"jax-cuda12-plugin[with-cuda]\" jax-cuda12-pjrt -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html\n\n```\n\n----------------------------------------\n\nTITLE: Setting up a Linear Logistic Regression Model in JAX (Python)\nDESCRIPTION: This snippet sets up the components for a linear logistic regression example. It initializes a JAX random key, defines helper functions `sigmoid` and `predict`, creates a toy dataset (`inputs`, `targets`), defines a `loss` function (negative log-likelihood), and initializes random model parameters `W` (weights) and `b` (bias). Requires JAX and jax.numpy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkey = jax.random.key(0)\n\ndef sigmoid(x):\n  return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n  return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12,  0.77],\n                    [0.88, -1.08, 0.15],\n                    [0.52, 0.06, -1.30],\n                    [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n  preds = predict(W, b, inputs)\n  label_probs = preds * targets + (1 - preds) * (1 - targets)\n  return -jnp.sum(jnp.log(label_probs))\n\n# Initialize random model coefficients\nkey, W_key, b_key = jax.random.split(key, 3)\nW = jax.random.normal(W_key, (3,))\nb = jax.random.normal(b_key, ())\n```\n\n----------------------------------------\n\nTITLE: Generating Binary Classification Dataset in NumPy\nDESCRIPTION: Creates a synthetic binary classification dataset using NumPy random functions. Generates feature vectors, true parameters, and binary labels.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(10009)\n\nnum_features = 10\nnum_points = 100\n\ntrue_beta = np.random.randn(num_features).astype(jnp.float32)\nall_x = np.random.randn(num_points, num_features).astype(jnp.float32)\ny = (np.random.rand(num_points) < sp.special.expit(all_x.dot(true_beta))).astype(jnp.int32)\n```\n\n----------------------------------------\n\nTITLE: Combining JAX Transformations: vmap and jit\nDESCRIPTION: Demonstrates how to compose JAX transformations by applying jit to a vmapped function, showcasing the composability of JAX's transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-vectorization.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\njitted_batch_convolve = jax.jit(auto_batch_convolve)\n\njitted_batch_convolve(xs, ws)\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients w.r.t. Specific Arguments using `jax.grad` (Python)\nDESCRIPTION: This snippet demonstrates using the `argnums` parameter of `jax.grad` to compute gradients of the previously defined `loss` function with respect to specific positional arguments. It calculates the gradient w.r.t. `W` (arg 0, default), `b` (arg 1), and both `W` and `b` (tuple `(0, 1)`), printing the results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Differentiate `loss` with respect to the first positional argument:\nW_grad = grad(loss, argnums=0)(W, b)\nprint(f'{W_grad=}')\n\n# Since argnums=0 is the default, this does the same thing:\nW_grad = grad(loss)(W, b)\nprint(f'{W_grad=}')\n\n# But you can choose different values too, and drop the keyword:\nb_grad = grad(loss, 1)(W, b)\nprint(f'{b_grad=}')\n\n# Including tuple values\nW_grad, b_grad = grad(loss, (0, 1))(W, b)\nprint(f'{W_grad=}')\nprint(f'{b_grad=}')\n```\n\n----------------------------------------\n\nTITLE: Compiling Neural Network Functions with JAX\nDESCRIPTION: JIT-compiles the loss function and creates a gradient function using JAX's automatic differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nloss_jit = jax.jit(loss)\ngradfun = jax.jit(jax.grad(loss))\n```\n\n----------------------------------------\n\nTITLE: Hessian-Vector Products using Reverse-over-Forward Mode\nDESCRIPTION: An alternative implementation of Hessian-vector products using reverse-over-forward mode differentiation. This is less efficient than the forward-over-reverse approach for most cases.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# reverse-over-forward\ndef hvp_revfwd(f, primals, tangents):\n  g = lambda primals: jvp(f, primals, tangents)[1]\n  return grad(g)(primals)\n```\n\n----------------------------------------\n\nTITLE: Implementing custom_jvp with pytree containers\nDESCRIPTION: Demonstrates how to use JAX's custom_jvp decorator with nested Python containers like namedtuples and dictionaries. The example defines a function that takes a Point namedtuple and returns a nested structure with dictionaries and tuples.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import namedtuple\nPoint = namedtuple(\"Point\", [\"x\", \"y\"])\n\n@custom_jvp\ndef f(pt):\n  x, y = pt.x, pt.y\n  return {'a': x ** 2,\n          'b': (jnp.sin(x), jnp.cos(y))}\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  pt, = primals\n  pt_dot, =  tangents\n  ans = f(pt)\n  ans_dot = {'a': 2 * pt.x * pt_dot.x,\n             'b': (jnp.cos(pt.x) * pt_dot.x, -jnp.sin(pt.y) * pt_dot.y)}\n  return ans, ans_dot\n\ndef fun(pt):\n  dct = f(pt)\n  return dct['a'] + dct['b'][0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom JVP with jax.custom_jvp\nDESCRIPTION: Basic example of defining a custom JVP rule for a function that multiplies sine of x by y, demonstrating the core pattern of the custom_jvp decorator and defjvp method.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import custom_jvp\n\n@custom_jvp\ndef f(x, y):\n  return jnp.sin(x) * y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primal_out = f(x, y)\n  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n  return primal_out, tangent_out\n```\n\n----------------------------------------\n\nTITLE: Implementing Transposed Convolution with JAX\nDESCRIPTION: Shows how to perform transposed convolution using JAX's lax.conv_general_dilated function. It uses VALID padding, no stride, and applies input dilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/convolutions.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nout = lax.conv_general_dilated(img,               # lhs = image tensor\n                               kernel,            # rhs = conv kernel tensor\n                               (1,1),             # window strides\n                               ((0, 0), (0, 0)),  # padding mode\n                               (2,2),             # lhs/image dilation\n                               (1,1),             # rhs/kernel dilation\n                               dn)                # dimension_numbers = lhs, rhs, out dimension permutation\nprint(\"out shape: \", out.shape, \"<-- larger than original!\")\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);\n```\n\n----------------------------------------\n\nTITLE: Parallel Matrix Multiplication across Devices\nDESCRIPTION: Distributes random matrix generation and matrix multiplication across multiple devices using pmap, demonstrating large-scale parallel computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nkeys = random.split(random.key(0), 8)\nmats = pmap(lambda key: random.normal(key, (5000, 5000)))(keys)\nresult = pmap(jnp.dot)(mats, mats)\nprint(pmap(jnp.mean)(result))\n```\n\n----------------------------------------\n\nTITLE: Using lax.cond for conditional operations\nDESCRIPTION: Shows how to use lax.cond for conditional operations in JIT-compiled functions, which is differentiable.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\n\noperand = jnp.array([0.])\nlax.cond(True, lambda x: x+1, lambda x: x-1, operand)\n# --> array([1.], dtype=float32)\nlax.cond(False, lambda x: x+1, lambda x: x-1, operand)\n# --> array([-1.], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Using `jax.test_util.check_grads` for Gradient Verification in Python\nDESCRIPTION: This snippet demonstrates using the JAX utility function `jax.test_util.check_grads` to conveniently check the gradients (and optionally higher-order derivatives) of a function against numerical differences. It calls `check_grads` on the `loss` function with arguments `(W, b)` and specifies `order=2` to check up to the second-order derivatives.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.test_util import check_grads\n\ncheck_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Neural Network Model in JAX Python\nDESCRIPTION: This snippet defines components of a basic neural network model for demonstration purposes. `layer` defines a single layer applying a dot product and a sine activation. `predict` stacks multiple `layer` calls followed by a final dot product. `loss` computes a simple mean squared error between predictions and targets.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef loss(params, x, y):\n  return jnp.sum((predict(params, x) - y)**2)\n\ndef predict(params, x):\n  *Ws, Wlast = params\n  for W in Ws:\n    x = layer(W, x)\n  x = jnp.dot(Wlast, x)\n  return x\n\ndef layer(W, x):\n  return jnp.sin(jnp.dot(W, x))\n```\n\n----------------------------------------\n\nTITLE: Checking Gradients Against Numerical Finite Differences in Python\nDESCRIPTION: This snippet illustrates how to verify the correctness of gradients computed by JAX's autodiff by comparing them against numerical approximations calculated using finite differences. It checks the gradient of `b` using scalar finite differences and the directional derivative of `W` using vector finite differences, comparing numerical results to the autodiff results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Set a step size for finite differences calculations\neps = 1e-4\n\n# Check b_grad with scalar finite differences\nb_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\nprint('b_grad_numerical', b_grad_numerical)\nprint('b_grad_autodiff', grad(loss, 1)(W, b))\n\n# Check W_grad with finite differences in a random direction\nkey, subkey = jax.random.split(key)\nvec = jax.random.normal(subkey, W.shape)\nunitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\nW_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\nprint('W_dirderiv_numerical', W_grad_numerical)\nprint('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec))\n```\n\n----------------------------------------\n\nTITLE: Calculating Gradient of Tanh using `jax.grad` in Python\nDESCRIPTION: This snippet demonstrates the basic usage of `jax.grad` to obtain the gradient function of `jnp.tanh`. It imports JAX libraries, defines `grad_tanh` as the gradient function of `jnp.tanh`, and then evaluates this gradient at the point `2.0`, printing the result. It requires JAX and jax.numpy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad\n\ngrad_tanh = grad(jnp.tanh)\nprint(grad_tanh(2.0))\n```\n\n----------------------------------------\n\nTITLE: Installing JAX for CPU-only (Linux/macOS/Windows)\nDESCRIPTION: Commands to install JAX for CPU-only usage across various operating systems including Linux, macOS, and Windows. This is useful for local development on laptops or systems without GPU acceleration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U jax\n```\n\n----------------------------------------\n\nTITLE: Using Named Checkpoints with Custom Policies\nDESCRIPTION: Demonstrates how to use checkpoint_name to label specific intermediate values and then create a policy that saves only those named checkpoints. This provides fine-grained control over which values are saved.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef f4(W1, W2, W3, x):\n  x = checkpoint_name(g(W1, x), name='a')\n  x = checkpoint_name(g(W2, x), name='b')\n  x = checkpoint_name(g(W3, x), name='c')\n  return x\n\nf4 = jax.checkpoint(f4, policy=jax.checkpoint_policies.save_only_these_names('a'))\njax.ad_checkpoint.print_saved_residuals(f4, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pytree Operations in Python\nDESCRIPTION: This code defines the core pytree operations including node type registration, tree flattening, and tree unflattening. It supports handling of various container types like tuples, lists, and dictionaries.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Hashable, Iterable, Iterator\nimport itertools as it\nfrom collections.abc import Callable\n\nclass NodeType(NamedTuple):\n  name: str\n  to_iterable: Callable\n  from_iterable: Callable\n\ndef register_pytree_node(ty: type, to_iter: Callable, from_iter: Callable\n                         ) -> None:\n  node_types[ty] = NodeType(str(ty), to_iter, from_iter)\n\nnode_types: dict[type, NodeType] = {}\nregister_pytree_node(tuple, lambda t: (None, t), lambda _, xs: tuple(xs))\nregister_pytree_node(list,  lambda l: (None, l), lambda _, xs:  list(xs))\nregister_pytree_node(dict,\n                     lambda d: map(tuple, unzip2(sorted(d.items()))),\n                     lambda keys, vals: dict(zip(keys, vals)))\n\nclass PyTreeDef(NamedTuple):\n  node_type: NodeType\n  node_metadata: Hashable\n  child_treedefs: tuple['PyTreeDef', ...]\n\nclass Leaf: pass\nleaf = Leaf()\n\ndef tree_flatten(x: Any) -> tuple[list[Any], PyTreeDef]:\n  children_iter, treedef = _tree_flatten(x)\n  return list(children_iter), treedef\n\ndef _tree_flatten(x: Any) -> tuple[Iterable, PyTreeDef]:\n  node_type = node_types.get(type(x))\n  if node_type:\n    node_metadata, children = node_type.to_iterable(x)\n    children_flat, child_trees = unzip2(map(_tree_flatten, children))\n    flattened = it.chain.from_iterable(children_flat)\n    return flattened, PyTreeDef(node_type, node_metadata, tuple(child_trees))\n  else:\n    return [x], leaf\n\ndef tree_unflatten(treedef: PyTreeDef, xs: list[Any]) -> Any:\n  return _tree_unflatten(treedef, iter(xs))\n\ndef _tree_unflatten(treedef: PyTreeDef, xs: Iterator) -> Any:\n  if treedef is leaf:\n    return next(xs)\n  else:\n    children = (_tree_unflatten(t, xs) for t in treedef.child_treedefs)\n    return treedef.node_type.from_iterable(treedef.node_metadata, children)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom VJP Rule for a Function in JAX (Python)\nDESCRIPTION: Shows how to use the @jax.custom_vjp decorator to provide a custom VJP rule for a function f that computes np.sin(x). f_fwd is the forward pass (returns both value and an auxiliary c: here cos(x)), and f_bwd is the backward pass (computes gradients using the auxiliary value and incoming gradients). Registration is done with f.defvjp(f_fwd, f_bwd). The snippet requires JAX and NumPy, supports arbitrary pytrees, and is suitable for higher-order differentiation and VJP customization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/2026-custom-derivatives.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# f :: a -> b\n@jax.custom_vjp\ndef f(x):\n  return np.sin(x)\n\n# f_fwd :: a -> (b, c)\ndef f_fwd(x):\n  return f(x), np.cos(x)\n\n# f_bwd :: (c, CT b) -> CT a\ndef f_bwd(cos_x, g):\n  return (cos_x * g,)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n----------------------------------------\n\nTITLE: Converting a JAX Function to Jaxpr Representation\nDESCRIPTION: Shows how to use jax.make_jaxpr to convert a JAX function into its jaxpr (JAX expression) representation for a given input.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/key-concepts.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(5.0)\njax.make_jaxpr(selu)(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing ELBO and Its Gradient for Variational Inference\nDESCRIPTION: Defines the Evidence Lower Bound (ELBO) function and its gradient for variational inference. Uses JAX's automatic differentiation for gradient computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef elbo(beta_loc, beta_log_scale, epsilon):\n    beta_sample = beta_loc + jnp.exp(beta_log_scale) * epsilon\n    return jnp.mean(batched_log_joint(beta_sample), 0) + jnp.sum(beta_log_scale - 0.5 * np.log(2*np.pi))\n\nelbo = jax.jit(elbo)\nelbo_val_and_grad = jax.jit(jax.value_and_grad(elbo, argnums=(0, 1)))\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Checkpointing in JAX for Logarithmic Memory Scaling\nDESCRIPTION: This code demonstrates how to implement recursive checkpointing in JAX to achieve logarithmic memory scaling for a chain of function compositions. It includes examples of both regular chain composition and recursive checkpointing for comparison.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef chain_compose(funs):\n  def f(x):\n    for fun in funs:\n      x = fun(x)\n    return x\n  return f\n\nf = chain_compose([jnp.sin] * 8)\nprint_saved_residuals(f, 3.)\n\nf = chain_compose([jnp.sin] * 16)\nprint_saved_residuals(f, 3.)\n\ndef recursive_checkpoint(funs):\n  if len(funs) == 1:\n    return funs[0]\n  elif len(funs) == 2:\n    f1, f2 = funs\n    return lambda x: f1(f2(x))\n  else:\n    f1 = recursive_checkpoint(funs[:len(funs)//2])\n    f2 = recursive_checkpoint(funs[len(funs)//2:])\n    return lambda x: f1(jax.checkpoint(f2)(x))\n\nf = recursive_checkpoint([jnp.sin] * 8)\nprint_saved_residuals(f, 3.)\n\nf = recursive_checkpoint([jnp.sin] * 16)\nprint_saved_residuals(f, 3.)\n\nf = chain_compose([jnp.sin] * 8)\nprint_fwd_bwd(f, 3.)\n\nf = recursive_checkpoint([jnp.sin] * 8)\nprint_fwd_bwd(f, 3.)\n```\n\n----------------------------------------\n\nTITLE: Applying Name-Based Checkpointing Policy in JAX Python\nDESCRIPTION: This snippet demonstrates a checkpointing policy that utilizes named intermediate values. It applies `jax.checkpoint` to the `loss` function using the policy `jax.checkpoint_policies.save_any_names_but_these('layer1_output')`. This policy saves all named residuals *except* the one named 'layer1_output', offering fine-grained control over memory usage based on the names assigned using `checkpoint_name`. The effect is shown by calling `print_saved_residuals`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nloss_checkpoint2 = jax.checkpoint(loss, policy=jax.checkpoint_policies.save_any_names_but_these('layer1_output'))\nprint_saved_residuals(loss_checkpoint2, params, x, y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom VJP with jax.custom_vjp\nDESCRIPTION: Example of defining a custom vector-Jacobian product (VJP) rule for the same sine multiplication function, showing the forward-backward pattern required by custom_vjp.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import custom_vjp\n\n@custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n  # Returns primal output and residuals to be used in backward pass by f_bwd.\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res # Gets residuals computed in f_fwd\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Addition with Pallas in Python\nDESCRIPTION: This snippet demonstrates how to use Pallas to implement a simple vector addition kernel. It shows the usage of Ref types for mutable memory access and the pallas_call function for kernel execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental import pallas as pl\n\ndef add_kernel(x_ref, y_ref, o_ref):\n  # In this code, `x_ref`, `y_ref` and `o_ref` are (8,)-shaped `Ref`s\n  x = x_ref[:]\n  y = y_ref[:]\n  o_ref[:] = x + y\nx, y = jnp.arange(8), jnp.arange(8, 16)\nadd = pl.pallas_call(add_kernel, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\nadd(x, y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Wave Equation Simulation with JAX\nDESCRIPTION: Defines functions for spatial partitioning, halo exchange, reshaping inputs/outputs for pmap, and implementing the physics of the wave equation using JAX. It includes the core simulation logic and time-stepping functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nimport jax\nfrom jax import lax\nfrom jax import tree_util\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport skimage.filters\nimport proglog\nfrom moviepy.editor import ImageSequenceClip\n\ndevice_count = jax.device_count()\n\n# Spatial partitioning via halo exchange\n\ndef send_right(x, axis_name):\n  # Note: if some devices are omitted from the permutation, lax.ppermute\n  # provides zeros instead. This gives us an easy way to apply Dirichlet\n  # boundary conditions.\n  left_perm = [(i, (i + 1) % device_count) for i in range(device_count - 1)]\n  return lax.ppermute(x, perm=left_perm, axis_name=axis_name)\n\ndef send_left(x, axis_name):\n  left_perm = [((i + 1) % device_count, i) for i in range(device_count - 1)]\n  return lax.ppermute(x, perm=left_perm, axis_name=axis_name)\n\ndef axis_slice(ndim, index, axis):\n  slices = [slice(None)] * ndim\n  slices[axis] = index\n  return tuple(slices)\n\ndef slice_along_axis(array, index, axis):\n  return array[axis_slice(array.ndim, index, axis)]\n\ndef tree_vectorize(func):\n  def wrapper(x, *args, **kwargs):\n    return tree_util.tree_map(lambda x: func(x, *args, **kwargs), x)\n  return wrapper\n\n@tree_vectorize\ndef halo_exchange_padding(array, padding=1, axis=0, axis_name='x'):\n  if not padding > 0:\n    raise ValueError(f'invalid padding: {padding}')\n  array = jnp.array(array)\n  if array.ndim == 0:\n    return array\n  left = slice_along_axis(array, slice(None, padding), axis)\n  right = slice_along_axis(array, slice(-padding, None), axis)\n  right, left = send_left(left, axis_name), send_right(right, axis_name)\n  return jnp.concatenate([left, array, right], axis)\n\n@tree_vectorize\ndef halo_exchange_inplace(array, padding=1, axis=0, axis_name='x'):\n  left = slice_along_axis(array, slice(padding, 2*padding), axis)\n  right = slice_along_axis(array, slice(-2*padding, -padding), axis)\n  right, left = send_left(left, axis_name), send_right(right, axis_name)\n  array = array.at[axis_slice(array.ndim, slice(None, padding), axis)].set(left)\n  array = array.at[axis_slice(array.ndim, slice(-padding, None), axis)].set(right)\n  return array\n\n# Reshaping inputs/outputs for pmap\n\ndef split_with_reshape(array, num_splits, *, split_axis=0, tile_id_axis=None):\n  if tile_id_axis is None:\n    tile_id_axis = split_axis\n  tile_size, remainder = divmod(array.shape[split_axis], num_splits)\n  if remainder:\n    raise ValueError('num_splits must equally divide the dimension size')\n  new_shape = list(array.shape)\n  new_shape[split_axis] = tile_size\n  new_shape.insert(split_axis, num_splits)\n  return jnp.moveaxis(jnp.reshape(array, new_shape), split_axis, tile_id_axis)\n\ndef stack_with_reshape(array, *, split_axis=0, tile_id_axis=None):\n  if tile_id_axis is None:\n    tile_id_axis = split_axis\n  array = jnp.moveaxis(array, tile_id_axis, split_axis)\n  new_shape = array.shape[:split_axis] + (-1,) + array.shape[split_axis+2:]\n  return jnp.reshape(array, new_shape)\n\ndef shard(func):\n  def wrapper(state):\n    sharded_state = tree_util.tree_map(\n        lambda x: split_with_reshape(x, device_count), state)\n    sharded_result = func(sharded_state)\n    result = tree_util.tree_map(stack_with_reshape, sharded_result)\n    return result\n  return wrapper\n\n# Physics\n\ndef shift(array, offset, axis):\n  index = slice(offset, None) if offset >= 0 else slice(None, offset)\n  sliced = slice_along_axis(array, index, axis)\n  padding = [(0, 0)] * array.ndim\n  padding[axis] = (-min(offset, 0), max(offset, 0))\n  return jnp.pad(sliced, padding, mode='constant', constant_values=0)\n\ndef laplacian(array, step=1):\n  left = shift(array, +1, axis=0)\n  right = shift(array, -1, axis=0)\n  up = shift(array, +1, axis=1)\n  down = shift(array, -1, axis=1)\n  convolved = (left + right + up + down - 4 * array)\n  if step != 1:\n    convolved *= (1 / step ** 2)\n  return convolved\n\ndef scalar_wave_equation(u, c=1, dx=1):\n  return c ** 2 * laplacian(u, dx)\n\n@jax.jit\ndef leapfrog_step(state, dt=0.5, c=1):\n  # https://en.wikipedia.org/wiki/Leapfrog_integration\n  u, u_t = state\n  u_tt = scalar_wave_equation(u, c)\n  u_t = u_t + u_tt * dt\n  u = u + u_t * dt\n  return (u, u_t)\n\n# Time stepping\n\ndef multi_step(state, count, dt=1/jnp.sqrt(2), c=1):\n  return lax.fori_loop(0, count, lambda i, s: leapfrog_step(s, dt, c), state)\n\ndef multi_step_pmap(state, count, dt=1/jnp.sqrt(2), c=1, exchange_interval=1,\n                    save_interval=1):\n\n  def exchange_and_multi_step(state_padded):\n    c_padded = halo_exchange_padding(c, exchange_interval)\n    evolved = multi_step(state_padded, exchange_interval, dt, c_padded)\n    return halo_exchange_inplace(evolved, exchange_interval)\n\n  @shard\n  @partial(jax.pmap, axis_name='x')\n  def simulate_until_output(state):\n    stop = save_interval // exchange_interval\n    state_padded = halo_exchange_padding(state, exchange_interval)\n    advanced = lax.fori_loop(\n        0, stop, lambda i, s: exchange_and_multi_step(s), state_padded)\n    xi = exchange_interval\n    return tree_util.tree_map(lambda array: array[xi:-xi, ...], advanced)\n\n  results = [state]\n  for _ in range(count // save_interval):\n    state = simulate_until_output(state)\n    tree_util.tree_map(lambda x: x.copy_to_host_async(), state)\n    results.append(state)\n  results = jax.device_get(results)\n  return tree_util.tree_map(lambda *xs: np.stack([np.array(x) for x in xs]), *results)\n\nmulti_step_jit = jax.jit(multi_step)\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobian-Vector Products (JVPs) in JAX\nDESCRIPTION: Demonstrates how to compute Jacobian-Vector products using JAX's jvp function, which is the foundation of forward-mode automatic differentiation. It pushes a random vector along the function evaluated at W.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jvp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nkey, subkey = random.split(key)\nv = random.normal(subkey, W.shape)\n\n# Push forward the vector `v` along `f` evaluated at `W`\ny, u = jvp(f, (W,), (v,))\n```\n\n----------------------------------------\n\nTITLE: Using static_argnames for JIT compilation with conditionals\nDESCRIPTION: Shows how to use static_argnames to allow JIT compilation of functions with input-value-dependent control flow.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  if x < 3:\n    return 3. * x ** 2\n  else:\n    return -4 * x\n\nf = jit(f, static_argnames='x')\n\nprint(f(2.))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JIT Caching Behavior in Loops with Python and JAX\nDESCRIPTION: This code snippet illustrates different approaches to using jax.jit within loops, showcasing how partial functions and lambdas can lead to repeated compilation, while using a normal function allows for proper caching. It includes timing comparisons to demonstrate the performance impact.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\ndef unjitted_loop_body(prev_i):\n  return prev_i + 1\n\ndef g_inner_jitted_partial(x, n):\n  i = 0\n  while i < n:\n    # Don't do this! each time the partial returns\n    # a function with different hash\n    i = jax.jit(partial(unjitted_loop_body))(i)\n  return x + i\n\ndef g_inner_jitted_lambda(x, n):\n  i = 0\n  while i < n:\n    # Don't do this!, lambda will also return\n    # a function with a different hash\n    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n  return x + i\n\ndef g_inner_jitted_normal(x, n):\n  i = 0\n  while i < n:\n    # this is OK, since JAX can find the\n    # cached, compiled function\n    i = jax.jit(unjitted_loop_body)(i)\n  return x + i\n\nprint(\"jit called in a loop with partials:\")\n%timeit g_inner_jitted_partial(10, 20).block_until_ready()\n\nprint(\"jit called in a loop with lambdas:\")\n%timeit g_inner_jitted_lambda(10, 20).block_until_ready()\n\nprint(\"jit called in a loop with caching:\")\n%timeit g_inner_jitted_normal(10, 20).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Illustrating JIT Failure with Value-Based Loops in JAX Python\nDESCRIPTION: Shows another limitation of `jax.jit`. Trying to JIT-compile a function `g` containing a standard Python `while` loop whose condition depends on the *value* of input `n` leads to a `ConcretizationTypeError`. JAX tracing requires loop bounds to be static during compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# While loop conditioned on x and n.\n\ndef g(x, n):\n  i = 0\n  while i < n:\n    i += 1\n  return x + i\n\njax.jit(g)(10, 20)  # Raises an error\n```\n\n----------------------------------------\n\nTITLE: Parallel Matrix Creation and Multiplication with JAX pmap in Python\nDESCRIPTION: This example demonstrates `jax.pmap` for SPMD parallel programming on 8 devices (e.g., GPUs). First, it creates 8 separate random keys and uses `pmap` to generate a unique 5000x6000 matrix on each device in parallel. Then, it uses `pmap` again to perform a matrix multiplication (`x @ x.T`) locally on each device's matrix in parallel. Finally, it computes the mean of each resulting matrix on its respective device and prints the array of means.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import random, pmap\nimport jax.numpy as jnp\n\n# Create 8 random 5000 x 6000 matrices, one per GPU\nkeys = random.split(random.key(0), 8)\nmats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)\n\n# Run a local matmul on each device in parallel (no data transfer)\nresult = pmap(lambda x: jnp.dot(x, x.T))(mats)  # result.shape is (8, 5000, 5000)\n\n# Compute the mean on each device in parallel and print the result\nprint(pmap(jnp.mean)(result))\n# prints [1.1566595 1.1805978 ... 1.2321935 1.2015157]\n```\n\n----------------------------------------\n\nTITLE: Sharding Single-Device Exported JAX Functions Across Multiple Devices - Python\nDESCRIPTION: This snippet shows the facility in JAX that allows a function exported for 1 device (with no sharding annotations) to be automatically sharded and executed on multiple devices if called with a sharded argument. It creates and exports a function, then demonstrates preparing a new mesh, sharding the input, and successfully calling the exported artifact. Required dependencies are JAX and related sharding subsystems.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax import export\n>>> from jax.sharding import Mesh, NamedSharding\n>>> from jax.sharding import PartitionSpec as P\n\n>>> def f(x):\n...   return jnp.cos(x)\n\n>>> arg = jnp.arange(4)\n>>> exp = export.export(jax.jit(f))(arg)\n>>> exp.in_avals\n(ShapedArray(int32[4]),)\n\n>>> exp.nr_devices\n1\n\n>>> # Prepare the mesh for calling `exp`.\n>>> calling_mesh = Mesh(jax.local_devices()[:4], (\"b\",))\n\n>>> # Shard the arg according to what `exp` expects.\n>>> sharded_arg = jax.device_put(arg,\n...                              NamedSharding(calling_mesh, P(\"b\")))\n>>> res = exp.call(sharded_arg)\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JIT Tracing in Python with JAX\nDESCRIPTION: This snippet shows how JIT compilation works by tracing a function. It illustrates how JAX uses tracer objects to represent inputs during compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x, y):\n  print(\"Running f():\")\n  print(f\"  x = {x}\")\n  print(f\"  y = {y}\")\n  result = jnp.dot(x + 1, y + 1)\n  print(f\"  result = {result}\")\n  return result\n\nx = np.random.randn(3, 4)\ny = np.random.randn(4)\nf(x, y)\n```\n\n----------------------------------------\n\nTITLE: Misusing holomorphic=True for Non-Holomorphic Complex Functions - grad in JAX - Python\nDESCRIPTION: Shows JAX's grad on a non-holomorphic function (complex conjugate) with holomorphic=True set. JAX returns only partial Jacobian information, demonstrating the limitation and requirement that the function be actually holomorphic for full correctness. Useful as a caution when handling complex gradients. Requires jnp and grad.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef f(z):\n  return jnp.conjugate(z)\n\nz = 3. + 4j\ngrad(f, holomorphic=True)(z)  # f is not actually holomorphic!\n\n```\n\n----------------------------------------\n\nTITLE: Computing Per-Example Gradients with JAX vmap and grad in Python\nDESCRIPTION: This snippet shows how to compute gradients of a loss function for each example in a batch individually using a combination of `jax.vmap` and `jax.grad`. `grad(loss)` creates a function that computes the gradient of `loss` with respect to its first argument. `partial(..., params)` fixes the `params` argument for the gradient function. `vmap` then maps this gradient computation over the `inputs` and `targets` arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nper_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)\n```\n\n----------------------------------------\n\nTITLE: Vectorizing Predictions with JAX vmap in Python\nDESCRIPTION: This snippet demonstrates using `jax.vmap` to automatically vectorize a prediction function over a batch of inputs. The `vmap` transformation maps the `predict` function across the `input_batch` (axis 0), while keeping the `params` fixed (indicated by `None` in `in_axes`). This effectively applies the prediction function to each item in the batch.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# or, alternatively\npredictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n```\n\n----------------------------------------\n\nTITLE: Training a JAX Linear Regression Model with Explicit State Handling in Python\nDESCRIPTION: Demonstrates the training loop for the previously defined JAX linear regression model. It generates synthetic data, initializes the model parameters (`params`) using the `init` function, and then iteratively updates the parameters by calling the JIT-compiled `update` function within a loop. The `params` state is explicitly passed into and received from the `update` function in each iteration. Finally, it visualizes the fitted regression line against the data using `matplotlib`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/stateful-computations.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nrng = jax.random.key(42)\n\n# Generate true data from y = w*x + b + noise\ntrue_w, true_b = 2, -1\nx_rng, noise_rng = jax.random.split(rng)\nxs = jax.random.normal(x_rng, (128, 1))\nnoise = jax.random.normal(noise_rng, (128, 1)) * 0.5\nys = xs * true_w + true_b + noise\n\n# Fit regression\nparams = init(rng)\nfor _ in range(1000):\n  params = update(params, xs, ys)\n\nplt.scatter(xs, ys)\nplt.plot(xs, params.weight * xs + params.bias, c='red', label='Model Prediction')\nplt.legend();\n```\n\n----------------------------------------\n\nTITLE: Validating Gradient Calculation with JAX\nDESCRIPTION: Example of using JAX's grad function with a custom primitive and verifying the result, showing how the transpose operations are used in reverse differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nassert api.grad(square_add_prim)(2., 10.) == 4.\n```\n\n----------------------------------------\n\nTITLE: Operations on Distributed Arrays\nDESCRIPTION: Demonstrates performing operations on distributed arrays, with the operations automatically executing in parallel across devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nz = y / 2\nprint(z)\n```\n\n----------------------------------------\n\nTITLE: Vector-Valued Gradients with VJPs\nDESCRIPTION: Implementation of a vector-valued gradient function using JAX's vjp. This is equivalent to TensorFlow's tf.gradients functionality but built using JAX's differentiation primitives.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import vjp\n\ndef vgrad(f, x):\n  y, vjp_fn = vjp(f, x)\n  return vjp_fn(jnp.ones(y.shape))[0]\n\nprint(vgrad(lambda x: 3*x**2, jnp.ones((2, 2))))\n```\n\n----------------------------------------\n\nTITLE: Validating Vectorized Mapping with JAX\nDESCRIPTION: Example of using JAX's vmap function with a custom primitive on batched inputs, demonstrating vectorized execution and verifying correct results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nassert np.allclose(api.vmap(square_add_prim, in_axes=0, out_axes=0)(\n  np.array([2., 3.]),\n  np.array([10., 20.])),\n  [14., 29.])\n```\n\n----------------------------------------\n\nTITLE: Wrapping External Bessel Function with pure_callback in JAX - Python\nDESCRIPTION: Implements a JAX-compatible wrapper for scipy.special.jv using jax.pure_callback, including shape/dtype annotation and broadcasting safety. Restricts order parameter to integers, and dtype handling is driven by JAX's result_type logic. Enables use of external computation in JAX pipelines, but not differentiable by default. Dependencies: jax, scipy.special, jax.numpy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport scipy.special\n\ndef jv(v, z):\n  v, z = jnp.asarray(v), jnp.asarray(z)\n\n  # Require the order v to be integer type: this simplifies\n  # the JVP rule below.\n  assert jnp.issubdtype(v.dtype, jnp.integer)\n\n  # Promote the input to inexact (float/complex).\n  # Note that jnp.result_type() accounts for the enable_x64 flag.\n  z = z.astype(jnp.result_type(float, z.dtype))\n\n  # Wrap scipy function to return the expected dtype.\n  _scipy_jv = lambda v, z: scipy.special.jv(v, z).astype(z.dtype)\n\n  # Define the expected shape & dtype of output.\n  result_shape_dtype = jax.ShapeDtypeStruct(\n      shape=jnp.broadcast_shapes(v.shape, z.shape),\n      dtype=z.dtype)\n\n  # Use vmap_method=\"broadcast_all\" because scipy.special.jv handles broadcasted inputs.\n  return jax.pure_callback(_scipy_jv, result_shape_dtype, v, z, vmap_method=\"broadcast_all\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Reverse-Mode Jacobian in JAX (jacrev)\nDESCRIPTION: Custom implementation of reverse-mode Jacobian computation using vector-Jacobian products (vjp) and vmap for efficient batching. The function computes the full Jacobian matrix by applying vjp to the Euclidean basis.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jacrev as builtin_jacrev\n\ndef our_jacrev(f):\n    def jacfun(x):\n        y, vjp_fun = vjp(f, x)\n        # Use vmap to do a matrix-Jacobian product.\n        # Here, the matrix is the Euclidean basis, so we get all\n        # entries in the Jacobian at once.\n        J, = vmap(vjp_fun, in_axes=0)(jnp.eye(len(y)))\n        return J\n    return jacfun\n\nassert jnp.allclose(builtin_jacrev(f)(W), our_jacrev(f)(W)), 'Incorrect reverse-mode Jacobian results!'\n```\n\n----------------------------------------\n\nTITLE: Using Sharded jax.Array with Automatic Parallelization in Python\nDESCRIPTION: This example demonstrates creating a sharded `jax.Array` across multiple devices using `jax.sharding.Mesh` and `jax.sharding.NamedSharding`. It shows how operations like matrix multiplication (`@`), element-wise functions (`jnp.sin`), copying (`jnp.copy`), and `jax.jit`-compiled functions can operate directly on sharded arrays, preserving the sharding and enabling automatic parallelization without requiring data consolidation to a single device.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nx = jnp.arange(8)\n\n# Let's say there are 8 devices in jax.devices()\nmesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x'))\n\nsharded_x = jax.device_put(x, sharding)\n\n# `matmul_sharded_x` and `sin_sharded_x` are sharded. `jit` is able to operate over a\n# sharded array without copying data to a single device.\nmatmul_sharded_x = sharded_x @ sharded_x.T\nsin_sharded_x = jnp.sin(sharded_x)\n\n# Even jnp.copy preserves the sharding on the output.\ncopy_sharded_x = jnp.copy(sharded_x)\n\n# double_out is also sharded\ndouble_out = jax.jit(lambda x: x * 2)(sharded_x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Cross-Shard Aggregation with psum in JAX\nDESCRIPTION: This snippet demonstrates how to perform cross-shard aggregation using jax.lax.psum within shard_map. It calculates the sum within each shard, then uses psum to combine results across all shards, producing a global sum.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  sum_in_shard = x.sum()\n  return jax.lax.psum(sum_in_shard, 'x')\n\nshard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P())(x)\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Modules\nDESCRIPTION: Imports key JAX modules including NumPy-compatible array operations, automatic differentiation, just-in-time compilation, and vectorized mapping.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\n```\n\n----------------------------------------\n\nTITLE: Using jax.debug.print with jax.vmap\nDESCRIPTION: Shows how jax.debug.print can be used to inspect values being mapped over in a vectorized function using jax.vmap.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  jax.debug.print(\"jax.debug.print(x) -> {}\", x)\n  y = jnp.sin(x)\n  jax.debug.print(\"jax.debug.print(y) -> {}\", y)\n  return y\n\nxs = jnp.arange(3.)\n\nresult = jax.vmap(f)(xs)\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation in JAX\nDESCRIPTION: This snippet demonstrates Just-In-Time (JIT) compilation in JAX. It defines a function that performs multiple iterations of array operations and compares the performance of the original function with its JIT-compiled version.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\n\nkey = random.key(0)\nx = random.normal(key, (5000, 5000))\n\ndef f(x):\n  y = x\n  for _ in range(10):\n    y = y - 0.1 * y + 3.\n  return y[:100, :100]\n\nf(x)\n\ng = jit(f)\ng(x)\n\n%timeit f(x).block_until_ready()\n\n%timeit -n 100 g(x).block_until_ready()\n\ngrad(jit(grad(jit(grad(jnp.tanh)))))(1.0)\n```\n\n----------------------------------------\n\nTITLE: Setting XLA Flags for GPU Performance in Python\nDESCRIPTION: Example of setting XLA flags at the beginning of a Python file to improve GPU performance. Shows how to enable Triton GEMM and latency hiding scheduler.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ['XLA_FLAGS'] = (\n    '--xla_gpu_triton_gemm_any=True '\n    '--xla_gpu_enable_latency_hiding_scheduler=true '\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing the Inverse Function Wrapper\nDESCRIPTION: Creates a function that traces the input function to obtain its JAX expression (jaxpr) and then applies custom inverse interpretation to it. Designed for unary functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef inverse(fun):\n  @wraps(fun)\n  def wrapped(*args, **kwargs):\n    # Since we assume unary functions, we won't worry about flattening and\n    # unflattening arguments.\n    closed_jaxpr = jax.make_jaxpr(fun)(*args, **kwargs)\n    out = inverse_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, *args)\n    return out[0]\n  return wrapped\n```\n\n----------------------------------------\n\nTITLE: Neural Network Implementation with JAX Scan\nDESCRIPTION: Reimplements the neural network using jax.lax.scan to iterate over layers, which reduces compilation time for large networks. This approach requires stacking the parameters beforehand.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nStackedWeights = jnp.ndarray  # all weight matrices stacked together\nStackedBiases = jnp.ndarray   # all bias vectors stacked together\n\nall_weights = jnp.stack([W for W, _ in params])\nall_biases = jnp.stack([b for _, b in params])\n\ndef layer(x, W_b_pair):\n  W, b = W_b_pair\n  out = jnp.maximum(jnp.dot(x, W) + b, 0.)\n  return out, None\n\ndef net(all_weights, all_biases, x):\n  x, _ = jax.lax.scan(layer, x, (all_weights, all_biases))\n  return x\n```\n\n----------------------------------------\n\nTITLE: Saving JAX Model with Parameters as TensorFlow SavedModel\nDESCRIPTION: Shows how to properly convert a JAX model to TensorFlow, ensuring parameters are saved as variables and not embedded in the computation graph.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef model_jax(params, inputs):\n  return params[0] + params[1] * inputs\n\n# Wrap the parameter constants as tf.Variables; this will signal to the model\n# saving code to save those constants as variables, separate from the\n# computation graph.\nparams_vars = tf.nest.map_structure(tf.Variable, params)\n\n# Build the prediction function by closing over the `params_vars`. If you\n# instead were to close over `params` your SavedModel would have no variables\n# and the parameters will be included in the function graph.\nprediction_tf = lambda inputs: jax2tf.convert(model_jax)(params_vars, inputs)\n\nmy_model = tf.Module()\n# Tell the model saver what the variables are.\nmy_model._variables = tf.nest.flatten(params_vars)\nmy_model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\ntf.saved_model.save(my_model)\n```\n\n----------------------------------------\n\nTITLE: Using Named Checkpoints with Custom Policies\nDESCRIPTION: Creating named checkpoints in a function and using a policy to selectively save only specific named intermediate values during differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef f4(W1, W2, W3, x):\n  x = checkpoint_name(g(W1, x), name='a')\n  x = checkpoint_name(g(W2, x), name='b')\n  x = checkpoint_name(g(W3, x), name='c')\n  return x\n\nf4 = jax.checkpoint(f4, policy=jax.checkpoint_policies.save_only_these_names('a'))\njax.ad_checkpoint.print_saved_residuals(f4, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Registering XLA Lowering Rule for JAX Primitive in Python\nDESCRIPTION: Provides and registers the MLIR/XLA lowering rule for a custom JAX primitive for the CPU backend. Uses the JAX HLO dialect to define an AddOp(MulOp(x, y), z) composition, returning the result as a list. Registers the lowering using mlir.register_lowering on the given primitive. Requires valid MLIR context, correct type for ctx and operands, and HLO and mlir modules from JAX. Inputs are MLIR value objects; output is a list containing the result value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom jax._src.lib.mlir.dialects import hlo\n\n@trace(\"multiply_add_lowering\")\ndef multiply_add_lowering(ctx, xc, yc, zc):\n  \"\"\"The compilation to XLA of the primitive.\n\n  Given an mlir.ir.Value for each argument, return the mlir.ir.Values for\n  the results of the function.\n\n  Does not need to be a JAX-traceable function.\n  \"\"\"\n  return [hlo.AddOp(hlo.MulOp(xc, yc), zc).result]\n\n# Now, register the lowering rule with JAX.\n# For GPU, refer to the https://docs.jax.dev/en/latest/Custom_Operation_for_GPUs.html\nfrom jax.interpreters import mlir\n\nmlir.register_lowering(multiply_add_p, multiply_add_lowering, platform='cpu')\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating vmap Usage in Python\nDESCRIPTION: Shows examples of using vmap to vectorize a scalar function and to compute the Jacobian of a function using forward-mode autodiff.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\ndef add_one_to_a_scalar(scalar):\n  assert np.ndim(scalar) == 0\n  return 1 + scalar\n\nvector_in = np.arange(3.)\nvector_out = vmap(add_one_to_a_scalar, (0,))(vector_in)\n\nprint(vector_in)\nprint(vector_out)\n\ndef jacfwd(f, x):\n  pushfwd = lambda v: jvp(f, (x,), (v,))[1]\n  vecs_in = np.eye(np.size(x)).reshape(np.shape(x) * 2)\n  return vmap(pushfwd, (0,))(vecs_in)\n\ndef f(x):\n  return sin(x)\n\njacfwd(f, np.arange(3.))\n```\n\n----------------------------------------\n\nTITLE: Generating Pseudorandom Numbers in JAX\nDESCRIPTION: Illustrates JAX's approach to pseudorandom number generation using explicit key management, demonstrating key creation, splitting, and use in random sampling.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/key-concepts.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import random\n\nkey = random.key(43)\nprint(key)\n\nprint(random.normal(key))\nprint(random.normal(key))\n\nfor i in range(3):\n  new_key, subkey = random.split(key)\n  del key  # The old key is consumed by split() -- we must never use it again.\n\n  val = random.normal(subkey)\n  del subkey  # The subkey is consumed by normal().\n\n  print(f\"draw {i}: {val}\")\n  key = new_key  # new_key is safe to use in the next iteration.\n```\n\n----------------------------------------\n\nTITLE: Higher-Order Differentiation in JAX\nDESCRIPTION: Demonstrates JAX's ability to compute higher-order derivatives by applying the grad function multiple times.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(grad(grad(tanh)))(1.0))\n# prints 0.62162673\n```\n\n----------------------------------------\n\nTITLE: Validating JVP for Bi-Linear Complex Functions with Randomization - JAX - Python\nDESCRIPTION: Checks the correctness of JAX's Jacobian-vector product (JVP) for functions decomposed into linear components u and v, whose coefficients are randomized per run using JAX's random module. This function prepares randomized coefficients, constructs a test point and tangent, computes JVP and compares the result against an explicit analytical expression, printing if they match. Requires JAX's jnp, grad, random, and jvp functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef check(seed):\n  key = random.key(seed)\n\n  # random coeffs for u and v\n  key, subkey = random.split(key)\n  a, b, c, d = random.uniform(subkey, (4,))\n\n  def fun(z):\n    x, y = jnp.real(z), jnp.imag(z)\n    return u(x, y) + v(x, y) * 1j\n\n  def u(x, y):\n    return a * x + b * y\n\n  def v(x, y):\n    return c * x + d * y\n\n  # primal point\n  key, subkey = random.split(key)\n  x, y = random.uniform(subkey, (2,))\n  z = x + y * 1j\n\n  # tangent vector\n  key, subkey = random.split(key)\n  c, d = random.uniform(subkey, (2,))\n  z_dot = c + d * 1j\n\n  # check jvp\n  _, ans = jvp(fun, (z,), (z_dot,))\n  expected = (grad(u, 0)(x, y) * c +\n              grad(u, 1)(x, y) * d +\n              grad(v, 0)(x, y) * c * 1j+\n              grad(v, 1)(x, y) * d * 1j)\n  print(jnp.allclose(ans, expected))\n\n```\n\n----------------------------------------\n\nTITLE: Combining JIT with Multiple Automatic Differentiation Layers\nDESCRIPTION: Demonstrates composition of JIT compilation with multiple layers of automatic differentiation applied to a mathematical function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ngrad(jit(grad(jit(grad(jnp.tanh)))))(1.0)\n```\n\n----------------------------------------\n\nTITLE: Testing JVP (Forward Mode) with Unimplemented Rule in JAX (Python)\nDESCRIPTION: Tests JAX's jvp (Jacobian-Vector Product) transformation on the custom primitive before a forward-mode/autodiff rule is defined. The code expects NotImplementedError, highlighting the requirement to register a JVP rule for forward-mode autodiff to work. Inputs are two tuples: argument values and tangent vectors. Relies on the presence of 'api.jvp', a primitive, and an exception helper.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwith expectNotImplementedError():\n  api.jvp(square_add_prim, (2., 10.), (1., 1.))\n\n```\n\n----------------------------------------\n\nTITLE: Analyzing Vectorized Gradient Computation\nDESCRIPTION: Uses make_jaxpr to display the intermediate representation of a function that computes per-example gradients in a batch, vectorizing the gradient operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nperex_grads = vmap(grad(loss), in_axes=(None, 0))\nmake_jaxpr(perex_grads)(params, batch)\n```\n\n----------------------------------------\n\nTITLE: Implementing Runge-Kutta 4 Integrator in JAX\nDESCRIPTION: Defines a JIT-compiled function 'rk4' that implements the fourth-order Runge-Kutta method for numerical integration of ODEs. This function is used to solve the Lorentz system.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef rk4(ys, dt, N):\n  @jit\n  def step(i, ys):\n    h = dt\n    t = dt * i\n    k1 = h * f(ys[i-1], t)\n    k2 = h * f(ys[i-1] + k1/2., dt * i + h/2.)\n    k3 = h * f(ys[i-1] + k2/2., t + h/2.)\n    k4 = h * f(ys[i-1] + k3, t + h)\n    \n    ysi = ys[i-1] + 1./6 * (k1 + 2 * k2 + 2 * k3 + k4)\n    return ops.index_update(ys, ops.index[i], ysi)\n  return lax.fori_loop(1, N, step, ys)\n```\n\n----------------------------------------\n\nTITLE: Basic Runtime Error Checking with jax.experimental.checkify\nDESCRIPTION: Shows how to add runtime error checking to JAX code using checkify.checkify transformation and checkify.check function. This allows for jit-compatible error checking that would normally be challenging in compiled JAX code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n  checkify.check(i >= 0, \"index needs to be non-negative!\")\n  y = x[i]\n  z = jnp.sin(y)\n  return z\n\njittable_f = checkify.checkify(f)\n\nerr, z = jax.jit(jittable_f)(jnp.ones((5,)), -1)\nprint(err.get())\n# >> index needs to be non-negative! (check failed at <...>:6 (f))\n```\n\n----------------------------------------\n\nTITLE: Implementing Primitive Transposition in Python\nDESCRIPTION: Conceptual example of how primitive transposition works in JAX for linear primitives, showing how cotangents are calculated for arguments based on the output cotangent.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\np_transpose(out_ct, x, _, _) = (None, out_ct*cy, out_ct*cz)\n```\n\n----------------------------------------\n\nTITLE: Using jax.debug.breakpoint for Interactive Debugging\nDESCRIPTION: Demonstrates how to use jax.debug.breakpoint to pause execution and interactively debug a JAX program, similar to using pdb in Python.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  y, z = jnp.sin(x), jnp.cos(x)\n  jax.debug.breakpoint()\n  return y * z\nf(2.) # ==> Pauses during execution\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Batched Log-Joint Function in JAX\nDESCRIPTION: Defines a non-batched log-joint probability function for Bayesian inference using JAX operations. Calculates the log probability of the model parameters and data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef log_joint(beta):\n    result = 0.\n    # Note that no `axis` parameter is provided to `jnp.sum`.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=1.))\n    result = result + jnp.sum(-jnp.log(1 + jnp.exp(-(2*y-1) * jnp.dot(all_x, beta))))\n    return result\n```\n\n----------------------------------------\n\nTITLE: Performing Parallelized Elementwise Operations with Sharded Arrays in JAX\nDESCRIPTION: This snippet demonstrates how to shard an array across devices and apply an elementwise operation (sine function) that preserves the input sharding pattern. The visualization shows that both input and output arrays maintain the same sharding strategy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nx = jax.device_put(x, NamedSharding(mesh, P('a', 'b')))\nprint('input sharding:')\njax.debug.visualize_array_sharding(x)\n\ny = jnp.sin(x)\nprint('output sharding:')\njax.debug.visualize_array_sharding(y)\n```\n\n----------------------------------------\n\nTITLE: Initializing Neural Network Model Parameters\nDESCRIPTION: Functions to initialize model layers and parameters with random values. Includes initialization of weights, biases, and test data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef init_layer(key, n_in, n_out):\n  k1, k2 = jax.random.split(key)\n  W = jax.random.normal(k1, (n_in, n_out)) / jnp.sqrt(n_in)\n  b = jax.random.normal(k2, (n_out,))\n  return W, b\n\ndef init_model(key, layer_sizes, batch_size):\n  key, *keys = jax.random.split(key, len(layer_sizes))\n  params = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n\n  key, *keys = jax.random.split(key, 3)\n  inputs = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))\n  targets = jax.random.normal(keys[1], (batch_size, layer_sizes[-1]))\n\n  return params, (inputs, targets)\n\nlayer_sizes = [784, 8192, 8192, 8192, 10]\nbatch_size = 8192\n\nparams, batch = init_model(jax.random.key(0), layer_sizes, batch_size)\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Prediction Function\nDESCRIPTION: Defines the forward pass of the neural network for a single example, using ReLU activation and a softmax output layer. This function will later be vectorized to handle batches.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.scipy.special import logsumexp\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  # per-example predictions\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n\n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits - logsumexp(logits)\n```\n\n----------------------------------------\n\nTITLE: Enabling jax.Array via JAX Configuration in Python\nDESCRIPTION: This snippet demonstrates how to programmatically enable the `jax.Array` feature within a Python script using `jax.config.update`. Setting `jax_array` to `True` makes `jax.Array` the default array type returned by JAX functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.config.update('jax_array', True)\n```\n\n----------------------------------------\n\nTITLE: Defining Example JAX Function with JIT\nDESCRIPTION: Demonstrates creating and JIT-compiling a simple neural network layer function that performs matrix multiplication and applies tanh activation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx = random.normal(random.key(0), (5000, 5000))\ndef f(w, b, x):\n  return jnp.tanh(jnp.dot(x, w) + b)\nfast_f = jit(f)\n```\n\n----------------------------------------\n\nTITLE: Validating JIT-Compiled Vectorization with JAX\nDESCRIPTION: Example of applying JAX's JIT compilation to vectorized mapping of a custom primitive, showing that JIT works with custom primitive batching transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nassert np.allclose(api.jit(api.vmap(square_add_prim, in_axes=0, out_axes=0))\n                    (np.array([2., 3.]),\n                     np.array([10., 20.])),\n                    [14., 29.])\n```\n\n----------------------------------------\n\nTITLE: Implementing bfloat16-Supported Tiled Matrix Multiplication Kernel with JAX/Pallas - Python\nDESCRIPTION: Defines a JAX/Pallas custom kernel and wrapper for tiled matrix multiplication supporting bfloat16 input and float32 accumulation with final downcast, intending efficient compute on TPU via VMEM usage. Dependencies: jax, jax.numpy (jnp), pl (Pallas), pltpu, functools. Parameters include input matrices x, y, block sizes bm/bk/bn, and kernel tiling. Outputs a matmul equivalent to x@y, supporting different datatypes and scratch buffers in VMEM. Assumes proper environment with Pallas and TPU support. Limitations: advanced, hardware-bound usage; may not run outside TPU-vm.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_kernel(x_ref, y_ref, z_ref, acc_ref, *, nsteps):\n  @pl.when(pl.program_id(2) == 0)\n  def _():\n    acc_ref[...] = jnp.zeros_like(acc_ref)\n\n  acc_ref[...] += jnp.dot(\n      x_ref[...], y_ref[...], preferred_element_type=jnp.float32\n  )\n\n  @pl.when(pl.program_id(2) == nsteps - 1)\n  def _():\n    z_ref[...] = acc_ref[...].astype(z_ref.dtype)\n\n\n@functools.partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])\ndef matmul(\n    x: jax.Array,\n    y: jax.Array,\n    *,\n    bm: int = 128,\n    bk: int = 128,\n    bn: int = 128,\n):\n  m, k = x.shape\n  _, n = y.shape\n  return pl.pallas_call(\n      functools.partial(matmul_kernel, nsteps=k // bk),\n      grid_spec=pltpu.PrefetchScalarGridSpec(\n        num_scalar_prefetch=0,\n        in_specs=[\n            pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)),\n            pl.BlockSpec((bk, bn), lambda i, j, k: (k, j)),\n        ],\n        out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n        scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n        grid=(m // bm, n // bn, k // bk),\n      ),\n      out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\n      compiler_params=pltpu.TPUCompilerParams(\n          dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n  )(x, y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Loss and Update Functions\nDESCRIPTION: Defines utility functions for one-hot encoding, accuracy calculation, loss computation, and parameter updates. The update function is JIT-compiled for efficiency.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef one_hot(x, k, dtype=jnp.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n\ndef accuracy(params, images, targets):\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\n\n@jit\ndef update(params, x, y):\n  grads = grad(loss)(params, x, y)\n  return [(w - step_size * dw, b - step_size * db)\n          for (w, b), (dw, db) in zip(params, grads)]\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom JVP Function in JAX\nDESCRIPTION: Demonstrates defining a custom Jacobian-vector product (JVP) rule for a function. The example shows how to define the primal function and its corresponding JVP rule using f.defjvp().\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# f_jvp :: (a, T a) -> (b, T b)\ndef f_jvp(primals, tangents):\n  x, = primals\n  t, = tangents\n  return f(x), jnp.cos(x) * t\n\nf.defjvp(f_jvp)\n```\n\n----------------------------------------\n\nTITLE: Implementing Manual Parallelism with shard_map in JAX\nDESCRIPTION: This snippet demonstrates how to use shard_map for manual parallelism in JAX. It creates a mesh of 8 devices and maps an elementwise function across shards of data, with in_specs and out_specs determining shard sizes and reassembly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental.shard_map import shard_map\nmesh = jax.make_mesh((8,), ('x',))\n\nf_elementwise_sharded = shard_map(\n    f_elementwise,\n    mesh=mesh,\n    in_specs=P('x'),\n    out_specs=P('x'))\n\narr = jnp.arange(32)\nf_elementwise_sharded(arr)\n```\n\n----------------------------------------\n\nTITLE: Implementing Automatic Parallelism with JAX Using Sharded Data\nDESCRIPTION: This code demonstrates automatic parallelism in JAX by creating a device mesh and sharding input data across devices. It shards the input vector across 8 devices while keeping weights fully replicated, allowing matrix multiplication to happen in parallel.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmesh = jax.make_mesh((8,), ('x',))\nx_sharded = jax.device_put(x, jax.NamedSharding(mesh, P('x')))\nweights_sharded = jax.device_put(weights, jax.NamedSharding(mesh, P()))\n\nlayer(x_sharded, weights_sharded, bias)\n```\n\n----------------------------------------\n\nTITLE: Implementing VJP for function composition in JAX\nDESCRIPTION: This snippet demonstrates two approaches to implementing the VJP for a composition of two functions. The first computes all VJPs on the forward pass, while the second uses checkpointing to compute some VJPs on the backward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  y = g(x)\n  z = h(y)\n  return z\n\ndef f_vjp(x):\n  y, g_vjp = jax.vjp(g, x)\n  z, h_vjp = jax.vjp(h, y)\n  def f_bwd(z_bar):\n    y_bar, = h_vjp(z_bar)\n    x_bar, = g_vjp(y_bar)\n    return x_bar\n  return z, f_bwd\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f_vjp_checkpoint(x):\n  y = g(x)\n  z, h_vjp = jax.vjp(h, y)\n  def f_bwd2(z_bar):\n    y_bar, = h_vjp(z_bar)\n    _, g_vjp = jax.vjp(g, x)\n    x_bar, = g_vjp(y_bar)\n    return x_bar\n  return z, f_bwd2\n```\n\n----------------------------------------\n\nTITLE: Using JAX Checkpoint with Custom Dot Product Policy\nDESCRIPTION: Applies a specific checkpoint policy that only saves matrix multiplications without batch dimensions. This demonstrates how to apply custom policies to control which intermediate values are saved versus recomputed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nloss_checkpoint = jax.checkpoint(loss, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\nprint_saved_residuals(loss_checkpoint, params, x, y)\n```\n\n----------------------------------------\n\nTITLE: Registering and Traversing Custom Pytree Nodes - JAX - Python\nDESCRIPTION: Demonstrates how to extend JAX's pytree registry so that a user-defined class (RegisteredSpecial) can be traversed as a pytree. It defines flatten and unflatten functions, and registers the class globally using jax.tree_util.register_pytree_node. After registration, leaves inside RegisteredSpecial instances can be correctly traversed and operated on. The code establishes new traversal rules for complex containers and is effective for advanced customization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.tree_util import register_pytree_node\\n\\nclass RegisteredSpecial(Special):\\n  def __repr__(self):\\n    return \"RegisteredSpecial(x={}, y={})\".format(self.x, self.y)\\n\\ndef special_flatten(v):\\n  \"\"\"Specifies a flattening recipe.\\n\\n  Params:\\n    v: The value of the registered type to flatten.\\n  Returns:\\n    A pair of an iterable with the children to be flattened recursively,\\n    and some opaque auxiliary data to pass back to the unflattening recipe.\\n    The auxiliary data is stored in the treedef for use during unflattening.\\n    The auxiliary data could be used, for example, for dictionary keys.\\n  \"\"\"\\n  children = (v.x, v.y)\\n  aux_data = None\\n  return (children, aux_data)\\n\\ndef special_unflatten(aux_data, children):\\n  \"\"\"Specifies an unflattening recipe.\\n\\n  Params:\\n    aux_data: The opaque data that was specified during flattening of the\\n      current tree definition.\\n    children: The unflattened children\\n\\n  Returns:\\n    A reconstructed object of the registered type, using the specified\\n    children and auxiliary data.\\n  \"\"\"\\n  return RegisteredSpecial(*children)\\n\\n# Global registration\\nregister_pytree_node(\\n    RegisteredSpecial,\\n    special_flatten,    # Instruct JAX what are the children nodes.\\n    special_unflatten   # Instruct JAX how to pack back into a `RegisteredSpecial`.\\n)\n```\n\n----------------------------------------\n\nTITLE: Using `jax.debug.print` for Runtime Value Printing in JAX `jit`\nDESCRIPTION: This snippet demonstrates the use of `jax.debug.print`, a JAX callback, to print the actual runtime value of an intermediate variable `y` within a `jax.jit`-compiled function `f`. Unlike standard `print`, `jax.debug.print` passes the runtime value back to the host for printing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  y = x + 1\n  jax.debug.print(\"intermediate value: {}\", y)\n  return y * 2\n\nresult = f(2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Jaxpr Tracing in Python\nDESCRIPTION: Defines classes and functions for tracing Python code to produce Jaxpr representations. This includes JaxprTracer and JaxprTrace classes, which are used in the tracing process.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nclass JaxprTracer(Tracer):\n  __slots__ = ['aval']\n  aval: ShapedArray\n\n  def __init__(self, trace, aval):\n    self._trace = trace\n    self.aval = aval\n\nclass JaxprTrace(Trace):\n  def new_arg(self, aval: ShapedArray) -> JaxprTracer:\n    aval = raise_to_shaped(aval)\n    tracer = self.builder.new_tracer(self, aval)\n    self.builder.tracer_to_var[id(tracer)] = Var(aval)\n    return tracer\n\n  def get_or_make_const_tracer(self, val: Any) -> JaxprTracer:\n    tracer = self.builder.const_tracers.get(id(val))\n    if tracer is None:\n      tracer = self.builder.new_tracer(self, raise_to_shaped(get_aval(val)))\n      self.builder.add_const(tracer, val)\n    return tracer\n  pure = lift = get_or_make_const_tracer\n\n  def process_primitive(self, primitive, tracers, params):\n    avals_in = [t.aval for t in tracers]\n    avals_out = abstract_eval_rules[primitive](*avals_in, **params)\n    out_tracers = [self.builder.new_tracer(self, a) for a in avals_out]\n    inputs = [self.builder.getvar(t) for t in tracers]\n    outvars = [self.builder.add_var(t) for t in out_tracers]\n    self.builder.add_eqn(JaxprEqn(primitive, inputs, params, outvars))\n    return out_tracers\n\n  @property\n  def builder(self):\n    return self.main.global_data\n```\n\n----------------------------------------\n\nTITLE: Performing Functional Array Updates in JAX\nDESCRIPTION: This snippet shows how to perform array updates in JAX using the functional `.at` property, which is the recommended approach instead of in-place updates.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\njax_array = jnp.zeros((3,3), dtype=jnp.float32)\nupdated_array = jax_array.at[1, :].set(1.0)\nprint(\"updated array:\\n\", updated_array)\n```\n\n----------------------------------------\n\nTITLE: Testing grad with a composite function\nDESCRIPTION: Example using the grad function to compute the derivative of a composite function involving sine and other operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_75\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\nprint(grad(f)(3.))\n```\n\n----------------------------------------\n\nTITLE: Performing 1D Convolution with JAX\nDESCRIPTION: Illustrates how to perform 1D convolution using JAX's lax.conv_general_dilated function. It creates a 1D kernel and 1D data, then applies the convolution operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/convolutions.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# 1D kernel - WIO layout\nkernel = jnp.array([[[1, 0, -1], [-1,  0,  1]],\n                    [[1, 1,  1], [-1, -1, -1]]],\n                    dtype=jnp.float32).transpose([2,1,0])\n# 1D data - NWC layout\ndata = np.zeros((1, 200, 2), dtype=jnp.float32)\nfor i in range(2):\n  for k in range(2):\n      x = 35*i + 30 + 60*k\n      data[0, x:x+30, k] = 1.0\n\nprint(\"in shapes:\", data.shape, kernel.shape)\n\nplt.figure(figsize=(10,5))\nplt.plot(data[0]);\ndn = lax.conv_dimension_numbers(data.shape, kernel.shape,\n                                ('NWC', 'WIO', 'NWC'))\nprint(dn)\n\nout = lax.conv_general_dilated(data,   # lhs = image tensor\n                               kernel, # rhs = conv kernel tensor\n                               (1,),   # window strides\n                               'SAME', # padding mode\n                               (1,),   # lhs/image dilation\n                               (1,),   # rhs/kernel dilation\n                               dn)     # dimension_numbers = lhs, rhs, out dimension permutation\nprint(\"out shape: \", out.shape)\nplt.figure(figsize=(10,5))\nplt.plot(out[0]);\n```\n\n----------------------------------------\n\nTITLE: Implementing VJP for Function Composition in JAX Python\nDESCRIPTION: This snippet defines a composite function `f(x) = h(g(x))` and its corresponding VJP function `f_vjp`. It explicitly uses `jax.vjp` for both `g` and `h`. The VJP for `g` (`g_vjp`) and its residuals are computed during the forward pass of `f_vjp`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  y = g(x)\n  z = h(y)\n  return z\n\ndef f_vjp(x):\n  y, g_vjp = jax.vjp(g, x)\n  z, h_vjp = jax.vjp(h, y)\n  def f_bwd(z_bar):\n    y_bar, = h_vjp(z_bar)\n    x_bar, = g_vjp(y_bar)\n    return x_bar\n  return z, f_bwd\n```\n\n----------------------------------------\n\nTITLE: Naming Intermediate Values for Checkpointing Policies in JAX Python\nDESCRIPTION: This snippet modifies the `predict` function to include `jax.ad_checkpoint.checkpoint_name`. This function acts as an identity function but assigns names ('layer0_output', 'layer1_output', etc.) to intermediate `x` values within the loop. These names can be used by specific checkpointing policies.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef predict(params, x):\n  *Ws, Wlast = params\n  for i, W in enumerate(Ws):\n    x = layer(W, x)\n    x = checkpoint_name(x, name=f'layer{i}_output')\n  x = jnp.dot(Wlast, x)\n  return x\n```\n\n----------------------------------------\n\nTITLE: Using jax.debug.callback for Custom Debugging Logic\nDESCRIPTION: Demonstrates how to use jax.debug.callback to implement custom debugging logic, such as logging, which is compatible with JAX transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\ndef log_value(x):\n  logging.warning(f'Logged value: {x}')\n\n@jax.jit\ndef f(x):\n  jax.debug.callback(log_value, x)\n  return x\n\nf(1.0);\n\nx = jnp.arange(5.0)\njax.vmap(f)(x);\n\njax.grad(f)(1.0);\n```\n\n----------------------------------------\n\nTITLE: Computing Higher-Order Derivatives with `jax.grad` in Python\nDESCRIPTION: This snippet shows how to compute higher-order derivatives by repeatedly applying `jax.grad`. It calculates and prints the second and third derivatives of `jnp.tanh` evaluated at `2.0`. This works because the output of `jax.grad` is itself a differentiable function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(grad(jnp.tanh))(2.0))\nprint(grad(grad(grad(jnp.tanh)))(2.0))\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward-Mode Jacobian in JAX (jacfwd)\nDESCRIPTION: Custom implementation of forward-mode Jacobian computation using Jacobian-vector products (jvp) and vmap for efficient batching. The function computes the transposed Jacobian and then transposes the result.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jacfwd as builtin_jacfwd\n\ndef our_jacfwd(f):\n    def jacfun(x):\n        _jvp = lambda s: jvp(f, (x,), (s,))[1]\n        Jt = vmap(_jvp, in_axes=1)(jnp.eye(len(x)))\n        return jnp.transpose(Jt)\n    return jacfun\n\nassert jnp.allclose(builtin_jacfwd(f)(W), our_jacfwd(f)(W)), 'Incorrect forward-mode Jacobian results!'\n```\n\n----------------------------------------\n\nTITLE: Setting Up PyTorch Data Loading\nDESCRIPTION: Configures PyTorch's DataLoader to work with NumPy arrays, enabling efficient data loading for use with JAX. Includes custom collate function and dataset transformation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom jax.tree_util import tree_map\nfrom torch.utils import data\nfrom torchvision.datasets import MNIST\n\ndef numpy_collate(batch):\n  return tree_map(np.asarray, data.default_collate(batch))\n\nclass NumpyLoader(data.DataLoader):\n  def __init__(self, dataset, batch_size=1,\n                shuffle=False, sampler=None,\n                batch_sampler=None, num_workers=0,\n                pin_memory=False, drop_last=False,\n                timeout=0, worker_init_fn=None):\n    super(self.__class__, self).__init__(dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        sampler=sampler,\n        batch_sampler=batch_sampler,\n        num_workers=num_workers,\n        collate_fn=numpy_collate,\n        pin_memory=pin_memory,\n        drop_last=drop_last,\n        timeout=timeout,\n        worker_init_fn=worker_init_fn)\n\nclass FlattenAndCast(object):\n  def __call__(self, pic):\n    return np.ravel(np.array(pic, dtype=jnp.float32))\n\n# Define our dataset, using torch datasets\nmnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\ntraining_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)\n```\n\n----------------------------------------\n\nTITLE: Using `jax.pure_callback` with `jax.vmap`\nDESCRIPTION: This snippet shows the application of `jax.vmap` to the function `f` which uses `jax.pure_callback`. This works because a `vmap_method` was specified when `jax.pure_callback` was called within `f`, instructing JAX how to handle the vectorization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\njax.vmap(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Layer with shard_map in JAX\nDESCRIPTION: This code implements the neural network layer using shard_map for manual parallelism. It uses jax.lax.psum to perform cross-shard collective operations required for the matrix product, explicitly defining how data is sharded and how results are assembled.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n@jax.jit\n@partial(shard_map, mesh=mesh,\n         in_specs=(P('x'), P('x', None), P(None)),\n         out_specs=P(None))\ndef layer_sharded(x, weights, bias):\n  return jax.nn.sigmoid(jax.lax.psum(x @ weights, 'x') + bias)\n\nlayer_sharded(x, weights, bias)\n```\n\n----------------------------------------\n\nTITLE: 2D Image Smoothing with Gaussian Convolution\nDESCRIPTION: Shows how to use jax.scipy.signal.convolve for 2D image processing, applying Gaussian smoothing to denoise an image.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/convolutions.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy import datasets\nimport jax.scipy as jsp\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 5))\n\nimage = jnp.array(datasets.face().mean(-1))\nax[0].imshow(image, cmap='binary_r')\nax[0].set_title('original')\n\nkey = random.key(1701)\nnoisy_image = image + 50 * random.normal(key, image.shape)\nax[1].imshow(noisy_image, cmap='binary_r')\nax[1].set_title('noisy')\n\nx = jnp.linspace(-3, 3, 7)\nwindow = jsp.stats.norm.pdf(x) * jsp.stats.norm.pdf(x[:, None])\nsmooth_image = jsp.signal.convolve(noisy_image, window, mode='same')\nax[2].imshow(smooth_image, cmap='binary_r')\nax[2].set_title('smoothed')\n```\n\n----------------------------------------\n\nTITLE: Using jax.checkpoint for memory optimization in JAX\nDESCRIPTION: This snippet shows how to use jax.checkpoint to optimize memory usage in a function composition scenario. It demonstrates applying checkpoint to the first stage of the composed function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef f_checkpoint(x):\n  y = jax.checkpoint(g)(x)\n  z = h(y)\n  return z\n```\n\n----------------------------------------\n\nTITLE: Manual Trace Capture from a Running JAX Program (Bash)\nDESCRIPTION: This Bash command invokes the jax.collect_profile module to trigger a manual profile capture from a running script serving profile data on the given port. The '<port>' placeholder specifies the profiler server port, and '<duration_in_ms>' sets the capture duration in milliseconds. Dependencies: an active Python process running a profiler server via jax.profiler.start_server(<port>). Output is a trace file written to a temporary or specified directory. This approach is useful for capturing on-demand profiling data without code modification.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m jax.collect_profile <port> <duration_in_ms>\n```\n\n----------------------------------------\n\nTITLE: Sharding Propagation Through Operations\nDESCRIPTION: Shows how shardings propagate through basic arithmetic operations, both outside and inside a jitted function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\narg0 = reshard(np.arange(4).reshape(4, 1), P(\"X\", None))\narg1 = reshard(np.arange(8).reshape(1, 8), P(None, \"Y\"))\n\nresult = arg0 + arg1\n\nprint(f\"arg0 sharding: {jax.typeof(arg0)}\")\nprint(f\"arg1 sharding: {jax.typeof(arg1)}\")\nprint(f\"result sharding: {jax.typeof(result)}\")\n```\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef add_arrays(x, y):\n  ans = x + y\n  print(f\"x sharding: {jax.typeof(x)}\")\n  print(f\"y sharding: {jax.typeof(y)}\")\n  print(f\"ans sharding: {jax.typeof(ans)}\")\n  return ans\n\nadd_arrays(arg0, arg1)\n```\n\n----------------------------------------\n\nTITLE: Computing gradients with custom_jvp pytree function\nDESCRIPTION: Demonstrates taking the gradient of a function that uses the custom_jvp implementation with pytree structures. Shows how JAX correctly propagates gradients through the custom structures.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(fun)(pt))\n```\n\n----------------------------------------\n\nTITLE: Applying Checkpointing to Neural Network Functions\nDESCRIPTION: Demonstrates how to apply jax.checkpoint to specific sub-functions to control which intermediate values are saved during the forward pass. This reduces memory usage by recomputing certain values during the backward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef f2(W1, W2, W3, x):\n  x = jax.checkpoint(g)(W1, x)\n  x = jax.checkpoint(g)(W2, x)\n  x = jax.checkpoint(g)(W3, x)\n  return x\n\njax.ad_checkpoint.print_saved_residuals(f2, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Core Functions\nDESCRIPTION: Defines the core predict and loss functions for the neural network. The predict function implements forward pass through layers while loss function calculates mean squared error.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.maximum(outputs, 0)\n  return outputs\n\ndef loss(params, batch):\n  inputs, targets = batch\n  predictions = predict(params, inputs)\n  return jnp.mean(jnp.sum((predictions - targets)**2, axis=-1))\n```\n\n----------------------------------------\n\nTITLE: Creating JAX Arrays with NumPy-style Functions\nDESCRIPTION: Demonstrates how to create JAX arrays using familiar NumPy-style functions from jax.numpy, and how to verify the type of the resulting array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/key-concepts.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nisinstance(x, jax.Array)\n```\n\n----------------------------------------\n\nTITLE: JAX Key Splitting and Random Generation\nDESCRIPTION: Demonstrates proper key management in JAX through splitting and generating random numbers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/random-numbers.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor i in range(3):\n  new_key, subkey = random.split(key)\n  del key  # The old key is consumed by split() -- we must never use it again.\n\n  val = random.normal(subkey)\n  del subkey  # The subkey is consumed by normal().\n\n  print(f\"draw {i}: {val}\")\n  key = new_key  # new_key is safe to use in the next iteration.\n```\n\n----------------------------------------\n\nTITLE: Hessian-Vector Products using Reverse-Mode Differentiation\nDESCRIPTION: Implementation of Hessian-vector products using JAX's reverse-mode autodiff. This approach computes the product between the Hessian of a function and a vector without explicitly materializing the full Hessian matrix.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing JIT-Compiled Function with JAX\nDESCRIPTION: Defines a Scaled Exponential Linear Unit (SELU) activation function and applies JIT compilation to it using JAX. The function is then used on a random array, demonstrating XLA compilation in action.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_cpu.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef selu(x, alpha=1.67, lmbda=1.05):\n  return lmbda * jax.numpy.where(x > 0, x, alpha * jax.numpy.exp(x) - alpha)\nx = jax.random.normal(key, (5000,))\nresult = selu(x).block_until_ready()\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipelined Matrix Addition with JAX Pallas on TPU (Python)\nDESCRIPTION: This snippet demonstrates a basic pipelined matrix addition using JAX Pallas for TPU execution. It defines a kernel `add_matrices_pipelined_kernel` that performs element-wise addition on blocks and uses `pl.pallas_call` in `add_matrices_pipelined` to orchestrate the computation across a grid, specifying input/output block shapes and index mapping. The example initializes random matrices and verifies the correctness of the pipelined addition against standard JAX addition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Note: This is a TPU example.\n\ntotal_shape = (4096, 4096)\nblock_shape = (512, 512)\n\ndef add_matrices_pipelined_kernel(x_ref, y_ref, o_ref):\n  o_ref[...] = x_ref[...] + y_ref[...]\n\ndef add_matrices_pipelined(x: jax.Array, y: jax.Array):\n  return pl.pallas_call(\n    add_matrices_pipelined_kernel,\n    grid=tuple(total // block for (total, block) in zip(total_shape, block_shape)),\n    in_specs=[\n      pl.BlockSpec(block_shape, index_map=lambda i, j: (i, j)),\n      pl.BlockSpec(block_shape, index_map=lambda i, j: (i, j))\n    ],\n    out_specs=pl.BlockSpec(block_shape, index_map=lambda i, j: (i, j)),\n    out_shape=jax.ShapeDtypeStruct(total_shape, dtype=jnp.float32),\n  )(x, y)\n\nx = jax.random.uniform(jax.random.key(0), total_shape, dtype=jnp.float32)\ny = jax.random.uniform(jax.random.key(1), total_shape, dtype=jnp.float32)\nresult = add_matrices_pipelined(x, y)\nnp.testing.assert_array_equal(\n    result, x + y\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing RMS Normalization in JAX\nDESCRIPTION: Reference implementation of root-mean-square normalization using native JAX functions. This serves as a comparison point for the FFI implementation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\n\ndef rms_norm_ref(x, eps=1e-5):\n  scale = jnp.sqrt(jnp.mean(jnp.square(x), axis=-1, keepdims=True) + eps)\n  return x / scale\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Custom VJP\nDESCRIPTION: Shows that when computing gradients of a custom VJP function, both the forward and backward passes are called. This demonstrates the complete flow of reverse-mode differentiation with custom rules.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(3.))\n```\n\n----------------------------------------\n\nTITLE: General Convolution with LAX Interface\nDESCRIPTION: Demonstrates the use of JAX's conv_general_dilated function with different padding modes, strides, and dimension layouts.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/convolutions.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\n\n# Define dimension numbers\ndn = lax.conv_dimension_numbers(img.shape, kernel.shape, ('NHWC', 'HWIO', 'NHWC'))\n\n# Example with SAME padding\nout = lax.conv_general_dilated(img,\n                               kernel,\n                               (1,1),  # window strides\n                               'SAME', # padding mode\n                               (1,1),  # lhs/image dilation\n                               (1,1),  # rhs/kernel dilation\n                               dn)\n```\n\n----------------------------------------\n\nTITLE: Calling TensorFlow Functions from JAX\nDESCRIPTION: Demonstrates using call_tf to combine JAX and TensorFlow functions, showing eager mode, JIT compilation, and gradient computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import jax2tf\n\ndef cos_tf(x):\n  return tf.math.cos(x)\n\ndef cos_tf_sin_jax(x):\n  return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))\n\n# Calls `cos_tf` in TF eager mode\nx = np.float32(1.)\ncos_tf_sin_jax(x)\n\n# Compiles `cos_tf` using TF and embeds the XLA computation into the JAX\n# XLA computation (containing `sin`). The XLA compiler may even be able to\n# fuse through JAX-TF computations.\njax.jit(cos_tf_sin_jax)(x)\n\n# Uses TF gradient for `cos_tf` and JAX gradient for `sin`\njax.grad(cos_tf_sin_jax)(x)\n```\n\n----------------------------------------\n\nTITLE: Computing gradients with custom_vjp pytree function\nDESCRIPTION: Demonstrates taking the gradient of a function that uses the custom_vjp implementation with pytree structures. Shows that both custom_jvp and custom_vjp approaches yield the same gradient results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(fun)(pt))\n```\n\n----------------------------------------\n\nTITLE: Using NamedTuple Containers as Pytrees - JAX - Python\nDESCRIPTION: Illustrates that Python NamedTuple subclasses are automatically treated as pytree nodes in JAX. It defines a MyOtherContainer NamedTuple with several fields, creates two instances, and extracts their leaves using jax.tree.leaves. All fields, including non-numeric ones, are considered leaves; the code works out-of-the-box with JAX and the Python typing module.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import NamedTuple, Any\\n\\nclass MyOtherContainer(NamedTuple):\\n  name: str\\n  a: Any\\n  b: Any\\n  c: Any\\n\\n# NamedTuple subclasses are handled as pytree nodes, so\\n# this will work out-of-the-box.\\njax.tree.leaves([\\n    MyOtherContainer('Alice', 1, 2, 3),\\n    MyOtherContainer('Bob', 4, 5, 6)\\n])\n```\n\n----------------------------------------\n\nTITLE: Viewing JAX Expression (jaxpr) in Python\nDESCRIPTION: This code shows how to use jax.make_jaxpr to view the JAX expression (jaxpr) that represents the sequence of operations in a function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import make_jaxpr\n\ndef f(x, y):\n  return jnp.dot(x + 1, y + 1)\n\nmake_jaxpr(f)(x, y)\n```\n\n----------------------------------------\n\nTITLE: Basic Pytree Examples\nDESCRIPTION: Examples of valid pytree structures showing different combinations of containers and leaves.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pytrees.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[1, \"a\", object()]  # 3 leaves\n\n(1, (2, 3), ())  # 3 leaves\n\n[1, {\"k1\": 2, \"k2\": (3, 4)}, 5]  # 5 leaves\n```\n\n----------------------------------------\n\nTITLE: Using shard_map with JAX FFI for Automatic Sharding\nDESCRIPTION: Example of using JAX's shard_map function to automatically partition data across devices when calling foreign functions. This approach requires minimal resharding when input and output shardings match the shard_map specifications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom jax.experimental.shard_map import shard_map\n\n@partial(shard_map, mesh=mesh, in_specs=P(\"x\", None), out_specs=P(\"x\", None))\ndef rms_norm_shmap(x):\n  return rms_norm(x)\n\nnp.testing.assert_allclose(rms_norm_shmap(x_batch_shd), rms_norm_ref(x), rtol=1e-5)\nprint(jax.jit(rms_norm_shmap, out_shardings=batch_shd).lower(x_batch_shd).compile().as_text().strip())\n```\n\n----------------------------------------\n\nTITLE: Tracing Python Functions to Jaxpr with make_jaxpr_v1 (Python)\nDESCRIPTION: Implements make_jaxpr_v1 to trace a standard Python callable into its corresponding Jaxpr and constants by applying the JaxprTrace machinery to the input PyTree structure. It handles flattening of nested argument trees and builds the output Jaxpr by replaying the function with abstract input values. Relies on functools.lru_cache for memoization and utilities (tree_flatten, flatten_fun, JaxprBuilder, new_main, etc). Takes a function and variable number of shaped array inputs; returns Jaxpr, constants, and output PyTreeDef.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import lru_cache\n\n@lru_cache  # ShapedArrays are hashable\ndef make_jaxpr_v1(f, *avals_in):\n  avals_in, in_tree = tree_flatten(avals_in)\n  f, out_tree = flatten_fun(f, in_tree)\n\n  builder = JaxprBuilder()\n  with new_main(JaxprTrace, builder) as main:\n    trace = JaxprTrace(main)\n    tracers_in = [trace.new_arg(aval) for aval in avals_in]\n    outs = f(*tracers_in)\n    tracers_out = [full_raise(trace, out) for out in outs]\n    jaxpr, consts = builder.build(tracers_in, tracers_out)\n  return jaxpr, consts, out_tree()\n```\n\n----------------------------------------\n\nTITLE: Custom tree_unflatten Function to Bypass __init__ in JAX PyTrees (Python)\nDESCRIPTION: Demonstrates how to implement a custom tree_unflatten function that reconstructs a PyTree object without invoking its __init__ method, thereby sidestepping potential validation or array conversion logic inside the constructor. This approach is useful when JAX needs to rebuild the object during transformations. All dependencies are standard Python/JAX; the function expects aux_data (ignored) and children (used to assign fields) as arguments. The output is a MyTree-like object reconstructed directly by setting attributes. Ensure consistency between this function and the class __init__ during future code changes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pytrees.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef tree_unflatten(aux_data, children):\n  del aux_data  # unused in this class\n  obj = object.__new__(MyTree)\n  obj.a = a\n  return obj\n```\n\n----------------------------------------\n\nTITLE: Defining Abstract Evaluation Rule for JAX Primitive in Python\nDESCRIPTION: Defines an abstract evaluation function for a custom JAX primitive, ensuring that shape and dtype constraints are met and returning a JAX ShapedArray. Registers this function as the abstract evaluation rule for the primitive. Depends on JAX's core module. Arguments (xs, ys, zs) are abstractions (not concrete arrays); the output is a new ShapedArray reflecting argument shape/dtype. All arguments must have identical shapes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import core\n\n@trace(\"multiply_add_abstract_eval\")\ndef multiply_add_abstract_eval(xs, ys, zs):\n  \"\"\"Abstract evaluation of the primitive.\n\n  This function does not need to be JAX traceable. It will be invoked with\n  abstractions of the actual arguments\n\n  Args:\n    xs, ys, zs: Abstractions of the arguments.\n\n  Result:\n    a ShapedArray for the result of the primitive.\n  \"\"\"\n  assert xs.shape == ys.shape\n  assert xs.shape == zs.shape\n  return core.ShapedArray(xs.shape, xs.dtype)\n\n# Now, register the abstract evaluation with JAX:\nmultiply_add_p.def_abstract_eval(multiply_add_abstract_eval)\n\n```\n\n----------------------------------------\n\nTITLE: Basic JAX Memory Profile Example\nDESCRIPTION: Example showing how to capture a device memory profile using JAX's profiler. Demonstrates memory allocation through nested function calls and saving the profile to disk.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/device_memory_profiling.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport jax.profiler\n\ndef func1(x):\n  return jnp.tile(x, 10) * 0.5\n\ndef func2(x):\n  y = func1(x)\n  return y, jnp.tile(x, 10) + 1\n\nx = jax.random.normal(jax.random.key(42), (1000, 1000))\ny, z = func2(x)\n\nz.block_until_ready()\n\njax.profiler.save_device_memory_profile(\"memory.prof\")\n```\n\n----------------------------------------\n\nTITLE: Batching Blocked Matrix Multiplication with jax.vmap and Fused Activation in Python\nDESCRIPTION: This snippet demonstrates how to apply JAX's vmap to batch the blocked, activation-fused matrix multiplication using the matmul function defined previously. It requires 'jax', 'numpy', and leverages partial with vmap for batched computation. Inputs are batches of arrays with shape (4, 1024, 1024), and outputs are compared to the batched, activation-fused reference. This pattern scales the block-based computation across multiple matrix pairs efficiently with little code change.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nk1, k2 = jax.random.split(jax.random.key(0))\nx = jax.random.normal(k1, (4, 1024, 1024))\ny = jax.random.normal(k2, (4, 1024, 1024))\nz = jax.vmap(partial(matmul, activation=jax.nn.relu))(x, y)\nnp.testing.assert_allclose(z, jax.nn.relu(jax.vmap(jnp.matmul)(x, y)))\n\n```\n\n----------------------------------------\n\nTITLE: Using a Custom VJP Function in JAX\nDESCRIPTION: Shows how to use a function with a custom VJP rule. The example imports grad from JAX and applies it to the function f, demonstrating how the custom VJP rule affects gradient computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad\n\nprint(f(3.))\nprint(grad(f)(3.))\n```\n\n----------------------------------------\n\nTITLE: Adding Fused Activation to Blocked Matrix Multiplication with Pallas in Python\nDESCRIPTION: This code extends the basic blocked matrix multiplication kernel to support fused activation by passing an activation function as a parameter, showcasing how to implement higher-order kernels with Pallas and JAX. Dependencies are 'jax', 'pl' (Pallas), and support for partial from functools. The matmul_kernel now applies the provided activation to the result, and matmul configures the Pallas call accordingly. This enables efficient computation and activation fusion for performance. Inputs are two (1024, 1024) arrays and an activation function; output is validated against the same fused operation in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_kernel(x_ref, y_ref, z_ref, *, activation):\n  z_ref[...] = activation(x_ref[...] @ y_ref[...])\n\ndef matmul(x: jax.Array, y: jax.Array, *, activation):\n  return pl.pallas_call(\n    partial(matmul_kernel, activation=activation),\n    out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),\n    grid=(2, 2),\n    in_specs=[\n        pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)),\n        pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))\n    ],\n    out_specs=pl.BlockSpec(\n        (x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)\n    ),\n  )(x, y)\nk1, k2 = jax.random.split(jax.random.key(0))\nx = jax.random.normal(k1, (1024, 1024))\ny = jax.random.normal(k2, (1024, 1024))\nz = matmul(x, y, activation=jax.nn.relu)\nnp.testing.assert_allclose(z, jax.nn.relu(x @ y))\n\n```\n\n----------------------------------------\n\nTITLE: Combining Control Flow and Autodiff in User Functions - JAX Python\nDESCRIPTION: Shows how the autodiff system handles user-defined functions containing Python control flow (if/else). The deriv function is applied to a conditional function, and outputs are shown for both branches. This demonstrates autodiff's behavior even with primitive Python control structures, within the available static tracing limitations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\\n  if x > 0.:  # Python control flow\\n    return 2. * x\\n  else:\\n    return x\\n\\nprint(deriv(f)(3.))\\nprint(deriv(f)(-3.))\n```\n\n----------------------------------------\n\nTITLE: Registering Dataclass Containers as Pytrees - JAX - Python\nDESCRIPTION: Shows how to register a dataclass as a JAX pytree node using jax.tree_util.register_dataclass with data_fields and meta_fields. The example sets up a dataclass with fields including both metadata and data, and applies the registration decorator for pytree compatibility. Dependencies include functools, dataclasses, Python 3.7+, and JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\\nimport functools\\n\\n@functools.partial(jax.tree_util.register_dataclass,\\n                   data_fields=['a', 'b', 'c'],\\n                   meta_fields=['name'])\\n@dataclass\\nclass MyDataclassContainer(object):\\n  name: str\\n  a: Any\\n  b: Any\\n  c: Any\n```\n\n----------------------------------------\n\nTITLE: Differentiating Through Cholesky Decomposition of Complex Matrices - JAX - Python\nDESCRIPTION: Demonstrates differentiating the loss of a complex-valued Cholesky decomposition using JAX. Constructs a Hermitian matrix A, computes the Cholesky factorization L, evaluates a nonlinear loss (sum of squared difference from its sinusoid), and applies grad(f, holomorphic=True) to A. Models advanced autodiff use cases in applications like complex-valued optimization or physics. Requires jnp and grad.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nA = jnp.array([[5.,    2.+3j,    5j],\n              [2.-3j,   7.,  1.+7j],\n              [-5j,  1.-7j,    12.]])\n\ndef f(X):\n    L = jnp.linalg.cholesky(X)\n    return jnp.sum((L - jnp.sin(L))**2)\n\ngrad(f, holomorphic=True)(A)\n\n```\n\n----------------------------------------\n\nTITLE: JIT with Derivative Transformation\nDESCRIPTION: Compares regular and JIT-compiled double derivatives of a function, demonstrating how JIT works with automatic differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\ndef deriv(f):\n  return lambda x: jvp(f, (x,), (1.,))[1]\n\nprint(    deriv(deriv(f))(3.))\nprint(jit(deriv(deriv(f)))(3.))\n```\n\n----------------------------------------\n\nTITLE: Defining shmap with all_gather and Unmapped Output in JAX\nDESCRIPTION: This snippet defines a JAX function `f4` using `shmap`. It takes an input `x` sharded along axis 'i', performs an `all_gather` operation across that axis within the `shmap` body, and returns the result as an unmapped output (`out_specs=P()`). This example shows a similar inefficiency pattern with `all_gather` as seen with `psum`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Example 4: all_gather to an unmapped output\nf4 = shmap(lambda x: all_gather(x, 'i'), P('i'), P())\n```\n```\n\n----------------------------------------\n\nTITLE: Using Checkpoint Policies to Control Saved Values\nDESCRIPTION: Demonstrating how to use checkpointing policies to control which values are saved during differentiation without modifying the function definition, specifically saving only dot products without batch dimensions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nf3 = jax.checkpoint(f, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\njax.ad_checkpoint.print_saved_residuals(f3, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JIT Compilation Limitations with Dynamic Shapes\nDESCRIPTION: Illustrates a limitation of JIT compilation: the inability to handle operations that result in arrays with shapes that depend on input values. Shows a function that works in op-by-op mode but fails with JIT compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_negatives(x):\n  return x[x < 0]\n\nx = jnp.array(np.random.randn(10))\nget_negatives(x)\n```\n\nLANGUAGE: python\nCODE:\n```\njit(get_negatives)(x)\n```\n\n----------------------------------------\n\nTITLE: Converting a Basic JAX Function for TensorFlow Eager Execution (Python)\nDESCRIPTION: This snippet demonstrates the fundamental use of `jax2tf.convert`. It defines a simple JAX function `f_jax` using `jnp` operations and then uses `jax2tf.convert` to create a TensorFlow-compatible version `f_tf`. The converted function `f_tf` can be executed eagerly with TensorFlow tensors or NumPy arrays as input. It also shows wrapping the converted function with `tf.function(..., autograph=False)` for potential graph optimization or SavedModel export, explicitly disabling Autograph as recommended.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import jax2tf\nfrom jax import numpy as jnp\n\nimport numpy as np\nimport tensorflow as tf\n\ndef f_jax(x):\n  return jnp.sin(jnp.cos(x))\n\n# jax2tf.convert is a higher-order function that returns a wrapped function with\n# the same signature as your input function but accepting TensorFlow tensors (or\n# variables) as input.\nf_tf = jax2tf.convert(f_jax)\n\n# For example you execute f_tf eagerly with valid TensorFlow inputs:\nf_tf(np.random.random(...))\n\n# Additionally you can use tools like `tf.function` to improve the execution\n# time of your function, or to stage it out to a SavedModel:\nf_tf_graph = tf.function(f_tf, autograph=False)\n```\n\n----------------------------------------\n\nTITLE: Observing Side Effects During JAX Tracing in Python\nDESCRIPTION: Illustrates that standard Python `print()` statements within a JAX-traced function execute during the tracing process but are considered side effects and thus are not included in the final jaxpr. The output shows the printed value is a JAX `Traced` object.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef log2_with_print(x):\n  print(\"printed x:\", x)\n  ln_x = jnp.log(x)\n  ln_2 = jnp.log(2.0)\n  return ln_x / ln_2\n\nprint(jax.make_jaxpr(log2_with_print)(3.))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Device Mesh and Data Sharding in JAX - Python\nDESCRIPTION: This Python snippet shows how to configure a device mesh with JAX for efficient model parallelism, ensuring that devices are assigned to model replicas and sharding the input batch accordingly. It relies on JAX's distributed APIs like `local_devices`, `process_count`, `sharding.Mesh`, and `sharding.NamedSharding`, as well as NumPy for device arrangement. It expects that `num_model_replicas_total` and `per_process_batch` are predefined, and that suitable multi-process hardware is available. Inputs include the number of replicas, process-local batch data, and JAX's APIs; output is a globally-sharded input `Array` suitable for distributed computation. Proper usage requires attention to the placement of devices and processes to prevent incorrect replication, as JAX will not warn about incorrect shard arrangements.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/distributed_data_loading.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# are grouped by process, and then resize so each row is a model replica.\\nmesh_devices = np.array([jax.local_devices(process_idx)\\n                         for process_idx in range(jax.process_count())])\\nmesh_devices = mesh_devices.reshape(num_model_replicas_total, -1)\\n# Double check that each replica's devices are on a single process.\\nfor replica_devices in mesh_devices:\\n  num_processes = len(set(d.process_index for d in replica_devices))\\n  assert num_processes == 1\\nmesh = jax.sharding.Mesh(mesh_devices, [\\\"model_replicas\\\", \\\"data_parallelism\\\"])\\n\\n# Shard the data across model replicas. You don't shard across the\\n# data_parallelism mesh axis, meaning each per-replica shard will be replicated\\n# across that axis.\\nsharding = jax.sharding.NamedSharding(\\n    mesh, jax.sharding.PartitionSpec(\\\"model_replicas\\\"))\\n\\nglobal_batch_array = jax.make_array_from_process_local_data(\\n    sharding, per_process_batch)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Out-of-Place Updates in JAX\nDESCRIPTION: This example illustrates that JAX's array update functions operate out-of-place, returning a new array while leaving the original array unmodified.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(\"original array unchanged:\\n\", jax_array)\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance: NumPy vs JAX\nDESCRIPTION: This snippet compares the performance of matrix multiplication between NumPy (on CPU) and JAX (on GPU/TPU). It uses the %timeit magic command to measure execution times.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nx_cpu = np.array(x)\n%timeit -n 5 -r 2 np.dot(x_cpu, x_cpu)\n\n%timeit -n 5 -r 5 jnp.dot(x, x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Implementing Manual 2D Weight Gathered Matrix Multiplication in JAX\nDESCRIPTION: This function implements a custom manual matrix multiplication with reduce-scatter operations for transformer models. It takes normalized input tensor, weight matrices, and layer information to perform optimized matmul operations with specific scatter dimensions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_2D_wg_manual(xnorm, q_wi, layer):\n  '''Calls a custom manual implementation of matmul_reducescatter'''\n  # [batch, maxlen, embed.X] @ [heads.YZ, embed.X, q_wi_per_head]\n  # -> (matmul)\n  # -> [batch, maxlen, heads.YZ, q_wi_per_head]{x unreduced}\n  # -> (reducescatter over x into X heads, B batches)\n  # -> [batch, maxlen, heads.YZX, q_wi_per_head]\n  with jax.named_scope('q_wi'):\n    xnorm = intermediate_dtype(xnorm)\n    q_wi = matmul_reducescatter(\n        'bte,hed->bthd',\n        xnorm,\n        params.q_wi,\n        scatter_dimension=(0, 2),\n        axis_name='i',\n        layer=layer)\n   return q_wi\n```\n\n----------------------------------------\n\nTITLE: Selecting PRNG Implementation and Uniform Sampling (Python)\nDESCRIPTION: Exhibits how to select a specific PRNG implementation when creating keys (e.g., threefry2x32 or rbg). Demonstrates that the implementation is embedded in the key dtype and impacts the buffer size and output. The sampled random numbers reflect the algorithm in use. Dependencies: JAX. Inputs: integer seed, algorithm string. Outputs: new-style key and sampled array. Shows flexibility possible with pluggable RNGs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> key = jax.random.key(0, impl='threefry2x32')  # this is the default impl\n>>> key\nArray((), dtype=key<fry>) overlaying:\n[0 0]\n>>> jax.random.uniform(key, shape=(3,))\nArray([0.947667  , 0.9785799 , 0.33229148], dtype=float32)\n\n>>> key = jax.random.key(0, impl='rbg')\n>>> key\nArray((), dtype=key<rbg>) overlaying:\n[0 0 0 0]\n>>> jax.random.uniform(key, shape=(3,))\nArray([0.39904642, 0.8805201 , 0.73571277], dtype=float32)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Layer with ShardMap for Parallel Computation in JAX\nDESCRIPTION: This function implements a forward pass through a transformer layer using both manual and automatic sharding strategies. It handles attention mechanisms, MLP computations, and optimizes computation through explicit sharding constraints and manual operation modes where beneficial.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport partitioning.logical_to_physical as l2phys\n\ndef pjit_transformer_layer(\n    hparams: HParams, layer: int, params: weights.Layer, sin: jnp.ndarray,\n    cos: jnp.ndarray, kv_caches: Sequence[attention.KVCache],\n    x: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Forward pass through a single layer, returning output, K, V.\"\"\"\n\n  def my_layer(t, axis=0):\n    \"\"\"Gets the parameters corresponding to a given layer.\"\"\"\n    return lax.dynamic_index_in_dim(t, layer, axis=axis, keepdims=False)\n\n  # 2D: [batch.Z, time, embed.XY]\n  x = _with_sharding_constraint(\n      x, ('residual_batch', 'residual_time', 'residual_embed'))\n  xnorm = _layernorm(x)\n  # 2D: [batch, time, embed.X]\n  xnorm = _with_sharding_constraint(\n      xnorm, ('post_norm_batch', 'time', 'post_norm_embed'))\n  # jump into manual mode where you want to optimise\n  if manual:\n    q_wi = shard_map(matmul_2D_wg_manual, mesh\n                in_specs=(l2phys('post_norm_batch', 'time', 'post_norm_embed'),\n                          l2phys('layers', 'heads', 'embed', 'q_wi_per_head')),\n                out_specs=l2phys('post_norm_batch', 'time', 'heads', 'q_wi_per_head'))(xnorm, q_wi, layer)\n  else:\n    q_wi = jnp.einsum('bte,hed->bthd', xnorm, my_layer(params.q_wi))\n    # 2D: [batch, time, heads.YZX, None]\n    q_wi = _with_sharding_constraint(q_wi,\n                                   ('post_norm_batch', 'time', 'heads', 'qkv'))\n  q = q_wi[:, :, :, :hparams.qkv]\n  q = _rope(sin, cos, q)\n  # unlike in https://arxiv.org/pdf/2002.05202.pdf, PaLM implements\n  # swiGLU with full d_ff dimension, rather than 2/3 scaled\n  wi0 = q_wi[:, :, :, hparams.qkv:hparams.qkv + (hparams.ff // hparams.heads)]\n  wi1 = q_wi[:, :, :, hparams.qkv + (hparams.ff // hparams.heads):]\n  kv = jnp.einsum('bte,ezd->btzd', xnorm, my_layer(params.kv))\n  k = kv[:, :, 0, :hparams.qkv]\n  v = kv[:, :, 0, hparams.qkv:]\n  k = _rope(sin, cos, k)\n\n  y_att = jnp.bfloat16(attention.attend(q, k, v, kv_caches, layer))\n\n  y_mlp = special2.swish2(wi0) * wi1\n  # 2D: [batch, time, heads.YZX, None]\n  y_mlp = _with_sharding_constraint(y_mlp,\n                                    ('post_norm_batch', 'time', 'heads', None))\n\n  y_fused = jnp.concatenate([y_att, y_mlp], axis=-1)\n  # do the second half of the mlp and the self-attn projection in parallel\n  y_out = jnp.einsum('bthd,hde->bte', y_fused, my_layer(params.o_wo))\n  # 2D: [batch.Z, time, embed.XY]\n  y_out = _with_sharding_constraint(\n      y_out, ('residual_batch', 'residual_time', 'residual_embed'))\n  z = y_out + x\n  z = _with_sharding_constraint(\n      z, ('residual_batch', 'residual_time', 'residual_embed'))\n  return z, k, v\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Potential Parallel Execution with JAX JIT\nDESCRIPTION: Defines two JAX functions, `f` and `g`, decorated with `jax.jit` and potentially assigned to different devices using `partial`. Calling them sequentially may result in parallel execution due to JAX's asynchronous dispatch, meaning `g` might execute before `f`. This highlights how JAX optimizes pure functions but deviates from the simple sequential model.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@partial(jax.jit, device=<device 0>)\ndef f():\n  return 2\n\n@partial(jax.jit, device=<device 1>)\ndef g():\n  return 3\nf()\ng()\n```\n\n----------------------------------------\n\nTITLE: Illustrating NumPy In-Place Array Update Syntax (Conceptual)\nDESCRIPTION: This snippet shows the standard NumPy syntax for mutating an array element in-place. This operation is not directly supported in JAX due to array immutability. It serves as a contrast to the functional update mechanism provided by JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.numpy.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nx[i] = y\n```\n\n----------------------------------------\n\nTITLE: Analyzing Default Residual Saving in JAX\nDESCRIPTION: Demonstrates the analysis of which values are saved as residuals during differentiation without checkpointing. This shows JAX's default behavior of computing and storing many intermediate values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint_saved_residuals(loss, params, x, y)\n```\n\n----------------------------------------\n\nTITLE: Enforcing Differentiation Convention with custom_jvp\nDESCRIPTION: Using custom_jvp to enforce a specific differentiation convention at the boundary point where the standard derivative is undefined.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x):\n  return x / (1 + jnp.sqrt(x))\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  ans = f(x)\n  ans_dot = ((jnp.sqrt(x) + 2) / (2 * (jnp.sqrt(x) + 1)**2)) * x_dot\n  return ans, ans_dot\n```\n\n----------------------------------------\n\nTITLE: Implementing Transposed Convolution with Kernel Rotation in JAX\nDESCRIPTION: Demonstrates a more detailed implementation of transposed convolution, including kernel rotation and custom output padding. This is equivalent to TensorFlow's conv2d_transpose operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/convolutions.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# rotate kernel 180deg:\nkernel_rot = jnp.rot90(jnp.rot90(kernel, axes=(0,1)), axes=(0,1))\n# need a custom output padding:\npadding = ((2, 1), (2, 1))\nout = lax.conv_general_dilated(img,     # lhs = image tensor\n                               kernel_rot,  # rhs = conv kernel tensor\n                               (1,1),   # window strides\n                               padding, # padding mode\n                               (2,2),   # lhs/image dilation\n                               (1,1),   # rhs/kernel dilation\n                               dn)      # dimension_numbers = lhs, rhs, out dimension permutation\nprint(\"out shape: \", out.shape, \"<-- transposed_conv\")\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);\n```\n\n----------------------------------------\n\nTITLE: Vectorizing a JIT-Compiled Function with Vmap\nDESCRIPTION: Shows how to apply vectorized mapping (vmap) to a JIT-compiled function to efficiently process multiple inputs in parallel.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nys = vmap(f, (0,))(np.arange(3.))\nprint(ys)\n```\n\n----------------------------------------\n\nTITLE: Implementing JAX Linearize Function with Partial Evaluation\nDESCRIPTION: Implementation of linearize and linearize_flat functions that extract the linear part of a JVP computation using partial evaluation. The functions create a linear function that computes tangent outputs from tangent inputs without repeating linearization work.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndef linearize_flat(f, *primals_in):\n  pvals_in = ([PartialVal.known(x) for x in primals_in] +\n              [PartialVal.unknown(vspace(get_aval(x))) for x in primals_in])\n  def f_jvp(*primals_tangents_in):\n    primals_out, tangents_out = jvp(f, *split_half(primals_tangents_in))\n    return [*primals_out, *tangents_out]\n  jaxpr, pvals_out, consts = partial_eval_flat(f_jvp, pvals_in)\n  primal_pvals, _ = split_half(pvals_out)\n  assert all(pval.is_known for pval in primal_pvals)\n  primals_out = [pval.const for pval in primal_pvals]\n  f_lin = lambda *tangents: eval_jaxpr(jaxpr, [*consts, *tangents])\n  return primals_out, f_lin\n\ndef linearize(f, *primals_in):\n  primals_in_flat, in_tree = tree_flatten(primals_in)\n  f, out_tree = flatten_fun(f, in_tree)\n  primals_out_flat, f_lin_flat = linearize_flat(f, *primals_in_flat)\n  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n\n  def f_lin(*tangents_in):\n    tangents_in_flat, in_tree2 = tree_flatten(tangents_in)\n    if in_tree != in_tree2: raise TypeError\n    tangents_out_flat = f_lin_flat(*tangents_in_flat)\n    return tree_unflatten(out_tree(), tangents_out_flat)\n\n  return primals_out, f_lin\n\ndef vspace(aval: ShapedArray) -> ShapedArray:\n  return raise_to_shaped(aval)  # TODO handle integers?\n```\n\n----------------------------------------\n\nTITLE: Using JIT to Compile and Execute JAX Functions\nDESCRIPTION: Demonstrates how to use JAX's JIT decorator to trace, compile and execute a function using XLA. The function is traced once and cached for subsequent calls.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x, y):\n  print('tracing!')\n  return sin(x) * cos(y)\n```\n\n----------------------------------------\n\nTITLE: Implementing and Visualizing Clipped Gradient Function\nDESCRIPTION: Creating a function that applies gradient clipping to sine and visualizing how it affects both the function and its gradient.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef clip_sin(x):\n  x = clip_gradient(-0.75, 0.75, x)\n  return jnp.sin(x)\n\nplt.plot(clip_sin(t))\nplt.plot(vmap(grad(clip_sin))(t))\n```\n\n----------------------------------------\n\nTITLE: Applying Checkpointing Policy to Save Specific Residuals in JAX Python\nDESCRIPTION: This snippet demonstrates using the `policy` argument of `jax.checkpoint`. It wraps the `loss` function with `jax.checkpoint` and provides the built-in policy `jax.checkpoint_policies.dots_with_no_batch_dims_saveable`. This policy instructs JAX to only save residuals from dot products without batch dimensions, potentially reducing memory usage compared to the default behavior. It then calls `print_saved_residuals` to show the effect.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nloss_checkpoint = jax.checkpoint(loss, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\nprint_saved_residuals(loss_checkpoint, params, x, y)\n```\n\n----------------------------------------\n\nTITLE: Comparing Pallas Implementation with XLA's Built-in Collective Operations\nDESCRIPTION: Implements and compares the custom Pallas implementation of reduce-scatter with JAX's built-in lax.psum_scatter operation. This demonstrates validation of the custom implementation against the standard XLA version.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# Now we compare our result to XLA.\ndef lax_reduce_sum_scatter(x):\n  x = x.reshape(num_devices, outer_block_size[0], outer_block_size[1])\n  return lax.psum_scatter(x, 'x')\n\n\nxla_result = jax.jit(\n    shard_map.shard_map(\n        lax_reduce_sum_scatter,\n        mesh=mesh,\n        in_specs=P(None, 'x'),\n        out_specs=P('x', None),\n    )\n)(input_arr)\n\nprint('Input:', input_arr.shape, input_arr[::4, 0])\nprint('Pallas Result:', pallas_result.shape, pallas_result[::4, 0])\nprint('lax.psum_scatter Result:', xla_result.shape, xla_result[::4, 0])\nprint(\n    'Difference |Pallas - lax.psum_scatter|:',\n    jnp.max(jnp.abs(pallas_result - xla_result)),\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Computation Cost of Recursive Checkpointing\nDESCRIPTION: Measures and prints the forward and backward computation operations for recursive checkpointing, showing the increased computation cost. Demonstrates the time-space tradeoff of this approach.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nf = recursive_checkpoint([jnp.sin] * 8)\nprint_fwd_bwd(f, 3.)\n```\n\n----------------------------------------\n\nTITLE: Implementing JVP with Flat Inputs and Outputs in Python\nDESCRIPTION: This function implements the core JVP logic assuming flat inputs and outputs. It creates a new JVPTrace, wraps inputs in JVPTracers, applies the function, and extracts primal and tangent outputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef jvp_flat(f, primals, tangents):\n  with new_main(JVPTrace) as main:\n    trace = JVPTrace(main)\n    tracers_in = [JVPTracer(trace, x, t) for x, t in zip(primals, tangents)]\n    outs = f(*tracers_in)\n    tracers_out = [full_raise(trace, out) for out in outs]\n    primals_out, tangents_out = unzip2((t.primal, t.tangent) for t in tracers_out)\n  return primals_out, tangents_out\n```\n\n----------------------------------------\n\nTITLE: Initializing Multi-layer Perceptron Parameters - Numpy - Python\nDESCRIPTION: This snippet defines a function init_mlp_params to initialize MLP parameters as a pytree (list of dicts, each holding weights and biases). It uses numpy for random normal initialization (He initialization) and constructs parameters for all layers. The function accepts a list of layer sizes and outputs a list of dictionaries for each network layer; each dictionary contains weights and biases arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\\n\\ndef init_mlp_params(layer_widths):\\n  params = []\\n  for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\\n    params.append(\\n        dict(weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\\n             biases=np.ones(shape=(n_out,))\\n            )\\n    )\\n  return params\\n\\nparams = init_mlp_params([1, 128, 128, 1])\n```\n\n----------------------------------------\n\nTITLE: Differentiating Through Nested Parallel Computations with JAX pmap and grad in Python\nDESCRIPTION: This example showcases the composability of JAX transformations, specifically differentiating (`grad`) through nested parallel computations (`pmap`). The outer function `f` is parallelized using `pmap`. Inside `f`, another function `g` is defined and also parallelized using `pmap`. The function `f` returns the gradient of the sum of `g`'s output with respect to `g`'s input `w`, evaluated at `x`. The final lines demonstrate applying `f` to data `x` (assumed defined) and also computing the gradient of the sum of `f`'s output with respect to the original input `x`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad\n\n@pmap\ndef f(x):\n  y = jnp.sin(x)\n  @pmap\n  def g(z):\n    return jnp.cos(z) * jnp.tan(y.sum()) * jnp.tanh(x).sum()\n  return grad(lambda w: jnp.sum(g(w)))(x)\n\nprint(f(x))\n# [[ 0.        , -0.7170853 ],\n#  [-3.1085174 , -0.4824318 ],\n#  [10.366636  , 13.135289  ],\n#  [ 0.22163185, -0.52112055]]\n\nprint(grad(lambda x: jnp.sum(f(x)))(x))\n# [[ -3.2369726,  -1.6356447],\n#  [  4.7572474,  11.606951 ],\n#  [-98.524414 ,  42.76499  ],\n#  [ -1.6007166,  -1.2568436]]\n```\n\n----------------------------------------\n\nTITLE: Applying JAX Vmap to a Pallas Call (Illustrative)\nDESCRIPTION: Illustrates applying the `jax.vmap` transformation externally to a `pallas_call` function. The default behavior augments the kernel with an extra grid dimension for the batch and transforms `BlockSpec`s accordingly. Customization is possible via `jax.custom_vmap`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nvmap(pallas_call)\n```\n\n----------------------------------------\n\nTITLE: Parallel Matrix Operations\nDESCRIPTION: Creates random matrices in parallel and performs batch matrix multiplication across devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import random\n\nkeys = random.split(random.key(0), 8)\nmats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)\nresult = pmap(lambda x: jnp.dot(x, x.T))(mats)\nprint(pmap(jnp.mean)(result))\n```\n\n----------------------------------------\n\nTITLE: Implementing Pure Data Parallelism with tf.data in JAX\nDESCRIPTION: This code demonstrates how to set up data parallelism using tf.data with JAX arrays. It first creates a Dataset sharded across processes, then converts per-process batches into a global JAX array with appropriate sharding for data parallelism.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/distributed_data_loading.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport tensorflow as tf\nimport numpy as np\n\n################################################################################\n# Step 1: setup the Dataset for pure data parallelism (do once)\n################################################################################\n# Fake example data (replace with your Dataset)\nds = tf.data.Dataset.from_tensor_slices(\n    [np.ones((16, 3)) * i for i in range(100)])\n\nds = ds.shard(num_shards=jax.process_count(), index=jax.process_index())\n\n################################################################################\n# Step 2: create a jax.Array of per-replica batches from the per-process batch\n# produced from the Dataset (repeat every step). This can be used with batches\n# produced by different data loaders as well!\n################################################################################\n# Grab just the first batch from the Dataset for this example\nper_process_batch = ds.as_numpy_iterator().next()\n\nmesh = jax.make_mesh((jax.device_count(),), ('batch',))\nsharding = jax.NamedSharding(mesh, jax.sharding.PartitionSpec('batch'))\nglobal_batch_array = jax.make_array_from_process_local_data(\n    sharding, per_process_batch)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Higher-Order Derivatives at a Point in Python\nDESCRIPTION: This snippet evaluates the first, second, third, and fourth derivative functions (`dfdx`, `d2fdx`, `d3fdx`, `d4fdx`, derived in previous snippets) of the polynomial function `f` at the specific input value `x=1.0`. It prints the results, demonstrating the computation of derivative values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-differentiation.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(dfdx(1.))\nprint(d2fdx(1.))\nprint(d3fdx(1.))\nprint(d4fdx(1.))\n```\n\n----------------------------------------\n\nTITLE: Applying `jax.jit` to a Function Using `jax.pure_callback`\nDESCRIPTION: This snippet demonstrates that a JAX function `f` containing `jax.pure_callback` can be successfully compiled using `jax.jit`. The JIT compilation optimizes the function while preserving the host callback execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\njax.jit(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Generating JAX vs NumPy Type Promotion Comparison Table using Python\nDESCRIPTION: This Python script generates an HTML table comparing the type promotion results between JAX (`jnp.promote_types`) and NumPy (`np.promote_types`). It iterates through pairs of specified NumPy and JAX types, determines the promoted type using JAX's internal `_lattice_result_type` function, and checks for differences compared to standard NumPy promotion. The resulting HTML table string highlights cells where JAX's promotion behavior diverges from NumPy's using the 'd' CSS class.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax._src import dtypes\n\ntypes = [np.bool_, np.uint8, np.uint16, np.uint32, np.uint64,\n          np.int8, np.int16, np.int32, np.int64,\n          jnp.bfloat16, np.float16, np.float32, np.float64,\n          np.complex64, np.complex128, int, float, complex]\n\ndef name(d):\n  if d == jnp.bfloat16:\n    return \"bf\"\n  itemsize = \"*\" if d in {int, float, complex} else np.dtype(d).itemsize\n  return f\"{np.dtype(d).kind}{itemsize}\"\n\nout = \"<tr><th></th>\"\nfor t in types:\n  out += \"<th>{}</th>\".format(name(t))\nout += \"</tr>\\n\"\n\nfor t1 in types:\n  out += \"<tr><td>{}</td>\".format(name(t1))\n  for t2 in types:\n    t, weak_type = dtypes._lattice_result_type(t1, t2)\n    if weak_type:\n      t = type(t.type(0).item())\n    different = jnp.bfloat16 in (t1, t2) or jnp.promote_types(t1, t2) is not np.promote_types(t1, t2)\n    out += \"<td{}>{}</td>\".format(\" class=\\\"d\\\"\" if different else \"\", name(t))\n  out += \"</tr>\\n\"\n\nprint(out)\n```\n\n----------------------------------------\n\nTITLE: Batching with Non-batched Arguments in JAX\nDESCRIPTION: Shows how to use vmap when only some arguments are batched, by setting in_axes to None for non-batched arguments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-vectorization.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbatch_convolve_v3 = jax.vmap(convolve, in_axes=[0, None])\n\nbatch_convolve_v3(xs, w)\n```\n\n----------------------------------------\n\nTITLE: Querying JAX Array Device Placement - Python\nDESCRIPTION: This snippet demonstrates how to check the device placement of a JAX array using the `devices()` method. This feature is essential to confirm on which hardware (CPU, GPU, or TPU) the array is located. The only dependency is `jax.numpy` as `jnp`. The input is a JAX array, the output is the set of devices it resides on. This operation may behave differently when executed in environments without GPU/TPU; outputs are typically informational.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import numpy as jnp\nprint(jnp.ones(3).devices())  # doctest: +SKIP\n```\n\n----------------------------------------\n\nTITLE: Basic Pytree Flattening Example\nDESCRIPTION: Demonstration of flattening and unflattening a pytree structure using tree_flatten and tree_unflatten.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pytrees.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.tree_util import tree_flatten, tree_unflatten\nimport jax.numpy as jnp\n\n# The structured value to be transformed\nvalue_structured = [1., (2., 3.)]\n\n# The leaves in value_flat correspond to the `*` markers in value_tree\nvalue_flat, value_tree = tree_flatten(value_structured)\nprint(f\"{value_flat=}\\n{value_tree=}\")\n\n# Transform the flat value list using an element-wise numeric transformer\ntransformed_flat = list(map(lambda v: v * 2., value_flat))\nprint(f\"{transformed_flat=}\")\n\n# Reconstruct the structured output, using the original\ntransformed_structured = tree_unflatten(value_tree, transformed_flat)\nprint(f\"{transformed_structured=}\")\n```\n\n----------------------------------------\n\nTITLE: Applying vmap and jit to Custom JVP Functions\nDESCRIPTION: Demonstrates using vmap and jit transformations with a custom JVP function. The example shows that these transformations work correctly with custom differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nprint(vmap(f)(jnp.arange(3.)))\nprint(jit(f)(3.))\n```\n\n----------------------------------------\n\nTITLE: Using lax.while_loop for iterative operations\nDESCRIPTION: Demonstrates the use of lax.while_loop for creating JIT-compilable and forward-mode differentiable loops.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninit_val = 0\ncond_fun = lambda x: x < 10\nbody_fun = lambda x: x+1\nlax.while_loop(cond_fun, body_fun, init_val)\n# --> array(10, dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Using a Custom JVP Function in JAX\nDESCRIPTION: Shows how to use a function with a custom JVP rule. The example imports jvp from JAX and applies it to the function f, demonstrating how the custom JVP rule affects differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jvp\n\nprint(f(3.))\n\ny, y_dot = jvp(f, (3.,), (1.,))\nprint(y)\nprint(y_dot)\n```\n\n----------------------------------------\n\nTITLE: Checking Available JAX Devices\nDESCRIPTION: Displays the list of available JAX devices (CPUs, GPUs, or TPUs) that can be used for parallel computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\njax.devices()\n```\n\n----------------------------------------\n\nTITLE: Basic Function Tracing Example\nDESCRIPTION: Shows how to trace a simple function into a Jaxpr representation using make_jaxpr.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import wraps\nfrom jax import lax\nfrom jax.extend import core\nfrom jax._src.util import safe_map\n\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\nclosed_jaxpr = jax.make_jaxpr(f)(jnp.ones(5))\nprint(closed_jaxpr.jaxpr)\nprint(closed_jaxpr.literals)\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients and Higher-Order Derivatives for Custom pure_callback Wrapper - Python\nDESCRIPTION: Demonstrates that, with a custom_jvp in place, grad and hessian can be applied to the pure_callback-wrapped external function. Shows successful computation of numerical derivatives for the J1 Bessel function using JAX's autodiff stack. Shows the completed solution for custom autodiff extension.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nj1 = partial(jv, 1)\nprint(jax.grad(j1)(2.0))\n```\n\nLANGUAGE: python\nCODE:\n```\njax.hessian(j1)(2.0)\n```\n\n----------------------------------------\n\nTITLE: Composing Derivative-Pulling Higher-Order Function - JAX Python\nDESCRIPTION: Defines a deriv function that returns the derivative function for a supplied function f using forward-mode autodiff via jvp_v1. The code demonstrates nesting (higher-order differentiation) and compares with known analytic results. Requires all prior forward-mode autodiff machinery. Input is a callable; outputs are repeatedly-nested derivative values evaluated at a point.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef deriv(f):\\n  return lambda x: jvp_v1(f, (x,), (1.,))[1]\\n\\nprint(deriv(sin)(3.))\\nprint(deriv(deriv(sin))(3.))\\nprint(deriv(deriv(deriv(sin)))(3.))\\nprint(deriv(deriv(deriv(deriv(sin))))(3.))\n```\n\n----------------------------------------\n\nTITLE: Validating VJP for Bi-Linear Complex Functions with Randomization - JAX - Python\nDESCRIPTION: Similar to the JVP test, this check(seed) implementation validates vector-Jacobian product (VJP) behavior for a randomized bi-linear function composed of u and v. Constructs random coefficients and a test cotangent vector, computes the VJP, then checks that the result matches the full analytical expression, asserting closeness. Requires JAX's random, grad, vjp, and jnp modules.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef check(seed):\n  key = random.key(seed)\n\n  # random coeffs for u and v\n  key, subkey = random.split(key)\n  a, b, c, d = random.uniform(subkey, (4,))\n\n  def fun(z):\n    x, y = jnp.real(z), jnp.imag(z)\n    return u(x, y) + v(x, y) * 1j\n\n  def u(x, y):\n    return a * x + b * y\n\n  def v(x, y):\n    return c * x + d * y\n\n  # primal point\n  key, subkey = random.split(key)\n  x, y = random.uniform(subkey, (2,))\n  z = x + y * 1j\n\n  # cotangent vector\n  key, subkey = random.split(key)\n  c, d = random.uniform(subkey, (2,))\n  z_bar = jnp.array(c + d * 1j)  # for dtype control\n\n  # check vjp\n  _, fun_vjp = vjp(fun, z)\n  ans, = fun_vjp(z_bar)\n  expected = (grad(u, 0)(x, y) * c +\n              grad(v, 0)(x, y) * (-d) +\n              grad(u, 1)(x, y) * c * (-1j) +\n              grad(v, 1)(x, y) * (-d) * (-1j))\n  assert jnp.allclose(ans, expected, atol=1e-5, rtol=1e-5)\n\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Forward-over-Reverse Hessian-Vector Products\nDESCRIPTION: Demonstrates how to use the hvp function to compute Hessian-vector products. This example validates the results against explicitly computing the full Hessian and then performing the matrix-vector multiplication.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef f(X):\n  return jnp.sum(jnp.tanh(X)**2)\n\nkey, subkey1, subkey2 = random.split(key, 3)\nX = random.normal(subkey1, (30, 40))\nV = random.normal(subkey2, (30, 40))\n\nans1 = hvp(f, (X,), (V,))\nans2 = jnp.tensordot(hessian(f)(X), V, 2)\n\nprint(jnp.allclose(ans1, ans2, 1e-4, 1e-4))\n```\n\n----------------------------------------\n\nTITLE: Implementing All-Gather Collective with JAX Pallas Kernel in Python\nDESCRIPTION: This Python code sets up a sharded device mesh and implements an all-gather collective operation across devices using a custom Pallas kernel, simulating the functionality of lax.all_gather. The approach relies on a ring communication pattern and iterative remote DMA sends/receives coordinated by semaphores, with local HBM buffering and careful management of DMA resource allocation using per-iteration and per-device synchronization. Dependencies are JAX (with sharding and pallas extensions) and the pltpu/pl modules. The kernel function executes iterative data redistribution across devices, and final results are validated by comparing against native lax.all_gather. Expected inputs are 2D arrays appropriately partitioned; the output is a stacked tensor of gathered slices per device. Constraints include required explicit allocation of receive semaphores per device/iteration to avoid DMA contention. The code highlights advanced distributed programming practices and proper handling of device-local versus cross-device memory transfers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npartition = P('x', None)\\nmesh = jax.make_mesh((num_devices,), ('x',))\\nsharding = jax.sharding.NamedSharding(mesh, partition)\\n\\n# Create an input array that shards the first dimension across\\n# all devices.\\ninput_arr = jax.random.uniform(jax.random.key(0), (8 * num_devices, 128))\\ninput_arr = jax.device_put(input_arr, sharding)\\n\\ndef all_gather_kernel(input_ref,\\n                      output_ref,\\n                      local_copy_sem,\\n                      send_sem,\\n                      recv_sems):\\n  outer_step = pl.program_id(0)\\n  my_id = lax.axis_index('x')\\n  right_neighbor = lax.rem(my_id + 1, num_devices)\\n  copy_slot = my_id - outer_step\\n  copy_slot = lax.rem(copy_slot + num_devices, num_devices)\\n\\n  @pl.when(outer_step == 0)\\n  def _():\\n    local_copy_op = pltpu.make_async_copy(\\n      src_ref=input_ref,\\n      dst_ref=output_ref.at[my_id],\\n      sem=local_copy_sem,\\n    )\\n    local_copy_op.start()\\n    local_copy_op.wait()\\n\\n  # Copy to our right neighbor.\\n  # Note that we will also be receiving data from our left neighbor,\\n  # but at `copy_slot-1` rather than `copy_slot`! This makes use of the fact\\n  # that the indices do not need to be symmetric between remote DMAs.\\n  remote_copy_op = pltpu.make_async_remote_copy(\\n      src_ref=output_ref.at[copy_slot],\\n      dst_ref=output_ref.at[copy_slot],\\n      send_sem=send_sem,\\n      recv_sem=recv_sems.at[outer_step],\\n      device_id=(right_neighbor,),\\n      device_id_type=pltpu.DeviceIdType.MESH,\\n  )\\n  remote_copy_op.start()\\n  remote_copy_op.wait()\\n\\nout_shape = jax.ShapeDtypeStruct((num_devices, 8, 128), jnp.float32)\\ngrid_spec = pltpu.PrefetchScalarGridSpec(\\n            num_scalar_prefetch=0,\\n            in_specs=[\\n                # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\\n                pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n            ],\\n            out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n            scratch_shapes=(\\n              # DMA semaphores are allocated in scratch memory.\\n              # We allocated one semaphore for a local HBM-VMEM copy,\\n              # and one for the remote send semaphore.\\n              [pltpu.SemaphoreType.DMA] * 2\\n              # We additionally allocate one receive semaphore per device.\\n              # This is to avoid situations where we have multiple\\n              # DMAs in flight, as we do not want to share a receive\\n              # semaphore between the DMAs.\\n              + [pltpu.SemaphoreType.DMA((num_devices-1,))]\\n\\n            ),\\n            grid=(num_devices-1,)\\n        )\\n\\nall_gather = pl.pallas_call(\\n      all_gather_kernel,\\n      out_shape=out_shape,\\n      grid_spec=grid_spec,\\n  )\\n\\n# Wrap the kernel within a shard_map to call.\\npallas_result = jax.jit(\\n      shard_map.shard_map(\\n          all_gather,\\n          mesh=mesh,\\n          in_specs=partition,\\n          out_specs=partition,\\n          check_rep=False\\n      )\\n)(input_arr)\\n\n```\n\n----------------------------------------\n\nTITLE: JIT-compiling a function with input-dependent loop\nDESCRIPTION: Shows a JIT-compiled function with a loop that depends on the input array's shape, which is allowed in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef g(x):\n  y = 0.\n  for i in range(x.shape[0]):\n    y = y + x[i]\n  return y\n\nprint(g(jnp.array([1., 2., 3.])))\n```\n\n----------------------------------------\n\nTITLE: Defining an Alternative Sine VJP in JAX Python\nDESCRIPTION: This snippet defines `sin_vjp2`, an alternative implementation of the VJP for `jnp.sin(x)`. Unlike `sin_vjp`, this version calculates `jnp.cos(x)` only during the backward pass when the returned VJP function is called. This demonstrates a trade-off, potentially reducing forward pass FLOPs but increasing backward pass FLOPs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef sin_vjp2(x):\n  y = jnp.sin(x)\n  return y, lambda y_bar: jnp.cos(x) * y_bar\n```\n\n----------------------------------------\n\nTITLE: Gradient of Parallel Mapped Function\nDESCRIPTION: Computes the gradient of a parallel mapped function, demonstrating composition of automatic differentiation with distributed computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ngrad(lambda x: jnp.sum(f(x)))(x)\n```\n\n----------------------------------------\n\nTITLE: Using Internal State in Pure Functions with JAX\nDESCRIPTION: This example shows how a function can be considered pure in JAX even if it uses internal state, as long as it doesn't interact with external state.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef pure_uses_internal_state(x):\n  state = dict(even=0, odd=0)\n  for i in range(10):\n    state['even' if i % 2 == 0 else 'odd'] += x\n  return state['even'] + state['odd']\n\nprint(jit(pure_uses_internal_state)(5.))\n```\n\n----------------------------------------\n\nTITLE: Python Assertions with grad in JAX Without checkify\nDESCRIPTION: Shows that for simple transformations like jax.grad, ordinary Python assertions can work for runtime checks as they execute eagerly during Python execution, unlike in jit, pmap, and other transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  assert x > 0., \"must be positive!\"\n  return jnp.log(x)\n\njax.grad(f)(0.)\n# ValueError: \"must be positive!\"\n```\n\n----------------------------------------\n\nTITLE: Setting JAX Configuration at Runtime Using Python\nDESCRIPTION: Shows how to configure JAX behavior directly in Python code using the config.update method. This approach allows changing configuration during program execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/config_options.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.config.update(\"jax_enable_x64\", True)\n```\n\n----------------------------------------\n\nTITLE: Refactoring a Counter for Explicit State Management in JAX (Python)\nDESCRIPTION: Introduces `CounterV2`, a revised counter implementation adhering to JAX's pure function requirement. The state (`n`, represented by `CounterState` alias) is now explicitly passed as an argument to the `count` method, which returns both the computed value and the updated state as a tuple. The `reset` method returns the initial state value. This pattern makes the logic compatible with `jax.jit`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/stateful-computations.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nCounterState = int\n\nclass CounterV2:\n\n  def count(self, n: CounterState) -> tuple[int, CounterState]:\n    # You could just return n+1, but here we separate its role as \n    # the output and as the counter state for didactic purposes.\n    return n+1, n+1\n\n  def reset(self) -> CounterState:\n    return 0\n\ncounter = CounterV2()\nstate = counter.reset()\n\nfor _ in range(3):\n  value, state = counter.count(state)\n  print(value)\n```\n\n----------------------------------------\n\nTITLE: Implementing GPU Pipeline Parallelism with JAX (Python)\nDESCRIPTION: A JAX example demonstrating SPMD-based pipeline parallelism across 4 GPUs configured in a ring (0->1->2->3->0). It defines utility functions for shifting data between devices (`shift_right`, `cycle_back`), conditional logic based on device ID or cycle (`select_on_first_device`, `select_on_last_device`, `select_on_first_cycle`), and the main pipeline loop (`while_body`) using `jax.lax.scan`. The `entry_computation` function orchestrates the setup and execution, initializing buffers, managing dummy data for edges, and running the pipeline. The `main` function sets up the device mesh, initializes weights and input data with appropriate sharding, and calls the main computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Imports and setup\nimport functools\nimport jax\nfrom jax import sharding\nfrom jax.experimental import mesh_utils\nimport jax.numpy as jnp\nimport jax.random\n\nNUM_DEVICES = 4\nNUM_MICROBATCHES = 5\nNUM_CIRC_REPEATS = 2\nCONTRACTING_DIM_SIZE = 4096\nNON_CONTRACTING_DIM_SIZE = 8192\nCOMPUTE_INTENSITY = 32\n\n# Creates a collective permute for the \"forward edge\".\n# 0->1, 1->2, ... (N-2)->(N-1)\ndef shift_right(arr):\n  padding = [[1, 0]] + [[0, 0]] * (arr.ndim - 1)\n  # Use lax.slice to guarantee the gradient is a pad.\n  return jax.lax.slice(jnp.pad(arr, padding), [0] * arr.ndim, arr.shape)\n\n\n# Creates a collective permute for the \"back edge\".\n# (N-1)->0\ndef cycle_back(arr):\n  padding = [[0, NUM_DEVICES - 1]] + [[0, 0]] * (arr.ndim - 1)\n  return jax.lax.slice(\n      jnp.pad(arr, padding),\n      [NUM_DEVICES - 1] + [0] * (arr.ndim - 1),\n      (NUM_DEVICES - 1 + arr.shape[0],) + arr.shape[1:],\n  )\n\n\ndef select_on_first_device(then_value, else_value):\n  assert then_value.shape == else_value.shape\n  is_first_device = jax.lax.broadcasted_iota(\"int32\", then_value.shape, 0) == 0\n  return jnp.where(is_first_device, then_value, else_value)\n\n\ndef select_on_last_device(then_value, else_value):\n  assert then_value.shape == else_value.shape\n  is_last_device = (\n      jax.lax.broadcasted_iota(\"int32\", then_value.shape, 0) == NUM_DEVICES - 1\n  )\n  return jnp.where(is_last_device, then_value, else_value)\n\n\ndef select_on_first_cycle(i, then_value, else_value):\n  assert then_value.shape == else_value.shape\n  is_first_cycle = i < NUM_MICROBATCHES\n  return jnp.where(is_first_cycle, then_value, else_value)\n\n\ndef while_body(carry, i):\n  \"\"\"Body of the pipeline while loop.\"\"\"\n  weights, input_buffer, output_buffer, fwd_edge_data, bwd_edge_data = carry\n\n  # Read input data from input buffer.\n  input_data = jax.lax.dynamic_slice(\n      input_buffer,\n      (0, (i + 0) % NUM_MICROBATCHES, 0, 0),\n      (NUM_DEVICES, 1, CONTRACTING_DIM_SIZE, NON_CONTRACTING_DIM_SIZE),\n  )\n\n  # Collective permute on the \"forward edge\" shifts data to the next stage.\n  fwd_edge_data = shift_right(fwd_edge_data)\n\n  # Select compute argument based on device and pipeline cycle.\n  compute_argument = select_on_first_device(\n      select_on_first_cycle(i, input_data, bwd_edge_data),\n      fwd_edge_data,\n  ).reshape((NUM_DEVICES, CONTRACTING_DIM_SIZE, NON_CONTRACTING_DIM_SIZE))\n\n  # A few matmuls to simulate compute.\n  tmp = compute_argument\n  for _ in range(COMPUTE_INTENSITY):\n    tmp = jax.lax.dot_general(weights, tmp, (((2,), (1,)), ((0,), (0,))))\n  compute_result = tmp.reshape(\n      (NUM_DEVICES, 1, CONTRACTING_DIM_SIZE, NON_CONTRACTING_DIM_SIZE)\n  )\n\n  # Read data from buffer to pass it to the first device of the pipeline on the\n  # \"back edge\".\n  bwd_edge_data = jax.lax.dynamic_slice(\n      output_buffer,\n      (0, (1 + i) % NUM_MICROBATCHES, 0, 0),\n      (NUM_DEVICES, 1, CONTRACTING_DIM_SIZE, NON_CONTRACTING_DIM_SIZE),\n  )\n\n  # Colelctive permute on the \"back edge\" passes data to the first device.\n  bwd_edge_data = cycle_back(bwd_edge_data)\n\n  # Update output buffer. We do this after reading from it to avoid the data\n  # dependency.\n  output_buffer = jax.lax.dynamic_update_slice(\n      output_buffer,\n      compute_result,\n      (0, (2 + i) % NUM_MICROBATCHES, 0, 0),\n  )\n\n  fwd_edge_data = compute_result\n  carry = (\n      weights,\n      input_buffer,\n      output_buffer,\n      fwd_edge_data,\n      bwd_edge_data,\n  )\n  return carry, i\n\n\n@functools.partial(jax.jit, static_argnames=[\"mesh\"])\ndef entry_computation(weights, input_buffer, mesh):\n\n  # Init output buffer.\n  output_buffer = jnp.zeros_like(input_buffer)\n\n  # Init dummy data for forward and backward edge passed through the while loop.\n  dummy_data = jnp.zeros(\n      shape=(NUM_DEVICES, 1, CONTRACTING_DIM_SIZE, NON_CONTRACTING_DIM_SIZE)\n  ).astype(jnp.float32)\n  dummy_data = jax.device_put(\n      dummy_data,\n      sharding.NamedSharding(\n          mesh, sharding.PartitionSpec(\"the_one_and_only_axis\")\n      ),\n  )\n\n  # Start pipeline.\n  carry = weights, input_buffer, output_buffer, dummy_data, dummy_data\n  num_iterations = NUM_CIRC_REPEATS * NUM_MICROBATCHES + NUM_DEVICES - 1\n  carry, _ = jax.lax.scan(while_body, carry, xs=jnp.arange(num_iterations))\n  _, _, output_buffer, _, _ = carry\n\n  return output_buffer\n\n\ndef main(_):\n\n  # Expect constant number of devices.\n  assert NUM_DEVICES == jax.local_device_count()\n\n  # Create mesh.\n  mesh = sharding.Mesh(\n      mesh_utils.create_device_mesh([NUM_DEVICES]),\n      axis_names=[\"the_one_and_only_axis\"],\n  )\n\n  # Init weights.\n  weights = 1.0 / CONTRACTING_DIM_SIZE\n  weights = jax.lax.broadcast_in_dim(\n      weights,\n      shape=(NUM_DEVICES, CONTRACTING_DIM_SIZE, CONTRACTING_DIM_SIZE),\n      broadcast_dimensions=(),\n  )\n  weights = jax.device_put(\n      weights,\n      sharding.NamedSharding(\n          mesh, sharding.PartitionSpec(\"the_one_and_only_axis\")\n      ),\n  )\n\n  # Init random input and replicate it across all devices.\n  random_key = jax.random.key(0)\n  input_buffer = jax.random.uniform(\n      random_key,\n      shape=(\n          NUM_MICROBATCHES,\n          CONTRACTING_DIM_SIZE,\n          NON_CONTRACTING_DIM_SIZE,\n      ),\n  )\n  input_buffer = jax.lax.broadcast_in_dim(\n      input_buffer,\n      shape=(\n          NUM_DEVICES,\n          NUM_MICROBATCHES,\n          CONTRACTING_DIM_SIZE,\n          NON_CONTRACTING_DIM_SIZE,\n      ),\n      broadcast_dimensions=[1, 2, 3],\n  )\n  input_buffer = jax.device_put(\n      input_buffer,\n      sharding.NamedSharding(\n          mesh, sharding.PartitionSpec(\"the_one_and_only_axis\")\n      ),\n  )\n\n  # Run computation.\n  output_buffer = entry_computation(weights, input_buffer, mesh)\n  print(f\"output_buffer = \\n{output_buffer}\")\n```\n\n----------------------------------------\n\nTITLE: Computing Higher-Order Derivatives in JAX\nDESCRIPTION: Shows JAX's ability to compute higher-order derivatives by applying the grad function multiple times to find second and third derivatives.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(grad(f))(-x))\nprint(grad(grad(grad(f)))(-x))\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Sine VJP in JAX Python\nDESCRIPTION: This snippet defines a function `sin_vjp` that computes the Vector-Jacobian Product (VJP) for `jnp.sin(x)`. It calculates both the primal output `y = jnp.sin(x)` and the cosine value `cos_x = jnp.cos(x)` during the forward pass. The returned VJP function uses the pre-computed `cos_x` for the backward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef sin_vjp(x):\n  y = jnp.sin(x)\n  cos_x = jnp.cos(x)\n  return y, lambda y_bar: cos_x * y_bar\n```\n\n----------------------------------------\n\nTITLE: Debugging NaN Values in JAX\nDESCRIPTION: Example showing how NaN values are caught and debugged in JAX using the NaN-checker. Demonstrates basic division operation that produces NaN.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n\njnp.divide(0., 0.)\n```\n\n----------------------------------------\n\nTITLE: Custom Pytree Node Registration Method 1\nDESCRIPTION: Example of registering a custom class as a pytree node using register_pytree_node with separate flatten/unflatten functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pytrees.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.tree_util import register_pytree_node\n\nclass RegisteredSpecial(Special):\n  def __repr__(self):\n    return \"RegisteredSpecial(x={}, y={})\".format(self.x, self.y)\n\ndef special_flatten(v):\n  children = (v.x, v.y)\n  aux_data = None\n  return (children, aux_data)\n\ndef special_unflatten(aux_data, children):\n  return RegisteredSpecial(*children)\n\nregister_pytree_node(\n    RegisteredSpecial,\n    special_flatten,\n    special_unflatten\n)\n```\n\n----------------------------------------\n\nTITLE: Using Collective Communication (psum) with JAX pmap in Python\nDESCRIPTION: This snippet illustrates using collective communication operations within a `pmap`-decorated function. The `normalize` function, parallelized by `pmap`, uses `lax.psum(x, 'i')` to sum the elements of `x` across all devices participating in the parallel computation, identified by the axis name 'i'. This allows normalization based on the global sum across devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom jax import lax\n\n@partial(pmap, axis_name='i')\ndef normalize(x):\n  return x / lax.psum(x, 'i')\n\nprint(normalize(jnp.arange(4.)))\n# prints [0.         0.16666667 0.33333334 0.5       ]\n```\n\n----------------------------------------\n\nTITLE: Calling JAX APIs Concurrently in Python\nDESCRIPTION: Demonstrates the permitted concurrent use of JAX APIs like jit and grad from separate Python threads. This is allowed and safe in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/concurrency.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit, grad\n```\n\n----------------------------------------\n\nTITLE: Using defjvps Convenience Wrapper for Multiple Arguments\nDESCRIPTION: Alternative approach to defining JVP rules using the defjvps convenience wrapper, which allows defining separate JVP rules for each input parameter.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Equivalent alternative using the defjvps convenience wrapper\n\n@custom_jvp\ndef f(x, y):\n  return jnp.sin(x) * y\n\nf.defjvps(lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y,\n          lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot)\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom PyTree Class for JIT Compilation in JAX - Python\nDESCRIPTION: This snippet demonstrates how to create a Python class with state that works correctly with JAX JIT compilation by registering it as a custom PyTree. It defines which attributes are treated as dynamic (JAX arrays) and static (Python booleans), implements both `_tree_flatten` and `_tree_unflatten`, and registers the class as a PyTree node. Required dependencies are `jax`, `jax.numpy`, and the JAX tree utility API. The input is a class with mixed static/dynamic state, and registering allows use as arguments to JITted functions, while still permitting controlled mutation. Changing registered attributes may affect JIT cache behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @jit\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n  def _tree_flatten(self):\n    children = (self.x,)  # arrays / dynamic values\n    aux_data = {'mul': self.mul}  # static values\n    return (children, aux_data)\n\n  @classmethod\n  def _tree_unflatten(cls, aux_data, children):\n    return cls(*children, **aux_data)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import tree_util\ntree_util.register_pytree_node(CustomClass,\n                               CustomClass._tree_flatten,\n                               CustomClass._tree_unflatten)\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Checkpointing for Logarithmic Memory Scaling\nDESCRIPTION: Defines a recursive checkpointing strategy that applies checkpoints in a divide-and-conquer manner. This function enables logarithmic scaling of memory usage with respect to the depth of the function chain.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef recursive_checkpoint(funs):\n  if len(funs) == 1:\n    return funs[0]\n  elif len(funs) == 2:\n    f1, f2 = funs\n    return lambda x: f1(f2(x))\n  else:\n    f1 = recursive_checkpoint(funs[:len(funs)//2])\n    f2 = recursive_checkpoint(funs[len(funs)//2:])\n    return lambda x: f1(jax.checkpoint(f2)(x))\n```\n\n----------------------------------------\n\nTITLE: Exporting, Serializing, and Rehydrating a Basic JAX Function using jax.export (Python)\nDESCRIPTION: This snippet shows the core process of using `jax.export`. It defines a simple JAX function `f`, JIT-compiles it, exports it using `export.export` for a given input shape/dtype, inspects the resulting `Exported` object's attributes (`fun_name`, `in_avals`, `mlir_module`), serializes it to bytes using `.serialize()`, deserializes it back using `export.deserialize()`, and demonstrates how to call the rehydrated function from another JAX computation `callee`. Dependencies include `jax`, `numpy`, and `re`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import re\n>>> import numpy as np\n>>> import jax\n>>> from jax import export\n\n>>> def f(x): return 2 * x * x\n\n\n>>> exported: export.Exported = export.export(jax.jit(f))(\n...    jax.ShapeDtypeStruct((), np.float32))\n\n>>> # You can inspect the Exported object\n>>> exported.fun_name\n'f'\n\n>>> exported.in_avals\n(ShapedArray(float32[]),)\n\n>>> print(re.search(r\".*@main.*\", exported.mlir_module()).group(0))\n  func.func public @main(%arg0: tensor<f32> loc(\"x\")) -> (tensor<f32> {jax.result_info = \"result\"}) {\n\n>>> # And you can serialize the Exported to a bytearray.\n>>> serialized: bytearray = exported.serialize()\n\n>>> # The serialized function can later be rehydrated and called from\n>>> # another JAX computation, possibly in another process.\n>>> rehydrated_exp: export.Exported = export.deserialize(serialized)\n>>> rehydrated_exp.in_avals\n(ShapedArray(float32[]),)\n\n>>> def callee(y):\n...  return 3. * rehydrated_exp.call(y * 4.)\n\n>>> callee(1.)\nArray(96., dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Matrix Multiplication with `shard_map` in JAX (Python)\nDESCRIPTION: Demonstrates a basic matrix multiplication using `shard_map`. It sets up a mesh with named axes ('i', 'j'), defines input arrays `a` and `b`, and applies `shard_map` using `functools.partial`. The `in_specs` define how input arrays are partitioned across the mesh (a is partitioned along 'i' and 'j', b along 'j'), and `out_specs` define the output partitioning. Inside the mapped function `matmul_basic`, it performs a local dot product on the per-device blocks (`a_block`, `b_block`) and then uses `jax.lax.psum` to sum partial results across the 'j' mesh axis to produce the final sharded output block `z_block`. Dependencies include `jax`, `jax.numpy`, `jax.sharding`, `jax.experimental.shard_map`, `functools`, and `numpy`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec as P\nfrom jax.experimental.shard_map import shard_map\n\nmesh = jax.make_mesh((4, 2), ('i', 'j'))\n\na = jnp.arange( 8 * 16.).reshape(8, 16)\nb = jnp.arange(16 * 32.).reshape(16, 32)\n\n@partial(shard_map, mesh=mesh, in_specs=(P('i', 'j'), P('j', None)),\n         out_specs=P('i', None))\ndef matmul_basic(a_block, b_block):\n  # a_block: f32[2, 8]\n  # b_block: f32[8, 32]\n  z_partialsum = jnp.dot(a_block, b_block)\n  z_block = jax.lax.psum(z_partialsum, 'j')\n  return z_block\n\nc = matmul_basic(a, b)  # c: f32[8, 32]\n```\n\n----------------------------------------\n\nTITLE: Higher-Order Differentiation with Custom JVP\nDESCRIPTION: Demonstrates that custom JVP rules apply correctly in higher-order differentiation when the rule calls the original function. The example shows computing second-order derivatives.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ngrad(grad(f))(3.)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with Transpose in JAX\nDESCRIPTION: Demonstrates matrix multiplication of an array with its transpose using JAX's NumPy implementation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\njnp.dot(x, x.T)\n```\n\n----------------------------------------\n\nTITLE: Using defjvps for Per-Argument JVP Rules\nDESCRIPTION: Demonstrates using the defjvps convenience wrapper to define separate JVP rules for each argument. This approach allows defining the tangent computation for each argument individually.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x):\n  return jnp.sin(x)\n\nf.defjvps(lambda t, ans, x: jnp.cos(x) * t)\n```\n\n----------------------------------------\n\nTITLE: Using checkify with vmap/pmap in JAX\nDESCRIPTION: Shows how to use checkify with JAX's vmap and pmap transformations. When mapping a checkified function, the error object contains information about which mapped indices had errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, i):\n  checkify.check(i >= 0, \"index needs to be non-negative!\")\n  return x[i]\n\nchecked_f = checkify.checkify(f, errors=checkify.all_checks)\nerrs, out = jax.vmap(checked_f)(jnp.ones((3, 5)), jnp.array([-1, 2, 100]))\nerrs.throw()\n\"\"\"\nValueError:\n  at mapped index 0: index needs to be non-negative! (check failed at <...>:2 (f))\n  at mapped index 2: out-of-bounds indexing at <...>:3 (f)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JIT Numerical Changes with Logarithm Function in JAX\nDESCRIPTION: This snippet shows how jit can subtly change numeric results due to XLA's optimizations. The example calculates log(sqrt(x)) with and without jit, showing slight differences in the computed value due to floating-point arithmetic and XLA's simplifications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\nimport jax.numpy as jnp\ndef f(x):\n  return jnp.log(jnp.sqrt(x))\nx = jnp.pi\nprint(f(x))\n\nprint(jit(f)(x))\n```\n\n----------------------------------------\n\nTITLE: Analyzing JAX Function Compilation with make_jaxpr\nDESCRIPTION: Imports and uses make_jaxpr to display the intermediate representation of a dot product operation, showing how JAX processes the function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import make_jaxpr\n\nmake_jaxpr(jnp.dot)(jnp.ones(8), jnp.ones(8))\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom JVP Rule for a Function in JAX (Python)\nDESCRIPTION: Demonstrates the basic usage of the modern @jax.custom_jvp decorator to specify a custom JVP rule for a function f. The function f computes np.sin(x), while f_jvp defines the forward (primal) and tangent procedures. After defining f_jvp, it is registered with f.defjvp(f_jvp). Dependencies: JAX and NumPy. Inputs are primals (original values) and tangents; outputs are tuples of (value, tangent). The example covers single-argument functions but supports arbitrary pytrees and keyword arguments as per the API.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/2026-custom-derivatives.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# f :: a -> b\n@jax.custom_jvp\ndef f(x):\n  return np.sin(x)\n\n# f_jvp :: (a, Ta) -> (b, T b)\ndef f_jvp(primals, tangents):\n  x, = primals\n  t, = tangents\n  return f(x), np.cos(x) * t\n\nf.defjvp(f_jvp)\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Vector Addition Kernel in Pallas\nDESCRIPTION: A simple Pallas kernel function that adds two vectors using Ref objects for input and output.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add_vectors_kernel(x_ref, y_ref, o_ref):\n  x, y = x_ref[...], y_ref[...]\n  o_ref[...] = x + y\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Custom JVP\nDESCRIPTION: Shows how to use grad() to compute gradients of a function with a custom JVP rule. The example demonstrates that the custom JVP rule is correctly used for reverse-mode differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(2., 3.))\n```\n\n----------------------------------------\n\nTITLE: Using Automatic Checks with checkify in JAX\nDESCRIPTION: Shows how to enable automatic checks for common error conditions using checkify. The example demonstrates user_checks, index_checks, and float_checks to catch issues like out-of-bounds indexing, negative index values, and NaN generation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nerrors = checkify.user_checks | checkify.index_checks | checkify.float_checks\nchecked_f = checkify.checkify(f, errors=errors)\n\nerr, z = checked_f(jnp.ones((5,)), 100)\nerr.throw()\n# ValueError: out-of-bounds indexing at <..>:7 (f)\n\nerr, z = checked_f(jnp.ones((5,)), -1)\nerr.throw()\n# ValueError: index needs to be non-negative! (check failed at <…>:6 (f))\n\nerr, z = checked_f(jnp.array([jnp.inf, 1]), 0)\nerr.throw()\n# ValueError: nan generated by primitive sin at <...>:8 (f)\n\nerr, z = checked_f(jnp.array([5, 1]), 0)\nerr.throw()  # if no error occurred, throw does nothing!\n```\n\n----------------------------------------\n\nTITLE: Testing Forward-Mode Autodiff (JVP) for JAX Primitive (Python)\nDESCRIPTION: Confirms that after registering a JVP rule, the custom primitive produces correct values and tangents under JAX's jvp function. Takes argument and tangent tuples, returns a tuple of (primal, tangent), checked for correct numeric behavior. Requires the JVP rule to be registered.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Tangent is: xt*y + x*yt + zt = 1.*2. + 2.*1. + 1. = 5.\nassert api.jvp(square_add_prim, (2., 10.), (1., 1.)) == (14., 5.)\n\n```\n\n----------------------------------------\n\nTITLE: Jacobian-Matrix Products with and without vmap\nDESCRIPTION: Implementations of Jacobian-Matrix products with and without JAX's vmap transformation. The vmap version vectorizes the JVP operations for improved performance compared to the loop-based version.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef loop_jmp(f, W, M):\n    # jvp immediately returns the primal and tangent values as a tuple,\n    # so we'll compute and select the tangents in a list comprehension\n    return jnp.vstack([jvp(f, (W,), (mi,))[1] for mi in M])\n\ndef vmap_jmp(f, W, M):\n    _jvp = lambda s: jvp(f, (W,), (s,))[1]\n    return vmap(_jvp)(M)\n\nnum_vecs = 128\nS = random.normal(key, (num_vecs,) + W.shape)\n\nloop_vs = loop_jmp(f, W, M=S)\nprint('Non-vmapped Jacobian-Matrix product')\n%timeit -n10 -r3 loop_jmp(f, W, M=S)\nvmap_vs = vmap_jmp(f, W, M=S)\nprint('\\nVmapped Jacobian-Matrix product')\n%timeit -n10 -r3 vmap_jmp(f, W, M=S)\n\nassert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Jacobian-Matrix products should be identical'\n```\n\n----------------------------------------\n\nTITLE: Executing Multiple VJP Checks with Randomized Seeds - JAX - Python\nDESCRIPTION: Runs the VJP check function for seeds 0, 1, and 2, confirming JAX's correctness of vector-Jacobian products on multiple random bi-linear function instances. All setup must exist (as previously defined); not standalone.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ncheck(0)\ncheck(1)\ncheck(2)\n\n```\n\n----------------------------------------\n\nTITLE: Gradients with Control Flow in Custom JVP\nDESCRIPTION: Shows the effect of control flow in custom JVP rules on gradient computation. The example demonstrates different gradient behavior based on the input condition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(1.))\nprint(grad(f)(-1.))\n```\n\n----------------------------------------\n\nTITLE: Initializing JAX Distributed Computing with Explicit Parameters\nDESCRIPTION: Example showing how to initialize JAX distributed computing on GPU clusters by specifying coordinator address, number of processes and process ID. This setup enables communication between multiple processes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/multi_process.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n\njax.distributed.initialize(coordinator_address=\"192.168.0.1:1234\",\n                           num_processes=2,\n                           process_id=0)\n```\n\n----------------------------------------\n\nTITLE: Defining Complex and Real Functions for Differentiation in JAX - Python\nDESCRIPTION: Defines a complex-valued function f(z) by decomposing z into real and imaginary components, utilizing auxiliary functions u(x, y) and v(x, y) for the real and imaginary parts, respectively. Also defines g(x, y), mapping from R² to R², which is useful for writing Jacobians or conversion. No external dependencies shown, but expects u(x, y) and v(x, y) to exist in scope.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef f(z):\n  x, y = jnp.real(z), jnp.imag(z)\n  return u(x, y) + v(x, y) * 1j\n\ndef g(x, y):\n  return (u(x, y), v(x, y))\n\n```\n\n----------------------------------------\n\nTITLE: Using checkify-of-vmap in JAX\nDESCRIPTION: Demonstrates the behavior of applying checkify to a vmapped function, which produces a single (unmapped) error rather than separate errors for each mapped index, catching only the first error encountered.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@jax.vmap\ndef f(x, i):\n  checkify.check(i >= 0, \"index needs to be non-negative!\")\n  return x[i]\n\nchecked_f = checkify.checkify(f, errors=checkify.all_checks)\nerr, out = checked_f(jnp.ones((3, 5)), jnp.array([-1, 2, 100]))\nerr.throw()\n# ValueError: index needs to be non-negative! (check failed at <...>:2 (f))\n```\n\n----------------------------------------\n\nTITLE: Disabling JIT Compilation for Traditional Debugging\nDESCRIPTION: Shows how to disable JIT compilation in JAX using jax_disable_jit configuration, allowing the use of Python debugging tools like breakpoint() and print statements.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/flags.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.config.update(\"jax_disable_jit\", True)\n\ndef f(x):\n  y = jnp.log(x)\n  if jnp.isnan(y):\n    breakpoint()\n  return y\njax.jit(f)(-2.)  # ==> Enters PDB breakpoint!\n```\n\n----------------------------------------\n\nTITLE: Applying JAX Grad Inside a Pallas Kernel (Illustrative)\nDESCRIPTION: Demonstrates the concept of using JAX transformations like `jax.grad` inside a Pallas kernel. This example shows applying `jax.grad` to `jnp.sin`. This is feasible if the resulting operation (e.g., `jnp.cos`) can be lowered to the target backend (Triton or Mosaic).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\njax.grad(jnp.sin)(...)\n```\n\n----------------------------------------\n\nTITLE: Testing Linear Algebra Operations with JAX SVD\nDESCRIPTION: This snippet tests JAX's linear algebra capabilities by performing Singular Value Decomposition (SVD) on a random matrix. It verifies that the output matrices have the expected dimensions and prints the singular values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_gpu.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nimport jax.random as rand\n\nN = 10\nM = 20\nkey = rand.PRNGKey(1701)\n\nX = rand.normal(key, (N, M))\nu, s, vt = jnp.linalg.svd(X)\nassert u.shape == (N, N)\nassert vt.shape == (M, M)\nprint(s)\n```\n\n----------------------------------------\n\nTITLE: Applying Symbolic Differentiation using Forward-Mode JVP - JAX Python\nDESCRIPTION: Demonstrates symbolic differentiation of basic math functions using the JVP API. Shows both direct computation of derivative values (sin and cos) and application to composite user-defined functions with parameters. Assumes proper configuration of all core autodiff machinery. Input values are numeric; outputs are primal (function value) and tangent (gradient).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nx = 3.0\\ny, sin_deriv_at_3 = jvp_v1(sin, (x,), (1.0,))\\nprint(sin_deriv_at_3)\\nprint(cos(3.0))\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\\n  y = sin(x) * 2.\\n  z = - y + x\\n  return z\\n\\nx, xdot = 3., 1.\\ny, ydot = jvp_v1(f, (x,), (xdot,))\\nprint(y)\\nprint(ydot)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking JIT-Compiled Function in JAX\nDESCRIPTION: Measures the execution time of the JIT-compiled function, showing performance improvement compared to the uncompiled version.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n%timeit g(x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Exporting JAX Functions for Specific and Multi-Platforms - Python\nDESCRIPTION: This snippet illustrates JAX's export functionality to create artifacts for specific platforms (e.g., 'tpu', 'cpu', 'cuda') and multi-platform targets. It covers safety checks that raise errors if the export and execution platforms are incompatible and shows how to disable these checks if needed. Key parameters include the export function, list of platforms, and an optional disabled_checks argument; dependencies are JAX and its export submodule.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax import export\n>>> from jax import lax\n\n>>> # You can specify the export platform, e.g., `tpu`, `cpu`, `cuda`, `rocm`\n>>> # even if the current machine does not have that accelerator.\n>>> exp = export.export(jax.jit(lax.cos), platforms=['tpu'])(1.)\n\n>>> # But you will get an error if you try to compile `exp`\n>>> # on a machine that does not have TPUs.\n>>> exp.call(1.)  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: Function 'cos' was lowered for platforms '('tpu',)' but it is used on '('cpu',)'.\n\n>>> # We can avoid the error if we pass a `DisabledSafetyCheck.platform`\n>>> # parameter to `export`, e.g., because you have reasons to believe\n>>> # that the code lowered will run adequately on the current\n>>> # compilation platform (which is the case for `cos` in this\n>>> # example):\n>>> exp_unsafe = export.export(jax.jit(lax.cos),\n...    platforms=['tpu'],\n...    disabled_checks=[export.DisabledSafetyCheck.platform()])(1.)\n\n>>> exp_unsafe.call(1.)\nArray(0.5403023, dtype=float32, weak_type=True)\n\n# and similarly with multi-platform lowering\n>>> exp_multi = export.export(jax.jit(lax.cos),\n...    platforms=['tpu', 'cpu', 'cuda'])(1.)\n>>> exp_multi.call(1.)\nArray(0.5403023, dtype=float32, weak_type=True)\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting JAX Function Primitives with Python\nDESCRIPTION: Defines a Python function `log2` using `jax.numpy` and demonstrates how `jax.make_jaxpr` converts it into a JAX expression (jaxpr). It highlights that Python side effects (like appending to `global_list`) are executed during tracing but not captured in the resulting jaxpr, as JAX focuses on pure functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\nglobal_list = []\n\ndef log2(x):\n  global_list.append(x)\n  ln_x = jnp.log(x)\n  ln_2 = jnp.log(2.0)\n  return ln_x / ln_2\n\nprint(jax.make_jaxpr(log2)(3.0))\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX GPU Memory Fraction with Environment Variable\nDESCRIPTION: Sets an environment variable to control what percentage of GPU memory JAX preallocates. This allows customizing the preallocation amount from the default 75% to help avoid OOM errors when starting JAX programs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_memory_allocation.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nXLA_PYTHON_CLIENT_MEM_FRACTION=.XX\n```\n\n----------------------------------------\n\nTITLE: Custom Pytree Node Registration Method 2\nDESCRIPTION: Example of registering a custom class as a pytree node using the register_pytree_node_class decorator with class methods.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pytrees.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.tree_util import register_pytree_node_class\n\n@register_pytree_node_class\nclass RegisteredSpecial2(Special):\n  def __repr__(self):\n    return \"RegisteredSpecial2(x={}, y={})\".format(self.x, self.y)\n\n  def tree_flatten(self):\n    children = (self.x, self.y)\n    aux_data = None\n    return (children, aux_data)\n\n  @classmethod\n  def tree_unflatten(cls, aux_data, children):\n    return cls(*children)\n```\n\n----------------------------------------\n\nTITLE: Optimizing ELBO Using Stochastic Gradient Descent in JAX\nDESCRIPTION: Implements stochastic gradient descent to optimize the ELBO for variational inference. Uses JAX's random number generation and JIT compilation for efficiency.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef normal_sample(key, shape):\n    \"\"\"Convenience function for quasi-stateful RNG.\"\"\"\n    new_key, sub_key = random.split(key)\n    return new_key, random.normal(sub_key, shape)\n\nnormal_sample = jax.jit(normal_sample, static_argnums=(1,))\n\nkey = random.key(10003)\n\nbeta_loc = jnp.zeros(num_features, jnp.float32)\nbeta_log_scale = jnp.zeros(num_features, jnp.float32)\n\nstep_size = 0.01\nbatch_size = 128\nepsilon_shape = (batch_size, num_features)\nfor i in range(1000):\n    key, epsilon = normal_sample(key, epsilon_shape)\n    elbo_val, (beta_loc_grad, beta_log_scale_grad) = elbo_val_and_grad(\n        beta_loc, beta_log_scale, epsilon)\n    beta_loc += step_size * beta_loc_grad\n    beta_log_scale += step_size * beta_log_scale_grad\n    if i % 10 == 0:\n        print('{}\t{}'.format(i, elbo_val))\n```\n\n----------------------------------------\n\nTITLE: Testing the Inverse Function with a Simple Example\nDESCRIPTION: Demonstrates how to use the inverse transformation on a composition of exponential and hyperbolic tangent, verifying that the inverse correctly recovers the original input.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\nf_inv = inverse(f)\nassert jnp.allclose(f_inv(f(1.0)), 1.0)\n```\n\n----------------------------------------\n\nTITLE: Strategy 1: Using JIT with Class Methods via External Helper Function\nDESCRIPTION: This pattern shows how to use jit with class methods by creating an external helper function. The class method calls the jitted helper function, passing the necessary class attributes as arguments, avoiding the need for JAX to handle the class instance.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  def calc(self, y):\n    return _calc(self.mul, self.x, y)\n\n@partial(jit, static_argnums=0)\ndef _calc(mul, x, y):\n  if mul:\n    return x * y\n  return y\n```\n\n----------------------------------------\n\nTITLE: Defining Batching Rules for Primitives in Python\nDESCRIPTION: Implements batching rules for various primitives such as binary operations, unary operations, and reduce_sum. These rules are used by the BatchTrace to process primitives during vectorized batching.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nfrom functools import partial\n\ndef binop_batching_rule(op, axis_size, vals_in, dims_in):\n  (x, y), (x_bdim, y_bdim) = vals_in, dims_in\n  if x_bdim != y_bdim:\n    if x_bdim is not_mapped:\n      x = move_batch_axis(axis_size, x_bdim, y_bdim, x)\n      x_bdim = y_bdim\n    else:\n      y = move_batch_axis(axis_size, y_bdim, x_bdim, y)\n  return [op(x, y)], [x_bdim]\nvmap_rules[add_p] = partial(binop_batching_rule, add)\nvmap_rules[mul_p] = partial(binop_batching_rule, mul)\n\ndef vectorized_unop_batching_rule(op, axis_size, vals_in, dims_in):\n  (x,), (x_bdim,) = vals_in, dims_in\n  return [op(x)], [x_bdim]\nvmap_rules[sin_p] = partial(vectorized_unop_batching_rule, sin)\nvmap_rules[cos_p] = partial(vectorized_unop_batching_rule, cos)\nvmap_rules[neg_p] = partial(vectorized_unop_batching_rule, neg)\n\ndef reduce_sum_batching_rule(axis_size, vals_in, dims_in, *, axis):\n  (x,), (x_bdim,) = vals_in, dims_in\n  new_axis = tuple(ax + (x_bdim <= ax) for ax in axis)\n  out_bdim = x_bdim - sum(ax < x_bdim for ax in axis)\n  return [reduce_sum(x, new_axis)], [out_bdim]\nvmap_rules[reduce_sum_p] = reduce_sum_batching_rule\n```\n\n----------------------------------------\n\nTITLE: Composing pmap with Automatic Differentiation\nDESCRIPTION: Demonstrates composing pmap with gradient operations and nested pmap calls, showing how JAX transformations can be flexibly combined.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n@pmap\ndef f(x):\n  y = jnp.sin(x)\n  @pmap\n  def g(z):\n    return jnp.cos(z) * jnp.tan(y.sum()) * jnp.tanh(x).sum()\n  return grad(lambda w: jnp.sum(g(w)))(x)\n\nf(x)\n```\n\n----------------------------------------\n\nTITLE: Testing log1pexp with defjvps Implementation\nDESCRIPTION: Verifying that the defjvps implementation of log1pexp produces the same numerically stable results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(log1pexp)(100.))\nprint(jit(log1pexp)(3.))\nprint(jit(grad(log1pexp))(3.))\nprint(vmap(jit(grad(log1pexp)))(jnp.arange(3.)))\n```\n\n----------------------------------------\n\nTITLE: Setting JAX Compilation Cache Directory with jax.config.update in Python\nDESCRIPTION: Sets the persistent compilation cache directory directly via JAX's configuration API within a Python script. This approach must be invoked before any program compilation to ensure cache writes and reads go to the proper location. Only requires the 'jax' module.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\njax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n```\n\n----------------------------------------\n\nTITLE: Handling Static and Traced Operations in JIT-compiled Function with JAX\nDESCRIPTION: This snippet illustrates the difference between static and traced operations in a JIT-compiled function, showing how to use numpy for static operations and jax.numpy for traced operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n@jit\ndef f(x):\n  return x.reshape((np.prod(x.shape),))\n\nf(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop with PRNG in Python\nDESCRIPTION: Demonstrates a training loop on the host that uses a PRNG for operations like dropout or VAE training. It shows how to split and manage the RNG state across iterations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrng = lax.rng.new_rng()\nfor i in xrange(num_steps):\n  rng, rng_input = lax.rng.split(rng)\n  params = compiled_update(rng_input, params, next(batches))\n```\n\n----------------------------------------\n\nTITLE: JVP Transformation Semantics with call and custom_jvp_call in JAX (Python)\nDESCRIPTION: Provides schematic representations of how the jvp (Jacobian-vector-product) transformation works in JAX with call and custom_jvp_call primitives. When jvp is applied to call(f), it is equivalent to calling jvp(f) inside call, while with custom_jvp_call the f_jvp custom rule is directly used. These relationships convey how JAX's differentiation framework integrates with user-overridden JVPs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/2026-custom-derivatives.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\njvp(call(f)) == call(jvp(f))\n\njvp(custom_jvp_call(f, f_jvp)) == f_jvp\n```\n\n----------------------------------------\n\nTITLE: Defining Batched Log-Joint Function for Variational Inference\nDESCRIPTION: Sets up a JIT-compiled and vmapped log-joint function for use in variational inference. Uses a normal prior and logistic likelihood.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef log_joint(beta):\n    result = 0.\n    # Note that no `axis` parameter is provided to `jnp.sum`.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=10.))\n    result = result + jnp.sum(-jnp.log(1 + jnp.exp(-(2*y-1) * jnp.dot(all_x, beta))))\n    return result\n\nbatched_log_joint = jax.jit(jax.vmap(log_joint))\n```\n\n----------------------------------------\n\nTITLE: Defining and Registering a New JAX Primitive with MLIR Lowering - Python\nDESCRIPTION: This snippet walks through creating a custom JAX primitive (`mul_add`) using the jax.extend API. It includes defining its implementation, abstract evaluation, and providing MLIR lowering for JIT compilation. The code demonstrates key steps: creation of a Primitive, implementation and abstract evaluation registration, MLIR lowering rule registration, and usage in both standard and jitted contexts. Required dependencies are JAX and access to `jax.extend` and its submodules. Key parameters include the primitive arguments (x, y, z) and the context-dependent objects for MLIR lowering. Inputs should be numerical values; outputs are computed results or lowered arrays. The pattern and API usage may change across JAX versions due to the absence of a compatibility contract.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/15856-jex.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.extend import core\\t         # Previously: from jax import core\\nfrom jax.extend.interpreters import mlir        # ... and similarly\\n\\nmul_add_p = core.Primitive('mul_add')\\nmul_add_p.def_impl(lambda x, y, z: x * y + z)\\n\\n@mul_add_p.def_abstract_eval\\ndef mul_add_abstract(x_sa, y_sa, z_sa):\\n  return core.ShapedArray(x_sa.shape, x_sa.dtype)\\n\\ndef mul_add_mlir(ctx, xc, yc, zc):\\n  add = mlir.hlo.AddOp\\n  mul = mlir.hlo.MulOp\\n  return add(mul(xc, yc), zc).results\\n\\nmlir.register_lowering(mul_add_p, mul_add_mlir)\\n\\nimport jax\\nprint(mul_add_p.bind(2, 3, 4))            # -> 10\\nprint(jax.jit(mul_add_p.bind)(2, 3, 4))   # -> Array(10, dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Basic Pytree Node Operations in JAX\nDESCRIPTION: Demonstrates creating and accessing pytree leaves using JAX tree utilities with a custom dataclass container.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\njax.tree.leaves([\n  MyDataclassContainer('apple', 5.3, 1.2, jnp.zeros([4])),\n  MyDataclassContainer('banana', np.array([3, 4]), -1., 0.)\n])\n```\n\n----------------------------------------\n\nTITLE: Examples of Common JAX Runtime Errors checkify Can Catch\nDESCRIPTION: Demonstrates common runtime errors in JAX code that checkify can automatically detect, including out of bounds indexing, NaN generation, and division by zero.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\njnp.arange(3)[5]                # out of bounds\njnp.sin(jnp.inf)                # NaN generated\njnp.ones((5,)) / jnp.arange(5)  # division by zero\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Offload Policy for Matrix Multiplications in JAX\nDESCRIPTION: This snippet demonstrates how to use JAX's custom checkpoint policy to offload matrix multiplication results to CPU memory. It applies the offload_dot_with_no_batch_dims policy to a function performing multiple matrix multiplications and sine operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.ad_checkpoint import checkpoint\n\ndef checkpoint_offload_dot_with_no_batch_dims(self):\n  policy = jax.checkpoint_policies.offload_dot_with_no_batch_dims(\n      \"device\", \"pinned_host\")\n\n  @functools.partial(checkpoint, policy=policy)\n  def f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Nested Parallel Mapping with Multiple Axes\nDESCRIPTION: Demonstrates nested pmap operations with different axis names, enabling collective operations along rows, columns, or both dimensions for 2D distributed computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n@partial(pmap, axis_name='rows')\n@partial(pmap, axis_name='cols')\ndef f(x):\n  row_sum = psum(x, 'rows')\n  col_sum = psum(x, 'cols')\n  total_sum = psum(x, ('rows', 'cols'))\n  return row_sum, col_sum, total_sum\n\nx = jnp.arange(8.).reshape((4, 2))\na, b, c = f(x)\n\nprint(\"input:\\n\", x)\nprint(\"row sum:\\n\", a)\nprint(\"col sum:\\n\", b)\nprint(\"total sum:\\n\", c)\n```\n\n----------------------------------------\n\nTITLE: Inspecting MLP Parameter Shapes with jax.tree.map - JAX - Python\nDESCRIPTION: This example uses jax.tree.map to traverse the MLP parameter pytree (list of dicts, each with numpy arrays) and returns a structure giving the shapes of all leaves. The mapping function extracts the .shape of every weight and bias array, producing a pytree of the same structure indicating the-dimensionality of each parameter. The code assumes params has been defined as in a previous snippet.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\njax.tree.map(lambda x: x.shape, params)\n```\n\n----------------------------------------\n\nTITLE: Compositionality stress test with nested transformations\nDESCRIPTION: Complex test case that combines JIT compilation, JVP, and grad transformations in various nested combinations to verify their consistent behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_77\n\nLANGUAGE: python\nCODE:\n```\n# from core_test.py fun_with_nested_calls_2\ndef foo(x):\n  @jit\n  def bar(y):\n    def baz(w):\n      q = jit(lambda x: y)(x)\n      q = q + jit(lambda: y)()\n      q = q + jit(lambda y: w + y)(y)\n      q = jit(lambda w: jit(sin)(x) * y)(1.0) + q\n      return q\n    p, t = jvp(baz, (x + 1.0,), (y,))\n    return t + (x * p)\n  return bar(x)\n\ndef assert_allclose(*vals):\n  for v1, v2 in zip(vals[:-1], vals[1:]):\n    np.testing.assert_allclose(v1, v2)\n\nans1 = f(3.)\nans2 = jit(f)(3.)\nans3, _ = jvp(f, (3.,), (5.,))\nans4, _ = jvp(jit(f), (3.,), (5.,))\nassert_allclose(ans1, ans2, ans3, ans4)\n\nderiv1 = grad(f)(3.)\nderiv2 = grad(jit(f))(3.)\nderiv3 = jit(grad(jit(f)))(3.)\n_, deriv4 = jvp(f, (3.,), (1.,))\n_, deriv5 = jvp(jit(f), (3.,), (1.,))\nassert_allclose(deriv1, deriv2, deriv3, deriv4, deriv5)\n\nhess1 = grad(grad(f))(3.)\nhess2 = grad(grad(jit(f)))(3.)\nhess3 = grad(jit(grad(f)))(3.)\nhess4 = jit(grad(grad(f)))(3.)\n_, hess5 = jvp(grad(f), (3.,), (1.,))\n_, hess6 = jvp(jit(grad(f)), (3.,), (1.,))\n_, hess7 = jvp(jit(grad(f)), (3.,), (1.,))\nassert_allclose(hess1, hess2, hess3, hess4, hess5, hess6, hess7)\n```\n\n----------------------------------------\n\nTITLE: Defining and Using `jax.pure_callback` with a NumPy Host Function\nDESCRIPTION: This code defines a host-side function `f_host` using NumPy and a JAX function `f` that calls `f_host` via `jax.pure_callback`. `jax.pure_callback` requires the host function, the expected result shape and dtype (`result_shape`), and the input arguments (`x`). It allows executing non-JAX Python code (like NumPy operations) within a JAX computation flow. The `vmap_method='sequential'` argument specifies how this callback should behave under `vmap`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f_host(x):\n  # call a numpy (not jax.numpy) operation:\n  return np.sin(x).astype(x.dtype)\n\ndef f(x):\n  result_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n  return jax.pure_callback(f_host, result_shape, x, vmap_method='sequential')\n\nx = jnp.arange(5.0)\nf(x)\n```\n\n----------------------------------------\n\nTITLE: Error When Using Python Assertions with jit in JAX\nDESCRIPTION: Demonstrates the limitation of using Python assertions with jit, which results in a ConcretizationTypeError because jit stages out computations rather than evaluating them eagerly during Python execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\njax.jit(f)(0.)\n# ConcretizationTypeError: \"Abstract tracer value encountered ...\"\n```\n\n----------------------------------------\n\nTITLE: Conditional Breakpointing with jax.lax.cond\nDESCRIPTION: Shows how to implement conditional breakpoints in JAX using jax.lax.cond, allowing for value-dependent debugging.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef breakpoint_if_nonfinite(x):\n  is_finite = jnp.isfinite(x).all()\n  def true_fn(x):\n    pass\n  def false_fn(x):\n    jax.debug.breakpoint()\n  jax.lax.cond(is_finite, true_fn, false_fn, x)\n\n@jax.jit\ndef f(x, y):\n  z = x / y\n  breakpoint_if_nonfinite(z)\n  return z\n\nf(2., 1.) # ==> No breakpoint\nf(2., 0.) # ==> Pauses during execution\n```\n\n----------------------------------------\n\nTITLE: Implementing VJP for sin function in JAX\nDESCRIPTION: This snippet shows two different implementations of the vector-Jacobian product (VJP) for the sine function. The first computes cos(x) on the forward pass, while the second computes it on the backward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef sin_vjp(x):\n  y = jnp.sin(x)\n  cos_x = jnp.cos(x)\n  return y, lambda y_bar: cos_x * y_bar\n```\n\nLANGUAGE: python\nCODE:\n```\ndef sin_vjp2(x):\n  y = jnp.sin(x)\n  return y, lambda y_bar: jnp.cos(x) * y_bar\n```\n\n----------------------------------------\n\nTITLE: JIT compilation with static_argnames for loops\nDESCRIPTION: Demonstrates using static_argnames to JIT compile a function with a loop that depends on an input value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, n):\n  y = 0.\n  for i in range(n):\n    y = y + x[i]\n  return y\n\nf = jit(f, static_argnames='n')\n\nf(jnp.array([2., 3., 4.]), 2)\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of JIT with Class Methods in JAX\nDESCRIPTION: This example demonstrates the problem that occurs when directly applying jit to a class method. JAX cannot handle the CustomClass self parameter, resulting in a TypeError when the method is called.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import jit\n \nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @jit  # <---- How to do this correctly?\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n```\n\n----------------------------------------\n\nTITLE: Enabling or Disabling JAX Rematerialization Optimization\nDESCRIPTION: Code snippets to enable or disable JAX's automatic rematerialization HLO pass. This can be useful for avoiding poor rematerialization choices by the compiler and allows manual control over compute/memory tradeoffs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_memory_allocation.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\njax.config.update('enable_remat_opt_pass', True)\n```\n\nLANGUAGE: python\nCODE:\n```\njax.config.update('enable_remat_opt_pass', False)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Untiling with shard_map in JAX\nDESCRIPTION: Shows different ways to use shard_map with varying output specifications (out_specs) to control array tiling behavior. Demonstrates how mesh axis specifications affect the output shape.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.array([[3.]])\n\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P('i', 'j'))()\nprint(z)  # prints the same as jnp.tile(x, (4, 2))\n\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P('i', None))()\nprint(z)  # prints the same as jnp.tile(x, (4, 1)), or just jnp.tile(x, (4,))\n\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P(None, None))()\nprint(z)  # prints the same as jnp.tile(x, (1, 1)), or just x\n```\n\n----------------------------------------\n\nTITLE: Performing Matrix Multiplication on GPU with JAX\nDESCRIPTION: This snippet demonstrates matrix multiplication using JAX on GPU. It creates a large random matrix, performs a dot product with its transpose, and calculates the mean value to verify numerical computations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_gpu.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\n\n# matrix multiplication on GPU\nkey = jax.random.PRNGKey(0)\nx = jax.random.normal(key, (3000, 3000))\nresult = jax.numpy.dot(x, x.T).mean()\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Key Path Operations with Pytrees\nDESCRIPTION: Demonstrates working with pytree key paths using tree flatten and map operations with explicit path tracking.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport collections\n\nATuple = collections.namedtuple(\"ATuple\", ('name'))\n\ntree = [1, {'k1': 2, 'k2': (3, 4)}, ATuple('foo')]\nflattened, _ = jax.tree_util.tree_flatten_with_path(tree)\n\nfor key_path, value in flattened:\n  print(f'Value of tree{jax.tree_util.keystr(key_path)}: {value}')\n```\n\n----------------------------------------\n\nTITLE: Conceptualizing Multi-Dimensional Grid Execution with pallas_call in Python\nDESCRIPTION: Illustrates how a multi-dimensional grid, such as `grid=(n, m)`, provided to `jax.experimental.pallas.pallas_call` is conceptually equivalent to nested Python `for` loops. The kernel `some_kernel` is invoked `n * m` times. This pattern extends to any number of dimensions in the grid tuple, enabling complex parallel execution patterns. Within the kernel, `program_id(axis=...)` can be used to identify the current position (e.g., values of `i` and `j`) within the grid.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\npl.pallas_call(some_kernel, grid=(n, m))(...) # Conceptual mapping\n```\n```\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Conceptual equivalent\nfor i in range(n):\n  for j in range(m):\n    some_kernel(...)\n```\n```\n\n----------------------------------------\n\nTITLE: Using JAX Debug Flags for NaN Detection\nDESCRIPTION: Shows how to enable JAX's jax_debug_nans flag to automatically detect NaN values in jit-compiled code. This configuration enables standard Python exception handling when NaNs are encountered.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.config.update(\"jax_debug_nans\", True)\n\ndef f(x, y):\n  return x / y\njax.jit(f)(0., 0.)  # ==> raises FloatingPointError exception!\n```\n\n----------------------------------------\n\nTITLE: Traversing Registered Custom Containers with jax.tree.map - JAX - Python\nDESCRIPTION: After registering a custom pytree class, this code applies jax.tree.map to increment all leaves in a list of RegisteredSpecial objects. It shows that the container's internal fields are traversed and modified as intended. This code depends on the registration snippet previously provided and requires JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\njax.tree.map(lambda x: x + 1,\\n  [\\n   RegisteredSpecial(0, 1),\\n   RegisteredSpecial(2, 4),\\n  ])\n```\n\n----------------------------------------\n\nTITLE: Using custom_jvp with multiple non-differentiable arguments\nDESCRIPTION: Shows how to call the custom_jvp function with multiple non-differentiable arguments (lambda functions). The example composes two functions and applies them to the input value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nprint(app2(lambda x: x ** 3, 3., lambda y: 5 * y))\n```\n\n----------------------------------------\n\nTITLE: Handling Tuples in JAX Conditional Branches (Python)\nDESCRIPTION: This code demonstrates how jax.lax.cond can be used when the operand is a tuple and a branch closes over a constant array, both in Python. It requires JAX and jnp (JAX NumPy), with each branch a lambda function; the false branch includes a constant created with jnp.ones. Inputs are a numeric predicate and a tuple operand. The output depends on selected branch function. Constants in branches can be hoisted as constvars.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef func8(arg1, arg2):  # Where `arg2` is a pair.\n  return lax.cond(arg1 >= 0.,\n                  lambda xtrue: xtrue[0],\n                  lambda xfalse: jnp.array([1]) + xfalse[1],\n                  arg2)\n\nprint(make_jaxpr(func8)(5., (jnp.zeros(1), 2.)))\n```\n\n----------------------------------------\n\nTITLE: Evaluating User-Defined Functions using Primitive Evaluation - JAX Python\nDESCRIPTION: Shows how a user function using high-level primitives (sin, multiplication, negation) and basic control flow can be executed using the previously defined evaluation interpreter. Assumes global registration of primitive handlers and access to requisite primitives. Inputs and outputs follow standard Python types; demonstrates the evaluation logic without transformation or autodiff.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\\n  y = sin(x) * 2.\\n  z = - y + x\\n  return z\\n\\nprint(f(3.0))\n```\n\n----------------------------------------\n\nTITLE: Donating Buffers for Pytree Arguments in JAX (Python)\nDESCRIPTION: Shows that donating a pytree (like a list of arrays) results in donating the buffers of all its leaf elements. When `donate_argnums=0` is used with `jax.jit` for the function `add_ones`, the buffers for both arrays inside the input list `xs` are marked for donation and potentially reused for the output list's elements. Requires `jax`, `numpy`, `typing.List`, and `jax.Array`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom typing import List\nfrom jax import Array\n\ndef add_ones(xs: List[Array]):\n  return [x + 1 for x in xs]\n\nxs = [jax.device_put(np.ones((2, 3))), jax.device_put(np.ones((3, 4)))]\n# Execute `add_ones` with donation of all the buffers for `xs`.\n# The outputs have the same shape and type as the elements of `xs`,\n# so they will share those buffers.\nz = jax.jit(add_ones, donate_argnums=0)(xs)\n```\n\n----------------------------------------\n\nTITLE: Converting Shape-Polymorphic JAX Function to TensorFlow\nDESCRIPTION: Shows how to convert a JAX function to a shape-polymorphic TensorFlow function, allowing it to handle inputs with varying batch sizes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nf_tf = tf.function(jax2tf.convert(f_jax,\n                                  polymorphic_shapes=[\"(b, 28, 28)\"]),\n                   autograph=False)\nf_tf.get_concrete_function(tf.TensorSpec([None, 28, 28], tf.float32))\n```\n\n----------------------------------------\n\nTITLE: Implementing Named Checkpoint Policy with Selective Saving and Offloading in JAX\nDESCRIPTION: This code snippet illustrates the use of JAX's save_and_offload_only_these_names policy. It demonstrates how to selectively save, offload, or recompute named checkpoints within a function that uses jax.lax.scan for repeated computations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.ad_checkpoint import checkpoint, checkpoint_name\nfrom jax._src import test_util as jtu\n\ndef checkpoint_names_saved_offloaded_recomputed(self):\n  mesh = jtu.create_mesh((2,), (\"x\",))\n  shape = (256, 128)\n  np_inp = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n  s = NamedSharding(mesh, P(\"x\"))\n  inp = jax.device_put(np_inp, s)\n\n  policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n      names_which_can_be_saved=[\"y\"], names_which_can_be_offloaded=[\"z\"],\n      offload_src='device', offload_dst='pinned_host')\n\n  @functools.partial(checkpoint, policy=policy)\n  def f(x):\n    def g(ys, _):\n      y, _ = ys\n      y = checkpoint_name(jnp.sin(y), \"y\")\n      z = checkpoint_name(jnp.sin(y), \"z\")\n      z = z.T\n      w = checkpoint_name(jnp.sin(z), \"w\")\n      return (w.T, jnp.sum(w)), None\n    _, scan_out = jax.lax.scan(g, (x, np.array(1, dtype=np.float32)), [np_inp])[0]\n    return scan_out\n```\n\n----------------------------------------\n\nTITLE: Creating Sharded Arrays\nDESCRIPTION: Demonstrates how to create sharded arrays using the reshard function and shows the resulting JAX-level types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nreplicated_array = np.arange(8).reshape(4, 2)\nsharded_array = reshard(replicated_array, P(\"X\", None))\n\nprint(f\"replicated_array type: {jax.typeof(replicated_array)}\")\nprint(f\"sharded_array type: {jax.typeof(sharded_array)}\")\n```\n\n----------------------------------------\n\nTITLE: Semantic Equivalent of jax.debug.print\nDESCRIPTION: Shows a Python function that is semantically equivalent to jax.debug.print, illustrating its basic functionality.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/print_breakpoint.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef debug.print(fmt: str, *args: PyTree[Array], **kwargs: PyTree[Array]) -> None:\n  print(fmt.format(*args, **kwargs))\n```\n\n----------------------------------------\n\nTITLE: Improved Strategy 2: Using static_argnums with Proper Hash Implementation\nDESCRIPTION: This code shows a better approach to using jit with class methods by implementing proper __hash__ and __eq__ methods. This allows the object to be used as a static argument, but requires the object to be treated as immutable to avoid hash inconsistencies.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @partial(jit, static_argnums=0)\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n  def __hash__(self):\n    return hash((self.x, self.mul))\n\n  def __eq__(self, other):\n    return (isinstance(other, CustomClass) and\n            (self.x, self.mul) == (other.x, other.mul))\n```\n\n----------------------------------------\n\nTITLE: Basic Vectorization with vmap\nDESCRIPTION: Demonstrates using vmap to vectorize a simple squaring operation across array elements, applying the function to each element automatically.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nprint(vmap(lambda x: x**2)(jnp.arange(8)))\n```\n\n----------------------------------------\n\nTITLE: Parallel Normalization with Collective Communication\nDESCRIPTION: Demonstrates using collective communication operations (psum) to normalize values across devices, summing all elements across devices and dividing by the total.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom jax.lax import psum\n\n@partial(pmap, axis_name='i')\ndef normalize(x):\n  return x / psum(x, 'i')\n\nprint(normalize(jnp.arange(8.)))\n```\n\n----------------------------------------\n\nTITLE: Debugging JIT-Compiled Functions with NaNs\nDESCRIPTION: Example demonstrating NaN detection in JIT-compiled functions, showing how JAX de-optimizes the code for debugging when NaNs are encountered.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\n\n@jit\ndef f(x, y):\n    a = x * y\n    b = (x + y) / (x - y)\n    c = a + 2\n    return a + b * c\n\nx = jnp.array([2., 0.])\ny = jnp.array([3., 0.])\nf(x, y)\n```\n\n----------------------------------------\n\nTITLE: Differentiation Fails for io_callback with Differentiated Variable - Python\nDESCRIPTION: Shows that io_callback cannot be differentiated if the callback output depends on a differentiable variable; jax.grad call raises an error. Demonstrates current limitations when integrating side-effectful or impure operations in autodiff pipelines.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\njax.grad(numpy_random_like)(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing vjp_flat for vector-Jacobian products\nDESCRIPTION: Implementation of the core vjp_flat function that combines linearization and transposition to compute vector-Jacobian products.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ndef vjp_flat(f, *primals_in):\n  pvals_in = ([PartialVal.known(x) for x in primals_in] +\n              [PartialVal.unknown(vspace(get_aval(x))) for x in primals_in])\n  primal_pvals_in, tangent_pvals_in = split_half(pvals_in)\n  def f_jvp(*primals_tangents_in):\n    primals_out, tangents_out = jvp(f, *split_half(primals_tangents_in))\n    return [*primals_out, *tangents_out]\n  jaxpr, pvals_out, consts = partial_eval_flat(f_jvp, pvals_in)  # linearize\n  primal_pvals, _ = split_half(pvals_out)\n  assert all(pval.is_known for pval in primal_pvals)\n  primals_out = [pval.const for pval in primal_pvals]\n  transpose_inputs = consts + [UndefPrimal(p.aval) for p in tangent_pvals_in]\n  f_vjp = lambda *cts: eval_jaxpr_transposed(jaxpr, transpose_inputs, cts)\n  return primals_out, f_vjp\n\ndef vjp(f, *primals_in):\n  primals_in_flat, in_tree = tree_flatten(primals_in)\n  f, out_tree = flatten_fun(f, in_tree)\n  primals_out_flat, f_vjp_flat = vjp_flat(f, *primals_in_flat)\n  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n\n  def f_vjp(*cotangents_out):\n    cotangents_out_flat, _ = tree_flatten(cotangents_out)\n    cotangents_in_flat = f_vjp_flat(*cotangents_out_flat)\n    return tree_unflatten(in_tree, cotangents_in_flat)\n\n  return primals_out, f_vjp\n\nclass UndefPrimal(NamedTuple):\n  aval: ShapedArray\n\nregister_pytree_node(UndefPrimal,\n                     lambda u: (u.aval, ()),\n                     lambda aval, _: UndefPrimal(aval))\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Partitioning for JAX FFI\nDESCRIPTION: Complex example of implementing custom partitioning logic for a JAX FFI function. This approach uses custom_partitioning to define how data should be sharded across devices, with support for specific sharding requirements where the last dimension must be replicated.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental.custom_partitioning import custom_partitioning\n\n@partial(custom_partitioning, static_argnums=(1,))\ndef rms_norm_partitioned(x, eps=1e-5):\n  return rms_norm(x, eps=eps)\n\ndef replicate_sharding_on_last_dim(mesh, sharding, target_info):\n  # Our implementation supports trivial sharding on any batch dimensions, but the data\n  # must be replicated on the last (non-batch) dimension.\n  rank = len(target_info.shape)\n  num_batch_dims = min(len(sharding.spec), rank - 1)\n\n  # The Nones here indicate which dimensions should be replicated.\n  names = tuple(sharding.spec[:num_batch_dims]) + (None,) * (rank - num_batch_dims)\n  return jax.NamedSharding(mesh, P(*names))\n\ndef rms_norm_infer_sharding_from_operands(eps, mesh, args_info, result_info):\n  del eps  # unused\n  arg_info, = args_info\n  result_sharding = replicate_sharding_on_last_dim(mesh, arg_info.sharding, result_info)\n\n  # In this case, we only have a single output, but the return value from this function\n  # must have the same pytree structure as the output from the underlying function\n  # (`rms_norm` in this case).\n  return result_sharding\n\ndef rms_norm_partition(eps, mesh, args_info, result_info):\n  arg_info, = args_info\n  arg_sharding = replicate_sharding_on_last_dim(mesh, arg_info.sharding, arg_info)\n  result_sharding = replicate_sharding_on_last_dim(mesh, arg_info.sharding, result_info)\n\n  # This is the function that computes the partitioned model on the appropriate subset\n  # of the data.\n  def partitioned_rms_norm(x):\n    return rms_norm(x, eps=eps)\n\n  # Note that the third element of our returned tuple must be the shardings for the\n  # _outputs_ and its pytree structure must match the output of `rms_norm`. Similarly,\n  # the fourth element must have the same pytree structure as the _inputs_ to\n  # `rms_norm`. In this case, there is only one input, but it must be returned within\n  # a `tuple` anyways.\n  return mesh, partitioned_rms_norm, result_sharding, (arg_sharding,)\n\nrms_norm_partitioned.def_partition(\n    infer_sharding_from_operands=rms_norm_infer_sharding_from_operands,\n    partition=rms_norm_partition,\n)\n\noutput = jax.jit(rms_norm_partitioned, out_shardings=batch_shd)(x_batch_shd)\nnp.testing.assert_allclose(output, rms_norm_ref(x), rtol=1e-5)\nprint(jax.jit(rms_norm_partitioned, out_shardings=batch_shd).lower(x_batch_shd).compile().as_text().strip())\n```\n\n----------------------------------------\n\nTITLE: Initializing JAX Distributed Computing with Auto-configuration\nDESCRIPTION: Shows how to initialize JAX distributed computing on Cloud TPU, Slurm and Open MPI environments using automatic configuration. No explicit parameters needed as defaults are determined from the environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/multi_process.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n\njax.distributed.initialize()\n```\n\n----------------------------------------\n\nTITLE: Performing Atrous Convolution with JAX\nDESCRIPTION: Demonstrates atrous convolution (dilated convolution) using JAX's lax.conv_general_dilated function. It uses VALID padding, no stride, and applies kernel dilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/convolutions.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nout = lax.conv_general_dilated(img,     # lhs = image tensor\n                               kernel,  # rhs = conv kernel tensor\n                               (1,1),   # window strides\n                               'VALID', # padding mode\n                               (1,1),   # lhs/image dilation\n                               (12,12), # rhs/kernel dilation\n                               dn)      # dimension_numbers = lhs, rhs, out dimension permutation\nprint(\"out shape: \", out.shape)\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);\n```\n\n----------------------------------------\n\nTITLE: Composing Inverse with Other JAX Transformations\nDESCRIPTION: Demonstrates the ability to compose the custom inverse transformation with standard JAX transformations like jit (just-in-time compilation), vmap (vectorization), and grad (gradient).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\njit(vmap(grad(inverse(f))))((jnp.arange(5) + 1.) / 5.)\n```\n\n----------------------------------------\n\nTITLE: Platform-Dependent Cross-Platform FFI Dispatch in JAX (Python)\nDESCRIPTION: This snippet defines a wrapper function that dispatches FFI calls to different backend targets ('rms_norm' for CPU and 'rms_norm_cuda' for CUDA) depending on the runtime platform. The implementation uses jax.lax.platform_dependent to select between platforms at tracing time, enabling transparent use of the best available implementation. Dependencies are JAX, NumPy, and both CPU/GPU FFI targets. Inputs are arrays and epsilon. Output is the result of FFI invocation. The function asserts float32 input types and returns a JAX array. Limitations: additional platforms would require extending the mapping.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef rms_norm_cross_platform(x, eps=1e-5):\n  assert x.dtype == jnp.float32\n  out_type = jax.ShapeDtypeStruct(x.shape, x.dtype)\n\n  def impl(target_name):\n    return lambda x: jax.ffi.ffi_call(\n      target_name,\n      out_type,\n      vmap_method=\"broadcast_all\",\n    )(x, eps=np.float32(eps))\n\n  return jax.lax.platform_dependent(x, cpu=impl(\"rms_norm\"), cuda=impl(\"rms_norm_cuda\"))\n\n\nnp.testing.assert_allclose(rms_norm_cross_platform(x), rms_norm_ref(x), rtol=1e-5)\n\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation with Static Arguments in Python using JAX\nDESCRIPTION: This example demonstrates how to mark certain arguments as static for JIT compilation, allowing control flow based on these static values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n@partial(jit, static_argnums=(1,))\ndef f(x, neg):\n  return -x if neg else x\n\nf(1, True)\n```\n\n----------------------------------------\n\nTITLE: Using emit_pipeline in Reduce-Scatter with Large HBM Blocks\nDESCRIPTION: Implementation using emit_pipeline to replace manual copying in the reduce-scatter kernel. This approach handles larger HBM blocks by creating an inner pipeline with smaller VMEM-friendly block sizes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef inner_kernel(input_ref, accum_ref):\n  accum_ref[...] = input_ref[...]\naccum_pipeline = pltpu.emit_pipeline(inner_kernel,\n                                     in_specs=[inner_block_spec],\n                                     out_specs=inner_block_spec,\n                                     should_accumulate_out=True,\n                                     grid=inner_grid)\n@pl.when(~last_iteration)\ndef _():\n  @pl.when(phase == LEFT)\n  def _():\n    accum_pipeline(x_ref.at[left_copy_device, left_copy_slice],\n                   hbm_scratch.at[working_slot, left_copy_slice],\n    )\n  @pl.when(phase == RIGHT)\n  def _():\n    accum_pipeline(x_ref.at[right_copy_device, right_copy_slice],\n                   hbm_scratch.at[working_slot, right_copy_slice],\n    )\n```\n\n----------------------------------------\n\nTITLE: Custom VJP with Multiple Arguments\nDESCRIPTION: Demonstrates how to define a custom VJP rule for a function with multiple arguments. The example shows how to handle multiple primal inputs and compute their corresponding gradients.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import custom_vjp\n\n@custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n----------------------------------------\n\nTITLE: Conditional Transposition Rule Implementation in Python\nDESCRIPTION: Implements transposition rules for conditional operations, handling undefined primals and constructing appropriate transposed jaxprs. Essential for reverse-mode autodifferentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_93\n\nLANGUAGE: python\nCODE:\n```\ndef cond_transpose_rule(cts, pred, *invals, true_jaxpr, false_jaxpr):\n  undef_primals = tuple(type(x) is UndefPrimal for x in invals)\n  true_jaxpr, true_consts = transpose_jaxpr(true_jaxpr, undef_primals)\n  false_jaxpr, false_consts = transpose_jaxpr(false_jaxpr, undef_primals)\n  true_jaxpr, false_jaxpr = _join_jaxpr_consts(\n      true_jaxpr, false_jaxpr, len(true_consts), len(false_consts))\n  res = [x for x in invals if type(x) is not UndefPrimal]\n  outs = bind_cond(pred, *true_consts, *false_consts, *res, *cts,\n                   true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n  outs = iter(outs)\n  return [None] + [next(outs) if type(x) is UndefPrimal else None for x in invals]\ntranspose_rules[cond_p] = cond_transpose_rule\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Custom VJP\nDESCRIPTION: Shows how to compute gradients of a function with a custom VJP rule. The example demonstrates that the custom VJP rule is correctly used for gradient computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(2., 3.))\n```\n\n----------------------------------------\n\nTITLE: Implementing Matrix Addition with Pallas on TPU\nDESCRIPTION: This code demonstrates how to implement matrix addition using JAX's Pallas library on TPUs. It shows both the kernel function that operates on VMEM references and the wrapper function that handles memory transfers between HBM and VMEM.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add_matrices_kernel(x_vmem_ref, y_vmem_ref, z_vmem_ref):\n  # Load x and y from VMEM into VREGs\n  x_vregs = x_vmem_ref[:, :]\n  y_vregs = y_vmem_ref[:, :]\n  # Execute a vectorized add\n  z_vregs = x_vregs + y_vregs\n  # Store the output values in VREGs back into VMEM\n  z_vmem_ref[:, :] = z_vregs\n\n\ndef add_matrices(x: jax.Array, y: jax.Array) -> jax.Array:\n  # pallas_call will first allocate scratch buffers for `x` and `y` in VMEM.\n  # It will then copy `x` and `y` from HBM into VMEM.\n  z = pl.pallas_call(\n      add_matrices_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)\n  )(x, y)\n  # pallas_call will also copy the output from VMEM back into HBM.\n  return z\n\n\nx, y = jnp.ones((512, 512)), jnp.ones((512, 512))\nadd_matrices(x, y)\n```\n\n----------------------------------------\n\nTITLE: Pallas Pipeline API Definition\nDESCRIPTION: API function signature for the Pallas pipelining system with grid, block specs and kernel parameters.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef pallas_call(\n  kernel,\n  grid: tuple[int, ...],\n  in_specs: Sequence[PyTree[BlockSpec]],\n  out_specs: PyTree[BlockSpec],\n  out_shape: PyTree[jax.ShapeDtypeStruct],\n) -> Callable:\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward-Mode Autodiff with JVP Tracer and Trace - JAX Python\nDESCRIPTION: Introduces the JVPTracer and JVPTrace classes, enabling forward-mode autodiff by packing each value as a primal-tangent pair. JVPTrace defines how to lift values and process primitives by calling registered JVP rules. Dependencies include Tracer, Trace, get_aval, zeros_like, and dynamic jvp_rules. Parameters are input tracers and primitive-specific arguments; return is a list of new JVPTracers with computed primals and tangents.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass JVPTracer(Tracer):\\n  def __init__(self, trace, primal, tangent):\\n    self._trace = trace\\n    self.primal = primal\\n    self.tangent = tangent\\n\\n  @property\\n  def aval(self):\\n    return get_aval(self.primal)\\n\\nclass JVPTrace(Trace):\\n  pure = lift = lambda self, val: JVPTracer(self, val, zeros_like(val))\\n\\n  def process_primitive(self, primitive, tracers, params):\\n    primals_in, tangents_in = unzip2((t.primal, t.tangent) for t in tracers)\\n    jvp_rule = jvp_rules[primitive]\\n    primal_outs, tangent_outs = jvp_rule(primals_in, tangents_in, **params)\\n    return [JVPTracer(self, x, t) for x, t in zip(primal_outs, tangent_outs)]\\n\\njvp_rules = {}\n```\n\n----------------------------------------\n\nTITLE: Implementing the Jaxpr Inverse Interpreter\nDESCRIPTION: Interprets a JAX expression (jaxpr) backwards, applying inverse operations from the registry to each equation. Raises an error if an inverse is not available for a primitive.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef inverse_jaxpr(jaxpr, consts, *args):\n  env = {}\n\n  def read(var):\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n  # Args now correspond to Jaxpr outvars\n  safe_map(write, jaxpr.outvars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Looping backward\n  for eqn in jaxpr.eqns[::-1]:\n    #  outvars are now invars\n    invals = safe_map(read, eqn.outvars)\n    if eqn.primitive not in inverse_registry:\n      raise NotImplementedError(\n          f\"{eqn.primitive} does not have registered inverse.\")\n    # Assuming a unary function\n    outval = inverse_registry[eqn.primitive](*invals)\n    safe_map(write, eqn.invars, [outval])\n  return safe_map(read, jaxpr.invars)\n```\n\n----------------------------------------\n\nTITLE: Utilizing jax.checkpoint to Rematerialize Constants Not Dependent on Arguments - JAX (Python)\nDESCRIPTION: This example demonstrates jax.checkpoint handling computations that do not depend on function arguments. Whereas the old implementation would save large constant intermediates, the new implementation rematerializes them on demand, reducing memory overhead. To use this, decorate functions with @jax.checkpoint, where some internal computation (like calling some_function on an arange) produces constants. The input x is multiplied by a constant a, which is expensive to save but cheap to recompute.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/11830-new-remat-checkpoint.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@jax.checkpoint\ndef f(x):\n  a = some_function(jnp.arange(10_000_000))  # `a` does not depend on `x`\n  return a * x\n```\n\n----------------------------------------\n\nTITLE: Examining JAX Tracing with make_jaxpr\nDESCRIPTION: This snippet uses make_jaxpr to examine the JAX tracing behavior for the permissive_sum function, showing how each list element is treated separately.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmake_jaxpr(permissive_sum)(x)\n```\n\n----------------------------------------\n\nTITLE: Vectorizing io_callback-Jitted Function with vmap in JAX - Python\nDESCRIPTION: Shows that io_callback is natively compatible with vmap: mapping numpy_random_like over an input x yields random arrays, but ordering is not guaranteed due to potential out-of-order execution. No additional dependencies beyond previous snippet. The unordered execution may result in variable host-side effects/output order across runs, especially on GPU/TPU.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\njax.vmap(numpy_random_like)(x)\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation of Forward-Mode JVP Function in JAX (Python)\nDESCRIPTION: Applies JIT compilation to a function wrapping JAX's jvp for the custom primitive, demonstrating performance with ahead-of-time compilation. Accepts argument and tangent tuples, returns (primal, tangent) as a tuple. Requires earlier registration of the JVP rule and JAX imports.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nassert api.jit(lambda arg_values, arg_tangents: \n                   api.jvp(square_add_prim, arg_values, arg_tangents))(\n         (2., 10.), (1., 1.)) == (14., 5.)\n\n```\n\n----------------------------------------\n\nTITLE: Using Implicit Constraints in Symbolic Shapes for JAX Export\nDESCRIPTION: Shows how defining a symbolic dimension with an arithmetic expression like 'b + 15' acts as an implicit constraint (dimension >= 16). This helps JAX verify conditions during export, such as ensuring a slice size (16) is not larger than the dimension size (`b + 15`). This avoids potential errors that might occur if the dimension was just 'b'.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> _ = export.export(jax.jit(lambda x: x[0:16]))(\n...    jax.ShapeDtypeStruct(export.symbolic_shape(\"b + 15\"), dtype=np.int32))\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom JVP with Decorator Syntax\nDESCRIPTION: Demonstrates an alternative way to define custom JVP rules using decorator syntax. This pattern is useful for maintaining code clarity when defining custom differentiation rules.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x):\n  ...\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Pytree Transposition with JAX\nDESCRIPTION: Shows two methods for transposing pytrees using jax.tree.map and jax.tree.transpose to convert between row-major and column-major formats.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef tree_transpose(list_of_trees):\n  \"\"\"\n  Converts a list of trees of identical structure into a single tree of lists.\n  \"\"\"\n  return jax.tree.map(lambda *xs: list(xs), *list_of_trees)\n\n# Convert a dataset from row-major to column-major.\nepisode_steps = [dict(t=1, obs=3), dict(t=2, obs=4)]\ntree_transpose(episode_steps)\n```\n\n----------------------------------------\n\nTITLE: Verifying JAX Pallas Matmul Implementation\nDESCRIPTION: Verifies the correctness of the Pallas-based `matmul` function against JAX NumPy's standard matrix multiplication (`@` operator). It generates two large random JAX arrays (`x`, `y`) using `jax.random` and uses `np.testing.assert_array_equal` (via NumPy) to ensure the results from the Pallas implementation match the standard JAX implementation exactly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nm, k, n = 4096, 4096, 4096\nk1, k2 = random.split(random.key(0), 2)\nx = random.normal(k1, (m, k), dtype=jnp.float32)\ny = random.normal(k2, (k, n), dtype=jnp.float32)\nnp.testing.assert_array_equal(x @ y, matmul(x, y))\n```\n\n----------------------------------------\n\nTITLE: Testing VJP with sine function\nDESCRIPTION: Example demonstrating vector-Jacobian product computation for the sine function and verifying it against the analytical derivative (cosine).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_74\n\nLANGUAGE: python\nCODE:\n```\ny, f_vjp = vjp(sin, 3.)\nprint(f_vjp(1.), cos(3.))\n```\n\n----------------------------------------\n\nTITLE: Implementing Convolution in Python with JAX\nDESCRIPTION: Defines a simple convolution function using JAX's NumPy implementation. This function computes the convolution of two one-dimensional vectors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-vectorization.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n  output = []\n  for i in range(1, len(x)-1):\n    output.append(jnp.dot(x[i-1:i+2], w))\n  return jnp.array(output)\n\nconvolve(x, w)\n```\n\n----------------------------------------\n\nTITLE: Smooth Gradient Computation using Sigmoid in JAX\nDESCRIPTION: Shows how to use sigmoid function as a smooth alternative to the step function, resulting in well-defined gradients throughout the domain.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef g(x):\n  return jax.nn.sigmoid(x)\n\ndg = jax.vmap(jax.grad(g))\n\nx = jnp.array([-10.0, -1.0, 0.0, 1.0, 10.0])\n\nwith np.printoptions(suppress=True, precision=2):\n  print(f\"g(x)  = {g(x)}\")\n  print(f\"dg(x) = {dg(x)}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking and Analyzing Fused Matmul Kernels with Activations - Python\nDESCRIPTION: This code provides a benchmarking function for fused matmul operations (with optional activation and transpose) and demonstrates its use across different block sizes and problem shapes. It generates input matrices using JAX, measures execution time and throughput, and prints utilization statistics, using jax.nn.relu as an exemplary activation. Dependencies include JAX, NumPy/jnp, and the matmul function from the prior snippet. Key parameters include matrix sizes (m, k, n), data types, activation choice, and block configurations. The output is printed performance information; the implementation assumes availability of TPU-relevant libraries and input matmul function signature.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef analyze_matmul(m: int, k: int, n: int, dtype: np.dtype,\n                   mm_func, transpose_rhs: bool = False,\n                   activation = lambda x: x):\n  x = jnp.ones((m, k), dtype=dtype)\n  if transpose_rhs:\n    y = jnp.ones((n, k), dtype=dtype)\n    @jax.jit\n    def _wrapper(x, y):\n      y = y.swapaxes(0, 1)\n      return mm_func(x, y, transpose_rhs=True, activation=activation)\n  else:\n    y = jnp.ones((k, n), dtype=dtype)\n    _wrapper = functools.partial(mm_func, activation=activation)\n  time = benchmark(_wrapper)(x, y)\n  print(f\"----- {m} x {k} x {n} -----\")\n  print(\"Matmul time: \", time)\n  mm_flops = matmul_flops(m, k, n) / time\n  print(\"Matmul FLOP/s: \", mm_flops)\n  print(f\"FLOP/s utilization: {mm_flops / v5e_flops * 100:.4f}%\")\n  print()\n\n\nactivation = jax.nn.relu\nprint(\"================bm=128, bk=128, bn=128===================\")\nmm = functools.partial(matmul, bm=128, bk=128, bn=128)\nanalyze_matmul(1024, 1024, 1024, jnp.bfloat16, mm, activation=activation)\nanalyze_matmul(4096, 4096, 4096, jnp.bfloat16, mm, activation=activation)\nanalyze_matmul(8192, 8192, 8192, jnp.bfloat16, mm, activation=activation)\n\nprint(\"================bm=512, bk=1024, bn=1024===================\")\nmm = functools.partial(matmul, bm=512, bk=1024, bn=1024)\nanalyze_matmul(1024, 1024, 1024, jnp.bfloat16, mm, activation=activation)\nanalyze_matmul(4096, 4096, 4096, jnp.bfloat16, mm, activation=activation)\nanalyze_matmul(8192, 8192, 8192, jnp.bfloat16, mm, activation=activation)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JAX Tracing with JIT Compilation\nDESCRIPTION: Illustrates how JAX uses tracers to represent array values during function transformation, shown through a simple jit-compiled function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/key-concepts.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  print(x)\n  return x + 1\n\nx = jnp.arange(5)\nresult = f(x)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Parallel Matrix Multiplication\nDESCRIPTION: Measures the execution time of parallel matrix multiplication across devices, showing the performance benefits of distributed computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ntimeit -n 5 -r 5 pmap(jnp.dot)(mats, mats).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Using `shard_map` with Partial Input Splitting in JAX (Python)\nDESCRIPTION: Shows an example of using `shard_map` where an input array axis is split over one mesh axis ('i') but not another ('j'). A 2D mesh `m` (4x2) is created. The `shard_map` decorator specifies `in_specs=P('i', None)`, meaning the first dimension of the input `x1` is split across the 'i' mesh dimension (size 4), while the second dimension is not split over any mesh axis. The function `f1` receives blocks (`x_block`) with shape `(3, 12)` (since 12/4=3), demonstrating that each device gets a full slice along the second dimension. The `out_specs=P('i', 'j')` indicate how the output should be sharded. Dependencies include `jax`, `jax.sharding`, `jax.experimental.shard_map`, `numpy`, and `functools`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndevices = np.array(jax.devices())\nm = Mesh(devices.reshape(4, 2), ('i', 'j'))\n\n@partial(shard_map, mesh=m, in_specs=P('i', None), out_specs=P('i', 'j'))\ndef f1(x_block):\n  print(x_block.shape)\n  return x_block\n\nx1 = np.arange(12 * 12).reshape(12, 12)\ny = f1(x1)  # prints (3,12)\n```\n\n----------------------------------------\n\nTITLE: Computing gradients with non-differentiable arguments in custom_vjp\nDESCRIPTION: Demonstrates taking the gradient of the app function with respect to its second argument, showing how the custom backward pass is used to compute the gradient.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(app, 1)(lambda x: x ** 2, 4.))\n```\n\n----------------------------------------\n\nTITLE: Computing Singular Value Decomposition with JAX\nDESCRIPTION: Performs singular value decomposition (SVD) on a randomly generated matrix using JAX's linear algebra functionality. Verifies the shapes of the resulting matrices and prints the singular values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_cpu.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nimport jax.random as rand\n\nN = 10\nM = 20\nkey = rand.PRNGKey(1701)\n\nX = rand.normal(key, (N, M))\nu, s, vt = jnp.linalg.svd(X)\nassert u.shape == (N, N)\nassert vt.shape == (M, M)\nprint(s)\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Compiling jaxpr via MLIR/XLA in JAX - Python\nDESCRIPTION: Defines the evaluation rule xla_call_impl for the xla_call primitive, which splits constants and arguments, sets up XLA compilation and caching, and returns the executed outputs. Implements xla_callable with lru_cache to compile a JAX jaxpr into MLIR/HLO, register MLIR dialects, generate functions for the XLA backend, and create executable callables. Includes helper functions for MLIR type conversion and constant generation. Relies on JAX's MLIR IR, SymbolTable, typing for NamedTuple, partial, numpy, and jax-backend imports. Handles the full code flow from jaxpr and argument staging to HLO program execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\nimport io\nfrom jax.extend.mlir import ir\nfrom jax.extend.mlir.dialects import func\nfrom jax.extend.mlir.dialects import stablehlo as hlo\nfrom jax._src import xla_bridge as xb\n\nclass MlirContext(NamedTuple):\n  module: ir.Module\n  symbol_table: ir.SymbolTable\n\ndef xla_call_impl(*args, jaxpr: Jaxpr, num_consts: int):\n  consts, args = args[:num_consts], args[num_consts:]\n  hashable_consts = tuple(map(IDHashable, consts))\n  execute = xla_callable(IDHashable(jaxpr), hashable_consts)\n  return execute(*args)\nimpl_rules[xla_call_p] = xla_call_impl\n\n@lru_cache\ndef xla_callable(hashable_jaxpr: IDHashable,\n                 hashable_consts: tuple[IDHashable, ...]):\n  jaxpr: Jaxpr = hashable_jaxpr.val\n  typecheck_jaxpr(jaxpr)\n  consts = [x.val for x in hashable_consts]\n  in_avals = [v.aval for v in jaxpr.in_binders[len(consts):]]\n\n  with ir.Context() as ctx, ir.Location.unknown(ctx):\n    hlo.register_dialect(ctx)\n    m = ir.Module.create()\n    c = MlirContext(m, ir.SymbolTable(m.operation))\n\n    with ir.InsertionPoint(c.module.body):\n      @func.func(*(aval_to_ir_type(aval) for aval in in_avals))\n      def main(*params):\n        return jaxpr_subcomp(c, jaxpr, _hlo_consts(consts) + params)\n\n  output = io.StringIO()\n  c.module.operation.print(file=output)\n  compiled = xb.get_backend(None).compile(output.getvalue())\n  return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n\ndef _mlir_dtype(dtype: np.dtype) -> ir.Type:\n  if np.issubdtype(dtype, np.signedinteger):\n    return ir.IntegerType.get_signless(np.iinfo(dtype).bits)\n  elif dtype == np.float32:\n    return ir.F32Type.get()\n  elif dtype == np.float64:\n    return ir.F64Type.get()\n  else:\n    raise NotImplementedError(\"MLIR conversion not implemented for \", dtype)\n\ndef aval_to_ir_type(aval: ShapedArray) -> ir.Type:\n  return ir.RankedTensorType.get(aval.shape, _mlir_dtype(aval.dtype))\n\ndef _hlo_const(x: Any) -> ir.Value:\n  a = np.asarray(x)\n  if a.dtype == np.bool_:\n    return hlo.constant(ir.DenseElementsAttr.get(\n      np.packbits(a, bitorder='little'), type=ir.IntegerType.get_signless(1),\n      shape=a.shape))\n  else:\n    return hlo.constant(ir.DenseElementsAttr.get(a))\n\ndef _hlo_consts(consts: list[Any]) -> list[ir.Value]:\n  unique_consts = {id(cnst): cnst for cnst in consts}\n  ir_consts = {id_: _hlo_const(cnst) for id_, cnst in unique_consts.items()}\n  return tuple(ir_consts[id(cnst)] for cnst in consts)\n\n```\n\n----------------------------------------\n\nTITLE: Optimizing Scan Layer Function with Checkpointing\nDESCRIPTION: Applies jax.checkpoint with a custom policy to the layer function used in scan. This improves the memory-computation tradeoff when differentiating through scan-based neural networks.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n@partial(jax.checkpoint,\n         policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\ndef layer(x, W_b_pair):\n  W, b = W_b_pair\n  out = jnp.maximum(jnp.dot(x, W) + b, 0.)\n  return out, None\n```\n\n----------------------------------------\n\nTITLE: Visualizing Type Promotion Lattice with NetworkX and Matplotlib - Python\nDESCRIPTION: This snippet constructs and visualizes a directed graph (lattice) representing proposed type promotions in JAX using NetworkX and matplotlib. It specifies nodes for various numeric types and their promotion paths, arranges them spatially for clarity, and draws the graph for visual reference. Dependencies include networkx, matplotlib, and a Python 3 interpreter; key parameters are the 'lattice' adjacency list (defining promotion relationships) and 'pos' dictionary (node positions). Output is a rendered graph displaying type promotion paths; limitations center around its static nature and reliance on the accuracy of the manual lattice construction.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\\nimport matplotlib.pyplot as plt\\nlattice = {\\n  'i*': ['u8', 'i8'], 'f*': ['c*', 'f16'], 'c*': ['c64'],\\n  'u8': ['u16', 'i16'], 'u16': ['u32', 'i32'], 'u32': ['u64', 'i64'],\\n  'i8': ['i16'], 'i16': ['i32'], 'i32': ['i64'], 'i64': ['f*'],\\n  'f16': ['f32'], 'f32': ['f64', 'c64'], 'f64': ['c128'],\\n  'c64': ['c128']\\n}\\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\\npos = {\\n  'i*': [-1.25, 0.5], 'f*': [-0.5, 2], 'c*': [0, 3],\\n  'u8': [0.5, 0], 'u16': [1.5, 0], 'u32': [2.5, 0], 'u64': [3.5, 0],\\n  'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\\n  'f16': [1.5, 2], 'f32': [2.5, 2], 'f64': [3.5, 2],\\n  'c64': [3, 3], 'c128': [4, 3],\\n}\\nfig, ax = plt.subplots(figsize=(6, 5))\\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\n```\n\n----------------------------------------\n\nTITLE: Defining a 'jit' Wrapper and Primitive for XLA Calls in JAX - Python\nDESCRIPTION: This snippet defines a user-facing jit wrapper to stage computations out of Python, generate a jaxpr using make_jaxpr, and bind it to the xla_call_p primitive for later compilation and execution on XLA. The xla_call_p primitive is instantiated and will later be assigned transformation rules for evaluation. Dependencies include JAX's make_jaxpr, Primitive, and tree utility functions. Arguments to the decorated function are shaped and abstracted, then packed together for staged execution. The output is reconstructed using the original Python function signature. Assumes the surrounding definitions of raise_to_shaped, get_aval, make_jaxpr, Primitive, bind, and tree_unflatten.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\ndef jit(f):\n  def f_jitted(*args):\n    avals_in = [raise_to_shaped(get_aval(x)) for x in args]\n    jaxpr, consts, out_tree = make_jaxpr(f, *avals_in)\n    outs = bind(xla_call_p, *consts, *args, jaxpr=jaxpr, num_consts=len(consts))\n    return tree_unflatten(out_tree, outs)\n  return f_jitted\n\nxla_call_p = Primitive('xla_call')\n\n```\n\n----------------------------------------\n\nTITLE: Registering Inverse Functions for Common JAX Primitives\nDESCRIPTION: Maps common JAX primitives like exponential and hyperbolic tangent to their corresponding inverse functions from the JAX NumPy library.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninverse_registry[lax.exp_p] = jnp.log\ninverse_registry[lax.tanh_p] = jnp.arctanh\n```\n\n----------------------------------------\n\nTITLE: Comparing Python 'and' and JAX 'logical_and' for Positive Even Check in Python\nDESCRIPTION: Defines and compares a pure Python function using `and` (which short-circuits) and a JAX function using `jnp.logical_and` (which does not short-circuit) decorated with `@jit` to check for positive even integers. Both functions are demonstrated with a scalar input (24). Requires `jax`, `jax.numpy` (as `jnp`).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef python_check_positive_even(x):\n  is_even = x % 2 == 0\n  # `and` short-circults, so when `is_even` is `False`, `x > 0` is not evaluated.\n  return is_even and (x > 0)\n\n@jit\ndef jax_check_positive_even(x):\n  is_even = x % 2 == 0\n  # `logical_and` does not short circuit, so `x > 0` is always evaluated.\n  return jnp.logical_and(is_even, x > 0)\n\nprint(python_check_positive_even(24))\nprint(jax_check_positive_even(24))\n```\n\n----------------------------------------\n\nTITLE: Using None to Indicate Zero JVP\nDESCRIPTION: Demonstrates using None in defjvps to indicate that the JVP for a particular argument is zero. This shorthand simplifies the definition of custom JVP rules with zero derivatives.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x, y):\n  return x ** 2 * y\n\nf.defjvps(lambda x_dot, primal_out, x, y: 2 * x * y * x_dot,\n          None)\n```\n\n----------------------------------------\n\nTITLE: Customizing Batch Axes with JAX's vmap\nDESCRIPTION: Demonstrates how to use vmap's in_axes and out_axes parameters to specify custom batch dimensions for inputs and outputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-vectorization.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nauto_batch_convolve_v2 = jax.vmap(convolve, in_axes=1, out_axes=1)\n\nxst = jnp.transpose(xs)\nwst = jnp.transpose(ws)\n\nauto_batch_convolve_v2(xst, wst)\n```\n\n----------------------------------------\n\nTITLE: Using ShapeDtypeStruct for Type-Only Tracing in JAX\nDESCRIPTION: Shows how to use ShapeDtypeStruct to provide only shape and dtype information during tracing instead of actual array values, which is useful for specializing a function without needing concrete data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/aot.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> i32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n>>> jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x, y)\nArray(10, dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Memory Leak Detection Example\nDESCRIPTION: Example program demonstrating how to capture multiple memory profiles to track memory leaks, showing accumulation of arrays in a Python list.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/device_memory_profiling.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport jax.profiler\n\ndef afunction():\n  return jax.random.normal(jax.random.key(77), (1000000,))\n\nz = afunction()\n\ndef anotherfunc():\n  arrays = []\n  for i in range(1, 10):\n    x = jax.random.normal(jax.random.key(42), (i, 10000))\n    arrays.append(x)\n    x.block_until_ready()\n    jax.profiler.save_device_memory_profile(f\"memory{i}.prof\")\n\nanotherfunc()\n```\n\n----------------------------------------\n\nTITLE: Calculating Block Slices for Pallas Invocations in Python\nDESCRIPTION: Defines a Python function `slices_for_invocation` that demonstrates how Pallas computes the slices for an array `x` based on its shape, a `BlockSpec`, the grid dimensions, and the specific invocation indices. The function asserts dimensional consistency and calculates element start indices by multiplying block indices (obtained from `x_spec.index_map`) by block sizes. It returns a tuple of slice objects representing the portion of the array accessed by the given invocation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax.experimental import pallas as pl\n>>> def slices_for_invocation(x_shape: tuple[int, ...],\n...                           x_spec: pl.BlockSpec,\n...                           grid: tuple[int, ...],\n...                           invocation_indices: tuple[int, ...]) -> tuple[slice, ...]:\n...   assert len(invocation_indices) == len(grid)\n...   assert all(0 <= i < grid_size for i, grid_size in zip(invocation_indices, grid))\n...   block_indices = x_spec.index_map(*invocation_indices)\n...   assert len(x_shape) == len(x_spec.block_shape) == len(block_indices)\n...   elem_indices = []\n...   for x_size, block_size, block_idx in zip(x_shape, x_spec.block_shape, block_indices):\n...     start_idx = block_idx * block_size\n...     # At least one element of the block must be within bounds\n...     assert start_idx < x_size\n...     elem_indices.append(slice(start_idx, start_idx + block_size))\n...   return elem_indices\n\n```\n\n----------------------------------------\n\nTITLE: Implementing and Executing Pallas-Based Reduce-Scatter Function in Python\nDESCRIPTION: Defines a function that uses the Pallas TPU kernel for reduce-scatter operations and executes it using JAX's shard_map functionality. The function reshapes input data and calls the kernel with appropriate specifications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef pallas_reduce_scatter(input_arr):\n  input_arr = input_arr.reshape(num_devices, block_size[0], block_size[1])\n  return pl.pallas_call(\n      reduce_scatter_kernel,\n      out_shape=out_shape,\n      grid_spec=grid_spec,\n      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n  )(input_arr)[0]\n\n\npallas_result = jax.jit(\n    shard_map.shard_map(\n        pallas_reduce_scatter,\n        mesh=mesh,\n        in_specs=P(None, 'x'),\n        out_specs=P('x', None),\n        check_rep=False,\n    )\n)(input_arr)\n\npallas_result = jax.block_until_ready(pallas_result)\n```\n\n----------------------------------------\n\nTITLE: Stateful Asynchronous Permutation in Loop with Alternating Buffers\nDESCRIPTION: This function demonstrates how stateful operations with Refs naturally lead to a double-buffering pattern in loops, avoiding the need for explicit unrolling and preventing aliasing issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  x_ref = make_ref(x)\n  y_ref = make_ref(zeros_like(x))\n  def body(i, _):\n    fut = ppermute_start_stateful(x_ref, y_ref)\n    ppermute_done_stateful(*fut, x_ref, y_ref)\n    # Now switch to y_ref -> x_ref\n    fut = ppermute_start_stateful(y_ref, x_ref)\n    ppermute_done_stateful(*fut, y_ref, x_ref)\n  fori_loop(0, 8 // 2, body, None)\n  return x_ref[...]\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Custom VJP Function\nDESCRIPTION: Shows that when evaluating a custom VJP function without differentiation, only the original function is called. This demonstrates JAX's behavior with custom differentiation when no gradients are needed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nprint(f(3.))\n```\n\n----------------------------------------\n\nTITLE: Hessian-Vector Products using Reverse-over-Reverse Mode\nDESCRIPTION: A third implementation of Hessian-vector products using reverse-over-reverse mode differentiation. This approach only works for single arguments and is typically less efficient than forward-over-reverse.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# reverse-over-reverse, only works for single arguments\ndef hvp_revrev(f, primals, tangents):\n  x, = primals\n  v, = tangents\n  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n\n\nprint(\"Forward over reverse\")\n%timeit -n10 -r3 hvp(f, (X,), (V,))\nprint(\"Reverse over forward\")\n%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))\nprint(\"Reverse over reverse\")\n%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))\n\nprint(\"Naive full Hessian materialization\")\n%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parameterized 2D Pipelined Matrix Addition in JAX Pallas\nDESCRIPTION: Extends the pipelined matrix addition to support 2D blocking with configurable block sizes. This allows for tuning the pipeline performance by controlling the granularity of the computation blocks and the number of pipeline iterations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef add_matrices_pipelined_2d(\n    x: jax.Array, y: jax.Array, *, bm: int = 256, bn: int = 256\n) -> jax.Array:\n  m, n = x.shape\n  block_spec = pl.BlockSpec((bm, bn), lambda i, j: (i, j))\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(m // bm, n // bn),\n  )(x, y)\n\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=256, bn=256), x + y\n)\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=128, bn=128), x + y\n)\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=512, bn=512), x + y\n)\n```\n\n----------------------------------------\n\nTITLE: Jaxpr Examination Utilities\nDESCRIPTION: Helper functions to inspect and print Jaxpr structure including input vars, output vars, constant vars and equations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef examine_jaxpr(closed_jaxpr):\n  jaxpr = closed_jaxpr.jaxpr\n  print(\"invars:\", jaxpr.invars)\n  print(\"outvars:\", jaxpr.outvars)\n  print(\"constvars:\", jaxpr.constvars)\n  for eqn in jaxpr.eqns:\n    print(\"equation:\", eqn.invars, eqn.primitive, eqn.outvars, eqn.params)\n  print()\n  print(\"jaxpr:\", jaxpr)\n\ndef foo(x):\n  return x + 1\nprint(\"foo\")\nprint(\"=====\")\nexamine_jaxpr(jax.make_jaxpr(foo)(5))\n\nprint()\n\ndef bar(w, b, x):\n  return jnp.dot(w, x) + b + jnp.ones(5), x\nprint(\"bar\")\nprint(\"=====\")\nexamine_jaxpr(jax.make_jaxpr(bar)(jnp.ones((5, 10)), jnp.ones(5), jnp.ones(10)))\n```\n\n----------------------------------------\n\nTITLE: Running JIT-Compiled Function with Static Arguments\nDESCRIPTION: Successfully executes the JIT-compiled function with a static loop bound specified by static_argnums.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ng(jnp.array([1., 2., 3.]), 5)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Neural Network Layer in JAX\nDESCRIPTION: This code defines a basic neural network layer using JAX's jit for compilation. The layer performs a matrix multiplication between inputs and weights, adds a bias term, and applies the sigmoid activation function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef layer(x, weights, bias):\n  return jax.nn.sigmoid(x @ weights + bias)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using an Explicit JAX Symbolic Scope\nDESCRIPTION: Demonstrates creating an explicit `jax.export.SymbolicScope` object and passing it to `jax.export.symbolic_shape` calls. This ensures that symbolic dimensions (`c` and `d`) created using the same explicit scope object can be used together in operations like addition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> my_scope = export.SymbolicScope()\n>>> c, = export.symbolic_shape(\"c\", scope=my_scope)\n>>> d, = export.symbolic_shape(\"d\", scope=my_scope)\n>>> c + d  # Allowed\nd + c\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving JAX Export Calling Convention Version in Python\nDESCRIPTION: Demonstrates how to export a JAX function and check its calling convention version. As of June 2024, all functions are exported with version 9.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from jax import export\n>>> exp: export.Exported = export.export(jnp.cos)(1.)\n>>> exp.calling_convention_version\n9\n```\n\n----------------------------------------\n\nTITLE: Implementing custom_vjp with non-differentiable arguments\nDESCRIPTION: Shows how to use nondiff_argnums with custom_vjp to handle function-valued arguments. Demonstrates the forward and backward pass implementations with non-differentiable arguments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_69\n\nLANGUAGE: python\nCODE:\n```\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef app(f, x):\n  return f(x)\n\ndef app_fwd(f, x):\n  return f(x), x\n\ndef app_bwd(f, x, g):\n  return (5 * g,)\n\napp.defvjp(app_fwd, app_bwd)\n```\n\n----------------------------------------\n\nTITLE: Using the Python Array API Namespace with JAX Arrays\nDESCRIPTION: This Python example demonstrates how to access and use the standard Python Array API namespace associated with a JAX array. The `__array_namespace__()` method returns the namespace object (`nx`), which can then be used to call standard API functions like `nx.sin` and `nx.cos`. Requires `jax.numpy` for creating the input array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.numpy.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n>>> def f(x):\n...   nx = x.__array_namespace__()\n...   return nx.sin(x) ** 2 + nx.cos(x) ** 2\n\n>>> import jax.numpy as jnp\n>>> x = jnp.arange(5)\n>>> f(x).round()\nArray([1., 1., 1., 1., 1.], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Producer-Consumer Synchronization with Barriers in JAX Pallas GPU (Python)\nDESCRIPTION: Demonstrates a producer-consumer pattern using triple buffering and barriers for synchronization between two threads (tid 0 and tid 1) in JAX Pallas GPU. Thread 0 produces items into a shared 'queue' buffer, signaling readiness via the 'produced' barrier array. It waits on the 'consumed' barrier before overwriting buffer slots. Thread 1 waits for items using the 'produced' barrier, consumes them from the 'queue', and signals consumption via the 'consumed' barrier array. This ensures safe concurrent access to the shared buffer.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n```python\ntid = jax.lax.axis_index(\"thread\")\nassert queue.shape == (buffering, *item_shape)\nassert produced.shape == consumed.shape == (buffering,)\n\ndef thread0_body(i, _):\n  slot = jax.lax.rem(i, buffering)\n  @pl.when(i >= buffering)\n  def _await_consumed():\n    plgpu.barrier_wait(consumed.at[slot])  # Wait for consumption of the value before overwriting it\n  # Option 1: Compute the next value\n  queue[slot] = produce()\n  plgpu.barrier_arrive(produced.at[slot])  # Signal the value is ready\n  # Option 2: Produce the value through async_copy\n  # plgpu.copy_gmem_to_smem(..., queue.at[slot], barrier=produced.at[slot])\npl.when(tid == 0)(lambda: jax.lax.fori_loop(0, steps, thread0_body, None))\n\ndef thread1_body(i, _):\n  slot = jax.lax.rem(i, buffering)\n  plgpu.barrier_wait(produced.at[slot])  # Wait for the value to be ready\n  consume(queue[slot])  # Load and compute\n  plgpu.barrier_arrive(consumed.at[slot])  # Signal that the value is consumed\npl.when(tid == 1)(lambda: jax.lax.fori_loop(0, steps, thread1_body, None))\n```\n```\n\n----------------------------------------\n\nTITLE: Custom JVP with Multiple Arguments\nDESCRIPTION: Demonstrates how to define a custom JVP rule for a function with multiple arguments. The example shows how to handle multiple primal inputs and their corresponding tangents.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x, y):\n  return x ** 2 * y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primal_out = f(x, y)\n  tangent_out = 2 * x * y * x_dot + x ** 2 * y_dot\n  return primal_out, tangent_out\n```\n\n----------------------------------------\n\nTITLE: Analyzing JAX Expression (jaxpr) of log1pexp Gradient\nDESCRIPTION: Using make_jaxpr to inspect the JAX expression for the gradient computation, revealing the source of numerical instability.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import make_jaxpr\n\nmake_jaxpr(grad(log1pexp))(100.)\n```\n\n----------------------------------------\n\nTITLE: Defining the JaxprBuilder and JAXPR Construction Logic (Python)\nDESCRIPTION: Defines the JaxprBuilder class, which tracks variables, tracers, constants, and equations (eqns) encountered while building a JAXPR program. The class supports methods for adding tracers, equations, variables, and constants, and constructing a complete JAXPR object alongside its top-level constants. It requires supporting JAX datastructures (Var, JaxprTracer, etc) and helper functions (unzip2, typecheck_jaxpr, _inline_literals), and is essential for dynamically tracing Python computations into Jaxpr form. Inputs and outputs are lists of tracers corresponding to computation arguments and results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nclass JaxprBuilder:\n  eqns: list[JaxprEqn]\n  tracer_to_var: dict[int, Var]\n  const_tracers: dict[int, JaxprTracer]\n  constvals: dict[Var, Any]\n  tracers: list[JaxprTracer]\n\n  def __init__(self):\n    self.eqns = []\n    self.tracer_to_var = {}\n    self.const_tracers = {}\n    self.constvals = {}\n    self.tracers = []\n\n  def new_tracer(self, trace: JaxprTrace, aval: ShapedArray) -> JaxprTracer:\n    tracer = JaxprTracer(trace, aval)\n    self.tracers.append(tracer)\n    return tracer\n\n  def add_eqn(self, eqn: JaxprEqn) -> None:\n    self.eqns.append(eqn)\n\n  def add_var(self, tracer: JaxprTracer) -> Var:\n    assert id(tracer) not in self.tracer_to_var\n    var = self.tracer_to_var[id(tracer)] = Var(tracer.aval)\n    return var\n\n  def getvar(self, tracer: JaxprTracer) -> Var:\n    var = self.tracer_to_var.get(id(tracer))\n    assert var is not None\n    return var\n\n  def add_const(self, tracer: JaxprTracer, val: Any) -> Var:\n    var = self.add_var(tracer)\n    self.const_tracers[id(val)] = tracer\n    self.constvals[var] = val\n    return var\n\n  def build(self, in_tracers: list[JaxprTracer], out_tracers: list[JaxprTracer]\n            ) -> tuple[Jaxpr, list[Any]]:\n    constvars, constvals = unzip2(self.constvals.items())\n    t2v = lambda t: self.tracer_to_var[id(t)]\n    in_binders = constvars + [t2v(t) for t in in_tracers]\n    out_vars = [t2v(t) for t in out_tracers]\n    jaxpr = Jaxpr(in_binders, self.eqns, out_vars)\n    typecheck_jaxpr(jaxpr)\n    jaxpr, constvals = _inline_literals(jaxpr, constvals)\n    return jaxpr, constvals\n```\n\n----------------------------------------\n\nTITLE: Using Name-Based Exclusion Policy in JAX Checkpoint\nDESCRIPTION: Applies a checkpoint policy that saves all named values except for a specific layer output. This shows how to selectively exclude certain named values from being saved.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nloss_checkpoint2 = jax.checkpoint(loss, policy=jax.checkpoint_policies.save_any_names_but_these('layer1_output'))\nprint_saved_residuals(loss_checkpoint2, params, x, y)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Explicit Sharding Mesh\nDESCRIPTION: Creates and sets an explicit sharding mesh using jax.make_mesh and set_mesh functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmesh = jax.make_mesh((2, 4), (\"X\", \"Y\"),\n                     axis_types=(AxisType.Explicit, AxisType.Explicit))\nset_mesh(mesh)\n\nprint(f\"Current mesh is: {get_abstract_mesh()}\")\n```\n\n----------------------------------------\n\nTITLE: Running Uncompiled Conditional Function\nDESCRIPTION: Executes the conditional function without JIT compilation to show normal behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nf(2)\n```\n\n----------------------------------------\n\nTITLE: Using io_callback in JAX with Side-Effectful Host Function - Python\nDESCRIPTION: Demonstrates the use of jax.experimental.io_callback to call a function with host-side side effects (numpy RNG, printing) from within JAX-jitted code. Dependencies: jax, jax.numpy, numpy. Inputs include an array x; outputs a randomly filled array of same shape and dtype as x. The callback function modifies global RNG state and prints diagnostics, thus not pure. Intended for demonstration, not recommended for random number generation in JAX production code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import io_callback\nfrom functools import partial\n\nglobal_rng = np.random.default_rng(0)\n\ndef host_side_random_like(x):\n  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n  # We have two side-effects here:\n  # - printing the shape and dtype\n  # - calling global_rng, thus updating its state\n  print(f'generating {x.dtype}{list(x.shape)}')\n  return global_rng.uniform(size=x.shape).astype(x.dtype)\n\n@jax.jit\ndef numpy_random_like(x):\n  return io_callback(host_side_random_like, x, x)\n\nx = jnp.zeros(5)\nnumpy_random_like(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing XLA Translation for JAX cond Primitive\nDESCRIPTION: Defines the translation of cond primitive to XLA's If operation. Creates corresponding IR blocks for true and false branches and populates them with the translated jaxpr subcomputations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_85\n\nLANGUAGE: python\nCODE:\n```\ndef cond_translation(c, in_avals, out_avals, in_vals, *, true_jaxpr, false_jaxpr):\n  del in_avals  # Unused\n  pred, *in_vals = in_vals\n\n  op = hlo.IfOp([aval_to_ir_type(aval) for aval in out_avals], pred)\n  with ir.InsertionPoint(op.true_branch.blocks.append()):\n    hlo.return_(jaxpr_subcomp(c, true_jaxpr, in_vals))\n  with ir.InsertionPoint(op.false_branch.blocks.append()):\n    hlo.return_(jaxpr_subcomp(c, false_jaxpr, in_vals))\n  return op.results\n\nhlo_translations[cond_p] = cond_translation\n```\n\n----------------------------------------\n\nTITLE: Illustrating JIT Behavior Change with Impure Functions in Python\nDESCRIPTION: This code demonstrates how jit changes the behavior of functions that use global state or have side-effects. The example shows a function that prints a global variable and adds it to the input, producing different results when jitted.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ny = 0\n\n# @jit   # Different behavior with jit\ndef impure_func(x):\n  print(\"Inside:\", y)\n  return x + y\n\nfor y in range(3):\n  print(\"Result:\", impure_func(y))\n```\n\n----------------------------------------\n\nTITLE: Preparing Haiku Models for jax2tf Conversion in Python\nDESCRIPTION: Outlines the steps to prepare a Haiku model for jax2tf conversion. It involves defining the model (`model_fn`), transforming it using `hk.transform` to obtain `init` and `apply` functions, training the model to get the final `params` (starting from `net.init()`), and defining the `predict_fn` using `hk.without_apply_rng(net).apply`. This resulting `predict_fn` accepts parameters and input, fitting the required structure for conversion and saving with jax2tf.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\nmodel_fn = ...define your Haiku model...\nnet = hk.transform(model_fn)  # get a pair of init and apply functions\nparams = ... train your model starting from net.init() ...\npredict_fn = hk.without_apply_rng(net).apply\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward-Mode Automatic Differentiation in Python\nDESCRIPTION: Defines a DualNumber class to represent primal-tangent pairs and implements forward-mode AD rules for addition and multiplication operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\n@dataclass\nclass DualNumber:\n  primal  : float\n  tangent : float\n\ndef add_dual(x : DualNumber, y: DualNumber) -> DualNumber:\n  return DualNumber(x.primal + y.primal, x.tangent + y.tangent)\n\ndef mul_dual(x : DualNumber, y: DualNumber) -> DualNumber:\n  return DualNumber(x.primal * y.primal, x.primal * y.tangent + x.tangent * y.primal)\n\ndef foo_dual(x : DualNumber) -> DualNumber:\n  return mul_dual(x, add_dual(x, DualNumber(3.0, 0.0)))\n\nprint(foo_dual(DualNumber(2.0, 1.0)))\n```\n\n----------------------------------------\n\nTITLE: Defining and Applying a Custom VJP Rule - JAX - Python\nDESCRIPTION: This snippet demonstrates the legacy use of JAX's `@jax.custom_transforms` decorator to define a function with a custom vector-Jacobian product (VJP, i.e., custom backward gradient). The custom VJP rule is associated using `jax.defvjp_all`, allowing custom behavior during differentiation. It illustrates the issue where vectorizing the function with `vmap` removes the custom transformation, leading to inconsistent gradient results. Dependencies: JAX, NumPy (for `np.ones`). Parameters are the differentiable input `x`. Inputs: scalar or array; Outputs: gradient or vector of gradients. The key limitation is the semantic mismatch between transformations and differentiation in the old API.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/2026-custom-derivatives.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# old custom_transforms api to be replaced\n@jax.custom_transforms\ndef f(x):\n  return 2. * x\n\n# f_vjp :: a -> (b, CT b --o CT a)\ndef f_vjp(x):\n  return f(x), lambda g: 3. * x  # 3 instead of 2\n\njax.defvjp_all(f, f_vjp)\n\ngrad(f)(1.)  # 3.\nvmap(grad(f))(np.ones(4))  # [3., 3., 3., 3.]\ngrad(lambda x: vmap(f)(x).sum())(np.ones(4))  # [2., 2., 2., 2.]\n```\n\n----------------------------------------\n\nTITLE: Basic AOT Compilation Process Example in Python with JAX\nDESCRIPTION: Demonstrates the complete AOT compilation process using JAX, including tracing, lowering to HLO, compiling, and executing a simple function that multiplies one argument by 2 and adds the second argument.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/aot.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n\n>>> def f(x, y): return 2 * x + y\n>>> x, y = 3, 4\n\n>>> traced = jax.jit(f).trace(x, y)\n\n>>> # Print the specialized, staged-out representation (as Jaxpr IR)\n>>> print(traced.jaxpr)\n{ lambda ; a:i32[] b:i32[]. let c:i32[] = mul 2 a; d:i32[] = add c b in (d,) }\n\n>>> lowered = traced.lower()\n\n>>> # Print lowered HLO\n>>> print(lowered.as_text())\nmodule @jit_f attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32> {jax.result_info = \"result\"}) {\n    %c = stablehlo.constant dense<2> : tensor<i32>\n    %0 = stablehlo.multiply %c, %arg0 : tensor<i32>\n    %1 = stablehlo.add %0, %arg1 : tensor<i32>\n    return %1 : tensor<i32>\n  }\n}\n\n>>> compiled = lowered.compile()\n\n>>> # Query for cost analysis, print FLOP estimate\n>>> compiled.cost_analysis()['flops']\n2.0\n\n>>> # Execute the compiled function!\n>>> compiled(x, y)\nArray(10, dtype=int32, weak_type=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing an Iota Kernel with Program ID in Pallas\nDESCRIPTION: A Pallas kernel that uses program_id to generate an array of sequential integers, demonstrating grid usage.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef iota_kernel(o_ref):\n  i = pl.program_id(0)\n  o_ref[i] = i\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JAX's Asynchronous Dispatch with Matrix Operations\nDESCRIPTION: This snippet demonstrates how JAX uses asynchronous dispatch when performing matrix operations. It shows how JAX immediately returns control to Python after operations like matrix multiplication, returning a future (jax.Array) that represents the result that will be computed on the device.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/async_dispatch.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax import random\nx = random.uniform(random.key(0), (1000, 1000))\n# Printing the result (i.e. evaluating `repr(result)` or `str(result)`)\n# will block until the value is ready.\njnp.dot(x, x) + 3.\n```\n\n----------------------------------------\n\nTITLE: Generating Arrays of New-Style Typed Keys via vmap (Python)\nDESCRIPTION: Illustrates vectorized creation of multiple typed PRNG keys using jax.vmap over jax.random.key. Useful when batches of independent keys are required for parallel operations. Dependencies: JAX. Input: an array of seed values. Output: an array of key<fry>-typed arrays. Limitation: only works after upgrading to version supporting typed keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> key_arr = jax.vmap(jax.random.key)(jnp.arange(4))\n>>> key_arr\nArray((4,), dtype=key<fry>) overlaying:\n[[0 0]\n [0 1]\n [0 2]\n [0 3]]\n>>> key_arr.shape\n(4,)\n\n```\n\n----------------------------------------\n\nTITLE: Batching FFI Calls with vmap and ffi_call - Python\nDESCRIPTION: These snippets demonstrate how to leverage the 'vmap_method' parameter in jax.ffi.ffi_call to control how an FFI call is batched under vmap. The code includes both 'broadcast_all' (for parallel/batched operation) and 'sequential' (fallback via lax.scan) examples. A reference equivalence assertion checks correct behavior. Inputs are batched arrays, outputs are batched-normalized results. Dependencies: JAX, a registered FFI handler, and numpy. Limitations arise if batch semantics are unsupported by the underlying foreign function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nnp.testing.assert_allclose(jax.vmap(rms_norm)(x), jax.vmap(rms_norm_ref)(x), rtol=1e-5)\n```\n\nLANGUAGE: python\nCODE:\n```\njax.make_jaxpr(jax.vmap(rms_norm))(x)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef rms_norm_sequential(x, eps=1e-5):\\n  return jax.ffi.ffi_call(\\n    \"rms_norm\",\\n    jax.ShapeDtypeStruct(x.shape, x.dtype),\\n    vmap_method=\"sequential\",\\n  )(x, eps=np.float32(eps))\\n\\n\\njax.make_jaxpr(jax.vmap(rms_norm_sequential))(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Common Transpose Operations in Python\nDESCRIPTION: Examples of transposition operations for common primitives like addition and multiplication, showing how cotangents are handled for different argument types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n add_transpose(out_ct, _, _) = (out_ct, out_ct)\n mult_transpose(out_ct, x, _) = (None, x * out_ct)\n mult_transpose(out_ct, _, y) = (out_ct * y, None)\n```\n\n----------------------------------------\n\nTITLE: Complex Array Operations in JAX\nDESCRIPTION: This snippet demonstrates more complex array operations in JAX, including matrix multiplication, element-wise multiplication, and advanced indexing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(jnp.dot(x, x.T))\n\nprint(jnp.dot(x, 2 * x)[[0, 2, 1, 0], ..., None, ::-1])\n```\n\n----------------------------------------\n\nTITLE: Defining Abstract Evaluation Rules for JAX Primitives (Python)\nDESCRIPTION: Establishes Python functions for the abstract evaluation rules of various JAX primitives (e.g., add, multiply, vectorized unary ops, comparisons). These rules guide how input types map to output types at the tracing/compilation stage, with signature checks and expected JAX types (ShapedArray). The functions depend on matching array shapes/dtypes and are registered in the abstract_eval_rules dictionary for later lookup, ensuring all input checks are performed at compile-time.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef binop_abstract_eval(x: ShapedArray, y: ShapedArray) -> list[ShapedArray]:\n  if not isinstance(x, ShapedArray) or not isinstance(y, ShapedArray):\n    raise TypeError\n  if raise_to_shaped(x) != raise_to_shaped(y): raise TypeError\n  return [ShapedArray(x.shape, x.dtype)]\n\nabstract_eval_rules[add_p] = binop_abstract_eval\nabstract_eval_rules[mul_p] = binop_abstract_eval\n\ndef compare_abstract_eval(x: ShapedArray, y: ShapedArray) -> list[ShapedArray]:\n  if not isinstance(x, ShapedArray) or not isinstance(y, ShapedArray):\n    raise TypeError\n  if x.shape != y.shape: raise TypeError\n  return [ShapedArray(x.shape, np.dtype('bool'))]\nabstract_eval_rules[greater_p] = compare_abstract_eval\nabstract_eval_rules[less_p] = compare_abstract_eval\n\ndef vectorized_unop_abstract_eval(x: ShapedArray) -> list[ShapedArray]:\n  return [ShapedArray(x.shape, x.dtype)]\n\nabstract_eval_rules[sin_p] = vectorized_unop_abstract_eval\nabstract_eval_rules[cos_p] = vectorized_unop_abstract_eval\nabstract_eval_rules[neg_p] = vectorized_unop_abstract_eval\n\ndef reduce_sum_abstract_eval(x: ShapedArray, *, axis: tuple[int, ...]\n                             ) -> list[ShapedArray]:\n  axis_ = set(axis)\n  new_shape = [d for i, d in enumerate(x.shape) if i not in axis_]\n  return [ShapedArray(tuple(new_shape), x.dtype)]\nabstract_eval_rules[reduce_sum_p] = reduce_sum_abstract_eval\n\ndef broadcast_abstract_eval(x: ShapedArray, *, shape: Sequence[int],\n                            axes: Sequence[int]) -> list[ShapedArray]:\n  return [ShapedArray(tuple(shape), x.dtype)]\nabstract_eval_rules[broadcast_p] = broadcast_abstract_eval\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Python scalar multiplication with JAX arrays\nDESCRIPTION: This example shows how Python scalar literals (which are weakly-typed) interact with JAX arrays without causing unwanted type promotion, allowing multiplication of a Python scalar with an int8 array while preserving the array's type.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> x = jnp.arange(5, dtype='int8')\n>>> 2 * x\nArray([0, 2, 4, 6, 8], dtype=int8)\n```\n\n----------------------------------------\n\nTITLE: JIT-Compiling a Function with Reduction\nDESCRIPTION: Shows how to JIT-compile a function that uses reduce_sum to perform array reduction.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  return reduce_sum(x, axis=0)\n\nprint(f(np.array([1., 2., 3.])))\n```\n\n----------------------------------------\n\nTITLE: Using vjp with Custom VJP Functions\nDESCRIPTION: Demonstrates using the vjp function with a custom VJP rule. The example shows how to compute the primal output and VJP function separately.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n\ny, f_vjp = vjp(f, 3.)\nprint(y)\n```\n\n----------------------------------------\n\nTITLE: Testing Reverse-Mode Autodiff (grad) before Rule Registration in JAX (Python)\nDESCRIPTION: Attempts to compute the gradient with respect to the first argument using JAX's grad function on the custom primitive. Expects a NotImplementedError because the reverse-mode rule is not yet implemented, highlighting that reverse-mode differentiation requires either a VJP rule or full autodiff support. Inputs are floats; output is an exception.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nwith expectNotImplementedError():\n  api.grad(square_add_prim)(2., 10.)\n\n```\n\n----------------------------------------\n\nTITLE: Testing Recursive Checkpointing with 16 Functions\nDESCRIPTION: Shows the logarithmic memory scaling of recursive checkpointing with a longer chain of 16 functions. Demonstrates that memory usage grows slowly as chain length doubles.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nf = recursive_checkpoint([jnp.sin] * 16)\nprint_saved_residuals(f, 3.)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Sharding to Replication with Constraint in JAX\nDESCRIPTION: Displays the sharding of the input array and the output array after applying the JIT-compiled function with a replication constraint. This shows how a sharded array is transformed into a fully replicated array across all devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\njax.debug.visualize_array_sharding(x)\ny = f(x)\njax.debug.visualize_array_sharding(y)\n```\n\n----------------------------------------\n\nTITLE: Implementing vmap Transformation API in Python\nDESCRIPTION: Defines the vmap_flat and vmap functions to initiate the batching trace. vmap_flat handles the core batching logic, while vmap provides a user-friendly interface for applying vectorized batching to functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\ndef vmap_flat(f, in_axes, *args):\n  axis_size, = {x.shape[ax] for x, ax in zip(args, in_axes)\n                if ax is not not_mapped}\n  with new_main(BatchTrace, axis_size) as main:\n    trace = BatchTrace(main)\n    tracers_in = [BatchTracer(trace, x, ax) if ax is not None else x\n                  for x, ax in zip(args, in_axes)]\n    outs = f(*tracers_in)\n    tracers_out = [full_raise(trace, out) for out in outs]\n    vals_out, bdims_out = unzip2((t.val, t.batch_dim) for t in tracers_out)\n  outs_transposed = [move_batch_axis(axis_size, bdim, 0, val_out)\n                     for val_out, bdim in zip(vals_out, bdims_out)]\n  return outs_transposed\n\ndef vmap(f, in_axes):\n  def batched_f(*args):\n    args_flat, in_tree = tree_flatten(args)\n    in_axes_flat, in_tree2 = tree_flatten(in_axes)\n    if in_tree != in_tree2: raise TypeError\n    f_flat, out_tree = flatten_fun(f, in_tree)\n    outs_flat = vmap_flat(f_flat, in_axes_flat, *args_flat)\n    return tree_unflatten(out_tree(), outs_flat)\n  return batched_f\n```\n\n----------------------------------------\n\nTITLE: Implementing Matrix Addition with SRAM in Pallas\nDESCRIPTION: This example demonstrates how to implement matrix addition using SRAM and registers in Pallas. It includes a kernel function that operates on SRAM references and a wrapper function that handles the HBM to SRAM data transfer.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add_matrices_kernel(x_sram_ref, y_sram_ref, z_sram_ref):\n  # Load x and y from SRAM into registers\n  x_regs = x_sram_ref[:, :]\n  y_regs = y_sram_ref[:, :]\n  # Execute a vectorized add\n  z_regs = x_regs + y_regs\n  # Store the output values in registers back into SRAM\n  z_sram_ref[:, :] = z_regs\n\n\ndef add_matrices(x: jax.Array, y: jax.Array) -> jax.Array:\n  # pallas_call will first allocate scratch buffers for `x` and `y` in SRAM.\n  # It will then copy `x` and `y` from HBM into SRAM.\n  z = pl.pallas_call(\n      add_matrices_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)\n  )(x, y)\n  # pallas_call will also copy the output from SRAM back into HBM.\n  return z\n\n\nx, y = jnp.ones((512, 512)), jnp.ones((512, 512))\nadd_matrices(x, y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Manually Batched Log-Joint Function in JAX\nDESCRIPTION: Defines a manually batched log-joint probability function for Bayesian inference using JAX operations. Handles batched inputs by specifying axis parameters and using transpositions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef batched_log_joint(beta):\n    result = 0.\n    # Here (and below) `sum` needs an `axis` parameter. At best, forgetting to set axis\n    # or setting it incorrectly yields an error; at worst, it silently changes the\n    # semantics of the model.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=1.),\n                           axis=-1)\n    # Note the multiple transposes. Getting this right is not rocket science,\n    # but it's also not totally mindless. (I didn't get it right on the first\n    # try.)\n    result = result + jnp.sum(-jnp.log(1 + jnp.exp(-(2*y-1) * jnp.dot(all_x, beta.T).T)),\n                           axis=-1)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Handling Custom Calls and Disabling Safety Checks in jax.export (Python)\nDESCRIPTION: This snippet explains the compatibility checks `jax.export` performs for custom calls. It defines a new JAX primitive `new_prim` backed by a custom call target (`\"my_new_prim\"`) without predefined stability guarantees. It first shows the MLIR generated and then demonstrates that attempting to export this function normally raises a `ValueError`. Finally, it shows how to bypass this specific safety check using `disabled_checks=[export.DisabledSafetyCheck.custom_call(\"my_new_prim\")]` in the `export.export` call, acknowledging the potential compatibility risks. Dependencies include `jax`, `jax.lax`, `jax._src.core`, and `jax._src.interpreters.mlir`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax import export\n>>> from jax import lax\n>>> from jax._src import core\n>>> from jax._src.interpreters import mlir\n>>> # Define a new primitive backed by a custom call\n>>> new_prim = core.Primitive(\"new_prim\")\n>>> _ = new_prim.def_abstract_eval(lambda x: x)\n>>> _ = mlir.register_lowering(new_prim, lambda ctx, o: mlir.custom_call(\"my_new_prim\", operands=[o], result_types=[o.type]).results)\n>>> print(jax.jit(new_prim.bind).lower(1.).compiler_ir())\nmodule @jit_bind attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.custom_call @my_new_prim(%arg0) {api_version = 2 : i32, backend_config = \"\"} : (tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n\n>>> # If we try to export, we get an error\n>>> export.export(jax.jit(new_prim.bind))(1.)  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: Cannot serialize code with custom calls whose targets have no compatibility guarantees: my_new_bind\n\n>>> # We can avoid the error if we pass a `DisabledSafetyCheck.custom_call`\n>>> exp = export.export(\n...    jax.jit(new_prim.bind),\n...    disabled_checks=[export.DisabledSafetyCheck.custom_call(\"my_new_prim\")])(1.)\n```\n\n----------------------------------------\n\nTITLE: Creating a Device Mesh for Sharding Constraints in JAX\nDESCRIPTION: Creates a 2D mesh of devices with dimensions 4×2 and names 'x' and 'y' for use in subsequent examples demonstrating sharding constraints within JIT-compiled functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmesh = jax.make_mesh((4, 2), ('x', 'y'))\n```\n\n----------------------------------------\n\nTITLE: Microbenchmarking JAX vs NumPy Code Using Function, JIT, and Device Transfer - Python\nDESCRIPTION: This snippet shows end-to-end benchmarking of matrix operations using both NumPy and JAX, including device transfer, function JIT compilation, and wall-time measurement. It uses IPython magics to benchmark: `%timeit` and `%time` are used interactively to separately measure execution, device transfer, and compilation time. Inputs are a large array of shape (1000, 1000) and a matrix operation function. Dependencies include NumPy, JAX, and being run in an IPython environment. `%time` and `%timeit` only work in IPython, not in plain Python scripts. Array size affects the meaning of observed runtimes due to JIT and device overhead amortization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport jax.numpy as jnp\nimport jax\n\ndef f(x):  # function we're benchmarking (works in both NumPy & JAX)\n  return x.T @ (x - x.mean(axis=0))\n\nx_np = np.ones((1000, 1000), dtype=np.float32)  # same as JAX default dtype\n%timeit f(x_np)  # measure NumPy runtime\n\n%time x_jax = jax.device_put(x_np)  # measure JAX device transfer time\nf_jit = jax.jit(f)\n%time f_jit(x_jax).block_until_ready()  # measure JAX compilation time\n%timeit f_jit(x_jax).block_until_ready()  # measure JAX runtime\n```\n\n----------------------------------------\n\nTITLE: API for Forward-Mode JVP Transformation - JAX Python\nDESCRIPTION: Implements a function jvp_v1 for forward-mode autodiff. It initializes a tracing context with a JVPTrace, boxes inputs and tangents as JVPTracers, calls the function under trace, and collects primal and tangent outputs. Depends on Trace stack/context management, full_raise, JVPTrace/JVPTracer, and input function f. Parameters are the function, primals, and tangents; outputs are primal and tangent tuples matching inputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef jvp_v1(f, primals, tangents):\\n  with new_main(JVPTrace) as main:\\n    trace = JVPTrace(main)\\n    tracers_in = [JVPTracer(trace, x, t) for x, t in zip(primals, tangents)]\\n    out = f(*tracers_in)\\n    tracer_out = full_raise(trace, out)\\n    primal_out, tangent_out = tracer_out.primal, tracer_out.tangent\\n  return primal_out, tangent_out\n```\n\n----------------------------------------\n\nTITLE: Using jax.debug.breakpoint for Interactive Debugging\nDESCRIPTION: Shows how to use jax.debug.breakpoint to pause execution and inspect values in a compiled JAX function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/print_breakpoint.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  y, z = jnp.sin(x), jnp.cos(x)\n  jax.debug.breakpoint()\n  return y * z\nf(2.) # ==> Pauses during execution!\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using JAX Transfer Guard in Python\nDESCRIPTION: This Python snippet demonstrates how to set the transfer guard in JAX using both a global configuration update and a context manager. It creates several JAX arrays and shows the effect of different transfer guard levels on printing (implicit transfer) and device fetch (explicit transfer). The context manager restricts implicit transfers and only allows explicit transfers. Required dependencies are the JAX library and its numpy wrapper (jnp). Inputs are simple scalar arrays, and the output is the printed value or a message when a transfer is blocked. Limitations include that transfer guards do not affect explicit device_get calls unless set explicitly to disallow_explicit, and exceptions are raised when an operation is not permitted.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/transfer_guard.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> jax.config.update(\"jax_transfer_guard\", \"allow\")  # This is default.\n>>>\n>>> x = jnp.array(1)\n>>> y = jnp.array(2)\n>>> z = jnp.array(3)\n>>>\n>>> print(\"x\", x)  # All transfers are allowed.\nx 1\n>>> with jax.transfer_guard(\"disallow\"):\n...   print(\"x\", x)  # x has already been fetched into the host.\n...   print(\"y\", jax.device_get(y))  # Explicit transfers are allowed.\n...   try:\n...     print(\"z\", z)  # Implicit transfers are disallowed.\n...     assert False, \"This line is expected to be unreachable.\"\n...   except:\n...     print(\"z could not be fetched\")  # doctest: +SKIP\nx 1\ny 2\nz could not be fetched\n```\n\n----------------------------------------\n\nTITLE: Applying JAX Grad to a Pallas Call (Illustrative)\nDESCRIPTION: Illustrates applying the `jax.grad` transformation externally to a `pallas_call` function, enabling automatic differentiation. This relies on JAX's JVP and transpose rules but can have performance implications due to memory access patterns in the transposed kernel. `jax.custom_vjp` provides an escape hatch.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ngrad(pallas_call)\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Distributed Partitioning\nDESCRIPTION: Sets up JAX mesh and sharding configuration for distributed computing. Defines a partition strategy and creates a mesh across multiple devices with named sharding.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npartition = P(None, 'x')\nmesh = jax.make_mesh((num_devices,), ('x',))\nsharding = jax.sharding.NamedSharding(mesh, partition)\n\n# We need a block size of (16, 128) to ensure that a half-slice is at least\n# of size (8, 128), which is the size of a VREG. This makes tiling easier\n```\n\n----------------------------------------\n\nTITLE: Using scan with Ordered io_callback in JAX - Python\nDESCRIPTION: Demonstrates that ordered io_callback functions are compatible with lax.scan. The body_fun applies the ordered io_callback per step, returning collected results. Works irrespective of the ordering property, and avoids the vmap ordering conflict. Dependencies: jax.lax.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef body_fun(_, x):\n  return _, numpy_random_like_ordered(x)\njax.lax.scan(body_fun, None, jnp.arange(5.0))[1]\n```\n\n----------------------------------------\n\nTITLE: io_callback Ignored by Compiler If Unused Output During Differentiation - Python\nDESCRIPTION: Demonstrates that if io_callback does not depend on a differentiated variable, it executes even during autodiff, and the callback's output is unused in further computation. Useful for side-effect-only callbacks such as debug printing inside transformed functions. Dependencies: jax.jit.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  io_callback(lambda: print('hello'), None)\n  return x\n\njax.grad(f)(1.0);\n```\n\n----------------------------------------\n\nTITLE: Initializing an Evaluation Interpreter for Primitives - JAX Python\nDESCRIPTION: Defines the EvalTrace class for primitive evaluation with no boxing (pure/lift return the value unchanged), registers evaluation rules for various primitives in a dictionary, and adds an instance of the bottom-level interpreter to the stack. Dependencies include Trace, MainTrace, primitive symbols like add_p, and numpy. Parameters to primitive rules align with operator arities; outputs are numpy operation results in lists. The code is a base for evaluation without autodiff or transformation logic.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass EvalTrace(Trace):\\n  pure = lift = lambda self, x: x  # no boxing in Tracers needed\\n\\n  def process_primitive(self, primitive, tracers, params):\\n    return impl_rules[primitive](*tracers, **params)\\n\\ntrace_stack.append(MainTrace(0, EvalTrace, None))  # special bottom of the stack\\n\\n# NB: in JAX, instead of a dict we attach impl rules to the Primitive instance\\nimpl_rules = {}\\n\\nimpl_rules[add_p] = lambda x, y: [np.add(x, y)]\\nimpl_rules[mul_p] = lambda x, y: [np.multiply(x, y)]\\nimpl_rules[neg_p] = lambda x: [np.negative(x)]\\nimpl_rules[sin_p] = lambda x: [np.sin(x)]\\nimpl_rules[cos_p] = lambda x: [np.cos(x)]\\nimpl_rules[reduce_sum_p] = lambda x, *, axis: [np.sum(x, axis)]\\nimpl_rules[greater_p] = lambda x, y: [np.greater(x, y)]\\nimpl_rules[less_p] = lambda x, y: [np.less(x, y)]\\nimpl_rules[transpose_p] = lambda x, *, perm: [np.transpose(x, perm)]\\n\\ndef broadcast_impl(x, *, shape, axes):\\n  for axis in sorted(axes):\\n    x = np.expand_dims(x, axis)\\n  return [np.broadcast_to(x, shape)]\\nimpl_rules[broadcast_p] = broadcast_impl\n```\n\n----------------------------------------\n\nTITLE: Implementing custom_vjp with pytree containers\nDESCRIPTION: Shows how to implement the same functionality using JAX's custom_vjp decorator instead of custom_jvp. The example defines forward and backward passes that work with the same pytree structure.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_60\n\nLANGUAGE: python\nCODE:\n```\n@custom_vjp\ndef f(pt):\n  x, y = pt.x, pt.y\n  return {'a': x ** 2,\n          'b': (jnp.sin(x), jnp.cos(y))}\n\ndef f_fwd(pt):\n  return f(pt), pt\n\ndef f_bwd(pt, g):\n  a_bar, (b0_bar, b1_bar) = g['a'], g['b']\n  x_bar = 2 * pt.x * a_bar + jnp.cos(pt.x) * b0_bar\n  y_bar = -jnp.sin(pt.y) * b1_bar\n  return (Point(x_bar, y_bar),)\n\nf.defvjp(f_fwd, f_bwd)\n\ndef fun(pt):\n  dct = f(pt)\n  return dct['a'] + dct['b'][0]\n```\n\n----------------------------------------\n\nTITLE: Side Effects in Custom JVP Functions\nDESCRIPTION: Demonstrates that side effects in custom JVP functions only execute when the corresponding functions are called. The example shows when the original function and the custom JVP rule are invoked.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x):\n  print('called f!')  # a harmless side-effect\n  return jnp.sin(x)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  print('called f_jvp!')  # a harmless side-effect\n  x, = primals\n  t, = tangents\n  return f(x), jnp.cos(x) * t\n```\n\n----------------------------------------\n\nTITLE: Defining JAX Pallas Matmul Kernel and Wrapper\nDESCRIPTION: Defines a Pallas kernel `matmul_kernel` for matrix multiplication on TPUs. The kernel initializes the output block `z_ref` to zeros only for the first iteration along the reduction dimension (k) and then performs the block matrix multiplication `x_ref[...] @ y_ref[...]`. A wrapper function `matmul` uses `pallas_call` to launch this kernel, specifying input/output shapes, block specifications (`BlockSpec`) for memory access patterns, the execution grid, and TPU-specific compiler parameters (`dimension_semantics`) to manage parallelism.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_kernel(x_ref, y_ref, z_ref):\n  @pl.when(pl.program_id(2) == 0)\n  def _():\n    z_ref[...] = jnp.zeros_like(z_ref)\n\n  z_ref[...] += x_ref[...] @ y_ref[...]\n\ndef matmul(\n    x: jax.Array,\n    y: jax.Array,\n    *,\n    bm: int = 128,\n    bk: int = 128,\n    bn: int = 128,\n):\n  m, k = x.shape\n  _, n = y.shape\n  return pl.pallas_call(\n      matmul_kernel,\n      out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\n      in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)),\n                pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))],\n      out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n      grid=(m // bm, n // bn, k // bk),\n      compiler_params=pltpu.TPUCompilerParams(\n          dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n  )(x, y)\n```\n\n----------------------------------------\n\nTITLE: Performing Matrix Multiplication with JAX\nDESCRIPTION: Demonstrates matrix multiplication by creating a random 3000×3000 matrix, computing its dot product with its transpose, and calculating the mean value. This tests JAX's numerical computation capabilities.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_cpu.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\n\n# matrix multiplication on GPU\nkey = jax.random.PRNGKey(0)\nx = jax.random.normal(key, (3000, 3000))\nresult = jax.numpy.dot(x, x.T).mean()\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Debugging Gradients with Custom VJP\nDESCRIPTION: Shows how to use custom_vjp with pdb to insert a debugger trace in the backward pass. This technique is useful for debugging gradient computation in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nimport pdb\n\n@custom_vjp\ndef debug(x):\n  return x  # acts like identity\n\ndef debug_fwd(x):\n  return x, x\n\ndef debug_bwd(x, g):\n  pdb.set_trace()\n  return g\n\ndebug.defvjp(debug_fwd, debug_bwd)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Forward/Backward Computation with Checkpointing Policy\nDESCRIPTION: Using the visualization utility to show JAX's forward and backward computation graphs with a dots-saving checkpoint policy applied, demonstrating the impact on computation pattern.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Using `jax.checkpoint` with policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable:\nprint_fwd_bwd(f3, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Arrays with Device Mesh\nDESCRIPTION: Create a distributed array by first defining a device mesh and then using jax.device_put with NamedSharding to distribute data across devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.sharding import PartitionSpec as P, NamedSharding\n\nmesh = jax.make_mesh((4, 2), ('x', 'y'))\nx = jax.random.normal(jax.random.key(0), (8192, 8192))\ny = jax.device_put(x, NamedSharding(mesh, P('x', 'y')))\n```\n\n----------------------------------------\n\nTITLE: Exporting a Function with Static and Symbolic Dimension Variables in JAX\nDESCRIPTION: Demonstrates how to export a function with both static and symbolic dimension variables, showing the differences in approach and potential errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> def my_top_k(k, x):  # x: i32[4, 10], k <= 10\n...   return lax.top_k(x, k)[0]  # : i32[4, 3]\n>>> x = np.arange(40, dtype=np.int32).reshape((4, 10))\n\n>>> # Export with static `k=3`. Since `k` appears in shapes it must be in `static_argnums`.\n>>> exp_static_k = export.export(jax.jit(my_top_k, static_argnums=0))(3, x)\n>>> exp_static_k.in_avals[0]\nShapedArray(int32[4,10])\n\n>>> exp_static_k.out_avals[0]\nShapedArray(int32[4,3])\n\n>>> # When calling the exported function we pass only the non-static arguments\n>>> exp_static_k.call(x)\nArray([[ 9,  8,  7],\n       [19, 18, 17],\n       [29, 28, 27],\n       [39, 38, 37]], dtype=int32)\n\n>>> # Now attempt to export with symbolic `k` so that we choose `k` after export.\n>>> k, = export.symbolic_shape(\"k\", constraints=[\"k <= 10\"])\n>>> export.export(jax.jit(my_top_k, static_argnums=0))(k, x)  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nUnexpectedDimVar: \"Encountered dimension variable 'k' that is not appearing in the shapes of the function arguments\n\n```\n\n----------------------------------------\n\nTITLE: Collective Communication with psum\nDESCRIPTION: Demonstrates collective communication using psum to normalize values across devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\n\n@partial(pmap, axis_name='i')\ndef normalize(x):\n  return x / lax.psum(x, 'i')\n\nprint(normalize(jnp.arange(4.)))\n```\n\n----------------------------------------\n\nTITLE: Implementing Correct Sum Reduction in Pallas (Python)\nDESCRIPTION: This snippet demonstrates the correct implementation of sum reduction using Pallas. It initializes the output array to zeros on the first iteration using pl.when and pl.program_id.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef sum_kernel(x_ref, o_ref):\n  @pl.when(pl.program_id(axis=0) == 0)\n  def _():\n    o_ref[...] = jnp.zeros_like(o_ref)\n\n  o_ref[...] += x_ref[...]\n\ndef sum(x: jax.Array) -> jax.Array:\n  grid, *out_shape = x.shape\n  return pl.pallas_call(\n      sum_kernel,\n      grid=grid,\n      # None in `block_shape` means we pick a size of 1 and squeeze it away\n      in_specs=[pl.BlockSpec((None, *out_shape), lambda i: (i, 0, 0))],\n      out_specs=pl.BlockSpec(out_shape, lambda i: (0, 0)),\n      out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype)\n  )(x)\n\nsum(x)\n```\n\n----------------------------------------\n\nTITLE: Detecting Typed vs. Raw PRNG Keys via issubdtype (Python)\nDESCRIPTION: Shows how to differentiate between new-style (typed) keys and raw (old-style) keys using jax.dtypes.issubdtype. Input: key object; Output: Boolean indicating if dtype matches prng_key subdtype. Useful for library authors or validation code that may wish to enforce the use of typed keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> typed_key = jax.random.key(0)\n>>> jax.dtypes.issubdtype(typed_key.dtype, jax.dtypes.prng_key)\nTrue\n>>> raw_key = jax.random.PRNGKey(0)\n>>> jax.dtypes.issubdtype(raw_key.dtype, jax.dtypes.prng_key)\nFalse\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Sharded Loss Function with pmean in JAX\nDESCRIPTION: This snippet defines a sample loss function using `shmap` for batch data parallelism across 8 devices. It takes parameters and a sharded batch as input. Inside, it computes predictions, calculates a local loss, and then uses `lax.pmean` (which utilizes `psum`) across the 'batch' mesh axis to compute a global loss, which is returned as an unmapped output (`out_specs=P()`). This setup is characteristic of the scenario leading to inefficient transposes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\ndevices = jax.devices()  # 8 devices\n\n@partial(shmap, mesh=Mesh(devices, ('batch',)),\n         in_specs=(P(None, None), P('batch', None)),\n         out_specs=P())\ndef loss(params, batch):\n  inputs, targets = batch\n  predictions = predict(params, inputs)\n  local_loss = jnp.mean(jnp.sum(predictions - targets, -1))\n  global_loss = lax.pmean(local_loss, 'batch'))\n  return global_loss\n```\n```\n\n----------------------------------------\n\nTITLE: Defining JVP Interpreter and Higher-Order Derivative Functions in Python\nDESCRIPTION: This snippet defines the core components for forward-mode automatic differentiation (JVP). `TaggedDualNumber` represents a number with its primal and tangent values. `JVPInterpreter` implements the JVP rules for operations like `add` and `mul`, handling nested interpreters for higher-order derivatives. Functions `jvp`, `derivative`, and `nth_order_derivative` provide APIs to compute derivatives. An initial test `jvp(foo, 2.0, 1.0)` demonstrates its usage. Dependencies include `dataclasses`, a base `Interpreter` class, an `Op` enum, context management functions (`set_interpreter`, `current_interpreter`), and dispatching functions (`add`, `mul`).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# in higher order differentiation.\n@dataclass\nclass TaggedDualNumber:\n  interpreter : Interpreter\n  primal  : float\n  tangent : float\n\nclass JVPInterpreter(Interpreter):\n  def __init__(self, prev_interpreter: Interpreter):\n    # We keep a pointer to the interpreter that was current when this\n    # interpreter was first invoked. That's the context in which our\n    # rules should run.\n    self.prev_interpreter = prev_interpreter\n\n  def interpret_op(self, op, args):\n    args = tuple(self.lift(arg) for arg in args)\n    with set_interpreter(self.prev_interpreter):\n      match op:\n        case Op.add:\n          # Notice that we use `add` and `mul` here, which are the\n          # interpreter-dispatching functions defined earlier.\n          x, y = args\n          return self.dual_number(\n              add(x.primal, y.primal),\n              add(x.tangent, y.tangent))\n\n        case Op.mul:\n          x, y = args\n          x = self.lift(x)\n          y = self.lift(y)\n          return self.dual_number(\n              mul(x.primal, y.primal),\n              add(mul(x.primal, y.tangent), mul(x.tangent, y.primal)))\n\n  def dual_number(self, primal, tangent):\n    return TaggedDualNumber(self, primal, tangent)\n\n  # Lift a constant value (constant with respect to this interpreter) to\n  # a TaggedDualNumber.\n  def lift(self, x):\n    if isinstance(x, TaggedDualNumber) and x.interpreter is self:\n      return x\n    else:\n      return self.dual_number(x, 0.0)\n\ndef jvp(f, primal, tangent):\n  jvp_interpreter = JVPInterpreter(current_interpreter)\n  dual_number_in = jvp_interpreter.dual_number(primal, tangent)\n  with set_interpreter(jvp_interpreter):\n    result = f(dual_number_in)\n  dual_number_out = jvp_interpreter.lift(result)\n  return dual_number_out.primal, dual_number_out.tangent\n\n# Let's try it out:\nprint(jvp(foo, 2.0, 1.0))\n\n# Because we were careful to consider nesting interpreters, higher-order AD\n# works out of the box:\n\ndef derivative(f, x):\n  _, tangent = jvp(f, x, 1.0)\n  return tangent\n\ndef nth_order_derivative(n, f, x):\n  if n == 0:\n    return f(x)\n  else:\n    return derivative(lambda x: nth_order_derivative(n-1, f, x), x)\n```\n\n----------------------------------------\n\nTITLE: Registering JVP Rules for Common Primitives - JAX Python\nDESCRIPTION: Defines JVP (Jacobian-Vector Product) rules for several common mathematical primitives (add, mul, sin, cos, neg, reduce_sum, greater, less). Each rule takes a tuple of primals and tangents, computes the transformed output and its derivative according to calculus rules, and registers itself in the global jvp_rules dict. Dependencies include access to the core primitives and helper functions such as zeros_like. Key for enabling symbolic differentiation of user functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef add_jvp(primals, tangents):\\n  (x, y), (x_dot, y_dot) = primals, tangents\\n  return [x + y], [x_dot + y_dot]\\njvp_rules[add_p] = add_jvp\\n\\ndef mul_jvp(primals, tangents):\\n  (x, y), (x_dot, y_dot) = primals, tangents\\n  return [x * y], [x_dot * y + x * y_dot]\\njvp_rules[mul_p] = mul_jvp\\n\\ndef sin_jvp(primals, tangents):\\n  (x,), (x_dot,) = primals, tangents\\n  return [sin(x)], [cos(x) * x_dot]\\njvp_rules[sin_p] = sin_jvp\\n\\ndef cos_jvp(primals, tangents):\\n  (x,), (x_dot,) = primals, tangents\\n  return [cos(x)], [-sin(x) * x_dot]\\njvp_rules[cos_p] = cos_jvp\\n\\ndef neg_jvp(primals, tangents):\\n  (x,), (x_dot,) = primals, tangents\\n  return [neg(x)], [neg(x_dot)]\\njvp_rules[neg_p] = neg_jvp\\n\\ndef reduce_sum_jvp(primals, tangents, *, axis):\\n  (x,), (x_dot,) = primals, tangents\\n  return [reduce_sum(x, axis)], [reduce_sum(x_dot, axis)]\\njvp_rules[reduce_sum_p] = reduce_sum_jvp\\n\\ndef greater_jvp(primals, tangents):\\n  (x, y), _ = primals, tangents\\n  out_primal = greater(x, y)\\n  return [out_primal], [zeros_like(out_primal)]\\njvp_rules[greater_p] = greater_jvp\\n\\ndef less_jvp(primals, tangents):\\n  (x, y), _ = primals, tangents\\n  out_primal = less(x, y)\\n  return [out_primal], [zeros_like(out_primal)]\\njvp_rules[less_p] = less_jvp\n```\n\n----------------------------------------\n\nTITLE: Serializing a JAX Function with Reverse-Mode AD Support using jax.export (Python)\nDESCRIPTION: This example demonstrates serializing a JAX function while including its Vector-Jacobian Product (VJP) for reverse-mode automatic differentiation (AD). The `vjp_order` parameter in `.serialize()` controls how many levels of differentiation are supported after deserialization (here, 3 levels). The snippet verifies this by successfully computing gradients up to the specified order using `jax.grad` and shows that attempting a higher order raises an error. Dependencies include `jax` and `typing.Callable`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax import export\n>>> from typing import Callable\n\n>>> def f(x): return 7 * x * x * x\n\n>>> # Serialize 3 levels of VJP along with the primal function\n>>> blob: bytearray = export.export(jax.jit(f))(1.).serialize(vjp_order=3)\n>>> rehydrated_f: Callable = export.deserialize(blob).call\n\n>>> rehydrated_f(0.1)  # 7 * 0.1^3\nArray(0.007, dtype=float32)\n\n>>> jax.grad(rehydrated_f)(0.1)  # 7*3 * 0.1^2\nArray(0.21000001, dtype=float32)\n\n>>> jax.grad(jax.grad(rehydrated_f))(0.1)  # 7*3*2 * 0.1\nArray(4.2, dtype=float32)\n\n>>> jax.grad(jax.grad(jax.grad(rehydrated_f)))(0.1)  # 7*3*2\nArray(42., dtype=float32)\n\n>>> jax.grad(jax.grad(jax.grad(jax.grad(rehydrated_f))))(0.1)  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: No VJP is available\n```\n\n----------------------------------------\n\nTITLE: Using `jax.pure_callback` within `jax.lax.scan`\nDESCRIPTION: This example shows `jax.pure_callback` being used inside the body function (`body_fun`) of a `jax.lax.scan` operation. This demonstrates its compatibility with higher-order control flow primitives like `scan`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef body_fun(_, x):\n  return _, f(x)\njax.lax.scan(body_fun, None, jnp.arange(5.0))[1]\n```\n\n----------------------------------------\n\nTITLE: Explicit Data Placement on Devices with jax.device_put - Python\nDESCRIPTION: This code sets a Python value to reside on a specific device using `jax.device_put`, then retrieves its device assignment. Inputs include a value and a device object from `jax.devices()`. This approach is important for benchmarking or running JAX computations explicitly on a chosen device rather than the default. Dependencies are `jax` and its submodules. Choosing a device index that does not exist results in an error; this must be managed by the user.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax import device_put\narr = device_put(1, jax.devices()[2])  # doctest: +SKIP\nprint(arr.devices())  # doctest: +SKIP\n```\n\n----------------------------------------\n\nTITLE: Computing gradients with multiple non-differentiable arguments in custom_jvp\nDESCRIPTION: Demonstrates taking the gradient of the app2 function with respect to its second argument, showing how JAX correctly handles multiple non-differentiable function arguments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(app2, 1)(lambda x: x ** 3, 3., lambda y: 5 * y))\n```\n\n----------------------------------------\n\nTITLE: Implementing custom_jvp with non-differentiable arguments\nDESCRIPTION: Demonstrates using the nondiff_argnums parameter with custom_jvp to handle function-valued arguments that shouldn't be differentiated. This example shows a simple function application with custom gradient rules.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n@partial(custom_jvp, nondiff_argnums=(0,))\ndef app(f, x):\n  return f(x)\n\n@app.defjvp\ndef app_jvp(f, primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  return f(x), 2. * x_dot\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain Composition of Functions in JAX\nDESCRIPTION: Creates a utility function that sequentially applies a list of functions to an input. This is used to demonstrate the memory scaling of standard approaches to function composition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef chain_compose(funs):\n  def f(x):\n    for fun in funs:\n      x = fun(x)\n    return x\n  return f\n\nf = chain_compose([jnp.sin] * 8)\nprint_saved_residuals(f, 3.)\n```\n\n----------------------------------------\n\nTITLE: Installing JAX with locally installed NVIDIA CUDA\nDESCRIPTION: Command to install JAX with support for a pre-existing CUDA installation. This approach requires CUDA and cuDNN to be installed separately and is only available for Linux x86_64 and Linux aarch64.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade pip\n\n# Installs the wheel compatible with NVIDIA CUDA 12 and cuDNN 9.0 or newer.\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda12-local]\"\n```\n\n----------------------------------------\n\nTITLE: Reverse-Mode Differentiation with Custom JVP\nDESCRIPTION: Shows that the custom JVP rule is also used for reverse-mode differentiation via automatic transposition. The example demonstrates computing gradients using a custom JVP rule.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(3.))\n```\n\n----------------------------------------\n\nTITLE: Testing JIT of JAX Primitive after Lowering Registration (Python)\nDESCRIPTION: Verifies that after providing both abstract evaluation and XLA lowering, the custom primitive works under JAX's JIT. Calls the primitive with two floats and checks for expected numeric output, confirming correct lowering and registration. Intended to be run with 'api.jit', 'square_add_prim' available, and the lowering rule registered.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nassert api.jit(lambda x, y: square_add_prim(x, y))(2., 10.) == 14.\n\n```\n\n----------------------------------------\n\nTITLE: Sharded Input Handling and HLO Inspection in JAX (Python)\nDESCRIPTION: These snippets demonstrate how to set up a named sharding mesh, place data onto sharded device arrays, and inspect compiled HLO for distributed computation patterns ('all-gather', 'all-reduce'). The code highlights how pure JAX reference operations and FFI-backed versions behave differently under sharding, enabling efficient batch sharding and identifying data movement operations. Dependencies: JAX with multi-device support, numpy, environment setup (XLA_FLAGS). Inputs: test arrays x, mesh configurations. Outputs: printed HLO strings and allclose assertions. Limitations: requires at least four devices in the environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.sharding import PartitionSpec as P\n\nassert len(jax.devices()) == 4  # Set using the XLA_FLAGS environment variable\nmesh = jax.make_mesh((4,), (\"x\",))\n\nbatch_shd = jax.NamedSharding(mesh, P(\"x\", None))\nx_batch_shd = jax.device_put(x, batch_shd)\nhlo_batch = jax.jit(rms_norm_ref, out_shardings=batch_shd).lower(x_batch_shd).compile().as_text()\nassert \"all-\" not in hlo_batch\n```\n\nLANGUAGE: python\nCODE:\n```\ndata_shd = jax.NamedSharding(mesh, P(None, \"x\"))\nx_data_shd = jax.device_put(x, data_shd)\nhlo_data = jax.jit(rms_norm_ref, out_shardings=data_shd).lower(x_data_shd).compile().as_text()\nassert \"all-reduce\" in hlo_data\n```\n\n----------------------------------------\n\nTITLE: Incorrect and Correct Arithmetic on PRNG Keys (Python, Red/Green Example)\nDESCRIPTION: Illustrates the danger of generating batches of keys via direct arithmetic versus the correct practice with random.split. The incorrect version produces correlated keys, while the correct uses API-supported splitting for safety. Only the latter is compatible with new-style keys. Demonstrates what not to do when generating random key batches.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Incorrect\nkey = random.PRNGKey(0)\nbatched_keys = key + jnp.arange(10, dtype=key.dtype)[:, None]\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Correct\nkey = random.PRNGKey(0)\nbatched_keys = random.split(key, 10)\n\n```\n\n----------------------------------------\n\nTITLE: Saving Multiple Function Signatures in a TensorFlow SavedModel (Python)\nDESCRIPTION: This snippet demonstrates how to include multiple concrete function traces (signatures) for different input shapes within a single TensorFlow SavedModel when saving a converted JAX function. It converts `f_jax` using `jax2tf.convert` and wraps it with `tf.function` without an initial `input_signature`. By calling the wrapped function (`my_model.f`) with example inputs of different shapes (e.g., batch sizes 1 and 16) *before* saving, TensorFlow traces and includes specialized graphs for each shape in the SavedModel.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_model.f = tf.function(jax2tf.convert(f_jax), autograph=False)\nmy_model.f(tf.ones([1, 28, 28]))  # a batch size of 1\nmy_model.f(tf.ones([16, 28, 28]))  # a batch size of 16\ntf.saved_model.save(my_model, '/some/directory',\n                    options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n```\n\n----------------------------------------\n\nTITLE: Mixing Sharding Modes with auto_axes in JAX Python\nDESCRIPTION: Demonstrates how to use the auto_axes decorator to mix sharding modes, allowing some mesh axes to be in Auto mode while others remain in Explicit mode.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\n@functools.partial(auto_axes, axes='X')\ndef g(y):\n  print(f'mesh inside g: {get_abstract_mesh()}')\n  print(f'y.sharding inside g: {jax.typeof(y) = }', end='\\n\\n')\n  return y * 2\n\n@jax.jit\ndef f(arr1):\n  print(f'mesh inside f: {get_abstract_mesh()}')\n  x = jnp.sin(arr1)\n  print(f'x.sharding: {jax.typeof(x)}', end='\\n\\n')\n\n  z = g(x, out_shardings=P(\"X\", \"Y\"))\n\n  print(f'z.sharding: {jax.typeof(z)}', end=\"\\n\\n\")\n  return z + 1\n\nsome_x = reshard(np.arange(16).reshape(4, 4), P(\"X\", \"Y\"))\nf(some_x)\n```\n\n----------------------------------------\n\nTITLE: Modified Prediction Function with Named Checkpoints\nDESCRIPTION: Updates the prediction function to use named checkpoints that can be referenced by policies. This allows selective saving based on layer names rather than operation types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef predict(params, x):\n  *Ws, Wlast = params\n  for i, W in enumerate(Ws):\n    x = layer(W, x)\n    x = checkpoint_name(x, name=f'layer{i}_output')\n  x = jnp.dot(Wlast, x)\n  return x\n```\n\n----------------------------------------\n\nTITLE: Autobatching Log-Joint Function with JAX vmap\nDESCRIPTION: Demonstrates autobatching of the log-joint function using JAX's vmap. Simplifies the code by automatically handling batched inputs without manual modifications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvmap_batched_log_joint = jax.vmap(log_joint)\nvmap_batched_log_joint(batched_test_beta)\n```\n\n----------------------------------------\n\nTITLE: Using dynamic_trace and Omnistaging in make_jaxpr (Python)\nDESCRIPTION: Defines a context manager new_dynamic to set the dynamic_trace global, and implements make_jaxpr, a replacement for make_jaxpr_v1 that properly stages computations regardless of traced inputs. This advances the JAX interpreter to enable omnistaging. The demonstration showcases tracing a lambda:mul(2.,2.) with correct graph construction, with lru_cache employed for memoization. Proper resource management via contextlib is used for dynamic interpreter stack control.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n@contextmanager\ndef new_dynamic(main: MainTrace):\n  global dynamic_trace\n  prev_dynamic_trace, dynamic_trace = dynamic_trace, main\n  try:\n    yield\n  finally:\n    dynamic_trace = prev_dynamic_trace\n\n@lru_cache\ndef make_jaxpr(f: Callable, *avals_in: ShapedArray,\n               ) -> tuple[Jaxpr, list[Any], PyTreeDef]:\n  avals_in, in_tree = tree_flatten(avals_in)\n  f, out_tree = flatten_fun(f, in_tree)\n\n  builder = JaxprBuilder()\n  with new_main(JaxprTrace, builder) as main:\n    with new_dynamic(main):\n      trace = JaxprTrace(main)\n      tracers_in = [trace.new_arg(aval) for aval in avals_in]\n      outs = f(*tracers_in)\n      tracers_out = [full_raise(trace, out) for out in outs]\n      jaxpr, consts = builder.build(tracers_in, tracers_out)\n  return jaxpr, consts, out_tree()\n\njaxpr, consts, _ = make_jaxpr(lambda: mul(2., 2.))\nprint(jaxpr)\n```\n\n----------------------------------------\n\nTITLE: Applying `jax.jit` to an Explicitly Stateful Counter in Python\nDESCRIPTION: Successfully applies `jax.jit` to the `count` method of the `CounterV2` class, which uses explicit state management. The code demonstrates the correct usage pattern: initialize the state, JIT-compile the state-updating function, and then iteratively call the compiled function, passing the current state and receiving the updated state in each step. The counter now increments correctly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/stateful-computations.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstate = counter.reset()\nfast_count = jax.jit(counter.count)\n\nfor _ in range(3):\n  value, state = fast_count(state)\n  print(value)\n```\n\n----------------------------------------\n\nTITLE: Modifying Global Variables in JAX JIT Compilation\nDESCRIPTION: This snippet demonstrates how attempts to modify global variables within a JIT-compiled function in JAX can lead to unexpected results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ng = 0.\ndef impure_saves_global(x):\n  global g\n  g = x\n  return x\n\n# JAX runs once the transformed function with special Traced values for arguments\nprint (\"First call: \", jit(impure_saves_global)(4.))\nprint (\"Saved global: \", g)  # Saved global has an internal JAX value\n```\n\n----------------------------------------\n\nTITLE: Showing Numerical Instability in log1pexp Gradient\nDESCRIPTION: Demonstrating the numerical instability issue when computing the gradient of log1pexp at a large input value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(log1pexp)(100.))\n```\n\n----------------------------------------\n\nTITLE: Pipelined Asynchronous Loading and MMA in JAX Pallas (Python)\nDESCRIPTION: Depicts the recommended loop structure for pipelining SMEM/GMEM asynchronous memory operations and WGMMA in JAX with a double/triple-buffered approach. The code defines buffer slots, fetching functions, synchronization steps, and pipelined execution using JAX's 'fori_loop', barriers, and plgpu utilities. Implements correctness via shape assertions and careful slot/buffer management. Requires 'jax', 'jnp', 'pl', 'plgpu', and arrays for a_gmem, b_gmem, a_smem, b_smem, acc_ref, and associated loading/barrier structures.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbuffers = 3  # In reality you might want even more\\nassert a_smem.shape == (buffers, m, k)\\nassert b_smem.shape == (buffers, k, n)\\nassert acc_ref.shape == (m, n)\\n\\ndef fetch_a_b(ki, slot):\\n  a_slice = ... # Replace with the right M/K slice\\n  b_slice = ... # Replace with the right K/N slice\\n  plgpu.copy_gmem_to_smem(a_gmem.at[a_slice], a_smem.at[slot], a_loaded.at[slot])\\n  plgpu.copy_gmem_to_smem(b_gmem.at[b_slice], b_smem.at[slot], b_loaded.at[slot])\\n\\ndef loop_body(i, _):\\n  slot = jax.lax.rem(i, buffers)\\n  plgpu.barrier_wait(a_loaded.at[slot])\\n  plgpu.barrier_wait(b_loaded.at[slot])\\n  plgpu.wgmma(acc_ref, a_smem.at[slot], b_smem.at[slot])\\n  # We know that only the last issued WGMMA is running, so we can issue a async load in\\n  # into the other buffer\\n  load_i = i + buffers - 1\\n  load_slot = jax.lax.rem(load_i, buffers)\\n  @pl.when(jnp.logical_and(load_i >= buffers, load_i < num_steps))\\n  def _do_fetch():\\n    fetch_a_b(load_i, slot)\\nfor slot in range(buffers):\\n  fetch_a_b(slot, slot)\\njax.lax.fori_loop(0, num_steps, loop_body, None)\n```\n\n----------------------------------------\n\nTITLE: Setting Initial Conditions for Wave Equation Simulation\nDESCRIPTION: Creates the initial conditions for the wave equation simulation, including the mesh grid and initial wave shape using Gaussian filtering.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.linspace(0, 8, num=8*1024, endpoint=False)\ny = jnp.linspace(0, 1, num=1*1024, endpoint=False)\nx_mesh, y_mesh = jnp.meshgrid(x, y, indexing='ij')\n\n# NOTE: smooth initial conditions are important, so we aren't exciting\n# arbitrarily high frequencies (that cannot be resolved)\nu = skimage.filters.gaussian(\n    ((x_mesh - 1/3) ** 2 + (y_mesh - 1/4) ** 2) < 0.1 ** 2,\n    sigma=1)\n\n# u = jnp.exp(-((x_mesh - 1/3) ** 2 + (y_mesh - 1/4) ** 2) / 0.1 ** 2)\n\n# u = skimage.filters.gaussian(\n#     (x_mesh > 1/3) & (x_mesh < 1/2) & (y_mesh > 1/3) & (y_mesh < 1/2),\n#     sigma=5)\n\nv = jnp.zeros_like(u)\nc = 1  # could also use a 2D array matching the mesh shape\n```\n\n----------------------------------------\n\nTITLE: Implementing Reduce-Scatter Kernel for TPU Communication in Python\nDESCRIPTION: Defines the main kernel function for reduce-scatter operation that coordinates data movement and reduction between TPU devices. It handles setup, communication, computation, and cleanup phases using semaphores for synchronization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef reduce_scatter_kernel(\n    x_ref,\n    o_ref,\n    hbm_scratch,\n    local_copy_sem,\n    left_recv_sem,\n    left_send_sem,\n    right_recv_sem,\n    right_send_sem,\n    left_capacity_sem,\n    right_capacity_sem,\n    accum_scratch,\n):\n  outer_step = pl.program_id(0)\n  phase = pl.program_id(1)\n  is_start = jnp.logical_and(outer_step == 0, phase == 0)\n  last_iteration = outer_step == pl.num_programs(0) - 1\n\n  working_slot = lax.rem(outer_step, 2)\n  receiving_slot = 1 - working_slot\n  my_id = lax.axis_index('x')\n  right_neighbor = mod(my_id + 1, num_devices)\n  left_neighbor = mod(my_id - 1, num_devices)\n\n  left_copy_device = mod(my_id + outer_step + 1, num_devices)\n  right_copy_device = mod(my_id - outer_step - 1, num_devices)\n  # Slices can be specified using pl.ds(start, size)\n  left_copy_slice = pl.ds(0, block_size[0] // 2)\n  right_copy_slice = pl.ds(block_size[0] // 2, block_size[0] // 2)\n  current_phase_slice = pl.ds(phase * (block_size[0] // 2), block_size[0] // 2)\n\n  initial_left_copy = pltpu.make_async_remote_copy(\n      src_ref=x_ref.at[my_id, left_copy_slice],\n      dst_ref=hbm_scratch.at[working_slot, left_copy_slice],\n      send_sem=left_send_sem,\n      recv_sem=left_recv_sem,\n      device_id=(left_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n\n  initial_right_copy = pltpu.make_async_remote_copy(\n      src_ref=x_ref.at[my_id, right_copy_slice],\n      dst_ref=hbm_scratch.at[working_slot, right_copy_slice],\n      send_sem=right_send_sem,\n      recv_sem=right_recv_sem,\n      device_id=(right_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n\n  left_copy = pltpu.make_async_remote_copy(\n      src_ref=hbm_scratch.at[working_slot, left_copy_slice],\n      dst_ref=hbm_scratch.at[receiving_slot, left_copy_slice],\n      send_sem=left_send_sem,\n      recv_sem=left_recv_sem,\n      device_id=(left_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n  right_copy = pltpu.make_async_remote_copy(\n      # Note: Right copy is flipped with regards to slots since we are copying\n      # to the next outer_step iteration.\n      src_ref=hbm_scratch.at[receiving_slot, right_copy_slice],\n      dst_ref=hbm_scratch.at[working_slot, right_copy_slice],\n      send_sem=right_send_sem,\n      recv_sem=right_recv_sem,\n      device_id=(right_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n\n  # --- Prologue ---\n  @pl.when(is_start)\n  def _():\n    # Barrier with both neighbors at the start, since we will be\n    # communicating with both.\n    local_barrier(left_neighbor, right_neighbor)\n\n    # Initialize o_ref, acc_scratch, and hbm_scratch with initial copies.\n    o_ref[...] = jnp.zeros_like(o_ref[...])\n    accum_scratch[...] = jnp.zeros_like(accum_scratch[...])\n\n    initial_left_copy.start()\n    initial_left_copy.wait()\n    initial_right_copy.start()\n\n    # We tell our left neighbor that it is allowed to send to the right.\n    # (and vice versa for right neighbor)\n    signal(LEFT, right_capacity_sem)\n    signal(RIGHT, left_capacity_sem)\n\n  # --- Body ---\n  # At the beginning of our kernel body, we start a DMA which copies\n  # the result we computed in the previous phase to our neighbor.\n  # This allows us to overlap the communication of sending our previous phase\n  # with the computation for the current phase.\n  @pl.when(~is_start)\n  def _():\n    @pl.when(phase == LEFT)\n    def _():\n      # We block here until our right neighbor tells use we can send to\n      # the right.\n      pltpu.semaphore_wait(right_capacity_sem, 1)\n      right_copy.start()\n\n    @pl.when(phase == RIGHT)\n    def _():\n      # We block here until our left neighbor tells use we can send to\n      # the left.\n      pltpu.semaphore_wait(left_capacity_sem, 1)\n      left_copy.start()\n\n  local_copy = pltpu.make_async_copy(\n      src_ref=hbm_scratch.at[working_slot, current_phase_slice],\n      dst_ref=accum_scratch,\n      sem=local_copy_sem,\n  )\n  local_copy.start()\n  local_copy.wait()\n\n  @pl.when(~last_iteration)\n  def _():\n    @pl.when(phase == LEFT)\n    def _():\n      accum_scratch[...] += x_ref[left_copy_device, left_copy_slice]\n\n    @pl.when(phase == RIGHT)\n    def _():\n      accum_scratch[...] += x_ref[right_copy_device, right_copy_slice]\n\n  local_copy = pltpu.make_async_copy(\n      src_ref=accum_scratch,\n      dst_ref=hbm_scratch.at[working_slot, current_phase_slice],\n      sem=local_copy_sem,\n  )\n  local_copy.start()\n  local_copy.wait()\n\n  @pl.when(is_start)\n  def _():\n    initial_right_copy.wait()\n\n  # At the end of our kernel body, we wait on the DMA of the previous phase\n  # to make sure the results are ready for the next phase.\n  @pl.when(~is_start)\n  def _():\n    @pl.when(phase == LEFT)\n    def _():\n      right_copy.wait()\n      signal(LEFT, right_capacity_sem)\n\n    @pl.when(phase == RIGHT)\n    def _():\n      left_copy.wait()\n      signal(RIGHT, left_capacity_sem)\n\n  # --- Epilogue ---\n  # Store result on last iteration.\n  @pl.when(last_iteration)\n  def _():\n    # Clean up semaphores so that they exit with a value of 0.\n    @pl.when(phase == LEFT)\n    def _():\n      o_ref[left_copy_slice, ...] = accum_scratch[...]\n      pltpu.semaphore_wait(right_capacity_sem, 1)\n\n    @pl.when(phase == RIGHT)\n    def _():\n      o_ref[right_copy_slice, ...] = accum_scratch[...]\n      pltpu.semaphore_wait(left_capacity_sem, 1)\n```\n\n----------------------------------------\n\nTITLE: Using checkify with grad in JAX\nDESCRIPTION: Demonstrates how checkify can instrument gradient computations when applied to jax.grad. The example shows detecting NaN generation in the gradient computation rather than in the forward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n return x / (1 + jnp.sqrt(x))\n\ngrad_f = jax.grad(f)\n\nerr, _ = checkify.checkify(grad_f, errors=checkify.nan_checks)(0.)\nprint(err.get())\n>> nan generated by primitive mul at <...>:3 (f)\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation with Static Arguments\nDESCRIPTION: Shows how to use JAX's JIT compilation with static arguments in pytree structures using function decorators.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x: MyDataclassContainer | MyOtherContainer):\n  return x.a + x.b\n\n# Works fine! `mdc.name` is static.\nmdc = MyDataclassContainer('mdc', 1, 2, 3)\ny = f(mdc)\n```\n\n----------------------------------------\n\nTITLE: Manual Vectorization of Convolution in JAX\nDESCRIPTION: Demonstrates a manually vectorized implementation of the convolution function. This approach is more efficient but requires rewriting the function to handle batched inputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-vectorization.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef manually_vectorized_convolve(xs, ws):\n  output = []\n  for i in range(1, xs.shape[-1] -1):\n    output.append(jnp.sum(xs[:, i-1:i+2] * ws, axis=1))\n  return jnp.stack(output, axis=1)\n\nmanually_vectorized_convolve(xs, ws)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using the JAX Persistent Compilation Cache in Python\nDESCRIPTION: Provides a step-by-step Python example for enabling and configuring JAX's persistent cache. It includes importing required JAX modules, setting cache location and thresholds, enabling XLA caches, and defining and running a JIT-compiled function. Dependencies include 'jax' and 'jax.numpy', and cache parameters should be adjusted as needed. The snippet demonstrates the expected workflow for caching compiled functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\njax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\njax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", -1)\njax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0)\njax.config.update(\"jax_persistent_cache_enable_xla_caches\", \"xla_gpu_per_fusion_autotune_cache_dir\")\n\n@jax.jit\ndef f(x):\n  return x + 1\n\nx = jnp.zeros((2, 2))\nf(x)\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with defjvps\nDESCRIPTION: Shows how to compute gradients of a function with JVP rules defined using defjvps. The example verifies that the custom differentiation rules work correctly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(3.))\n```\n\n----------------------------------------\n\nTITLE: Illustrating Ineffective Checkpointing (Last Function) in JAX Python\nDESCRIPTION: This snippet shows another ineffective use case: applying `jax.checkpoint` only to the *last* function `h` in the composition `f(x) = h(g(x))`. This also fails to save memory and introduces redundant computation, as `h`'s forward pass is computed twice (steps 2 and 3).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef f_grad_bad2(x):\n  y, g_vjp = jax.vjp(g, x)  # step 1\n  z = h(y)                  # step 2\n  _, h_vjp = jax.vjp(h, y)  # step 3\n  y_bar, = h_vjp(1.0)       # step 3\n  x_bar, = g_vjp(y_bar)     # step 5\n  return x_bar\n```\n\n----------------------------------------\n\nTITLE: RMS Normalization C++ Implementation\nDESCRIPTION: Basic C++ implementation of root-mean-square normalization that will be exposed through the FFI. This function computes the normalization scale and applies it to the input array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n#include <cmath>\n#include <cstdint>\n\nfloat ComputeRmsNorm(float eps, int64_t size, const float *x, float *y) {\n  float sm = 0.0f;\n  for (int64_t n = 0; n < size; ++n) {\n    sm += x[n] * x[n];\n  }\n  float scale = 1.0f / std::sqrt(sm / float(size) + eps);\n  for (int64_t n = 0; n < size; ++n) {\n    y[n] = x[n] * scale;\n  }\n  return scale;\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Error Checking with Multiple Built-in Checkify Checks\nDESCRIPTION: Demonstrates using checkify with multiple built-in check types (user_checks, index_checks, float_checks) to automatically detect common errors like out-of-bounds indexing and NaN generation in JAX code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nerrors = checkify.user_checks | checkify.index_checks | checkify.float_checks\nchecked_f = checkify.checkify(f, errors=errors)\n\nerr, z = checked_f(jnp.ones((5,)), 100)\nerr.throw()\n# ValueError: out-of-bounds indexing at <..>:7 (f)\n\nerr, z = checked_f(jnp.ones((5,)), -1)\nerr.throw()\n# ValueError: index needs to be non-negative! (check failed at <…>:6 (f))\n\nerr, z = checked_f(jnp.array([jnp.inf, 1]), 0)\nerr.throw()\n# ValueError: nan generated by primitive sin at <...>:8 (f)\n```\n\n----------------------------------------\n\nTITLE: Vectorizing debug.callback with vmap in JAX - Python\nDESCRIPTION: Shows that debug.callback works inside vmap-transformed functions, enabling side effects (e.g., logging vector elements) per mapped invocation, with no change to output semantics. Essential for batch debugging/tracing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.arange(5.0)\njax.vmap(f)(x);\n```\n\n----------------------------------------\n\nTITLE: Additional shard_map Examples with Different Axis Specifications\nDESCRIPTION: Further examples of shard_map usage showing how different output specifications affect array shapes when combined with partial sum operations across various mesh axes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@partial(shard_map, mesh=m, in_specs=P('i', 'j'), out_specs=P(None, 'j'))\ndef f4(x_block):\n  return jax.lax.psum(x_block, 'i')\n\nx = np.arange(12 * 12).reshape(12, 12)\ny4 = f4(x)\nprint(y4.shape)  # (3,12)\n\n\n@partial(shard_map, mesh=m, in_specs=P('i', 'j'), out_specs=P(None, None))\ndef f5(x_block):\n  return jax.lax.psum(x_block, ('i', 'j'))\n\ny5 = f5(x)\nprint(y5.shape)  # (3,6)\n```\n\n----------------------------------------\n\nTITLE: Basic Parallel Mapping with pmap\nDESCRIPTION: Demonstrates basic parallel mapping operation by squaring numbers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult = pmap(lambda x: x ** 2)(jnp.arange(7))\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Correct Implementation of Pallas Sum Reduction with Initialization (Python)\nDESCRIPTION: This snippet provides the corrected implementation for sum reduction using JAX Pallas on TPUs. The `correct_sum_kernel` explicitly initializes the output buffer `o_ref` to zeros using `@pl.when(pl.program_id(2) == 0)` before accumulation. The `correct_sum` function sets up the `pl.pallas_call` with the reduction dimension correctly placed as the *last* axis of the `grid`, ensuring proper accumulation within SRAM before writing results back to HBM.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Note: This is a TPU example.\n\ndef correct_sum_kernel(x_ref, o_ref):\n  @pl.when(pl.program_id(2) == 0)\n  def _():\n    o_ref[...] = jnp.zeros_like(o_ref)\n  o_ref[...] += x_ref[...]\n\ndef correct_sum(x: jax.Array,\n              block_size: tuple[int, ...] = (256, 256)) -> jax.Array:\n  reduction_size, *out_shape = x.shape\n  # We moved the reduction to the last axis of the grid.\n  grid = (*(out // blk for out, blk in zip(out_shape, block_size)), reduction_size)\n  return pl.pallas_call(\n      correct_sum_kernel,\n      grid=grid,\n      # None in `block_shape` means we pick a size of 1 and squeeze it away\n      in_specs=[pl.BlockSpec((None, *block_size), lambda i, j, k: (k, i, j))],\n      out_specs=pl.BlockSpec(block_size, lambda i, j, k: (i, j)),\n      out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype),\n  )(x)\n\nresult = correct_sum(x)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Implementing transpose rules for primitive operations\nDESCRIPTION: Definitions of transpose rules for primitive operations like multiplication, negation, addition, and reduction, which are needed for reverse-mode automatic differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_72\n\nLANGUAGE: python\nCODE:\n```\ndef mul_transpose_rule(cts, x, y):\n  z_bar, = cts\n  assert (type(x) is UndefPrimal) ^ (type(y) is UndefPrimal)\n  return [mul(z_bar, y), None] if type(x) is UndefPrimal else [None, mul(x, z_bar)]\ntranspose_rules[mul_p] = mul_transpose_rule\n\ndef neg_transpose_rule(cts, x):\n  ybar, = cts\n  assert type(x) is UndefPrimal\n  return [neg(ybar)]\ntranspose_rules[neg_p] = neg_transpose_rule\n\ndef add_transpose_rule(cts, x, y):\n  z_bar, = cts\n  return [z_bar, z_bar]\ntranspose_rules[add_p] = add_transpose_rule\n\ndef reduce_sum_transpose_rule(cts, x, *, axis):\n  y_bar, = cts\n  return [broadcast(y_bar, x.aval.shape, axis)]\ntranspose_rules[reduce_sum_p] = reduce_sum_transpose_rule\n\ndef xla_call_transpose_rule(cts, *invals, jaxpr, num_consts):\n  del num_consts  # Unused\n  undef_primals = [type(x) is UndefPrimal for x in invals]\n  transposed_jaxpr, new_consts = transpose_jaxpr(jaxpr, tuple(undef_primals))\n  residuals, _ = partition_list(undef_primals, invals)\n  outs = bind(xla_call_p, *new_consts, *residuals, *cts,\n              jaxpr=transposed_jaxpr, num_consts=len(new_consts))\n  outs = iter(outs)\n  return [next(outs) if undef else None for undef in undef_primals]\ntranspose_rules[xla_call_p] = xla_call_transpose_rule\n\n@lru_cache\ndef transpose_jaxpr(jaxpr: Jaxpr, undef_primals: tuple[bool, ...]\n                    ) -> tuple[Jaxpr, list[Any]]:\n  avals_in, avals_out = typecheck_jaxpr(jaxpr)\n  traceable = partial(eval_jaxpr_transposed, jaxpr)\n  args = [UndefPrimal(a) if u else a for a, u in zip(avals_in, undef_primals)]\n  trans_jaxpr, consts, _ = make_jaxpr(traceable, tuple(args), tuple(avals_out))\n  typecheck_jaxpr(trans_jaxpr)\n  return trans_jaxpr, consts\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Profiler Capture Using Context Manager (Python)\nDESCRIPTION: This Python example shows how to use the jax.profiler.trace context manager for concise profiler trace capturing to a directory compatible with TensorBoard. The code generates a random matrix, performs a large dot product, and ensures completion of computation. All operations within the with block are included in the trace. Dependencies: import jax. Output trace files can be viewed in TensorBoard's trace viewer. This approach is an alternative to the explicit start_trace and stop_trace sequence.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n\nwith jax.profiler.trace(\"/tmp/tensorboard\"):\n  key = jax.random.key(0)\n  x = jax.random.normal(key, (5000, 5000))\n  y = x @ x\n  y.block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Ensuring SMEM Write Visibility with commit_smem in PLGPU (Python)\nDESCRIPTION: Demonstrates correct placement of 'plgpu.commit_smem()' to synchronize SMEM writes for subsequent asynchronous operations (such as SMEM→GMEM copy or WGMMA execution). Assigns a value to an SMEM reference, commits the write, then launches an asynchronous operation. Correct usage is crucial to avoid data races. Requires 'plgpu' and properly-typed SMEM reference.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsmem_ref[...] = value\\nplgpu.commit_smem()\\nplgpu.copy_smem_to_gmem(smem_ref, ...)\n```\n\n----------------------------------------\n\nTITLE: Solving and Plotting Single ODE Solution in JAX\nDESCRIPTION: Demonstrates solving a single ODE solution using the jitted Runge-Kutta solver and plots the result using the custom plotting utilities. This snippet sets up the initial conditions and solves for N steps.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nN = 40000\n\n# set initial condition\nstate0 = jnp.array([1., 1., 1.])\nys = jnp.zeros((N,) + state0.shape)\nys = ops.index_update(ys, ops.index[0], state0)\n\n# solve for N steps\nys = rk4(ys, 0.004, N).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Generating NaN Gradients with jnp.where (Python)\nDESCRIPTION: Defines a function `my_log` using `jnp.where` to handle `log(x)` for `x <= 0`. While `my_log(0.)` correctly returns `0.`, calculating its gradient using `jax.grad(my_log)(0.)` results in `NaN`. This occurs because the gradient calculation implicitly involves the gradient of `jnp.log(x)`, which is undefined (infinite) at `x=0`, propagating `NaN` through the `jnp.where`. Requires `jax` and `jax.numpy`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\ndef my_log(x):\n  return jnp.where(x > 0., jnp.log(x), 0.)\n\nprint(my_log(0.)) # ==> 0.  Ok\nprint(jax.grad(my_log)(0.)) # ==> NaN\n```\n\n----------------------------------------\n\nTITLE: Templated Matrix Multiplication with Activation\nDESCRIPTION: GPU-optimized matrix multiplication implementation with configurable block sizes and activation function, demonstrating tiled computation and memory access patterns.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_kernel(x_ref, y_ref, o_ref, *, activation, block_k):\n  acc = jnp.zeros((x_ref.shape[0], y_ref.shape[1]), jnp.float32)\n  for k in range(x_ref.shape[1] // block_k):\n    x = x_ref[:, k*block_k:(k+1)*block_k]\n    y = y_ref[k*block_k:(k+1)*block_k, :]\n    acc += x @ y\n  o_ref[:, :] = activation(acc).astype(o_ref.dtype)\n\nx, y = jnp.ones((512, 256)), jnp.ones((256, 1024))\nblock_shape = 128, 256, 128\n\n@partial(jax.jit, static_argnames=[\"block_shape\", \"activation\"])\ndef matmul(x, y, *, block_shape, activation):\n  block_m, block_n, block_k = block_shape\n  fused_matmul = pl.pallas_call(\n      partial(matmul_kernel, block_k=block_k, activation=activation),\n      out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1],), jnp.float32),\n      in_specs=[\n          pl.BlockSpec((block_m, x.shape[1]), lambda i, j: (i, 0)),\n          pl.BlockSpec((y.shape[0], block_n), lambda i, j: (0, j))\n      ],\n      out_specs=pl.BlockSpec((block_m, block_n), lambda i, j: (i, j)),\n      grid=(4, 4),\n  )\n  return fused_matmul(x, y)\n\nz = matmul(x, y, block_shape=block_shape, activation=jax.nn.gelu)\n```\n\n----------------------------------------\n\nTITLE: Testing Data Dimension Sharding with custom_partitioning\nDESCRIPTION: Code snippet that tests how custom_partitioning handles inputs sharded along the data dimension. This demonstrates a key difference from shard_map: custom_partitioning gathers data onto each device rather than resharding on the batch dimension.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nhlo_data_partitioned = jax.jit(rms_norm_partitioned, out_shardings=data_shd).lower(x_data_shd).compile().as_text().strip()\nassert \"all-gather\" in hlo_data_partitioned\n```\n\n----------------------------------------\n\nTITLE: Correct Usage: Using numpy for Shape Calculation in JIT (Python)\nDESCRIPTION: This snippet shows the recommended approach—using standard numpy for shape calculations inside JAX-jitted functions. By replacing jnp.prod with np.prod, the shape calculation is computed as a regular Python value, avoiding tracer errors. The function assumes numpy is imported, and can be used with various input arrays without raising tracing errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n@jit\ndef f(x):\n  input_size = np.prod(x.shape)\n  if input_size > 100:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Correct Key Splitting Pattern in JAX PRNG\nDESCRIPTION: Demonstrates the correct pattern for generating independent random streams in JAX by splitting the original key into separate subkeys for each random operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Correct\nkey = random.PRNGKey(0)\nkey1, key2 = random.split(random.key(0))\nx = random.uniform(key1, (100,))\ny = random.uniform(key2, (100,))\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Dense Mask to Block-Sparse Format in JAX\nDESCRIPTION: Converts a dense boolean mask into a block-sparse representation with prefetch maps for pipeline optimization. Returns block mask, prefetch indices, and compressed mask data for non-zero blocks.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef sparsify_mask(mask: jax.Array,\n                  block_shape: tuple[int, int]):\n  \"\"\"Preprocesses a mask into a sparse reprentation.\n\n  Args:\n    mask: A boolean array of shape [M, N]\n    block_shape: The size of a single block.\n\n  Returns:\n    block_mask: A block_shape array of booleans indicating whether a block\n      is all-zeros (0) or contains non-zero elements (1).\n    prefetch_mask: A block_shape array of integers indicating the index of the\n      next non-zero block.\n    mask_data: A (num_blocks, block_shape) array containing\n      the data for non-zero blocks of the mask.\n  \"\"\"\n  M, N = mask.shape\n  bm, bn = block_shape\n\n  block_mask = jnp.zeros((M // bm, N // bn), dtype=mask.dtype)\n  mask_types_finder = []\n  mask_data = []\n  mask_type_idxs = []\n\n  next_mask_type_idx = 0\n  prefetch_mask = jnp.zeros_like(block_mask)\n  next_i = (M // bm) - 1\n  next_j = (N // bn) - 1\n  prefetch_i = jnp.zeros_like(block_mask)\n  prefetch_j = jnp.zeros_like(block_mask)\n  for i in range(M // bm, -1, -1):\n    for j in range(N // bn, -1, -1):\n      mask_block = mask[i * bm :(i + 1) * bm,\n                        j * bn :(j + 1) * bn]\n      is_nonzero = jnp.any(mask_block)\n      if is_nonzero:\n        try:\n          type_index = mask_types_finder.index(str(mask_block))\n        except ValueError:\n          type_index = len(mask_types_finder)\n          mask_types_finder.append(str(mask_block))\n          mask_data.append(mask_block)\n        next_mask_type_idx = type_index\n        next_i = i\n        next_j = j\n      else:\n        type_index = -1\n      mask_type_idxs.append(type_index)\n      block_mask = block_mask.at[i, j].set(is_nonzero)\n      prefetch_mask = prefetch_mask.at[i, j].set(next_mask_type_idx)\n      prefetch_i = prefetch_i.at[i, j].set(next_i)\n      prefetch_j = prefetch_j.at[i, j].set(next_j)\n  return block_mask, prefetch_mask, prefetch_i, prefetch_j, jnp.stack(mask_data)\n```\n\n----------------------------------------\n\nTITLE: Using Checkpointing Policies for Matrix Operations\nDESCRIPTION: Shows how to use a checkpointing policy to control what gets saved without modifying the original function. This example uses a policy to save only the results of dot operations with no batch dimensions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nf3 = jax.checkpoint(f, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\njax.ad_checkpoint.print_saved_residuals(f3, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Configuring Global and Local JAX Logging Levels in Python\nDESCRIPTION: Demonstrates two ways to set the logging verbosity for JAX: globally via an environment variable and locally using jax.config.update. The global setting applies to the process, while the local update is effective within a specific script or session. Both require setting the level to values such as 'DEBUG' or other standard logging levels.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"JAX_LOGGING_LEVEL\"] = \"DEBUG\"\n# or locally with\njax.config.update(\"jax_logging_level\", \"DEBUG\")\n```\n\n----------------------------------------\n\nTITLE: Combining checkify with jit in JAX\nDESCRIPTION: Demonstrates that checkify and jit can be composed in either order. Both checkify-of-jit and jit-of-checkify work correctly for detecting and reporting errors like out-of-bounds indexing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, i):\n  return x[i]\n\ncheckify_of_jit = checkify.checkify(jax.jit(f))\njit_of_checkify = jax.jit(checkify.checkify(f))\nerr, _ =  checkify_of_jit(jnp.ones((5,)), 100)\nerr.get()\n# out-of-bounds indexing at <..>:2 (f)\nerr, _ = jit_of_checkify(jnp.ones((5,)), 100)\n# out-of-bounds indexing at <..>:2 (f)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JAX Tracer Objects\nDESCRIPTION: Example showing how JAX transforms replace array values with Tracer objects during function execution under JIT compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  print(type(x))\n  return x\n\nf(jnp.arange(5))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Sharding Error with Permuted Device Orders in JAX\nDESCRIPTION: Shows that attempting to combine arrays sharded across the same devices but in different orders raises a ValueError, illustrating JAX's behavior when explicit shardings have inconsistent device orderings.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndevices = jax.devices()\npermuted_devices = [devices[i] for i in [0, 1, 2, 3, 6, 7, 4, 5]]\n\nsharding1 = NamedSharding(Mesh(devices, 'x'), P('x'))\nsharding2 = NamedSharding(Mesh(permuted_devices, 'x'), P('x'))\n\ny = jax.device_put(x, sharding1)\nz = jax.device_put(x, sharding2)\ntry: y + z\nexcept ValueError as e: print_exception(e)\n```\n\n----------------------------------------\n\nTITLE: Implementing topological sort for JAX tracers\nDESCRIPTION: Defines a topological sort function for JAX tracers, used in the process of converting tracers to jaxprs. It also includes helper functions for removing duplicates and checking the validity of the sort.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_65\n\nLANGUAGE: Python\nCODE:\n```\ndef toposort(out_nodes: list[Any], parents: Callable[[Any], list[Any]]):\n  if not out_nodes: return []\n  out_nodes = remove_duplicates(out_nodes)\n\n  child_counts = {}\n  stack = list(out_nodes)\n  while stack:\n    node = stack.pop()\n    if id(node) in child_counts:\n      child_counts[id(node)] += 1\n    else:\n      child_counts[id(node)] = 1\n      stack.extend(parents(node))\n  for node in out_nodes:\n    child_counts[id(node)] -= 1\n\n  sorted_nodes = []\n  childless_nodes = [node for node in out_nodes if not child_counts[id(node)]]\n  while childless_nodes:\n    node = childless_nodes.pop()\n    sorted_nodes.append(node)\n    for parent in parents(node):\n      if child_counts[id(parent)] == 1:\n        childless_nodes.append(parent)\n      else:\n        child_counts[id(parent)] -= 1\n\n  sorted_nodes = sorted_nodes[::-1]\n  check_toposort(sorted_nodes, parents)\n  return sorted_nodes\n\ndef remove_duplicates(lst):\n  seen = set()\n  return [x for x in lst if id(x) not in seen and not seen.add(id(x))]\n\ndef check_toposort(nodes: list[Any], parents: Callable[[Any], list[Any]]):\n  seen = set()\n  for node in nodes:\n    assert all(id(parent) in seen for parent in parents(node))\n    seen.add(id(node))\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Sharding Mode in JAX\nDESCRIPTION: This snippet demonstrates explicit sharding in JAX by creating a mesh with explicit axis types. It places data on devices with specific sharding patterns and uses a decorated function to track and display the sharding information of inputs and outputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nexplicit_mesh = jax.make_mesh((8,), ('X',), axis_types=(AxisType.Explicit,))\n\nx_sharded = jax.device_put(x, jax.NamedSharding(explicit_mesh, P('X')))\nweights_sharded = jax.device_put(weights, jax.NamedSharding(explicit_mesh, P()))\n\n@jax.jit\ndef layer_auto(x, weights, bias):\n  print(f\"x sharding: {jax.typeof(x)}\")\n  print(f\"weights sharding: {jax.typeof(weights)}\")\n  print(f\"bias sharding: {jax.typeof(bias)}\")\n  out = layer(x, weights, bias)\n  print(f\"out sharding: {jax.typeof(out)}\")\n  return out\n\nwith jax.sharding.use_mesh(explicit_mesh):\n  layer_auto(x_sharded, weights_sharded, bias)\n```\n\n----------------------------------------\n\nTITLE: Implementing Double-Buffering in Pallas for TPU\nDESCRIPTION: This code snippet demonstrates the double-buffering technique in Pallas for TPU. Double-buffering is used to avoid race conditions when reading from and writing to the same buffer. It alternates between two buffers for working and receiving data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef kernel(...):\n  # ...\n  iteration = pl.program_id(0)\n  working_slot = lax.rem(iteration, 2)\n  receiving_slot = 1 - working_slot\n  # ...\n\n  local_copy_op = pltpu.make_async_copy(\n    src_ref=dst_ref.at[working_slot, ...],\n    dst_ref=local_scratch_ref,\n    sem=local_copy_sem,\n  )\n  local_copy_op.start()\n  remote_copy_op = pltpu.make_async_remote_copy(\n    src_ref=src_ref,\n    dst_ref=dst_ref.at[receiving_slot, ...],\n    send_sem=send_sem,\n    recv_sem=recv_sem,\n    device_id=target_device,\n    device_id_type=pltpu.DeviceIdType.MESH,\n  )\n  remote_copy_op.start()\n  \n  local_copy_op.wait()\n  # ... do work on local_scratch while waiting for async_copy_op to finish.\n  remote_copy_op.wait()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Forward-Backward Computation Visualization Utility\nDESCRIPTION: Defines a utility function that visualizes both the forward and backward computation graphs of a function. This is useful for understanding how checkpointing affects the computation pattern.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.tree_util import tree_flatten, tree_unflatten\n\nfrom rich.console import Console\nfrom rich.table import Table\nimport rich.text\n\ndef print_fwd_bwd(f, *args, **kwargs) -> None:\n  args, in_tree = tree_flatten((args, kwargs))\n\n  def f_(*args):\n    args, kwargs = tree_unflatten(in_tree, args)\n    return f(*args, **kwargs)\n\n  fwd = jax.make_jaxpr(lambda *args: jax.vjp(f_, *args))(*args).jaxpr\n\n  y, f_vjp = jax.vjp(f_, *args)\n  res, in_tree = tree_flatten(f_vjp)\n\n  def g_(*args):\n    *res, y = args\n    f_vjp = tree_unflatten(in_tree, res)\n    return f_vjp(y)\n\n  bwd = jax.make_jaxpr(g_)(*res, y).jaxpr\n\n  table = Table(show_header=False, show_lines=True, padding=(1, 2, 0, 2), box=None)\n  table.add_row(\"[bold green]forward computation:\",\n                \"[bold green]backward computation:\")\n  table.add_row(rich.text.Text.from_ansi(str(fwd)),\n                rich.text.Text.from_ansi(str(bwd)))\n  console = Console(width=240, force_jupyter=True)\n  console.print(table)\n\ndef _renderable_repr(self):\n  return self.html\nrich.jupyter.JupyterRenderable._repr_html_ = _renderable_repr\n```\n\n----------------------------------------\n\nTITLE: Implementing 1D Smoothing with JAX Convolution\nDESCRIPTION: Demonstrates basic 1D convolution using jax.numpy.convolve to smooth noisy sinusoidal data using a simple moving average window.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/convolutions.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\n\nkey = random.key(1701)\n\nx = jnp.linspace(0, 10, 500)\ny = jnp.sin(x) + 0.2 * random.normal(key, shape=(500,))\n\nwindow = jnp.ones(10) / 10\ny_smooth = jnp.convolve(y, window, mode='same')\n\nplt.plot(x, y, 'lightgray')\nplt.plot(x, y_smooth, 'black')\n```\n\n----------------------------------------\n\nTITLE: Visualizing JAX Final Type Promotion Lattice - NetworkX and Matplotlib - Python\nDESCRIPTION: This snippet produces a visual representation of the finalized JAX type promotion lattice, showing all type nodes, including weak scalar ('i*', 'f*', 'c*'), unsigned, signed, floating, and complex types, and their promotion relationships. Using NetworkX for the directed graph and matplotlib for display, the snippet specifies manual positioning to clarify the relations. The code requires networkx and matplotlib; expected output is a figure depicting the full lattice as described in JAX's reference. Some visual details can be customized by editing the position or optional drawing commands.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\\nimport matplotlib.pyplot as plt\\nlattice = {\\n  'i*': ['u8', 'i8'], 'f*': ['c*', 'f16', 'bf16'], 'c*': ['c64'],\\n  'u8': ['u16', 'i16'], 'u16': ['u32', 'i32'], 'u32': ['u64', 'i64'], 'u64': ['f*'],\\n  'i8': ['i16'], 'i16': ['i32'], 'i32': ['i64'], 'i64': ['f*'],\\n  'f16': ['f32'], 'bf16': ['f32'], 'f32': ['f64', 'c64'], 'f64': ['c128'],\\n  'c64': ['c128']\\n}\\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\\npos = {\\n  'i*': [-1.25, 0.5], 'f*': [4.5, 0.5], 'c*': [5, 1.5],\\n  'u8': [0.5, 0], 'u16': [1.5, 0], 'u32': [2.5, 0], 'u64': [3.5, 0],\\n  'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\\n  'f16': [5.75, 0.8], 'bf16': [5.75, 0.2], 'f32': [7, 0.5], 'f64': [8, 0.5],\\n  'c64': [7.5, 1.5], 'c128': [8.5, 1.5],\\n}\\nfig, ax = plt.subplots(figsize=(10, 4))\\nax.set_ylim(-0.5, 2)\\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\\n# ax.patches[12].set_linestyle((0, (2, 4)))\n```\n\n----------------------------------------\n\nTITLE: Handling Complex Cases with Nested jnp.where for Safe Gradients (Python)\nDESCRIPTION: Demonstrates applying the safe gradient pattern in a slightly more complex function `my_log_or_y`. It uses an outer `jnp.where` for the primary condition (`x > 0.`) and selects between `log(x)` and `y`. To ensure gradient safety, the `log(x)` branch uses the nested `jnp.where` pattern `jnp.log(jnp.where(x > 0., x, 1.))` to prevent `NaN`s arising from the logarithm's derivative at `x=0`. Requires `jax` and `jax.numpy`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n\ndef my_log_or_y(x, y):\n  \"\"\"Return log(x) if x > 0 or y\"\"\"\n  return jnp.where(x > 0., jnp.log(jnp.where(x > 0., x, 1.)), y)\n```\n\n----------------------------------------\n\nTITLE: Testing grad with nested JIT-compiled functions\nDESCRIPTION: Example showing how grad works with nested JIT-compiled functions, demonstrating the composability of transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_76\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  y = x * 2.\n  z = g(y)\n  return z\n\n@jit\ndef g(x):\n  return cos(x) * 2.\n\nprint(grad(f)(3.))\n```\n\n----------------------------------------\n\nTITLE: Illustrating Ineffective Checkpointing (Whole Function) in JAX Python\nDESCRIPTION: This snippet demonstrates an incorrect or ineffective use of `jax.checkpoint`. Applying `jax.checkpoint` to the entire composite function `f` does not save memory. It results in running the forward pass, discarding residuals (step 1), immediately re-running the forward pass to save residuals (step 2), and then running the backward pass (step 3), performing redundant computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef f_grad_bad(x):\n  _ = f(x)                  # step 1\n  _, f_vjp = jax.vjp(f, x)  # step 2\n  x_bar, = f_vjp(1.0)       # step 3\n  return x_bar\n```\n\n----------------------------------------\n\nTITLE: Specifying Explicit Symbolic Constraints with jax2tf.convert\nDESCRIPTION: Demonstrates how to use the polymorphic_constraints parameter to specify explicit constraints between dimension variables, ensuring shape compatibility for operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: x[:x.shape[1], :16],\n               polymorphic_shapes=\"(a, b)\",\n               polymorphic_constraints=(\"a >= b\", \"b >= 16\"))\n```\n\n----------------------------------------\n\nTITLE: Calculating 3rd Order Derivative using nth_order_derivative in Python\nDESCRIPTION: This snippet demonstrates calculating the 3rd order derivative using the `nth_order_derivative` function. As noted in the surrounding text, this is expected to be zero because the presumed function `foo` is a second-order polynomial. It calls the function with n=3 on `foo` at point 2.0.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_6\n\nLANGUAGE: ipython3\nCODE:\n```\n# The rest are zero because `foo` is only a second-order polymonial\nprint(nth_order_derivative(3, foo, 2.0))\n```\n\n----------------------------------------\n\nTITLE: Profiling JAX Computation and Generating Perfetto Trace Links (Python)\nDESCRIPTION: This Python snippet demonstrates how to use the JAX profiler to generate a computation trace suitable for visualization in the Perfetto UI. The context manager jax.profiler.trace initiates trace capture to the specified directory and, with create_perfetto_link=True, blocks execution until a link (to the trace visualizer) is visited. This example includes generation of random data, matrix multiplication, and synchronization to ensure all work is recorded. Dependencies include the JAX library (imported as 'jax'). Required parameters are the output path and the 'create_perfetto_link' flag. It captures all operations performed within the context. The output is a Perfetto-compatible trace file, and the process requires user interaction to proceed beyond link generation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n  # Run the operations to be profiled\n  key = jax.random.key(0)\n  x = jax.random.normal(key, (5000, 5000))\n  y = x @ x\n  y.block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Registering FFI Targets and Custom VJP with JAX (Python)\nDESCRIPTION: This snippet demonstrates how to register FFI targets for a custom operation ('rms_norm_fwd' and 'rms_norm_bwd') and define a custom VJP (vector-Jacobian product) rule using JAX's custom_vjp mechanism in Python. Dependencies include jax, numpy, and access to compiled FFI libraries (e.g., rms_norm_lib). Inputs include the forward function outputs as tuple (primal results and residuals), custom backward function, and VJP registration. Outputs ensure correct differentiation against a JAX-native reference. Limitations include lack of forward-mode AD support, and restrictions to reverse-mode differentiation only.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\njax.ffi.register_ffi_target(\n  \"rms_norm_fwd\", jax.ffi.pycapsule(rms_norm_lib.RmsNormFwd), platform=\"cpu\"\n)\njax.ffi.register_ffi_target(\n  \"rms_norm_bwd\", jax.ffi.pycapsule(rms_norm_lib.RmsNormBwd), platform=\"cpu\"\n)\n\n\ndef rms_norm_fwd(x, eps=1e-5):\n  y, res = jax.ffi.ffi_call(\n    \"rms_norm_fwd\",\n    (\n      jax.ShapeDtypeStruct(x.shape, x.dtype),\n      jax.ShapeDtypeStruct(x.shape[:-1], x.dtype),\n    ),\n    vmap_method=\"broadcast_all\",\n  )(x, eps=np.float32(eps))\n  return y, (res, x)\n\n\ndef rms_norm_bwd(eps, res, ct):\n  del eps\n  res, x = res\n  assert res.shape == ct.shape[:-1]\n  assert x.shape == ct.shape\n  return (\n    jax.ffi.ffi_call(\n      \"rms_norm_bwd\",\n      jax.ShapeDtypeStruct(ct.shape, ct.dtype),\n      vmap_method=\"broadcast_all\",\n    )(res, x, ct),\n  )\n\n\nrms_norm = jax.custom_vjp(rms_norm, nondiff_argnums=(1,))\nrms_norm.defvjp(rms_norm_fwd, rms_norm_bwd)\n\n# Check that this gives the right answer when compared to the reference version\nct_y = jnp.ones_like(x)\nnp.testing.assert_allclose(\n  jax.vjp(rms_norm, x)[1](ct_y), jax.vjp(rms_norm_ref, x)[1](ct_y), rtol=1e-5\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Neural Network Functions Without Checkpointing\nDESCRIPTION: Defines a simple neural network function with three layers, where each layer applies a matrix multiplication followed by a sine activation. This example shows the standard approach without using checkpointing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef g(W, x):\n  y = jnp.dot(W, x)\n  return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n  x = g(W1, x)\n  x = g(W2, x)\n  x = g(W3, x)\n  return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\n# Inspect the 'residual' values to be saved on the forward pass\n# if we were to evaluate `jax.grad(f)(W1, W2, W3, x)`\nfrom jax.ad_checkpoint import print_saved_residuals\njax.ad_checkpoint.print_saved_residuals(f, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Corrected JAX Function with Explicit Data Dependency\nDESCRIPTION: This revised Python function `f(x)` uses the modified `ppermute_start` and `ppermute_done`. `ppermute_start` now returns the input `x` (aliased), and this `x` is explicitly passed to `ppermute_done`. This creates a data dependency that prevents XLA from freeing `x` prematurely, ensuring the buffer remains valid throughout the asynchronous `ppermute` operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  *sems, x ,out = ppermute_start(x)\n  z = x + 1\n  y = ppermute_done(*sems, x, out)\n  return y, z\n```\n```\n\n----------------------------------------\n\nTITLE: Demonstrating type promotion with strongly-typed JAX values\nDESCRIPTION: This example shows how multiplication with a strongly-typed JAX integer causes type promotion, converting the int8 array to int32.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> jnp.int32(2) * x\nArray([0, 2, 4, 6, 8], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Enforcing Order in io_callback with Ordered Argument and vmap - Python\nDESCRIPTION: Illustrates enforcing callback ordering in io_callback with ordered=True, which disables vmap support and causes an error if attempted. Shows definition of an ordered variant and demonstrates the error upon vectorizing with vmap. Key for cases when deterministic side-effect order is required; vmap raises exception as ordered callbacks must preserve strict sequence.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef numpy_random_like_ordered(x):\n  return io_callback(host_side_random_like, x, x, ordered=True)\n\njax.vmap(numpy_random_like_ordered)(x)\n```\n\n----------------------------------------\n\nTITLE: Fusing Activation Functions into TPU Matmul Kernels with JAX and Pallas - Python\nDESCRIPTION: This code implements a high-performance matrix multiplication kernel for TPUs in Python, supporting optional activation fusion and transpose of the right-hand matrix. It uses the JAX and Pallas libraries to define both a kernel for block-wise matmul and an outer function that applies proper data tiling, casting, and grid specification via the TPU-specific Pallas interface. Dependencies include JAX, Pallas (pl, pltpu), NumPy, and functools. Inputs are JAX arrays x and y, with various block and activation parameters; the output matches the dtype and shape of input x (or as cast by activation). Limitations may include requirement for TPU hardware and dependencies, and that activation functions and data types must be compatible with JAX/Pallas APIs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_kernel(\n    x_ref, y_ref, z_ref, acc_ref, *, nsteps, transpose_rhs, activation\n):\n  @pl.when(pl.program_id(2) == 0)\n  def _():\n    acc_ref[...] = jnp.zeros_like(acc_ref)\n\n  if transpose_rhs:\n    dims = ((1,), (1,)), ((), ())\n  else:\n    dims = ((1,), (0,)), ((), ())\n\n  acc_ref[...] += jax.lax.dot_general(\n      x_ref[...],\n      y_ref[...],\n      dims,\n      preferred_element_type=jnp.float32,\n  )\n\n  @pl.when(pl.program_id(2) == nsteps - 1)\n  def _():\n    z_ref[...] = activation(acc_ref[...]).astype(z_ref.dtype)\n\n\n@functools.partial(jax.jit, static_argnames=['bm', 'bk', 'bn', 'activation'])\ndef matmul(\n    x: jax.Array,\n    y: jax.Array,\n    *,\n    bm: int = 128,\n    bk: int = 128,\n    bn: int = 128,\n    transpose_rhs: bool = False,\n    activation: Callable[[jax.Array], jax.Array] = lambda x: x,\n):\n  if transpose_rhs:\n    y = y.swapaxes(0, 1)\n    y_block_spec = pl.BlockSpec((bn, bk), lambda i, j, k: (j, k))\n  else:\n    y_block_spec = pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))\n  m, k = x.shape\n  _, n = y.shape\n  return pl.pallas_call(\n      functools.partial(\n          matmul_kernel,\n          nsteps=k // bk,\n          transpose_rhs=transpose_rhs,\n          activation=activation,\n      ),\n      grid_spec=pltpu.PrefetchScalarGridSpec(\n          num_scalar_prefetch=0,\n          in_specs=[\n              pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)),\n              y_block_spec,\n          ],\n          out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n          scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n          grid=(m // bm, n // bn, k // bk),\n      ),\n      out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\n      compiler_params=pltpu.TPUCompilerParams(\n          dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n  )(x, y)\n```\n\n----------------------------------------\n\nTITLE: Using checkify with User-Defined Checks in JAX\nDESCRIPTION: Shows how checkify automates the error plumbing process for user-defined checks. The example demonstrates adding a check for positive values and using the error object's throw method to raise an appropriate exception.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  checkify.check(x > 0., \"{} must be positive!\", x)  # convenient but effectful API\n  return jnp.log(x)\n\nf_checked = checkify(f)\n\nerr, x = jax.jit(f_checked)(-1.)\nerr.throw()\n# ValueError: -1. must be positive! (check failed at <...>:2 (f))\n```\n\n----------------------------------------\n\nTITLE: Extracting jaxpr for a Nested Python Function with Control Flow using JAX in Python\nDESCRIPTION: Shows how JAX traces a function with nested calls and Python control-flow, flattening higher-order and conditional constructs. Dependencies: JAX and jax.numpy. The primary function ('func3') calls another function ('func2'), which applies a further nested function ('inner') that contains a conditional based on input shape; the conditional is inlined during tracing. Inputs are two arrays; output is the printed jaxpr, which remains flat. The snippet underscores how Python-level constructs do not directly translate to jaxpr control-flow.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef func2(inner, first, second):\n  temp = first + inner(second) * 3.\n  return jnp.sum(temp)\n\ndef inner(second):\n  if second.shape[0] > 4:\n    return jnp.sin(second)\n  else:\n    assert False\n\ndef func3(first, second):\n  return func2(inner, first, second)\n\nprint(make_jaxpr(func3)(jnp.zeros(8), jnp.ones(8)))\n```\n\n----------------------------------------\n\nTITLE: Illustrating Gradient Computation Steps with jax.checkpoint in Python\nDESCRIPTION: This snippet manually outlines the steps performed when computing the gradient of `f_checkpoint` (where `g` was checkpointed). It shows the initial forward pass of `g` (step 1, residuals discarded), forward pass of `h` (step 2, residuals saved), backward pass of `h` (step 3), recomputation of `g`'s forward pass (step 4, residuals saved), and backward pass of `g` (step 5).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef f_checkpoint_grad(x):\n  y = g(x)                  # step 1\n  _, h_vjp = jax.vjp(h)(y)  # step 2\n  y_bar, = h_vjp(1.0)       # step 3\n  _, g_vjp = jax.vjp(g, x)  # step 4\n  x_bar, = g_vjp(y_bar)     # step 5\n  return x_bar\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Automatic Differentiation\nDESCRIPTION: Imports the gradient function from JAX, which is used for automatic differentiation of functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX NaN Detection with jax_debug_nans\nDESCRIPTION: Demonstrates how to enable NaN detection in JAX using jax_debug_nans configuration. When enabled, it raises a FloatingPointError when NaN values are produced in JIT-compiled code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/flags.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.config.update(\"jax_debug_nans\", True)\n\ndef f(x, y):\n  return x / y\njax.jit(f)(0., 0.)  # ==> raises FloatingPointError exception!\n```\n\n----------------------------------------\n\nTITLE: Rule 30 Simulation with Parallel Processing\nDESCRIPTION: Implements a Rule 30 cellular automaton simulation using parallel processing with halo exchange.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@partial(pmap, axis_name='i')\ndef step(board_slice):\n  left, right = board_slice[:1], board_slice[-1:]\n  right, left = send_left(left, 'i'), send_right(right, 'i')\n  enlarged_board_slice = jnp.concatenate([left, board_slice, right])\n  return update_board(enlarged_board_slice)\n```\n\n----------------------------------------\n\nTITLE: Neural Network Implementation with Parameter Lists\nDESCRIPTION: Defines a simple fully-connected neural network using explicit loops over layer parameters. This approach leads to longer compilation times for large networks but allows natural gradient flow.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nLayerParam = tuple[jnp.ndarray, jnp.ndarray]  # weights, bias pair for a layer\nParamsList = list[LayerParam]\n\ndef net(params: ParamsList, x: jnp.ndarray):\n  for W, b in params:\n    x = jnp.maximum(jnp.dot(x, W) + b, 0.)\n  return x\n```\n\n----------------------------------------\n\nTITLE: Avoiding NaN Gradients using Nested jnp.where (Python)\nDESCRIPTION: Presents the recommended way to implement functions like `log` to avoid `NaN` gradients. The `safe_for_grad_log` function nests a `jnp.where` *inside* the `jnp.log` call. This inner `jnp.where` ensures that `jnp.log` always receives a positive argument (replacing non-positive `x` with `1.`), thus preventing the undefined gradient at `x=0`. Consequently, `jax.grad(safe_for_grad_log)(0.)` correctly returns `0.`. Requires `jax` and `jax.numpy`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\ndef safe_for_grad_log(x):\n  return jnp.log(jnp.where(x > 0., x, 1.))\n\nprint(safe_for_grad_log(0.)) # ==> 0.  Ok\nprint(jax.grad(safe_for_grad_log)(0.)) # ==> 0.  Ok\n```\n\n----------------------------------------\n\nTITLE: Annotating Function Return Type for IDE Autocompletion in Python\nDESCRIPTION: This snippet demonstrates Level 2 type annotation from `jax.random`, focusing on improving IDE autocompletion. The `shuffle` function is annotated to return `jnp.ndarray`, an abstract base class representing JAX arrays. This allows IDEs to provide accurate method and attribute suggestions for the function's output, enhancing the developer experience. It depends on the `jnp.ndarray` type and potentially other JAX-specific types like `KeyArray` and `Array`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/12049-type-annotations.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef shuffle(key: KeyArray, x: Array, axis: int = 0) -> jnp.ndarray:\n  ...\n```\n```\n\n----------------------------------------\n\nTITLE: Using strict dtype promotion with context manager\nDESCRIPTION: This example shows how to enable strict dtype promotion using a context manager, which prevents implicit type promotion and raises TypePromotionError when attempting to mix incompatible types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> x = jnp.float32(1)\n>>> y = jnp.int32(1)\n>>> with jax.numpy_dtype_promotion('strict'):\n...   z = x + y  # doctest: +SKIP\n...\nTraceback (most recent call last):\nTypePromotionError: Input dtypes ('float32', 'int32') have no available implicit\ndtype promotion path when jax_numpy_dtype_promotion=strict. Try explicitly casting\ninputs to the desired output type, or set jax_numpy_dtype_promotion=standard.\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Issues with Iterators in JAX Functions\nDESCRIPTION: This snippet illustrates the problems that can arise when using iterators in JAX functions, particularly with control flow primitives.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import make_jaxpr\n\n# lax.fori_loop\narray = jnp.arange(10)\nprint(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\niterator = iter(range(10))\nprint(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0\n\n# lax.scan\ndef func11(arr, extra):\n    ones = jnp.ones(arr.shape)\n    def body(carry, aelems):\n        ae1, ae2 = aelems\n        return (carry + ae1 * ae2 + extra, carry)\n    return lax.scan(body, 0., (arr, ones))\nmake_jaxpr(func11)(jnp.arange(16), 5.)\n# make_jaxpr(func11)(iter(range(16)), 5.) # throws error\n\n# lax.cond\narray_operand = jnp.array([0.])\nlax.cond(True, lambda x: x+1, lambda x: x-1, array_operand)\niter_operand = iter(range(10))\n# lax.cond(True, lambda x: next(x)+1, lambda x: next(x)-1, iter_operand) # throws error\n```\n\n----------------------------------------\n\nTITLE: Applying JIT Compilation to a Function\nDESCRIPTION: Uses JAX's jit function to compile the previously defined function for faster execution on accelerators.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ng = jit(f)\ng(x)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Element Indexing Mode with Padding in Python\nDESCRIPTION: This code snippet demonstrates the use of pl.Element indexing mode with virtual padding. It pads the array with 1 row and 2 columns, showing how the program IDs are distributed in this case.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> # element, first pad the array with 1 row and 2 columns.\n>>> show_program_ids(x_shape=(7, 7),\n...                  block_shape=(pl.Element(2, (1, 0)),\n...                               pl.Element(3, (2, 0))),\n...                  grid=(4, 3),\n...                  index_map=lambda i, j: (2*i, 3*j))\n    [[ 0  1  1  1  2  2  2]\n     [10 11 11 11 12 12 12]\n     [10 11 11 11 12 12 12]\n     [20 21 21 21 22 22 22]\n     [20 21 21 21 22 22 22]\n     [30 31 31 31 32 32 32]\n     [30 31 31 31 32 32 32]]\n```\n\n----------------------------------------\n\nTITLE: Defining Type Aliases and Annotating Functions for Documentation in Python\nDESCRIPTION: This snippet showcases the use of `typing.Any` to create type aliases (`Array`, `Shape`) primarily for documentation purposes (Level 1) within the JAX codebase, specifically from `lax/slicing.py`. It defines the `slice` function signature using these aliases and standard types like `Sequence` and `Optional`, serving mainly as inline documentation without imposing strict static checking constraints.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/12049-type-annotations.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nArray = Any\nShape = core.Shape\n\ndef slice(operand: Array, start_indices: Sequence[int],\n          limit_indices: Sequence[int],\n          strides: Optional[Sequence[int]] = None) -> Array:\n  ...\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Helper Functions for Vectorized Batching in Python\nDESCRIPTION: Defines helper functions for producing mapped abstract values and moving batch dimensions. These functions are used in the vmap implementation to handle batch axes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\ndef mapped_aval(batch_dim, aval):\n  shape = list(aval.shape)\n  del shape[batch_dim]\n  return ShapedArray(tuple(shape), aval.dtype)\n\ndef move_batch_axis(axis_size, src, dst, x):\n  if src is not_mapped:\n    target_shape = list(np.shape(x))\n    target_shape.insert(dst, axis_size)\n    return broadcast(x, target_shape, [dst])\n  elif src == dst:\n    return x\n  else:\n    return moveaxis(x, src, dst)\n\ndef moveaxis(x, src: int, dst: int):\n  perm = [i for i in range(np.ndim(x)) if i != src]\n  perm.insert(dst, src)\n  return transpose(x, perm)\n```\n\n----------------------------------------\n\nTITLE: Executing Iota Kernel on GPU using Pallas\nDESCRIPTION: Shows how to call the iota kernel on GPU using pallas_call with a specified grid size.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef iota(size: int):\n  return pl.pallas_call(iota_kernel,\n                        out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),\n                        grid=(size,))()\niota(8)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Program IDs with Out-of-Bounds Writes in Python\nDESCRIPTION: Demonstrates calling `show_program_ids` with an array shape (7, 5) that does not evenly accommodate the blocks (2, 3) generated by the grid (4, 2). The output shows the program IDs written to the valid parts of the array, implicitly showing that writes outside the 7x5 bounds are discarded.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> # An example with out-of-bounds accesses\n>>> show_program_ids(x_shape=(7, 5), block_shape=(2, 3), grid=(4, 2),\n...                  index_map=lambda i, j: (i, j))\n[[ 0  0  0  1  1]\n [ 0  0  0  1  1]\n [10 10 10 11 11]\n [10 10 10 11 11]\n [20 20 20 21 21]\n [20 20 20 21 21]\n [30 30 30 31 31]]\n\n```\n\n----------------------------------------\n\nTITLE: Performance Benchmarking JAX on GPU/TPU\nDESCRIPTION: Benchmarks JAX matrix multiplication performance with block_until_ready() to ensure accurate timing by waiting for the accelerator to complete.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%timeit -n 5 -r 5 jnp.dot(x, x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Implementing Block Dynamic Slice with Scalar Prefetch (Pallas, Python)\nDESCRIPTION: This executable code defines a block-aligned dynamic slice kernel using Pallas, employing scalar prefetch for dynamic block indexing. It consists of a simple kernel function assigning input to output, a decorated function that sets up PrefetchScalarGridSpec, validates start indices, computes the block index, and invokes the kernel with checkify integration for runtime assertions. Inputs include a tensor, start indices, and block sizes; the output is the corresponding block slice. Requirements: JAX, Pallas, and checkify. Indices must be block-aligned (divisible by size), and VMEM size must fit the block.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef dynamic_slice_kernel(indices, x_ref, o_ref):\\n  del indices\\n  o_ref[...] = x_ref[...]\\n\\n@checkify.checkify\\n@functools.partial(jax.jit, static_argnums=(2,))\\ndef block_dynamic_slice(x, starts, sizes):\\n  grid_spec = pltpu.PrefetchScalarGridSpec(\\n      num_scalar_prefetch=1,\\n      grid=(1, 1),\\n      in_specs=[pl.BlockSpec(\\n          sizes,\\n          lambda i, j, block_idx: (block_idx[0], block_idx[1]))],\\n      out_specs=pl.BlockSpec(sizes, lambda *_: (0, 0)),\\n  )\\n\\n  kernel = pl.pallas_call(\\n    dynamic_slice_kernel,\\n    grid_spec=grid_spec,\\n    out_shape=jax.ShapeDtypeStruct(shape=sizes, dtype=x.dtype),\\n  )\\n  # Checkify inserts a runtime assert that starts are divisible by block size.\\n  checkify.check(starts[0] % sizes[0] == 0, \\\"Starts must be divisible by size.\\\")\\n  checkify.check(starts[1] % sizes[1] == 0, \\\"Starts must be divisible by size.\\\")\\n  block_idx = jnp.array([starts[0] // sizes[0], starts[1] // sizes[1]])\\n  return kernel(block_idx, x)\\n\\nshape = (512, 512)\\nx = jnp.reshape(jnp.arange(np.prod(shape), dtype=jnp.int32), shape)\\nerr, result = block_dynamic_slice(x, starts=(128, 256), sizes=(128, 128))\\nerr.throw()\\nref = lax.dynamic_slice(x, start_indices=(128, 256), slice_sizes=(128, 128))\\ndiff = jnp.max(jnp.abs(result - ref))\\nprint(\\\"Error |result - lax.dynamic_slice| =\\\", diff)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pallas Reduce-Scatter Operation with JAX\nDESCRIPTION: Creates a function that wraps the reduce-scatter kernel implementation with JAX's pallas_call mechanism. This function handles reshaping input data and configuring the compiler parameters for TPU execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef pallas_reduce_scatter(input_arr):\n  input_arr = input_arr.reshape(\n      num_devices, outer_block_size[0], outer_block_size[1]\n  )\n  return pl.pallas_call(\n      reduce_scatter_kernel,\n      out_shape=out_shape,\n      grid_spec=grid_spec,\n      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n  )(input_arr)[0]\n\n\npallas_result = jax.jit(\n    shard_map.shard_map(\n        pallas_reduce_scatter,\n        mesh=mesh,\n        in_specs=P(None, 'x'),\n        out_specs=P('x', None),\n        check_rep=False,\n    )\n)(input_arr)\n\npallas_result = jax.block_until_ready(pallas_result)\n```\n\n----------------------------------------\n\nTITLE: TPU Device Assignment for jax2tf Converted Functions\nDESCRIPTION: Shows how to explicitly assign a TPU device when using jax2tf-converted functions with jit_compile=False. This workaround ensures the function executes on TPU instead of CPU when explicit compilation is disabled.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nf_tf = jax2tf.convert(jnp.sin)\nx = np.float32(.5)\n\n@tf.function(autograph=False, jit_compile=False)\ndef f_tf_wrapped(x):\n  with tf.device('/device:TPU:0'):\n    return f_tf(x)\n\nwith tf.device('/device:TPU:0'):\n  self.assertAllClose(np.sin(x), f_tf_wrapped(x))\n```\n\n----------------------------------------\n\nTITLE: NumPy Random Number Generation\nDESCRIPTION: Shows sequential random number generation using NumPy's global state-based PRNG.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/random-numbers.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(np.random.random())\nprint(np.random.random())\nprint(np.random.random())\n```\n\n----------------------------------------\n\nTITLE: Using nondiff_argnums for Non-Array Arguments in custom_vjp - JAX - Python\nDESCRIPTION: This code demonstrates an appropriate use case for nondiff_argnums in custom_vjp, passing a non-array argument (Python callable) as nondiff_argnums. The function skip_app applies the callable f to x and defines custom forward and backward rules. Dependencies: functools.partial and JAX. Inputs: f (callable), x (array or compatible input). Outputs: f(x). Limitations: Only suitable for truly non-array nondiff_argnums (not Tracers or arrays).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4008-custom-vjp-update.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\\nimport jax\\n\\n@partial(jax.custom_vjp, nondiff_argnums=(0,))\\ndef skip_app(f, x):\\n  return f(x)\\n\\ndef skip_app_fwd(f, x):\\n  return skip_app(f, x), None\\n\\ndef skip_app_bwd(f, _, g):\\n  return (g,)\\n\\nskip_app.defvjp(skip_app_fwd, skip_app_bwd)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Frontend rms_norm API Using jax.ffi.ffi_call - Python\nDESCRIPTION: This code defines a Python wrapper for the FFI-backed rms_norm operation using JAX's ffi_call API. It checks the input's dtype, creates a bound ffi_call for the 'rms_norm' target, configures shape/dtype and vmap behavior, and returns the callable. The snippet illustrates essential FFI details such as static argument requirements and numpy dtype usage for C float compatibility. Inputs are an array 'x' and an optional epsilon 'eps'. Outputs are the normalized array matching the input shape and dtype. Testing checks equivalence to a reference implementation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\\n\\n\\ndef rms_norm(x, eps=1e-5):\\n  # We only implemented the `float32` version of this function, so we start by\\n  # checking the dtype. This check isn't strictly necessary because type\\n  # checking is also performed by the FFI when decoding input and output\\n  # buffers, but it can be useful to check types in Python to raise more\\n  # informative errors.\\n  if x.dtype != jnp.float32:\\n    raise ValueError(\"Only the float32 dtype is implemented by rms_norm\")\\n\\n  call = jax.ffi.ffi_call(\\n    # The target name must be the same string as we used to register the target\\n    # above in `register_custom_call_target`\\n    \"rms_norm\",\\n\\n    # In this case, the output of our FFI function is just a single array with\\n    # the same shape and dtype as the input. We discuss a case with a more\\n    # interesting output type below.\\n    jax.ShapeDtypeStruct(x.shape, x.dtype),\\n\\n    # The `vmap_method` parameter controls this function's behavior under `vmap`\\n    # as discussed below.\\n    vmap_method=\"broadcast_all\",\\n  )\\n\\n  # Note that here we're use `numpy` (not `jax.numpy`) to specify a dtype for\\n  # the attribute `eps`. Our FFI function expects this to have the C++ `float`\\n  # type (which corresponds to numpy's `float32` type), and it must be a\\n  # static parameter (i.e. not a JAX array).\\n  return call(x, eps=np.float32(eps))\\n\\n\\n# Test that this gives the same result as our reference implementation\\nx = jnp.linspace(-0.5, 0.5, 32).reshape((8, 4))\\nnp.testing.assert_allclose(rms_norm(x), rms_norm_ref(x), rtol=1e-5)\n```\n\n----------------------------------------\n\nTITLE: Switching Among Branches Using JAX lax.switch (Python)\nDESCRIPTION: This snippet demonstrates the use of jax.lax.switch to select among multiple functional branches at runtime in Python. It requires JAX installed and uses a list of lambda functions each taking a single input; the correct one is selected via the 'index' argument. Inputs are an index and a numerical argument, output is the result of the selected function. Only one operand is passed to the chosen functional; care must be taken that index is within range.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\n\ndef one_of_three(index, arg):\n  return lax.switch(index, [lambda x: x + 1.,\n                            lambda x: x - 2.,\n                            lambda x: x + 3.],\n                    arg)\n\nprint(make_jaxpr(one_of_three)(1, 5.))\n```\n\n----------------------------------------\n\nTITLE: Computing gradients with non-differentiable arguments in custom_jvp\nDESCRIPTION: Demonstrates taking the gradient of the app function with respect to its second argument, showing how JAX correctly handles the non-differentiable function argument.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_65\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(app, 1)(lambda x: x ** 3, 3.))\n```\n\n----------------------------------------\n\nTITLE: Tracing a Function with Flattened Tuple Inputs using JAX in Python\nDESCRIPTION: Illustrates how JAX handles structured (tuple) inputs by flattening them during tracing and jaxpr extraction. Dependencies: JAX and jax.numpy. The function takes a tuple (pair) as input, performs element-wise computation combining sine and addition, multiplies by a constant, and returns the sum. Input: a tuple of two arrays; Output: printed jaxpr with flattened input variables. Demonstrates that jaxpr has no tuple types and handles structured data as flat input lists.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef func4(arg):  # The `arg` is a pair.\n  temp = arg[0] + jnp.sin(arg[1]) * 3.\n  return jnp.sum(temp)\n\nprint(make_jaxpr(func4)((jnp.zeros(8), jnp.ones(8))))\n```\n\n----------------------------------------\n\nTITLE: Implementing Block-Based Index Mapping for Pipelining in JAX Pallas\nDESCRIPTION: Creates an index mapping function for a BlockSpec that selects a specific block of data based on the iteration index. This function is used to specify how to construct inputs for each kernel invocation in a pipeline.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef x_index_map(i):\n  return (i, 0)\n```\n\n----------------------------------------\n\nTITLE: Unpacking Future to Show Aliasing in ppermute Example\nDESCRIPTION: This version of `f(x)` explicitly unpacks the results from `ppermute_start`, including the aliased input `x2` (which is identical to the original `x`) and the output buffer `y`. This highlights the structure that XLA analyzes for potential buffer conflicts and aliasing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  *sems, x2, y = ppermute_start(x)\n  z = x + 1\n  y = ppermute_done((*sems, x2, y))\n  return y, z\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Arrays with Default 32-bit Types in JAX\nDESCRIPTION: Demonstrates JAX's default behavior with jax_enable_x64=False, where all array creation functions default to 32-bit datatypes regardless of what is requested.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/default_dtypes.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax.numpy as jnp\n\n>>> jnp.arange(5)\nArray([0, 1, 2, 3, 4], dtype=int32)\n\n>>> jnp.zeros(5)\nArray([0., 0., 0., 0., 0.], dtype=float32)\n\n>>> jnp.ones(5, dtype=int)\nArray([1, 1, 1, 1, 1], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Implementing Effects Barrier in JAX\nDESCRIPTION: Defines a function to block on the output token, ensuring all side effects have completed before continuing execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef effects_barrier():\n  output_token.block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Extracting Pytree Leaves - JAX - Python\nDESCRIPTION: This code demonstrates how to extract all leaves from multiple example pytrees using jax.tree.leaves. It creates several nested data structures (lists, tuples, dicts, numpy arrays) and iterates through them, printing the total number of leaves and their contents. The snippet depends on JAX and JAX NumPy (imported as jnp), and the output explains the leaf structure for each container, aiding understanding of pytree flattening.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\\nimport jax.numpy as jnp\\n\\nexample_trees = [\\n    [1, 'a', object()],\\n    (1, (2, 3), ()),\\n    [1, {'k1': 2, 'k2': (3, 4)}, 5],\\n    {'a': 2, 'b': (2, 3)},\\n    jnp.array([1, 2, 3]),\\n]\\n\\n# Print how many leaves the pytrees have.\\nfor pytree in example_trees:\\n  # This `jax.tree.leaves()` method extracts the flattened leaves from the pytrees.\\n  leaves = jax.tree.leaves(pytree)\\n  print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")\n```\n\n----------------------------------------\n\nTITLE: Migrating pjit Host-Local Inputs using multihost_utils in Python\nDESCRIPTION: This snippet provides the recommended approach for migrating multi-process `pjit` computations that use host-local inputs when `jax.Array` is enabled. It utilizes `jax.experimental.multihost_utils.host_local_array_to_global_array` to explicitly cast local input arrays to globally shaped arrays before passing them to `pjit`. Similarly, `global_array_to_host_local_array` can be used to convert global outputs back to host-local arrays if needed. This explicitly handles the global view required by `pjit` under `jax.Array`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import multihost_utils\n\nglobal_inps = multihost_utils.host_local_array_to_global_array(\n    local_inputs, mesh, in_pspecs)\n\nglobal_outputs = pjit(f, in_shardings=in_pspecs,\n                      out_shardings=out_pspecs)(global_inps)\n\nlocal_outs = multihost_utils.global_array_to_host_local_array(\n    global_outputs, mesh, out_pspecs)\n```\n\n----------------------------------------\n\nTITLE: JAX Random Key Generation\nDESCRIPTION: Shows JAX's explicit state management through random keys using the new-style typed PRNG keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/random-numbers.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import random\n\nkey = random.key(42)\nprint(key)\n```\n\n----------------------------------------\n\nTITLE: Using with_sharding_constraint to Transpose Sharding Pattern in JAX\nDESCRIPTION: Defines a JIT-compiled function that adds 1 to an input array and then applies a sharding constraint to transpose the sharding pattern from ('x', 'y') to ('y', 'x'). This demonstrates how to explicitly control the sharding of intermediate values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  x = x + 1\n  y = jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P('y', 'x')))\n  return y\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Just-In-Time Compilation\nDESCRIPTION: Imports the jit function from JAX, which is used to compile functions for faster execution on accelerators.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\n```\n\n----------------------------------------\n\nTITLE: Defensive Initialization of Custom PyTree Class (Python)\nDESCRIPTION: Provides an alternative __init__ method for a custom PyTree that defensively checks the type of its input before applying array conversion. This prevents errors when JAX transformations pass objects or None. No external dependencies beyond jax.numpy required. The main parameter is 'a', which can be an object, None, a MyTree instance, or suitable for jnp.asarray. Outputs a robustly initialized MyTree instance that avoids exceptions even when transformations pass unexpected types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pytrees.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass MyTree:\n  def __init__(self, a):\n    if not (type(a) is object or a is None or isinstance(a, MyTree)):\n      a = jnp.asarray(a)\n    self.a = a\n```\n\n----------------------------------------\n\nTITLE: Implementing Primitive Binding Logic\nDESCRIPTION: Implements the bind function and top trace finding logic. Handles routing primitive operations through the appropriate interpreter based on tracer levels.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef bind(prim, *args, **params):\n  top_trace = find_top_trace(args)\n  tracers = [full_raise(top_trace, arg) for arg in args]\n  outs = top_trace.process_primitive(prim, tracers, params)\n  return [full_lower(out) for out in outs]\n\nimport operator as op\n\ndef find_top_trace(xs) -> Trace:\n  top_main = max((x._trace.main for x in xs if isinstance(x, Tracer)),\n                 default=trace_stack[0], key=op.attrgetter('level'))\n  if dynamic_trace and dynamic_trace.level > top_main.level:\n    top_main = dynamic_trace\n  return top_main.trace_type(top_main)\n```\n\n----------------------------------------\n\nTITLE: Converting Mixed Partition Specs with FROM_GDA\nDESCRIPTION: Example of migrating pjit code that uses a mix of FROM_GDA and PartitionSpecs to use host_local_array_to_global_array with jax.Array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npjitted_f = pjit(\n    f, in_shardings=(FROM_GDA, P('x'), FROM_GDA, P(None)),\n    out_shardings=...)\npjitted_f(gda1, np_array1, gda2, np_array2)\n```\n\n----------------------------------------\n\nTITLE: Using grad for Real Output of Complex Input Functions - JAX - Python\nDESCRIPTION: Defines a function computing the squared magnitude of the input (from its real and imaginary parts), and shows using grad to compute the derivative at a complex value. The grad(f)(z) call illustrates JAX's ability to compute gradients of complex-to-real functions. Requires jnp and grad imports and a variable z as input.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef f(z):\n  x, y = jnp.real(z), jnp.imag(z)\n  return x**2 + y**2\n\nz = 3. + 4j\ngrad(f)(z)\n\n```\n\n----------------------------------------\n\nTITLE: Using `slices_for_invocation` with Partially Out-of-Bounds Access in Python\nDESCRIPTION: Illustrates `slices_for_invocation` when the calculated block extends beyond the array dimensions. The array shape is (100, 90), block shape is (10, 20), and grid is (10, 5). For invocation (2, 4), the second slice `slice(80, 100, None)` exceeds the second dimension's size (90), demonstrating how Pallas handles out-of-bounds access (padding on input, discarding on output).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> # An example when the block is partially out-of-bounds in the 2nd axis.\n>>> slices_for_invocation(x_shape=(100, 90),\n...                       x_spec = pl.BlockSpec((10, 20), lambda i, j: (i, j)),\n...                       grid = (10, 5),\n...                       invocation_indices = (2, 4))\n[slice(20, 30, None), slice(80, 100, None)]\n\n```\n\n----------------------------------------\n\nTITLE: Analyzing Vectorized Dot Product with make_jaxpr\nDESCRIPTION: Uses make_jaxpr to show the intermediate representation of a vectorized dot product operation across the first dimension of two arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nmake_jaxpr(vmap(jnp.dot))(jnp.ones((10, 8)), jnp.ones((10, 8)))\n```\n\n----------------------------------------\n\nTITLE: JAX LAX Type Promotion Implementation\nDESCRIPTION: Implements lower-level type promotion testing using JAX's lax interface, which doesn't perform implicit type promotion. Handles both standard and weakly-typed arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport pandas as pd\nfrom IPython import display\njax.config.update('jax_enable_x64', True)\n\njnp_dtypes = {\n  'b': jnp.bool_.dtype,\n  'u8': jnp.uint8.dtype, 'u16': jnp.uint16.dtype, 'u32': jnp.uint32.dtype, 'u64': jnp.uint64.dtype,\n  'i8': jnp.int8.dtype, 'i16': jnp.int16.dtype, 'i32': jnp.int32.dtype, 'i64': jnp.int64.dtype,\n  'bf16': jnp.bfloat16.dtype, 'f16': jnp.float16.dtype, 'f32': jnp.float32.dtype, 'f64': jnp.float64.dtype,\n  'c64': jnp.complex64.dtype, 'c128': jnp.complex128.dtype,\n  'i*': int, 'f*': float, 'c*': complex}\n\njnp_dtype_to_code = {val: key for key, val in jnp_dtypes.items()}\n\ndef make_jnp_zero(dtype):\n  if dtype in {int, float, complex}:\n    return dtype(0)\n  else:\n    return jnp.zeros((), dtype=dtype)\n\ndef jnp_result_code(dtype1, dtype2):\n  try:\n    out = jax.lax.add(make_jnp_zero(dtype1), make_jnp_zero(dtype2))\n  except TypeError:\n    return '-'\n  else:\n    if hasattr(out, 'aval') and out.aval.weak_type:\n      return out.dtype.kind + '*'\n    elif type(out) in {int, float, complex}:\n      return jnp_dtype_to_code[type(out)]\n    else:\n      return jnp_dtype_to_code[out.dtype]\n\ngrid = [[jnp_result_code(dtype1, dtype2)\n         for dtype2 in jnp_dtypes.values()]\n        for dtype1 in jnp_dtypes.values()]\ntable = pd.DataFrame(grid, index=jnp_dtypes.keys(), columns=jnp_dtypes.keys())\ndisplay.HTML(table.to_html())\n```\n\n----------------------------------------\n\nTITLE: Programmatic Capture of TensorBoard Profiler Trace with JAX (Python)\nDESCRIPTION: This Python snippet captures a TensorBoard profiler trace using the jax.profiler.start_trace and jax.profiler.stop_trace functions. Trace data is saved in the specified directory (here, /tmp/tensorboard). It includes a random key generation, matrix operation, and device synchronization. Dependencies: import jax. The block_until_ready call is necessary to ensure all device work is included in the trace. Outputs a profiler trace to the provided logdir for later analysis in TensorBoard.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n\njax.profiler.start_trace(\"/tmp/tensorboard\")\n\n# Run the operations to be profiled\nkey = jax.random.key(0)\nx = jax.random.normal(key, (5000, 5000))\ny = x @ x\ny.block_until_ready()\n\njax.profiler.stop_trace()\n```\n\n----------------------------------------\n\nTITLE: Using shard_map with Partial Sum Operations\nDESCRIPTION: Example showing how to use shard_map with partial sum operations across specific mesh axes. Demonstrates how output specifications affect the resulting array shape when combined with collective operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@partial(shard_map, mesh=m, in_specs=P('i', 'j'), out_specs=P('i', None))\ndef f3(x_block):\n  return jax.lax.psum(x_block, 'j')\n\nx = np.arange(12 * 12).reshape(12, 12)\ny3 = f3(x)\nprint(y3.shape)  # (12,6)\n```\n\n----------------------------------------\n\nTITLE: Computing Averages with Symbolic Dimensions in JAX\nDESCRIPTION: Illustrates how to compute averages using jax2tf.convert with polymorphic shapes, where the batch dimension is represented symbolically.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: jnp.sum(x, axis=0) / x.shape[0],\n               polymorphic_shapes=[\"(v, _)\"])(np.ones((3, 4)))\n```\n\n----------------------------------------\n\nTITLE: Converting Host Local Arrays to Global Arrays\nDESCRIPTION: Demonstrates how to convert host local inputs to global jax.Array in multi-process environments using host_local_array_to_global_array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import multihost_utils\n\nbatch = multihost_utils.host_local_array_to_global_array(\n    batch, mesh, batch_partition_spec)\n```\n\n----------------------------------------\n\nTITLE: Applying jax.checkpoint with Policy Argument for Selective Rematerialization - JAX (Python)\nDESCRIPTION: This snippet shows how to use jax.checkpoint (or jax.remat) with the policy argument to control which intermediate results are saved during the forward pass. By specifying policy=jax.checkpoint_policies.checkpoint_dots, only matrix multiplication results are saved, improving performance by reducing memory usage on TPUs where elementwise computations are cheap. Required dependencies are functools.partial, jax, and jnp; input parameters include a list or array of parameters and an input array x. The output of predict is the result of passing x through a sequence of sinusoidal and dot-product layers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/11830-new-remat-checkpoint.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nimport jax\n\ndef apply_layer(W, x):\n  return jnp.sin(jnp.dot(W, x))\n\n@partial(jax.checkpoint, policy=jax.checkpoint_policies.checkpoint_dots)\ndef predict(params, x):\n  for W in params[:-1]:\n    x = apply_layer(W, x)\n  return jnp.dot(params[-1], x)\n```\n\n----------------------------------------\n\nTITLE: Performing Indexed Addition in JAX\nDESCRIPTION: This snippet demonstrates how to perform indexed addition on JAX arrays using the `.at` property, showcasing more complex update operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprint(\"original array:\")\njax_array = jnp.ones((5, 6))\nprint(jax_array)\n\nnew_jax_array = jax_array.at[::2, 3:].add(7.)\nprint(\"new array post-addition:\")\nprint(new_jax_array)\n```\n\n----------------------------------------\n\nTITLE: Performing Matrix Multiplication with JAX\nDESCRIPTION: This snippet demonstrates matrix multiplication using JAX's NumPy-like operations. It shows how to perform dot product and access specific elements of the result.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ny = jnp.dot(x, x)\nprint(y[0, 0])\n```\n\n----------------------------------------\n\nTITLE: Allocating an MMA Accumulator from Existing Data in Pallas (Python)\nDESCRIPTION: Illustrates allocating an MMA accumulator initialized with external data using 'pl.run_state' and 'plgpu.ACC.init(init_array)'. The 'compute' function operates on the accumulator, and only the final value is returned. Awaiting all outstanding operations is handled automatically. Dependencies: 'pl', 'plgpu', 'jnp', and initialization array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef compute(acc_ref):\\n  ...\\n  return # pl.run_state only returns the final value of the accumulator\\noutput = pl.run_state(compute)(plgpu.ACC.init(init_array))\n```\n\n----------------------------------------\n\nTITLE: Registering nanobind Extension Handler with JAX FFI - Python\nDESCRIPTION: This Python snippet demonstrates registering a nanobind-wrapped FFI handler with JAX's FFI system. By importing the compiled nanobind extension (assumed to be 'rms_norm'), it retrieves the capsule and registers it as the 'rms_norm' FFI target for the 'cpu' platform. Dependencies are JAX (with FFI), the compiled nanobind extension (rms_norm), and all its transitively required libraries. No input/output is produced beyond handler registration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Assuming that we compiled a nanobind extension called `rms_norm`:\\nimport rms_norm as rms_norm_lib\\n\\njax.ffi.register_ffi_target(\"rms_norm\", rms_norm_lib.rms_norm(), platform=\"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Using Device-Resident Arrays with JIT and JVP\nDESCRIPTION: Demonstrates the use of device-resident arrays with JIT compilation and automatic differentiation, avoiding unnecessary host-device transfers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_56\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\nx, xdot = 3., 1.\ny, ydot = jvp(f, (x,), (xdot,))\nprint(y)\nprint(ydot)\n```\n\n----------------------------------------\n\nTITLE: Installing JAX with NVIDIA GPU Support via pip\nDESCRIPTION: Command to install JAX with NVIDIA CUDA 12 support using pip, which installs all required CUDA libraries automatically. This is the recommended approach for NVIDIA GPU users.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade pip\n\n# NVIDIA CUDA 12 installation\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda12]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing PartialEvalTracer for JAX Partial Evaluation\nDESCRIPTION: The PartialEvalTracer class is a tracer that tracks partial values and their recipes during partial evaluation. It maintains both the partial value and the recipe for how it was computed, forming nodes in the computational graph.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nclass PartialEvalTracer(Tracer):\n  pval: PartialVal\n  recipe: JaxprRecipe | None\n\n  def __init__(self, trace, pval, recipe):\n    self._trace = trace\n    self.pval = pval\n    self.recipe = recipe\n\n  aval = property(lambda self: self.pval.aval)\n\n  def full_lower(self):\n    if self.pval.is_known:\n      return full_lower(self.pval.const)\n    return self\n```\n\n----------------------------------------\n\nTITLE: Converting tracers to jaxpr in JAX\nDESCRIPTION: Implements the conversion of tracers to a jaxpr (JAX program representation). It handles different types of recipes (LambdaBinding, Const, JaxprEqn) and builds the jaxpr structure.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_64\n\nLANGUAGE: Python\nCODE:\n```\ndef tracers_to_jaxpr(tracers_in: list[PartialEvalTracer],\n                     tracers_out: list[PartialEvalTracer]):\n  tracer_to_var: dict[int, Var] = {id(t): Var(raise_to_shaped(t.aval))\n                                   for t in tracers_in}\n  constvar_to_val: dict[int, Any] = {}\n  constid_to_var: dict[int, Var] = {}\n  processed_eqns: set[int] = set()\n  eqns: list[JaxprEqn] = []\n  for t in toposort(tracers_out, tracer_parents):\n    if isinstance(t.recipe, LambdaBindingRecipe):\n      assert id(t) in set(map(id, tracers_in))\n    elif isinstance(t.recipe, ConstRecipe):\n      val = t.recipe.val\n      var = constid_to_var.get(id(val))\n      if var is None:\n        aval = raise_to_shaped(get_aval(val))\n        var = constid_to_var[id(val)] = Var(aval)\n        constvar_to_val[var] = val\n      tracer_to_var[id(t)] = var\n    elif isinstance(t.recipe, JaxprEqnRecipe):\n      if id(t.recipe) not in processed_eqns:\n        eqns.append(recipe_to_eqn(tracer_to_var, t.recipe))\n        processed_eqns.add(id(t.recipe))\n    else:\n      raise TypeError(t.recipe)\n\n  constvars, constvals = unzip2(constvar_to_val.items())\n  in_binders = constvars + [tracer_to_var[id(t)] for t in tracers_in]\n  out_vars = [tracer_to_var[id(t)] for t in tracers_out]\n  jaxpr = Jaxpr(in_binders, eqns, out_vars)\n  typecheck_jaxpr(jaxpr)\n  return jaxpr, constvals\n\ndef recipe_to_eqn(tracer_to_var: dict[int, Var], recipe: JaxprEqnRecipe\n                  ) -> JaxprEqn:\n  inputs = [tracer_to_var[id(t)] for t in recipe.tracers_in]\n  out_binders = [Var(aval) for aval in recipe.avals_out]\n  for t_ref, var in zip(recipe.tracer_refs_out, out_binders):\n    if t_ref() is not None: tracer_to_var[id(t_ref())] = var\n  return JaxprEqn(recipe.prim, inputs, recipe.params, out_binders)\n\ndef tracer_parents(t: PartialEvalTracer) -> list[PartialEvalTracer]:\n  return t.recipe.tracers_in if isinstance(t.recipe, JaxprEqnRecipe) else []\n```\n\n----------------------------------------\n\nTITLE: Demonstrating NumPy's Sequential-Equivalent PRNG Guarantee in Python\nDESCRIPTION: This Python snippet using NumPy illustrates the 'sequential-equivalent guarantee'. It shows that generating an array of size 2 (`rng.randn(2)`) produces the same sequence of random numbers as generating two individual numbers sequentially (`np.stack([rng.randn() for _ in range(2)])`) when starting from the same `RandomState`. JAX intentionally drops this guarantee to enable more efficient vectorization using counter-based PRNGs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nIn [1]: rng = np.random.RandomState(0)\n\nIn [2]: rng.randn(2)\nOut[2]: array([1.76405235, 0.40015721])\n\nIn [3]: rng = np.random.RandomState(0)\n\nIn [4]: np.stack([rng.randn() for _ in range(2)])\nOut[4]: array([1.76405235, 0.40015721])\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Evaluation Rule for JAX cond Primitive\nDESCRIPTION: Defines the evaluation implementation for the cond primitive. It evaluates either the true_jaxpr or false_jaxpr based on the predicate value using the provided operands.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_81\n\nLANGUAGE: python\nCODE:\n```\ndef cond_impl(pred, *operands, true_jaxpr, false_jaxpr):\n  if pred:\n    return eval_jaxpr(true_jaxpr, operands)\n  else:\n    return eval_jaxpr(false_jaxpr, operands)\nimpl_rules[cond_p] = cond_impl\n```\n\n----------------------------------------\n\nTITLE: Implementing Lorentz Dynamics in JAX\nDESCRIPTION: Defines the Lorentz attractor system using JAX. The function 'f' represents the differential equations of the Lorentz system, which is used in the ODE solver.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsigma = 10.\nbeta = 8./3\nrho = 28.\n\n@jit\ndef f(state, t):\n  x, y, z = state\n  return jnp.array([sigma * (y - x), x * (rho - z) - y, x * y - beta * z])\n```\n\n----------------------------------------\n\nTITLE: Executing a JIT-Compiled Function\nDESCRIPTION: Executes a JIT-compiled function. The first call traces and compiles the function, while subsequent calls reuse the compiled version from cache.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nz = f(3., 4.)  # 'tracing!' prints the first time\nprint(z)\n```\n\n----------------------------------------\n\nTITLE: Using checkify with pjit in JAX\nDESCRIPTION: Shows how to use checkify with JAX's pjit transformation for partitioned execution. The example demonstrates specifying sharding for inputs and outputs, with the error value not being partitioned.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return x / x\n\nf = checkify.checkify(f, errors=checkify.float_checks)\nf = pjit(\n  f,\n  in_shardings=PartitionSpec('x', None),\n  out_shardings=(None, PartitionSpec('x', None)))\n\nwith jax.sharding.Mesh(mesh.devices, mesh.axis_names):\n err, data = f(input_data)\nerr.throw()\n# ValueError: divided by zero at <...>:4 (f)\n```\n\n----------------------------------------\n\nTITLE: Implementing the grad function for gradient computation\nDESCRIPTION: Implementation of the grad function that computes gradients of scalar-valued functions with respect to their inputs using vjp.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_73\n\nLANGUAGE: python\nCODE:\n```\ndef grad(f):\n  def gradfun(x, *xs):\n    y, f_vjp = vjp(f, x, *xs)\n    if np.shape(y) != (): raise TypeError\n    x_bar, *_ = f_vjp(np.ones(np.shape(y), np.result_type(y)))\n    return x_bar\n  return gradfun\n```\n\n----------------------------------------\n\nTITLE: Creating Utility for Visualizing Forward and Backward Passes\nDESCRIPTION: Defining a utility function using Rich library to print and compare JAX's forward and backward computation graphs for better understanding of how checkpointing affects automatic differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.tree_util import tree_flatten, tree_unflatten\n\nfrom rich.console import Console\nfrom rich.table import Table\nimport rich.text\n\ndef print_fwd_bwd(f, *args, **kwargs) -> None:\n  args, in_tree = tree_flatten((args, kwargs))\n\n  def f_(*args):\n    args, kwargs = tree_unflatten(in_tree, args)\n    return f(*args, **kwargs)\n\n  fwd = jax.make_jaxpr(lambda *args: jax.vjp(f_, *args))(*args).jaxpr\n\n  y, f_vjp = jax.vjp(f_, *args)\n  res, in_tree = tree_flatten(f_vjp)\n\n  def g_(*args):\n    *res, y = args\n    f_vjp = tree_unflatten(in_tree, res)\n    return f_vjp(y)\n\n  bwd = jax.make_jaxpr(g_)(*res, y).jaxpr\n\n  table = Table(show_header=False, show_lines=True, padding=(1, 2, 0, 2), box=None)\n  table.add_row(\"[bold green]forward computation:\",\n                \"[bold green]backward computation:\")\n  table.add_row(rich.text.Text.from_ansi(str(fwd)),\n                rich.text.Text.from_ansi(str(bwd)))\n  console = Console(width=240, force_jupyter=True)\n  console.print(table)\n\ndef _renderable_repr(self):\n  return self.html\nrich.jupyter.JupyterRenderable._repr_html_ = _renderable_repr\n```\n\n----------------------------------------\n\nTITLE: Handling Shape Polynomial Division in JAX to TensorFlow XLA Conversion\nDESCRIPTION: This error occurs when converting a JAX CNN model with dynamic shapes to TensorFlow XLA. The conversion fails due to an inability to compute stride for dimension 'b' with window size and stride of 2, as it cannot divide 'b + -2' by '2'.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nInconclusiveDimensionOperation(\"Cannot compute stride for dimension 'b', window_size '2', stride '2'.\\nDetails: Cannot divide 'b + -2' by '2'.\\nSee https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#division-of-shape-polynomials-is-partially-supported.\\n\\nThis error arises for arithmetic or comparison operations with shapes that\\nare non-constant, and the result of the operation cannot be represented as\\na polynomial of dimension variables, or a boolean constant (for comparisons).\\n\\nPlease see https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#computing-with-dimension-variables\\nfor more details.\\n.\\n\\nThis error arises for arithmetic or comparison operations with shapes that\\nare non-constant, and the result of the operation cannot be represented as\\na polynomial of dimension variables, or a boolean constant (for comparisons).\\n\\nPlease see https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#computing-with-dimension-variables\\nfor more details.\\n\")\n```\n\n----------------------------------------\n\nTITLE: JVP Rule for XLA Call Primitive\nDESCRIPTION: Implements the Jacobian-vector product (JVP) transformation rule for the XLA call primitive, enabling differentiation of JIT-compiled functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndef xla_call_jvp_rule(primals, tangents, *, jaxpr, num_consts):\n  del num_consts  # Unused\n  new_jaxpr, new_consts = jvp_jaxpr(jaxpr)\n  outs = bind(xla_call_p, *new_consts, *primals, *tangents, jaxpr=new_jaxpr,\n              num_consts=len(new_consts))\n  n = len(outs) // 2\n  primals_out, tangents_out = outs[:n], outs[n:]\n  return primals_out, tangents_out\njvp_rules[xla_call_p] = xla_call_jvp_rule\n\n@lru_cache\ndef jvp_jaxpr(jaxpr: Jaxpr) -> tuple[Jaxpr, list[Any]]:\n  def jvp_traceable(*primals_and_tangents):\n    n = len(primals_and_tangents) // 2\n    primals, tangents = primals_and_tangents[:n], primals_and_tangents[n:]\n    return jvp(jaxpr_as_fun(jaxpr), primals, tangents)\n\n  in_avals = [v.aval for v in jaxpr.in_binders]\n  new_jaxpr, new_consts, _ = make_jaxpr(jvp_traceable, *in_avals, *in_avals)\n  return new_jaxpr, new_consts\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Using Command-line Flags with Abseil\nDESCRIPTION: Demonstrates the two-part process of enabling command-line flag configuration for JAX using Abseil. Requires code changes and specific command-line syntax when running the program.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/config_options.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# In your code:\nimport jax\njax.config.parse_flags_with_absl()\n```\n\nLANGUAGE: bash\nCODE:\n```\n# When running:\npython my_program.py --jax_enable_x64=True\n```\n\n----------------------------------------\n\nTITLE: Implementing JAX cond Primitive Core Function in Python\nDESCRIPTION: Defines the core 'cond' function that implements conditional application in JAX. It takes a predicate, true and false functions, and operands, creates jaxprs from the functions, and ensures they have compatible types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_78\n\nLANGUAGE: python\nCODE:\n```\ndef cond(pred, true_fn, false_fn, *operands):\n  avals_in = [raise_to_shaped(get_aval(x)) for x in operands]\n  true_jaxpr, true_consts, out_tree = make_jaxpr(true_fn, *avals_in)\n  false_jaxpr, false_consts, out_tree_ = make_jaxpr(false_fn, *avals_in)\n  if out_tree != out_tree_: raise TypeError\n  true_jaxpr, false_jaxpr = _join_jaxpr_consts(\n      true_jaxpr, false_jaxpr, len(true_consts), len(false_consts))\n  if typecheck_jaxpr(true_jaxpr) != typecheck_jaxpr(false_jaxpr):\n    raise TypeError\n  outs = bind_cond(pred, *true_consts, *false_consts, *operands,\n                   true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n  return tree_unflatten(out_tree, outs)\ncond_p = Primitive('cond')\n```\n\n----------------------------------------\n\nTITLE: Basic JAX Function Structure for Differentiation\nDESCRIPTION: Defines a neural network prediction pipeline with a loss function, where multiple layers perform matrix multiplications and apply sine activation functions. This code demonstrates typical pattern that would benefit from custom checkpoint policies.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef loss(params, x, y):\n  return jnp.sum((predict(params, x) - y)**2)\n\ndef predict(params, x):\n  *Ws, Wlast = params\n  for W in Ws:\n    x = layer(W, x)\n  x = jnp.dot(Wlast, x)\n  return x\n\ndef layer(W, x):\n  return jnp.sin(jnp.dot(W, x))\n```\n\n----------------------------------------\n\nTITLE: Resharding Inputs for Exported JAX Modules Using New Mesh at Call Site - Python\nDESCRIPTION: This code demonstrates how to correctly shard arguments for an exported JAX module by constructing a new mesh at call time and using the exported artifact's in_shardings_jax API. It provides step-by-step logic for exporting, preparing the devices/mesh, sharding inputs appropriately, and calling the exported module. Dependencies include JAX, numpy, Mesh, NamedSharding, and PartitionSpec.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax import export\n>>> from jax.sharding import Mesh, NamedSharding\n>>> from jax.sharding import PartitionSpec as P\n\n>>> export_devices = jax.local_devices()\n>>> export_mesh = Mesh(np.array(export_devices), (\"a\",))\n>>> def f(x):\n...   return x.T\n\n\n>>> exp = export.export(jax.jit(f))(\n...    jax.ShapeDtypeStruct((4 * len(export_devices),), dtype=np.int32,\n...                         sharding=NamedSharding(export_mesh, P(\"a\"))))\n\n>>> # Prepare the mesh for calling `exp`.\n>>> calling_mesh = Mesh(np.array(export_devices[::-1]), (\"b\",))\n\n>>> # Shard the arg according to what `exp` expects.\n>>> arg = jnp.arange(4 * len(export_devices))\n>>> sharded_arg = jax.device_put(arg, exp.in_shardings_jax(calling_mesh)[0])\n>>> res = exp.call(sharded_arg)\n\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Derivative Rule via custom_jvp for pure_callback Bessel Function - Python\nDESCRIPTION: Uses jax.custom_jvp to supply a custom JVP (Jacobian-vector-product) rule for the pure_callback-wrapped jv function, enabling correct differentiation with respect to the second argument (z) but not the order (v, which is integer and non-differentiable). This extension makes grad and higher-order derivatives (via hessian) available. Dependencies: jax.custom_jvp.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\njv = jax.custom_jvp(jv)\n\n@jv.defjvp\ndef _jv_jvp(primals, tangents):\n  v, z = primals\n  _, z_dot = tangents  # Note: v_dot is always 0 because v is integer.\n  jv_minus_1, jv_plus_1 = jv(v - 1, z), jv(v + 1, z)\n  djv_dz = jnp.where(v == 0, -jv_plus_1, 0.5 * (jv_minus_1 - jv_plus_1))\n  return jv(v, z), z_dot * djv_dz\n```\n\n----------------------------------------\n\nTITLE: Querying JAX-level Type\nDESCRIPTION: Demonstrates how to query the JAX-level type of an array using jax.typeof, both outside and inside a jitted function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsome_array = np.arange(8)\nprint(f\"JAX-level type of some_array: {jax.typeof(some_array)}\")\n```\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef foo(x):\n  print(f\"JAX-level type of x during tracing: {jax.typeof(x)}\")\n  return x + x\n\nfoo(some_array)\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging for JAX Persistent Compilation Cache in Python\nDESCRIPTION: Enables detailed logging for modules involved in compilation and cache logic by setting the 'JAX_DEBUG_LOG_MODULES' environment variable. Insert this at the top of a script to capture cache-related activities. Requires the 'os' module and should be set before importing or initializing JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"JAX_DEBUG_LOG_MODULES\"] = \"jax._src.compiler,jax._src.lru_cache\"\n```\n\n----------------------------------------\n\nTITLE: Helper Function for Mesh Sharding\nDESCRIPTION: Define a helper function to simplify creation of NamedSharding objects using a default mesh configuration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndefault_mesh = jax.make_mesh((4, 2), ('a', 'b'))\n\ndef mesh_sharding(\n    pspec: PartitionSpec, mesh: Optional[Mesh] = None,\n  ) -> NamedSharding:\n  if mesh is None:\n    mesh = default_mesh\n  return NamedSharding(mesh, pspec)\n```\n\n----------------------------------------\n\nTITLE: Differentiating Through debug.callback-Enabled Function in JAX - Python\nDESCRIPTION: Demonstrates debug.callback's compatibility with grad/autodiff: the callback executes during differentiation, making debug.callback useful for tracing both forward and backward computations. No modification to derivative calculation occurs, only side effects executed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\njax.grad(f)(1.0);\n```\n\n----------------------------------------\n\nTITLE: JIT-Compiling Inner Functions with jax.jit (Python)\nDESCRIPTION: This snippet demonstrates the use of jax.jit for compiling inner functions in Python, with a closure over a constant. JAX and jnp are required; the inner function is decorated with @jit and captures both the outer scope variable and a jnp.ones constant. The function takes a single numeric input. The output is the sum of original input and the inner compiled function's result. Demonstrates closure capture and nested JIT compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\n\ndef func12(arg):\n  @jit\n  def inner(x):\n    return x + arg * jnp.ones(1)  # Include a constant in the inner function.\n  return arg + inner(arg - 2.)\n\nprint(make_jaxpr(func12)(1.))\n```\n\n----------------------------------------\n\nTITLE: Implementing Output Tokens for Asynchronous Operations in JAX\nDESCRIPTION: Shows how to modify a JIT-compiled function to return a runtime token, allowing blocking on side-effecting computations without explicit outputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f():\n  jax.print(\"hello world\")\n  return new_runtime_token()\nf() # Executed asynchronously\n```\n\n----------------------------------------\n\nTITLE: Implementing Helper Function to Join JAX Jaxpr Constants\nDESCRIPTION: Defines a utility function that joins the constant binders of two jaxprs to make their input types consistent. This is needed because jaxprs can only represent closed terms and might close over different constants.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_79\n\nLANGUAGE: python\nCODE:\n```\ndef _join_jaxpr_consts(jaxpr1: Jaxpr, jaxpr2: Jaxpr, n1: int, n2: int\n                       ) -> tuple[Jaxpr, Jaxpr]:\n  jaxpr1_type, jaxpr2_type = typecheck_jaxpr(jaxpr1), typecheck_jaxpr(jaxpr2)\n  assert jaxpr1_type.in_types[n1:] == jaxpr2_type.in_types[n2:]\n  consts1, rest1 = split_list(jaxpr1.in_binders, n1)\n  consts2, rest2 = split_list(jaxpr2.in_binders, n2)\n  new_jaxpr1 = Jaxpr(consts1 + consts2 + rest1, jaxpr1.eqns, jaxpr1.outs)\n  new_jaxpr2 = Jaxpr(consts1 + consts2 + rest2, jaxpr2.eqns, jaxpr2.outs)\n  return new_jaxpr1, new_jaxpr2\n```\n\n----------------------------------------\n\nTITLE: Displaying weak type flag in JAX arrays\nDESCRIPTION: This example shows how Python scalars converted to JAX arrays maintain a weak_type flag that preserves their special promotion behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> jnp.asarray(2)\nArray(2, dtype=int32, weak_type=True)\n```\n\n----------------------------------------\n\nTITLE: Example of Device Variance in JAX Operations with Annotations\nDESCRIPTION: Demonstrates how device variance types are annotated in a JAX shmap function that uses psum. Shows the original example and its version with explicit device variance type annotations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Example 1 again\nf1 = shmap(lambda x: psum(g(x), 'i'),\n           in_specs=P('i'), out_specs=P())\n\n# Example 1 with intermediate device variance types annotated\n@partial(shmap, in_specs=P('i'), out_specs=P())\ndef f1(x: f32[3,4]{i}):\n  w:f32[]{i} = g(x)\n  y:f32[]{} = psum(w, 'i')\n  return y\n```\n\n----------------------------------------\n\nTITLE: Defining a Pallas Kernel to Visualize Program IDs in Python\nDESCRIPTION: Defines a Python function `show_program_ids` which uses `pallas_call` to run a kernel (`program_ids_kernel`). This kernel demonstrates reading program IDs using `pl.program_id(axis)` for each grid dimension and writing a combined value into the corresponding output block. The function takes array/block shapes, grid dimensions, and an optional `index_map` to configure the Pallas call for visualization purposes. Requires `jax`, `jnp`, `numpy` as `np`, and `pallas` as `pl`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> def show_program_ids(x_shape, block_shape, grid,\n...                      index_map=lambda i, j: (i, j)):\n...   def program_ids_kernel(o_ref):  # Fill the output block with 10*program_id(1) + program_id(0)\n...     axes = 0\n...     for axis in range(len(grid)):\n...       axes += pl.program_id(axis) * 10**(len(grid) - 1 - axis)\n...     o_ref[...] = jnp.full(o_ref.shape, axes)\n...   res = pl.pallas_call(program_ids_kernel,\n...                        out_shape=jax.ShapeDtypeStruct(x_shape, dtype=np.int32),\n...                        grid=grid,\n...                        in_specs=[],\n...                        out_specs=pl.BlockSpec(block_shape, index_map),\n...                        interpret=True)()\n...   print(res)\n\n```\n\n----------------------------------------\n\nTITLE: Basic JAX Permute Function Example\nDESCRIPTION: Simple example showing a JAX function that could benefit from async optimization, combining a permute operation with basic arithmetic.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  y = ppermute(x)\n  z = x + 1\n  return y, z\n```\n\n----------------------------------------\n\nTITLE: Reusing Compiled JVP Function\nDESCRIPTION: Demonstrates that the JVP of a JIT-compiled function is also cached for subsequent calls, avoiding retracing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ny, ydot = jvp(f, (x,), (xdot,))  # 'tracing!' not printed\n```\n\n----------------------------------------\n\nTITLE: Implementing log1pexp with Numerical Stability Issues\nDESCRIPTION: Implementation of log1pexp function that computes log(1 + exp(x)) using JAX numpy, demonstrating a function that has numerical stability issues when differentiated.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef log1pexp(x):\n  return jnp.log(1. + jnp.exp(x))\n\nlog1pexp(3.)\n```\n\n----------------------------------------\n\nTITLE: Implementing Block Matrix Multiplication in NumPy\nDESCRIPTION: Defines two functions in NumPy to perform block matrix multiplication. `matmul_small` handles multiplication of small matrices (up to 256x256), asserting the size constraints. `block_matmul` implements the block decomposition algorithm, iterating through blocks of the input matrices (`x`, `y`) and accumulating the results of `matmul_small` into the output matrix `z`. It assumes input dimensions are divisible by the block sizes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_small(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n  m, k, n = x.shape[0], x.shape[1], y.shape[0]\n  assert m <= 256\n  assert k <= 256\n  assert n <= 256\n  return np.matmul(x, y)\n\ndef block_matmul(\n    x: np.ndarray,\n    y: np.ndarray,\n    *,\n    bm: int = 256,\n    bk: int = 256,\n    bn: int = 256,\n) -> np.ndarray:\n  m, k = x.shape\n  _, n = y.shape\n\n  z = np.zeros((m, n), dtype=x.dtype)\n  for m_i in range(m // bm):\n    for n_i in range(n // bn):\n      for k_i in range(k // bk):\n        m_slice = slice(m_i * bm, (m_i + 1) * bm)\n        k_slice = slice(k_i * bk, (k_i + 1) * bk)\n        n_slice = slice(n_i * bn, (n_i + 1) * bn)\n        x_block = x[m_slice, k_slice]\n        y_block = y[k_slice, n_slice]\n        z[m_slice, n_slice] += matmul_small(x_block, y_block)\n  return z\n```\n\n----------------------------------------\n\nTITLE: Enabling 64-bit Types with jax_enable_x64 Configuration\nDESCRIPTION: Shows how to enable 64-bit datatypes by setting the jax_enable_x64 flag to True, which changes the default array creation behavior to use 64-bit types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/default_dtypes.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\njax.config.update('jax_enable_x64', True)\n\nprint(repr(jnp.arange(5)))\nprint(repr(jnp.zeros(5)))\nprint(repr(jnp.ones(5, dtype=int)))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JVP with Nested Containers in Python\nDESCRIPTION: This code snippet demonstrates the usage of the implemented JVP function with a user-defined function that has nested container outputs. It shows how the JVP can handle arbitrary input and output structures.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return {'hi': z, 'there': [x, y]}\n\nx, xdot = 3., 1.\ny, ydot = jvp(f, (x,), (xdot,))\nprint(y)\nprint(ydot)\n```\n\n----------------------------------------\n\nTITLE: Registering FFI Handler Using ctypes and JAX - Python\nDESCRIPTION: This snippet loads the compiled shared library for RmsNorm using Python's ctypes and registers it with JAX's XLA FFI system. The process involves finding the library file, loading it, wrapping the handler with a PyCapsule, and calling 'jax.ffi.register_ffi_target'. Dependencies include Python, JAX with FFI support, ctypes, and the built shared library. The key parameter 'platform' specifies the target device (here, 'cpu'). The output is registration of the FFI handler for subsequent usage in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ctypes\\nfrom pathlib import Path\\n\\npath = next(Path(\"ffi\").glob(\"librms_norm*\"))\\nrms_norm_lib = ctypes.cdll.LoadLibrary(path)\\njax.ffi.register_ffi_target(\\n    \"rms_norm\", jax.ffi.pycapsule(rms_norm_lib.RmsNorm), platform=\"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Conceptualizing 1D Grid Execution with pallas_call in Python\nDESCRIPTION: Demonstrates how a one-dimensional `grid=(n,)` passed to `jax.experimental.pallas.pallas_call` conceptually maps to a single Python `for` loop. This means the provided `some_kernel` function is executed `n` times sequentially in this conceptual model, though Pallas executes them potentially in parallel on the target hardware. It introduces the basic idea of iterating over a grid dimension.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\npl.pallas_call(some_kernel, grid=(n,))(...) # Conceptual mapping\n```\n```\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Conceptual equivalent\nfor i in range(n):\n  some_kernel(...)\n```\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for Tracing JAX Function Calls\nDESCRIPTION: Utility functions that help trace and visualize how JAX transforms and evaluates functions. These helpers print indented execution traces showing function calls, arguments, and return values, making JAX's internal processes more transparent.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#@title Helper functions (execute this cell)\nimport functools\nimport traceback\n\n_indentation = 0\ndef _trace(msg=None):\n    \"\"\"Print a message at current indentation.\"\"\"\n    if msg is not None:\n        print(\"  \" * _indentation + msg)\n\ndef _trace_indent(msg=None):\n    \"\"\"Print a message and then indent the rest.\"\"\"\n    global _indentation\n    _trace(msg)\n    _indentation = 1 + _indentation\n\ndef _trace_unindent(msg=None):\n    \"\"\"Unindent then print a message.\"\"\"\n    global _indentation\n    _indentation = _indentation - 1\n    _trace(msg)\n\ndef trace(name):\n  \"\"\"A decorator for functions to trace arguments and results.\"\"\"\n\n  def trace_func(func):  # pylint: disable=missing-docstring\n    def pp(v):\n        \"\"\"Print certain values more succinctly\"\"\"\n        vtype = str(type(v))\n        if \"jax._src.xla_bridge._JaxComputationBuilder\" in vtype:\n            return \"<JaxComputationBuilder>\"\n        elif \"jaxlib.xla_extension.XlaOp\" in vtype:\n            return \"<XlaOp at 0x{:x}>\".format(id(v))\n        elif (\"partial_eval.JaxprTracer\" in vtype or\n              \"batching.BatchTracer\" in vtype or\n              \"ad.JVPTracer\" in vtype):\n            return \"Traced<{}>\".format(v.aval)\n        elif isinstance(v, tuple):\n            return \"({})\".format(pp_values(v))\n        else:\n            return str(v)\n    def pp_values(args):\n        return \", \".join([pp(arg) for arg in args])\n    \n    @functools.wraps(func)\n    def func_wrapper(*args):\n      _trace_indent(\"call {}({})\".format(name, pp_values(args)))\n      res = func(*args)\n      _trace_unindent(\"|<- {} = {}\".format(name, pp(res)))\n      return res\n\n    return func_wrapper\n\n  return trace_func\n\nclass expectNotImplementedError(object):\n  \"\"\"Context manager to check for NotImplementedError.\"\"\"\n  def __enter__(self): pass\n  def __exit__(self, type, value, tb):\n    global _indentation\n    _indentation = 0\n    if type is NotImplementedError:\n      print(\"\\nFound expected exception:\")\n      traceback.print_exc(limit=3)\n      return True\n    elif type is None:  # No exception\n      assert False, \"Expected NotImplementedError\"\n    else:\n      return False\n```\n\n----------------------------------------\n\nTITLE: Gradient Computation Through Conditionals in Python\nDESCRIPTION: Example showing gradient computation through a conditional expression. Demonstrates autodifferentiation capabilities with conditional operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_94\n\nLANGUAGE: python\nCODE:\n```\nout = grad(lambda x: cond(True, lambda: x * x, lambda: 0.))(1.)\nprint(out)\n```\n\n----------------------------------------\n\nTITLE: Implementing JVP with Pytree Handling in Python\nDESCRIPTION: This function wraps the flat JVP implementation to handle arbitrary input and output containers. It flattens inputs, calls the flat JVP, and then unflattens the outputs using pytree operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef jvp(f, primals, tangents):\n  primals_flat, in_tree = tree_flatten(primals)\n  tangents_flat, in_tree2 = tree_flatten(tangents)\n  if in_tree != in_tree2: raise TypeError\n  f, out_tree = flatten_fun(f, in_tree)\n  primals_out_flat, tangents_out_flat = jvp_flat(f, primals_flat, tangents_flat)\n  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n  tangents_out = tree_unflatten(out_tree(), tangents_out_flat)\n  return primals_out, tangents_out\n```\n\n----------------------------------------\n\nTITLE: Correct Implementation of Asynchronous Permutation with Unrolling\nDESCRIPTION: This example demonstrates the correct implementation of asynchronous permutation by using loop unrolling to avoid aliasing issues and ensure proper buffer lifetimes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  fut = ppermute_start(x)\n  def body(i, fut):\n    x = ppermute_done(fut)\n    fut = ppermute_start(x)\n    return fut\n  fut = fori_loop(0, 7, body, x, unroll=2)\n  return ppermute_done(fut)\n```\n\n----------------------------------------\n\nTITLE: Using auto_axes for Unimplemented Sharding Rules in Python\nDESCRIPTION: Demonstrates how to use the auto_axes decorator to work around unimplemented sharding rules or override the sharding-in-types system. It shows adding arrays with incompatible shardings by temporarily dropping into auto-sharding mode.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsome_x = reshard(np.arange(16).reshape(4, 4), P(\"X\", None))\nsome_y = reshard(np.arange(16).reshape(4, 4), P(None, \"X\"))\n\ntry:\n  some_x + some_y\nexcept Exception as e:\n  print(\"ERROR!\")\n  print(e)\n\nprint(\"=== try again with auto_axes ===\")\n\n@auto_axes\ndef add_with_out_sharding_kwarg(x, y):\n  print(f\"We're in auto-sharding mode here. This is the current mesh: {get_abstract_mesh()}\")\n  return x + y\n\nresult = add_with_out_sharding_kwarg(some_x, some_y, out_shardings=P(\"X\", None))\nprint(f\"Result type: {jax.typeof(result)}\")\n```\n\n----------------------------------------\n\nTITLE: Failing JIT compilation with input-dependent conditional\nDESCRIPTION: Demonstrates a function that fails to JIT compile due to an input-value-dependent conditional statement.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  if x < 3:\n    return 3. * x ** 2\n  else:\n    return -4 * x\n\n# This will fail!\nf(2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Partial Evaluation for JAX cond Primitive\nDESCRIPTION: Defines the partial evaluation rule for cond needed for reverse-mode automatic differentiation. It handles splitting computation into known and unknown parts, and ensures consistent output types between transformed jaxprs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_86\n\nLANGUAGE: python\nCODE:\n```\ndef cond_partial_eval(trace, tracers, *, true_jaxpr, false_jaxpr):\n  pred_tracer, *tracers = tracers\n  assert pred_tracer.pval.is_known\n  pred = pred_tracer.pval.const\n  in_uks = [not t.pval.is_known for t in tracers]\n\n  *jaxprs, out_uks, num_res = _cond_partial_eval(true_jaxpr, false_jaxpr, in_uks)\n  t_jaxpr1, f_jaxpr1, t_jaxpr2, f_jaxpr2 = jaxprs\n\n  known_tracers, unknown_tracers = partition_list(in_uks, tracers)\n  known_vals = [t.pval.const for t in known_tracers]\n  outs1_res = bind_cond(pred, *known_vals,\n                        true_jaxpr=t_jaxpr1, false_jaxpr=f_jaxpr1)\n  outs1, res = split_list(outs1_res, len(outs1_res) - num_res)\n  pred_tracer_ = trace.instantiate_const(full_raise(trace, pred_tracer))\n  res_tracers = [trace.instantiate_const(full_raise(trace, x)) for x in res]\n  outs2 = [PartialEvalTracer(trace, PartialVal.unknown(v.aval), None)\n           for v in t_jaxpr2.outs]\n  eqn = JaxprEqnRecipe(cond_p, [pred_tracer_, *res_tracers, *unknown_tracers],\n                       dict(true_jaxpr=t_jaxpr2, false_jaxpr=f_jaxpr2),\n                       [v.aval for v in t_jaxpr2.outs], map(ref, outs2))\n  for t in outs2: t.recipe = eqn\n  return merge_lists(out_uks, outs1, outs2)\npartial_eval_rules[cond_p] = cond_partial_eval\n```\n\n----------------------------------------\n\nTITLE: Organizing Activation and Utility Functions of jax.nn in reStructuredText\nDESCRIPTION: This snippet sets up a documentation index for the jax.nn module using Sphinx reStructuredText. It includes toctree directives to reference both the initializers and module details, as well as autosummary tables covering a variety of activation and utility functions used in neural network programming with JAX. The format assumes dependencies on Sphinx and its autodoc/autosummary extensions for generating detailed API documentation subpages.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.nn.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``jax.nn`` module\n=================\n\n.. currentmodule:: jax.nn\n\n.. toctree::\n    :maxdepth: 1\n\n    jax.nn.initializers\n\n.. automodule:: jax.nn\n\n\nActivation functions\n--------------------\n\n.. autosummary::\n  :toctree: _autosummary\n\n    relu\n    relu6\n    sigmoid\n    softplus\n    sparse_plus\n    sparse_sigmoid\n    soft_sign\n    silu\n    swish\n    log_sigmoid\n    leaky_relu\n    hard_sigmoid\n    hard_silu\n    hard_swish\n    hard_tanh\n    elu\n    celu\n    selu\n    gelu\n    glu\n    squareplus\n    mish\n    identity\n\nOther functions\n---------------\n\n.. autosummary::\n  :toctree: _autosummary\n\n    softmax\n    log_softmax\n    logsumexp\n    standardize\n    one_hot\n    dot_product_attention\n    scaled_matmul\n    get_scaled_dot_general_config\n    scaled_dot_general\n\n```\n\n----------------------------------------\n\nTITLE: Using Automatic Transposition with Custom JVP\nDESCRIPTION: Shows how JAX automatically transposes the linear computation from a custom JVP rule to enable reverse-mode differentiation. The example demonstrates using grad() on a function with only a JVP rule defined.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad\n\nprint(grad(f)(3.))\nprint(grad(grad(f))(3.))\n```\n\n----------------------------------------\n\nTITLE: Implementing Reduce-Scatter Kernel for TPU Communication in JAX Pallas\nDESCRIPTION: Defines the core kernel for reduce-scatter operations across TPU devices. It handles cross-device communication, memory management, and synchronization using semaphores. The kernel implements phases for prologue, body, and epilogue operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef reduce_scatter_kernel(\n    x_ref,\n    o_ref,\n    hbm_scratch,\n    left_recv_sem,\n    left_send_sem,\n    copy_sem,\n    right_recv_sem,\n    right_send_sem,\n    left_capacity_sem,\n    right_capacity_sem,\n):\n  outer_step = pl.program_id(0)\n  phase = pl.program_id(1)\n  is_start = jnp.logical_and(outer_step == 0, phase == 0)\n  last_iteration = outer_step == pl.num_programs(0) - 1\n\n  working_slot = lax.rem(outer_step, 2)\n  receiving_slot = 1 - working_slot\n  my_id = lax.axis_index('x')\n  right_neighbor = mod(my_id + 1, num_devices)\n  left_neighbor = mod(my_id - 1, num_devices)\n\n  left_copy_device = mod(my_id + outer_step + 1, num_devices)\n  right_copy_device = mod(my_id - outer_step - 1, num_devices)\n  left_copy_slice = pl.ds(0, outer_block_size[0] // 2)\n  right_copy_slice = pl.ds(outer_block_size[0] // 2, outer_block_size[0] // 2)\n  current_phase_slice = pl.ds(\n      phase * (outer_block_size[0] // 2), outer_block_size[0] // 2\n  )\n\n  initial_left_copy = pltpu.make_async_remote_copy(\n      src_ref=x_ref.at[my_id, left_copy_slice],\n      dst_ref=hbm_scratch.at[working_slot, left_copy_slice],\n      send_sem=left_send_sem,\n      recv_sem=left_recv_sem,\n      device_id=(left_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n\n  initial_right_copy = pltpu.make_async_remote_copy(\n      src_ref=x_ref.at[my_id, right_copy_slice],\n      dst_ref=hbm_scratch.at[working_slot, right_copy_slice],\n      send_sem=right_send_sem,\n      recv_sem=right_recv_sem,\n      device_id=(right_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n\n  left_copy = pltpu.make_async_remote_copy(\n      src_ref=hbm_scratch.at[working_slot, left_copy_slice],\n      dst_ref=hbm_scratch.at[receiving_slot, left_copy_slice],\n      send_sem=left_send_sem,\n      recv_sem=left_recv_sem,\n      device_id=(left_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n  right_copy = pltpu.make_async_remote_copy(\n      src_ref=hbm_scratch.at[receiving_slot, right_copy_slice],\n      dst_ref=hbm_scratch.at[working_slot, right_copy_slice],\n      send_sem=right_send_sem,\n      recv_sem=right_recv_sem,\n      device_id=(right_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n\n  # --- Prologue ---\n  @pl.when(is_start)\n  def _():\n    # Barrier with both neighbors at the start, since we will be\n    # communicating with both.\n    local_barrier(left_neighbor, right_neighbor)\n\n    initial_left_copy.start()\n    initial_left_copy.wait()\n    initial_right_copy.start()\n\n    # We tell our left neighbor that it is allowed to send to the right.\n    # (and vice versa for right neighbor)\n    signal(LEFT, right_capacity_sem)\n    signal(RIGHT, left_capacity_sem)\n\n  @pl.when(~is_start)\n  def _():\n    @pl.when(phase == LEFT)\n    def _():\n      # We block here until our right neighbor tells use we can send to\n      # the right.\n      pltpu.semaphore_wait(right_capacity_sem, 1)\n      right_copy.start()\n\n    @pl.when(phase == RIGHT)\n    def _():\n      # We block here until our left neighbor tells use we can send to\n      # the left.\n      pltpu.semaphore_wait(left_capacity_sem, 1)\n      left_copy.start()\n\n  # --- Body ---\n  def inner_kernel(input_ref, accum_ref):\n    # We do not explicitly use += because we set should_accumulate_out=True.\n    accum_ref[...] = input_ref[...]\n\n  accum_pipeline = pltpu.emit_pipeline(\n      inner_kernel,\n      in_specs=[inner_block_spec],\n      out_specs=inner_block_spec,\n      should_accumulate_out=True,\n      grid=inner_grid,\n  )\n\n  @pl.when(~last_iteration)\n  def _():\n    @pl.when(phase == LEFT)\n    def _():\n      accum_pipeline(\n          x_ref.at[left_copy_device, left_copy_slice],\n          hbm_scratch.at[working_slot, left_copy_slice],\n      )\n\n    @pl.when(phase == RIGHT)\n    def _():\n      accum_pipeline(\n          x_ref.at[right_copy_device, right_copy_slice],\n          hbm_scratch.at[working_slot, right_copy_slice],\n      )\n\n  # --- Epilogue ---\n  @pl.when(is_start)\n  def _():\n    initial_right_copy.wait()\n\n  @pl.when(~is_start)\n  def _():\n    @pl.when(phase == LEFT)\n    def _():\n      right_copy.wait()\n      signal(LEFT, right_capacity_sem)\n\n    @pl.when(phase == RIGHT)\n    def _():\n      left_copy.wait()\n      signal(RIGHT, left_capacity_sem)\n\n  # Store result on last iteration.\n  @pl.when(last_iteration)\n  def _():\n    output_copy = pltpu.make_async_copy(\n        src_ref=hbm_scratch.at[working_slot, current_phase_slice],\n        dst_ref=o_ref.at[current_phase_slice],\n        sem=copy_sem,\n    )\n    output_copy.start()\n    output_copy.wait()\n\n    # Clean up semaphores so that they exit with a value of 0.\n    @pl.when(phase == LEFT)\n    def _():\n      pltpu.semaphore_wait(right_capacity_sem, 1)\n\n    @pl.when(phase == RIGHT)\n    def _():\n      pltpu.semaphore_wait(left_capacity_sem, 1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Combined Data and Model Parallelism with tf.data in JAX\nDESCRIPTION: This code snippet demonstrates how to implement both data parallelism and model parallelism within a single process using tf.data with JAX arrays. It sets up data sharding across processes and then creates a JAX array with the appropriate partial replication pattern.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/distributed_data_loading.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport tensorflow as tf\nimport numpy as np\n\n################################################################################\n# Step 1: Set up the Dataset with a different data shard per-process (do once)\n#         (same as for pure data parallelism)\n################################################################################\n# Fake example data (replace with your Dataset)\nper_process_batches = [np.ones((16, 3)) * i for i in range(100)]\nds = tf.data.Dataset.from_tensor_slices(per_process_batches)\n\nds = ds.shard(num_shards=jax.process_count(), index=jax.process_index())\n\n################################################################################\n# Step 2: Create a jax.Array of per-replica batches from the per-process batch\n# produced from the Dataset (repeat every step)\n################################################################################\n# Grab just the first batch from the Dataset for this example\nper_process_batch = ds.as_numpy_iterator().next()\n\nnum_model_replicas_per_process = 2 # set according to your parallelism strategy\nnum_model_replicas_total = num_model_replicas_per_process * jax.process_count()\n\n# Create an example `Mesh` for per-process data parallelism. Make sure all devices\n```\n\n----------------------------------------\n\nTITLE: Implementing Right Permute with JAX Pallas Kernel in Python\nDESCRIPTION: This Python code sets up a sharded device mesh and implements a distributed right-permute operation (akin to lax.ppermute) across devices using a Pallas kernel. It demonstrates proper use of remote DMA synchronization (semaphores), peripheral setup with NamedSharding, and collective kernel invocation with shard_map. Required dependencies include JAX, jax.sharding, pallas (pl), and pltpu modules. Key parameters include the partitioning specification, device mesh, kernel function with asynchronous remote copy operations, and grid/specification setup. The code expects a 2D array input and outputs the permuted result; it also compares the custom kernel's output with native JAX lax.ppermute to verify correctness. This implementation assumes a fixed number of devices, focuses on inter-device communication correctness, and highlights constraints related to DMA semantics and device topology.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npartition = P(None, 'x')\\nmesh = jax.make_mesh((num_devices,), ('x',))\\nsharding = jax.sharding.NamedSharding(mesh, partition)\\n\\n# Create an input array that shards the last dimension across\\n# all devices.\\ninput_arr = jax.random.uniform(jax.random.key(0), (8, 128 * num_devices))\\ninput_arr = jax.device_put(input_arr, sharding)\\n\\ndef right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\\n  my_id = lax.axis_index('x')\\n  right_neighbor = lax.rem(my_id + 1, num_devices)\\n  remote_copy_op = pltpu.make_async_remote_copy(\\n      src_ref=input_ref,\\n      dst_ref=output_ref,\\n      send_sem=send_sem,\\n      recv_sem=recv_sem,\\n      device_id=(right_neighbor,),\\n      device_id_type=pltpu.DeviceIdType.MESH,\\n  )\\n  remote_copy_op.start()\\n  remote_copy_op.wait()\\n\\nout_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\\ngrid_spec = pltpu.PrefetchScalarGridSpec(\\n    num_scalar_prefetch=0,\\n    # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\\n    in_specs=[\\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n    ],\\n    out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n    scratch_shapes=(\\n        # We allocate DMA semaphores in scratch memory.\\n        [pltpu.SemaphoreType.DMA] * 2\\n    ),\\n)\\nright_permute = pl.pallas_call(\\n    right_permute_kernel,\\n    out_shape=out_shape,\\n    grid_spec=grid_spec,\\n)\\n# Wrap the kernel within a shard_map to call.\\npallas_result = jax.jit(\\n    shard_map.shard_map(\\n        right_permute,\\n        mesh=mesh,\\n        in_specs=partition,\\n        out_specs=partition,\\n        check_rep=False,\\n    )\\n)(input_arr)\\n\\n# Compare Pallas result to XLA shard_map result.\\nperm = tuple((src, (src + 1) % num_devices) for src in range(num_devices))\\n\\nxla_result = jax.jit(\\n    shard_map.shard_map(\\n        lambda x: lax.ppermute(x, 'x', perm),\\n        mesh=mesh, in_specs=partition, out_specs=partition)\\n)(input_arr)\\n\\nprint('Input = ', input_arr[0, ::128])\\nprint('Pallas Result = ', pallas_result[0, ::128])\\nprint('lax.ppermute Result = ', xla_result[0, ::128])\\nprint(\\n    'Difference |Pallas - lax.ppermute| = ',\\n    jnp.mean(jnp.abs(pallas_result - xla_result)),\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Using linearize with nested jit functions\nDESCRIPTION: Example demonstrating linearize applied to a function that calls another JIT-compiled function, showing how transformations compose.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_69\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  y = sin(x) * 2.\n  z = g(x, y)\n  return z\n\n@jit\ndef g(x, y):\n  return cos(x) + y\n\ny, f_lin = linearize(f, 3.)\ny_dot = f_lin(1.)\nprint(y, y_dot)\n```\n\n----------------------------------------\n\nTITLE: Comparing Type Promotion in jax.numpy vs jax.lax\nDESCRIPTION: Shows how jax.numpy implicitly promotes types (like NumPy) while jax.lax requires explicit type promotion, demonstrating the stricter but more explicit nature of the lower-level lax API.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\njnp.add(1, 1.0)  # jax.numpy API implicitly promotes mixed types.\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\nlax.add(1, 1.0)  # jax.lax API requires explicit type promotion.\n```\n\nLANGUAGE: python\nCODE:\n```\nlax.add(jnp.float32(1), 1.0)\n```\n\n----------------------------------------\n\nTITLE: Generating Text Representation of Lowered JAX Functions in Python\nDESCRIPTION: Calls the `as_text()` method on a `jax.stages.Lowered` object (`lowered`) to get a textual representation of the lowered JAX function. This is useful for manual inspection and debugging. Using the optional `debug_info` parameter can include additional information like source location. The exact output format is backend-dependent and not guaranteed to be stable.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/aot.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlowered.as_text()\n```\n\n----------------------------------------\n\nTITLE: Finite Differences Verification\nDESCRIPTION: Implementation of finite differences to verify gradient computation accuracy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/quickstart.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef first_finite_differences(f, x, eps=1E-3):\n  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n                   for v in jnp.eye(len(x))])\n\nprint(first_finite_differences(sum_logistic, x_small))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Computation Cost of Standard Chain Composition\nDESCRIPTION: Measures and prints the forward and backward computation operations for a standard chain of functions. Establishes a baseline for comparing computational efficiency.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nf = chain_compose([jnp.sin] * 8)\nprint_fwd_bwd(f, 3.)\n```\n\n----------------------------------------\n\nTITLE: NumPy Random State Inspection\nDESCRIPTION: Function to display truncated version of NumPy's random state for demonstration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/random-numbers.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef print_truncated_random_state():\n  \"\"\"To avoid spamming the outputs, print only part of the state.\"\"\"\n  full_random_state = np.random.get_state()\n  print(str(full_random_state)[:460], '...')\n\nprint_truncated_random_state()\n```\n\n----------------------------------------\n\nTITLE: Splitting and Generating Random Numbers with JAX Random API (Python)\nDESCRIPTION: Demonstrates standard uses of a PRNG key: splitting it for further use and generating uniform random numbers. Both split and uniform require a valid PRNG key (old or new style). Input: PRNG key, output: tuple with two keys (after split) and random data array from uniform. The example applies equally to both old-style and new-style keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# split\nnew_key, subkey = jax.random.split(key)\n\n# random number generation\ndata = jax.random.uniform(key, shape=(5,))\n\n```\n\n----------------------------------------\n\nTITLE: Auto-documenting the jax.stages.Traced Class using Sphinx\nDESCRIPTION: This Sphinx directive generates documentation for the `Traced` class within the `jax.stages` module. It includes documentation for the `jaxpr`, `out_info`, and `lower` members.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.stages.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: Traced\n   :members: jaxpr, out_info, lower\n```\n\n----------------------------------------\n\nTITLE: Illustrative ValueError with pjit and Host-Local Inputs under jax.Array\nDESCRIPTION: This code snippet shows a `ValueError` that can occur when using `pjit` with the `jax.Array` feature enabled and passing host-local arrays. The error arises because `pjit` with `jax.Array` expects globally shaped inputs, and it no longer automatically concatenates process-local arrays. The error message indicates that the input dimension size (4) is not divisible by the required sharding dimension size (8) derived from the mesh and partition spec.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nValueError: One of pjit arguments was given the sharding of\nNamedSharding(mesh={'x': 2, 'y': 2, 'chips': 2}, partition_spec=PartitionSpec(('x', 'y', 'chips'),)),\nwhich implies that the global size of its dimension 0 should be divisible by 8,\nbut it is equal to 4\n```\n\n----------------------------------------\n\nTITLE: Defining nanobind C++ Extension for FFI Handler - C++\nDESCRIPTION: This C++ snippet defines a minimal nanobind extension exposing the RmsNorm FFI handler as a Python-callable capsule. The code includes a template utility for encapsulating function pointers, static type checks for safety, and a module definition exposing 'rms_norm'. It requires the nanobind library, the JAX xla/ffi API headers, and proper setup for Python extension compilation. The output is a loadable extension module (e.g., rms_norm.so/.pyd) that can be imported in Python as 'rms_norm' for subsequent FFI registration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\n#include <type_traits>\\n\\n#include \"nanobind/nanobind.h\"\\n#include \"xla/ffi/api/c_api.h\"\\n\\nnamespace nb = nanobind;\\n\\ntemplate <typename T>\\nnb::capsule EncapsulateFfiCall(T *fn) {\\n  // This check is optional, but it can be helpful for avoiding invalid handlers.\\n  static_assert(std::is_invocable_r_v<XLA_FFI_Error *, T, XLA_FFI_CallFrame *>,\\n                \"Encapsulated function must be and XLA FFI handler\");\\n  return nb::capsule(reinterpret_cast<void *>(fn));\\n}\\n\\nNB_MODULE(rms_norm, m) {\\n  m.def(\"rms_norm\", []() { return EncapsulateFfiCall(RmsNorm); });\\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting Functions with AbstractMesh Sharding for Device-Polymorphic Execution - JAX - Python\nDESCRIPTION: This example demonstrates the export of a JAX function using an AbstractMesh to enable device-polymorphic behavior and flexible sharding. It shows creation of logical mesh topologies for exporting, checking exported metadata (number of devices and input sharding), and verifying result consistency when using different physical devices or mesh shapes, provided the count matches. The snippet uses features from jax, jax.export, jax.sharding, and numpy, and prints details about the resulting sharded arrays and devices involved.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax import export\n>>> from jax.sharding import AbstractMesh, Mesh, NamedSharding\n>>> from jax.sharding import PartitionSpec as P\n>>>\n>>> # Use an AbstractMesh for exporting\n>>> export_mesh = AbstractMesh((4,), (\"a\",))\n\n>>> def f(x):\n...   return x.T\n\n>>> exp = export.export(jax.jit(f))(\n...    jax.ShapeDtypeStruct((32,), dtype=np.int32,\n...                         sharding=NamedSharding(export_mesh, P(\"a\"))))\n\n>>> # `exp` knows for how many devices it was exported.\n>>> exp.nr_devices\n4\n\n>>> # and it knows the shardings for the inputs. These will be applied\n>>> # when the exported is called.\n>>> exp.in_shardings_hlo\n({devices=[4]<= [4]},)\n\n>>> # You can also use a concrete set of devices for exporting\n>>> concrete_devices = jax.local_devices()[:4]\n>>> concrete_mesh = Mesh(concrete_devices, (\"a\",))\n>>> exp2 = export.export(jax.jit(f))(\n...    jax.ShapeDtypeStruct((32,), dtype=np.int32,\n...                         sharding=NamedSharding(concrete_mesh, P(\"a\"))))\n\n>>> # You can expect the same results\n>>> assert exp.in_shardings_hlo == exp2.in_shardings_hlo\n\n>>> # When you call an Exported, you must use a concrete set of devices\n>>> arg = jnp.arange(8 * 4)\n>>> res1 = exp.call(jax.device_put(arg,\n...                                NamedSharding(concrete_mesh, P(\"a\"))))\n\n>>> # Check out the first 2 shards of the result\n>>> [f\"device={s.device} index={s.index}\" for s in res1.addressable_shards[:2]]\n['device=TFRT_CPU_0 index=(slice(0, 8, None),)',\n 'device=TFRT_CPU_1 index=(slice(8, 16, None),)']\n\n>>> # We can call `exp` with some other 4 devices and another\n>>> # mesh with a different shape, as long as the number of devices is\n>>> # the same.\n>>> other_mesh = Mesh(np.array(jax.local_devices()[2:6]).reshape((2, 2)), (\"b\", \"c\"))\n>>> res2 = exp.call(jax.device_put(arg,\n...                                NamedSharding(other_mesh, P(\"b\"))))\n\n>>> # Check out the first 2 shards of the result. Notice that the output is\n>>> # sharded similarly; this means that the input was resharded according to the\n>>> # exp.in_shardings.\n>>> [f\"device={s.device} index={s.index}\" for s in res2.addressable_shards[:2]]\n['device=TFRT_CPU_2 index=(slice(0, 8, None),)',\n 'device=TFRT_CPU_3 index=(slice(8, 16, None),)']\n\n```\n\n----------------------------------------\n\nTITLE: Preparing a Sharded Array for Constraint Examples in JAX\nDESCRIPTION: Creates a random array and places it on a 2D mesh with a specific sharding pattern (sharded across both dimensions 'x' and 'y'). This array will be used to demonstrate sharding constraints in JIT-compiled functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nx = jax.random.normal(jax.random.key(0), (8192, 8192))\nx = jax.device_put(x, NamedSharding(mesh, P('x', 'y')))\n```\n\n----------------------------------------\n\nTITLE: Comparing Pallas and XLA shard_map Results in Python\nDESCRIPTION: This code snippet compares the results of a Pallas implementation with an XLA shard_map implementation of an all_gather operation. It uses jax.jit and shard_map to create and execute the XLA version, then prints and compares the results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Compare Pallas result to XLA shard_map result.\nxla_result = jax.jit(\n    shard_map.shard_map(\n        lambda x: lax.all_gather(x, 'x'),\n        mesh=mesh, in_specs=partition, out_specs=partition\n    )\n)(input_arr)\n\nprint('Input: ', input_arr.shape, input_arr[::8, 0])\nprint('Pallas Result: ', pallas_result.shape, pallas_result[:, 0, 0])\nprint('lax.all_gather Result: ', xla_result.shape, xla_result[:, 0, 0])\nprint('Difference |Pallas - lax.all_gather| = ',\n      jnp.mean(jnp.abs(pallas_result - xla_result)))\n```\n\n----------------------------------------\n\nTITLE: Mocking a Multi-Node GPU Topology for JAX Compilation Cache Preparation in Python\nDESCRIPTION: Shows how to use the 'jax_mock_gpu_topology' config to simulate a multi-node/multi-GPU environment on a single machine, thus pre-populating the compilation cache before running in a distributed setting. The argument string indicates nodes x processes per node x GPUs per process. Use with caution, as communication results will not reflect actual multi-node execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\njax.config.update(\"jax_mock_gpu_topology\", \"4x8x1\")\n```\n\n----------------------------------------\n\nTITLE: Illustrating Functional PRNG with Splitting in Python\nDESCRIPTION: This snippet demonstrates JAX's preferred functional PRNG model utilizing 'splitting'. The `split` function takes one PRNG key (`rng_1`) and produces two new, independent keys (`rng_2`, `rng_3`). These keys can then be passed to separate functions (`bar`, `baz`) which can now execute in parallel as there is no sequential dependency between them. This model enhances expressiveness (functions don't need to return updated keys) and enables parallelization and scaling.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef foo(rng_1):\n   rng_2, rng_3 = split(rng_1, 2)\n   return bar(rng_2) + baz(rng_3)\n\ndef bar(x, rng):\n  return rand(rng, (3, 4))\n\ndef baz(x, rng):\n  return rand(rng, (3, 4))\n\ndef main():\n  foo(RandomState(0))\n```\n```\n\n----------------------------------------\n\nTITLE: Abstract Evaluation and Translation for XLA Call\nDESCRIPTION: Implements abstract evaluation for type checking and translation to XLA HLO for the XLA call primitive.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndef xla_call_abstract_eval_rule(*in_types, jaxpr, num_consts):\n  del num_consts  # Unused\n  jaxpr_type = typecheck_jaxpr(jaxpr)\n  if not all(t1 == t2 for t1, t2 in zip(jaxpr_type.in_types, in_types)):\n    raise TypeError\n  return jaxpr_type.out_types\nabstract_eval_rules[xla_call_p] = xla_call_abstract_eval_rule\n\ndef xla_call_translation(c, in_avals, out_avals, in_vals, *, jaxpr, num_consts):\n  del num_consts, out_avals\n  # Calling jaxpr_subcomp directly would inline. We generate a Call HLO instead.\n  with ir.InsertionPoint(c.module.body):\n    @func.func(*(aval_to_ir_type(aval) for aval in in_avals))\n    def inner_xla_call(*params):\n      return jaxpr_subcomp(c, jaxpr, params)\n    name = c.symbol_table.insert(inner_xla_call.func_op)\n  return func.CallOp(inner_xla_call.func_op, in_vals).results\nhlo_translations[xla_call_p] = xla_call_translation\n```\n\n----------------------------------------\n\nTITLE: Generalized Pipeline Function\nDESCRIPTION: Generic implementation of double-buffered pipeline that can handle different grid sizes, kernels and data slices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef double_buffered_pipeline(\n    grid: tuple[int, ...],\n    kernel: Callable,\n    in_slices: Callable,\n    out_slices: Callable):\n  # Prologue\n  copy_in_start(in_hbm[in_slices(0)], in_sram[0])\n\n  # Main loop\n  grid_size = prod(grid)\n  for i in range(grid_size):\n    cur_slot = i % 2\n    next_slot = (i + 1) % 2\n    if i < grid_size:\n      copy_in_start(in_hbm[data_slices(i+1)], in_sram[next_slot])\n    copy_in_wait(in_sram[cur_slot])\n\n    kernel(inputs, outputs)\n\n    copy_out_start(out_sram[cur_slot], out_hbm[out_slices(i)])\n    if i > 0:\n      copy_out_wait(out_sram[next_slot])\n\n  # Epilogue\n  copy_out_wait(out_sram[1])\n```\n\n----------------------------------------\n\nTITLE: Simple Shape Polymorphism Example\nDESCRIPTION: Basic example of call_tf with shape polymorphism for a function that preserves input shape.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndef fun_jax(x):\n  return jax2tf.call_tf(tf.math.sin,\n                        output_shape_dtype=x)\n  )(x)\n\njax2tf.convert(fun_jax, polymorphic_shapes=[\"b, ...\"])(x)\n```\n\n----------------------------------------\n\nTITLE: Defining and Registering a Custom PyTree in JAX (Python)\nDESCRIPTION: Demonstrates how to define a custom PyTree by implementing a simple class and registering it with JAX using register_pytree_node. Shows the potential pitfalls when custom input validation is performed in the constructor, which can cause errors if JAX passes unexpected objects during transformations like vmap or jacobian. Dependencies: jax, jax.numpy, and proper PyTree registration methods are required. Key parameter is 'a', the value(s) to store as a JAX array, expected to be suitable for jnp.asarray. Output is a PyTree-compatible custom object, but initialization will fail if input is not as expected by the constructor.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pytrees.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MyTree:\n  def __init__(self, a):\n    self.a = jnp.asarray(a)\n\nregister_pytree_node(MyTree, lambda tree: ((tree.a,), None),\n    lambda _, args: MyTree(*args))\n\ntree = MyTree(jnp.arange(5.0))\n\njax.vmap(lambda x: x)(tree)      # Error because object() is passed to MyTree.\njax.jacobian(lambda x: x)(tree)  # Error because MyTree(...) is passed to MyTree\n```\n\n----------------------------------------\n\nTITLE: Implementing Semaphore Signaling for Inter-Device Communication in Python\nDESCRIPTION: Defines a function to signal neighboring devices using semaphores. It determines the appropriate neighbor ID based on the direction (left or right) and signals using TPU-specific semaphore operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef signal(left_or_right, semaphore):\n  my_id = lax.axis_index('x')\n  if left_or_right == LEFT:\n    neighbor = mod(my_id - 1, num_devices)\n  else:\n    neighbor = mod(my_id + 1, num_devices)\n  pltpu.semaphore_signal(\n      semaphore,\n      inc=1,\n      device_id=(neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n```\n\n----------------------------------------\n\nTITLE: Implementing transposed JAX expression evaluation\nDESCRIPTION: Implementation of eval_jaxpr_transposed, which evaluates a JAX expression backward to compute gradients, along with initialization of transpose rules.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_71\n\nLANGUAGE: python\nCODE:\n```\n# NB: the analogous function in JAX is called 'backward_pass'\ndef eval_jaxpr_transposed(jaxpr: Jaxpr, args: list[Any], cotangents: list[Any]\n                          ) -> list[Any]:\n  primal_env: dict[Var, Any] = {}\n  ct_env: dict[Var, Any] = {}\n\n  def read_primal(x: Atom) -> Any:\n    return primal_env.get(x, UndefPrimal(x.aval)) if type(x) is Var else x.val\n\n  def write_primal(v: Var, val: Any) -> None:\n    if type(val) is not UndefPrimal:\n      primal_env[v] = val\n\n  def read_cotangent(v: Var) -> Any:\n    return ct_env.pop(v, np.zeros(v.aval.shape, v.aval.dtype))\n\n  def write_cotangent(x: Atom, val: Any):\n    if type(x) is Var and val is not None:\n      ct_env[x] = add(ct_env[x], val) if x in ct_env else val\n\n  map(write_primal, jaxpr.in_binders, args)\n  map(write_cotangent, jaxpr.outs, cotangents)\n  for eqn in jaxpr.eqns[::-1]:\n    primals_in = map(read_primal, eqn.inputs)\n    cts_in = map(read_cotangent, eqn.out_binders)\n    rule = transpose_rules[eqn.primitive]\n    cts_out = rule(cts_in, *primals_in, **eqn.params)\n    map(write_cotangent, eqn.inputs, cts_out)\n\n  return [read_cotangent(v) for v, x in zip(jaxpr.in_binders, args)\n          if type(x) is UndefPrimal]\n\ntranspose_rules = {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multicore TPU Execution in Pallas\nDESCRIPTION: Example of how to configure pallas_call for multicore TPU execution by specifying dimension_semantics. This allows parallelization of certain grid axes across multiple TPU cores.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/details.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npallas_call(\n    ...,\n    compiler_params=pltpu.TPUCompilerParams(\n        dimension_semantics=[\"parallel\", \"parallel\", \"arbitrary\"]\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Training and exporting a JAX model to SavedModel\nDESCRIPTION: Command to run the saved_model_main.py script to train a model and export it as a SavedModel with specified parameters like model type, path, version, and batch size.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython ${JAX2TF_EXAMPLES}/saved_model_main.py --model=${MODEL} \\\n    --model_path=${MODEL_PATH} --model_version=${MODEL_VERSION} \\\n    --serving_batch_size=${SERVING_BATCH_SIZE_SAVE} \\\n    --compile_model \\\n    --noshow_model\n```\n\n----------------------------------------\n\nTITLE: Defining shmap with all_gather and Mapped Output in JAX\nDESCRIPTION: This snippet defines a JAX function `f5` using `shmap`. It takes inputs `x` and `y`, both sharded along axis 'i'. It computes `all_gather(x, 'i')` and multiplies it by `y`. The result is returned as a mapped output, also sharded along 'i' (`out_specs=P('i')`). This serves as a contrast to `f4`, where the collective's transpose is necessary.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Example 5: all_gather to a mapped output\nf5 = shmap(lambda x, y: all_gather(x, 'i') * y,\n           in_specs=(P('i'), P('i')), out_specs=P('i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Illustrating Error with Python Logical Operators on JAX Arrays in Python\nDESCRIPTION: Demonstrates that attempting to use the `python_check_positive_even` function (which uses Python's `and` operator) with a multi-element JAX array results in an error. This highlights the incompatibility of standard Python logical operators with JAX arrays, similar to NumPy. Requires the `python_check_positive_even` function and a JAX array variable `x` (assumed defined from previous snippets).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Assuming python_check_positive_even and x are defined as in previous snippets\nprint(python_check_positive_even(x))\n```\n\n----------------------------------------\n\nTITLE: Scanning with State Using JAX lax.scan (Python)\nDESCRIPTION: This code uses jax.lax.scan to perform a stateful scan across pairs of input arrays in Python, yielding both the final carry and an output array. JAX and jnp are required; input is a pair of arrays and an extra numeric value. The scan body updates a running total (carry) based on elementwise operations and the extra parameter. Output is a tuple (carry, output array). Useful for efficient compositional loops and differentiability over a static loop structure.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef func11(arr, extra):\n  ones = jnp.ones(arr.shape)  #  A constant\n  def body(carry, aelems):\n    # carry: running dot-product of the two arrays\n    # aelems: a pair with corresponding elements from the two arrays\n    ae1, ae2 = aelems\n    return (carry + ae1 * ae2 + extra, carry)\n  return lax.scan(body, 0., (arr, ones))\n\nprint(make_jaxpr(func11)(np.ones(16), 5.))\n```\n\n----------------------------------------\n\nTITLE: Configuring XLA Flags for Optimized GPU Pipeline Parallelism (Bash)\nDESCRIPTION: Provides recommended XLA command-line flags for enabling and optimizing SPMD-based pipeline parallelism on GPUs. These flags activate the latency hiding scheduler, adjust command buffer usage (potentially disabling it with an empty string), disable the `collective-permute-motion` HLO pass, and explicitly enable the experimental pipeline parallelism optimization level. These settings aim to improve performance by allowing computation and communication to overlap.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n--xla_gpu_enable_latency_hiding_scheduler=true\n--xla_gpu_enable_command_buffer=''\n--xla_disable_hlo_passes=collective-permute-motion\n--xla_gpu_experimental_pipeline_parallelism_opt_level=PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE\n```\n\n----------------------------------------\n\nTITLE: Controlling JAX Export Calling Convention Version in Python\nDESCRIPTION: Shows how to check supported calling convention versions and set a specific version using a configuration context manager.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> from jax import export\n>>> (export.minimum_supported_calling_convention_version, export.maximum_supported_calling_convention_version)\n(9, 9)\n\n>>> from jax._src import config\n>>> with config.jax_export_calling_convention_version(9):\n...  exp = export.export(jnp.cos)(1.)\n...  exp.calling_convention_version\n9\n```\n\n----------------------------------------\n\nTITLE: JAX jaxpr Interpreter: Subcompiling jaxpr to MLIR/XLA, Executing Compiled HLO - Python\nDESCRIPTION: Implements jaxpr_subcomp, which interprets a JAX jaxpr by mapping variables and operations to MLIR/HLO, and sets up the environment for code generation and primitive translation. The environment traces values, reads and writes variable bindings, looks up rules for each primitive, and generates MLIR/HLO operations. Also defines execute_compiled, which runs the compiled executable on backend buffers, translates results, and handles input/output processing. Utility dictionaries and default handlers for primitive operations are also established. Assumes definitions for Var, Atom, ShapedArray, NumPy, and access to the appropriate primitive translation registry.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\ndef jaxpr_subcomp(c: MlirContext, jaxpr: Jaxpr, args: list[ir.Value]) -> list[ir.Value]:\n  env: dict[Var, ir.Value] = {}\n\n  def read(x: Atom) -> ir.Value:\n    return env[x] if type(x) is Var else _hlo_const(np.asarray(x.val))\n\n  def write(v: Var, val: ir.Value) -> None:\n    env[v] = val\n\n  map(write, jaxpr.in_binders, args)\n  for eqn in jaxpr.eqns:\n    in_avals = [x.aval for x in eqn.inputs]\n    in_vals = map(read, eqn.inputs)\n    out_avals = [x.aval for x in eqn.out_binders]\n    rule = hlo_translations[eqn.primitive]\n    assert all(isinstance(v, ir.Value) for v in in_vals), in_vals\n    out_vals = rule(c, in_avals, out_avals, in_vals, **eqn.params)\n    assert all(isinstance(v, ir.Value) for v in out_vals), out_vals\n    map(write, eqn.out_binders, out_vals), out_vals\n  return map(read, jaxpr.outs)\n\ndef execute_compiled(compiled, out_avals, *args):\n  input_bufs = [input_handlers[type(x)](x) for x in args]\n  out_bufs = compiled.execute(input_bufs)\n  return [handle_result(aval, buf) for aval, buf in zip(out_avals, out_bufs)]\n\ndefault_input_handler = xb.get_backend(None).buffer_from_pyval\ninput_handlers = {ty: default_input_handler for ty in\n                  [bool, int, float, np.ndarray, np.float64, np.float32]}\n\ndef handle_result(aval: ShapedArray, buf):\n  del aval  # Unused for now\n  return np.asarray(buf)\n\nhlo_translations = {}\n\n```\n\n----------------------------------------\n\nTITLE: Importing Module Documentation with Sphinx automodule\nDESCRIPTION: This reStructuredText directive instructs Sphinx to automatically generate documentation for the specified Python module (`jax.experimental.jet`) by extracting docstrings and other metadata from the source code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.jet.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: jax.experimental.jet\n```\n\n----------------------------------------\n\nTITLE: Computing with Dimension Variables Example\nDESCRIPTION: Demonstrates how to perform computations with dimension variables, including reshaping and arithmetic operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nf = lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],))\narg_spec = jax.ShapeDtypeStruct(export.symbolic_shape(\"b, 4\"), jnp.int32)\nexp = export.export(jax.jit(f))(arg_spec)\nexp.out_avals\n```\n\n----------------------------------------\n\nTITLE: GPU Array Addition with Block Specifications\nDESCRIPTION: Implementation of array addition using BlockSpecs to operate on 2-element blocks in parallel on GPU.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef add_kernel(x_ref, y_ref, o_ref):\n  # In this code, `x_ref`, `y_ref` and `o_ref` are (2,)-shaped `Ref`s\n  x = x_ref[:]\n  y = y_ref[:]\n  o_ref[:] = x + y\nx, y = jnp.arange(8), jnp.arange(8, 16)\nadd = pl.pallas_call(\n    add_kernel,\n    out_shape=jax.ShapeDtypeStruct((8,), jnp.int32),\n    in_specs=[\n        pl.BlockSpec((2,), lambda i: i),\n        pl.BlockSpec((2,), lambda i: i)\n    ],\n    out_specs=pl.BlockSpec((2,), lambda i: i),\n    grid=(4,))\nadd(x, y)\n```\n\n----------------------------------------\n\nTITLE: Conceptualizing Counter-Based PRNG Design in Haskell-like Pseudocode\nDESCRIPTION: This Haskell-like pseudocode outlines the core concepts of the Threefry counter-based PRNG used in JAX. It defines types `Key`, `Count`, and `Sample`, and functions: `hash` (the core random function), `split` (generates two new keys from one using the hash), and `draw_samples` (generates `n` samples from a key by hashing the key with incrementing counts `[1..n]`, enabling vectorized generation). The distinction between using hash output as a `Key` (for splitting) versus a `Sample` (for random values) is noted.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md#2025-04-22_snippet_4\n\nLANGUAGE: haskell\nCODE:\n```\n```haskell\ntype Sample = Int256\ntype Key = Sample  -- important identification for splitting\ntype Count = Int32\n\nhash :: Key -> Count -> Int256  -- output type equal to Key and Sample\n\nsplit :: Key -> (Key, Key)\nsplit key = (hash key 0, hash key 1)\n\ndraw_samples :: Key -> Int -> [Sample]\ndraw_samples key n = map (hash key) [1..n]\n```\n```\n\n----------------------------------------\n\nTITLE: Misleading Microbenchmark Due to Asynchronous Dispatch\nDESCRIPTION: This snippet demonstrates how JAX's asynchronous dispatch can lead to misleading microbenchmarks. The timing only measures dispatch time, not the actual execution time, resulting in extremely fast reported times for operations that are actually much more computationally intensive.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/async_dispatch.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%time jnp.dot(x, x)\n```\n\n----------------------------------------\n\nTITLE: Migrating FROM_GDA to jax.Array in PJIT\nDESCRIPTION: Shows how to update pjit calls by removing FROM_GDA from in_shardings when migrating to jax.Array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npjit(f, in_shardings=FROM_GDA, out_shardings=...) can be replaced by pjit(f, out_shardings=...)\n```\n\n----------------------------------------\n\nTITLE: Attempting to Differentiate Through pure_callback Wrapped Bessel Function - Python\nDESCRIPTION: Illustrates the limitation that, without a custom JVP or VJP, calling jax.grad on a pure_callback-wrapped function raises an error due to lack of autodiff rules. Highlights the need for explicit differentiation support when wrapping external functions in JAX. The code triggers an exception as expected.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\njax.grad(j1)(z)\n```\n\n----------------------------------------\n\nTITLE: Applying JVP to JIT-Compiled Function\nDESCRIPTION: Shows how to apply Jacobian-vector product (JVP) to a JIT-compiled function. The function is traced and compiled only during the first call.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  print('tracing!')\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\nx, xdot = 3., 1.\ny, ydot = jvp(f, (x,), (xdot,))\nprint(y)\nprint(ydot)\n```\n\n----------------------------------------\n\nTITLE: Using checkify with pmap for Parallel Mapping in JAX\nDESCRIPTION: Shows how to use checkify with JAX's parallel mapping transformation (pmap). The example demonstrates how mapped errors retain information about which mapped indices had errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nerr, z = jax.pmap(checked_f)(jnp.ones((3, 5)), jnp.array([-1, 2, 100]))\nerr.throw()\n\"\"\"\nValueError:\n..  at mapped index 0: index needs to be non-negative! (check failed at :6 (f))\n..  at mapped index 2: out-of-bounds indexing at <..>:7 (f)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Sharding Error with Different Device Sets in JAX\nDESCRIPTION: Shows that attempting to combine arrays sharded across disjoint sets of devices raises a ValueError, illustrating JAX's behavior when explicit shardings are incompatible due to different device assignments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsharding1 = NamedSharding(Mesh(jax.devices()[:4], 'x'), P('x'))\nsharding2 = NamedSharding(Mesh(jax.devices()[4:], 'x'), P('x'))\n\ny = jax.device_put(x, sharding1)\nz = jax.device_put(x, sharding2)\ntry: y + z\nexcept ValueError as e: print_exception(e)\n```\n\n----------------------------------------\n\nTITLE: Comparing Print Order in vmap and lax.map\nDESCRIPTION: Demonstrates how jax.debug.print reveals differences in evaluation order between jax.vmap and jax.lax.map.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/print_breakpoint.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nxs = jnp.arange(3.)\n\ndef f(x):\n  jax.debug.print(\"x: {}\", x)\n  y = jnp.sin(x)\n  jax.debug.print(\"y: {}\", y)\n  return y\njax.vmap(f)(xs)\n# Prints: x: 0.0\n#         x: 1.0\n#         x: 2.0\n#         y: 0.0\n#         y: 0.841471\n#         y: 0.9092974\njax.lax.map(f, xs)\n# Prints: x: 0.0\n#         y: 0.0\n#         x: 1.0\n#         y: 0.841471\n#         x: 2.0\n#         y: 0.9092974\n```\n\n----------------------------------------\n\nTITLE: Implementing Fast Jitted Plotter for ODE Solution in JAX\nDESCRIPTION: Defines a jitted plotting function 'jplotter' that efficiently plots the ODE solution using the custom line drawing algorithm. This function is then used to visualize the Lorentz attractor.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# plotting size and region:\nxlim, zlim = (-20, 20), (0, 50)\nxN, zN = 800, 600\n\n# fast, jitted plotting function\n@partial(jax.jit, static_argnums=(2,3,4,5))\ndef jplotter(xs, zs, xlim, zlim, xN, zN):\n  im = jnp.zeros((xN, zN))\n  xpixels = (xs - xlim[0])/(1.0 * (xlim[1] - xlim[0])) * xN\n  zpixels = (zs - zlim[0])/(1.0 * (zlim[1] - zlim[0])) * zN\n  def body_fun(i, im):\n    return drawline(im, xpixels[i-1], zpixels[i-1], xpixels[i], zpixels[i])\n  return lax.fori_loop(1, xpixels.shape[0], body_fun, im)\n\nim = jplotter(ys[...,0], ys[...,2], xlim, zlim, xN, zN)\nplot_image(im[:,::-1].T, cmap='magma')\n```\n\n----------------------------------------\n\nTITLE: Preserving Sharding in Array Copy Operations in JAX\nDESCRIPTION: Demonstrates that copying a sharded array with jnp.copy() preserves the original array's sharding pattern, showing that sharding information propagates through basic array operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nw_copy = jnp.copy(w)\njax.debug.visualize_array_sharding(w_copy)\n```\n\n----------------------------------------\n\nTITLE: Multi-Device Asynchronous Execution with Output Tokens in JAX\nDESCRIPTION: Demonstrates how to use output tokens to block on side-effecting computations executed on different devices without explicit sequencing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@partial(jax.jit, device=<device 0>)\ndef f():\n  jax.print(\"hello\")\n  return new_runtime_token()\n\n@partial(jax.jit, device=<device 1>)\ndef g():\n  jax.print(\"world\")\n  return new_runtime_token()\n\nt0 = f()\nt1 = g()\nblock_until_ready((t0, t1))\n```\n\n----------------------------------------\n\nTITLE: SSH Tunnel Setup for Google Cloud Compute Remote Profiling (Bash)\nDESCRIPTION: This command creates an SSH tunnel to forward local port 9001 to the VM's same port using Google Cloud's 'gcloud compute ssh'. Replace <machine-name> with your instance. Prerequisites: 'gcloud' must be installed and authenticated, and you must have permission to connect to the remote machine. This enables browser-based profiling through Perfetto UI when the profiled JAX code runs on a Google Cloud VM.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ gcloud compute ssh <machine-name> -- -L 9001:127.0.0.1:9001\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Uncommitted Arrays with Sharded Arrays in JAX\nDESCRIPTION: Shows that uncommitted arrays (like those created with jnp.ones_like or jnp.arange) can be combined with explicitly sharded arrays without errors. JAX automatically moves uncommitted arrays to match the sharding of committed arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ny = jax.device_put(x, sharding1)\ny + jnp.ones_like(y)\ny + jnp.arange(y.size).reshape(y.shape)\nprint('no error!')\n```\n\n----------------------------------------\n\nTITLE: Performing Standard Axis Reduction with JAX NumPy (Python)\nDESCRIPTION: This snippet illustrates a basic array reduction using standard JAX NumPy functionality. It initializes a 3D JAX array filled with ones and then computes the sum along the first axis (`axis=0`) using `jnp.sum`, demonstrating the target operation for the subsequent Pallas reduction examples.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.ones((8, 1024, 1024))\njnp.sum(x, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Transform Relations with vmap and custom_jvp_call in JAX (Python)\nDESCRIPTION: Illustrates the interaction between JAX's vmap transformation and custom_jvp_call primitives. Shows that vmap(call(f)) equals call(vmap(f)), and for custom_jvp_call, vmap applies to both the function and its custom JVP rule. Demonstrates semantics for transformation pass-throughs, vital for custom differentiation and batching transformations. No explicit parameters are given; notation is schematic for conceptual understanding.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/2026-custom-derivatives.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nvmap(call(f)) == call(vmap(f))\n```\n\nLANGUAGE: python\nCODE:\n```\nvmap(custom_jvp_call(f, f_jvp)) == custom_jvp_call(vmap(f), vmap(f_jvp))\n```\n\n----------------------------------------\n\nTITLE: Implementing All-Reduce Sum with Double-Buffering in JAX Pallas\nDESCRIPTION: This code implements a distributed all-reduce sum kernel using double-buffering and semaphores for synchronization. It includes barrier functions, memory management across HBM and VMEM, and proper signal handling to avoid race conditions between neighboring devices in a mesh.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npartition = P(None, 'x')\nmesh = jax.make_mesh((num_devices,), ('x',))\nsharding = jax.sharding.NamedSharding(mesh, partition)\n\ninput_arr = jax.random.uniform(jax.random.key(0), shape=(8, 128 * num_devices))\ninput_arr = jax.device_put(input_arr, sharding)\n\n\ndef local_barrier(left_neighbor, right_neighbor, double_barrier=True):\n  \"\"\"Performs a barrier with neighbors on the global barrier semaphore.\n\n  Optionally performs a second barrier, which prevents a potential race\n  when re-using the same collective_id across kernel invocations.\n  \"\"\"\n  barrier_sem = pltpu.get_barrier_semaphore()\n  for neighbor in [left_neighbor, right_neighbor]:\n    pltpu.semaphore_signal(\n      barrier_sem,\n      inc=1,\n      device_id=(neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n    )\n  pltpu.semaphore_wait(barrier_sem, 2)\n  if double_barrier:\n    # The double-barrier prevents a race condition where one neighbor can\n    # re-enter the kernel again on a subsequent call and increment the\n    # barrier semaphore a second time. This would unblock the current device\n    # even if the other neighbor is not ready yet.\n    # To implement a double-barrier, we stack-allocate a second REGULAR\n    # semaphore using run_scoped.\n    @functools.partial(pl.run_scoped,\n                       second_barrier=pltpu.SemaphoreType.REGULAR)\n    def _(second_barrier):\n      for neighbor in [left_neighbor, right_neighbor]:\n        pltpu.semaphore_signal(\n          second_barrier,\n          inc=1,\n          device_id=(neighbor,),\n          device_id_type=pltpu.DeviceIdType.MESH,\n        )\n      pltpu.semaphore_wait(second_barrier, 2)\n\n\ndef all_reduce_kernel(\n    x_ref,\n    o_ref,\n    hbm_scratch,\n    copy_sem,\n    remote_recv_sem,\n    remote_send_sem,\n    capacity_sem,\n    receive_scratch,\n):\n  outer_step = pl.program_id(0)\n  working_slot = lax.rem(outer_step, 2)\n  receiving_slot = 1 - working_slot\n\n  my_id = lax.axis_index('x')\n  right_neighbor = lax.rem(my_id + 1, num_devices)\n  left_neighbor = lax.rem(my_id - 1 + num_devices, num_devices)\n\n  @pl.when(outer_step == 0)\n  def _():\n    # Barrier with both neighbors at the start, since we will be\n    # communicating with both.\n    local_barrier(left_neighbor, right_neighbor)\n\n    # Initialize o_ref, acc_scratch, and hbm_scratch.\n    o_ref[...] = jnp.zeros_like(o_ref)\n    receive_scratch[...] = jnp.zeros_like(receive_scratch)\n    initial_copy = pltpu.make_async_remote_copy(\n        src_ref=x_ref,\n        dst_ref=hbm_scratch.at[working_slot],\n        send_sem=remote_send_sem,\n        recv_sem=remote_recv_sem,\n        device_id=(right_neighbor,),\n        device_id_type=pltpu.DeviceIdType.MESH,\n    )\n    initial_copy.start()\n    initial_copy.wait()\n\n  # Signal to our left neighbor that we are ready to receive.\n  # Without this signal, our left neighbor can be >=1 iteration ahead,\n  # meaning it could write into our working slot.\n  pltpu.semaphore_signal(\n      capacity_sem,\n      inc=1,\n      device_id=(left_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n\n  # Copy the partial result our left neighbor sent to us into VMEM for\n  # computation.\n  local_copy = pltpu.make_async_copy(\n      src_ref=hbm_scratch.at[working_slot],\n      dst_ref=receive_scratch,\n      sem=copy_sem,\n  )\n  local_copy.start()\n\n  # Block until our right neighbor is ready to receive.\n  pltpu.semaphore_wait(capacity_sem, 1)\n  # Pass the value to our right neighbor.\n  remote_copy = pltpu.make_async_remote_copy(\n      src_ref=hbm_scratch.at[working_slot],\n      dst_ref=hbm_scratch.at[receiving_slot],\n      send_sem=remote_send_sem,\n      recv_sem=remote_recv_sem,\n      device_id=(right_neighbor,),\n      device_id_type=pltpu.DeviceIdType.MESH,\n  )\n  remote_copy.start()\n  # Finish local copy and accumulate while remote_copy is happening.\n  local_copy.wait()\n  o_ref[...] += receive_scratch[...]\n  # Block until remote copy finishes.\n  remote_copy.wait()\n\n\nout_shape = (\n    jax.ShapeDtypeStruct((8, 128), jnp.float32),\n    # We allocate the double-buffer as a Pallas output so that it is\n    # resident in HBM.\n    jax.ShapeDtypeStruct((2, 8, 128), jnp.float32),  # hbm_scratch\n)\n\ngrid_spec = pltpu.PrefetchScalarGridSpec(\n    num_scalar_prefetch=0,\n    in_specs=[\n        # Our input lives in VMEM\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n    ],\n    out_specs=[\n        # Our output lives in VMEM\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n        # Our double-buffer lives in HBM\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n    ],\n    grid=(num_devices,),\n    scratch_shapes=(\n        [pltpu.SemaphoreType.DMA] * 3\n        + [pltpu.SemaphoreType.REGULAR]  # capacity_sem\n        + [pltpu.VMEM((8, 128), jnp.float32)]  # receive_scratch\n    ),\n)\n\nkernel = pl.pallas_call(\n    all_reduce_kernel,\n    out_shape=out_shape,\n    grid_spec=grid_spec,\n    compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n)\n\npallas_result = jax.jit(\n    shard_map.shard_map(\n        kernel,\n        mesh=mesh,\n        in_specs=partition,\n        out_specs=partition,\n        check_rep=False,\n    )\n)(input_arr)\npallas_result = jax.block_until_ready(pallas_result)[0]\n\n\ndef lax_sum(x):\n  return lax.psum(x, 'x')\n\n\nxla_result = jax.jit(\n    shard_map.shard_map(\n        lax_sum, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x')\n    )\n)(input_arr)\n\nprint('Input = ', input_arr[0, ::128])\nprint('Pallas result = ', pallas_result[0, ::128])\nprint('lax.psum result = ', xla_result[0, ::128])\ndifference = jnp.mean(jnp.abs(pallas_result - xla_result))\nprint('Difference |Pallas - lax.psum| = ', difference)\n```\n\n----------------------------------------\n\nTITLE: Implementing Interpreter Stack Management\nDESCRIPTION: Defines the MainTrace class and stack management functions for handling interpreter state. Provides context manager for pushing/popping interpreters onto the transformation stack.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Sequence\nfrom contextlib import contextmanager\nfrom typing import Any\n\nclass MainTrace(NamedTuple):\n  level: int\n  trace_type: type['Trace']\n  global_data: Any | None\n\ntrace_stack: list[MainTrace] = []\ndynamic_trace: MainTrace | None = None\n\n@contextmanager\ndef new_main(trace_type: type['Trace'], global_data=None):\n  level = len(trace_stack)\n  main = MainTrace(level, trace_type, global_data)\n  trace_stack.append(main)\n\n  try:\n    yield main\n  finally:\n    trace_stack.pop()\n```\n\n----------------------------------------\n\nTITLE: Applying VJP Function with Custom VJP\nDESCRIPTION: Shows how to apply the VJP function returned by vjp. The example demonstrates applying the function to compute cotangents.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nprint(f_vjp(1.))\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Gradient with Residuals (Updated Approach) - JAX - Python\nDESCRIPTION: This snippet shows the recommended modern method for custom_vjp in JAX, avoiding nondiff_argnums and using residuals instead. It passes all arguments explicitly and uses the custom_vjp decorator without nondiff_argnums, saving necessary values as residuals for use in the backward pass. Dependencies: JAX. Inputs: lo, hi, x (all can be Tracers). Outputs: x (the input). Limitations: None with respect to Tracers; fully supports advanced JAX transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4008-custom-vjp-update.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\\n\\n@jax.custom_vjp  # no nondiff_argnums!\\ndef clip_gradient(lo, hi, x):\\n  return x  # identity function\\n\\ndef clip_gradient_fwd(lo, hi, x):\\n  return x, (lo, hi)  # save lo and hi values as residuals\\n\\ndef clip_gradient_bwd(res, g):\\n  lo, hi = res\\n  return (None, None, jnp.clip(g, lo, hi))  # return None for lo and hi\\n\\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)\n```\n\n----------------------------------------\n\nTITLE: Installing CUDA-enabled JAXLIB with Conda - Bash\nDESCRIPTION: Installs the CUDA-enabled jaxlib package and JAX using conda-forge, ensuring that the installed jaxlib version supports CUDA (GPU acceleration). The wildcard selector 'jaxlib=*=*cuda*' ensures a CUDA build is chosen. Prerequisite: NVIDIA GPU and properly configured drivers; Conda must be installed. Input is the command-line instruction, and output is JAX and CUDA-enabled jaxlib installed in your active environment. May require compatible GPU hardware and CUDA runtime support.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nconda install \"jaxlib=*=*cuda*\" jax -c conda-forge\n\n```\n\n----------------------------------------\n\nTITLE: Forward-Mode Differentiation with Custom JVP\nDESCRIPTION: Shows that the custom JVP rule is invoked during forward-mode differentiation. The example demonstrates computing JVPs using a custom rule.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ny, y_dot = jvp(f, (3.,), (1.,))\nprint(y_dot)\n```\n\n----------------------------------------\n\nTITLE: JAX NumPy Type Promotion Implementation\nDESCRIPTION: Implements type promotion rules for JAX's numpy interface following official JAX type promotion documentation. Handles both standard and weakly-typed arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport pandas as pd\nfrom IPython import display\njax.config.update('jax_enable_x64', True)\n\njnp_dtypes = {\n  'b': jnp.bool_.dtype,\n  'u8': jnp.uint8.dtype, 'u16': jnp.uint16.dtype, 'u32': jnp.uint32.dtype, 'u64': jnp.uint64.dtype,\n  'i8': jnp.int8.dtype, 'i16': jnp.int16.dtype, 'i32': jnp.int32.dtype, 'i64': jnp.int64.dtype,\n  'bf16': jnp.bfloat16.dtype, 'f16': jnp.float16.dtype, 'f32': jnp.float32.dtype, 'f64': jnp.float64.dtype,\n  'c64': jnp.complex64.dtype, 'c128': jnp.complex128.dtype,\n  'i*': int, 'f*': float, 'c*': complex}\n\njnp_dtype_to_code = {val: key for key, val in jnp_dtypes.items()}\n\ndef make_jnp_zero(dtype):\n  if dtype in {int, float, complex}:\n    return dtype(0)\n  else:\n    return jnp.zeros((), dtype=dtype)\n\ndef jnp_result_code(dtype1, dtype2):\n  try:\n    out = jnp.add(make_jnp_zero(dtype1), make_jnp_zero(dtype2))\n  except TypeError:\n    return '-'\n  else:\n    if hasattr(out, 'aval') and out.aval.weak_type:\n      return out.dtype.kind + '*'\n    elif type(out) in {int, float, complex}:\n      return jnp_dtype_to_code[type(out)]\n    else:\n      return jnp_dtype_to_code[out.dtype]\n\ngrid = [[jnp_result_code(dtype1, dtype2)\n         for dtype2 in jnp_dtypes.values()]\n        for dtype1 in jnp_dtypes.values()]\ntable = pd.DataFrame(grid, index=jnp_dtypes.keys(), columns=jnp_dtypes.keys())\ndisplay.HTML(table.to_html())\n```\n\n----------------------------------------\n\nTITLE: Installing JAX for NVIDIA GPUs with CUDA 12\nDESCRIPTION: Command to install JAX with NVIDIA GPU support using CUDA 12. This is for users with NVIDIA graphics cards who want GPU acceleration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"jax[cuda12]\"\n```\n\n----------------------------------------\n\nTITLE: Collective Communication with pmap in JAX\nDESCRIPTION: This snippet shows how to use collective communication operations with pmap in JAX. It demonstrates the use of psum for summing values across devices and normalizing the results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom jax.lax import psum\n\n@partial(pmap, axis_name='i')\ndef f(x):\n  total = psum(x, 'i')\n  return x / total, total\n\nnormalized, total = f(jnp.arange(8.))\n\nprint(f\"normalized:\\n{normalized}\\n\")\nprint(\"total:\", total)\n```\n\n----------------------------------------\n\nTITLE: Implementing jax.Array Base Class in Python\nDESCRIPTION: Initial implementation of jax.Array base class in Python, using dynamic registration for correct isinstance behavior. This will later be reimplemented in pybind11.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/12049-type-annotations.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass Array:\n    # Implementation details\n    pass\n\n# Make jnp.ndarray an alias of jax.Array\njnp.ndarray = Array\n```\n\n----------------------------------------\n\nTITLE: Basic Pallas Program ID Usage in Python\nDESCRIPTION: Demonstrates using pallas.program_id to identify execution index in a grid axis for parallel processing. Shows how to access array elements and apply exponential function using references.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef f(x_ref, o_ref):\n  i = pl.program_id(axis=0)  # execution index in the first axis of the grid\n  o_ref[i] = jnp.exp(x_ref[i])\n```\n\n----------------------------------------\n\nTITLE: Using fori_loop unroll Parameter to Avoid Copies\nDESCRIPTION: This Python JAX function shows the idiomatic way to avoid defensive copies in loops like the ring communication example. By specifying `unroll=2` in the `fori_loop` call, JAX/XLA automatically unrolls the loop body, effectively creating the double-buffering pattern internally. This prevents the aliasing conflicts between iterations and eliminates the need for XLA to insert expensive `copy` operations, achieving the same result as manual unrolling but with simpler code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  def body(i, x):\n    fut = ppermute_start(x)\n    y = ppermute_done(fut)\n    return y\n  return fori_loop(0, 8, body, x, unroll=2)\n```\n```\n\n----------------------------------------\n\nTITLE: Performing Local Aggregation with shard_map in JAX\nDESCRIPTION: This example shows how aggregation functions like sum work in shard_map. By default, they only operate on the local shard data, resulting in separate sums for each shard rather than a global sum across all shards.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return jnp.sum(x, keepdims=True)\n\nshard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Version for Build\nDESCRIPTION: Command to explicitly specify which Python version to use for the hermetic build environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython build/build.py build --python_version=3.12\n```\n\n----------------------------------------\n\nTITLE: Validating Custom bfloat16 Matmul Kernel Output - JAX - Python\nDESCRIPTION: Generates large random input matrices in bfloat16 using JAX, runs both standard and custom blockwise matmul, and asserts numerical output equality. Dependencies: jax, jax.random, jax.numpy (jnp), numpy (np), custom matmul function from above. Parameters: m/k/n (dimensions), random key. Ensures correctness of the custom kernel with large shapes and bfloat16, errors out if results mismatch. Demonstrates applied kernel in a realistic ML data scenario. Requires sufficient memory for 4096x4096 matrices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nm, k, n = 4096, 4096, 4096\nk1, k2 = random.split(random.key(0), 2)\nx = random.normal(k1, (m, k), dtype=jnp.bfloat16)\ny = random.normal(k2, (k, n), dtype=jnp.bfloat16)\nnp.testing.assert_array_equal(x @ y, matmul(x, y))\n```\n\n----------------------------------------\n\nTITLE: Static Shape Alternative for nansum\nDESCRIPTION: This snippet provides an alternative implementation of nansum that avoids dynamic shapes and works with JAX transforms.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef nansum_2(x):\n  mask = ~jnp.isnan(x)  # boolean mask selecting non-nan values\n  return jnp.where(mask, x, 0).sum()\n\nprint(nansum_2(x))\n```\n\n----------------------------------------\n\nTITLE: Setting JAX Configuration Using Environment Variables\nDESCRIPTION: Demonstrates how to configure JAX behavior using environment variables before running a Python program. This method sets configuration globally before the program starts execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/config_options.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport JAX_ENABLE_X64=True\npython my_program.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Dropout Layer with PRNG in Python\nDESCRIPTION: Shows how to implement a dropout layer using the PRNG model in the stax neural net builder library. It includes both the initialization and application functions for the dropout layer.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef Dropout(rate, mode='train'):\n  def init_fun(input_shape):\n    return input_shape, ()\n  def apply_fun(rng, params, inputs):\n    if mode == 'train':\n      keep = lax.random.bernoulli(rng, rate, inputs.shape)\n      return np.where(keep, inputs / rate, 0)\n    else:\n      return inputs\n  return init_fun, apply_fun\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipelined Matrix Addition for Megacore TPUs in Python\nDESCRIPTION: This function implements pipelined matrix addition optimized for Megacore TPUs. It uses Pallas to split the computation across multiple TensorCores by specifying parallel dimension semantics.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef add_matrices_pipelined_megacore(x: jax.Array, y: jax.Array) -> jax.Array:\n  block_spec = pl.BlockSpec((256, 512), lambda i: (i, 0))\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(2,),\n      compiler_params=pltpu.TPUCompilerParams(dimension_semantics=(\"parallel\",))\n  )(x, y)\n\nx, y = jnp.ones((512, 512)), jnp.ones((512, 512))\nadd_matrices_pipelined_megacore(x, y)\n```\n\n----------------------------------------\n\nTITLE: Updated jax.Array Implementation\nDESCRIPTION: Shows the updated implementation using host_local_array_to_global_array to convert numpy arrays to jax.Array with specific partition specs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npjitted_f = pjit(f, out_shardings=...)\n\narray2, array3 = multihost_utils.host_local_array_to_global_array(\n    (np_array1, np_array2), mesh, (P('x'), P(None)))\n\npjitted_f(array1, array2, array3, array4)\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variable to Enable 64-bit Types\nDESCRIPTION: Demonstrates how to set the JAX_ENABLE_X64 environment variable to enable 64-bit datatypes from the command line when launching a Python program.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/default_dtypes.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ JAX_ENABLE_X64=1 python main.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Plotter for ODE Solutions in JAX\nDESCRIPTION: Defines a parallel plotting function 'pplotter' that uses JAX's pmap to efficiently plot multiple ODE solutions simultaneously. This function is then used to visualize both individual and combined ODE traces.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# parallel plotter using lexical closure and pmap'd core plotting function\ndef pplotter(_xs, _zs, xlim, zlim, xN, zN):\n  N_dev = _xs.shape[0]\n  im = jnp.zeros((N_dev, xN, zN))\n  @jax.pmap\n  def plotfn(im, xs, zs):\n    xpixels = (xs - xlim[0])/(1.0 * (xlim[1] - xlim[0])) * xN\n    zpixels = (zs - zlim[0])/(1.0 * (zlim[1] - zlim[0])) * zN\n    def body_fun(i, im):\n      return drawline(im, xpixels[i-1], zpixels[i-1], xpixels[i], zpixels[i])\n    return lax.fori_loop(1, xpixels.shape[0], body_fun, im)\n  return plotfn(im, _xs, _zs)\n\nxlim, zlim = (-20, 20), (0, 50)\nxN, zN = 200, 150\n# above, plot ODE traces separately\nims = pplotter(ys[...,0], ys[...,2], xlim, zlim, xN, zN)\nim = pack_images(ims[..., None], 4, 2)[..., 0]\nplot_image(im[:,::-1].T, cmap='magma')\n# below, plot combined ODE traces\nims = pplotter(ys[...,0], ys[...,2], xlim, zlim, xN*4, zN*4)\nplot_image(jnp.sum(ims, axis=0)[:,::-1].T, cmap='magma')\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Device-Local Shape in shard_map Functions\nDESCRIPTION: This code shows how functions inside shard_map only see the device-local portion of data. It prints both the global shape of the input array and the shape visible to the sharded function, illustrating how data is distributed across devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.arange(32)\nprint(f\"global shape: {x.shape=}\")\n\ndef f(x):\n  print(f\"device local shape: {x.shape=}\")\n  return x * 2\n\ny = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Sparse Pallas Kernel vs Dense JAX Implementation\nDESCRIPTION: This snippet defines a benchmarking function and compares the performance of the sparse Pallas kernel against a dense JAX implementation. It runs multiple trials and reports average execution times.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(f, ntrials: int = 100):\n  def run(*args, **kwargs):\n    # Compile function first\n    jax.block_until_ready(f(*args, **kwargs))\n    # Time function\n    result = timeit.timeit(lambda: jax.block_until_ready(f(*args, **kwargs)),\n                           number=ntrials)\n    time = result / ntrials\n    return time\n  return run\n\n\nn_trials = 100\n\npallas_impl = lambda *args: kernel(*args)\ntime = benchmark(pallas_impl, n_trials)(indices_i, indices_k, X_blocks, Y, zeros)\nprint(\"Sparse Kernel: %.3f ms (avg over %d trials)\" % (time * 1000, n_trials))\n\nref_impl = jax.jit(lambda x, y: x @ y)\ntime = benchmark(ref_impl, n_trials)(X_dense, Y)\nprint(\"Reference: %.3f ms (avg over %d trials)\" % (time * 1000, n_trials))\n```\n\n----------------------------------------\n\nTITLE: SMEM Allocation with Transforms\nDESCRIPTION: Demonstrates how to allocate shared memory with specific transforms in Pallas. Shows transform specification during SMEM allocation with tile and swizzle transforms.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntransforms = (plgpu.TileTransform((8, 64)), plgpu.SwizzleTransform(128))\nf = pl.pallas_call(\n  scratch_shapes=plgpu.SMEM((128, 128), jnp.float16, transforms=transforms),\n  ...\n)\n```\n\n----------------------------------------\n\nTITLE: Manually Unrolling Loop to Avoid Copies\nDESCRIPTION: This snippet demonstrates manual loop unrolling by performing two `ppermute` steps within a single loop body. The first `ppermute` reads from `x` and writes to `x2`, and the second reads from `x2` and writes to `y`. This effectively creates a double-buffering pattern (`x` and `x2`/`y` swap roles). By explicitly managing two distinct buffers within the unrolled body, the aliasing conflict that forced XLA to insert copies is removed. The loop now iterates half as many times.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  def body(i, x):\n    *sems, x, x2 = ppermute_start(x)\n    x2 = ppermute_done((*sems, x, x2))\n    \n    *sems, x2, y = ppermute_start(x2)\n    y = ppermute_done((*sems, x2, y))\n    return y\n  return fori_loop(0, 4, body, x)\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Type Definitions in JAX\nDESCRIPTION: Create initial type definitions in a private jax._src.typing module. This includes simple types like ArrayLike, DType, and Shape.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/12049-type-annotations.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nArray = Any  # Temporary alias\nArrayLike = Union[...]  # Types valid as inputs to jax.numpy functions\nDType = ...\nDTypeLike = ...\nShape = ...\nNamedShape = ...\nShapeLike = ...\n```\n\n----------------------------------------\n\nTITLE: Timing Single-Device Matrix Multiplication in JAX\nDESCRIPTION: Measures the execution time of matrix multiplication when both inputs are placed on a single device. The operation is timed for multiple iterations with block_until_ready() to ensure accurate timing results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%timeit -n 5 -r 5 jnp.dot(x_single, x_single).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: NumPy Accepting Python List Input\nDESCRIPTION: This snippet demonstrates NumPy's ability to accept Python lists as input to its API functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nnp.sum([1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with Reduce-Scatter using `shard_map` in JAX (Python)\nDESCRIPTION: Shows another variant of matrix multiplication using `shard_map`, this time producing a fully sharded result across both mesh dimensions 'i' and 'j'. It uses the same mesh and input specs as the previous example but specifies `out_specs=P('i', 'j')`. Inside the function `matmul_reduce_scatter`, it performs a local matrix multiplication (`jnp.matmul`) and then uses `jax.lax.psum_scatter` to sum partial results and scatter them across the 'j' mesh axis simultaneously along the second dimension (`scatter_dimension=1`). This demonstrates using different collectives within `shard_map` to achieve different sharding patterns for the output. Dependencies are the same as the first snippet.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@partial(shard_map, mesh=mesh, in_specs=(P('i', 'j'), P('j', None)),\n         out_specs=P('i', 'j'))\ndef matmul_reduce_scatter(a_block, b_block):\n  # c_partialsum: f32[8/X, 32]\n  c_partialsum = jnp.matmul(a_block, b_block)\n  # c_block: f32[8/X, 32/Y]\n  c_block = jax.lax.psum_scatter(c_partialsum, 'j', scatter_dimension=1, tiled=True)\n  return c_block\n\nc = matmul_reduce_scatter(a, b)\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto PGLE with Environment Variables in Bash\nDESCRIPTION: Commands to enable auto Profile Guided Latency Estimator (PGLE) by setting environment variables. Includes both mandatory and optional settings.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport JAX_ENABLE_PGLE=true\n\n# For JAX version <= 0.5.0 make sure to include:\nexport XLA_FLAGS=\"--xla_gpu_enable_latency_hiding_scheduler=true\"\n```\n\n----------------------------------------\n\nTITLE: Attempting to Create 64-bit Arrays with X64 Disabled\nDESCRIPTION: Shows how JAX prevents the creation of 64-bit arrays when jax_enable_x64=False, automatically truncating to 32-bit types with a warning.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/default_dtypes.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> jnp.arange(5, dtype='float64')  # doctest: +SKIP\nUserWarning: Explicitly requested dtype float64 requested in arange is not available, and will be \ntruncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the \nJAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\nArray([0., 1., 2., 3., 4.], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Implementing Asymmetric DMA Communication Pattern\nDESCRIPTION: This code snippet demonstrates an example of asymmetric DMA communication between 4 TPU devices. It uses pl.when to create device-specific behavior for copying data between devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef example_kernel(input_ref, output_ref, send_sem, recv_sem):\n    device_id = lax.axis_index('x')\n    copy_0_to_1 = pltpu.make_async_remote_copy(\n        src_ref=input_ref,\n        dst_ref=output_ref,\n        send_sem=send_sem,\n        recv_sem=recv_sem,\n        device_id=1,\n    )\n    copy_2_to_3 = pltpu.make_async_remote_copy(\n        src_ref=input_ref,\n        dst_ref=output_ref,\n        send_sem=send_sem,\n        recv_sem=recv_sem,\n        device_id=3,\n    )\n    copy_3_to_2 = pltpu.make_async_remote_copy(\n        src_ref=input_ref,\n        dst_ref=output_ref,\n        send_sem=send_sem,\n        recv_sem=recv_sem,\n        device_id=2,\n    )\n    @pl.when(device_id == 0)\n    def _():\n      copy_0_to_1.start()\n      copy_0_to_1.wait_send()\n    @pl.when(device_id == 1)\n    def _():\n      copy_0_to_1.wait_recv()\n    @pl.when(device_id == 2)\n    def _():\n      copy_2_to_3.start()\n      copy_2_to_3.wait_send()\n      copy_3_to_2.wait_recv()\n    @pl.when(device_id == 3)\n    def _():\n      copy_3_to_2.start()\n      copy_3_to_2.wait_send()\n      copy_2_to_3.wait_recv()\n```\n\n----------------------------------------\n\nTITLE: Handling Data-Dependent Control Flow with jax.jit (Python)\nDESCRIPTION: Defines a function `divide` with a conditional statement (`if y >= 1.`) that depends on the runtime value of `y`. Attempting to JIT-compile this function directly fails because `y` becomes an abstract tracer, which cannot be used in a Python boolean context. The surrounding text explains that specifying `static_argnums=1` for `jax.jit` keeps `y` as a regular Python value, allowing the condition to be evaluated before compilation. Requires `jax`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef divide(x, y):\n  return x / y if y >= 1. else 0.\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Translation for XLA Primitive\nDESCRIPTION: Implements the translation from JAX's broadcasting primitive to XLA HLO operations by creating a broadcast_in_dim operation with appropriate dimension mapping.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n(x,), (out_aval,) = in_vals, out_avals\ndims_complement = [i for i in range(len(shape)) if i not in axes]\nreturn [hlo.broadcast_in_dim(aval_to_ir_type(out_aval), x, dims_complement)]\n```\n\n----------------------------------------\n\nTITLE: Illustrating Side-Effect Reordering in JAX JIT Functions\nDESCRIPTION: Modifies the previous JAX example by adding `jax.print` calls within the JIT-compiled functions `f` and `g`. Due to asynchronous dispatch and potential parallel execution on different devices, the \"world\" print from `g` might occur before the \"hello\" print from `f`, demonstrating the breakdown of the sequential execution illusion for side-effects.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@partial(jax.jit, device=<device 0>)\ndef f():\n  jax.print(\"hello\")\n  return 2\n\n@partial(jax.jit, device=<device 1>)\ndef g():\n  jax.print(\"world\")\n  return 3\nf()\ng()\n```\n\n----------------------------------------\n\nTITLE: Testing the defjvps Implementation\nDESCRIPTION: Verifying that the defjvps implementation produces the same results as the original defjvp implementation by testing with jvp and grad.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(f(2., 3.))\ny, y_dot = jvp(f, (2., 3.), (1., 0.))\nprint(y)\nprint(y_dot)\nprint(grad(f)(2., 3.))\n```\n\n----------------------------------------\n\nTITLE: Allocating a Zero-Initialized MMA Accumulator in Pallas (Python)\nDESCRIPTION: Shows the process of using Pallas 'pl.run_scoped' to create and initialize a mutable TensorCore accumulator reference, which receives results in-place from MMA operations. The example function operates on the accumulator and returns it. At the function's end, dereferencing waits for any outstanding WGMMA operations. Requires access to 'pl', 'plgpu', and JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef compute(acc_ref):\\n  ...\\n  return acc_ref[...]\\noutput = pl.run_scoped(compute, plgpu.ACC((m, n), jnp.float32))\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Components for an Untyped IR in Python\nDESCRIPTION: This snippet defines fundamental types and structures for a simple, untyped Intermediate Representation (IR) based on Administrative Normal Form (ANF). `Var` is an alias for string-based variable names. `Atom` represents either a variable (`Var`) or a float literal. The `Equation` dataclass models a single line of computation in the IR, assigning the result of an operation (`op`) applied to arguments (`args`) to a variable (`var`). This IR structure facilitates program staging for transformations like dead-code elimination or reverse-mode AD.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_9\n\nLANGUAGE: ipython3\nCODE:\n```\nVar = str           # Variables are just strings in this untyped IR\nAtom = Var | float  # Atoms (arguments to operations) can be variables or (float) literals\n\n# Equation - a single line in our IR like `z = mul(x, y)`\n@dataclass\nclass Equation:\n  var  : Var         # The variable name of the result\n  op   : Op          # The primitive operation we're applying\n  args : tuple[Atom] # The arguments we're applying the primitive operation to\n```\n\n----------------------------------------\n\nTITLE: Confirming GPU Device Availability in JAX\nDESCRIPTION: This snippet creates a random array using JAX and checks that it's running on a GPU device. It asserts that the platform is 'gpu' to ensure proper hardware acceleration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_gpu.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nkey = jax.random.PRNGKey(1701)\narr = jax.random.normal(key, (1000,))\ndevice = list(arr.devices())[0]\nprint(f\"JAX device type: {device}\")\nassert device.platform == \"gpu\", \"unexpected JAX device type\"\n```\n\n----------------------------------------\n\nTITLE: Testing a Neural Network Layer with Sample Data in JAX\nDESCRIPTION: This snippet initializes random input data, weights, and bias values to test the neural network layer. It creates a 32-dimensional input vector, a 32x4 weight matrix, and a 4-dimensional bias vector using NumPy's random number generator.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/sharded-computation.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nrng = np.random.default_rng(0)\n\nx = rng.normal(size=(32,))\nweights = rng.normal(size=(32, 4))\nbias = rng.normal(size=(4,))\n\nlayer(x, weights, bias)\n```\n\n----------------------------------------\n\nTITLE: Type-Checking Enforcement for Typed PRNG Keys (Python)\nDESCRIPTION: Provides an example utility function for ensuring that an input key is a new-style typed key by checking the dtype. If the key is not typed, raises a TypeError, enforcing best practices for function inputs. Dependencies: from jax import dtypes. Input: key (jax.Array). Output: key (if valid), error (otherwise). For use in library code to require typed keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import dtypes\n\ndef ensure_typed_key_array(key: Array) -> Array:\n  if dtypes.issubdtype(key.dtype, dtypes.prng_key):\n    return key\n  else:\n    raise TypeError(\"New-style typed JAX PRNG keys required\")\n\n```\n\n----------------------------------------\n\nTITLE: Illustrating Problematic In-Place Operation in JAX\nDESCRIPTION: This Python JAX function `f()` shows why in-place operations can be problematic. The operation `add_one_inplace(x)` modifies `x`'s buffer. However, the original value of `x` is needed later for `x * 2`. Executing this directly would lead to `x * 2` using the modified value, which is incorrect. Dependencies: `jax.numpy as jnp`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f():\n  x = jnp.arange(...)\n  y = add_one_inplace(x)\n  return y, x * 2 # another x consumer!\n```\n```\n\n----------------------------------------\n\nTITLE: Memory Reference Transform Usage in Pallas GPU\nDESCRIPTION: Demonstrates how to use memory reference transforms for asynchronous copying between GMEM and SMEM, and subsequent TensorCore operations. Shows basic transform application in a kernel body.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef body(..., scratch_ref):\n  # Asynchronous copy will reformat the GMEM data to match the SMEM transforms\n  plgpu.copy_gmem_to_smem(..., scratch_ref, barrier)\n  barrier.wait()\n  plgpu.wgmma(..., scratch_ref)  # wgmma only accepts properly transformed refs\n  ...\n```\n\n----------------------------------------\n\nTITLE: Error When Calling Exported JAX Modules with Wrong Device Count - Python\nDESCRIPTION: This snippet demonstrates JAX's enforcement of device-count invariants for exported modules. Exporting a function for a set number of devices and attempting to call it with a different device count triggers an error. The example uses JAX's Mesh and NamedSharding APIs and outputs a ValueError when called with an incorrect device count. Prerequisites: JAX, numpy, and correct environment for multi-device setups.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax import export\n>>> from jax.sharding import Mesh, NamedSharding\n>>> from jax.sharding import PartitionSpec as P\n\n>>> export_devices = jax.local_devices()\n>>> export_mesh = Mesh(np.array(export_devices), (\"a\",))\n>>> def f(x):\n...   return x.T\n\n>>> exp = export.export(jax.jit(f))(\n...    jax.ShapeDtypeStruct((4 * len(export_devices),), dtype=np.int32,\n...                         sharding=NamedSharding(export_mesh, P(\"a\"))))\n\n>>> arg = jnp.arange(4 * len(export_devices))\n>>> exp.call(arg)  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: Exported module f was lowered for 8 devices and is called in a context with 1 devices. This is disallowed because: the module was lowered for more than 1 device.\n\n```\n\n----------------------------------------\n\nTITLE: Adding Explicit Constraint for Strided Operations\nDESCRIPTION: Demonstrates adding a symbolic constraint to fix an inconclusive inequality in a strided operation, ensuring the operation can be validated during compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: x[: 4*(x.shape[0] // 4)],\n               polymorphic_shapes=(\"b, ...\",),\n               polymorphic_constraints=(\"b >= 4*floordiv(b, 4)\",))\n```\n\n----------------------------------------\n\nTITLE: Recovering Old-Style Buffer from New-Style PRNG Key (Python)\nDESCRIPTION: Demonstrates how to obtain the raw uint32 buffer that underlies a typed PRNG key via jax.random.key_data. Useful for compatibility with code expecting old-style key format or for legacy library support. Input: typed key. Output: Array with dtype uint32, shape (2,). Use as a temporary workaround only; avoid for new code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> jax.random.key_data(key)\nArray([0, 0], dtype=uint32)\n\n```\n\n----------------------------------------\n\nTITLE: Running pytest for JAX export with logging\nDESCRIPTION: This snippet shows how to run pytest for a specific test file with logging enabled, outputting the log to a file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npytest tests/export_test.py -k test_basic --log-level=3 --log-file=/tmp/mylog.txt\n```\n\n----------------------------------------\n\nTITLE: Setting JAX Compilation Cache Directory Using os.environ in Python\nDESCRIPTION: This snippet sets the 'JAX_COMPILATION_CACHE_DIR' environment variable at the start of a Python script, ensuring JAX uses the specified disk directory for its persistent compilation cache. It should be called before any JAX operations that may trigger compilation. Requires the 'os' module.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"JAX_COMPILATION_CACHE_DIR\"] = \"/tmp/jax_cache\"\n```\n\n----------------------------------------\n\nTITLE: Illustrating JAX Tracers with jax.jit and Print (Python)\nDESCRIPTION: This snippet demonstrates how JAX transformations like `jax.jit` replace function arguments with special tracer values. Printing the argument `x` inside the JIT-compiled function reveals it's a `Traced<ShapedArray(...)>` object, not a standard Python float, illustrating JAX's internal tracing mechanism. Requires `jax` and `jax.numpy`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\ndef func(x):\n  print(x)\n  return jnp.cos(x)\n\nres = jax.jit(func)(0.)\n# Output includes:\n# Traced<ShapedArray(float32[])>\n```\n\n----------------------------------------\n\nTITLE: All_gather Examples with Device Variance in JAX\nDESCRIPTION: Demonstrates the use of all_gather_invariant and pscatter primitives, showing how device variance annotations make transposes efficient.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Example 4 rewritten with explicit all_reduce_invariant\nf4 = shmap(lambda x: all_gather_invariant(x, 'i'), P('i'), P())\n\n# Example 4 with intermediate device variance types annotated\n@partial(shmap, P('i'), P())\ndef f4(x:f32[1]{i}):\n  y:f32[8]{} = all_gather_invariant(x, 'i')\n  return y\n\n# Example 4 transpose with intermediate device variance types annotated\n@partial(shmap, in_specs=P(), out_specs=P('i'))\ndef f4_transpose(ybar:f32[8]):\n  xbar:f32[1]{i} = pscatter(ybar, 'i')\n  return xbar\n```\n\n----------------------------------------\n\nTITLE: Registering Primitive Operations via HLO Translation Rules in JAX - Python\nDESCRIPTION: Establishes translation rules for elementary operations (add, multiply, negate, sin, cos, compare, reduce_sum, broadcast) between JAX primitives and corresponding MLIR/HLO operations. Uses direct_translation as a generic rule for elementwise primitives, compare_translation for comparison, and reduce_sum_translation for reductions. Each rule extracts input/output types, applies the registered operation, asserts correct MLIR typing, and uses functools.partial to bind specific ops. Assumes `hlo.add`, `hlo.multiply`, and similar objects are available, as well as preexisting primitives like add_p, mul_p, etc. Broadcast translation is started but not shown completed in this snippet.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\ndef direct_translation(op, c, in_avals, out_avals, in_vals):\n  del c, in_avals, out_avals\n  return [op(*in_vals)]\n\nhlo_translations[add_p] = partial(direct_translation, hlo.add)\nhlo_translations[mul_p] = partial(direct_translation, hlo.multiply)\nhlo_translations[neg_p] = partial(direct_translation, hlo.negate)\nhlo_translations[sin_p] = partial(direct_translation, hlo.sine)\nhlo_translations[cos_p] = partial(direct_translation, hlo.cosine)\n\ndef compare_translation(op, c, in_avals, out_avals, in_vals):\n  del c, out_avals\n  return [hlo.compare(*in_vals, hlo.ComparisonDirectionAttr.get(op))]\n\nhlo_translations[greater_p] = partial(compare_translation, \"GT\")\nhlo_translations[less_p] = partial(compare_translation, \"LT\")\n\ndef reduce_sum_translation(c, in_avals, out_avals, in_vals, *, axis):\n  del c\n  (x_aval,), (out_aval,), (x,) = in_avals, out_avals, in_vals\n  op = hlo.ReduceOp(\n    [aval_to_ir_type(out_aval)], [x], [_hlo_const(np.array(0, x_aval.dtype))],\n    axis)\n  scalar_type = aval_to_ir_type(ShapedArray((), x_aval.dtype))\n  reducer_region = op.body.blocks.append(scalar_type, scalar_type)\n  with ir.InsertionPoint(reducer_region):\n    hlo.return_([hlo.add(*reducer_region.arguments)])\n  return op.results\n\nhlo_translations[reduce_sum_p] = reduce_sum_translation\n\ndef broadcast_translation(c, in_avals, out_avals, in_vals, *, shape, axes):\n  del c\n\n```\n\n----------------------------------------\n\nTITLE: Using checkify with custom_vjp for Gradient Checks in JAX\nDESCRIPTION: Shows how to use checkify.check with custom vector-Jacobian products (VJPs) to add checks on gradient values. This example ensures gradients are negative by adding a custom backward pass with a check.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@jax.custom_vjp\ndef assert_gradient_negative(x):\n return x\n\ndef fwd(x):\n return assert_gradient_negative(x), None\n\ndef bwd(_, grad):\n checkify.check(grad < 0, \"gradient needs to be negative!\")\n return (grad,)\n\nassert_gradient_negative.defvjp(fwd, bwd)\n\njax.grad(assert_gradient_negative)(-1.)\n# ValueError: gradient needs to be negative!\n```\n\n----------------------------------------\n\nTITLE: Checking Array Types in NumPy and JAX\nDESCRIPTION: Shows how to check the underlying types of NumPy and JAX arrays, demonstrating that they are implemented as different Python types despite their similar interfaces.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/thinking_in_jax.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntype(x_np)\n```\n\nLANGUAGE: python\nCODE:\n```\ntype(x_jnp)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Registered PyTree CustomClass with State Mutation - Python\nDESCRIPTION: This code snippet shows usage of the previously registered PyTree class, instantiating the class, invoking its JIT-compiled method, mutating its state, and handling both standard Python data and JAX arrays. Dependencies include the earlier CustomClass code and JAX installations. Inputs are different construction patterns and state mutation; outputs show correct propagation of state to JITted methods. This demonstrates robustness of the PyTree approach for both hashable and non-hashable data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nc = CustomClass(2, True)\nprint(c.calc(3))\n\nc.mul = False  # mutation is detected\nprint(c.calc(3))\n\nc = CustomClass(jnp.array(2), True)  # non-hashable x is supported\nprint(c.calc(3))\n```\n\n----------------------------------------\n\nTITLE: Implementing Array Abstractions\nDESCRIPTION: Defines ShapedArray and ConcreteArray classes for array abstraction. Handles array shapes, dtypes and concrete values with operator implementations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ShapedArray:\n  array_abstraction_level = 1\n  shape: tuple[int, ...]\n  dtype: np.dtype\n\n  def __init__(self, shape, dtype):\n    self.shape = shape\n    self.dtype = dtype\n\n  @property\n  def ndim(self):\n    return len(self.shape)\n\n  _neg = staticmethod(neg)\n  _add = staticmethod(add)\n  _radd = staticmethod(swap(add))\n  _mul = staticmethod(mul)\n  _rmul = staticmethod(swap(mul))\n  _gt = staticmethod(greater)\n  _lt = staticmethod(less)\n\n  @staticmethod\n  def _bool(tracer):\n    raise Exception(\"ShapedArray can't be unambiguously converted to bool\")\n\n  @staticmethod\n  def _nonzero(tracer):\n    raise Exception(\"ShapedArray can't be unambiguously converted to bool\")\n\n  def str_short(self):\n    return f'{self.dtype.name}[{\",\".join(str(d) for d in self.shape)}]'\n\n  def __hash__(self):\n    return hash((self.shape, self.dtype))\n\n  def __eq__(self, other):\n    return (type(self) is type(other) and\n            self.shape == other.shape and self.dtype == other.dtype)\n\n  def __repr__(self):\n    return f\"ShapedArray(shape={self.shape}, dtype={self.dtype})\"\n\nclass ConcreteArray(ShapedArray):\n  array_abstraction_level = 2\n  val: np.ndarray\n\n  def __init__(self, val):\n    self.val = val\n    self.shape = val.shape\n    self.dtype = val.dtype\n\n  @staticmethod\n  def _bool(tracer):\n    return bool(tracer.aval.val)\n\n  @staticmethod\n  def _nonzero(tracer):\n    return bool(tracer.aval.val)\n\ndef get_aval(x):\n  if isinstance(x, Tracer):\n    return x.aval\n  elif type(x) in jax_types:\n    return ConcreteArray(np.asarray(x))\n  else:\n    raise TypeError(x)\n\njax_types = {bool, int, float,\n             np.bool_, np.int32, np.int64, np.float32, np.float64, np.ndarray}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Single-Device Array Placement in JAX\nDESCRIPTION: Places an array on a single device (the first device) and visualizes its sharding to contrast with multi-device sharded arrays. This is used as part of a timing experiment to compare performance between single-device and multi-device computations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx_single = jax.device_put(x, jax.devices()[0])\njax.debug.visualize_array_sharding(x_single)\n```\n\n----------------------------------------\n\nTITLE: Setting JAX Compilation Cache Directory using set_cache_dir in Python\nDESCRIPTION: Demonstrates how to set the persistent compilation cache directory using the set_cache_dir() function from jax.experimental.compilation_cache.compilation_cache. This approach is useful for advanced or programmatic setups. Requires importing the appropriate submodule from 'jax.experimental'.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental.compilation_cache import compilation_cache as cc\ncc.set_cache_dir(\"/tmp/jax_cache\")\n```\n\n----------------------------------------\n\nTITLE: Testing Resharding with shard_map in JAX FFI\nDESCRIPTION: Example showing how shard_map handles inputs with shardings that don't match the specified specs. This code verifies the resharding by checking for all-to-all operations in the compiled HLO representation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nhlo_data_shmap = jax.jit(rms_norm_shmap, out_shardings=data_shd).lower(x_data_shd).compile().as_text()\nassert \"all-to-all\" in hlo_data_shmap\n```\n\n----------------------------------------\n\nTITLE: Building jaxlib on Windows\nDESCRIPTION: Command to build jaxlib from source on Windows once all the prerequisites are installed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython .\\build\\build.py build --wheels=jaxlib\n```\n\n----------------------------------------\n\nTITLE: Linearized Conditional Function Evaluation in Python\nDESCRIPTION: Example of linearizing a conditional function and evaluating it with a specific input. Demonstrates the use of linearize with a simple conditional expression.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_90\n\nLANGUAGE: python\nCODE:\n```\n_, f_lin = linearize(lambda x: cond(True, lambda: x, lambda: 0.), 1.)\nout = f_lin(3.14)\nprint(out)\n```\n\n----------------------------------------\n\nTITLE: Stateful Implementation of Asynchronous Permutation using Refs\nDESCRIPTION: This snippet demonstrates a stateful approach to asynchronous permutation using Refs instead of values. It creates separate Refs for input and output, avoiding many of the issues associated with value-based approaches.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  x_ref = make_ref(x)\n  y_ref = make_ref(zeros_like(x))\n  fut = ppermute_start_stateful(x_ref, y_ref)\n  ppermute_done_stateful(*fut, x_ref, y_ref)\n  return y_ref[...]\n```\n\n----------------------------------------\n\nTITLE: Initializing JAX Dependencies\nDESCRIPTION: Imports key JAX modules and initializes random number generator key.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\n\nkey = random.key(0)\n```\n\n----------------------------------------\n\nTITLE: Warning for Unused Donated Buffers in JAX (Python)\nDESCRIPTION: Demonstrates a scenario where JAX issues a `UserWarning` because more buffers were donated than could be used. When `add(x, y)` is JIT-compiled with `donate_argnums=(0, 1)`, both input buffers are donated. However, the function only produces one output, so only one donated buffer can be reused, leading to a warning about the unused donated buffer. Requires `jax` and the `add`, `x`, `y` variables from previous examples.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Assume add, x, y are defined as before\n# x = jax.device_put(np.ones((2, 3)))\n# y = jax.device_put(np.ones((2, 3)))\n\n# Execute `add` with donation of the buffers for both `x` and `y`.\n# One of those buffers will be used for the result, but the other will\n# not be used.\nz = jax.jit(add, donate_argnums=(0, 1))(x, y)\n# >> UserWarning: Some donated buffers were not usable: f32[2,3]{1,0}\n```\n\n----------------------------------------\n\nTITLE: Collecting and Processing JAX Profile for Manual PGLE\nDESCRIPTION: Python code that demonstrates how to collect a JAX profile, post-process it, and save the extracted instruction latencies to a binary protobuf file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom etils import epath\nimport jax\nfrom jax.experimental import profiler as exp_profiler\n\n# Define your profile directory\nprofile_dir = 'gs://my_bucket/profile'\njax.profiler.start_trace(profile_dir)\n\n# run your workflow\n# for i in range(10):\n#   train_step()\n\n# Stop trace\njax.profiler.stop_trace()\nprofile_dir = epath.Path(profile_dir)\ndirectories = profile_dir.glob('plugins/profile/*/')\ndirectories = [d for d in directories if d.is_dir()]\nrundir = directories[-1]\nlogging.info('rundir: %s', rundir)\n\n# Post process the profile\nfdo_profile = exp_profiler.get_profiled_instructions_proto(os.fspath(rundir))\n\n# Save the profile proto to a file.\ndump_dir = rundir / 'profile.pb'\ndump_dir.parent.mkdir(parents=True, exist_ok=True)\ndump_dir.write_bytes(fdo_profile)\n```\n\n----------------------------------------\n\nTITLE: Executing Asynchronous Side Effect in JAX\nDESCRIPTION: Demonstrates a JIT-compiled function with a side effect (printing) but no return value, which executes asynchronously.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f():\n  jax.print(\"hello world\")\n  return\nf() # Executed asynchronously\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison: NumPy on CPU\nDESCRIPTION: Benchmarks matrix multiplication performance using standard NumPy on CPU, converting the JAX array to a NumPy array first.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nx_cpu = np.array(x)\n%timeit -n 1 -r 1 np.dot(x_cpu, x_cpu)\n```\n\n----------------------------------------\n\nTITLE: Mask Construction and Select with JAX Transforms (Python)\nDESCRIPTION: This example performs boolean mask construction using jax.numpy and applies lax.select after jitting. It demonstrates differences in staging constants and mask construction logic before and after omnistaging. Dependencies include JAX, JAX numpy (jnp), the lax module, and assumes input is a numpy array of shape (M, N). The primary function returns a selected or zeroed array based on the lower triangular mask logic.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import lax\n\n@jit\ndef select_tril(x):\n  mask = jnp.arange(x.shape[0])[:, None] > jnp.arange(x.shape[1])\n  return lax.select(mask, x, jnp.zeros_like(x))  # lax.select is like jnp.where\n\nx = np.arange(12).reshape((3, 4))\nselect_tril(x)\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation with Static Argument for JAX Primitive (Python)\nDESCRIPTION: Illustrates JIT for a function with one static argument using JAX's jit(static_argnums=1). Compiles only w.r.t. the first argument, passing the second argument as concrete, thus causing the JAX abstract evaluation machinery to handle a mix of ShapedArray and ConcreteArray. Checks the numeric correctness. Requires prior registration of the primitive and JAX API imports.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nassert api.jit(lambda x, y: square_add_prim(x, y), \n               static_argnums=1)(2., 10.) == 14.\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Matrix Multiplication FLOPs and Memory Bandwidth - JAX/Numpy - Python\nDESCRIPTION: Provides functions for computing the total number of floating point operations (FLOPs) and memory bandwidth usage required for a matrix multiplication given matrix shapes and a specified dtype. Requires dependencies: jax (as jnp), numpy (as np). Key parameters are m and n (matrix dimensions), k (shared dimension), dtype (numpy/jax dtype object). Inputs are matrix sizes and type; outputs are the required FLOPs and bytes moved. Limitations: Assumes matrices fit in memory, approximate formula used for FLOPs. Useful for performance estimation in ML workloads.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_flops(m: int, k: int, n: int):\n  return 2 * m * k * n\n\ndef matmul_membw(m: int, k: int, n: int, dtype: jnp.dtype):\n  return (m * k + k * n + m * n) * np.dtype(dtype).itemsize\n\nprint(matmul_flops(1024, 1024, 1024))\nprint(matmul_membw(1024, 1024, 1024, jnp.float32))\n```\n\n----------------------------------------\n\nTITLE: Implementing Checkpointed VJP for Composition in JAX Python\nDESCRIPTION: This snippet defines `f_vjp_checkpoint`, an alternative VJP for the composite function `f(x) = h(g(x))`. It delays the computation of `g`'s VJP (`g_vjp`) until the backward pass (`f_bwd2`). This mimics the behavior of `jax.checkpoint`, potentially saving memory by not storing residuals from `g` during the forward pass, at the cost of recomputing `g(x)` during the backward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef f_vjp_checkpoint(x):\n  y = g(x)\n  z, h_vjp = jax.vjp(h, y)\n  def f_bwd2(z_bar):\n    y_bar, = h_vjp(z_bar)\n    _, g_vjp = jax.vjp(g, x)\n    x_bar, = g_vjp(y_bar)\n    return x_bar\n  return z, f_bwd2\n```\n\n----------------------------------------\n\nTITLE: Tracing Through a Jaxpr Interpreter\nDESCRIPTION: Shows how to trace through the inverse function to get its JAX expression representation, demonstrating that custom interpreters are first-class transformations in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\njax.make_jaxpr(inverse(f))(f(1.))\n```\n\n----------------------------------------\n\nTITLE: Auto-documenting the jax.stages.Compiled Class using Sphinx\nDESCRIPTION: This Sphinx directive generates documentation for the `Compiled` class within the `jax.stages` module. It specifically includes documentation for the `in_tree`, `out_tree`, `as_text`, `cost_analysis`, `memory_analysis`, `runtime_executable` members and the special `__call__` method.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.stages.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: Compiled\n   :members: in_tree, out_tree, as_text, cost_analysis, memory_analysis, runtime_executable\n   :special-members: __call__\n```\n\n----------------------------------------\n\nTITLE: Correct PRNG Key Handling in JAX\nDESCRIPTION: Demonstrates the proper way to handle PRNG keys in JAX by using random.split to create a batch of keys and then using vmap with in_axes=0 to map the random function over these keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nkeys = random.split(random.PRNGKey(0))\ndata = jax.vmap(random.uniform, in_axes=0)(keys)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Variable Capture Issues in JAX JIT with call_tf\nDESCRIPTION: This example demonstrates how capturing and modifying TensorFlow variables in a function can cause errors when used with JAX's JIT compilation through call_tf.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nvar1 = tf.Variable(1.)\ndef impure_func_tf(x):\n  var1.write(11.)  # BAD: should not write to variables\n  return x + var1\n\njax2tf.call_tf(impure_func_tf)(tf.constant(2.))  # Works in eager mode\njax.jit(jax2tf.call_tf(impure_func_tf))(tf.constant(2.))  # Fails in jit mode\n```\n\n----------------------------------------\n\nTITLE: Applying JAX Logical Operators Element-wise on Arrays in Python\nDESCRIPTION: Shows the element-wise application of the `jax_check_positive_even` function (using `jnp.logical_and`) on a JAX array. This demonstrates how JAX logical operators handle array inputs. Depends on `jax.numpy` and the `jax_check_positive_even` function (assumed defined from previous snippet).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Assuming jax_check_positive_even is defined as in the previous snippet\n# import jax.numpy as jnp\nx = jnp.array([-1, 2, 5])\nprint(jax_check_positive_even(x))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Standard Autodiff Computation\nDESCRIPTION: Uses the print_fwd_bwd utility to visualize the forward and backward computation of the standard neural network function without checkpointing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# no use of jax.checkpoint:\nprint_fwd_bwd(f, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Conditional Partial Evaluation Implementation in Python\nDESCRIPTION: Implements partial evaluation rules for conditional operations in JAX. Handles unknown inputs, splits computations, and constructs new conditional equations with appropriate jaxprs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_91\n\nLANGUAGE: python\nCODE:\n```\ndef cond_peval_eqn(unks_in: list[bool], eqn: JaxprEqn,\n                   ) -> tuple[JaxprEqn, JaxprEqn, list[bool], list[Atom]]:\n  pred_unk, *unks_in = unks_in\n  assert not pred_unk\n  true_jaxpr, false_jaxpr = eqn.params['true_jaxpr'], eqn.params['false_jaxpr']\n  *jaxprs, unks_out, num_res = _cond_partial_eval(true_jaxpr, false_jaxpr, unks_in)\n  t_jaxpr1, f_jaxpr1, t_jaxpr2, f_jaxpr2 = jaxprs\n  ins1, ins2 = partition_list(unks_in, eqn.inputs[1:])\n  outs1, outs2 = partition_list(unks_out, eqn.out_binders)\n  residuals, _ = split_list(t_jaxpr2.in_binders, num_res)\n  eqn1 = JaxprEqn(cond_p, [eqn.inputs[0], *ins1],\n                  dict(true_jaxpr=t_jaxpr1, false_jaxpr=f_jaxpr1),\n                  outs1 + residuals)\n  eqn2 = JaxprEqn(cond_p, [eqn.inputs[0], *residuals, *ins2],\n                  dict(true_jaxpr=t_jaxpr2, false_jaxpr=f_jaxpr2),\n                  outs2)\n  res = [eqn.inputs[0], *residuals] if type(eqn.inputs[0]) is Var else residuals\n  return eqn1, eqn2, unks_out, res\npartial_eval_jaxpr_rules[cond_p] = cond_peval_eqn\n```\n\n----------------------------------------\n\nTITLE: Current Inefficient Transpose of shmap with all_gather in JAX\nDESCRIPTION: This snippet displays the current transpose `t(f4)` generated by JAX for the `f4` function. It takes an unmapped input `ybar`, applies `psum_scatter(ybar / 8, 'i')` (the transpose of `all_gather`), and produces an output sharded along 'i'. The `psum_scatter` involves potentially unnecessary communication because `ybar` is device-invariant.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Currently we get this inefficient transpose\nt(f4) = shmap(lambda ybar: psum_scatter(ybar / 8, 'i'), P(), P('i'))\n```\n```\n\n----------------------------------------\n\nTITLE: JAX Fine-Grained Control for Out-of-Bounds Indexing\nDESCRIPTION: These snippets show how to use ndarray.at for more control over out-of-bounds indexing behavior in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\njnp.arange(10.0).at[11].get()\n```\n\nLANGUAGE: python\nCODE:\n```\njnp.arange(10.0).at[11].get(mode='fill', fill_value=jnp.nan)\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Libraries for Checkpointing\nDESCRIPTION: Imports the necessary JAX modules for working with checkpointing functionality.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Creating a Device Mesh in JAX\nDESCRIPTION: Creates a 2D mesh of devices with dimensions 4×2, assigning dimension names 'a' and 'b' for use in subsequent sharding operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmesh = jax.make_mesh((4, 2), ('a', 'b'))\n```\n\n----------------------------------------\n\nTITLE: Implementing BatchTracer and BatchTrace for Vectorized Batching in Python\nDESCRIPTION: Defines BatchTracer and BatchTrace classes for handling vectorized batching. BatchTracer carries a batched value and an optional batch axis, while BatchTrace processes primitives using vmap rules.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Union\n\nclass NotMapped: pass\nnot_mapped = NotMapped()\n\nBatchAxis = Union[NotMapped, int]\n\nclass BatchTracer(Tracer):\n  def __init__(self, trace, val, batch_dim: BatchAxis):\n    self._trace = trace\n    self.val = val\n    self.batch_dim = batch_dim\n\n  @property\n  def aval(self):\n    if self.batch_dim is not_mapped:\n      return get_aval(self.val)\n    else:\n      return mapped_aval(self.batch_dim, get_aval(self.val))\n\n  def full_lower(self):\n    if self.batch_dim is not_mapped:\n      return full_lower(self.val)\n    else:\n      return self\n\nclass BatchTrace(Trace):\n  pure = lift = lambda self, val: BatchTracer(self, val, not_mapped)\n\n  def process_primitive(self, primitive, tracers, params):\n    vals_in, bdims_in = unzip2((t.val, t.batch_dim) for t in tracers)\n    vmap_rule = vmap_rules[primitive]\n    val_outs, bdim_outs = vmap_rule(self.axis_size, vals_in, bdims_in, **params)\n    return [BatchTracer(self, x, bd) for x, bd in zip(val_outs, bdim_outs)]\n\n  @property\n  def axis_size(self):\n    return self.main.global_data\n\nvmap_rules = {}\n```\n\n----------------------------------------\n\nTITLE: Verifying NumPy Block Matmul Implementation\nDESCRIPTION: Verifies the correctness of the custom `block_matmul` function against NumPy's built-in matrix multiplication (`@` operator). It generates two large random matrices (`x`, `y`) and uses `np.testing.assert_allclose` to compare the results of the two methods within a specified tolerance.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nm, k, n = 4096, 4096, 4096\nx = np.random.uniform(size=(m, k)).astype(np.float32)\ny = np.random.uniform(size=(k, n)).astype(np.float32)\nnp.testing.assert_allclose(x @ y, block_matmul(x, y), atol=1e-6, rtol=1e-6)\n```\n\n----------------------------------------\n\nTITLE: Implementing Flatten Function and Store Class in Python\nDESCRIPTION: This code defines a flatten_fun function that wraps a user function to handle flattened inputs and outputs. It also includes a Store class for managing mutable state during function execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef flatten_fun(f, in_tree):\n  store = Store()\n\n  def flat_fun(*args_flat):\n    pytree_args = tree_unflatten(in_tree, args_flat)\n    out = f(*pytree_args)\n    out_flat, out_tree = tree_flatten(out)\n    store.set_value(out_tree)\n    return out_flat\n\n  return flat_fun, store\n\nclass Empty: pass\nempty = Empty()\n\nclass Store:\n  val = empty\n\n  def set_value(self, val):\n    assert self.val is empty\n    self.val = val\n\n  def __call__(self):\n    return self.val\n```\n\n----------------------------------------\n\nTITLE: Executing Multiple JVP Checks with Randomized Seeds - JAX - Python\nDESCRIPTION: Calls the above check(seed) function multiple times with different seeds (0, 1, 2) to validate JVP correctness under various random parameters. This demonstrates reproducibility and variability in automatic differentiation validation. No additional dependencies beyond previously defined functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ncheck(0)\ncheck(1)\ncheck(2)\n\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Libraries for Neural Network Training\nDESCRIPTION: Imports essential JAX modules for numerical operations, automatic differentiation, just-in-time compilation, and vectorization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\n```\n\n----------------------------------------\n\nTITLE: Visualizing Wave Equation Simulation Results\nDESCRIPTION: Creates various visualizations of the wave equation simulation results, including a single frame plot and a multi-frame plot showing the wave's evolution over time.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(18, 6))\nplt.axis('off')\nplt.imshow(u_final[-1].T, cmap='RdBu');\n```\n\nLANGUAGE: python\nCODE:\n```\nfig, axes = plt.subplots(9, 1, figsize=(14, 14))\n[ax.axis('off') for ax in axes]\naxes[0].imshow(u_final[0].T, cmap='RdBu', aspect='equal', vmin=-1, vmax=1)\nfor i in range(8):\n  axes[i+1].imshow(u_final[4*i+1].T / abs(u_final[4*i+1]).max(), cmap='RdBu', aspect='equal', vmin=-1, vmax=1)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Type Promotion Lattice with bfloat16 Included - NetworkX and Matplotlib - Python\nDESCRIPTION: This code snippet visualizes a modified type promotion lattice in which 'bfloat16' is added as a separate branch, using NetworkX and matplotlib for the visualization. It distinguishes between 'f16' and 'bf16' nodes and their respective promotion paths. Required packages are networkx and matplotlib; primary parameters are the lattice dictionary with updated nodes and the customized positions for each node. The output is a matplotlib figure of the lattice with both 'f16' and 'bf16' paths visible; constraints include the exclusion of dynamic type relationships or framework-specific promotion behaviors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\\nimport matplotlib.pyplot as plt\\nlattice = {\\n  'i*': ['u8', 'i8'], 'f*': ['c*', 'f16', 'bf16'], 'c*': ['c64'],\\n  'u8': ['u16', 'i16'], 'u16': ['u32', 'i32'], 'u32': ['u64', 'i64'],\\n  'i8': ['i16'], 'i16': ['i32'], 'i32': ['i64'], 'i64': ['f*'],\\n  'f16': ['f32'], 'bf16': ['f32'], 'f32': ['f64', 'c64'], 'f64': ['c128'],\\n  'c64': ['c128']\\n}\\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\\npos = {\\n  'i*': [-1.25, 0.5], 'f*': [-0.5, 2], 'c*': [0, 3],\\n  'u8': [0.5, 0], 'u16': [1.5, 0], 'u32': [2.5, 0], 'u64': [3.5, 0],\\n  'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\\n  'f16': [1.8, 1.7], 'bf16': [1.8, 2.3], 'f32': [3.0, 2], 'f64': [4.0, 2],\\n  'c64': [3.5, 3], 'c128': [4.5, 3],\\n}\\nfig, ax = plt.subplots(figsize=(6, 5))\\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\n```\n\n----------------------------------------\n\nTITLE: Illustrating `pure_callback` Elision When Result is Unused\nDESCRIPTION: Similar to the previous example, this defines `print_something` and calls it via `jax.pure_callback` inside `f2`. However, in `f2`, the return value of the callback is ignored (the function returns a constant `1.0`). Because JAX treats `pure_callback` functions as pure (no side effects), and the result is unused, the compiler eliminates the callback call entirely, and nothing is printed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f2():\n  jax.pure_callback(print_something, np.int32(0))\n  return 1.0\nf2();\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for Wave Equation Simulation\nDESCRIPTION: Installs required packages for the wave equation simulation, including Pillow for image processing, moviepy for creating animations, and scikit-image for image filtering.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -U -q Pillow moviepy proglog scikit-image\n```\n\n----------------------------------------\n\nTITLE: GPU Block Specification with Transforms\nDESCRIPTION: Shows how to specify transforms when creating GPU block specifications for Pallas calls. Demonstrates setting up tile and swizzle transforms for input and output specifications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransforms = (plgpu.TileTransform((8, 64)), plgpu.SwizzleTransform(128))\nf = pl.pallas_call(\n  in_specs=plgpu.GPUBlockSpec(in_block_shape, in_index_map, transforms=transforms),\n  out_specs=plgpu.GPUBlockSpec(out_block_shape, out_index_map, transforms=transforms),\n  ...\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Setuptools Version with Hash Values for JAX Project\nDESCRIPTION: This snippet defines the required version of setuptools (76.0.0) along with its SHA256 hash values. It's marked as potentially unsafe in a requirements file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_12.txt#2025-04-22_snippet_4\n\nLANGUAGE: Text\nCODE:\n```\nsetuptools==76.0.0 \\\n    --hash=sha256:199466a166ff664970d0ee145839f5582cb9bca7a0a3a2e795b6a9cb2308e9c6 \\\n    --hash=sha256:43b4ee60e10b0d0ee98ad11918e114c70701bc6051662a9a675a0496c1a158f4\n    # via\n    #   -r build/requirements.in\n    #   -r build/test-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Demonstrating NumPy Rank Promotion Example\nDESCRIPTION: Shows how NumPy automatically promotes ranks when adding arrays of different dimensions. Creates a 4x3 array and adds it to a 1D array, demonstrating rank promotion behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/rank_promotion_warning.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nx = np.arange(12).reshape(4, 3)\ny = np.array([0, 1, 0])\nx + y\n```\n\n----------------------------------------\n\nTITLE: Pallas Call Implementation Logic\nDESCRIPTION: Illustrates the semantic implementation of pallas_call, showing how kernels are executed across a grid of indices with input/output transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef pallas_call(kernel, out_shape, *, in_specs, out_specs, grid):\n  def execute(*args):\n    outputs = map(empty_ref, out_shape)\n    grid_indices = map(range, grid)\n    for indices in itertools.product(*grid_indices): # Could run in parallel!\n      local_inputs = [in_spec.transform(arg, indices) for arg, in_spec in\n                      zip(args, in_specs)]\n      local_outputs = [out_spec.transform(arg, indices) for arg, out_spec  in\n                       zip(outputs, out_specs)]\n      kernel(*local_inputs, *local_outputs) # writes to outputs\n  return execute\n```\n\n----------------------------------------\n\nTITLE: Custom Pallas Permute Kernel Implementation\nDESCRIPTION: Implementation of a custom permute kernel in Pallas using async remote copy operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef ppermute_kernel(x_ref, y_ref, send_sem, recv_sem):\n  right_neighbor = ...\n  descriptor = pltpu.make_async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=right_neighbor)\n  descriptor.start()\n  descriptor.wait_send()\n  descriptor.wait_recv()\n\ndef ppermute(x):\n  return pl.pallas_call(ppermute_kernel, out_shape=x, ...)(x)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Side Effects with JAX JIT and Random Number Generation\nDESCRIPTION: Shows how side effects in JAX can lead to unexpected behavior when using jit transformation with random number generation. The example demonstrates how global state and tracers can cause issues with omnistaging enabled.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\nfrom jax import random\n\nkey = random.PRNGKey(0)\n\ndef init():\n  global key\n  key, subkey = random.split(key)\n  return random.normal(subkey, ())\n\nprint(init())  # -1.2515389\nprint(init())  # -0.58665067\n\ninit = jit(init)\nprint(init())  # 0.48648298\nprint(init())  # 0.48648298  !!\n```\n\n----------------------------------------\n\nTITLE: Executing Iota Kernel on TPU using Pallas\nDESCRIPTION: Demonstrates how to call the iota kernel on TPU using pallas_call, specifying TPU-specific memory space requirements.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental.pallas import tpu as pltpu\n\ndef iota(size: int):\n  return pl.pallas_call(iota_kernel,\n                        out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM),\n                        out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),\n                        grid=(size,))()\niota(8)\n```\n\n----------------------------------------\n\nTITLE: Inspecting JAX Array Devices and Sharding\nDESCRIPTION: Shows how to inspect the devices and sharding of a JAX array using the devices() method and sharding attribute.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/key-concepts.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx.devices()\n```\n\nLANGUAGE: python\nCODE:\n```\nx.sharding\n```\n\n----------------------------------------\n\nTITLE: Specifying Dimension Divisibility for Reshape\nDESCRIPTION: Demonstrates how to specify that a symbolic dimension is a multiple of a specific value to enable division operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: jnp.reshape(x, (2, -1)),\n               polymorphic_shapes=[\"(2*b, ...)\"])(np.ones((4, 5, 7)))\n```\n\n----------------------------------------\n\nTITLE: Accurate Benchmarking by Forcing Wait for Computation Completion\nDESCRIPTION: This snippet demonstrates two methods to get accurate timing measurements for JAX operations: converting the result to a NumPy array, which forces a wait for completion, or using the block_until_ready() method to explicitly wait for the computation to finish without transferring data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/async_dispatch.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%time np.asarray(jnp.dot(x, x))\n%time jnp.dot(x, x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Manual Batching of Convolution in Python\nDESCRIPTION: Implements a naive batching approach by looping over the input batches in Python. This method is correct but not efficient for large batches.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/automatic-vectorization.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef manually_batched_convolve(xs, ws):\n  output = []\n  for i in range(xs.shape[0]):\n    output.append(convolve(xs[i], ws[i]))\n  return jnp.stack(output)\n\nmanually_batched_convolve(xs, ws)\n```\n\n----------------------------------------\n\nTITLE: Non-Compilable TensorFlow Function Example\nDESCRIPTION: Demonstrates limitations when using non-XLA-compilable TensorFlow functions with call_tf.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndef f_tf_non_compilable(x):\n   return tf.strings.length(tf.strings.format(\"Hello {}!\", [x]))\n\nf_jax = jax2tf.call_tf(f_tf_non_compilable)\n# Works in op-by-op mode\nf_jax(np.float32(42.))\n\n# Fails in jit mode\njax.jit(f_jax)(np.float(42.))\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Parameters and Data in JAX Python\nDESCRIPTION: This snippet initializes sample data for the neural network model defined previously. It creates weight matrices `W1`, `W2`, `W3` as identity matrices, groups them into `params`, and defines input `x` and target `y` vectors of ones.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto PGLE in Python with JAX Config\nDESCRIPTION: Shows how to enable Profile Guided Latency Estimator directly in Python code using JAX configuration context managers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax._src import config\n\nwith config.enable_pgle(True), config.pgle_profiling_runs(1):\n  # Run with the profiler collecting performance information.\n  train_step()\n  # Automatically re-compile with PGLE profile results\n  train_step()\n  ...\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Type Annotation Stripping in JAX Transformations\nDESCRIPTION: Example showing how JAX transformations strip type annotations from decorated functions, with static type checking implications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/12049-type-annotations.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x: ArrayAnnotation) -> ArrayAnnotation:\n  assert isinstance(x, core.Tracer)\n  return x\n```\n\n----------------------------------------\n\nTITLE: Illustrating Non-Lattice DAGs with NetworkX in Python\nDESCRIPTION: This code uses NetworkX and Matplotlib to draw two examples of directed acyclic graphs (DAGs) that do not satisfy the properties of a lattice. The first DAG shows nodes with no common upper bound, and the second shows nodes where the least upper bound is not unique, illustrating key concepts for lattice-based type promotion. Requires NetworkX and Matplotlib.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#@title\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 2))\n\nlattice = {'A': ['B', 'C']}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {'A': [0, 0], 'B': [1, 0.5], 'C': [1, -0.5]}\nnx.draw(graph, with_labels=True, node_size=2000, node_color='lightgray', pos=pos, ax=ax[0], arrowsize=20)\nax[0].set(xlim=[-0.5, 1.5], ylim=[-1, 1])\n\nlattice = {'A': ['C', 'D'], 'B': ['C', 'D']}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {'A': [0, 0.5], 'B': [0, -0.5], 'C': [1, 0.5], 'D': [1, -0.5]}\nnx.draw(graph, with_labels=True, node_size=2000, node_color='lightgray', pos=pos, ax=ax[1], arrowsize=20)\nax[1].set(xlim=[-0.5, 1.5], ylim=[-1, 1]);\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Stateful Counter Class in Python\nDESCRIPTION: Defines a basic Python class `Counter` with internal state `n`. The `count` method increments this state as a side effect and returns the new value, while `reset` sets the state back to zero. This demonstrates a typical object-oriented approach with mutable state, which is problematic for JAX transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/stateful-computations.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\nclass Counter:\n  \"\"\"A simple counter.\"\"\"\n\n  def __init__(self):\n    self.n = 0\n\n  def count(self) -> int:\n    \"\"\"Increments the counter and returns the new value.\"\"\"\n    self.n += 1\n    return self.n\n\n  def reset(self):\n    \"\"\"Resets the counter to zero.\"\"\"\n    self.n = 0\n\n\ncounter = Counter()\n\nfor _ in range(3):\n  print(counter.count())\n```\n\n----------------------------------------\n\nTITLE: Implementing Serial and Parallel Combinators with PRNG in Python\nDESCRIPTION: Demonstrates the implementation of serial and parallel combinators that handle the PRNG splitting across multiple layers. These combinators are essential for building complex neural network architectures.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef serial(*layers):\n  init_funs, apply_funs = zip(*layers)\n  def init_fun(input_shape):\n    ...\n  def apply_fun(rng, params, inputs):\n    rngs = split(rng, len(layers))\n    for rng, param, apply_fun in zip(rngs, params, apply_funs):\n      inputs = apply_fun(rng, param, inputs)\n    return inputs\n  return init_fun, apply_fun\n\ndef parallel(*layers):\n  init_funs, apply_funs = zip(*layers)\n  def init_fun(input_shape):\n    ...\n  def apply_fun(rng, params, inputs):\n    rngs = split(rng, len(layers))\n    return [f(r, p, x) for f, r, p, x in zip(apply_funs, rngs, params, inputs)]\n  return init_fun, apply_fun\n```\n\n----------------------------------------\n\nTITLE: Using jax2tf.dtype_of_val for Consistent Dtype Handling\nDESCRIPTION: This snippet shows how to use jax2tf.dtype_of_val to create TensorFlow Variables or TensorSpecs with consistent dtypes, regardless of the JAX_ENABLE_X64 setting.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# The following two calls will lower jax_fun at the same dtypes\n# independently of the value of JAX_ENABLE_X64.\njax2tf.convert(jax_fun)(3.14)\njax2tf.convert(jax_fun)(tf.Variable(3.14, dtype=jax2tf.dtype_of_val(3.14)))\n```\n\n----------------------------------------\n\nTITLE: Handling None Prefixes in jax.tree.map - Python\nDESCRIPTION: This code snippet describes how to preserve previous behavior when using 'jax.tree.map' with None prefixes, which are no longer supported without an explicit is_leaf function. The lambda function returns None if the 'x' parameter is None, else applies 'f' to 'x' and 'y'. You must provide 'is_leaf=lambda x: x is None' to treat None as a leaf. Inputs are typically two trees 'a' and 'b', and the output matches the structure of 'a' and 'b' with 'None' where appropriate. The constraint is that now you must handle 'None' explicitly as a leaf.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\njax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)\n```\n\n----------------------------------------\n\nTITLE: Defining PrefetchScalarGridSpec and Kernel API for Scalar Prefetch (Pallas, Python)\nDESCRIPTION: This documentation code describes the PrefetchScalarGridSpec API for Pallas's scalar prefetch feature on TPUs. It defines the interface for constructing grid specs that incorporate scalar prefetch into index maps and kernels, explaining the order and passing of prefetch, input, output, and scratch references. This is API-level context for developers, not meant for direct execution. Dependencies are Pallas (pltpu.PrefetchScalarGridSpec) and JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass PrefetchScalarGridSpec:\\n  def __init__(self,\\n    num_scalar_prefetch: int,\\n    grid: tuple[int, ...],\\n    in_specs: PyTree[BlockSpec],\\n    out_specs: PyTree[BlockSpec],\\n    scratch_shapes: tuple[MemorySpace, ...]):\\n      ...\n```\n\nLANGUAGE: python\nCODE:\n```\ndef index_map(*grid_indices, *prefetch_refs):\\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\ndef kernel(*prefetch_refs, *input_refs, *output_refs, *scratch_refs):\\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\nkernel = pl.pallas_call(...)\\nresult = kernel(*prefetch_args, *input_args)\n```\n\n----------------------------------------\n\nTITLE: Implementing Functionalized Assertions with Checkify in Pallas\nDESCRIPTION: Example showing how to use checkify to implement assertions that throw Python errors rather than halting the TPU. This approach allows for catching errors and continuing execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/g3doc/debugging.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import checkify\n\ndef kernel(...):\n  checkify.check(x > y, \"Check x > y failed\")  # Will throw an error if x <= y\n\nkernel = pl.pallas_call(...)\ncheckified_kernel = checkify.checkify(kernel,\n  errors=checkify.all_checks)\nerror, result = checkified_kernel(x)\nerror.throw()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building JAX C++/CUDA Extension Modules - CMake\nDESCRIPTION: This CMake script sets up the project to compile native C++ extension modules for JAX using nanobind, with optional support for building additional CUDA-enabled modules. It locates the required Python and nanobind installations, dynamically discovers CPU and CUDA extension projects, configures include directories, and defines build/install steps. Prerequisites include Python 3.10+, nanobind, and (optionally) CUDA Toolkit. The script expects specific source files for CPU and CUDA examples, and handles target properties for position-independent code and correct linkage.\nSOURCE: https://github.com/jax-ml/jax/blob/main/examples/ffi/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15...3.30)\nproject(${SKBUILD_PROJECT_NAME} LANGUAGES CXX)\n\noption(JAX_FFI_EXAMPLE_ENABLE_CUDA \"Enable CUDA support\" OFF)\n\nfind_package(Python 3.10 REQUIRED COMPONENTS Interpreter Development.Module)\nexecute_process(\n  COMMAND \"${Python_EXECUTABLE}\"\n          \"-c\" \"from jax.extend import ffi; print(ffi.include_dir())\"\n  OUTPUT_STRIP_TRAILING_WHITESPACE OUTPUT_VARIABLE XLA_DIR)\nmessage(STATUS \"XLA include directory: ${XLA_DIR}\")\n\nfind_package(nanobind CONFIG REQUIRED)\n\nset(\n  JAX_FFI_EXAMPLE_CPU_PROJECTS\n  \"rms_norm\"\n  \"cpu_examples\"\n)\n\nforeach(PROJECT ${JAX_FFI_EXAMPLE_CPU_PROJECTS})\n  nanobind_add_module(\"_${PROJECT}\" NB_STATIC \"src/jax_ffi_example/${PROJECT}.cc\")\n  target_include_directories(\"_${PROJECT}\" PUBLIC ${XLA_DIR})\n  install(TARGETS \"_${PROJECT}\" LIBRARY DESTINATION ${SKBUILD_PROJECT_NAME})\nendforeach()\n\nif(JAX_FFI_EXAMPLE_ENABLE_CUDA)\n  enable_language(CUDA)\n  find_package(CUDAToolkit REQUIRED)\n\n  add_library(_cuda_examples SHARED \"src/jax_ffi_example/cuda_examples.cu\")\n  set_target_properties(_cuda_examples PROPERTIES POSITION_INDEPENDENT_CODE ON\n                                                  CUDA_STANDARD 17)\n  target_include_directories(_cuda_examples PUBLIC ${XLA_DIR})\n  install(TARGETS _cuda_examples LIBRARY DESTINATION ${SKBUILD_PROJECT_NAME})\n\n  nanobind_add_module(_gpu_examples NB_STATIC \"src/jax_ffi_example/gpu_examples.cc\")\n  target_include_directories(_gpu_examples PUBLIC ${XLA_DIR})\n  target_link_libraries(_gpu_examples PRIVATE CUDA::cudart)\n  install(TARGETS _gpu_examples LIBRARY DESTINATION ${SKBUILD_PROJECT_NAME})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining CPU and CUDA Foreign Function Interfaces for RmsNorm (C++)\nDESCRIPTION: These snippets illustrate the expected C++ FFI signatures for custom operators in JAX/XLA, first for CPU and then for CUDA. The CPU signature does not include a stream argument, whereas the CUDA version introduces a cudaStream_t for kernel launch coordination. The handler macro demonstrates correct binding of attributes and buffers for use in JAX/XLA FFI calls. Dependencies include JAX’s FFI API and supporting implementations/libraries for RmsNorm functionality. Inputs and outputs are strictly typed buffers. These C++ definitions must be compiled and linked into the Python runtime to be available for registration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: c++\nCODE:\n```\nffi::Error RmsNormImpl(float eps, ffi::Buffer<ffi::F32> x,\n                       ffi::ResultBuffer<ffi::F32> y)\n```\n\nLANGUAGE: c++\nCODE:\n```\nffi::Error RmsNormImpl(cudaStream_t stream, float eps,\n                       ffi::Buffer<ffi::F32> x,\n                       ffi::ResultBuffer<ffi::F32> y)\n```\n\nLANGUAGE: c++\nCODE:\n```\nXLA_FFI_DEFINE_HANDLER(\n    RmsNorm, RmsNormImpl,\n    ffi::Ffi::Bind()\n        .Ctx<ffi::PlatformStream<cudaStream_t>>()\n        .Attr<float>(\"eps\")\n        .Arg<ffi::Buffer<ffi::F32>>()  // x\n        .Ret<ffi::Buffer<ffi::F32>>()  // y\n);\n```\n\n----------------------------------------\n\nTITLE: Using Implicit Constraints with jax2tf.convert\nDESCRIPTION: Example showing how to use implicit constraints by adding a constant to a dimension variable to ensure it meets minimum size requirements for slicing operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: x[0:16],\n               polymorphic_shapes=\"b + 15, ...\")\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Dependencies\nDESCRIPTION: Imports required JAX modules and function transformations like jit, grad, vmap and random number generation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, grad, vmap\nfrom jax import random\n```\n\n----------------------------------------\n\nTITLE: Configuring Traceback Display in Jupyter - IPython Magic - Python\nDESCRIPTION: This snippet uses the IPython magic command %xmode to set a minimal traceback display mode, making error outputs more concise in Jupyter notebooks. It must be run in an interactive Jupyter/IPython session and does not function in standard Python scripts. This improves readability of error messages but may hide detailed debugging information.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# This ensures that code cell tracebacks appearing below will be concise.\\n%xmode minimal\n```\n\n----------------------------------------\n\nTITLE: Defining Modified ppermute_start Pallas Kernel and Wrapper in Python/JAX\nDESCRIPTION: Defines the `ppermute_start_kernel` for TPU using JAX Pallas (`pltpu`) and a wrapper function `ppermute_start`. The kernel initiates an asynchronous remote copy to a neighboring device. Crucially, the `pallas_call` in the wrapper uses `input_output_aliases={0:2}` to alias the input `x` to the third output, returning `x` alongside semaphores and the output buffer. This aliasing helps extend the lifetime of `x` by making it an output dependency. Dependencies: `jax`, `jax.lax`, `jax.pallas as pl`, `jax.experimental.pallas.tpu as pltpu`, `functools`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef ppermute_start_kernel(\n    in_ref, send_sem, recv_sem, out_ref, _, *, axis_name,\n):\n  axis_size = jax.lax.psum(1, axis_name)\n  left_neighbor = jax.lax.rem(\n      jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size\n  )\n  right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n  barrier_sem = pltpu.get_barrier_semaphore()\n  pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n  pltpu.semaphore_wait(barrier_sem, 1)\n  pltpu.make_async_remote_copy(\n      in_ref, out_ref, send_sem, recv_sem, device_id=right_neighbor\n  ).start()\n\ndef ppermute_start(x, *, axis_name) -> tuple[Semaphore, Semaphore, Array, Array]:\n  send_sem, recv_sem, x, out = pl.pallas_call(\n      functools.partial(ppermute_start_kernel, axis_name=axis_name),\n      out_shape=(\n          pltpu.SemaphoreType.DMA(()),\n          pltpu.SemaphoreType.DMA(()),\n          jax.ShapeDtypeStruct(\n              x.shape,\n              dtype=x.dtype,\n          ),\n\t   jax.ShapeDtypeStruct(\n              x.shape,\n              dtype=x.dtype,\n          ),\n      ),\n      in_specs=[\n          pl.BlockSpec(memory_space=pltpu.ANY),\n      ],\n      out_specs=(\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.ANY),\n          pl.BlockSpec(memory_space=pltpu.ANY),\n      ),\n      input_output_aliases={0:2}\n  )(x)\n  return send_sem, recv_sem, x, out\n```\n```\n\n----------------------------------------\n\nTITLE: Documenting Image Manipulation API with reStructuredText in Python Projects\nDESCRIPTION: This code snippet defines a Sphinx/reStructuredText documentation page for the `jax.image` module. It organizes the module contents by listing key image manipulation functions (like `resize` and `scale_and_translate`) using the `autosummary` directive for auto-generating summaries and documentation structure, and uses `automodule` and `autoclass` directives to include full documentation for the module and its argument classes. Dependencies include Sphinx and its Napoleon/auto-doc extensions. Inputs are function/class identifiers and Sphinx directives; outputs are well-structured documentation files as part of a Python project's API reference.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.image.rst#2025-04-22_snippet_0\n\nLANGUAGE: reST\nCODE:\n```\n``jax.image`` module\n====================\n\n.. currentmodule:: jax.image\n\n.. automodule:: jax.image\n\n\nImage manipulation functions\n----------------------------\n\n.. autosummary::\n  :toctree: _autosummary\n\n    resize\n    scale_and_translate\n\nArgument classes\n----------------\n\n.. currentmodule:: jax.image\n\n.. autoclass:: ResizeMethod\n```\n\n----------------------------------------\n\nTITLE: Specifying Explicit Inequality Constraints for Symbolic Shapes in JAX Export\nDESCRIPTION: Demonstrates how to provide explicit constraints (e.g., 'a >= b', 'b >= 16') when creating symbolic shapes using `jax.export.symbolic_shape`. These constraints help JAX's reasoning during export, allowing it to verify shape-dependent operations like slicing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> # Introduce dimension variable with constraints.\n>>> a, b = export.symbolic_shape(\"a, b\",\n...                              constraints=(\"a >= b\", \"b >= 16\"))\n>>> _ = export.export(jax.jit(lambda x: x[:x.shape[1], :16]))(\n...    jax.ShapeDtypeStruct((a, b), dtype=np.int32))\n\n```\n\n----------------------------------------\n\nTITLE: Running CI Docker Container for Linux/Windows Users\nDESCRIPTION: Command to start a Docker container named \"jax\" that will be used to run CI scripts. This container sets up the necessary environment for building and testing JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/ci/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./ci/utilities/run_docker_container.sh\n```\n\n----------------------------------------\n\nTITLE: Applying JAX Vmap Inside a Pallas Kernel (Illustrative)\nDESCRIPTION: Illustrates a potential use case of JAX transformations inside a Pallas kernel, specifically `jax.vmap` applied to `lax.dynamic_slice`. The feasibility depends on whether the resulting operation (potentially a gather) can be successfully lowered.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\njax.vmap(lax.dynamic_slice)\n```\n\n----------------------------------------\n\nTITLE: Converting JAX Function with Multiple Polymorphic Dimensions\nDESCRIPTION: Demonstrates how to convert a JAX function with multiple unknown dimensions to a shape-polymorphic TensorFlow function, maintaining dimension relationships.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nf_tf = tf.function(jax2tf.convert(f_jax, polymorphic_shapes=[\"(b, d, d)\"]), autograph=False)\nf_tf.get_concrete_function(tf.TensorSpec([None, None, None], tf.float32))\n```\n\n----------------------------------------\n\nTITLE: Replacing Deprecated `PRNGKeyArray.unsafe_raw_array` in JAX Random (Python)\nDESCRIPTION: Illustrates the replacement for the removed `PRNGKeyArray.unsafe_raw_array` method, as noted in JAX 0.4.24 deprecations. Users should now use the `jax.random.key_data` function to access the underlying data of a JAX PRNG key object.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Old, removed method (example usage):\n# raw_data = key.unsafe_raw_array\n\n# New, recommended function:\n# Assumes 'key' is a PRNGKeyArray object\nraw_data = jax.random.key_data(key)\n```\n\n----------------------------------------\n\nTITLE: Documenting Members with Sphinx - reStructuredText\nDESCRIPTION: This snippet utilizes Sphinx reStructuredText syntax to declare and document the jax.typing module. It configures Sphinx to automatically generate summaries of the ArrayLike and DTypeLike type definitions for the API reference, supporting discoverability and structured navigation in documentation builds. Dependencies are Sphinx and the JAX Python library; expected output is generated HTML or other documentation formats.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.typing.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``jax.typing`` module\n=====================\n\n.. automodule:: jax.typing\n\nList of Members\n---------------\n\n.. autosummary::\n  :toctree: _autosummary\n\n    ArrayLike\n    DTypeLike\n```\n\n----------------------------------------\n\nTITLE: Fused Pallas Kernel Implementation\nDESCRIPTION: Example of explicitly fusing add_one operation into the permute kernel for overlapped execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef add_one(x_ref, z_ref):\n  z_ref[...] = x_ref[...] + 1\n\ndef ppermute_add_one_kernel(x_ref, y_ref, z_ref, send_sem, recv_sem):\n  right_neighbor = ...\n  descriptor = pltpu.make_async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=right_neighbor)\n  descriptor.start()\n\n  # Explicitly schedule inner kernel between start/wait\n  pltpu.emit_pipeline(add_one)(x_ref, z_ref)\n\n  descriptor.wait_send()\n  descriptor.wait_recv()\n\ndef ppermute_and_add_one(x):\n  return pl.pallas_call(ppermute_add_one_kernel, out_shape=(x, x), ...)(x)\n```\n\n----------------------------------------\n\nTITLE: JIT-Compiled Conditional Linearization in Python\nDESCRIPTION: Demonstrates linearization of a JIT-compiled conditional function and its evaluation. Shows integration of JIT compilation with conditional operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_92\n\nLANGUAGE: python\nCODE:\n```\n_, f_lin = linearize(jit(lambda x: cond(True, lambda: x, lambda: 0.)), 1.)\nout = f_lin(3.14)\nprint(out)\n```\n\n----------------------------------------\n\nTITLE: Efficient Transpose of shmap with psum and Mapped Output in JAX\nDESCRIPTION: This snippet shows the transpose of `f2` with respect to its first argument (`t(f2, 0)`). It takes sharded inputs `y` and `zbar`, computes `psum(zbar * y, 'i')`, applies `t(g)` to the result, and returns an output sharded along 'i'. In this case, the `psum` inside the transpose is necessary and efficient because `zbar * y` is potentially device-varying.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\n# The transpose we currently get for Example 2 is efficient\nt(f2, 0) = shmap(lambda y, zbar: t(g)(psum(zbar * y, 'i')),\n                in_specs=(P('i'), P('i')), out_specs=P('i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Attempting In-Place Updates with JAX Arrays\nDESCRIPTION: This snippet demonstrates that attempting to perform in-place updates on JAX arrays results in an error, unlike in NumPy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\njax_array = jnp.zeros((3,3), dtype=jnp.float32)\n\n# In place update of JAX's array will yield an error!\njax_array[1, :] = 1.0\n```\n\n----------------------------------------\n\nTITLE: Using Barrier Semaphores in Pallas for TPU\nDESCRIPTION: This code snippet shows how to use barrier semaphores in Pallas for TPU. Barrier semaphores are globally-allocated and used to synchronize devices across an entire program. The example demonstrates how to create and use a barrier semaphore at the beginning of a kernel.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental.pallas import tpu as pltpu\n\ndef example_kernel(...):\n  # Use barrier semaphores at the beginning of a kernel.\n  # is_start_of_kernel = ...\n  # right_neighbor = ...\n  # ...\n  @pl.when(is_start_of_kernel)\n  def _():\n    barrier_sem = pltpu.get_barrier_semaphore()\n    # Increment the semaphore of your right neighbor.\n    pltpu.semaphore_signal(\n          barrier_sem,\n          device_id=right_neighbor,\n          device_id_type=pltpu.DeviceIdType.LOGICAL,\n    )\n    # Wait until your left neighbor has incremented your semaphore\n    pltpu.semaphore_wait(barrier_sem, 1)\n  # ...\n```\n\n----------------------------------------\n\nTITLE: Custom_jvp with multiple non-differentiable arguments\nDESCRIPTION: Extends the previous example to show how to use nondiff_argnums with multiple non-differentiable arguments (functions in this case) placed at different positions in the argument list.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_66\n\nLANGUAGE: python\nCODE:\n```\n@partial(custom_jvp, nondiff_argnums=(0, 2))\ndef app2(f, x, g):\n  return f(g((x)))\n\n@app2.defjvp\ndef app2_jvp(f, g, primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  return f(g(x)), 3. * x_dot\n```\n\n----------------------------------------\n\nTITLE: Illustrating `pure_callback` Execution When Result is Used\nDESCRIPTION: This snippet defines a host function `print_something` with a side effect (printing) and returns a value. It's called via `jax.pure_callback` inside a `jax.jit`-compiled function `f1`. Because the return value of the callback is used as the return value of `f1`, the callback is executed, and the message is printed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef print_something():\n  print('printing something')\n  return np.int32(0)\n\n@jax.jit\ndef f1():\n  return jax.pure_callback(print_something, np.int32(0))\nf1();\n```\n\n----------------------------------------\n\nTITLE: Transposing MMA Operands for TensorCore in JAX (Python)\nDESCRIPTION: Demonstrates how to transpose a matrix operand for the TensorCore MMA operation in JAX. The snippet confirms the operand and accumulator shapes, performs a transpose on the 'a_ref' array with PLGPU utilities, verifies the result has the correct axes for wgmma, and uses it in the TensorCore operation. Requires 'plgpu' and properly-shaped arrays as input.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nassert acc_ref.shape == (M, N) and a_ref.shape == (K, M) and b_ref.shape == (K, N)\\na_ref_t = plgpu.transpose_ref(a_ref, (1, 0))\\nassert a_ref_t.shape == (M, K)  # The shape expected by plgpu.wgmma\\nplgpu.wgmma(acc, a_ref_t, b_ref)\n```\n\n----------------------------------------\n\nTITLE: Checking PRNG Key Dtype Properties in JAX\nDESCRIPTION: Demonstrates how to inspect the dtype properties of a typed PRNG key array, confirming it is both an extended dtype and specifically a PRNG key dtype.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> key = jax.random.key(0)\n>>> jax.dtypes.issubdtype(key.dtype, jax.dtypes.extended)\nTrue\n>>> jax.dtypes.issubdtype(key.dtype, jax.dtypes.prng_key)\nTrue\n```\n\n----------------------------------------\n\nTITLE: Staging Example: Jitting Add with Data-Independent Constant (Python)\nDESCRIPTION: This snippet demonstrates JAX's jit transformation on a function with a constant add operation. It highlights the difference between pre-omnistaging (where add is executed during tracing) and post-omnistaging (where add is staged out to XLA). No external dependencies beyond JAX and its numpy wrapper are required. The return is the product of x and y, where y is 2, and all operations become part of the staged computation after omnistaging.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef f(x):\n  y = jnp.add(1, 1)\n  return x * y\n\nf(3)\n```\n\n----------------------------------------\n\nTITLE: Illustrating `pmap`'s Rank-Reducing Behavior in JAX (Python)\nDESCRIPTION: Provides a conceptual equivalence illustrating the rank-reducing behavior of `pmap`. It shows that applying `pmap` with `in_axes=[0]` and `out_axes=0` is conceptually similar to unstacking the input `xs` along the first axis, applying the function `f` to each resulting slice `x` (which has one less dimension/rank than `xs`), and then stacking the results using `jnp.stack`. This contrasts with `shmap`'s rank-preserving behavior where the function operates on blocks of the same rank as the input. Dependencies include `jax.numpy` and potentially `jax` for `pmap`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npmap(f, in_axes=[0], out_axes=0)(xs) == jnp.stack([f(x) for x in xs])\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Value-Dependent Type Promotion in NumPy\nDESCRIPTION: This example demonstrates NumPy's value-dependent promotion. When a Python scalar value is too large for the array's dtype, the result is promoted to a compatible type that can represent both values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nx = np.zeros(1, dtype='int8')  # int8 array\ny = 1000  # int64 scalar\n(x + y).dtype\n```\n\n----------------------------------------\n\nTITLE: Implementing Device Memory Persistence with Array Class\nDESCRIPTION: Creates a custom Array class to wrap XLA buffers and avoid unnecessary transfers between device and host memory, supporting JAX's transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ndef handle_result(aval: ShapedArray, buf):  # noqa: F811\n  return Array(aval, buf)\n\nclass Array:\n  buf: Any\n  aval: ShapedArray\n\n  def __init__(self, aval, buf):\n    self.aval = aval\n    self.buf = buf\n\n  dtype = property(lambda self: self.aval.dtype)\n  shape = property(lambda self: self.aval.shape)\n  ndim  = property(lambda self: self.aval.ndim)\n\n  def __array__(self): return np.asarray(self.buf)\n  def __repr__(self):  return repr(np.asarray(self.buf))\n  def __str__(self):   return str(np.asarray(self.buf))\n\n  _neg = staticmethod(neg)\n  _add = staticmethod(add)\n  _radd = staticmethod(add)\n  _mul = staticmethod(mul)\n  _rmul = staticmethod(mul)\n  _gt = staticmethod(greater)\n  _lt = staticmethod(less)\ninput_handlers[Array] = lambda x: x.buf\n\njax_types.add(Array)\n```\n\n----------------------------------------\n\nTITLE: Transpose of a JAX Function Using Device Variance Types\nDESCRIPTION: Shows how the transpose of a function is expressed using the new device variance type system, demonstrating that pbroadcast operations become no-ops for efficiency.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Example 1 transpose using device variance types (go ahead and transpose this again!)\nt(f1) = shmap(lambda ybar: t(g)(pbroadcast(ybar, 'i')),\n              in_specs=P(), out_specs=P('i'))\n\n# Example 1 transpose with intermediate device variance types annotated\n@partial(shmap, in_specs=P('i'), out_specs=P())\ndef f1_transpose(ybar: f32[]):\n  wbar:f32[]{i} = pbroadcast(ybar, 'i')\n  xbar:f32[3,4]{i} = transpose(g)(wbar)\n  return xbar\n```\n\n----------------------------------------\n\nTITLE: Computing Per-Example Hessians with Parallel Mapping\nDESCRIPTION: Combines the Hessian computation with parallel mapping to efficiently compute per-example Hessians across multiple devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ninput_hess = hessian(lambda inputs: loss(params, (inputs, targets)))\nper_example_hess = pmap(input_hess)  # pmap!\nper_example_hess(inputs)\n```\n\n----------------------------------------\n\nTITLE: Generating Output Data for Wave Equation Animation\nDESCRIPTION: Runs the wave equation simulation with more frequent outputs to generate data for creating an animation of the wave propagation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%time\n# save more outputs for a movie -- this is slow!\nu_final, _ = multi_step_pmap(\n    (u, v), count=2**15, c=c, dt=0.2, exchange_interval=4, save_interval=2**10)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Perturbation Confusion Avoidance in Python\nDESCRIPTION: This snippet defines a function `f` with a nested function `g` that captures `x`. It then computes the derivative of `f` at 0.0. This example is designed to test if the AD system correctly handles constants originating from different interpreter contexts, preventing 'perturbation confusion'. The correct implementation relies on the `x.interpreter is self` check within `JVPInterpreter.lift`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_8\n\nLANGUAGE: ipython3\nCODE:\n```\ndef f(x):\n  # g is constant in its (ignored) argument `y`. Its derivative should be zero\n  # but our AD will mess it up if we don't distinguish perturbations from\n  # different interpreters.\n  def g(y):\n    return x\n  should_be_zero = derivative(g, 0.0)\n  return mul(x, should_be_zero)\n\nprint(derivative(f, 0.0))\n```\n\n----------------------------------------\n\nTITLE: Implementing Partial Evaluation for JAX\nDESCRIPTION: The partial_eval_flat function takes a function and partially evaluates it with respect to a list of partial values. It returns a jaxpr representing the delayed computation, partial values for outputs, and any constants needed for later evaluation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndef partial_eval_flat(f: Callable, pvals_in: list[PartialVal]\n                      ) -> tuple[Jaxpr, list[PartialVal], list[Any]]:\n  with new_main(PartialEvalTrace) as main:\n    trace = PartialEvalTrace(main)\n    tracers_in = [trace.new_arg(pval) for pval in pvals_in]\n    outs = f(*tracers_in)\n    tracers_out = [full_raise(trace, out) for out in outs]\n    pvals_out = [t.pval for t in tracers_out]\n    unk_tracers_in  = [t for t in tracers_in  if t.pval.is_unknown]\n    unk_tracers_out = [t for t in tracers_out if t.pval.is_unknown]\n    jaxpr, consts = tracers_to_jaxpr(unk_tracers_in, unk_tracers_out)\n  return jaxpr, pvals_out, consts\n```\n\n----------------------------------------\n\nTITLE: Side Effects in Custom VJP Functions\nDESCRIPTION: Demonstrates that side effects in custom VJP functions only execute when the corresponding functions are called. The example shows when the original function and the forward and backward passes are invoked.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n@custom_vjp\ndef f(x):\n  print(\"called f!\")\n  return jnp.sin(x)\n\ndef f_fwd(x):\n  print(\"called f_fwd!\")\n  return f(x), jnp.cos(x)\n\ndef f_bwd(cos_x, y_bar):\n  print(\"called f_bwd!\")\n  return (cos_x * y_bar,)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n----------------------------------------\n\nTITLE: Implementing forward compatibility in JAX lowering rules\nDESCRIPTION: This Python snippet shows how to implement forward compatibility in JAX lowering rules when introducing a new custom call target.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom jax._src import config\nfrom jax._src.lib import version as jaxlib_version\n\ndef my_lowering_rule(ctx: LoweringRuleContext, ...):\n  if ctx.is_forward_compat() or jaxlib_version < (0, 4, 31):\n    # this is the old lowering, using target T, while we\n    # are in forward compatibility mode for T, or we\n    # are in OSS and are using an old jaxlib.\n    return hlo.custom_call(\"T\", ...)\n  else:\n    # This is the new lowering, using target T_NEW, for\n    # when we use a jaxlib with version `>= (0, 4, 31)`\n    # (or when this is internal usage), and also we are\n    # in JIT mode.\n    return hlo.custom_call(\"T_NEW\", ...)\n```\n\n----------------------------------------\n\nTITLE: Implementing Reverse Differentiation in Python\nDESCRIPTION: Example showing how JAX processes JVP computation backwards to perform reverse differentiation, accumulating cotangents for each operation starting from the output and working backward to inputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n  # Initialize cotangents of inputs and intermediate variables:\n  xct = yct = act = bct = cct = 0.\n  # Initialize cotangent of the output:\n  fct = 1.\n  # Process `ft = c + yt`:\n  cct += fct\n  yct += fct\n  # Process `c = a + b`:\n  act += cct\n  bct += cct\n  # Process `b = 2. * yt`:\n  yct += 2. * bct\n  # Process `a = xt * 4.`:\n  xct += act * 4.\n```\n\n----------------------------------------\n\nTITLE: Pallas Permute Done Kernel Implementation\nDESCRIPTION: Implementation of the done portion of a decomposed permute operation, handling semaphore waiting and completion.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef ppermute_done_kernel(ref, send_sem, recv_sem, _):\n  pltpu.make_async_copy(ref, ref, send_sem).wait()\n  pltpu.make_async_copy(ref, ref, recv_sem).wait()\n\ndef ppermute_done(send_sem, recv_sem, out) ->Array:\n  out = pl.pallas_call(\n      ppermute_done_kernel,\n      out_shape=(\n          jax.ShapeDtypeStruct(\n              out.shape,\n              dtype=out.dtype,\n          ),\n      ),\n      in_specs=[\n          pl.BlockSpec(memory_space=pltpu.ANY),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n      ],\n      out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n      input_output_aliases={0:0}\n  )(out, send_sem, recv_sem)\n  return out\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Permutation with Futures in Python\nDESCRIPTION: This snippet demonstrates how to implement an asynchronous permutation operation using futures across loop boundaries. It includes a prologue that starts the permutation before the loop and waits on it at the beginning of each iteration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  fut = ppermute_start(x)\n  def body(i, fut):\n    x = ppermute_done(fut)\n    fut = ppermute_start(x)\n    return fut\n  fut = fori_loop(0, 7, body, fut)\n  return ppermute_done(fut)\n```\n\n----------------------------------------\n\nTITLE: Implementing Function with Non-differentiable Point\nDESCRIPTION: Example function that is not differentiable at zero when considered on the entire real line, demonstrating a case where custom differentiation rules are useful.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return x / (1 + jnp.sqrt(x))\n```\n\n----------------------------------------\n\nTITLE: Referencing JAX implementation of scipy.special.bessel_jn\nDESCRIPTION: Mentions the JAX implementation of bessel_jn function, which has weaknesses in terms of robustness and usage. Suggests considering a new, more robust implementation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/18137-numpy-scipy-scope.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\njax.scipy.special.bessel_jn\n```\n\n----------------------------------------\n\nTITLE: Importing jax.extend as jex in Python\nDESCRIPTION: This snippet demonstrates the recommended approach for importing the `jax.extend` module under the alias `jex`. This form keeps code concise and signals the use of the extendable internals intended for advanced library development. No additional dependencies are required beyond JAX; inputs and outputs are standard Python imports with no runtime computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/15856-jex.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.extend as jex\n```\n\n----------------------------------------\n\nTITLE: Hashable Wrapper for Constants in JAX Compilation Caches - Python\nDESCRIPTION: Defines an IDHashable class to use the Python object's identity as the hashing and equality mechanism, which is useful for caching purposes where physical identity (not semantic equality) is desired. This is essential for making constants hashable in situations where their value alone is insufficient for correct cache semantics, such as when passing constants to lru_cache-decorated functions. Requires the Any type from typing, and expects val to be any Python object.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\nclass IDHashable:\n  val: Any\n\n  def __init__(self, val):\n    self.val = val\n\n  def __hash__(self) -> int:\n    return id(self.val)\n\n  def __eq__(self, other):\n    return type(other) is IDHashable and id(self.val) == id(other.val)\n\n```\n\n----------------------------------------\n\nTITLE: Using `slices_for_invocation` with an Extra Grid Dimension in Python\nDESCRIPTION: Demonstrates calling `slices_for_invocation` with a 3D grid (10, 5, 4), even though the `index_map` only uses the first two invocation indices (`lambda i, j, k: (i, j)`). For invocation indices (2, 4, 0), the result is the same as the previous 2D grid example, as the third grid dimension doesn't affect the slice calculation here.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> # Same shape of the array and blocks, but we iterate over each block 4 times\n>>> slices_for_invocation(x_shape=(100, 100),\n...                       x_spec = pl.BlockSpec((10, 20), lambda i, j, k: (i, j)),\n...                       grid = (10, 5, 4),\n...                       invocation_indices = (2, 4, 0))\n[slice(20, 30, None), slice(80, 100, None)]\n\n```\n\n----------------------------------------\n\nTITLE: Arithmetic Operations with Symbolic Dimensions in JAX\nDESCRIPTION: Shows how symbolic dimensions are automatically converted to JAX arrays when used in arithmetic operations with non-integers, such as floats or JAX arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: (x + x.shape[0] + jnp.sin(x.shape[0]),\n                          5. + x.shape[0],\n                          x.shape[0] - np.ones((5,), dtype=np.int32)),\n               polymorphic_shapes=[\"b\"])(np.ones(3))\n```\n\n----------------------------------------\n\nTITLE: Current Inefficient Transpose of shmap with psum in JAX\nDESCRIPTION: This snippet shows the transpose `t(f1)` that JAX currently generates for the function `f1`. It takes an unmapped input `ybar`, applies a `psum` (with division by the axis size, 8) inside the `shmap` body before applying the transposed function `t(g)`, and produces an output sharded along 'i'. The inner `psum` is inefficient because `ybar` is already device-invariant due to `in_specs=P()`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\n# The transpose we currently get for Example 1 (which is fine to transpose again)\nt(f1) = shmap(lambda ybar: t(g)(psum(ybar / 8, 'i')),\n              in_specs=P(), out_specs=P('i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Inspecting Default Saved Residuals in JAX Python\nDESCRIPTION: This snippet calls an assumed function `print_saved_residuals` (not defined in the text) to inspect the intermediate values (residuals) that JAX saves by default during the forward pass when preparing to differentiate the `loss` function with the given parameters and data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprint_saved_residuals(loss, params, x, y)\n```\n\n----------------------------------------\n\nTITLE: Calculating Matrix Multiplication Arithmetic Intensity - JAX/Numpy - Python\nDESCRIPTION: Implements a helper function to compute arithmetic intensity (FLOPs per byte) for a given matrix multiplication size and dtype, using earlier flops and bandwidth formulas. Dependencies: jax (as jnp), numpy (as np). Parameters: m, k, n (sizes), dtype (type). Returns: floating point ratio of compute to transfer. Helps classify workloads as memory or compute bound. Assumes functions matmul_flops and matmul_membw are defined earlier.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_flops_intensity(m: int, k: int, n: int, dtype: jnp.dtype):\n  flops = matmul_flops(m, k, n)\n  membw = matmul_membw(m, k, n, dtype)\n  return flops / membw\n```\n\n----------------------------------------\n\nTITLE: Analyzing Fused Transpose Matrix Multiplication Performance in Python\nDESCRIPTION: Benchmarks the performance of the fused transpose matrix multiplication implementation, comparing different block sizes and matrix dimensions to evaluate efficiency and utilization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef analyze_matmul(m: int, k: int, n: int, dtype: np.dtype,\n                   mm_func, transpose_rhs: bool = False):\n  x = jnp.ones((m, k), dtype=dtype)\n  if transpose_rhs:\n    y = jnp.ones((n, k), dtype=dtype)\n    @jax.jit\n    def _wrapper(x, y):\n      y = y.swapaxes(0, 1)\n      return mm_func(x, y, transpose_rhs=True)\n  else:\n    y = jnp.ones((k, n), dtype=dtype)\n    _wrapper = mm_func\n  time = benchmark(_wrapper)(x, y)\n  print(f\"----- {m} x {k} x {n} -----\")\n  print(\"Matmul time: \", time)\n  mm_flops = matmul_flops(m, k, n) / time\n  print(\"Matmul FLOP/s: \", mm_flops)\n  print(f\"FLOP/s utilization: {mm_flops / v5e_flops * 100:.4f}%\")\n  print()\n\nprint(\"================bm=128, bk=128, bn=128=\"==================)\nmm = functools.partial(matmul, bm=128, bk=128, bn=128)\nanalyze_matmul(1024, 1024, 1024, jnp.bfloat16, mm, transpose_rhs=True)\nanalyze_matmul(4096, 4096, 4096, jnp.bfloat16, mm, transpose_rhs=True)\nanalyze_matmul(8192, 8192, 8192, jnp.bfloat16, mm, transpose_rhs=True)\n\nprint(\"================bm=512, bk=1024, bn=1024=\"==================)\nmm = functools.partial(matmul, bm=512, bk=1024, bn=1024)\nanalyze_matmul(1024, 1024, 1024, jnp.bfloat16, mm, transpose_rhs=True)\nanalyze_matmul(4096, 4096, 4096, jnp.bfloat16, mm, transpose_rhs=True)\nanalyze_matmul(8192, 8192, 8192, jnp.bfloat16, mm, transpose_rhs=True)\n```\n\n----------------------------------------\n\nTITLE: Basic Shape Polymorphism Example in Python\nDESCRIPTION: Demonstrates how to export a JAX function with symbolic shapes using concatenation. Shows construction of symbolic dimension variables and their usage in shapes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax import export\nfrom jax import numpy as jnp\ndef f(x):  # f: f32[a, b]\n  return jnp.concatenate([x, x], axis=1)\n\n# We construct symbolic dimension variables.\na, b = export.symbolic_shape(\"a, b\")\n\n# We can use the symbolic dimensions to construct shapes.\nx_shape = (a, b)\nx_shape\n\n# Then we export with symbolic shapes:\nexp: export.Exported = export.export(jax.jit(f))(\n    jax.ShapeDtypeStruct(x_shape, jnp.int32))\nexp.in_avals\nexp.out_avals\n\n# We can later call with concrete shapes (with a=3 and b=4), without re-tracing `f`.\nres = exp.call(np.ones((3, 4), dtype=np.int32))\nres.shape\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Side Effects in JAX JIT Compilation\nDESCRIPTION: This snippet illustrates how side effects like print statements are handled differently in JAX's JIT compilation compared to regular Python execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef impure_print_side_effect(x):\n  print(\"Executing function\")  # This is a side-effect\n  return x\n\n# The side-effects appear during the first run\nprint (\"First call: \", jit(impure_print_side_effect)(4.))\n\n# Subsequent runs with parameters of same type and shape may not show the side-effect\n# This is because JAX now invokes a cached compilation of the function\nprint (\"Second call: \", jit(impure_print_side_effect)(5.))\n\n# JAX re-runs the Python function when the type or shape of the argument changes\nprint (\"Third call, different type: \", jit(impure_print_side_effect)(jnp.array([5.])))\n```\n\n----------------------------------------\n\nTITLE: Implementing Fused Right-Hand-Side Transpose Matrix Multiplication in JAX\nDESCRIPTION: Implements a matrix multiplication kernel with fused right-hand-side transpose using JAX and Pallas. This optimizes the computation of x @ y.T by avoiding separate transpose operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef matmul_kernel(x_ref, y_ref, z_ref, acc_ref, *, nsteps, transpose_rhs):\n  @pl.when(pl.program_id(2) == 0)\n  def _():\n    acc_ref[...] = jnp.zeros_like(acc_ref)\n\n  if transpose_rhs:\n    dims = ((1,), (1,)), ((), ())\n  else:\n    dims = ((1,), (0,)), ((), ())\n\n  acc_ref[...] += jax.lax.dot_general(\n      x_ref[...], y_ref[...], dims, preferred_element_type=jnp.float32,\n  )\n\n  @pl.when(pl.program_id(2) == nsteps - 1)\n  def _():\n    z_ref[...] = acc_ref[...].astype(z_ref.dtype)\n\n\n@functools.partial(jax.jit, static_argnames=['bm', 'bk', 'bn', 'transpose_rhs'])\ndef matmul(\n    x: jax.Array,\n    y: jax.Array,\n    *,\n    bm: int = 128,\n    bk: int = 128,\n    bn: int = 128,\n    transpose_rhs: bool = False,\n):\n  if transpose_rhs:\n    y = y.swapaxes(0, 1)\n    y_block_spec = pl.BlockSpec((bn, bk), lambda i, j, k: (j, k))\n  else:\n    y_block_spec = pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))\n  m, k = x.shape\n  _, n = y.shape\n  return pl.pallas_call(\n      functools.partial(matmul_kernel, nsteps=k // bk, transpose_rhs=transpose_rhs),\n      grid_spec=pltpu.PrefetchScalarGridSpec(\n        num_scalar_prefetch=0,\n        in_specs=[\n            pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)),\n            y_block_spec,\n        ],\n        out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n        scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n        grid=(m // bm, n // bn, k // bk),\n      ),\n      out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\n      compiler_params=pltpu.TPUCompilerParams(\n          dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n  )(x, y)\n```\n\n----------------------------------------\n\nTITLE: Defining Duck-Typed Tiling with Flexible Argument Handling in Python\nDESCRIPTION: This Python function replicates numpy.tile's behavior and demonstrates classic duck-typing for its 'reps' parameter. The function tries to convert 'reps' to a tuple; if it fails (e.g., for an int), it wraps it in a singleton tuple. The logic supports both single and multiple repetition values. Dependencies include a compatible Python environment; the snippet itself is self-contained and illustrates the challenge of intent vs. implementation typing. The 'A' and 'reps' inputs accept any types compatible with tiling semantics, and the omitted implementation ('...') would perform the array tiling operation. Output is expected to be a tiled array structure, typically matching numpy semantics.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/12049-type-annotations.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef tile(A, reps):\n  try:\n    tup = tuple(reps)\n  except TypeError:\n    tup = (reps,)\n  d = len(tup)\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Transposition of Cursed Identity with 'P-sum' Solution in JAX\nDESCRIPTION: This snippet shows how the `cursed_identity` function would transpose if the 'P-sum' solution were implemented. The first transpose `t(cursed_identity)` still involves division by the axis size (8) but expresses the sum via `out_specs=P(sum='i')`. Subsequent transposes `t(t(cursed_identity))` remain the same, preventing the unbounded growth of the previous inefficient method, although it doesn't eliminate the division.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n```python\n# How it would transpose with the P-sum partial solution:\nt(cursed_identity) = shmap(lambda x: x / 8, P(), P(sum='i'))\nt(t(cursed_identity)) = shmap(lambda x: x / 8, P(), P(sum='i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Shape Computation Pitfall with jax.numpy in JIT (Python)\nDESCRIPTION: This function illustrates a common mistake when computing shape information inside a jitted function using jax.numpy. The function uses jnp.prod on a JAX array created from the input's shape, then tries to reshape x. With omnistaging, this raises a ConcretizationTypeError because shape values are not available as Python integers in the staged context. Requirements are JAX and jax.numpy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef ex1(x):\n  size = jnp.prod(jnp.array(x.shape))\n  return x.reshape((size,))\n\nex1(jnp.ones((3, 4)))\n```\n\n----------------------------------------\n\nTITLE: JAX JIT Error with Dynamic Shapes\nDESCRIPTION: This snippet shows the error that occurs when trying to use jax.jit with the nansum function due to dynamic shapes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\njax.jit(nansum)(x)\n```\n\n----------------------------------------\n\nTITLE: Creating strongly-typed JAX arrays with explicit dtype\nDESCRIPTION: This example demonstrates how to create a strongly-typed JAX array by explicitly specifying the dtype parameter when converting a Python scalar.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> jnp.asarray(2, dtype='int32')\nArray(2, dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Casting to bfloat16 dtype in JAX\nDESCRIPTION: This snippet demonstrates how to explicitly cast arrays to bfloat16 dtype in JAX, which is useful for optimizing performance on TPUs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\njax.numpy.array(x, dtype=jax.numpy.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Experimental Export Import Path (Python)\nDESCRIPTION: Documents the refactored API import path for `jax.experimental.export` as mentioned in JAX 0.4.24 release notes. The recommended import statement is now `from jax.experimental import export` instead of the older `from jax.experimental.export import export`. The old import path is deprecated but will work for a limited time.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Old, deprecated import:\nfrom jax.experimental.export import export\n\n# New, recommended import:\nfrom jax.experimental import export\n```\n\n----------------------------------------\n\nTITLE: Optimizing Numerical Stability with JIT Compilation in JAX\nDESCRIPTION: This example demonstrates how XLA's optimizations during jit compilation can improve numerical results. The function log(exp(x)) would overflow in normal execution when x=100, but XLA simplifies the expression to just x, avoiding the overflow entirely.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return jnp.log(jnp.exp(x))\nx = 100.0\nprint(f(x))\n\nprint(jit(f)(x))\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel ODE Solutions with Pmap in JAX\nDESCRIPTION: Demonstrates solving multiple ODE solutions in parallel using JAX's pmap. This snippet sets up initial conditions for multiple replicates and solves them concurrently across available devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nN_dev = jax.device_count()\nN = 4000\n\n# set some initial conditions for each replicate\nys = jnp.zeros((N_dev, N, 3))\nstate0 = jr.uniform(jr.key(1), \n                    minval=-1., maxval=1.,\n                    shape=(N_dev, 3))\nstate0 = state0 * jnp.array([18,18,1]) + jnp.array((0.,0.,10.))\nys = ops.index_update(ys, ops.index[:, 0], state0)\n\n# solve each replicate in parallel using `pmap` of rk4 solver:\nys = jax.pmap(rk4)(ys, \n                   0.004 * jnp.ones(N_dev), \n                   N * jnp.ones(N_dev, dtype=np.int32)\n                  ).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Version-based Conditional Code Path in JAX with Python\nDESCRIPTION: This snippet demonstrates how to use the runtime-exposed jaxlib extension version (`jaxlib_extension_version`) to select between code paths for maintaining forward and backward compatibility during development. The dependency is the internal module `jax._src.lib`, which provides the version integer. The snippet shows a conditional branching pattern where new functionality is used if the extension version meets or exceeds a threshold, otherwise legacy logic is executed. Inputs and outputs depend solely on the logic inside the conditional blocks. This approach is limited by its reliance on up-to-date and correctly managed version numbers in both JAX and JAXLIB.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9419-jax-versioning.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom jax._src.lib import jaxlib_extension_version\n\n# 123 is the new version number for _version in xla_client.py\nif jaxlib_extension_version >= 123:\n  # Use new code path\n  ...\nelse:\n  # Use old code path.\n\n```\n\n----------------------------------------\n\nTITLE: Unpacking Futures in Asynchronous Permutation Loop\nDESCRIPTION: This code expands on the previous example by unpacking the future to reveal the underlying operations. It shows how semaphores, input buffer, and target output buffer are threaded as loop carry.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  fut = ppermute_start(x)\n  def body(i, fut):\n    *sems, x, out = fut\n    x = ppermute_done((*sems, x, out))\n    (*sems, x, out) = ppermute_start(x)\n    return (*sems, x, out)\n  (*sems, x, out) = fori_loop(0, 7, body, x)\n  return ppermute_done((*sems, x, out))\n```\n\n----------------------------------------\n\nTITLE: Binding JAX cond Primitive in Python\nDESCRIPTION: Defines a helper function that binds the conditional primitive to inputs. It ensures that the number of arguments matches the number of input binders in both true and false jaxprs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_80\n\nLANGUAGE: python\nCODE:\n```\ndef bind_cond(pred, *args, true_jaxpr, false_jaxpr):\n  assert len(args) == len(true_jaxpr.in_binders) == len(false_jaxpr.in_binders)\n  return bind(cond_p, pred, *args, true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n```\n\n----------------------------------------\n\nTITLE: Testing Recursive Checkpointing with 8 Functions\nDESCRIPTION: Demonstrates the reduced memory usage of recursive checkpointing with 8 sequential sine functions. Shows logarithmic scaling in action with a moderate-sized function chain.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nf = recursive_checkpoint([jnp.sin] * 8)\nprint_saved_residuals(f, 3.)\n```\n\n----------------------------------------\n\nTITLE: Importing Experimental Array API Module (Pre-JAX v0.4.32)\nDESCRIPTION: Shows the import statement previously required to enable Python Array API standard compatibility for JAX arrays before JAX version 0.4.32. This import is no longer necessary from v0.4.32 onwards and will raise a deprecation warning, eventually becoming an error in v0.5.0.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.numpy.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport jax.experimental.array_api\n```\n\n----------------------------------------\n\nTITLE: Defining Jaxpr Data Structures in Python\nDESCRIPTION: Defines the core data structures for representing Jaxpr terms, including Var, Lit, Atom, JaxprEqn, and Jaxpr classes. These structures are used to represent the abstract syntax of JAX programs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass Var:\n  aval: ShapedArray\n  def __init__(self, aval): self.aval = aval\n\nclass Lit:\n  val: Any\n  aval: ShapedArray\n\n  def __init__(self, val):\n    self.aval = aval = raise_to_shaped(get_aval(val))\n    self.val = np.array(val, aval.dtype)\n\nAtom = Union[Var, Lit]\n\nclass JaxprEqn(NamedTuple):\n  primitive: Primitive\n  inputs: list[Atom]\n  params: dict[str, Any]\n  out_binders: list[Var]\n\nclass Jaxpr(NamedTuple):\n  in_binders: list[Var]\n  eqns: list[JaxprEqn]\n  outs: list[Atom]\n\n  def __hash__(self): return id(self)\n  __eq__ = op.is_\n\ndef raise_to_shaped(aval):\n  return ShapedArray(aval.shape, aval.dtype)\n```\n\n----------------------------------------\n\nTITLE: Pallas Permute Start Kernel Implementation\nDESCRIPTION: Implementation of the start portion of a decomposed permute operation, handling semaphores and async copy initialization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef ppermute_start_kernel(\n    in_ref, send_sem, recv_sem, out_ref, *, axis_name,\n):\n  axis_size = jax.lax.psum(1, axis_name)\n  left_neighbor = jax.lax.rem(\n      jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size\n  )\n  right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n  barrier_sem = pltpu.get_barrier_semaphore()\n  pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n  pltpu.semaphore_wait(barrier_sem, 1)\n  pltpu.make_async_remote_copy(\n      in_ref, out_ref, send_sem, recv_sem, device_id=right_neighbor\n  ).start()\n\ndef ppermute_start(x, *, axis_name) -> tuple[Semaphore, Semaphore, Array]:\n  send_sem, recv_sem, out = pl.pallas_call(\n      functools.partial(ppermute_start_kernel, axis_name=axis_name),\n      out_shape=(\n          pltpu.SemaphoreType.DMA(()),\n          pltpu.SemaphoreType.DMA(()),\n          jax.ShapeDtypeStruct(\n              x.shape,\n              dtype=x.dtype,\n          ),\n      ),\n      in_specs=[\n          pl.BlockSpec(memory_space=pltpu.ANY),\n      ],\n      out_specs=(\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.ANY),\n      ),\n  )(x)\n  return send_sem, recv_sem, out\n```\n\n----------------------------------------\n\nTITLE: Sparse Masked Matrix Multiplication Kernel in JAX\nDESCRIPTION: Implements an optimized matrix multiplication kernel that skips computation for masked-out blocks. Uses prefetch maps for improved pipeline performance and handles block-level masking.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef sparse_mask_matmul(\n    block_mask_ref, prefetch_mask, prefetch_i, prefetch_j, # Scalar prefetch inputs.\n    x_ref, y_ref, mask_ref, o_ref,  # Kernel inputs.\n    accum_scratch\n    ):\n  del prefetch_mask, prefetch_i, prefetch_j\n  i, j, k = pl.program_id(0), pl.program_id(1), pl.program_id(2)\n  should_compute = block_mask_ref[i, j] != 0\n  @pl.when(k == 0)\n  def _():\n    o_ref[...] = jnp.zeros_like(o_ref)\n    accum_scratch[...] = jnp.zeros_like(accum_scratch[...])\n\n  # We only compute the output for blocks with non-zero masks.\n  # Otherwise we skip the computation entirely.\n  @pl.when(should_compute)\n  def _():\n    result = jnp.dot(x_ref[...], y_ref[...], preferred_element_type=jnp.float32)\n    accum_scratch[...] += result\n    @pl.when(k == pl.num_programs(2) - 1)\n    def _():\n      o_ref[...] = (mask_ref[0, ...] * accum_scratch[...]).astype(o_ref.dtype)\n```\n\n----------------------------------------\n\nTITLE: Installing JAX with Conda - Bash\nDESCRIPTION: Installs the community-supported JAX package from conda-forge using the conda package manager. This command automatically detects system capabilities, installing a CUDA-enabled jaxlib if an NVIDIA GPU is present. Prerequisite: Conda must be installed on your system. Input is the single command, with no additional parameters required. Output is an up-to-date JAX installation in your current Conda environment. Limited to the set of builds available on conda-forge.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nconda install jax -c conda-forge\n\n```\n\n----------------------------------------\n\nTITLE: Configuring XLA Compiler Flags via Environment Variables in Python - Python\nDESCRIPTION: This snippet demonstrates how to set XLA compiler flags from within a Python script or Jupyter/Colab notebook by assigning a space-separated string of flags to the 'XLA_FLAGS' environment variable using os.environ. The flags should be set before importing Jax or related libraries for correct effect. Inputs are provided as a string, and there is no explicit output; the global process environment is modified. Required dependency: the 'os' standard library.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/xla_flags.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\\n\\n# Set multiple flags separated by spaces\\nos.environ['XLA_FLAGS'] = '--flag1=value1 --flag2=value2'\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies for JAX (Bash)\nDESCRIPTION: Command to install required packages for rebuilding JAX documentation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_49\n\nLANGUAGE: bash\nCODE:\n```\npip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Handling Replicated and Mixed Inputs with pjit under jax.Array in Python\nDESCRIPTION: This code illustrates scenarios with `pjit` and `jax.Array`. The first part shows that fully replicated inputs (e.g., a PRNG key with `P(None)` sharding) do not require the `host_local_array_to_global_array` utility as their shape is already global. The second part demonstrates mixing inputs: a fully replicated input (`key`) is passed directly, while a host-local input (`local_inp`) is converted to a global array using the utility before being passed to `pjit` along with appropriate sharding specifications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkey = jax.random.PRNGKey(1)\n\n# As you can see, using host_local_array_to_global_array is not required since in_axis_resources says\n# that the input is fully replicated via P(None)\npjit(f, in_shardings=None, out_shardings=None)(key)\n\n# Mixing inputs\nglobal_inp = multihost_utils.host_local_array_to_global_array(\n    local_inp, mesh, P('data'))\nglobal_out = pjit(f, in_shardings=(P(None), P('data')),\n                  out_shardings=...)(key, global_inp)\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Error Classes in Python\nDESCRIPTION: This snippet demonstrates how to import and use various error classes from the jax.errors module. These classes represent different types of errors that can occur when using JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/errors.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: jax.errors\n.. autoclass:: ConcretizationTypeError\n.. autoclass:: KeyReuseError\n.. autoclass:: JaxRuntimeError\n.. autoclass:: NonConcreteBooleanIndexError\n.. autoclass:: TracerArrayConversionError\n.. autoclass:: TracerBoolConversionError\n.. autoclass:: TracerIntegerConversionError\n.. autoclass:: UnexpectedTracerError\n```\n\n----------------------------------------\n\nTITLE: Redefining shmap with all_gather (Example 4 Context)\nDESCRIPTION: This snippet repeats the definition of `f4` to provide context for the subsequent `f4_better` example. It shows a `shmap` performing an `all_gather` internally and returning an unmapped output, which leads to inefficient transposes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Example 4 again\nf4 = shmap(lambda x: all_gather(x, 'i'), P('i'), P())\n```\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Reduced Bandwidth for bfloat16 Matrix Multiplication - JAX - Python\nDESCRIPTION: Prints the arithmetic intensity for a larger matrix multiplication using the bfloat16 datatype, showing how this change affects the computation/memory bandwidth ratio. Dependencies: jax (as jnp). Parameters: matrix sizes, dtype. Output demonstrates improved compute bounding for this configuration. No outputs beyond print; relies on matmul_flops_intensity as defined above.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"{matmul_flops_intensity(1024, 1024, 1024, jnp.bfloat16)} flops/byte\")\n```\n\n----------------------------------------\n\nTITLE: Workaround for Unsolvable Dimension Variables in JAX\nDESCRIPTION: Shows a workaround for cases where dimension variables cannot be derived from input shapes by using an array with a specific shape to pass the dimension information.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> def my_top_k_with_dimensions(dimensions, x):  # dimensions: i32[0, k], x: i32[4, 10]\n...   return my_top_k(dimensions.shape[1], x)\n>>> exp = export.export(jax.jit(my_top_k_with_dimensions))(\n...     jax.ShapeDtypeStruct((0, k), dtype=np.int32),\n...     x)\n>>> exp.in_avals\n(ShapedArray(int32[0,k]), ShapedArray(int32[4,10]))\n\n>>> exp.out_avals[0]\nShapedArray(int32[4,k])\n\n>>> # When we invoke `exp` we must construct and pass an array of shape (0, k)\n>>> exp.call(np.zeros((0, 3), dtype=np.int32), x)\nArray([[ 9,  8,  7],\n       [19, 18, 17],\n       [29, 28, 27],\n       [39, 38, 37]], dtype=int32)\n\n```\n\n----------------------------------------\n\nTITLE: Incorrect Key Reuse Pattern in JAX PRNG\nDESCRIPTION: Shows the incorrect pattern of reusing the same PRNG key for multiple random operations, which results in identical random values being generated.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Incorrect\nkey = random.PRNGKey(0)\nx = random.uniform(key, (100,))\ny = random.uniform(key, (100,))  # Identical values!\n```\n\n----------------------------------------\n\nTITLE: Applying Buffer Donation with jax.pmap (Python)\nDESCRIPTION: Illustrates the use of buffer donation for memory optimization with `jax.pmap`. The `donate_argnums=(0, 1)` argument tells JAX that the input buffers corresponding to the positional arguments `params` and `state` are no longer needed after the `update_fn` call and can be reused for storing the function's output. This is useful for in-place-like updates in training loops. Assumes `update_fn`, `params`, and `state` are defined elsewhere. Requires `jax`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n\n# Assume update_fn, params, state are defined\n# params, state = ...\n\nparams, state = jax.pmap(update_fn, donate_argnums=(0, 1))(params, state)\n```\n\n----------------------------------------\n\nTITLE: JAX Error on Python List Input\nDESCRIPTION: This snippet shows JAX's behavior of returning an error when a Python list is passed as input, unlike NumPy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\njnp.sum([1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Advanced JAX All_gather Example with Device Variance\nDESCRIPTION: Shows a more complex example using the device-varying all_gather primitive along with its transpose implementation using psum_scatter.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Example 5 with intermediate device variance types annotated\n@partial(shmap, in_specs=(P('i'), P('i')), out_specs=P('i'))\ndef f5(x:f32[1]{i}, y:f32[8]{i}):\n  z:f32[8]{i} = all_gather(x, 'i')\n  w:f32[8]{i} = z * y\n  return w\n\n# Transpose with respect to first argument\n@partial(shmap, in_specs=(P('i'), P('i')), out_specs=P('i'))\ndef f5_transpose(y:f32[8]{i}, wbar:f32[8]{i}):\n  zbar:f32[8]{i} = wbar * y\n  xbar:f32[1]{i} = psum_scatter(zbar, 'i')\n  return xbar\n```\n\n----------------------------------------\n\nTITLE: Using `None` in `block_shape` to Squeeze Dimensions in Pallas Kernels (Python)\nDESCRIPTION: Defines a simple Pallas kernel and calls it using `pallas_call`. The key feature is `out_specs=pl.BlockSpec((None, 2), ...)`. Using `None` for a dimension in `block_shape` acts like `1` but also squeezes that dimension from the reference passed to the kernel. The `assert o_ref.shape == (2,)` inside the kernel confirms the first dimension (corresponding to `None`) is squeezed. The output array shows the result of the kernel writing program IDs to these squeezed blocks.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> def kernel(o_ref):\n...   assert o_ref.shape == (2,)\n...   o_ref[...] = jnp.full((2,), 10 * pl.program_id(1) + pl.program_id(0))\n>>> pl.pallas_call(kernel,\n...                jax.ShapeDtypeStruct((3, 4), dtype=np.int32),\n...                out_specs=pl.BlockSpec((None, 2), lambda i, j: (i, j)),\n...                grid=(3, 2), interpret=True)()\nArray([[ 0,  0, 10, 10],\n       [ 1,  1, 11, 11],\n       [ 2,  2, 12, 12]], dtype=int32)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Inner Grid and Block Specification for TPU Memory\nDESCRIPTION: Defines the inner grid structure and block specifications for TPU memory operations. This setup determines how data blocks are mapped and stored in TPU memory spaces.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ninner_grid = (\n    outer_block_size[0] // inner_block_size[0] // 2,\n    outer_block_size[1] // inner_block_size[1],\n)\ninner_block_spec = pl.BlockSpec(\n    index_map=lambda i, j: (i, j),\n    block_shape=inner_block_size,\n    memory_space=pltpu.TPUMemorySpace.ANY,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sparse @ Dense Matrix Multiplication Kernel with Pallas\nDESCRIPTION: This snippet defines a kernel for multiplying a sparse matrix with a dense matrix using Pallas. It includes the main kernel function, mapping functions for input and output, and the grid specification for the Pallas call.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef dsd_kernel(idxs_i_ref, idxs_k_ref, # Scalar prefetch inputs.\n               x_ref, y_ref, _, o_ref, # Kernel inputs.\n               accum_scratch,\n               ):\n  \"\"\"A DSD (Dense = Sparse @ Dense) matmul kernel.\"\"\"\n  del idxs_k_ref\n  blk_idx = pl.program_id(1)\n  is_start = blk_idx == 0\n  changed_blocks = (idxs_i_ref[blk_idx] != idxs_i_ref[jnp.maximum(blk_idx-1, 0)])\n  @pl.when(is_start | changed_blocks)\n  def _():\n    accum_scratch[...] = jnp.zeros_like(accum_scratch)\n  accum_scratch[...] += jnp.dot(x_ref[0, :, :], y_ref[...], preferred_element_type=jnp.float32)\n\n  next_block_change = (idxs_i_ref[blk_idx] != idxs_i_ref[jnp.minimum(blk_idx+1, num_blocks)])\n  is_end = blk_idx == (num_blocks - 1)\n  @pl.when(is_end | next_block_change)\n  def _():\n    o_ref[...] = accum_scratch[...].astype(o_ref.dtype)\n\n\ndef x_map(j, blk_idx, blk_idxs_i, blk_idxs_k):\n  del j, blk_idxs_i, blk_idxs_k\n  return (blk_idx, 0, 0)\ndef y_map(j, blk_idx, blk_idxs_i, blk_idxs_k):\n  del blk_idxs_i\n  return (blk_idxs_k[blk_idx], j)\ndef o_map(j, blk_idx, blk_idxs_i, blk_idxs_k):\n  del blk_idxs_k\n  return (blk_idxs_i[blk_idx], j)\n\ngrid_spec = pltpu.PrefetchScalarGridSpec(\n    num_scalar_prefetch=2,\n    # Note that while num_blocks is static here, Pallas does support\n    # dynamic grid sizes.\n    grid=(N // blk_N, num_blocks),\n    in_specs=[pl.BlockSpec((1, blk_M, blk_K), x_map),\n              pl.BlockSpec((blk_K, blk_N), y_map),\n              # Placeholder for a zeros-array used by input_output_aliases.\n              pl.BlockSpec((blk_M, blk_N), o_map),\n              ],\n    out_specs=pl.BlockSpec((blk_M, blk_N), o_map),\n    scratch_shapes=[pltpu.VMEM((blk_M, blk_N), dtype=jnp.float32)]\n)\nkernel = pl.pallas_call(\n  dsd_kernel,\n  grid_spec=grid_spec,\n  out_shape=out_shape,\n  # We use input-output aliases to zero-out o_ref for blocks that we never\n  # visit. By passing in an array of zeros we avoid having o_ref start with\n  # uninitialized values.\n  input_output_aliases={4: 0},  # Map zeros to o_ref.\n)\n```\n\n----------------------------------------\n\nTITLE: Installing JAX Fork for Development in Python\nDESCRIPTION: This snippet shows how to clone the JAX repository, install testing requirements, and install JAX in editable mode for development purposes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/contributing.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR_USERNAME/jax\ncd jax\npip install -r build/test-requirements.txt  # Installs all testing requirements.\npip install -e \".[cpu]\"  # Installs JAX from the current directory in editable mode.\n```\n\n----------------------------------------\n\nTITLE: Pytree Traversal with Unregistered Custom Objects - JAX - Python\nDESCRIPTION: This code defines a class Special and illustrates what happens when instances are traversed as pytrees prior to registration—they are treated as leaves. It creates a list of Special objects and extracts leaves with jax.tree.leaves, showing that the container fields are not traversed. JAX must be imported, and class objects must not yet be registered as pytree nodes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Special(object):\\n  def __init__(self, x, y):\\n    self.x = x\\n    self.y = y\\n\\njax.tree.leaves([\\n    Special(0, 1),\\n    Special(2, 4),\\n])\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for Forward-Mode Autodiff and Primal-Tangent Pair Management - JAX Python\nDESCRIPTION: Defines several helper utilities to facilitate packing/unpacking of argument pairs and mapping functions across lists for autodiff logic. Includes zeros_like (creates an array of zeros matching the input), unzip2 (splits sequences of pairs), and safe implementations of map/zip compatible with JAX tracing. These utilities are vital for broadcasting and managing tangent values during JVP computations. Dependencies include numpy and get_aval.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport builtins\\n\\ndef zeros_like(val):\\n  aval = get_aval(val)\\n  return np.zeros(aval.shape, aval.dtype)\\n\\ndef unzip2(pairs):\\n  lst1, lst2 = [], []\\n  for x1, x2 in pairs:\\n    lst1.append(x1)\\n    lst2.append(x2)\\n  return lst1, lst2\\n\\ndef map(f, *xs):\\n  return list(builtins.map(f, *xs))\\n\\ndef zip(*args):\\n  fst, *rest = args = map(list, args)\\n  n = len(fst)\\n  for arg in rest:\\n    assert len(arg) == n\\n  return list(builtins.zip(*args))\n```\n\n----------------------------------------\n\nTITLE: Using pallas.load and pallas.store in Python Kernels\nDESCRIPTION: This snippet demonstrates the use of pallas.load and pallas.store primitives for more flexible memory access in Pallas kernels, including dynamic slicing and various indexing techniques.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef f(x_ref, o_ref):\n  # Reading from memory via pallas.load\n  x = pl.load(x_ref, (0, slice(2, 5), slice(None)))\n  # Using integer indexing automatically broadcasts\n  x = pl.load(x_ref, (0, 2 + jnp.arange(3), slice(None)))\n  # You can also use `pl.dynamic_slice` (`pl.ds` for short) objects as well\n  pl.store(o_ref, (0, pl.ds(start=2, size=3), slice(None)), x)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Non-Associativity in NumPy Type Promotion in Python\nDESCRIPTION: This snippet highlights a potential issue with NumPy's type promotion system by showing it's not always associative. It adds `int8`, `uint8`, and `float16` scalars in different orders and prints the resulting data types (`dtype`), demonstrating that `(a + b) + c` yields a different type than `a + (b + c)`. It requires the NumPy library.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\na, b, c = np.int8(1), np.uint8(1), np.float16(1)\nprint(np.dtype((a + b) + c))\nprint(np.dtype(a + (b + c)))\n```\n\n----------------------------------------\n\nTITLE: Running ruff Linter in JAX (Bash)\nDESCRIPTION: Commands to install pre-commit and run the ruff linter for code quality checks in the JAX project.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_48\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit run ruff --all-files\n```\n\n----------------------------------------\n\nTITLE: Creating Old-Style PRNG Keys with JAX (Python)\nDESCRIPTION: Demonstrates how to create a traditional, raw-style PRNG key in JAX with jax.random.PRNGKey. The resulting key is a 2-element uint32 array. No additional dependencies except for JAX itself. Inputs: a seed (integer). Outputs: an Array of shape (2,) and dtype uint32. Limitations: this style of key will eventually be deprecated as JAX transitions to typed keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> key = jax.random.PRNGKey(0)\n>>> key\nArray([0, 0], dtype=uint32)\n>>> key.shape\n(2,)\n>>> key.dtype\ndtype('uint32')\n\n```\n\n----------------------------------------\n\nTITLE: Comparing In-Place Addition Behavior in JAX and NumPy\nDESCRIPTION: This example illustrates the difference in behavior between JAX and NumPy when using in-place addition operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\njax_array = jnp.array([10, 20])\njax_array_new = jax_array\njax_array_new += 10\nprint(jax_array_new)  # `jax_array_new` is rebound to a new value [20, 30], but...\nprint(jax_array)      # the original value is unodified as [10, 20] !\n\nnumpy_array = np.array([10, 20])\nnumpy_array_new = numpy_array\nnumpy_array_new += 10\nprint(numpy_array_new)  # `numpy_array_new is numpy_array`, and it was updated\nprint(numpy_array)      # in-place, so both are [20, 30] !\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Rank Promotion with Context Manager\nDESCRIPTION: Shows how to use the JAX context manager to configure rank promotion warnings locally within a specific code block.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/rank_promotion_warning.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith jax.numpy_rank_promotion(\"warn\"):\n  z = x + y\n```\n\n----------------------------------------\n\nTITLE: Testing Custom Differentiation Convention\nDESCRIPTION: Verifying that the custom differentiation rule produces the desired value (1.0) at the boundary point zero.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(0.))\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Constraints to Overcome JAX Symbolic Reasoning Limits\nDESCRIPTION: Provides a workaround for the `InconclusiveDimensionOperation` shown previously by explicitly adding the problematic inequality ('b >= mod(b, 3)') as a constraint when defining the symbolic variable 'b'. This assures JAX that the condition holds, allowing the export to proceed successfully.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> b, = export.symbolic_shape(\"b\",\n...                            constraints=[\"b >= mod(b, 3)\"])\n>>> f = lambda x: lax.slice_in_dim(x, 0, x.shape[0] % 3)\n>>> _ = export.export(jax.jit(f))(\n...     jax.ShapeDtypeStruct((b,), dtype=np.int32))\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Sparse Access Pattern on Dense Data with Prefetch Maps\nDESCRIPTION: This snippet demonstrates how to implement a sparse access pattern on dense data using prefetch maps. It includes an example of constructing an index map function that uses the prefetch map to determine the next non-zero block to process.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef mask_index_map(prefetch_map, i, j, ...):\n  next_nonzero_block = prefetch_map[i, j]\n  return (next_nonzero_block, 0, 0)\n```\n\n----------------------------------------\n\nTITLE: Slicing a Pallas Ref (Illustrative)\nDESCRIPTION: Illustrates slicing a Pallas `Ref` object named `x_ref`. When lowering slices like `x_ref[4, :]` to Triton, it involves generating an array of pointers corresponding to the elements in the slice (e.g., `5 * 4 + jnp.arange(3)` for a slice of length 3 starting at row 4 of a Ref with 5 columns).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nx_ref[4, :]\n```\n\n----------------------------------------\n\nTITLE: JAX Export Shape Assertions Check Function in MLIR\nDESCRIPTION: Demonstrates the structure of the _check_shape_assertions function, which performs runtime checks on input shapes and dimension variables.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_14\n\nLANGUAGE: mlir\nCODE:\n```\n       func private _check_shape_assertions(arg: f32[?, ?]) {\n         # Check that w is >= 1\n         arg_w = hlo.get_dimension_size(arg, 0)\n         custom_call @shape_assertion(arg_w >= 1, arg_w,\n            error_message=\"Dimension variable 'w' must have integer value >= 1. Found {0}\")\n         # Check that dim1 is even\n         dim1 = hlo.get_dimension_size(arg, 1)\n         custom_call @shape_assertion(dim1 % 2 == 0, dim1 % 2,\n            error_message=\"Division had remainder {0} when computing the value of 'h')\n         # Check that h >= 1\n         arg_h = hlo.floordiv(dim1, 2)\n         custom_call @shape_assertion(arg_h >= 1, arg_h,\n            error_message=\"\"Dimension variable 'h' must have integer value >= 1. Found {0}\")\n```\n\n----------------------------------------\n\nTITLE: Testing JAX with Default Python and Custom Lock File using Bazel\nDESCRIPTION: This command runs a JAX test target using Bazel with the default configured Python interpreter for the specified version (3.13) but uses a custom set of dependencies defined in a specific lock file. The `--repo_env` flags set the `HERMETIC_PYTHON_VERSION` (which implicitly selects the default interpreter for that version) and `HERMETIC_REQUIREMENTS_LOCK` to point to the custom requirements file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nbazel test <target> \\\n  --repo_env=HERMETIC_PYTHON_VERSION=3.13 \\\n  --repo_env=HERMETIC_REQUIREMENTS_LOCK=\"/absolute/path/to/build:custom_requirements_lock.txt\"\n```\n\n----------------------------------------\n\nTITLE: Migrating from concrete=True to static_argnums in jax.checkpoint - JAX (Python)\nDESCRIPTION: This snippet illustrates the old way of using jax.checkpoint with the deprecated concrete=True option to trace on concrete values, such as for branch selection by static arguments. The decorated function foo takes a data argument x and a static argument is_training, and applies different computation paths depending on the static value. This approach is no longer supported; refer to the updated API example for migration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/11830-new-remat-checkpoint.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@partial(jax.checkpoint, concrete=True)  # OLD jax.checkpoint API\ndef foo(x, is_training):\n  if is_training:\n    return g(x)\n  else:\n    return h(x)\n```\n\n----------------------------------------\n\nTITLE: Incorrect Mixing of Symbolic Scopes\nDESCRIPTION: Example showing an error case where symbolic dimensions from different scopes cannot be mixed in operations, even if they use the same variable names.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\na1, = export.symbolic_shape(\"a,\")\na2, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\n\na1 + a2\n```\n\n----------------------------------------\n\nTITLE: Type Checking and Jaxpr Construction for Conditionals in Python\nDESCRIPTION: Performs type checking on two jaxpr objects and constructs new jaxprs with zero padding for residuals. Handles splitting of output types and creation of zero literals for proper shape matching.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_89\n\nLANGUAGE: python\nCODE:\n```\njaxpr1_type, jaxpr2_type = typecheck_jaxpr(jaxpr1), typecheck_jaxpr(jaxpr2)\nout_types1, _ = split_list(jaxpr1_type.out_types, len(jaxpr1.outs) - n1)\nout_types2, _ = split_list(jaxpr2_type.out_types, len(jaxpr2.outs) - n2)\nassert out_types1 == out_types2\nouts1, res1 = split_list(jaxpr1.outs, len(jaxpr1.outs) - n1)\nouts2, res2 = split_list(jaxpr2.outs, len(jaxpr2.outs) - n2)\nzeros_like1 = [Lit(np.zeros(v.aval.shape, v.aval.dtype)) for v in res1]\nzeros_like2 = [Lit(np.zeros(v.aval.shape, v.aval.dtype)) for v in res2]\nnew_jaxpr1 = Jaxpr(jaxpr1.in_binders, jaxpr1.eqns, outs1 + res1 + zeros_like2)\nnew_jaxpr2 = Jaxpr(jaxpr2.in_binders, jaxpr2.eqns, outs2 + zeros_like1 + res2)\nreturn new_jaxpr1, new_jaxpr2\n```\n\n----------------------------------------\n\nTITLE: Alternative Efficient Implementation using out_specs in JAX\nDESCRIPTION: This snippet presents `f4_better` as a more efficient way to achieve the same result as `f4`. Instead of using `all_gather` inside the `shmap` body, it uses a simple identity function (`lambda x: x`) and specifies the gathering behavior through `out_specs=P('i')`. This version avoids the transpose inefficiency associated with the internal collective.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Why didn't we just write it like this?\nf4_better = shmap(lambda x: x, P('i'), P('i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing JVP Rule for JAX cond Primitive\nDESCRIPTION: Defines the Jacobian-vector product rule for the cond primitive. It applies JVP transformation to both true and false jaxprs, joins their constants, and then applies cond to both primals and tangents.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_82\n\nLANGUAGE: python\nCODE:\n```\ndef cond_jvp_rule(primals, tangents, *, true_jaxpr, false_jaxpr):\n  pred, *primals = primals\n  _   , *tangents = tangents\n  true_jaxpr , true_consts  = jvp_jaxpr(true_jaxpr)\n  false_jaxpr, false_consts = jvp_jaxpr(false_jaxpr)\n  true_jaxpr, false_jaxpr = _join_jaxpr_consts(\n      true_jaxpr, false_jaxpr, len(true_consts), len(false_consts))\n  assert typecheck_jaxpr(true_jaxpr) == typecheck_jaxpr(false_jaxpr)\n  outs = bind_cond(pred, *true_consts, *false_consts, *primals, *tangents,\n                   true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n  primals_out, tangents_out = split_half(outs)\n  return primals_out, tangents_out\njvp_rules[cond_p] = cond_jvp_rule\n```\n\n----------------------------------------\n\nTITLE: Implementing Differentiation Convention with defjvps\nDESCRIPTION: Alternative implementation of the custom differentiation convention using the defjvps convenience wrapper.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@custom_jvp\ndef f(x):\n  return x / (1 + jnp.sqrt(x))\n\nf.defjvps(lambda t, ans, x: ((jnp.sqrt(x) + 2) / (2 * (jnp.sqrt(x) + 1)**2)) * t)\n```\n\n----------------------------------------\n\nTITLE: Implementing Vectorization (vmap) Rule for JAX cond Primitive\nDESCRIPTION: Defines the vectorization rule for cond primitive that handles transforming the jaxprs for batch processing. Currently doesn't support batched predicates, only batched operands.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_83\n\nLANGUAGE: python\nCODE:\n```\ndef cond_vmap_rule(axis_size, vals_in, dims_in, *, true_jaxpr, false_jaxpr):\n  pred    , *vals_in = vals_in\n  pred_dim, *dims_in = dims_in\n  if pred_dim is not not_mapped: raise NotImplementedError  # TODO\n  true_jaxpr, true_consts = vmap_jaxpr(true_jaxpr, axis_size, tuple(dims_in))\n  false_jaxpr, false_consts = vmap_jaxpr(false_jaxpr, axis_size, tuple(dims_in))\n  true_jaxpr, false_jaxpr = _join_jaxpr_consts(\n      true_jaxpr, false_jaxpr, len(true_consts), len(false_consts))\n  assert typecheck_jaxpr(true_jaxpr) == typecheck_jaxpr(false_jaxpr)\n  outs = bind_cond(pred, *true_consts, *false_consts, *vals_in,\n                   true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n  return outs, [0] * len(outs)\nvmap_rules[cond_p] = cond_vmap_rule\n```\n\n----------------------------------------\n\nTITLE: Setting Up TPU Block Sizes and Input Array for Collective Operations in JAX\nDESCRIPTION: Initializes block sizes for TPU memory virtualization and creates a distributed input array with proper sharding across devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# We pick a smaller VMEM block size for the inner kernel.\ninner_block_size = (128, 128)\ninput_arr = jax.random.uniform(\n    jax.random.key(0),\n    shape=(\n        outer_block_size[0] * num_devices,\n        outer_block_size[1] * num_devices,\n    ),\n)\ninput_arr = jax.device_put(input_arr, sharding)\n```\n\n----------------------------------------\n\nTITLE: Explicit Array Conversion for JAX Sum\nDESCRIPTION: This snippet demonstrates the correct way to use a Python list with JAX functions by explicitly converting it to an array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\njnp.sum(jnp.array(x))\n```\n\n----------------------------------------\n\nTITLE: C++ XLA FFI Wrapper for RMS Normalization\nDESCRIPTION: C++ code that wraps the RMS normalization function for XLA FFI. It handles buffer dimensions, processes batch dimensions, and exposes the function to the JAX FFI interface with proper type bindings.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n#include <functional>\n#include <numeric>\n#include <utility>\n\n#include \"xla/ffi/api/c_api.h\"\n#include \"xla/ffi/api/ffi.h\"\n\nnamespace ffi = xla::ffi;\n\n// A helper function for extracting the relevant dimensions from `ffi::Buffer`s.\n// In this example, we treat all leading dimensions as batch dimensions, so this\n// function returns the total number of elements in the buffer, and the size of\n// the last dimension.\ntemplate <ffi::DataType T>\nstd::pair<int64_t, int64_t> GetDims(const ffi::Buffer<T> &buffer) {\n  auto dims = buffer.dimensions();\n  if (dims.size() == 0) {\n    return std::make_pair(0, 0);\n  }\n  return std::make_pair(buffer.element_count(), dims.back());\n}\n\n// A wrapper function providing the interface between the XLA FFI call and our\n// library function `ComputeRmsNorm` above. This function handles the batch\n// dimensions by calling `ComputeRmsNorm` within a loop.\nffi::Error RmsNormImpl(float eps, ffi::Buffer<ffi::F32> x,\n                       ffi::ResultBuffer<ffi::F32> y) {\n  auto [totalSize, lastDim] = GetDims(x);\n  if (lastDim == 0) {\n    return ffi::Error::InvalidArgument(\"RmsNorm input must be an array\");\n  }\n  for (int64_t n = 0; n < totalSize; n += lastDim) {\n    ComputeRmsNorm(eps, lastDim, &(x.typed_data()[n]), &(y->typed_data()[n]));\n  }\n  return ffi::Error::Success();\n}\n\n// Wrap `RmsNormImpl` and specify the interface to XLA. If you need to declare\n// this handler in a header, you can use the `XLA_FFI_DECLARE_HANDLER_SYMBOL`\n// macro: `XLA_FFI_DECLARE_HANDLER_SYMBOL(RmsNorm)`.\nXLA_FFI_DEFINE_HANDLER_SYMBOL(\n    RmsNorm, RmsNormImpl,\n    ffi::Ffi::Bind()\n        .Attr<float>(\"eps\")\n        .Arg<ffi::Buffer<ffi::F32>>()  // x\n        .Ret<ffi::Buffer<ffi::F32>>()  // y\n);\n```\n\n----------------------------------------\n\nTITLE: Parameterizing Block Shapes in JAX Pallas Pipelined Matrix Addition (Python)\nDESCRIPTION: This snippet shows how to parameterize the block shapes (`bm`, `bn`) for a pipelined matrix addition kernel using JAX Pallas. The function `add_matrices_pipelined_param` allows tuning performance by adjusting block sizes, which controls the granularity of the pipelined execution. It calls `pl.pallas_call` with dynamically calculated grid dimensions and block specifications based on the input parameters, using a predefined `add_matrices_kernel` (not shown in this snippet, but assumed to perform element-wise addition). The code includes tests verifying correctness for different block sizes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef add_matrices_pipelined_param(\n    x: jax.Array, y: jax.Array, *, bm: int = 256, bn: int = 256\n) -> jax.Array:\n  m, n = x.shape\n  block_spec = pl.BlockSpec((bm, bn), lambda i, j: (i, j))\n  return pl.pallas_call(\n      add_matrices_kernel,  # Assumes add_matrices_kernel is defined elsewhere\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(m // bm, n // bn),\n  )(x, y)\n\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_param(x, y, bm=256, bn=256), x + y\n)\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_param(x, y, bm=128, bn=128), x + y\n)\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_param(x, y, bm=512, bn=512), x + y\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating `jax.grad` Incompatibility with `jax.pure_callback`\nDESCRIPTION: This code attempts to compute the gradient of the function `f` (which uses `jax.pure_callback`) using `jax.grad`. This results in an error because `jax.pure_callback` does not have defined autodifferentiation rules by default. The surrounding text mentions `custom_jvp` can be used to make it compatible.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\njax.grad(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Generating List of Common jax.numpy Functions in Python\nDESCRIPTION: This Python script dynamically generates a list of functions available in both `jax.numpy` and standard `numpy`. It uses `set` operations on the results of `dir()` to find common members and filters for callable objects. This is primarily used for generating documentation content (`doctest: +SKIP`) rather than typical runtime usage. Requires `jax.numpy` and `numpy` to be imported.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.numpy.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> import jax.numpy, numpy\n>>> fns = set(dir(numpy)) & set(dir(jax.numpy))\n>>> print('\\n'.join('    ' + x for x in fns if callable(getattr(jax.numpy, x))))  # doctest: +SKIP\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Equality Constraints as Rewrite Rules in JAX\nDESCRIPTION: Illustrates defining explicit equality constraints (e.g., 'a * b == c + d') for symbolic dimensions using `jax.export.symbolic_shape`. JAX treats these constraints as rewrite rules, substituting occurrences of the left-hand side expression with the right-hand side during symbolic evaluation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> # Introduce dimension variable with equality constraints.\n>>> a, b, c, d = export.symbolic_shape(\"a, b, c, d\",\n...                                    constraints=(\"a * b == c + d\",))\n>>> 2 * b * a\n2*d + 2*c\n\n>>> a * b * b\nb*d + b*c\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Loop Function for JIT\nDESCRIPTION: Creates a function with a dynamic while loop that will be used to demonstrate JIT compilation constraints with dynamic loops.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, n):\n  i = 0\n  while i < n:\n    x = x * x\n    i += 1\n  return x\n\ng = jit(f)\n```\n\n----------------------------------------\n\nTITLE: Running All JAX Tests with Pytest in Parallel\nDESCRIPTION: This command executes all tests within the `tests` directory using `pytest`. The `-n auto` flag utilizes the `pytest-xdist` plugin (assumed installed) to automatically detect the number of available CPU cores and run tests in parallel across them. This requires test dependencies listed in `build/test-requirements.txt` to be installed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_39\n\nLANGUAGE: shell\nCODE:\n```\npytest -n auto tests\n```\n\n----------------------------------------\n\nTITLE: Executing Multi-process Parallel Computation with JAX\nDESCRIPTION: Demonstrates running a parallel computation across multiple processes using pmap with collective operations. Shows device counting and performing a parallel sum operation across all devices in the cluster.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/multi_process.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> jax.distributed.initialize()  # On GPU, see above for the necessary arguments.\n>>> jax.device_count()  # total number of accelerator devices in the cluster\n32\n>>> jax.local_device_count()  # number of accelerator devices attached to this host\n8\n# The psum is performed over all mapped devices across the pod slice\n>>> xs = jax.numpy.ones(jax.local_device_count())\n>>> jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(xs)\nShardedDeviceArray([32., 32., 32., 32., 32., 32., 32., 32.], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Sequencing Side-Effects within JAX JIT using Tokens\nDESCRIPTION: Rewrites a JIT-compiled function `f` to accept and return a `token`. The `jax.print` function is modified (hypothetically) to also take and return the token. By threading the token through the print calls, an explicit data dependency is created, forcing the \"hello\" print to execute before the \"world\" print.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(token, x):\n  token = jax.print(token, \"hello\")\n  token = jax.print(token, \"world\")\n  return token, x\n```\n\n----------------------------------------\n\nTITLE: Masked Loading with pallas.load in Python\nDESCRIPTION: This snippet shows how to use masking with pallas.load for conditional memory access in Pallas kernels, which is important for handling out-of-bounds loads and stores.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef f(x_ref, o_ref):\n  # Reading from memory via pallas.load\n  idx = jnp.arange(8)\n  mask = idx < 5\n  x = pl.load(x_ref, (idx,), mask=mask, other=float('-inf'))\n```\n\n----------------------------------------\n\nTITLE: Implementing Jaxpr Evaluation in Python\nDESCRIPTION: Defines functions for evaluating Jaxpr structures, including eval_jaxpr and jaxpr_as_fun. These functions interpret the Jaxpr representation to execute the corresponding computations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef eval_jaxpr(jaxpr: Jaxpr, args: list[Any]) -> list[Any]:\n  env: dict[Var, Any] = {}\n\n  def read(x: Atom) -> Any:\n    return env[x] if type(x) is Var else x.val\n\n  def write(v: Var, val: Any) -> None:\n    assert v not in env  # single-assignment\n    env[v] = val\n\n  map(write, jaxpr.in_binders, args)\n  for eqn in jaxpr.eqns:\n    in_vals = map(read, eqn.inputs)\n    outs = bind(eqn.primitive, *in_vals, **eqn.params)\n    map(write, eqn.out_binders, outs)\n  return map(read, jaxpr.outs)\n\ndef jaxpr_as_fun(jaxpr: Jaxpr):\n  return lambda *args: eval_jaxpr(jaxpr, args)\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Bazel Build for CUDA GPU Tests\nDESCRIPTION: This command uses the JAX build script (`build/build.py`) to configure the Bazel build environment for running tests on CUDA-enabled GPUs. The `--wheels=jax-cuda-plugin` flag specifies the CUDA plugin wheel, and `--configure_only` stops the script after configuration. This sets up the Bazel workspace for running GPU-specific tests.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\npython build/build.py build --wheels=jax-cuda-plugin --configure_only\n```\n\n----------------------------------------\n\nTITLE: Generating Python Built-in Type Promotion Table with Pandas in Python\nDESCRIPTION: This code generates a type promotion table for Python's fundamental numeric types (`int`, `float`, `complex`). It uses nested list comprehension to calculate the result type of adding instances of each type pair and displays this information neatly using a Pandas DataFrame. It requires the Pandas library.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ntypes = [int, float, complex]\nname = lambda t: t.__name__\npd.DataFrame([[name(type(t1(1) + t2(1))) for t1 in types] for t2 in types],\n             index=[name(t) for t in types], columns=[name(t) for t in types])\n```\n\n----------------------------------------\n\nTITLE: Start JAX Profiler Server in Python Program (Python)\nDESCRIPTION: This snippet starts a JAX profiler server on port 9999 within a Python process via jax.profiler.start_server(9999). This enables manual or remote capture of profiler traces, such as from TensorBoard when using the manual capture workflow. Importing jax.profiler is required prior to server start. The server must be running before capturing traces from external tools. The profiler can be stopped afterward with jax.profiler.stop_server().\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport jax.profiler\njax.profiler.start_server(9999)\n```\n\n----------------------------------------\n\nTITLE: Testing pure_callback Bessel Wrapper with jit and vmap - Python\nDESCRIPTION: Applies the jv pure_callback wrapper in several JAX transformations: direct call, through jit, and via vmap. Demonstrates output consistency and correct wrapping across JAX computational modes. Useful for integrating external/scipy code into JAX pipelines before custom autodiff is added.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nj1 = partial(jv, 1)\nz = jnp.arange(5.0)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(j1(z))\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(jax.jit(j1)(z))\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(jax.vmap(j1)(z))\n```\n\n----------------------------------------\n\nTITLE: Creating a BlockSpec for Pipelined Computation in JAX Pallas\nDESCRIPTION: Constructs a BlockSpec object that defines how to divide and access blocks of an array during pipelined computation. It specifies the shape of each block and how to map iteration indices to array indices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nblock_spec = pl.BlockSpec((256, 512), x_index_map)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Gradient with nondiff_argnums - JAX - Python\nDESCRIPTION: This snippet demonstrates the deprecated usage of nondiff_argnums with jax.custom_vjp, showing how non-differentiable arguments (lo, hi) were previously passed via nondiff_argnums. The approach fails for Tracer inputs as of JAX PR #4008. Dependencies: functools.partial and JAX. Functions define a forward and backward rule for a simple gradient clipping operation. Inputs: lo, hi, x (with lo/hi not allowed to be Tracers). Outputs: x (identical to input). Limitations: Not compatible with Tracer arguments in nondiff_argnums after PR #4008.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4008-custom-vjp-update.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\\nimport jax\\n\\n@partial(jax.custom_vjp, nondiff_argnums=(0, 1))\\ndef clip_gradient(lo, hi, x):\\n  return x  # identity function\\n\\ndef clip_gradient_fwd(lo, hi, x):\\n  return x, None  # no residual values to save\\n\\ndef clip_gradient_bwd(lo, hi, _, g):\\n  return (jnp.clip(g, lo, hi),)\\n\\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipelined Matrix Addition with JAX Pallas\nDESCRIPTION: Defines a function that adds two matrices using pipelining to optimize memory usage and computation efficiency. It uses BlockSpec and grid to split the computation into blocks that can be processed in parallel with data transfers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef add_matrices_pipelined(x: jax.Array, y: jax.Array) -> jax.Array:\n  block_spec = pl.BlockSpec((256, 512), lambda i: (i, 0))\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(2,)\n  )(x, y)\n\nadd_matrices_pipelined(x, y)\n```\n\n----------------------------------------\n\nTITLE: Calculating 2nd Order Derivative using nth_order_derivative in Python\nDESCRIPTION: This snippet demonstrates calculating the 2nd order derivative using the previously defined `nth_order_derivative` function. It calls the function with n=2 on a presumed function `foo` at point 2.0.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_5\n\nLANGUAGE: ipython3\nCODE:\n```\nprint(nth_order_derivative(2, foo, 2.0))\n```\n\n----------------------------------------\n\nTITLE: Prototype for _join_jaxpr_res Helper Function\nDESCRIPTION: Shows the function signature for the _join_jaxpr_res utility which would handle joining the output types of transformed jaxprs to make them consistent. This is needed for supporting reverse-mode automatic differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_88\n\nLANGUAGE: python\nCODE:\n```\ndef _join_jaxpr_res(jaxpr1: Jaxpr, jaxpr2: Jaxpr, n1: int, n2: int\n                    ) -> tuple[Jaxpr, Jaxpr]:\n```\n\n----------------------------------------\n\nTITLE: Setting up JAX and TensorFlow serving environment\nDESCRIPTION: Commands to clone JAX repository, install necessary packages including JAX, JAXlib, Flax, TensorFlow, and TensorFlow Serving API, and pull the TensorFlow serving Docker image.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/jax-ml/jax\nJAX2TF_EXAMPLES=$(pwd)/jax/jax/experimental/jax2tf/examples\npip install -e jax\npip install flax jaxlib tensorflow_datasets tensorflow_serving_api tf_nightly\n```\n\n----------------------------------------\n\nTITLE: Using Debug Function in a Computation\nDESCRIPTION: Demonstrates incorporating the debugging function into a larger computation. The example shows how to insert a debugging trace at a specific point in the computation graph.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ndef foo(x):\n  y = x ** 2\n  y = debug(y)  # insert pdb in corresponding backward pass step\n  return jnp.sin(y)\n```\n\n----------------------------------------\n\nTITLE: Defining Modified ppermute_done Pallas Kernel and Wrapper in Python/JAX\nDESCRIPTION: Defines the `ppermute_done_kernel` and its wrapper `ppermute_done` using JAX Pallas. The kernel waits on semaphores signaled by the corresponding `ppermute_start`. The wrapper function `ppermute_done` now explicitly takes the original input buffer `x` as an argument (although `ppermute_done_kernel` ignores it, marked by `_`). This creates a data dependency, forcing XLA to keep `x` alive until `ppermute_done` is called, thus solving the lifetime issue. It uses `input_output_aliases={1:0}` to alias the input `out` buffer to the output. Dependencies: `jax`, `jax.pallas as pl`, `jax.experimental.pallas.tpu as pltpu`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef ppermute_done_kernel(_, ref, send_sem, recv_sem, _):\n  pltpu.make_async_copy(ref, ref, send_sem).wait()\n  pltpu.make_async_copy(ref, ref, recv_sem).wait()\n\ndef ppermute_done(send_sem, recv_sem, x, out) ->Array:\n  out = pl.pallas_call(\n      ppermute_done_kernel,\n      out_shape=(\n          jax.ShapeDtypeStruct(\n              out.shape,\n              dtype=out.dtype,\n          ),\n      ),\n      in_specs=[\n          pl.BlockSpec(memory_space=pltpu.ANY),\n          pl.BlockSpec(memory_space=pltpu.ANY),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n      ],\n      out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n      input_output_aliases={1:0}\n  )(x, out, send_sem, recv_sem)\n  return out\n\n```\n```\n\n----------------------------------------\n\nTITLE: Limitation of Buffer Donation with Keyword Arguments (Python)\nDESCRIPTION: Highlights a limitation of JAX's buffer donation feature: it does not work when arguments are passed using keywords. Even though `donate_argnums=(0, 1)` is specified, providing `params` and `state` as keyword arguments prevents their buffers from being donated. Donation only applies to positional arguments. Requires `jax`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n\n# Assume update_fn, params, state are defined\n# params, state = ...\n\n# This code will NOT donate any buffers because keyword arguments are used.\nparams, state = jax.pmap(update_fn, donate_argnums=(0, 1))(params=params, state=state)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Python Type Promotion Lattice with NetworkX in Python\nDESCRIPTION: This snippet visualizes Python's numeric type promotion (`int`, `float`, `complex`) as a lattice diagram. It uses the NetworkX library to define the graph structure representing allowed implicit promotions and Matplotlib to render the visualization. The lattice shows `int` promotes to `float`, which promotes to `complex`. Requires NetworkX and Matplotlib.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#@title\nimport networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {'int': ['float'], 'float': ['complex']}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {'int': [0, 0], 'float': [1, 0], 'complex': [2, 0]}\nfig, ax = plt.subplots(figsize=(8, 2))\nnx.draw(graph, with_labels=True, node_size=4000, node_color='lightgray', pos=pos, ax=ax, arrowsize=20)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Error from Mixing JAX Symbolic Scopes\nDESCRIPTION: Demonstrates that symbolic dimensions created through separate calls to `jax.export.symbolic_shape` belong to different 'SymbolicScopes'. Attempting arithmetic operations between expressions from different scopes (like `a1 + a2` here) results in a `ValueError` because their underlying variable definitions and constraints are isolated.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> a1, = export.symbolic_shape(\"a,\")\n>>> a2, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\n\n>>> a1 + a2  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: Invalid mixing of symbolic scopes for linear combination.\nExpected  scope 4776451856 created at <doctest shape_poly.md[31]>:1:6 (<module>)\nand found for 'a' (unknown) scope 4776979920 created at <doctest shape_poly.md[32]>:1:6 (<module>) with constraints:\n  a >= 8\n\n```\n\n----------------------------------------\n\nTITLE: Visualizing Standard Forward/Backward Computation without Checkpointing\nDESCRIPTION: Using the visualization utility to show JAX's forward and backward computation graphs without any checkpointing applied.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Without using `jax.checkpoint`:\nprint_fwd_bwd(f, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Testing Differentiation Convention with defjvps\nDESCRIPTION: Verifying that the defjvps implementation produces the same result at the boundary point.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(0.))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Mixed Promotion Lattice for Signed & Unsigned Integers in Python\nDESCRIPTION: This code snippet creates and visualizes a graph representing the promotion lattice for mixed signed and unsigned integer types. It uses NetworkX to create the graph and Matplotlib to draw it.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n  'i*': ['f*', 'u8', 'i8'], 'f*': ['c*', 'f16'], 'c*': ['c64'],\n  'u8': ['u16', 'i16'], 'u16': ['u32', 'i32'], 'u32': ['u64', 'i64'],\n  'i8': ['i16'], 'i16': ['i32'], 'i32': ['i64'],\n  'f16': ['f32'], 'f32': ['f64', 'c64'], 'f64': ['c128'],\n  'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n  'i*': [-1.25, 0.5], 'f*': [-0.5, 2], 'c*': [0, 3],\n  'u8': [0.5, 0], 'u16': [1.5, 0], 'u32': [2.5, 0], 'u64': [3.5, 0],\n  'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\n  'f16': [0.5, 2], 'f32': [1.5, 2], 'f64': [2.5, 2],\n  'c64': [2, 3], 'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 5))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\n```\n\n----------------------------------------\n\nTITLE: Error Handling with Incompatible Arguments in AOT-Compiled Functions\nDESCRIPTION: Demonstrates how JAX raises errors when an AOT-compiled function is called with arguments that don't match the shapes or types for which it was compiled, highlighting type safety.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/aot.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> x_1d = y_1d = jnp.arange(3)\n>>> jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x_1d, y_1d)  # doctest: +IGNORE_EXCEPTION_DETAIL\n...\nTraceback (most recent call last):\nTypeError: Argument types differ from the types for which this computation was compiled. The mismatches are:\nArgument 'x' compiled with int32[] and called with int32[3]\nArgument 'y' compiled with int32[] and called with int32[3]\n\n>>> x_f = y_f = jnp.float32(72.)\n>>> jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x_f, y_f)  # doctest: +IGNORE_EXCEPTION_DETAIL\n...\nTraceback (most recent call last):\nTypeError: Argument types differ from the types for which this computation was compiled. The mismatches are:\nArgument 'x' compiled with int32[] and called with float32[]\nArgument 'y' compiled with int32[] and called with float32[]\n```\n\n----------------------------------------\n\nTITLE: Using custom_jvp with non-differentiable function argument\nDESCRIPTION: Shows how to call the custom_jvp function with a lambda function as a non-differentiable argument. The example applies a cubic function to the input value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nprint(app(lambda x: x ** 3, 3.))\n```\n\n----------------------------------------\n\nTITLE: Handling Global Variables in JAX JIT Compilation\nDESCRIPTION: This example shows how JAX handles global variables differently from regular Python, potentially leading to unexpected behavior in JIT-compiled functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ng = 0.\ndef impure_uses_globals(x):\n  return x + g\n\n# JAX captures the value of the global during the first run\nprint (\"First call: \", jit(impure_uses_globals)(4.))\ng = 10.  # Update the global\n\n# Subsequent runs may silently use the cached value of the globals\nprint (\"Second call: \", jit(impure_uses_globals)(5.))\n\n# JAX re-runs the Python function when the type or shape of the argument changes\n# This will end up reading the latest value of the global\nprint (\"Third call, different type: \", jit(impure_uses_globals)(jnp.array([4.])))\n```\n\n----------------------------------------\n\nTITLE: Conditional Execution with JAX lax.cond (Python)\nDESCRIPTION: This snippet shows how to use jax.lax.cond for conditional execution within a JAX-traced context in Python. It requires JAX, and specifies a boolean predicate, a true branch, a false branch, and the operand. Each branch is a lambda function taking a single argument. Inputs are a single numeric argument, and outputs are determined by the chosen branch. Suitable for scalar branching and compositional with JAX transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\n\ndef func7(arg):\n  return lax.cond(arg >= 0.,\n                  lambda xtrue: xtrue + 3.,\n                  lambda xfalse: xfalse - 3.,\n                  arg)\n\nprint(make_jaxpr(func7)(5.))\n```\n\n----------------------------------------\n\nTITLE: Stateful Asynchronous Permutation with Intermediate Operation\nDESCRIPTION: This example shows how stateful operations using Refs can enforce ordering constraints, addressing scheduling ambiguity issues that arise in value-based approaches.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  x_ref = make_ref(x)\n  y_ref = make_ref(zeros_like(x))\n  fut = ppermute_start_stateful(x_ref, y_ref)\n  x_ref[...] += 1\n  ppermute_done_stateful(*fut, x_ref, y_ref)\n  return y_ref[...]\n```\n\n----------------------------------------\n\nTITLE: Defining a Conditional Function for JIT\nDESCRIPTION: Creates a function with conditional branching that will be used to demonstrate JIT compilation constraints with dynamic control flow.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  if x > 0:\n    return 2 * x ** 2\n  else:\n    return 3 * x\n\ng = jit(f)\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of JAX with Threading\nDESCRIPTION: Illustrates the incorrect way of using threading to manipulate JAX values inside a function passed to jit. This is not permitted and may lead to errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/concurrency.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit\n\n# This is not permitted and may cause errors\ndef f(x):\n    # Incorrect: manipulating JAX values with threading inside jitted function\n    # Implementation details omitted\n```\n\n----------------------------------------\n\nTITLE: Pointer Calculation for Pallas Indexing (Illustrative)\nDESCRIPTION: Illustrates the row-major pointer offset calculation corresponding to indexing `x_ref[3, 2]` for a hypothetical `Ref` `x_ref` with shape (4, 5). This calculation is performed when lowering Pallas indexing to Triton pointer arithmetic.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n5 * 3 + 2 * 1\n```\n\n----------------------------------------\n\nTITLE: Defining and Initializing a Neural Network with JAX Stax in Python\nDESCRIPTION: This snippet demonstrates how to define a sequential convolutional neural network using the JAX Stax mini-library. It imports necessary components like Conv, Dense, and activation functions, then combines them using `stax.serial`. The network's initialization function (`net_init`) is called to get the output shape and initial parameters for a given input shape, and the network's application function (`net_apply`) is used to run dummy data through the initialized network.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/example_libraries/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax import random\nfrom jax.example_libraries import stax\nfrom jax.example_libraries.stax import (\n    Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax)\n\n# Use stax to set up network initialization and evaluation functions\nnet_init, net_apply = stax.serial(\n    Conv(32, (3, 3), padding='SAME'), Relu,\n    Conv(64, (3, 3), padding='SAME'), Relu,\n    MaxPool((2, 2)), Flatten,\n    Dense(128), Relu,\n    Dense(10), LogSoftmax,\n)\n\n# Initialize parameters, not committing to a batch shape\nrng = random.key(0)\nin_shape = (-1, 28, 28, 1)\nout_shape, net_params = net_init(rng, in_shape)\n\n# Apply network to dummy inputs\ninputs = jnp.zeros((128, 28, 28, 1))\npredictions = net_apply(net_params, inputs)\n```\n\n----------------------------------------\n\nTITLE: Saving a Converted JAX Function as a TensorFlow SavedModel (Python)\nDESCRIPTION: This snippet illustrates how to serialize a JAX function, previously converted using `jax2tf.convert`, into a TensorFlow SavedModel. It wraps the converted function `f_jax` (from the previous example) within `tf.function`, providing an `input_signature` to define the expected input tensor shape and type. This function is then attached to a `tf.Module` instance, which is subsequently saved using `tf.saved_model.save`. The example also shows how to load the SavedModel back using `tf.saved_model.load`, noting that the restored model requires XLA but not necessarily JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# You can save the model just like you would with any other TensorFlow function:\nmy_model = tf.Module()\n# Save a function that can take scalar inputs.\nmy_model.f = tf.function(jax2tf.convert(f_jax), autograph=False,\n                         input_signature=[tf.TensorSpec([], tf.float32)])\ntf.saved_model.save(my_model, '/some/directory',\n                    options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n\n# Restoring (note: the restored model does *not* require JAX to run, just XLA).\nrestored_model = tf.saved_model.load('/some/directory')\n```\n\n----------------------------------------\n\nTITLE: Installing JAX in Editable Mode using Pip\nDESCRIPTION: This command installs the JAX library in 'editable' mode using pip. The `-e .` argument tells pip to install the package located in the current directory (`.`) by creating symbolic links to the source files instead of copying them. This allows changes made to the source code to be immediately reflected in the installed package without needing reinstallation. This command should be run from the root of the JAX repository after `jaxlib` has been installed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\npip install -e .  # installs jax\n```\n\n----------------------------------------\n\nTITLE: Implementing Naive Sum Reduction in Pallas (Python)\nDESCRIPTION: This snippet shows an incorrect implementation of sum reduction using Pallas. It fails to initialize the output array, leading to accumulation into garbage values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef naive_sum_kernel(x_ref, o_ref):\n  o_ref[...] += x_ref[...]\n\ndef naive_sum(x: jax.Array) -> jax.Array:\n  grid, *out_shape = x.shape\n  return pl.pallas_call(\n      naive_sum_kernel,\n      grid=grid,\n      # None in `block_shape` means we pick a size of 1 and squeeze it away\n      in_specs=[pl.BlockSpec((None, *out_shape), lambda i: (i, 0, 0))],\n      out_specs=pl.BlockSpec(out_shape, lambda i: (0, 0)),\n      out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype),\n  )(x)\nnaive_sum(x)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Saved Residuals (Baseline Before Naming Policy) in JAX Python\nDESCRIPTION: This snippet calls `print_saved_residuals` again for the original `loss` function. Although the `predict` function might have been redefined in the execution context to include `checkpoint_name`, this call likely serves as a baseline comparison before applying a name-based policy in the next step.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint_saved_residuals(loss, params, x, y)\n```\n\n----------------------------------------\n\nTITLE: XLA Decomposed Permute Operations\nDESCRIPTION: Shows how XLA automatically decomposes the permute operation into start and done operations for better scheduling.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  fut = ppermute_start(x)\n  z = x + 1  # happens at the same time as ppermute\n  y = ppermute_done(fut)\n  return y, z\n```\n\n----------------------------------------\n\nTITLE: Error from Accessing Invalidated Donated Buffers (Python)\nDESCRIPTION: Illustrates the `RuntimeError` that occurs when attempting to use an array (`y`) after its buffer has been donated in a preceding JAX computation. The call `jax.jit(add, donate_argnums=(1,))(x, y)` invalidates the buffer associated with `y`. Subsequently using `y` in an operation like `w = y + 1` is illegal and raises an error. Requires `jax` and the `add`, `x`, `y` variables from previous examples.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Assume add, x, y are defined as before\n# x = jax.device_put(np.ones((2, 3)))\n# y = jax.device_put(np.ones((2, 3)))\n\n# Donate the buffer for `y`\nz = jax.jit(add, donate_argnums=(1,))(x, y)\nw = y + 1  # Reuses `y` whose buffer was donated above\n# >> RuntimeError: Invalid argument: CopyToHostAsync() called on invalid buffer\n```\n\n----------------------------------------\n\nTITLE: Reusing Compiled Function from Cache\nDESCRIPTION: Demonstrates cache hit for previously compiled JIT function, avoiding retracing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nz = f(4., 5.)  # 'tracing!' doesn't print, compilation cache hit!\nprint(z)\n```\n\n----------------------------------------\n\nTITLE: Basic Loop Implementation (Pseudocode)\nDESCRIPTION: Initial implementation showing elementwise program with blocking copy operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_2\n\nLANGUAGE: pseudocode\nCODE:\n```\nfor i in range(N):\n  copy_in(A[i], X)\n  Y = X + 1\n  copy_out(Y, A[i])\n```\n\n----------------------------------------\n\nTITLE: Building jaxlib with Specific CUDA and CUDNN Versions\nDESCRIPTION: Command to build jaxlib with specific CUDA and CUDNN versions for JAX v0.4.32 and later.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython build/build.py build --wheels=jax-cuda-plugin --cuda_version=12.3.2 \\\n--cudnn_version=9.1.1\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JAX JIT Failure with TensorFlow Conditional\nDESCRIPTION: This snippet shows a TensorFlow function with dynamic output shape that will fail when used with JAX's JIT compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndef f_tf_dynamic_output_shape(x):\n  return tf.cond(x[0] >= 0, lambda: x, lambda: x[1:])\n\nx = np.array([1, 2], dtype=np.int32)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Lorentz ODE Solver in Python\nDESCRIPTION: Imports necessary libraries for numerical computing, JAX operations, and visualization. Sets up matplotlib for inline plotting in a Jupyter notebook environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport os\nfrom functools import partial\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax import vmap, jit, grad, ops, lax, config\nfrom jax import random as jr\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom IPython.display import display_png\n\nmpl.rcParams['savefig.pad_inches'] = 0\nplt.style.use('seaborn-dark')\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: CNN Dimension Error with jax2tflite\nDESCRIPTION: Error when converting CNN model with dynamic dimensions. The converter cannot compute stride for dimension 'b' due to limitations with shape polynomials division.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nInconclusiveDimensionOperation(\"Cannot compute stride for dimension 'b', window_size '2', stride '2'.\nDetails: Cannot divide 'b + -2' by '2'.\nSee https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#division-of-shape-polynomials-is-partially-supported.\")\n```\n\n----------------------------------------\n\nTITLE: Handling Multiple Inputs in Flax Models for jax2tf in Python\nDESCRIPTION: Adapts the `predict_fn` lambda for Flax models that accept multiple input arguments. It takes a single tuple `input` (representing all model inputs) and uses argument unpacking (`*input`) to pass the individual elements as separate arguments to the `model.apply` method. This maintains the required two-argument signature (`params`, `inputs`) for the `predict_fn` used in the jax2tf conversion process.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\npredict_fn = lambda params, input: model.apply({\"params\": params}, *input)\n```\n```\n\n----------------------------------------\n\nTITLE: Start TensorBoard Server for Trace Viewing (Shell)\nDESCRIPTION: This Shell command launches a TensorBoard server with its logdir set to /tmp/tensorboard/. It is required to visualize profiler traces produced by JAX or TensorFlow. The command will output a local URL (default port 6006) for access in a web browser. Adjust --logdir or --port as necessary for your workflow.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ntensorboard --logdir=/tmp/tensorboard\n```\n\n----------------------------------------\n\nTITLE: Visualizing NumPy Intra-Category Type Promotion Lattices with NetworkX in Python\nDESCRIPTION: This code visualizes the type promotion rules *within* different numerical categories in NumPy (unsigned integers, signed integers, floats, complex floats) as separate lattices on a single plot. It uses NetworkX to define the promotion hierarchies (e.g., `uint8` -> `uint16`) and Matplotlib to render the graph. Requires NetworkX and Matplotlib.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#@title\nimport networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n  'u8': ['u16'], 'u16': ['u32'], 'u32': ['u64'],\n  'i8': ['i16'], 'i16': ['i32'], 'i32': ['i64'],\n  'f16': ['f32'], 'f32': ['f64'],\n  'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n  'u8': [0, 0], 'u16': [1, 0], 'u32': [2, 0], 'u64': [3, 0],\n  'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\n  'f16': [1, 2], 'f32': [2, 2], 'f64': [3, 2],\n  'c64': [2, 3], 'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 4))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\n```\n\n----------------------------------------\n\nTITLE: Pytree Structure Debugging\nDESCRIPTION: Code to view the pytree definition of an object for debugging purposes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pytrees.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.tree_util import tree_structure\nprint(tree_structure(object))\n```\n\n----------------------------------------\n\nTITLE: Hypothetical 'P-sum' Solution for shmap in JAX\nDESCRIPTION: This snippet proposes a hypothetical `f1_better` function using a potential 'P-sum' solution. It suggests extending `out_specs` to include a `sum='i'` directive, allowing the summation over axis 'i' to be expressed declaratively rather than via a `psum` collective inside the `shmap` body. This would enable more efficient transposition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n```python\n# What if we could write an output sum like this?\nf1_better = shmap(g, in_specs=P('i'), out_specs=P(sum='i'))  # sum='i' means sum over that axis\n```\n```\n\n----------------------------------------\n\nTITLE: Using Concrete Tracers in jax.grad for Conditionals (Python)\nDESCRIPTION: This snippet shows that `jax.grad` can successfully handle the `divide` function (defined previously) without needing `static_argnums`. This works because `jax.grad` introduces concrete tracers for its arguments, which carry the actual numerical values, allowing conditionals like `y >= 1.` to be resolved during the differentiation trace. Requires `jax` and the `divide` function from the previous example.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\njax.grad(divide)(3., 2.)  # Works\n```\n\n----------------------------------------\n\nTITLE: Visualizing Variational Inference Results with Matplotlib\nDESCRIPTION: Plots the results of variational inference, comparing true parameter values with estimated posterior means and error bars using Matplotlib.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(7, 7))\nplt.plot(true_beta, beta_loc, '.', label='Approximated Posterior Means')\nplt.plot(true_beta, beta_loc + 2*jnp.exp(beta_log_scale), 'r.', label=r'Approximated Posterior $2\\sigma$ Error Bars')\nplt.plot(true_beta, beta_loc - 2*jnp.exp(beta_log_scale), 'r.')\nplot_scale = 3\nplt.plot([-plot_scale, plot_scale], [-plot_scale, plot_scale], 'k')\nplt.xlabel('True beta')\nplt.ylabel('Estimated beta')\nplt.legend(loc='best')\n```\n\n----------------------------------------\n\nTITLE: Running JAX Doctests within a Specific Python Module\nDESCRIPTION: This command uses `pytest` to execute doctests found within the docstrings of a specific Python module (`jax/_src/numpy/lax_numpy.py`). The `--doctest-modules` flag enables this mode. Environment variables `JAX_TRACEBACK_FILTERING=off` and XLA flags (`--xla_force_host_platform_device_count=8`) are set to configure the testing environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_46\n\nLANGUAGE: shell\nCODE:\n```\nJAX_TRACEBACK_FILTERING=off XLA_FLAGS=--xla_force_host_platform_device_count=8 pytest --doctest-modules jax/_src/numpy/lax_numpy.py\n```\n\n----------------------------------------\n\nTITLE: Demonstrating NumPy Out-of-Bounds Indexing Error\nDESCRIPTION: This snippet shows how NumPy throws an error when indexing an array outside its bounds.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnp.arange(10)[11]\n```\n\n----------------------------------------\n\nTITLE: Implementing PartialEvalTrace class for JAX partial evaluation\nDESCRIPTION: Defines the PartialEvalTrace class which handles tracing for partial evaluation in JAX. It includes methods for creating new arguments, lifting values, instantiating constants, and processing primitives.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_63\n\nLANGUAGE: Python\nCODE:\n```\nclass PartialEvalTrace(Trace):\n  def new_arg(self, pval: PartialVal) -> Any:\n    return PartialEvalTracer(self, pval, LambdaBindingRecipe())\n\n  def lift(self, val: Any) -> PartialEvalTracer:\n    return PartialEvalTracer(self, PartialVal.known(val), None)\n  pure = lift\n\n  def instantiate_const(self, tracer: PartialEvalTracer) -> PartialEvalTracer:\n    if tracer.pval.is_unknown:\n      return tracer\n    else:\n      pval = PartialVal.unknown(raise_to_shaped(tracer.aval))\n      return PartialEvalTracer(self, pval, ConstRecipe(tracer.pval.const))\n\n  def process_primitive(self, primitive, tracers, params):\n    if all(t.pval.is_known for t in tracers):\n      return bind(primitive, *map(full_lower, tracers), **params)\n    rule = partial_eval_rules.get(primitive)\n    if rule: return rule(self, tracers, **params)\n    tracers_in = [self.instantiate_const(t) for t in tracers]\n    avals_in = [t.aval for t in tracers_in]\n    avals_out = abstract_eval_rules[primitive](*avals_in, **params)\n    tracers_out = [PartialEvalTracer(self, PartialVal.unknown(aval), None)\n                   for aval in avals_out]\n    eqn = JaxprEqnRecipe(primitive, tracers_in, params, avals_out,\n                         map(ref, tracers_out))\n    for t in tracers_out: t.recipe = eqn\n    return tracers_out\n\npartial_eval_rules = {}\n```\n\n----------------------------------------\n\nTITLE: Initializing Input Data for TPU Reduce-Scatter Operation in Python\nDESCRIPTION: Sets up the input array with random values and places it on devices according to a specified sharding strategy. Defines LEFT and RIGHT constants for neighbor communication directions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nblock_size = (16, 128)\ninput_arr = jax.random.uniform(\n    jax.random.key(0),\n    shape=(block_size[0] * num_devices, block_size[1] * num_devices),\n)\ninput_arr = jax.device_put(input_arr, sharding)\n\nLEFT = 0\nRIGHT = 1\n```\n\n----------------------------------------\n\nTITLE: Unpacking Future within Loop Body\nDESCRIPTION: This snippet shows the loop body from the previous example with the `ppermute_start` future unpacked. It explicitly shows the aliased `x` and the output `y` returned by `ppermute_start`, making the potential buffer reuse across iterations more apparent.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  def body(i, x):\n    *sems, x, y = ppermute_start(x)\n    y = ppermute_done(*sems, x, y)\n    return y\n  return fori_loop(0, 8, body, x)\n```\n```\n\nLANGUAGE: python\nCODE:\n```\n# Code used conceptually for alias analysis (coloring)\n```py\ndef f(x):\n  def body(i, x): # x is buffer A (input)\n    *sems, x, y = ppermute_start(x) # ppermute_start aliases input x (A) to output x (A)\n                                     # y gets a new buffer B\n    y = ppermute_done((*sems, x, y)) # ppermute_done aliases input y (B) to output y (B)\n    return y # returns buffer B\n  # fori_loop aliases output (B) to input (A) for next iter\n  return fori_loop(0, 8, body, x)\n```\n```\n\n----------------------------------------\n\nTITLE: Setting JAX Compilation Cache Directory via Shell Environment Variable\nDESCRIPTION: Shows how to set the JAX persistent compilation cache directory by exporting an environment variable in a shell. This method should be used before running Python scripts that use JAX, ensuring the specified cache location (e.g., /tmp/jax_cache) is accessible to the process. Works in bash or similar shell environments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nexport JAX_COMPILATION_CACHE_DIR=\"/tmp/jax_cache\"\n```\n\n----------------------------------------\n\nTITLE: Computing Different Gradients with defjvps\nDESCRIPTION: Shows how to compute gradients with respect to different arguments when using defjvps. The example demonstrates computing the full gradient and gradients with respect to specific arguments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(2., 3.))\nprint(grad(f, 0)(2., 3.))  # same as above\nprint(grad(f, 1)(2., 3.))\n```\n\n----------------------------------------\n\nTITLE: Using jax.checkpoint Decorator to Control Residual Storage\nDESCRIPTION: Applying jax.checkpoint to subfunctions to reduce memory usage by having JAX not save certain intermediate values during differentiation, instead recomputing them during the backward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f2(W1, W2, W3, x):\n  x = jax.checkpoint(g)(W1, x)\n  x = jax.checkpoint(g)(W2, x)\n  x = jax.checkpoint(g)(W3, x)\n  return x\n\njax.ad_checkpoint.print_saved_residuals(f2, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Indexing a Pallas Ref (Illustrative)\nDESCRIPTION: Illustrates indexing into a Pallas `Ref` object named `x_ref` at index (3, 2). In the context of lowering to Triton, this operation requires calculating a memory pointer based on the array's shape and layout (e.g., row-major calculation: `5 * 3 + 2 * 1` for a (4, 5) shape).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nx_ref[3, 2]\n```\n\n----------------------------------------\n\nTITLE: Evaluation and Compilation Pass-through with call and custom_jvp_call (Python/JAX)\nDESCRIPTION: Shows how the eval and jit compilation steps interact with call and custom_jvp_call primitives in JAX. These code snippets state that evaluation with eval ignores transformation wrappers, while jit with call or custom_jvp_call wraps the function with an XLA HLO call. Useful for understanding transformation 'exit points' in JAX's computation graph and differentiation system.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/2026-custom-derivatives.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\neval(call(f)) == eval(f)\njit(call(f)) == hlo_call(jit(f))\n\neval(custom_jvp_call(f, f_jvp)) == eval(f)\njit(custom_jvp_call(f, f_jvp)) == hlo_call(jit(f))\n```\n\n----------------------------------------\n\nTITLE: Efficient Transposes with Hypothetical 'P-sum' Solution in JAX\nDESCRIPTION: This snippet shows how the hypothetical `f1_better` function (using the 'P-sum' solution) would transpose. The first transpose `t(f1_better)` becomes efficient, simply mapping `t(g)` with appropriate specs. The second transpose `t(t(f1_better))` correctly reverts to the original `P(sum='i')` specification, avoiding the nested collectives seen in the current inefficient approach.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Then it could transpose like this:\nt(f1_better) = shmap(t(g), in_specs=P(), out_specs=P('i'))\nt(t(f1_better)) = shmap(t(t(g)), in_specs=P('i'), P(sum='i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Installing JAX with Google TPU Support using pip\nDESCRIPTION: This command installs the latest version of JAX with support for Google Tensor Processing Units (TPUs). The `[tpu]` extra ensures the correct dependencies for TPU execution are included. This is typically run in an environment where TPUs are accessible, like Google Colab or Google Cloud.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"jax[tpu]\"\n```\n\n----------------------------------------\n\nTITLE: Illustrating Legal In-Place Operation in JAX\nDESCRIPTION: A simple Python JAX function `f()` demonstrating a scenario where an in-place operation (`add_one_inplace`) is safe. Since the input `x` is not used after being passed to `add_one_inplace`, modifying it in place doesn't cause issues. Dependencies: `jax.numpy as jnp`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f():\n  x = jnp.arange(...)\n  y = add_one_inplace(x)   return y\n```\n```\n\n----------------------------------------\n\nTITLE: Double-Buffered Pipeline Implementation\nDESCRIPTION: Final optimized implementation of double-buffered pipeline with prologue, main loop and epilogue sections.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Prologue\ncopy_in_start(A[0], X[0])\n\n# Main loop\nfor i in range(N):\n  cur_slot = i % 2\n  next_slot = (i + 1) % 2\n\n  if i < N:\n    copy_in_start(A[i+1], X[next_slot])\n  \n  copy_in_wait(X[cur_slot])\n  Y[cur_slot] = X[cur_slot] + 1\n  copy_out_start(Y[cur_slot], A[i])\n\n  if i > 0:\n    copy_out_wait(Y[next_slot])\n\n# Epilogue\ncopy_out_wait(Y[1])\n```\n\n----------------------------------------\n\nTITLE: Accessing Compiler Cost Analysis for Compiled JAX Functions in Python\nDESCRIPTION: Calls the `cost_analysis()` method on a `jax.stages.Compiled` object (`compiled`) to retrieve cost and memory analysis results from the underlying compiler. This information is useful for understanding performance but its availability and format depend heavily on the specific compiler, platform, and runtime. The method returns `None` if the analysis is unavailable.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/aot.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncompiled.cost_analysis()\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Type Promotion Implementation\nDESCRIPTION: Creates a type promotion table for TensorFlow datatypes using pandas DataFrame. Defines helper functions to test type combinations and format results.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nimport pandas as pd\nfrom IPython import display\n\ntf_dtypes = {\n  'b': tf.bool,\n  'u8': tf.uint8, 'u16': tf.uint16, 'u32': tf.uint32, 'u64': tf.uint64,\n  'i8': tf.int8, 'i16': tf.int16, 'i32': tf.int32, 'i64': tf.int64,\n  'bf16': tf.bfloat16, 'f16': tf.float16, 'f32': tf.float32, 'f64': tf.float64,\n  'c64': tf.complex64, 'c128': tf.complex128,\n  'i*': int, 'f*': float, 'c*': complex}\n\ntf_dtype_to_code = {val: key for key, val in tf_dtypes.items()}\n\ndef make_tf_zero(dtype):\n  if dtype in {int, float, complex}:\n    return dtype(0)\n  else:\n    return tf.zeros(1, dtype=dtype)\n\ndef result_code(dtype1, dtype2):\n  try:\n    out = tf.add(make_tf_zero(dtype1), make_tf_zero(dtype2))\n  except (TypeError, tf.errors.InvalidArgumentError):\n    return '-'\n  else:\n    if type(out) in {int, float, complex}:\n      return tf_dtype_to_code[type(out)]\n    else:\n      return tf_dtype_to_code[out.dtype]\n\ngrid = [[result_code(dtype1, dtype2)\n         for dtype2 in tf_dtypes.values()]\n        for dtype1 in tf_dtypes.values()]\ntable = pd.DataFrame(grid, index=tf_dtypes.keys(), columns=tf_dtypes.keys())\ndisplay.HTML(table.to_html())\n```\n\n----------------------------------------\n\nTITLE: Explicitly Showing Default `block_shape` Equivalence in Python\nDESCRIPTION: Calls `show_program_ids` explicitly setting `block_shape=(4, 4)` (the full array shape) and `index_map=None`. This configuration mirrors the default behavior shown in the previous example where `block_shape=None` was used. The identical output confirms that `block_shape=None` defaults to the full shape of the array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> show_program_ids(x_shape=(4, 4), block_shape=(4, 4), grid=(2, 3),\n...                  index_map=None)\n[[12 12 12 12]\n [12 12 12 12]\n [12 12 12 12]\n [12 12 12 12]]\n\n```\n\n----------------------------------------\n\nTITLE: GNN Conversion Error without XLA\nDESCRIPTION: Error when converting GNN model without XLA enabled. The reduce_window operation cannot be converted for int32 type operands without XLA support.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nNotImplementedError(\"Call to reduce_window cannot be converted with enable_xla=False. Add pooling does not support operands of type <dtype: 'int32'> - See source code for the precise conditions under which it can be converted without XLA.\")\n```\n\n----------------------------------------\n\nTITLE: Initializing JAX Dependencies and Device Check\nDESCRIPTION: Import required JAX libraries and verify availability of 8 devices required for the tutorial.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp\n\nif len(jax.local_devices()) < 8:\n  raise Exception(\"Notebook requires 8 devices to run\")\n```\n\n----------------------------------------\n\nTITLE: Verifying JAX Installation and Version on Colab\nDESCRIPTION: This snippet imports JAX and JAXlib, displays the Colab hostname, and prints the versions of both packages to verify the environment setup.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_gpu.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jaxlib\n\n!cat /var/colab/hostname\nprint(jax.__version__)\nprint(jaxlib.__version__)\n```\n\n----------------------------------------\n\nTITLE: Importing Function Documentation with Sphinx autofunction\nDESCRIPTION: This reStructuredText directive tells Sphinx to automatically generate documentation specifically for the `jet` function within its module, likely `jax.experimental.jet` based on context. It pulls the function's signature and docstring.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.jet.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autofunction:: jet\n```\n\n----------------------------------------\n\nTITLE: Reconstructing Data from Flattened Pytree in tf.Module\nDESCRIPTION: Shows how to reconstruct the original data structure from a flattened pytree stored in a tf.Module. This completes the workaround for tf.Module magic conversion issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ninput_data = jax.tree_util.tree_unflatten(m.input_data['tree_def'], m.input_data['flat'])\n```\n\n----------------------------------------\n\nTITLE: Simplified shmap with psum and Unmapped Output in JAX\nDESCRIPTION: This snippet defines a simplified JAX function `f1` using `shmap`. It applies a function `g` to an input `x` sharded along axis 'i', performs a `psum` over the same axis 'i', and returns the result as an unmapped output (`out_specs=P()`). This example distills the core structure causing inefficient transposition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Example 1: shmap involving psum and unmapped output with inefficient transpose\nf1 = shmap(lambda x: psum(g(x), 'i'),\n           in_specs=P('i'), out_specs=P())\n```\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Dependencies with Nightly Wheels using Python Script (Shell)\nDESCRIPTION: This command uses the `build.py` script with the `--nightly_update` flag to update dependencies for Python 3.12. This mode accepts pre-release, dev, and nightly packages, searches an additional index URL for scientific Python nightly wheels, and omits hashes from the resulting lock file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython build/build.py requirements_update --python_version=3.12 --nightly_update\n```\n\n----------------------------------------\n\nTITLE: Error Example: Incompatible Shapes for Broadcasting in JAX\nDESCRIPTION: Demonstrates an error scenario where incompatible shapes are used for broadcasting in a shape-polymorphic context.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfour_ones = np.ones((4,))\njax2tf.convert(lambda x, y: x + y,\n               polymorphic_shapes=[\"(v,)\", \"(4,)\"])(four_ones, four_ones)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating NumPy Type Promotion (int32 + float32) in Python\nDESCRIPTION: This snippet uses NumPy to illustrate its type promotion rules. It adds a 32-bit integer (`np.int32`) and a 32-bit float (`np.float32`) and then retrieves the data type (`dtype`) of the result, showing that NumPy promotes the output to `float64`, which can be inefficient on accelerators. It requires the NumPy library.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nnp.dtype(np.int32(1) + np.float32(1))\n```\n\n----------------------------------------\n\nTITLE: Enabling Detailed Cache Miss Explanations in JAX via Python Configuration\nDESCRIPTION: Configures JAX to log detailed explanations for all persistent compilation cache misses by setting 'jax_explain_cache_misses' to True. This helps in debugging and understanding caching inefficiencies. Should be set before program execution for meaningful output.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\njax.config.update(\"jax_explain_cache_misses\", True)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Program IDs with Array Shape Smaller Than Block Shape in Python\nDESCRIPTION: Calls `show_program_ids` with an array shape (1, 2) smaller than the block shape (2, 3). With a grid of (1, 1), only one invocation runs (program ID 0). The output `[[0 0]]` shows that the kernel writes to the available portion of the array within the first block.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> # It is allowed for the shape to be smaller than block_shape\n>>> show_program_ids(x_shape=(1, 2), block_shape=(2, 3), grid=(1, 1),\n...                  index_map=lambda i, j: (i, j))\n[[0 0]]\n\n```\n\n----------------------------------------\n\nTITLE: Basic Buffer Donation using jax.jit (Python)\nDESCRIPTION: Demonstrates a simple use case of buffer donation with `jax.jit`. The `donate_argnums=(1,)` parameter indicates that the buffer holding the second positional argument (`y`) can be reused for the output of the `add` function. This reduces memory allocation if the output has the same shape and dtype as the donated input. Requires `jax` and `numpy`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef add(x, y):\n  return x + y\n\nx = jax.device_put(np.ones((2, 3)))\ny = jax.device_put(np.ones((2, 3)))\n# Execute `add` with donation of the buffer for `y`. The result has\n# the same shape and type as `y`, so it will share its buffer.\nz = jax.jit(add, donate_argnums=(1,))(x, y)\n```\n\n----------------------------------------\n\nTITLE: Installing JAX Nightly for Google Cloud TPU with Pip - Bash\nDESCRIPTION: Installs nightly pre-release versions of JAX, jaxlib, libtpu, and requests via pip for use with Google Cloud TPUs. The command uses multiple -f links to ensure correct versions are sourced for both jax and libtpu. Prerequisite: Python and pip; execution environment must support Google Cloud TPU workloads when used. Input is the command; output is a pre-release JAX stack ready for TPU. Nightly builds may contain untested or unstable changes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install -U --pre jax jaxlib libtpu requests -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n\n```\n\n----------------------------------------\n\nTITLE: Building jaxlib with a Modified XLA Repository\nDESCRIPTION: Command to build jaxlib using a locally-modified copy of the XLA repository by specifying a custom path.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython build/build.py build --wheels=jaxlib --local_xla_path=/path/to/xla\n```\n\n----------------------------------------\n\nTITLE: Performing 3D Convolution with JAX\nDESCRIPTION: Demonstrates 3D convolution using JAX's lax.conv_general_dilated function. It creates a 3D kernel and 3D data, applies the convolution, and visualizes the results using 3D scatter plots.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/convolutions.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib as mpl\n\n# Random 3D kernel - HWDIO layout\nkernel = jnp.array([\n  [[0, 0,  0], [0,  1,  0], [0,  0,   0]],\n  [[0, -1, 0], [-1, 0, -1], [0,  -1,  0]],\n  [[0, 0,  0], [0,  1,  0], [0,  0,   0]]],\n  dtype=jnp.float32)[:, :, :, jnp.newaxis, jnp.newaxis]\n\n# 3D data - NHWDC layout\ndata = jnp.zeros((1, 30, 30, 30, 1), dtype=jnp.float32)\nx, y, z = np.mgrid[0:1:30j, 0:1:30j, 0:1:30j]\ndata += (jnp.sin(2*x*jnp.pi)*jnp.cos(2*y*jnp.pi)*jnp.cos(2*z*jnp.pi))[None,:,:,:,None]\n\nprint(\"in shapes:\", data.shape, kernel.shape)\ndn = lax.conv_dimension_numbers(data.shape, kernel.shape,\n                                ('NHWDC', 'HWDIO', 'NHWDC'))\nprint(dn)\n\nout = lax.conv_general_dilated(data,    # lhs = image tensor\n                               kernel,  # rhs = conv kernel tensor\n                               (1,1,1), # window strides\n                               'SAME',  # padding mode\n                               (1,1,1), # lhs/image dilation\n                               (1,1,1), # rhs/kernel dilation\n                               dn)      # dimension_numbers\nprint(\"out shape: \", out.shape)\n\n# Make some simple 3d density plots:\ndef make_alpha(cmap):\n  my_cmap = cmap(jnp.arange(cmap.N))\n  my_cmap[:,-1] = jnp.linspace(0, 1, cmap.N)**3\n  return mpl.colors.ListedColormap(my_cmap)\nmy_cmap = make_alpha(plt.cm.viridis)\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(x.ravel(), y.ravel(), z.ravel(), c=data.ravel(), cmap=my_cmap)\nax.axis('off')\nax.set_title('input')\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(x.ravel(), y.ravel(), z.ravel(), c=out.ravel(), cmap=my_cmap)\nax.axis('off')\nax.set_title('3D conv output');\n```\n\n----------------------------------------\n\nTITLE: Inspecting JAXPR and Lowered HLO for Platform Dependency (Python)\nDESCRIPTION: These two snippets show how to generate intermediate representations (JAXPR and HLO) for a JAX function, inspecting which FFI targets are present after lowering and compilation. This is useful to confirm that at compile-time, only the applicable FFI backend is chosen and there is no runtime overhead or unused code. Dependencies: JAX, already-registered functions from previous snippets. Inputs are test arrays; outputs are printed representations. These are analysis/inspection tools rather than end-user code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\njax.make_jaxpr(rms_norm_cross_platform)(x)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(jax.jit(rms_norm_cross_platform).lower(x).as_text().strip())\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(jax.jit(rms_norm_cross_platform).lower(x).as_text(dialect=\"hlo\").strip())\n```\n\n----------------------------------------\n\nTITLE: Specifying JAX Dependencies in Plaintext\nDESCRIPTION: This snippet lists the required dependencies for the JAX project. It includes TensorFlow as the main dependency, a specific version of the TensorBoard profile plugin, and Protobuf which is needed for the profile plugin to function correctly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/collect-profile-requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntensorflow\ntensorboard-plugin-profile<=2.19.0\n# Needed for the profile plugin to work without error\nprotobuf\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Bazel Build for CPU Tests\nDESCRIPTION: This command uses the JAX build script (`build/build.py`) to configure the Bazel build environment specifically for running CPU tests. The `--wheels=jaxlib` flag indicates the target wheel is the core `jaxlib`, and `--configure_only` prevents a full build, stopping after the configuration phase. This prepares the Bazel workspace for subsequent `bazel test` commands targeting CPU tests.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\npython build/build.py build --wheels=jaxlib --configure_only\n```\n\n----------------------------------------\n\nTITLE: Conversion Error Wrapping IndexError for Dynamic Slices in JAX2TFJS/TFLite/Flex (Text)\nDESCRIPTION: This code snippet shows the output when a JAX-to-TFjs, TFLite, or Flex conversion attempt fails owing to a dynamic slice index in NumPy APIs. The message gives an informative wrapper ('Conversion error') followed by a detailed IndexError. Dependencies are model/array conversion tools; key parameters are array slicing indices. The expected input involves dynamic/batch-symbolic indices, and the output is an error message explaining the unsupported operation. Intended for troubleshooting array indexing during conversions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nConversion error\\nIndexError('Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(None, b, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).')\n```\n\n----------------------------------------\n\nTITLE: Pointer Array Calculation for Pallas Slicing (Illustrative)\nDESCRIPTION: Illustrates the calculation producing an array of pointers for the slice `x_ref[4, :]`, assuming `x_ref` has 5 columns and the slice length is 3. This uses `jnp.arange` to generate the sequence of column indices and calculates the corresponding row-major offsets.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n5 * 4 + jnp.arange(3)\n```\n\n----------------------------------------\n\nTITLE: Calculating Arithmetic Intensity of Hardware - Python\nDESCRIPTION: Assigns variables for TPU v5e hardware to record theoretical FLOPs/sec, bandwidth, and computes the arithmetic intensity (FLOPs/byte). No external dependencies needed. Useful for comparing with computed intensities to determine compute vs. memory bound status. Inputs: hardware characteristics; Outputs: operation intensity scaling factor.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nv5e_flops = 197e12\nv5e_membw = 819e9\nv5e_op_intensity = v5e_flops / v5e_membw  # ~240.5\n```\n\n----------------------------------------\n\nTITLE: Timing Multi-Device Sharded Matrix Multiplication in JAX\nDESCRIPTION: Measures the execution time of matrix multiplication when inputs are sharded across multiple devices, demonstrating the performance benefit of parallel computation compared to the single-device approach.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n%timeit -n 5 -r 5 jnp.dot(y, z).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Installing JAX and jaxlib with CUDA/cuDNN Support - Bash\nDESCRIPTION: This set of bash command snippets demonstrates how to upgrade pip and install the jaxlib package compatible with specific versions of CUDA and cuDNN using pip. The commands include options for CUDA 11 with cuDNN 8.2 and 8.0.5, and explicitly set the pip find-links index to the official JAX releases page. Required dependencies include an appropriate CUDA (11.x) and cuDNN (8.x) setup. Users must ensure their system matches the driver/toolkit requirements to avoid runtime issues. Invocations should be executed in a terminal environment with administrative or virtual environment permissions; version compatibility is critical for GPU functionality.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade pip\n\n# Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.\npip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n# Installs the wheel compatible with Cuda 11 and cudnn 8.2 or newer.\npip install jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n# Installs the wheel compatible with Cuda 11 and cudnn 8.0.5 or newer.\npip install jax[cuda11_cudnn805] -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage: Using jax.numpy for Shape Calculation in JIT (Python)\nDESCRIPTION: This code demonstrates an incorrect pattern—using jax.numpy (jnp) to perform shape computation inside a jitted function. Such usage leads to AbstractTracer errors with omnistaging, since shape computations should happen outside the staged context or with standard numpy. The function will error if input_size is accessed as a Python integer.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  input_size = jnp.prod(x.shape)\n  if input_size > 100:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing PartialVal Class for JAX Partial Evaluation\nDESCRIPTION: The PartialVal class represents values that can be either known or unknown during partial evaluation. It's a core data structure for the partial evaluation machinery, tracking both the abstract value and concrete value if known.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nclass PartialVal(NamedTuple):\n  aval: ShapedArray\n  const: Any | None\n\n  @classmethod\n  def known(cls, val: Any):\n    return PartialVal(get_aval(val), val)\n\n  @classmethod\n  def unknown(cls, aval: ShapedArray):\n    return PartialVal(aval, None)\n\n  is_known   = property(lambda self: self.const is not None)\n  is_unknown = property(lambda self: self.const is     None)\n```\n\n----------------------------------------\n\nTITLE: Generating JAX Type Lattice Visualization using Python\nDESCRIPTION: This Python script utilizes the `networkx` and `matplotlib` libraries to generate a directed graph visualization of the JAX type promotion lattice. It defines the lattice relationships in a dictionary, positions the nodes for clarity, draws the graph with labels, and saves the output as an SVG image ('type_lattice.svg'). This visualization aids in understanding how different data types are promoted when combined in JAX operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n  'b1': ['i*'], 'u1': ['u2', 'i2'], 'u2': ['i4', 'u4'], 'u4': ['u8', 'i8'], 'u8': ['f*'],\n  'i*': ['u1', 'i1'], 'i1': ['i2'], 'i2': ['i4'], 'i4': ['i8'], 'i8': ['f*'],\n  'f*': ['c*', 'f2', 'bf'], 'bf': ['f4'], 'f2': ['f4'], 'f4': ['c8', 'f8'], 'f8': ['c16'],\n  'c*': ['c8'], 'c8': ['c16'], 'c16': [],\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n  'b1': [0, 0], 'u1': [2, 0], 'u2': [3, 0], 'u4': [4, 0], 'u8': [5, 0],\n  'i*': [1, 1], 'i1': [2, 2], 'i2': [3, 2], 'i4': [4, 2], 'i8': [5, 2],\n  'f*': [6, 1], 'bf': [7.5, 0.6], 'f2': [7.5, 1.4], 'f4': [9, 1], 'f8': [10, 1],\n  'c*': [7, 2], 'c8': [10, 2], 'c16': [11, 2],\n}\nfig, ax = plt.subplots(figsize=(8, 2.6))\nnx.draw(graph, with_labels=True, node_size=650, node_color='lightgray', pos=pos, ax=ax)\nfig.savefig('type_lattice.svg', bbox_inches='tight')\n```\n\n----------------------------------------\n\nTITLE: Implementing Ring Communication using ppermute in a JAX Loop\nDESCRIPTION: This Python function `f(x)` uses `jax.lax.fori_loop` to repeatedly apply `ppermute` (via `ppermute_start` and `ppermute_done`), effectively sending data around a ring of devices. This introduces potential aliasing issues between loop iterations because `fori_loop` typically aliases inputs and outputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  def body(i, x):\n    fut = ppermute_start(x)\n    y = ppermute_done(fut)\n    return y\n  return fori_loop(0, 8, body, x)\n```\n```\n\n----------------------------------------\n\nTITLE: Compiling FFI Shared Library via CMake - Python\nDESCRIPTION: This snippet uses shell commands in Python cells (via the '!' prefix, typically Jupyter notebook) to build and install a C++ shared library for FFI. It leverages CMake for configuring, building, and installing the foreign function handler (RmsNorm). The commands assume that all sources reside in the 'ffi' directory and outputs are placed in 'ffi/_build'. Dependencies include Python, a working CMake installation, and a compatible C++ compiler. Execution is meant for interactive notebook environments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!cmake -DCMAKE_BUILD_TYPE=Release -B ffi/_build ffi\\n!cmake --build ffi/_build\\n!cmake --install ffi/_build\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Element Indexing Mode without Padding in Python\nDESCRIPTION: This code snippet shows how to use the pl.Element indexing mode without padding. It uses the show_program_ids function to display the program IDs for a given array shape, block shape, grid, and index map.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> # element without padding\n>>> show_program_ids(x_shape=(8, 6), block_shape=(pl.Element(2), pl.Element(3)),\n...                  grid=(4, 2),\n...                  index_map=lambda i, j: (2*i, 3*j))\n    [[ 0  0  0  1  1  1]\n     [ 0  0  0  1  1  1]\n     [10 10 10 11 11 11]\n     [10 10 10 11 11 11]\n     [20 20 20 21 21 21]\n     [20 20 20 21 21 21]\n     [30 30 30 31 31 31]\n     [30 30 30 31 31 31]]\n```\n\n----------------------------------------\n\nTITLE: Division Error with Symbolic Dimensions\nDESCRIPTION: Example showing a division error when JAX cannot evenly divide a symbolic expression during a reshape operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: jnp.reshape(x, (2, -1)),\n               polymorphic_shapes=[\"(b, ...)\"])(np.ones((4, 5, 7)))\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx toctree for Pallas Design Documentation\nDESCRIPTION: Sets up a table of contents tree (toctree) in reStructuredText format for organizing Pallas design documentation. Includes configuration for caption, depth level, and document references.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :caption: Design\n   :maxdepth: 2\n\n   design\n   async_note\n```\n\n----------------------------------------\n\nTITLE: Explicit Barrier Arrival in JAX Pallas GPU (Python)\nDESCRIPTION: Signals that the current thread has arrived at a specific synchronization point defined by the 'barrier' object. This function is used in conjunction with barrier waiting mechanisms for cross-thread coordination.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n```python\nplgpu.barrier_arrive(barrier)\n```\n```\n\n----------------------------------------\n\nTITLE: Inefficient Repeated Transposes of Cursed Identity in JAX\nDESCRIPTION: This snippet demonstrates the result of repeatedly transposing the `cursed_identity` function using JAX's current mechanism. The first transpose `t(cursed_identity)` introduces an unnecessary `psum(x / 8, 'i')`. The second transpose `t(t(cursed_identity))` nests another `psum`, making the computation progressively larger and more inefficient with each transposition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Currently we get these inefficient transposes\nt(cursed_identity) = shmap(lambda x: psum(x / 8, 'i'), P(), P())\nt(t(cursed_identity)) = shmap(lambda x: psum(psum(x / 8 / 8, 'i'), 'i')), P(), P())\n...\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Bazel to Use Custom Requirements Lock File (Shell)\nDESCRIPTION: This Bazel argument forces the build system to use a specific, custom requirements lock file instead of the default ones located in the `build/` directory. The absolute path to the custom lock file is provided via the `HERMETIC_REQUIREMENTS_LOCK` environment variable. Using this disables automatic local wheel resolution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\n--repo_env=HERMETIC_REQUIREMENTS_LOCK=\"/absolute/path/to/custom_requirements_lock.txt\"\n```\n\n----------------------------------------\n\nTITLE: Displaying JAX Array\nDESCRIPTION: Demonstrates how JAX arrays can be displayed in interactive environments, showing the array representation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nx\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Basic NumPy Type Promotion Between Scalar Types\nDESCRIPTION: This code demonstrates how NumPy handles type promotion between a NumPy int8 scalar and a Python integer. When adding these values, normal promotion rules apply, resulting in an int64 dtype.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx = np.int8(0)  # int8 scalar\ny = 1  # Python int = int64 scalar\n(x + y).dtype\n```\n\n----------------------------------------\n\nTITLE: Incorrect and Correct PRNG Key Buffer Indexing (Python, Red/Green Example)\nDESCRIPTION: Compares improper direct indexing of PRNG key arrays with the correct approach utilizing random.split. The incorrect version illustrates dangerous practices leading to duplicated or correlated keys; the correct uses JAX's safe splitting. Both require JAX. Inputs/Outputs: see code. The context warns that new-style keys prevent the mistakes shown in the incorrect snippet.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Incorrect\nkey = random.PRNGKey(999)\nnew_key = random.PRNGKey(key[1])  # identical to the original key!\n```\n\nLANGUAGE: python\nCODE:\n```\n# Correct\nkey = random.PRNGKey(999)\nkey, new_key = random.split(key)\n\n```\n\n----------------------------------------\n\nTITLE: Visualizing Program IDs with `show_program_ids` in Python\nDESCRIPTION: Example call to the `show_program_ids` function with an 8x6 array, 2x3 blocks, and a 4x2 grid. The output array is populated with values like `0` (from invocation 0,0), `1` (from 0,1), `10` (from 1,0), `11` (from 1,1), etc., clearly showing which invocation wrote to which block.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> show_program_ids(x_shape=(8, 6), block_shape=(2, 3), grid=(4, 2),\n...                  index_map=lambda i, j: (i, j))\n[[ 0  0  0  1  1  1]\n [ 0  0  0  1  1  1]\n [10 10 10 11 11 11]\n [10 10 10 11 11 11]\n [20 20 20 21 21 21]\n [20 20 20 21 21 21]\n [30 30 30 31 31 31]\n [30 30 30 31 31 31]]\n\n```\n\n----------------------------------------\n\nTITLE: Using Platform Allocator in JAX with Environment Variable\nDESCRIPTION: Sets an environment variable to make JAX allocate and deallocate GPU memory on demand. This is the only configuration that actually returns memory to the GPU when no longer needed, but comes with significant performance penalties.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_memory_allocation.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nXLA_PYTHON_CLIENT_ALLOCATOR=platform\n```\n\n----------------------------------------\n\nTITLE: Using Default `block_shape` (Full Array) with `show_program_ids` in Python\nDESCRIPTION: Calls `show_program_ids` demonstrating default behaviors. Setting `block_shape=None` makes the block shape equal to the full array shape (4, 4). Setting `index_map=None` uses a default map `lambda *args: (0,) * ndim`, meaning all invocations target block (0, 0). Since the block is the full array, all invocations write to the entire array. The final output shows the value from the last invocation (grid=(2,3) -> last is (1,2) -> ID = 1*10 + 2 = 12).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> show_program_ids(x_shape=(4, 4), block_shape=None, grid=(2, 3),\n...                  index_map=None)\n[[12 12 12 12]\n [12 12 12 12]\n [12 12 12 12]\n [12 12 12 12]]\n\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Distributed Computing on TPUs\nDESCRIPTION: This code snippet imports the necessary libraries and modules for distributed computing on TPUs using JAX and Pallas. It also checks for the availability of multiple TPU devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport jax\nfrom jax import lax\nfrom jax import numpy as jnp\nfrom jax.experimental import pallas as pl\nfrom jax.experimental import shard_map\nfrom jax.experimental.pallas import tpu as pltpu\n\nP = jax.sharding.PartitionSpec\n\nnum_devices = jax.local_device_count()\nassert num_devices > 1, \"Please run this notebook with more than one device.\"\nassert \"TPU\" in jax.devices()[0].device_kind, \"Please run this notebook with TPU devices.\"\nprint(f\"Running with {num_devices} {jax.devices()[0].device_kind} devices.\")\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Asynchronous Permutation with Accumulation\nDESCRIPTION: This function combines all the learned techniques to perform asynchronous permutations in a loop while accumulating the result. It uses unrolling and proper future handling to avoid aliasing and correctness issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  out = jnp.zeros_like(x)\n  fut = (*sems, x, out) = ppermute_start(x)\n  out = out + x\n  def body(i, carry):\n    out, fut = carry\n    x = ppermute_done(fut)\n    fut = (*sems, x, out) = ppermute_start(x)\n    out = out + x\n    return out, fut\n  out, fut = fori_loop(0, 7, body, (out, fut), unroll=2)\n  return out, ppermute_done(fut)\n```\n\n----------------------------------------\n\nTITLE: Starting a TensorFlow serving Docker container\nDESCRIPTION: Command to run a Docker container with TensorFlow serving, mounting the SavedModel directory and enabling XLA CPU compilation for the JAX2TF converted model.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -p 8500:8500 -p 8501:8501 \\\n   --mount type=bind,source=${MODEL_PATH}/${MODEL}/,target=/models/${MODEL} \\\n   -e MODEL_NAME=${MODEL} -t --rm --name=serving ${DOCKER_IMAGE}  \\\n   --xla_cpu_compilation_enabled=true &\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiply-Add Using JAX NumPy Functions\nDESCRIPTION: An alternative implementation of multiply-add using jax.numpy functions that are already written in terms of JAX primitives. This example demonstrates how JAX traces function calls with both concrete and abstract arguments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nimport numpy as np\n\n@trace(\"multiply_add_numpy\")\ndef multiply_add_numpy(x, y, z):\n    return jnp.add(jnp.multiply(x, y), z)\n\n@trace(\"square_add_numpy\")\ndef square_add_numpy(a, b):\n    return multiply_add_numpy(a, a, b)\n\nprint(\"\\nNormal evaluation:\")  \nprint(\"square_add_numpy = \", square_add_numpy(2., 10.))\nprint(\"\\nGradient evaluation:\")\nprint(\"grad(square_add_numpy) = \", api.grad(square_add_numpy)(2.0, 10.))\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Dependencies with Pre-release SciPy\nDESCRIPTION: Specifies the use of pre-release SciPy package from the scientific-python-nightly-wheels index. This allows JAX to work with the latest SciPy features before their official release.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_14_ft.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n--pre\n--extra-index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple\nscipy\n```\n\n----------------------------------------\n\nTITLE: Inefficient Batch Processing with map in JAX\nDESCRIPTION: Shows a naive approach to batch processing using map, which is semantically correct but computationally inefficient as it processes one example at a time.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\npredictions = jnp.stack(list(map(partial(predict, params), input_batch)))\n```\n\n----------------------------------------\n\nTITLE: Building jaxlib for a Different Python Version\nDESCRIPTION: Command to build jaxlib for a specific Python version that differs from the system installation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython build/build.py build --wheels=jaxlib --python_version=3.12 --verbose\n```\n\n----------------------------------------\n\nTITLE: Pinned Dependencies with Hash Verification for JAX Requirements\nDESCRIPTION: A pip requirements file that specifies exact versions and hash verification for JAX dependencies. Each package includes SHA256 hashes for verifying package integrity and comments indicating which requirements file sourced the dependency.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_10.txt#2025-04-22_snippet_1\n\nLANGUAGE: pip\nCODE:\n```\nabsl-py==2.1.0 \\\n    --hash=sha256:526a04eadab8b4ee719ce68f204172ead1027549089702d99b9059f129ff1308 \\\n    --hash=sha256:7820790efbb316739cde8b4e19357243fc3608a152024288513dd968d7d959ff\n    # via -r build/test-requirements.txt\nattrs==23.2.0 \\\n    --hash=sha256:935dc3b529c262f6cf76e50877d35a4bd3c1de194fd41f47a2b7ae8f19971f30 \\\n    --hash=sha256:99b87a485a5820b23b879f04c2305b44b951b502fd64be915879d77a7e8fc6f1\n    # via hypothesis\nauditwheel==6.1.0 \\\n    --hash=sha256:3bdc686e774cf9e355e924b0fe5a562d55caa385d72234ffe7b81b378dba360f \\\n    --hash=sha256:e52f734861859e3743eb29fcac7da9c4921a1e4bea58f954b52f2926f8e9e364\n    # via -r build/test-requirements.txt\nbuild==1.2.1 \\\n    --hash=sha256:526263f4870c26f26c433545579475377b2b7588b6f1eac76a001e873ae3e19d \\\n    --hash=sha256:75e10f767a433d9a86e50d83f418e83efc18ede923ee5ff7df93b6cb0306c5d4\n    # via -r build/test-requirements.txt\ncloudpickle==3.0.0 \\\n    --hash=sha256:246ee7d0c295602a036e86369c77fecda4ab17b506496730f2f576d9016fd9c7 \\\n    --hash=sha256:996d9a482c6fb4f33c1a35335cf8afd065d2a56e973270364840712d9131a882\n    # via -r build/test-requirements.txt\ncolorama==0.4.6 \\\n    --hash=sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44 \\\n    --hash=sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6\n    # via -r build/test-requirements.txt\ncontourpy==1.2.1 \\\n    --hash=sha256:00e5388f71c1a0610e6fe56b5c44ab7ba14165cdd6d695429c5cd94021e390b2 \\\n    --hash=sha256:10a37ae557aabf2509c79715cd20b62e4c7c28b8cd62dd7d99e5ed3ce28c3fd9 \\\n    --hash=sha256:11959f0ce4a6f7b76ec578576a0b61a28bdc0696194b6347ba3f1c53827178b9 \\\n    --hash=sha256:187fa1d4c6acc06adb0fae5544c59898ad781409e61a926ac7e84b8f276dcef4 \\\n    --hash=sha256:1a07fc092a4088ee952ddae19a2b2a85757b923217b7eed584fdf25f53a6e7ce \\\n    --hash=sha256:1cac0a8f71a041aa587410424ad46dfa6a11f6149ceb219ce7dd48f6b02b87a7 \\\n    --hash=sha256:1d59e739ab0e3520e62a26c60707cc3ab0365d2f8fecea74bfe4de72dc56388f \\\n    --hash=sha256:2855c8b0b55958265e8b5888d6a615ba02883b225f2227461aa9127c578a4922 \\\n    --hash=sha256:2e785e0f2ef0d567099b9ff92cbfb958d71c2d5b9259981cd9bee81bd194c9a4 \\\n    --hash=sha256:309be79c0a354afff9ff7da4aaed7c3257e77edf6c1b448a779329431ee79d7e \\\n    --hash=sha256:39f3ecaf76cd98e802f094e0d4fbc6dc9c45a8d0c4d185f0f6c2234e14e5f75b \\\n    --hash=sha256:457499c79fa84593f22454bbd27670227874cd2ff5d6c84e60575c8b50a69619 \\\n    --hash=sha256:49e70d111fee47284d9dd867c9bb9a7058a3c617274900780c43e38d90fe1205 \\\n    --hash=sha256:4c75507d0a55378240f781599c30e7776674dbaf883a46d1c90f37e563453480 \\\n    --hash=sha256:4c863140fafc615c14a4bf4efd0f4425c02230eb8ef02784c9a156461e62c965 \\\n    --hash=sha256:4d8908b3bee1c889e547867ca4cdc54e5ab6be6d3e078556814a22457f49423c \\\n    --hash=sha256:5b9eb0ca724a241683c9685a484da9d35c872fd42756574a7cfbf58af26677fd \\\n    --hash=sha256:6022cecf8f44e36af10bd9118ca71f371078b4c168b6e0fab43d4a889985dbb5 \\\n    --hash=sha256:6150ffa5c767bc6332df27157d95442c379b7dce3a38dff89c0f39b63275696f \\\n    --hash=sha256:62828cada4a2b850dbef89c81f5a33741898b305db244904de418cc957ff05dc \\\n    --hash=sha256:7b4182299f251060996af5249c286bae9361fa8c6a9cda5efc29fe8bfd6062ec \\\n    --hash=sha256:94b34f32646ca0414237168d68a9157cb3889f06b096612afdd296003fdd32fd \\\n    --hash=sha256:9ce6889abac9a42afd07a562c2d6d4b2b7134f83f18571d859b25624a331c90b \\\n    --hash=sha256:9cffe0f850e89d7c0012a1fb8730f75edd4320a0a731ed0c183904fe6ecfc3a9 \\\n    --hash=sha256:a12a813949e5066148712a0626895c26b2578874e4cc63160bb007e6df3436fe \\\n    --hash=sha256:a1eea9aecf761c661d096d39ed9026574de8adb2ae1c5bd7b33558af884fb2ce \\\n    --hash=sha256:a31f94983fecbac95e58388210427d68cd30fe8a36927980fab9c20062645609 \\\n    --hash=sha256:ac58bdee53cbeba2ecad824fa8159493f0bf3b8ea4e93feb06c9a465d6c87da8 \\\n    --hash=sha256:af3f4485884750dddd9c25cb7e3915d83c2db92488b38ccb77dd594eac84c4a0 \\\n    --hash=sha256:b33d2bc4f69caedcd0a275329eb2198f560b325605810895627be5d4b876bf7f \\\n    --hash=sha256:b59c0ffceff8d4d3996a45f2bb6f4c207f94684a96bf3d9728dbb77428dd8cb8 \\\n    --hash=sha256:bb6834cbd983b19f06908b45bfc2dad6ac9479ae04abe923a275b5f48f1a186b \\\n    --hash=sha256:bd3db01f59fdcbce5b22afad19e390260d6d0222f35a1023d9adc5690a889364 \\\n    --hash=sha256:bd7c23df857d488f418439686d3b10ae2fbf9bc256cd045b37a8c16575ea1040 \\\n    --hash=sha256:c2528d60e398c7c4c799d56f907664673a807635b857df18f7ae64d3e6ce2d9f \\\n    --hash=sha256:d31a63bc6e6d87f77d71e1abbd7387ab817a66733734883d1fc0021ed9bfa083 \\\n    --hash=sha256:d4492d82b3bc7fbb7e3610747b159869468079fe149ec5c4d771fa1f614a14df \\\n    --hash=sha256:ddcb8581510311e13421b1f544403c16e901c4e8f09083c881fab2be80ee31ba \\\n    --hash=sha256:e1d59258c3c67c865435d8fbeb35f8c59b8bef3d6f46c1f29f6123556af28445 \\\n    --hash=sha256:eb3315a8a236ee19b6df481fc5f997436e8ade24a9f03dfdc6bd490fea20c6da \\\n    --hash=sha256:ef2b055471c0eb466033760a521efb9d8a32b99ab907fc8358481a1dd29e3bd3 \\\n    --hash=sha256:ef5adb9a3b1d0c645ff694f9bca7702ec2c70f4d734f9922ea34de02294fdf72 \\\n    --hash=sha256:f32c38afb74bd98ce26de7cc74a67b40afb7b05aae7b42924ea990d51e4dac02 \\\n    --hash=sha256:fe0ccca550bb8e5abc22f530ec0466136379c01321fd94f30a22231e8a48d985\n    # via matplotlib\ncycler==0.12.1 \\\n    --hash=sha256:85cef7cff222d8644161529808465972e51340599459b8ac3ccbac5a854e0d30 \\\n    --hash=sha256:88bb128f02ba341da8ef447245a9e138fae777f6a23943da4540077d3601eb1c\n    # via matplotlib\netils[epath,epy]==1.7.0 \\\n    --hash=sha256:61af8f7c242171de15e22e5da02d527cb9e677d11f8bcafe18fcc3548eee3e60 \\\n    --hash=sha256:97b68fd25e185683215286ef3a54e38199b6245f5fe8be6bedc1189be4256350\n    # via -r build/requirements.in\nexceptiongroup==1.2.1 \\\n    --hash=sha256:5258b9ed329c5bbdd31a309f53cbfb0b155341807f6ff7606a1e801a891b29ad \\\n    --hash=sha256:a4785e48b045528f5bfe627b6ad554ff32def154f42372786903b7abcfe1aa16\n    # via\n    #   hypothesis\n    #   pytest\nexecnet==2.1.1 \\\n    --hash=sha256:26dee51f1b80cebd6d0ca8e74dd8745419761d3bef34163928cbebbdc4749fdc \\\n    --hash=sha256:5189b52c6121c24feae288166ab41b32549c7e2348652736540b9e6e7d4e72e3\n    # via pytest-xdist\nfilelock==3.14.0 \\\n    --hash=sha256:43339835842f110ca7ae60f1e1c160714c5a6afd15a2873419ab185334975c0f \\\n    --hash=sha256:6ea72da3be9b8c82afd3edcf99f2fffbb5076335a5ae4d03248bb5b6c3eae78a\n    # via -r build/test-requirements.txt\nflatbuffers==24.3.25 \\\n    --hash=sha256:8dbdec58f935f3765e4f7f3cf635ac3a77f83568138d6a2311f524ec96364812 \\\n    --hash=sha256:de2ec5b203f21441716617f38443e0a8ebf3d25bf0d9c0bb0ce68fa00ad546a4\n    # via -r build/test-requirements.txt\nfonttools==4.51.0 \\\n    --hash=sha256:0118ef998a0699a96c7b28457f15546815015a2710a1b23a7bf6c1be60c01636 \\\n    --hash=sha256:0d145976194a5242fdd22df18a1b451481a88071feadf251221af110ca8f00ce \\\n    --hash=sha256:0e19bd9e9964a09cd2433a4b100ca7f34e34731e0758e13ba9a1ed6e5468cc0f \\\n    --hash=sha256:0f08c901d3866a8905363619e3741c33f0a83a680d92a9f0e575985c2634fcc1 \\\n    --hash=sha256:1250e818b5f8a679ad79660855528120a8f0288f8f30ec88b83db51515411fcc \\\n    --hash=sha256:15c94eeef6b095831067f72c825eb0e2d48bb4cea0647c1b05c981ecba2bf39f \\\n    --hash=sha256:1621ee57da887c17312acc4b0e7ac30d3a4fb0fec6174b2e3754a74c26bbed1e \\\n    --hash=sha256:180194c7fe60c989bb627d7ed5011f2bef1c4d36ecf3ec64daec8302f1ae0716 \\\n    --hash=sha256:278e50f6b003c6aed19bae2242b364e575bcb16304b53f2b64f6551b9c000e15 \\\n    --hash=sha256:32b17504696f605e9e960647c5f64b35704782a502cc26a37b800b4d69ff3c77 \\\n    --hash=sha256:3bee3f3bd9fa1d5ee616ccfd13b27ca605c2b4270e45715bd2883e9504735034 \\\n    --hash=sha256:4060acc2bfa2d8e98117828a238889f13b6f69d59f4f2d5857eece5277b829ba \\\n    --hash=sha256:54dcf21a2f2d06ded676e3c3f9f74b2bafded3a8ff12f0983160b13e9f2fb4a7 \\\n    --hash=sha256:56fc244f2585d6c00b9bcc59e6593e646cf095a96fe68d62cd4da53dd1287b55 \\\n    --hash=sha256:599bdb75e220241cedc6faebfafedd7670335d2e29620d207dd0378a4e9ccc5a \\\n    --hash=sha256:5f6bc991d1610f5c3bbe997b0233cbc234b8e82fa99fc0b2932dc1ca5e5afec0 \\\n    --hash=sha256:60a3409c9112aec02d5fb546f557bca6efa773dcb32ac147c6baf5f742e6258b \\\n    --hash=sha256:68b3fb7775a923be73e739f92f7e8a72725fd333eab24834041365d2278c3671 \\\n    --hash=sha256:76f1777d8b3386479ffb4a282e74318e730014d86ce60f016908d9801af9ca2a \\\n    --hash=sha256:806e7912c32a657fa39d2d6eb1d3012d35f841387c8fc6cf349ed70b7c340039 \\\n    --hash=sha256:84d7751f4468dd8cdd03ddada18b8b0857a5beec80bce9f435742abc9a851a74 \\\n    --hash=sha256:865a58b6e60b0938874af0968cd0553bcd88e0b2cb6e588727117bd099eef836 \\\n    --hash=sha256:8ac27f436e8af7779f0bb4d5425aa3535270494d3bc5459ed27de3f03151e4c2 \\\n    --hash=sha256:8b4850fa2ef2cfbc1d1f689bc159ef0f45d8d83298c1425838095bf53ef46308 \\\n    --hash=sha256:8b5ad456813d93b9c4b7ee55302208db2b45324315129d85275c01f5cb7e61a2 \\\n    --hash=sha256:8e2f1a4499e3b5ee82c19b5ee57f0294673125c65b0a1ff3764ea1f9db2f9ef5 \\\n    --hash=sha256:9696fe9f3f0c32e9a321d5268208a7cc9205a52f99b89479d1b035ed54c923f1 \\\n    --hash=sha256:96a48e137c36be55e68845fc4284533bda2980f8d6f835e26bca79d7e2006438 \\\n    --hash=sha256:a8feca65bab31479d795b0d16c9a9852902e3a3c0630678efb0b2b7941ea9c74 \\\n    --hash=sha256:aefa011207ed36cd280babfaa8510b8176f1a77261833e895a9d96e57e44802f \\\n    --hash=sha256:b2b92381f37b39ba2fc98c3a45a9d6383bfc9916a87d66ccb6553f7bdd129097 \\\n    --hash=sha256:b3c61423f22165541b9403ee39874dcae84cd57a9078b82e1dce8cb06b07fa2e \\\n    --hash=sha256:b5b48a1121117047d82695d276c2af2ee3a24ffe0f502ed581acc2673ecf1037 \\\n    --hash=sha256:c18b49adc721a7d0b8dfe7c3130c89b8704baf599fb396396d07d4aa69b824a1 \\\n    --hash=sha256:c5b8cab0c137ca229433570151b5c1fc6af212680b58b15abd797dcdd9dd5051 \\\n    --hash=sha256:c7e91abdfae1b5c9e3a543f48ce96013f9a08c6c9668f1e6be0beabf0a569c1b \\\n    --hash=sha256:cadf4e12a608ef1d13e039864f484c8a968840afa0258b0b843a0556497ea9ed \\\n    --hash=sha256:dc0673361331566d7a663d7ce0f6fdcbfbdc1f59c6e3ed1165ad7202ca183c68 \\\n    --hash=sha256:de7c29bdbdd35811f14493ffd2534b88f0ce1b9065316433b22d63ca1cd21f14 \\\n    --hash=sha256:e9d9298be7a05bb4801f558522adbe2feea1b0b103d5294ebf24a92dd49b78e5 \\\n    --hash=sha256:ee1af4be1c5afe4c96ca23badd368d8dc75f611887fb0c0dac9f71ee5d6f110e\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Version and Hash\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and SHA256 hashes for security. It includes examples for matplotlib, numpy, and NVIDIA CUDA libraries.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_10.txt#2025-04-22_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nmatplotlib==3.8.4 ; python_version == \"3.10\" \\\n    --hash=sha256:1c13f041a7178f9780fb61cc3a2b10423d5e125480e4be51beaf62b172413b67 \\\n    --hash=sha256:232ce322bfd020a434caaffbd9a95333f7c2491e59cfc014041d95e38ab90d1c \\\n    --hash=sha256:493e9f6aa5819156b58fce42b296ea31969f2aab71c5b680b4ea7a3cb5c07d94\n```\n\n----------------------------------------\n\nTITLE: Querying Current Mesh in JAX Python\nDESCRIPTION: Shows how to query the current mesh using get_abstract_mesh() to determine the current sharding mode.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Current mesh is: {get_abstract_mesh()}\")\n```\n\n----------------------------------------\n\nTITLE: Defining shmap with psum and Mapped Output in JAX\nDESCRIPTION: This snippet defines a JAX function `f2` using `shmap`. It takes two inputs `x` and `y`, both sharded along axis 'i'. It computes `psum(g(x), 'i')` and multiplies it by `y`. The result is returned as a mapped output, also sharded along 'i' (`out_specs=P('i')`). This contrasts with Example 1, having a mapped output.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Example 2: shmap involving psum and *mapped* output with efficient transpose\nf2 = shmap(lambda x, y: psum(g(x), 'i') * y,\n          in_specs=(P('i'), P('i')), out_specs=P('i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Output Shape and Grid Specifications for TPU Collective Operations\nDESCRIPTION: Defines the output shape structure and grid specifications for TPU operations, including memory space allocation and semaphore configuration for synchronization between devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nout_shape = (\n    jax.ShapeDtypeStruct(\n        (outer_block_size[0], outer_block_size[1]), jnp.float32\n    ),\n    # Shape: [working/recv, block[0], block[1]]\n    jax.ShapeDtypeStruct(\n        (2, outer_block_size[0], outer_block_size[1]), jnp.float32\n    ),  # hbm_scratch\n)\n\ngrid_spec = pltpu.PrefetchScalarGridSpec(\n    num_scalar_prefetch=0,\n    in_specs=[\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n    ],\n    out_specs=[\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n    ],\n    grid=(num_devices, 2),\n    scratch_shapes=(\n        [pltpu.SemaphoreType.DMA] * 5\n        + [pltpu.SemaphoreType.REGULAR] * 2  # Capacity semaphores\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Stateful Global PRNG Model in Python\nDESCRIPTION: This snippet demonstrates a stateful global PRNG model, similar to patterns often used in NumPy. It highlights the challenges this model presents for JAX, particularly regarding reproducibility (dependent on evaluation order of `bar` and `baz`), parallelizability (due to shared state `RNG`), and invariance across compilation boundaries. The global state makes it difficult to fit within JAX's functional and parallel execution model.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef foo(): return bar() + baz()\ndef bar(): return rand(RNG, (3, 4))\ndef baz(): return rand(RNG, (3, 4))\ndef main():\n  global RNG\n  RNG = RandomState(0)\n  return foo()\n```\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Setting Up Environment\nDESCRIPTION: Imports necessary libraries and configures JAX to use 8 CPU devices for demonstration purposes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec as P, AxisType, set_mesh, get_abstract_mesh\nfrom jax.experimental.shard import reshard, auto_axes, explicit_axes\n\njax.config.update('jax_num_cpu_devices', 8)\n```\n\n----------------------------------------\n\nTITLE: Using Shard Map to Fix Compilation Cache Issues in JAX\nDESCRIPTION: Implementation of a workaround that uses shard_map to wrap the custom_partitioning primitive (LayerNorm), ensuring consistent cache keys during compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax.experimental.shard_map import shard_map\n\ndef G(x1, x2, gamma, beta, mesh, ispecs, ospecs):\n   ln_out = shard_map(LayerNorm, mesh, in_specs=ispecs, out_specs=ospecs, check_rep=False)(x1, x2, gamma, beta)\n   return ln_out @ x2\n\nispecs = jax.sharding.PartitionSpec(...)\nospecs = jax.sharding.PartitionSpec(...)\nmesh = jax.sharding.Mesh(...)\nlayernorm_matmul_with_shard_map = jax.jit(G, static_argnames=['mesh', 'ispecs', 'ospecs'])(x1, x2, gamma, beta, mesh, ispecs, ospecs)\n```\n\n----------------------------------------\n\nTITLE: Running mypy for Type Checking in JAX (Bash)\nDESCRIPTION: Commands to install pre-commit and run mypy for type checking in the JAX project. Includes options for checking all files or only modified files.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_47\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit run mypy --all-files\n```\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run mypy\n```\n\n----------------------------------------\n\nTITLE: Importing JAX Parallel Mapping Module\nDESCRIPTION: Imports the pmap function from JAX, which is used for parallel execution across multiple devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import pmap\n```\n\n----------------------------------------\n\nTITLE: Error with Static Arguments and Type-Only Structures in JAX\nDESCRIPTION: Shows how using ShapeDtypeStruct for static arguments raises an error since static arguments need concrete values for tracing, while demonstrating a working example with concrete static value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/aot.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> jax.jit(f, static_argnums=0).trace(i32_scalar, i32_scalar)  # doctest: +SKIP\nTraceback (most recent call last):\nTypeError: unsupported operand type(s) for *: 'int' and 'ShapeDtypeStruct'\n\n>>> jax.jit(f, static_argnums=0).trace(10, i32_scalar).lower().compile()(5)\nArray(25, dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Registering CUDA FFI Target for JAX (Python)\nDESCRIPTION: This Python code registers a CUDA-based FFI implementation ('rms_norm_cuda') with JAX, linking the front end to a Python-wrapped CUDA operation (presumably built via PyCapsule or similar means). Required dependencies include 'jax', 'rms_norm_lib_cuda', and a valid CUDA runtime. Inputs include the FFI operation name and Python object. Outputs enable the JAX runtime to dispatch FFI calls to the GPU implementation. Platform must be set to 'CUDA'.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\njax.ffi.register_ffi_target(\n  \"rms_norm_cuda\", rms_norm_lib_cuda.rms_norm(), platform=\"CUDA\"\n)\n```\n\n----------------------------------------\n\nTITLE: Reinstalling TensorFlow to Resolve TensorBoard Conflicts\nDESCRIPTION: These pip commands uninstall conflicting TensorFlow and TensorBoard packages, then reinstall a single version to resolve duplicate plugin errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npip uninstall tensorflow tf-nightly tensorboard tb-nightly\npip install tensorflow\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with SHA-256 Hashes\nDESCRIPTION: A list of Python package dependencies with their corresponding SHA-256 hashes for the jax-ml/jax project. The dependencies are referenced from a non-free threading requirements file, with setuptools specifically marked as unsafe.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_11.txt#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:4ac59d5d6910b220141c1737b79d4a5aa9e57466e7469a012ed42ce2d3995e88 \\\n    --hash=sha256:53866a9d8ab363271c9e80c7c2e9441814961d47f88c9bc3b248142c32141d94 \\\n    --hash=sha256:589402548251056878d2e7c8859286eb91bd841af117dbe4ab000e6450987e08 \\\n    --hash=sha256:68953dc84b244b053c0d5f137a21ae8287ecf51b20872eccf8eaac0302d3e3b0 \\\n    --hash=sha256:6c25b8eb733d4e741246151d895dd0308137532737f337411160ff69ca24f93a \\\n    --hash=sha256:7034d381789f45576ec3f1fa0e15d741828146439228dc3f7c59856c5bcd3292 \\\n    --hash=sha256:73a1d6bd01961e9fd447162e137ed949c01bdb830dfca487c4a14e9742dccc93 \\\n    --hash=sha256:8226a33c542bcb54cd6bd0a366067b610b41713b64c9abec1bc4533d69f51e70 \\\n    --hash=sha256:888196c9c8893a1e8ff5e89b8f894e7f4f0e64a5af4d8f3c410f0319128bb2f8 \\\n    --hash=sha256:88c5b4b47a8a138338a07fc94e2ba3b1535f69247670abfe422de4e0b344aae2 \\\n    --hash=sha256:8a1b2effa96a5f019e72874969394edd393e2fbd6414a8208fea363a22803b45 \\\n    --hash=sha256:93e1856c8313bc688d5df069e106a4bc962eef3d13372020cc6e3ebf5e045202 \\\n    --hash=sha256:9501f36fac6b875c124243a379267d879262480bf85b1dbda61f5ad4d01b75a3 \\\n    --hash=sha256:959665072bd60f45c5b6b5d711f15bdefc9849dd5da9fb6c873e35f5d34d8cfb \\\n    --hash=sha256:a1d67d0d53d2a138f9e29d8acdabe11310c185e36f0a848efa104d4e40b808e4 \\\n    --hash=sha256:a493d470183ee620a3df1e6e55b3e4de8143c0ba1b16f3ded83208ea8ddfd91d \\\n    --hash=sha256:a7ccf5825fd71d4542c8ab28d4d482aace885f5ebe4b40faaa290eed8e095a4c \\\n    --hash=sha256:a88b7df61a292603e7cd662d92565d915796b094ffb3d206579aaebac6b85d5f \\\n    --hash=sha256:a97079b955b00b732c6f280d5023e0eefe359045e8b83b08cf0333af9ec78f26 \\\n    --hash=sha256:d22fdef58976457c65e2796e6730a3ea4a254f3ba83777ecfc8592ff8d77d303 \\\n    --hash=sha256:d75f693bb4e92c335e0645e8845e553cd09dc91616412d1d4650da835b5449df \\\n    --hash=sha256:d8593f8464fb64d58e8cb0b905b272d40184eac9a18d83cf8c10749c3eafcd7e \\\n    --hash=sha256:d8fff0f0c1d8bc5d866762ae95bd99d53282337af1be9dc0d88506b340e74b73 \\\n    --hash=sha256:de20a212ef3d00d609d0b22eb7cc798d5a69035e81839f549b538eff4105d01c \\\n    --hash=sha256:e9e9d4e2e336c529d4c435baad846a181e39a982f823f7e4495ec0b0ec8538d2 \\\n    --hash=sha256:f058a77ef0ece4e210bb0450e68408d4223f728b109764676e1a13537d056bb0 \\\n    --hash=sha256:f1a4b358947a65b94e2501ce3e078bbc929b039ede4679ddb0460829b12f7375 \\\n    --hash=sha256:f9b2cde1cd1b2a10246dbc143ba49d942d14fb3d2b4bccf4618d475c65464912 \\\n    --hash=sha256:fe3390c538f12437b859d815040763abc728955a52ca6ff9c5d4ac707c4ad98e\n    # via -r build/nonfreethreading-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Triggering Debug Trace in Gradient Computation\nDESCRIPTION: Shows how the debug trace is triggered during gradient computation. The example demonstrates the interaction with the debugger during the backward pass.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_56\n\nLANGUAGE: python\nCODE:\n```\njax.grad(foo)(3.)\n\n> <ipython-input-113-b19a2dc1abf7>(12)debug_bwd()\n-> return g\n(Pdb) p x\nArray(9., dtype=float32)\n(Pdb) p g\nArray(-0.91113025, dtype=float32)\n(Pdb) q\n```\n\n----------------------------------------\n\nTITLE: Installing pprof via Go\nDESCRIPTION: Command to install the pprof profiling tool using Go package manager. Requires Go 1.16+ and Graphviz as prerequisites.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/device_memory_profiling.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngo install github.com/google/pprof@latest\n```\n\n----------------------------------------\n\nTITLE: FFI Partitioning Suboptimality Inspection Using Compiled HLO (Python)\nDESCRIPTION: This snippet shows how to inspect the compiled HLO for a sharded FFI-backed function in JAX. It reveals that FFI partitioning is not optimal out-of-the-box: JAX/XLA replicates data before running the FFI and then slices the outputs. Dependencies: JAX, multi-device environment, previously defined FFI functions. Inputs are sharded device arrays. Outputs are printed HLO sections to demonstrate data movement and execution patterns. This highlights areas for further optimization using custom partitioning tools.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\noutput = jax.jit(rms_norm, out_shardings=batch_shd)(x_batch_shd)\nnp.testing.assert_allclose(output, rms_norm_ref(x), rtol=1e-5)\n```\n\nLANGUAGE: python\nCODE:\n```\nhlo = jax.jit(rms_norm, out_shardings=batch_shd).lower(x_batch_shd).compile().as_text().strip()\nprint(hlo.split(\"\\n\\n\")[-1])\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation Structure for JAX Extensions\nDESCRIPTION: ReStructuredText (RST) markup defining the structure of JAX extension documentation, including toctree directives for organizing content about custom interpreters and library building guides.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/extensions.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _extensions:\n\nExtension guides\n================\n\nGuides for extending JAX's capabilities, and for building libraries\nthat use or interface with JAX.\n\n.. toctree::\n   :caption: Extensible JAX internals\n   :maxdepth: 1\n\n   notebooks/Writing_custom_interpreters_in_Jax\n   jax.extend\n\n.. toctree::\n   :caption: Libraries and extensions\n   :maxdepth: 1\n\n   building_on_jax\n```\n\n----------------------------------------\n\nTITLE: Handling Shape Compatibility with Equality Constraints\nDESCRIPTION: Example of a function that fails due to broadcasting shape incompatibility when using symbolic dimensions with floor division.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x, y: x + y[:y.shape[0] // 2],\n               polymorphic_shapes=(\"a\", \"b\"))(x, y)\n```\n\n----------------------------------------\n\nTITLE: Extracting Output Info from Lowered Stage - Python\nDESCRIPTION: This snippet describes how to use the '.out_info' property of 'jax.stages.Lowered' to obtain output information such as tree structure, shape, and dtype. This replaces direct uses of the deprecated 'jax.xla_computation'. Pre-requisite: use JAX 0.4.30 or above and replace any code retrieving output info via XLA computation APIs. The main inputs are a lowered stage object; the output is metadata about the computation outputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\njax.jit(fn).lower(*args, **kwargs).out_info\n```\n\n----------------------------------------\n\nTITLE: Double Precision Random Number Generation\nDESCRIPTION: Example showing JAX's default single-precision behavior when generating random numbers, even when explicitly requesting double precision.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nx = random.uniform(random.key(0), (1000,), dtype=jnp.float64)\nx.dtype\n```\n\n----------------------------------------\n\nTITLE: Triggering Escaped Tracer Error in JAX\nDESCRIPTION: Demonstrates how using an escaped tracer leads to an error when attempting to use it in computations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrandom.normal(key, ())\n```\n\n----------------------------------------\n\nTITLE: Demonstrating `jax.jit` Failure with Stateful Methods in Python\nDESCRIPTION: Attempts to apply `jax.jit` to the `count` method of the stateful `Counter` class. The output shows that the counter does not increment as expected because the side effect (`self.n += 1`) is only executed during the initial tracing phase of JIT compilation, not on subsequent calls. This highlights the incompatibility of JAX transformations with functions having side effects.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/stateful-computations.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncounter.reset()\nfast_count = jax.jit(counter.count)\n\nfor _ in range(3):\n  print(fast_count())\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Multiple Invocations Writing to the Same Output Block in Python\nDESCRIPTION: Calls `show_program_ids` with a 3D grid (4, 2, 10) but an `index_map` (`lambda i, j, k: (i, j)`) that only depends on the first two grid dimensions. This causes 10 different invocations (indexed by `k` from 0 to 9) to write to the same output blocks. The output shows the result of the *last* invocation (where k=9) for each block when run sequentially (`interpret=True`), highlighting that concurrent writes have platform-dependent behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> show_program_ids(x_shape=(8, 6), block_shape=(2, 3), grid=(4, 2, 10),\n...                  index_map=lambda i, j, k: (i, j))\n[[  9   9   9  19  19  19]\n [  9   9   9  19  19  19]\n [109 109 109 119 119 119]\n [109 109 109 119 119 119]\n [209 209 209 219 219 219]\n [209 209 209 219 219 219]\n [309 309 309 319 319 319]\n [309 309 309 319 319 319]]\n\n```\n\n----------------------------------------\n\nTITLE: Defining Sharding and Block Size for Reduce-Scatter with Large Blocks\nDESCRIPTION: Setup code for the reduce-scatter operation with large HBM blocks, defining the partition, mesh, sharding, and outer block size that is too large for VMEM placement.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npartition = P(None, 'x')\nmesh = jax.make_mesh((num_devices,), ('x',))\nsharding = jax.sharding.NamedSharding(mesh, partition)\n\n# We pick a large outer kernel block size that we do not want to place\n# in VMEM. For pedagogical purposes we use (4096, 4096), although in\n# principle this can be much larger.\nouter_block_size = (4096, 4096)\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Creating Random Arrays\nDESCRIPTION: This snippet imports JAX and its NumPy-like module, creates a random key, and generates a random normal array. It demonstrates basic array operations in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nkey = random.key(0)\nkey, subkey = random.split(key)\nx = random.normal(key, (5000, 5000))\n\nprint(x.shape)\nprint(x.dtype)\n```\n\n----------------------------------------\n\nTITLE: Implementing XlaScatter using TensorFlow's tensor_scatter_nd_update in Python\nDESCRIPTION: The XlaScatter operation is implemented using TensorFlow's tensor_scatter_nd_update function. It supports various scatter operations for unique indices and single depth scatters for non-unique indices. There are limitations on supported data types and scatter modes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/no_xla_limitations.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntf.tensor_scatter_nd_update\n```\n\n----------------------------------------\n\nTITLE: Plotting JAX Array Data with Matplotlib\nDESCRIPTION: Shows how JAX arrays can be used with matplotlib for visualization, plotting the first row of the array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nplt.plot(x[0])\n```\n\n----------------------------------------\n\nTITLE: Demonstrating ineffective use of jax.checkpoint in JAX\nDESCRIPTION: These snippets illustrate cases where applying jax.checkpoint doesn't provide memory savings. They show applying checkpoint to the entire function and to the last stage of a composed function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef f_grad_bad(x):\n  _ = f(x)                  # step 1\n  _, f_vjp = jax.vjp(f, x)  # step 2\n  x_bar, = f_vjp(1.0)       # step 3\n  return x_bar\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f_grad_bad2(x):\n  y, g_vjp = jax.vjp(g, x)  # step 1\n  z = h(y)                  # step 2\n  _, h_vjp = jax.vjp(h, y)  # step 3\n  y_bar, = h_vjp(1.0)       # step 3\n  x_bar, = g_vjp(y_bar)     # step 5\n  return x_bar\n```\n\n----------------------------------------\n\nTITLE: Configuring Bazel to Use Local Custom Python Archive (Shell)\nDESCRIPTION: These Bazel command-line arguments configure the build to use a custom Python interpreter provided as a local tarball archive. The `HERMETIC_PYTHON_URL` points to the file path, and `HERMETIC_PYTHON_SHA256` provides its checksum for verification.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n--repo_env=HERMETIC_PYTHON_URL=\"file:///local/path/to/my_python.tgz\"\n--repo_env=HERMETIC_PYTHON_SHA256=<file's_sha256_sum>\n```\n\n----------------------------------------\n\nTITLE: Using custom_vjp with non-differentiable function argument\nDESCRIPTION: Shows how to call the custom_vjp function with a lambda function as a non-differentiable argument. The example applies a quadratic function to the input value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_70\n\nLANGUAGE: python\nCODE:\n```\nprint(app(lambda x: x ** 2, 4.))\n```\n\n----------------------------------------\n\nTITLE: Pretty-printing Jaxpr Graphs with Custom Indentation (Python)\nDESCRIPTION: Defines a helper PPrint class for indented pretty-printing of program representations (Jaxprs), along with related utility functions for formatting variables, equations, and output signatures. The printable view leverages pretty-print rules and allows for visually nested structures, aiding debugging and inspection of traced computations. Expected inputs are Jaxpr objects, and output is a user-friendly multiline string. Depends on features like string formatting, itertools, and default dictionaries.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\nimport string\n\nclass PPrint:\n  lines: list[tuple[int, str]]\n\n  def __init__(self, lines):\n    self.lines = lines\n\n  def indent(self, indent: int) -> 'PPrint':\n    return PPrint([(indent + orig_indent, s) for orig_indent, s in self.lines])\n\n  def __add__(self, rhs: 'PPrint') -> 'PPrint':\n    return PPrint(self.lines + rhs.lines)\n\n  def __rshift__(self, rhs: 'PPrint') -> 'PPrint':\n    if not rhs.lines: return self\n    if not self.lines: return rhs\n    indent, s = self.lines[-1]\n    indented_block = rhs.indent(indent + len(s))\n    common_line = s + ' ' * rhs.lines[0][0] + rhs.lines[0][1]\n    return PPrint(self.lines[:-1]\n                  + [(indent, common_line)]\n                  + indented_block.lines[1:])\n\n  def __str__(self) -> str:\n    return '\\n'.join(' ' * indent + s for indent, s in self.lines)\n\ndef pp(s: Any) -> PPrint:\n  return PPrint([(0, line) for line in str(s).splitlines()])\n\ndef vcat(ps: list[PPrint]) -> PPrint:\n  return sum(ps, pp(''))\n\ndef pp_jaxpr(jaxpr: Jaxpr) -> PPrint:\n  namegen = (''.join(s) for r in it.count(1)\n             for s in it.permutations(string.ascii_lowercase, r))\n  names = defaultdict(lambda: next(namegen))\n  in_binders = ', '.join(var_str(names, x) for x in jaxpr.in_binders)\n  eqns = vcat([pp_eqn(names, e) for e in jaxpr.eqns])\n  outs = ', '.join(names[v] if isinstance(v, Var) else str(v.val)\n                   for v in jaxpr.outs)\n  return (pp(f'{{ lambda {in_binders} .') +\n          ((pp('let ') >> eqns) + pp(f'in ( {outs} ) }}')).indent(2))\n\ndef var_str(names: defaultdict[Var, str], v: Var) -> str:\n  return f'{names[v]}:{v.aval.str_short()}'\n\ndef pp_eqn(names: defaultdict[Var, str], eqn: JaxprEqn) -> PPrint:\n  rule = pp_rules.get(eqn.primitive)\n  if rule:\n    return rule(names, eqn)\n  else:\n    lhs = pp(' '.join(var_str(names, v) for v in eqn.out_binders))\n    rhs = (pp(eqn.primitive.name) >> pp_params(eqn.params) >>\n           pp(' '.join(names[x] if isinstance(x, Var) else str(x.val)\n                       for x in eqn.inputs)))\n    return lhs >> pp(' = ') >> rhs\n\ndef pp_params(params: dict[str, Any]) -> PPrint:\n  items = sorted(params.items())\n  if items:\n    return pp(' [ ') >> vcat([pp(f'{k}={v}') for k, v in items]) >> pp(' ] ')\n  else:\n    return pp(' ')\n\nJaxpr.__repr__ = lambda self: str(pp_jaxpr(self))\npp_rules: dict[Primitive, Callable[..., PPrint]] = {}\n```\n\n----------------------------------------\n\nTITLE: Comparing Custom Pallas Implementation with XLA's Built-in Reduce-Scatter in Python\nDESCRIPTION: Implements a reference version using JAX's built-in lax.psum_scatter operation and compares the results with the custom Pallas implementation to verify correctness. It prints shapes and sample values from both implementations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Compare our result to XLA.\ndef lax_reduce_sum_scatter(x):\n  x = x.reshape(num_devices, block_size[0], block_size[1])\n  return lax.psum_scatter(x, 'x')\n\n\nxla_result = jax.jit(\n    shard_map.shard_map(\n        lax_reduce_sum_scatter,\n        mesh=mesh,\n        in_specs=P(None, 'x'),\n        out_specs=P('x', None),\n    )\n)(input_arr)\n\nprint('Input:', input_arr.shape, input_arr[::4, 0])\nprint('Pallas Result:', pallas_result.shape, pallas_result[::4, 0])\nprint('lax.psum_scatter Result:', xla_result.shape, xla_result[::4, 0])\nprint(\n    'Difference |Pallas - lax.psum_scatter|:',\n    jnp.max(jnp.abs(pallas_result - xla_result)),\n)\n```\n\n----------------------------------------\n\nTITLE: Applying `static_argnames` with JIT Decorator using `functools.partial` in Python\nDESCRIPTION: Illustrates a common pattern for applying `jax.jit` as a decorator while specifying static arguments. It uses `functools.partial` to create a partial function object from `jax.jit` with `static_argnames=['n']` pre-filled, which is then used as the decorator for `g_jit_decorated`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jit-compilation.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['n'])\ndef g_jit_decorated(x, n):\n  i = 0\n  while i < n:\n    i += 1\n  return x + i\n\nprint(g_jit_decorated(10, 20))\n```\n\n----------------------------------------\n\nTITLE: Building jaxlib with Local CUDA/CUDNN/NCCL Paths\nDESCRIPTION: Command to build jaxlib using CUDA/CUDNN/NCCL redistributions from local file system paths.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython build/build.py build --wheels=jax-cuda-plugin \\\n--bazel_options=--repo_env=LOCAL_CUDA_PATH=\"/foo/bar/nvidia/cuda\" \\\n--bazel_options=--repo_env=LOCAL_CUDNN_PATH=\"/foo/bar/nvidia/cudnn\" \\\n--bazel_options=--repo_env=LOCAL_NCCL_PATH=\"/foo/bar/nvidia/nccl\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Function with Custom Partitioning Primitive in JAX\nDESCRIPTION: Example function that uses a custom partitioning primitive (LayerNorm) followed by matrix multiplication, which would cause compilation cache issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n\ndef F(x1, x2, gamma, beta):\n   ln_out = LayerNorm(x1, gamma, beta)\n   return ln_out @ x2\n```\n\n----------------------------------------\n\nTITLE: Creating Random Input Data for JIT Compilation\nDESCRIPTION: Initializes a large random matrix that will be used to demonstrate the performance benefits of just-in-time compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nkey = random.key(0)\nx = random.normal(key, (5000, 5000))\n```\n\n----------------------------------------\n\nTITLE: Illustrating Sequential Execution in Standard Python\nDESCRIPTION: Defines two simple Python functions, `f` and `g`, each printing a string. Calling them sequentially (`f()` then `g()`) reliably prints \"hello\" followed by \"world\", illustrating the expected single-threaded execution order. This serves as a baseline comparison for JAX behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef f():\n  print(\"hello\")\n  return 2\ndef g():\n  print(\"world\")\n  return 3\nf()\ng()\n```\n\n----------------------------------------\n\nTITLE: Disabling Omnistaging in JAX (Python)\nDESCRIPTION: This Python statement disables omnistaging in JAX for versions between 0.2.0 and 0.2.11, reverting to the previous tracing behavior. It must be included near the top of the main script before other JAX operations to be effective. Omnistaging is not disable-able in v0.2.12 and higher; no other dependencies required except JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\njax.config.disable_omnistaging()\n```\n\n----------------------------------------\n\nTITLE: Optional Environment Variables for Auto PGLE Configuration\nDESCRIPTION: Additional environment variables to customize the Auto PGLE behavior, including setting the number of profiling runs, aggregation percentile, and command buffer handling.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport JAX_PGLE_PROFILING_RUNS=3\nexport JAX_PGLE_AGGREGATION_PERCENTILE=85\n\n# Right now the auto PGLE profile collection doesn't work with command buffer.\n# If the command buffer is enabled, Auto PGLE will disable it during profile\n# colletion and enable it back after the recompilation. If you need to have a\n# consistent command buffer logic with and with PGLE profile you can disable it\n# manually:\nexport XLA_FLAGS=\"${XLA_FLAGS} --xla_gpu_enable_command_buffer=''\"\n```\n\n----------------------------------------\n\nTITLE: JAX Vectorized Random Generation\nDESCRIPTION: Shows JAX's vectorized approach to random number generation using vmap.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/random-numbers.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nprint(\"vectorized:\", jax.vmap(random.normal)(subkeys))\n```\n\n----------------------------------------\n\nTITLE: Setuptools Package Requirements\nDESCRIPTION: Specific SHA256 hash definitions for setuptools v76.0.0, marked as an unsafe package in requirements file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13.txt#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nsetuptools==76.0.0 \\\n    --hash=sha256:199466a166ff664970d0ee145839f5582cb9bca7a0a3a2e795b6a9cb2308e9c6 \\\n    --hash=sha256:43b4ee60e10b0d0ee98ad11918e114c70701bc6051662a9a675a0496c1a158f4\n```\n\n----------------------------------------\n\nTITLE: Markdown Note Block for JAX Parallel Computation Documentation\nDESCRIPTION: A markdown note block indicating that this is a placeholder for the parallel computation section in the new JAX tutorials. It directs readers to related content in the old documentation, specifically on multi-process execution and distributed arrays with automatic parallelization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/_tutorials/parallelism.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{note}\nThis is a placeholder for a section in the new {ref}`jax-tutorials-draft`.\n\nFor the time being, you may find some related content in the old documentation:\n- {doc}`../multi_process`\n- {doc}`../notebooks/Distributed_arrays_and_automatic_parallelization`\n```\n```\n\n----------------------------------------\n\nTITLE: TPU Data Type Support\nDESCRIPTION: List of supported data types in Pallas TPU, including floating point, integer, and boolean types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/details.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\njnp.float32\njnp.bfloat16\njnp.int*  # all precisions, except for jnp.int4\njnp.uint*  # all precisions\njnp.bool_\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Dispatch Machinery for Output Tokens\nDESCRIPTION: Shows the proposed changes to JAX's dispatch machinery to handle output tokens for side-effecting computations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _execute(compiled_computation, *args):\n  output_token, *outputs = compiled_computation.execute(runtime_token, *args)\n  update_output_token(output_token, compiled_computation.device)\n  return outputs\n```\n\n----------------------------------------\n\nTITLE: Executing Build Scripts in Docker Container\nDESCRIPTION: Command to execute a specific build script inside the running Docker container. This example shows how to build the jaxlib artifact within the container.\nSOURCE: https://github.com/jax-ml/jax/blob/main/ci/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec jax ./ci/build_artifacts.sh jaxlib\n```\n\n----------------------------------------\n\nTITLE: Enabling Logging for PGLE in XLA\nDESCRIPTION: Command to set the TensorFlow C++ logging level to include INFO messages, which allows checking if the PGLE profile is being used correctly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport TF_CPP_MIN_LOG_LEVEL=0\n```\n\n----------------------------------------\n\nTITLE: JAX Sharding Example with Cache Miss\nDESCRIPTION: Demonstrates how JIT tracing cache now keys on input NamedShardings, showing a cache miss when using different sharding types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  return x\n\n# inp1.sharding is of type SingleDeviceSharding\ninp1 = jnp.arange(8)\nf(inp1)\n\nmesh = jax.make_mesh((1,), ('x',))\n# inp2.sharding is of type NamedSharding\ninp2 = jax.device_put(jnp.arange(8), NamedSharding(mesh, P('x')))\nf(inp2)  # tracing cache miss\n```\n\n----------------------------------------\n\nTITLE: Preventing TensorFlow from Using GPU\nDESCRIPTION: Code snippet that prevents TensorFlow from accessing any GPU devices. This helps avoid memory conflicts when running TensorFlow alongside JAX on the same GPU.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_memory_allocation.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntf.config.experimental.set_visible_devices([], \"GPU\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Mixed Promotion Lattice for Integer and Floating Types in Python\nDESCRIPTION: This code snippet creates and visualizes a graph representing the promotion lattice for mixed integer and floating-point types. It demonstrates a potential approach to handling these promotions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n  'i*': ['f*', 'u8', 'i8'], 'f*': ['c*', 'f16'], 'c*': ['c64'],\n  'u8': ['u16', 'i16', 'f16'], 'u16': ['u32', 'i32', 'f32'], 'u32': ['u64', 'i64', 'f64'],\n  'i8': ['i16', 'f16'], 'i16': ['i32', 'f32'], 'i32': ['i64', 'f64'],\n  'f16': ['f32'], 'f32': ['f64', 'c64'], 'f64': ['c128'],\n  'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n  'i*': [-1.25, 0.5], 'f*': [-0.5, 2], 'c*': [0, 3],\n  'u8': [0.5, 0], 'u16': [1.5, 0], 'u32': [2.5, 0], 'u64': [3.5, 0],\n  'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\n  'f16': [0.5, 2], 'f32': [1.5, 2], 'f64': [2.5, 2],\n  'c64': [2, 3], 'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 5))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\n```\n\n----------------------------------------\n\nTITLE: Handling Integer Arguments with Gradients in JAX to TensorFlow\nDESCRIPTION: Shows how JAX's float0 type for integer gradients is handled in TensorFlow conversion. When using jax2tf.convert with integer arguments, gradients are translated from JAX's special float0 type to int32 tensors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nx = np.int16(2)\ndef f_jax(x):  # x: int16\n  return x * 2.\n\njax.grad(f_jax, allow_int=True)(x)\n# returns a special `float0`: array((b'',), dtype=[('float0', 'V')])\n\njax2tf.convert(jax.grad(f_jax, allow_int=True))(x)\n# returns a tf.Tensor(0, shape=(), dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Comparing Concrete and Type-Specified Shardings in JAX Python\nDESCRIPTION: Illustrates the difference between concrete array sharding (x.sharding) and type-specified sharding (jax.typeof(x).sharding) in different mesh contexts.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef compare_shardings(x):\n  print(f\"=== with mesh: {get_abstract_mesh()} ===\")\n  print(f\"Concrete value sharding: {x.sharding.spec}\")\n  print(f\"Type-specified sharding: {jax.typeof(x).sharding.spec}\")\n\nmy_array = jnp.sin(reshard(np.arange(8), P(\"X\")))\ncompare_shardings(my_array)\n\n@auto_axes\ndef check_in_auto_context(x):\n  compare_shardings(x)\n  return x\n\ncheck_in_auto_context(my_array, out_shardings=P(\"X\"))\n```\n\n----------------------------------------\n\nTITLE: Incorrect Implementation of Pallas Sum Reduction (Python)\nDESCRIPTION: This snippet presents an intentionally incorrect implementation of sum reduction using JAX Pallas to illustrate common errors. The `incorrect_sum_kernel` attempts accumulation (`+=`) without initializing the output buffer, and the `incorrect_sum` function incorrectly configures the grid dimensions for reduction (reduction axis is first). Running this code produces wrong results, highlighting the need for careful initialization and grid setup in Pallas reductions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Note: This is a TPU example.\n\n# Warning: this implementation is incorrect!\ndef incorrect_sum_kernel(x_ref, o_ref):\n  o_ref[...] += x_ref[...]\n\ndef incorrect_sum(x: jax.Array,\n              block_size: tuple[int, ...] = (256, 256)) -> jax.Array:\n  reduction_size, *out_shape = x.shape\n  grid = (reduction_size, *(out // blk for out, blk in zip(out_shape, block_size)))\n  return pl.pallas_call(\n      incorrect_sum_kernel,\n      grid=grid,\n      # None in `block_shape` means we pick a size of 1 and squeeze it away\n      in_specs=[pl.BlockSpec((None, *block_size), lambda i, j, k: (i, j, k))],\n      out_specs=pl.BlockSpec(block_size, lambda i, j, k: (j, k)),\n      out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype),\n  )(x)\n\nresult = incorrect_sum(x)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Generating Random Normal Data with JAX\nDESCRIPTION: Creates random data using JAX's PRNGKey system, demonstrating key splitting and generating a 5000x5000 matrix of normal random values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nkey, subkey = random.split(key)\nx = random.normal(key, (5000, 5000))\n\nprint(x.shape)\nprint(x.dtype)\n```\n\n----------------------------------------\n\nTITLE: Plotting Distributed Array Data\nDESCRIPTION: Shows how distributed JAX arrays can be visualized with matplotlib, with data automatically gathered from multiple devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nplt.plot(y)\n```\n\n----------------------------------------\n\nTITLE: Running Uncompiled Loop Function\nDESCRIPTION: Executes the loop function without JIT compilation to show normal behavior with dynamic loop bounds.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nf(jnp.array([1., 2., 3.]), 5)\n```\n\n----------------------------------------\n\nTITLE: Dependency Specifications with Hash Verification for JAX\nDESCRIPTION: A requirements file listing Python package dependencies with exact versions and hash verification values. Each entry includes the package name, version, hash values for verification, and a comment indicating the source requirement file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n#\nabsl-py==2.1.0 \\\n    --hash=sha256:526a04eadab8b4ee719ce68f204172ead1027549089702d99b9059f129ff1308 \\\n    --hash=sha256:7820790efbb316739cde8b4e19357243fc3608a152024288513dd968d7d959ff\n    # via -r build/test-requirements.txt\nattrs==24.2.0 \\\n    --hash=sha256:5cfb1b9148b5b086569baec03f20d7b6bf3bcacc9a42bebf87ffaaca362f6346 \\\n    --hash=sha256:81921eb96de3191c8258c199618104dd27ac608d9366f5e35d011eae1867ede2\n    # via hypothesis\nauditwheel==6.1.0 \\\n    --hash=sha256:3bdc686e774cf9e355e924b0fe5a562d55caa385d72234ffe7b81b378dba360f \\\n    --hash=sha256:e52f734861859e3743eb29fcac7da9c4921a1e4bea58f954b52f2926f8e9e364\n    # via -r build/test-requirements.txt\nbuild==1.2.2.post1 \\\n    --hash=sha256:1d61c0887fa860c01971625baae8bdd338e517b836a2f70dd1f7aa3a6b2fc5b5 \\\n    --hash=sha256:b36993e92ca9375a219c99e606a122ff365a760a2d4bba0caa09bd5278b608b7\n    # via -r build/test-requirements.txt\ncloudpickle==3.0.0 \\\n    --hash=sha256:246ee7d0c295602a036e86369c77fecda4ab17b506496730f2f576d9016fd9c7 \\\n    --hash=sha256:996d9a482c6fb4f33c1a35335cf8afd065d2a56e973270364840712d9131a882\n    # via -r build/test-requirements.txt\ncolorama==0.4.6 \\\n    --hash=sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44 \\\n    --hash=sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6\n    # via -r build/test-requirements.txt\ncontourpy==1.3.0 \\\n    --hash=sha256:00ccd0dbaad6d804ab259820fa7cb0b8036bda0686ef844d24125d8287178ce0 \\\n    --hash=sha256:0be4d8425bfa755e0fd76ee1e019636ccc7c29f77a7c86b4328a9eb6a26d0639 \\\n    --hash=sha256:0dce35502151b6bd35027ac39ba6e5a44be13a68f55735c3612c568cac3805fd \\\n    --hash=sha256:0fa4c02abe6c446ba70d96ece336e621efa4aecae43eaa9b030ae5fb92b309ad \\\n    --hash=sha256:14e262f67bd7e6eb6880bc564dcda30b15e351a594657e55b7eec94b6ef72843 \\\n    --hash=sha256:167d6c890815e1dac9536dca00828b445d5d0df4d6a8c6adb4a7ec3166812fa8 \\\n    --hash=sha256:1ec4dc6bf570f5b22ed0d7efba0dfa9c5b9e0431aeea7581aa217542d9e809a4 \\\n    --hash=sha256:303c252947ab4b14c08afeb52375b26781ccd6a5ccd81abcdfc1fafd14cf93c1 \\\n    --hash=sha256:31cd3a85dbdf1fc002280c65caa7e2b5f65e4a973fcdf70dd2fdcb9868069294 \\\n    --hash=sha256:32b238b3b3b649e09ce9aaf51f0c261d38644bdfa35cbaf7b263457850957a84 \\\n    --hash=sha256:33c92cdae89ec5135d036e7218e69b0bb2851206077251f04a6c4e0e21f03927 \\\n    --hash=sha256:345af746d7766821d05d72cb8f3845dfd08dd137101a2cb9b24de277d716def8 \\\n    --hash=sha256:3634b5385c6716c258d0419c46d05c8aa7dc8cb70326c9a4fb66b69ad2b52e09 \\\n    --hash=sha256:364174c2a76057feef647c802652f00953b575723062560498dc7930fc9b1cb7 \\\n    --hash=sha256:36e0cff201bcb17a0a8ecc7f454fe078437fa6bda730e695a92f2d9932bd507f \\\n    --hash=sha256:36f965570cff02b874773c49bfe85562b47030805d7d8360748f3eca570f4cab \\\n    --hash=sha256:3bb3808858a9dc68f6f03d319acd5f1b8a337e6cdda197f02f4b8ff67ad2057b \\\n    --hash=sha256:3e1c7fa44aaae40a2247e2e8e0627f4bea3dd257014764aa644f319a5f8600e3 \\\n    --hash=sha256:3faeb2998e4fcb256542e8a926d08da08977f7f5e62cf733f3c211c2a5586223 \\\n    --hash=sha256:420d39daa61aab1221567b42eecb01112908b2cab7f1b4106a52caaec8d36973 \\\n    --hash=sha256:4553c421929ec95fb07b3aaca0fae668b2eb5a5203d1217ca7c34c063c53d087 \\\n    --hash=sha256:4865cd1d419e0c7a7bf6de1777b185eebdc51470800a9f42b9e9decf17762081 \\\n    --hash=sha256:4cfb5c62ce023dfc410d6059c936dcf96442ba40814aefbfa575425a3a7f19dc \\\n    --hash=sha256:4d63ee447261e963af02642ffcb864e5a2ee4cbfd78080657a9880b8b1868e18 \\\n    --hash=sha256:570ef7cf892f0afbe5b2ee410c507ce12e15a5fa91017a0009f79f7d93a1268f \\\n    --hash=sha256:637f674226be46f6ba372fd29d9523dd977a291f66ab2a74fbeb5530bb3f445d \\\n    --hash=sha256:68a32389b06b82c2fdd68276148d7b9275b5f5cf13e5417e4252f6d1a34f72a2 \\\n    --hash=sha256:69375194457ad0fad3a839b9e29aa0b0ed53bb54db1bfb6c3ae43d111c31ce41 \\\n    --hash=sha256:6cb6cc968059db9c62cb35fbf70248f40994dfcd7aa10444bbf8b3faeb7c2d67 \\\n    --hash=sha256:710a26b3dc80c0e4febf04555de66f5fd17e9cf7170a7b08000601a10570bda6 \\\n    --hash=sha256:732896af21716b29ab3e988d4ce14bc5133733b85956316fb0c56355f398099b \\\n    --hash=sha256:75ee7cb1a14c617f34a51d11fa7524173e56551646828353c4af859c56b766e2 \\\n    --hash=sha256:76a896b2f195b57db25d6b44e7e03f221d32fe318d03ede41f8b4d9ba1bff53c \\\n    --hash=sha256:76c905ef940a4474a6289c71d53122a4f77766eef23c03cd57016ce19d0f7b42 \\\n    --hash=sha256:7a52040312b1a858b5e31ef28c2e865376a386c60c0e248370bbea2d3f3b760d \\\n    --hash=sha256:7ffa0db17717a8ffb127efd0c95a4362d996b892c2904db72428d5b52e1938a4 \\\n    --hash=sha256:81cb5ed4952aae6014bc9d0421dec7c5835c9c8c31cdf51910b708f548cf58e5 \\\n    --hash=sha256:834e0cfe17ba12f79963861e0f908556b2cedd52e1f75e6578801febcc6a9f49 \\\n    --hash=sha256:87ddffef1dbe5e669b5c2440b643d3fdd8622a348fe1983fad7a0f0ccb1cd67b \\\n    --hash=sha256:880ea32e5c774634f9fcd46504bf9f080a41ad855f4fef54f5380f5133d343c7 \\\n    --hash=sha256:8ca947601224119117f7c19c9cdf6b3ab54c5726ef1d906aa4a69dfb6dd58102 \\\n    --hash=sha256:90f73a5116ad1ba7174341ef3ea5c3150ddf20b024b98fb0c3b29034752c8aeb \\\n    --hash=sha256:92f8557cbb07415a4d6fa191f20fd9d2d9eb9c0b61d1b2f52a8926e43c6e9af7 \\\n    --hash=sha256:94e848a6b83da10898cbf1311a815f770acc9b6a3f2d646f330d57eb4e87592e \\\n    --hash=sha256:9c0da700bf58f6e0b65312d0a5e695179a71d0163957fa381bb3c1f72972537c \\\n    --hash=sha256:a11077e395f67ffc2c44ec2418cfebed032cd6da3022a94fc227b6faf8e2acb8 \\\n    --hash=sha256:aea348f053c645100612b333adc5983d87be69acdc6d77d3169c090d3b01dc35 \\\n    --hash=sha256:b11b39aea6be6764f84360fce6c82211a9db32a7c7de8fa6dd5397cf1d079c3b \\\n    --hash=sha256:c6c7c2408b7048082932cf4e641fa3b8ca848259212f51c8c59c45aa7ac18f14 \\\n    --hash=sha256:c6ec93afeb848a0845a18989da3beca3eec2c0f852322efe21af1931147d12cb \\\n    --hash=sha256:cacd81e2d4b6f89c9f8a5b69b86490152ff39afc58a95af002a398273e5ce589 \\\n    --hash=sha256:d402880b84df3bec6eab53cd0cf802cae6a2ef9537e70cf75e91618a3801c20c \\\n    --hash=sha256:d51fca85f9f7ad0b65b4b9fe800406d0d77017d7270d31ec3fb1cc07358fdea0 \\\n    --hash=sha256:d73f659398a0904e125280836ae6f88ba9b178b2fed6884f3b1f95b989d2c8da \\\n    --hash=sha256:d78ab28a03c854a873787a0a42254a0ccb3cb133c672f645c9f9c8f3ae9d0800 \\\n    --hash=sha256:da84c537cb8b97d153e9fb208c221c45605f73147bd4cadd23bdae915042aad6 \\\n    --hash=sha256:dbc4c3217eee163fa3984fd1567632b48d6dfd29216da3ded3d7b844a8014a66 \\\n    --hash=sha256:e12968fdfd5bb45ffdf6192a590bd8ddd3ba9e58360b29683c6bb71a7b41edca \\\n    --hash=sha256:e1fd23e9d01591bab45546c089ae89d926917a66dceb3abcf01f6105d927e2cb \\\n    --hash=sha256:e8134301d7e204c88ed7ab50028ba06c683000040ede1d617298611f9dc6240c \\\n    --hash=sha256:eb8b141bb00fa977d9122636b16aa67d37fd40a3d8b52dd837e536d64b9a4d06 \\\n    --hash=sha256:eca7e17a65f72a5133bdbec9ecf22401c62bcf4821361ef7811faee695799779 \\\n    --hash=sha256:f317576606de89da6b7e0861cf6061f6146ead3528acabff9236458a6ba467f8 \\\n    --hash=sha256:fd2a0fc506eccaaa7595b7e1418951f213cf8255be2600f1ea1b61e46a60c55f \\\n    --hash=sha256:fe41b41505a5a33aeaed2a613dccaeaa74e0e3ead6dd6fd3a118fb471644fd6c\n    # via matplotlib\ncycler==0.12.1 \\\n    --hash=sha256:85cef7cff222d8644161529808465972e51340599459b8ac3ccbac5a854e0d30 \\\n    --hash=sha256:88bb128f02ba341da8ef447245a9e138fae777f6a23943da4540077d3601eb1c\n    # via matplotlib\netils[epath,epy]==1.9.4 \\\n    --hash=sha256:4387e7a4911a3b5cc4b92b99a9211386d176b43bae1dac8e2fe345fc2cb95e4b \\\n    --hash=sha256:fad950414f0a1ca58c70c70915b0014f9953dd9bcf8aa951a0f75ff9becbeb24\n    # via -r build/requirements.in\nexecnet==2.1.1 \\\n    --hash=sha256:26dee51f1b80cebd6d0ca8e74dd8745419761d3bef34163928cbebbdc4749fdc \\\n    --hash=sha256:5189b52c6121c24feae288166ab41b32549c7e2348652736540b9e6e7d4e72e3\n    # via pytest-xdist\nfilelock==3.16.1 \\\n    --hash=sha256:2082e5703d51fbf98ea75855d9d5527e33d8ff23099bec374a134febee6946b0 \\\n    --hash=sha256:c249fbfcd5db47e5e2d6d62198e565475ee65e4831e2561c8e313fa7eb961435\n    # via -r build/test-requirements.txt\nflatbuffers==24.3.25 \\\n    --hash=sha256:8dbdec58f935f3765e4f7f3cf635ac3a77f83568138d6a2311f524ec96364812 \\\n    --hash=sha256:de2ec5b203f21441716617f38443e0a8ebf3d25bf0d9c0bb0ce68fa00ad546a4\n    # via -r build/test-requirements.txt\nfonttools==4.54.1 \\\n    --hash=sha256:07e005dc454eee1cc60105d6a29593459a06321c21897f769a281ff2d08939f6 \\\n    --hash=sha256:0a911591200114969befa7f2cb74ac148bce5a91df5645443371aba6d222e263 \\\n    --hash=sha256:0d1d353ef198c422515a3e974a1e8d5b304cd54a4c2eebcae708e37cd9eeffb1 \\\n    --hash=sha256:0e88e3018ac809b9662615072dcd6b84dca4c2d991c6d66e1970a112503bba7e \\\n    --hash=sha256:1d152d1be65652fc65e695e5619e0aa0982295a95a9b29b52b85775243c06556 \\\n    --hash=sha256:262705b1663f18c04250bd1242b0515d3bbae177bee7752be67c979b7d47f43d \\\n    --hash=sha256:278913a168f90d53378c20c23b80f4e599dca62fbffae4cc620c8eed476b723e \\\n    --hash=sha256:301540e89cf4ce89d462eb23a89464fef50915255ece765d10eee8b2bf9d75b2 \\\n    --hash=sha256:31c32d7d4b0958600eac75eaf524b7b7cb68d3a8c196635252b7a2c30d80e986 \\\n    --hash=sha256:357cacb988a18aace66e5e55fe1247f2ee706e01debc4b1a20d77400354cddeb \\\n    --hash=sha256:37cddd62d83dc4f72f7c3f3c2bcf2697e89a30efb152079896544a93907733bd \\\n    --hash=sha256:41bb0b250c8132b2fcac148e2e9198e62ff06f3cc472065dff839327945c5882 \\\n    --hash=sha256:4aa4817f0031206e637d1e685251ac61be64d1adef111060df84fdcbc6ab6c44 \\\n    --hash=sha256:4e10d2e0a12e18f4e2dd031e1bf7c3d7017be5c8dbe524d07706179f355c5dac \\\n    --hash=sha256:5419771b64248484299fa77689d4f3aeed643ea6630b2ea750eeab219588ba20 \\\n    --hash=sha256:54471032f7cb5fca694b5f1a0aaeba4af6e10ae989df408e0216f7fd6cdc405d \\\n    --hash=sha256:58974b4987b2a71ee08ade1e7f47f410c367cdfc5a94fabd599c88165f56213a \\\n    --hash=sha256:58d29b9a294573d8319f16f2f79e42428ba9b6480442fa1836e4eb89c4d9d61c \\\n    --hash=sha256:5eb2474a7c5be8a5331146758debb2669bf5635c021aee00fd7c353558fc659d \\\n    --hash=sha256:6e37561751b017cf5c40fce0d90fd9e8274716de327ec4ffb0df957160be3bff \\\n    --hash=sha256:76ae5091547e74e7efecc3cbf8e75200bc92daaeb88e5433c5e3e95ea8ce5aa7 \\\n    --hash=sha256:7965af9b67dd546e52afcf2e38641b5be956d68c425bef2158e95af11d229f10 \\\n    --hash=sha256:7e3b7d44e18c085fd8c16dcc6f1ad6c61b71ff463636fcb13df7b1b818bd0c02\n```\n\n----------------------------------------\n\nTITLE: Installing etils Dependency using pip in Python\nDESCRIPTION: Demonstrates how to install the 'etils' package which is required if the compilation cache will not be on a local filesystem. 'etils' is necessary for remote cache backends (e.g., Google Cloud). The command should be run in a shell before enabling non-local JAX disk caching. No parameters are needed; ensure pip is available in the environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install etils\n```\n\n----------------------------------------\n\nTITLE: TensorFlow and TensorBoard Profiler Plugin Installation (Shell)\nDESCRIPTION: This Shell command installs TensorFlow and the TensorBoard Profiler Plugin via pip. It should be run in an environment where TensorFlow and pip are available. This is necessary for enabling advanced TensorBoard profiling features with JAX. For users with TensorFlow already installed, only tensorboard-plugin-profile needs to be added. Having multiple versions of TensorBoard or TensorFlow installed may cause duplicate plugin errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install tensorflow tensorboard-plugin-profile\n```\n\n----------------------------------------\n\nTITLE: Reporting Inconclusive Dimension Operations in JAX2TFJS/TFLite/Flex (Text)\nDESCRIPTION: This snippet displays the conversion error message, followed by the InconclusiveDimensionOperation details, as encountered when translating JAX models to TensorFlow.js or to TFLite/TFLite+Flex. No dependencies are needed; it is an illustrative output of the converter. Key parameters involve non-constant shape dimensions; the expected input is a model with dynamic shapes, and the output is an explanatory error. This output flags limitations when dynamic dimensions are involved, serving as documentation for converter edge cases.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nConversion error\\nInconclusiveDimensionOperation(\"Dimension polynomial comparison 'b' == '2' is inconclusive\\n\\nThis error arises for arithmetic or comparison operations with shapes that\\nare non-constant, and the result of the operation cannot be represented as\\na polynomial of dimension variables, or a boolean constant (for comparisons).\\n\\nPlease see https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#computing-with-dimension-variables\\nfor more details.\\n\")\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Creating Random Data\nDESCRIPTION: Basic imports for JAX and initialization of a random key for generating reproducible pseudo-random numbers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nkey = random.key(0)\n```\n\n----------------------------------------\n\nTITLE: Setting NumPy Random Seed\nDESCRIPTION: Demonstrates how to initialize NumPy's global random state with a seed value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/random-numbers.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nnp.random.seed(0)\n```\n\n----------------------------------------\n\nTITLE: Embedding Flax Model Parameters as Constants for jax2tf in Python\nDESCRIPTION: Illustrates how to embed Flax model parameters (`optimizer.target`) directly into the TensorFlow computation graph as constants, rather than saving them as separate variables. This is achieved by defining `params` as an empty tuple and capturing the actual trained parameters within the `predict_fn` lambda's closure. The lambda ignores its first argument (`_`) and uses the captured parameters. This can increase GraphDef size but may allow for compiler optimizations like constant folding.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\nparams = ()\npredict_fn = lambda _, input: model.apply({\"params\": optimizer.target}, input)\n```\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Step Function in JAX\nDESCRIPTION: Demonstrates gradient computation of a step function using JAX, showing how gradients are zero everywhere due to the function's discontinuity at x=0.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport numpy as np\nimport jax.numpy as jnp\n\ndef f(x):\n  return (x > 0).astype(float)\n\ndf = jax.vmap(jax.grad(f))\n\nx = jnp.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n\nprint(f\"f(x)  = {f(x)}\")\nprint(f\"df(x) = {df(x)}\")\n```\n\n----------------------------------------\n\nTITLE: Templated Kernel Generation Example\nDESCRIPTION: Shows how to use higher-order functions to template Pallas kernels with different element-wise operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef make_kernel(eltwise_kernel):\n  def add(x_ref, y_ref, o_ref):\n    x = pl.load(x_ref, ())\n    y = pl.load(y_ref, ())\n    pl.store(o_ref, (), eltwise_kernel(x + y))\n  return add\n\nkernel1 = make_kernel(lambda x: x * 2)\nkernel2 = make_kernel(jnp.exp)\n\npl.pallas_call(kernel1, out_shape=x, grid=1)(1., 1.)\npl.pallas_call(kernel2, out_shape=x, grid=1)(1., 1.)\n```\n\n----------------------------------------\n\nTITLE: Building Jaxpr using a Staging Interpreter in Python\nDESCRIPTION: Defines `StagingInterpreter` which inherits from `Interpreter`. It intercepts operations during function execution, assigns unique variable names (`fresh_var`), and records them as `Equation` objects in `self.equations`. The `build_jaxpr` function utilizes this interpreter within a context (`set_interpreter`) to trace a given Python function `f` with a specified number of arguments (`num_args`), returning the constructed `Jaxpr`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass StagingInterpreter(Interpreter):\n  def __init__(self):\n    self.equations = []         # A mutable list of all the ops we've seen so far\n    self.name_counter = 0  # Counter for generating unique names\n\n  def fresh_var(self):\n    self.name_counter += 1\n    return \"v_\" + str(self.name_counter)\n\n  def interpret_op(self, op, args):\n    binder = self.fresh_var()\n    self.equations.append(Equation(binder, op, args))\n    return binder\n\ndef build_jaxpr(f, num_args):\n  interpreter = StagingInterpreter()\n  parameters = tuple(interpreter.fresh_var() for _ in range(num_args))\n  with set_interpreter(interpreter):\n    result = f(*parameters)\n  return Jaxpr(parameters, interpreter.equations, result)\n```\n\n----------------------------------------\n\nTITLE: Single Device Asynchronous Execution in JAX\nDESCRIPTION: Illustrates how multiple JIT-compiled functions with side effects are executed asynchronously on a single device.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f():\n  jax.print(\"hello\")\n\n@jax.jit\ndef g():\n  jax.print(\"world\")\n\nf()\ng()\n```\n\n----------------------------------------\n\nTITLE: Implementing Jaxpr Type-checking in Python\nDESCRIPTION: Defines the JaxprType class and implements type-checking for Jaxpr structures. It ensures that variables are bound correctly and that equation types match their output binders.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass JaxprType(NamedTuple):\n  in_types:  list[ShapedArray]\n  out_types: list[ShapedArray]\n\n  def __repr__(self):\n    in_types = ', '.join(aval.str_short() for aval in self.in_types)\n    out_types = ', '.join(aval.str_short() for aval in self.out_types)\n    return f'({in_types}) -> ({out_types})'\n\ndef typecheck_jaxpr(jaxpr: Jaxpr) -> JaxprType:\n  env: set[Var] = set()\n\n  for v in jaxpr.in_binders:\n    if v in env: raise TypeError\n    env.add(v)\n\n  for eqn in jaxpr.eqns:\n    in_types = [typecheck_atom(env, x) for x in eqn.inputs]\n    out_types = abstract_eval_rules[eqn.primitive](*in_types, **eqn.params)\n    for out_binder, out_type in zip(eqn.out_binders, out_types):\n      if not out_type == out_binder.aval: raise TypeError\n    for out_binder in eqn.out_binders:\n      if out_binder in env: raise TypeError\n      env.add(out_binder)\n\n  in_types = [v.aval for v in jaxpr.in_binders]\n  out_types = [typecheck_atom(env, x) for x in jaxpr.outs]\n  return JaxprType(in_types, out_types)\n\ndef typecheck_atom(env: set[Var], x: Atom) -> ShapedArray:\n  if isinstance(x, Var):\n    if x not in env: raise TypeError(\"unbound variable\")\n    return x.aval\n  elif isinstance(x, Lit):\n    return raise_to_shaped(get_aval(x.val))\n  else:\n    assert False\n```\n\n----------------------------------------\n\nTITLE: Computing MMA Operand Transforms in JAX with PLGPU (Python)\nDESCRIPTION: This function determines the necessary tiling and swizzle transforms for matrix operands based on their shape and dtype, to make them compatible with TensorCore MMA operations. It asserts the operand has a 2D shape with a row count divisible by 8, and iterates through supported swizzle sizes (128, 64, 32 bytes), computing the number of elements per swizzle for the given dtype. If a suitable transform is found (shape aligns with swizzle constraints), it returns the required PLGPU transform objects. If not, a ValueError is raised. Requires 'jax', 'plgpu', and a shape-dtype struct as input.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef mma_transforms(shape_dtype: jax.ShapeDtypeStruct):\\n  assert len(shape_dtype.shape) == 2\\n  if shape_dtype.shape[0] % 8:\\n    raise ValueError(\\\"Number of rows must be divisible by 8\\\")\\n  for swizzle_bytes in (128, 64, 32):\\n    swizzle_elems = swizzle_bytes // shape_dtype.dtype.itemsize\\n    if shape_dtype.shape[-1] % swizzle_elems == 0:\\n      return (plgpu.TilingTransform((8, swizzle_elems)),\\n              plgpu.SwizzleTransform(swizzle_bytes))\\n  raise ValueError(\\\"Failed to find transforms for the specified window type\\\")\n```\n\n----------------------------------------\n\nTITLE: Running JAX Test Suite with Pytest\nDESCRIPTION: These commands demonstrate how to run the full JAX test suite, specific test files, or individual tests using pytest.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/contributing.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest -n auto tests/\npytest -n auto tests/lax_scipy_test.py\npytest -n auto tests/lax_scipy_test.py -k testLogSumExp\n```\n\n----------------------------------------\n\nTITLE: Specifying NVIDIA CUDA dependencies with hashes\nDESCRIPTION: This snippet demonstrates how NVIDIA CUDA library dependencies are specified with their versions and SHA256 hashes. It includes examples for nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, and nvidia-cuda-nvcc-cu12 packages.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_11.txt#2025-04-22_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nnvidia-cublas-cu12==12.8.3.14 ; sys_platform == \"linux\" \\\n    --hash=sha256:3f0e05e7293598cf61933258b73e66a160c27d59c4422670bf0b79348c04be44 \\\n    --hash=sha256:93a4e0e386cc7f6e56c822531396de8170ed17068a1e18f987574895044cd8c3 \\\n    --hash=sha256:9ae5eae500aead01fc4bdfc458209df638b1a3551557ce11a78eea9ece602ae9\n    # via\n    #   -r build/gpu-test-requirements.txt\n    #   nvidia-cudnn-cu12\n    #   nvidia-cusolver-cu12\nnvidia-cuda-cupti-cu12==12.8.57 ; sys_platform == \"linux\" \\\n    --hash=sha256:8e0b2eb847de260739bee4a3f66fac31378f4ff49538ff527a38a01a9a39f950 \\\n    --hash=sha256:bbed719c52a476958a74cfc42f2b95a3fd6b3fd94eb40134acc4601feb4acac3 \\\n    --hash=sha256:ff154211724fd824e758ce176b66007b558eea19c9a5135fc991827ee147e317\n    # via -r build/gpu-test-requirements.txt\nnvidia-cuda-nvcc-cu12==12.8.61 ; sys_platform == \"linux\" \\\n    --hash=sha256:171f605044ba17bc455d19cad289946c3dbea029a90c60dfa7b88e545bc8e329\n```\n\n----------------------------------------\n\nTITLE: Sending prediction requests to the model server\nDESCRIPTION: Command to run the model_server_request.py script to send gRPC requests to the TensorFlow serving instance with the specified batch size and number of images.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython ${JAX2TF_EXAMPLES}/serving/model_server_request.py --model_spec_name=${MODEL} \\\n    --use_grpc --prediction_service_addr=localhost:8500 \\\n    --serving_batch_size=${SERVING_BATCH_SIZE} \\\n    --count_images=128\n```\n\n----------------------------------------\n\nTITLE: Generating JAX Python Dependencies with Bazel\nDESCRIPTION: This code snippet shows the command used to generate the Python dependencies file for the JAX project using Bazel. It runs a specific Bazel target to update the requirements.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_11.txt#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n#    bazel run //build:requirements.update\n```\n\n----------------------------------------\n\nTITLE: Querying Maximum Supported Serialization Version in TensorFlow\nDESCRIPTION: This snippet demonstrates how to retrieve the maximum supported serialization version from the installed version of TensorFlow using the xla module.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorflow.compiler.tf2xla.python import xla as tfxla\ntfxla.call_module_maximum_supported_version()\n```\n\n----------------------------------------\n\nTITLE: Resolving Variable Capture Issues by Explicit Passing\nDESCRIPTION: This snippet shows how to avoid errors caused by variable capture in JAX JIT by explicitly passing and returning variable values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_46\n\nLANGUAGE: python\nCODE:\n```\ndef pure_func_tf(x, var1)\n  new_var1 = 11.\n  return x + new_var1, new_var1\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Requirement Lock Files in Bazel WORKSPACE\nDESCRIPTION: This Starlark code snippet shows how to define a dictionary named `requirements` within a Bazel `WORKSPACE` file. This allows mapping different logical Python version strings (including custom scenarios like '3.13-scenario1') to specific requirement lock file targets (e.g., `//build:requirements_lock_3_10.txt`). This enables selecting different dependency sets for the same base Python version using the `HERMETIC_PYTHON_VERSION` environment variable during Bazel builds or tests.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_24\n\nLANGUAGE: starlark\nCODE:\n```\nrequirements = {\n  \"3.10\": \"//build:requirements_lock_3_10.txt\",\n  \"3.11\": \"//build:requirements_lock_3_11.txt\",\n  \"3.12\": \"//build:requirements_lock_3_12.txt\",\n  \"3.13\": \"//build:requirements_lock_3_13.txt\",\n  \"3.13-scenario1\": \"//build:scenario1_requirements_lock_3_13.txt\",\n  \"3.13-scenario2\": \"//build:scenario2_requirements_lock_3_13.txt\",\n},\n```\n\n----------------------------------------\n\nTITLE: Replacing Deprecated `device_buffer` with `addressable_data` in JAX (Python)\nDESCRIPTION: Shows how to replace the deprecated `arr.device_buffer` property of JAX arrays, as per the JAX 0.4.22 release notes. To access the data of the first addressable shard (which corresponds to the old `device_buffer` for single-device arrays), use `arr.addressable_data(0)`. This aligns with the newer array sharding interface.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Old, deprecated property:\narr.device_buffer\n\n# New, recommended method:\narr.addressable_data(0)\n```\n\n----------------------------------------\n\nTITLE: Using CUDA Async Allocator with Environment Variable\nDESCRIPTION: Sets an environment variable to replace XLA's BFC memory allocator with CUDA's asynchronous allocator. This experimental feature eliminates the large fixed pre-allocation and uses a memory pool that grows dynamically.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_memory_allocation.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nTF_GPU_ALLOCATOR=cuda_malloc_async\n```\n\n----------------------------------------\n\nTITLE: Completing XLA call partial evaluation equation\nDESCRIPTION: Implementation of partial evaluation rule for the XLA call primitive, which splits a jaxpr equation into two parts during linearization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nout_binders1 + residuals)\n  eqn2 = JaxprEqn(xla_call_p, residuals + ins2,\n                  dict(jaxpr=jaxpr2, num_consts=0), out_binders2)\n  return eqn1, eqn2, unks_out, residuals\npartial_eval_jaxpr_rules[xla_call_p] = xla_call_peval_eqn\n```\n\n----------------------------------------\n\nTITLE: Error Demonstration: Mapping Over Leaves of Unregistered Custom Objects - JAX - Python\nDESCRIPTION: Attempts to map a function over a list of unregistered custom class objects with jax.tree.map. Because the objects are not registered as pytrees, a TypeError is raised, since their internal fields are not traversed. This snippet shows what goes wrong if user-defined classes intended as containers are not properly registered. Requires the Special class from a previous snippet and JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/working-with-pytrees.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\njax.tree.map(lambda x: x + 1,\\n  [\\n    Special(0, 1),\\n    Special(2, 4)\\n  ])\n```\n\n----------------------------------------\n\nTITLE: Using `slices_for_invocation` for a 2D Grid in Python\nDESCRIPTION: Example usage of the `slices_for_invocation` function. It calculates the slices for an array of shape (100, 100) with a block shape of (10, 20), a grid of (10, 5), and for the specific invocation at indices (2, 4). The resulting slices `[slice(20, 30, None), slice(80, 100, None)]` show the calculated access region.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/grid_blockspec.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> slices_for_invocation(x_shape=(100, 100),\n...                       x_spec = pl.BlockSpec((10, 20), lambda i, j: (i, j)),\n...                       grid = (10, 5),\n...                       invocation_indices = (2, 4))\n[slice(20, 30, None), slice(80, 100, None)]\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Recipe Classes for JAX Partial Evaluation\nDESCRIPTION: Recipe classes represent formulas for computing values during partial evaluation. These include LambdaBindingRecipe for arguments, ConstRecipe for constants, and JaxprEqnRecipe for primitive applications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nfrom weakref import ref, ReferenceType\n\nclass LambdaBindingRecipe(NamedTuple):\n  pass\n\nclass ConstRecipe(NamedTuple):\n  val: Any\n\nclass JaxprEqnRecipe(NamedTuple):\n  prim: Primitive\n  tracers_in: list['PartialEvalTracer']\n  params: dict[str, Any]\n  avals_out: list[ShapedArray]\n  tracer_refs_out: list['ReferenceType[PartialEvalTracer]']\n\nJaxprRecipe = Union[LambdaBindingRecipe, ConstRecipe, JaxprEqnRecipe]\n```\n\n----------------------------------------\n\nTITLE: Handling Custom Pytree Containers in jax2tf Conversion\nDESCRIPTION: This example demonstrates how to register a custom container with JAX's tree_util module and use it with jax2tf conversion. It also shows the limitations when saving to SavedModel or computing gradients.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass CustomPair:\n  def __init__(self, a, b):\n    self.a = a\n    self.b = b\n\n# Register it with the JAX tree_util module\njax.tree_util.register_pytree_node(CustomPair,\n                                   lambda x: ((x.a, x.b), None),\n                                   lambda _, ab: CustomPair(*ab))\ndef f_jax(pair: CustomPair):\n  return 2. * pair.a + 3. * pair.b\n\nx = CustomPair(4., 5.)\nres_jax = f_jax(x)\n# TF execution works as long as JAX can flatten the arguments\nres_tf = jax2tf.convert(f_jax)(x)\nself.assertAllClose(res_jax, res_tf.numpy())\nres_tf_2 = tf.function(jax2tf.convert(f_jax), autograph=False, jit_compile=True)(x)\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Dependencies with Nightly Wheels using Bazel (Shell)\nDESCRIPTION: This command directly invokes the Bazel target (`//build:requirements_nightly.update`) for updating dependencies using nightly/pre-release packages. It's equivalent to the `--nightly_update` script command, accepting unstable packages, using an extra index, and omitting hashes. The target Python version is specified via `HERMETIC_PYTHON_VERSION`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nbazel run //build:requirements_nightly.update --repo_env=HERMETIC_PYTHON_VERSION=3.12\n```\n\n----------------------------------------\n\nTITLE: Using custom JVP in JAX (Python)\nDESCRIPTION: Example of using the new jax.custom_jvp decorator to define custom forward-mode autodiff rules, introduced in JAX 0.1.63.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\n@jax.custom_jvp\ndef f(x):\n  return x ** 2\n```\n\n----------------------------------------\n\nTITLE: Disabling jax.Array via JAX Configuration in Python\nDESCRIPTION: This snippet shows how to programmatically disable the `jax.Array` feature (reverting to older array types like `DeviceArray`) within a Python script using `jax.config.update`. Setting `jax_array` to `False` opts out of the new unified array type. This option was available until March 15, 2023.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax_array_migration.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.config.update('jax_array', False)\n```\n\n----------------------------------------\n\nTITLE: Comparing Memory Profiles with pprof\nDESCRIPTION: Shell command to compare two memory profiles using pprof's diff feature to identify memory growth.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/device_memory_profiling.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npprof --web --diff_base memory1.prof memory9.prof\n```\n\n----------------------------------------\n\nTITLE: Running Linting and Type Checks for JAX\nDESCRIPTION: These commands install pre-commit and run linting and type checks on the JAX codebase.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/contributing.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit run --all\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage as JAX Compilation Cache Directory in Python\nDESCRIPTION: Demonstrates how to configure the JAX compilation cache directory to point to a Google Cloud Storage (GCS) bucket using jax.config.update. This approach is for distributed/cloud use cases where the cache should be remotely accessible by all cluster nodes. Ensure the storage URI is valid and that appropriate permissions are granted.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\njax.config.update(\"jax_compilation_cache_dir\", \"gs://jax-cache\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multiple CPUs Environment in JAX\nDESCRIPTION: Code to configure the environment to simulate multiple CPU devices for demonstrating sharded FFI calls in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=4\"\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Matrix Multiplication Performance in Python\nDESCRIPTION: Defines utility functions for benchmarking matrix multiplication performance, including timing, FLOP/s calculation, and utilization percentage compared to TPU v5e's theoretical peak performance.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport timeit\n\ndef benchmark(f, ntrials: int = 100):\n  def run(*args, **kwargs):\n    # Compile function first\n    jax.block_until_ready(f(*args, **kwargs))\n    # Time function\n    result = timeit.timeit(lambda: jax.block_until_ready(f(*args, **kwargs)),\n                           number=ntrials)\n    time = result / ntrials\n    # print(f\"Time: {time}\")\n    return time\n  return run\n\ndef analyze_matmul(m: int, k: int, n: int, dtype: np.dtype,\n                   mm_func):\n  x = jnp.ones((m, k), dtype=dtype)\n  y = jnp.ones((k, n), dtype=dtype)\n  time = benchmark(mm_func)(x, y)\n  print(f\"----- {m} x {k} x {n} -----\")\n  print(\"Matmul time: \", time)\n  mm_flops = matmul_flops(m, k, n) / time\n  print(\"Matmul FLOP/s: \", mm_flops)\n  print(f\"FLOP/s utilization: {mm_flops / v5e_flops * 100:.4f}%\")\n  print()\n```\n\n----------------------------------------\n\nTITLE: Efficient Transpose of shmap with all_gather and Mapped Output\nDESCRIPTION: This snippet shows the transpose of `f5` with respect to its first argument (`t(f5, 0)`). It takes sharded inputs `y` and `zbar`, computes `psum_scatter(zbar * y, 'i')`, and returns an output sharded along 'i'. In this scenario, the `psum_scatter` is efficient and necessary because `zbar * y` is potentially device-varying due to the mapped inputs/outputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Currently we get this efficient transpose\nt(f5, 0) = shmap(lambda y, zbar: psum_scatter(zbar * y, 'i'),\n                 in_specs=(P('i'), P('i')), out_specs=P('i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Defining a Cursed Identity Function with shmap in JAX\nDESCRIPTION: This snippet defines a 'cursed' identity function using `shmap`. It takes an unmapped input (`in_specs=P()`) and returns it as an unmapped output (`out_specs=P()`). This simple function highlights the inefficiency issues when repeatedly transposed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Example 3: cursed identity\ncursed_identity = shmap(lambda x: x, P(), P())\n```\n```\n\n----------------------------------------\n\nTITLE: Successful Reshape with Compatible Dimensions\nDESCRIPTION: Examples of reshape operations that succeed with symbolic dimensions because the divisions can be performed exactly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n## The resulting symbolic shape is (2, 15 b).\njax2tf.convert(lambda x: jnp.reshape(x, (2, -1)),\n               polymorphic_shapes=[\"(b, ...)\"])(np.ones((4, 5, 6)))\n\n## The resulting symbolic shape is (6 b2, b1).\njax2tf.convert(lambda x: jnp.reshape(x, (-1, x.shape[0])),\n               polymorphic_shapes=[\"(b1, b2, ...)\"])(np.ones((4, 5, 6)))\n```\n\n----------------------------------------\n\nTITLE: Illustrating Potential Side-Effect Issues with `pure_callback` and `vmap`\nDESCRIPTION: This code (presented in the text, not a runnable cell) demonstrates potential undefined or unexpected behavior when using impure functions (like raising exceptions) within `jax.pure_callback`, especially under transformations like `jax.vmap`. The example defines a function to raise an error via callback conditionally. Applying it element-wise works as expected, but applying `jax.vmap` might trigger the error unexpectedly due to `vmap`'s execution patterns interacting with the side-effecting callback, violating the purity assumption of `pure_callback`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\ndef raise_via_callback(x):\n  def _raise(x):\n    raise ValueError(f\"value of x is {x}\")\n  return jax.pure_callback(_raise, x, x)\n\ndef raise_if_negative(x):\n  return jax.lax.cond(x < 0, raise_via_callback, lambda x: x, x)\n\nx_batch = jnp.arange(4)\n\n[raise_if_negative(x) for x in x_batch]  # does not raise\n\njax.vmap(raise_if_negative)(x_batch)  # ValueError: value of x is 0\n```\n\n----------------------------------------\n\nTITLE: Specifying XLA_FLAGS on the Command Line for Python Scripts - Bash\nDESCRIPTION: This bash snippet shows how to pass XLA_FLAGS directly in the CLI environment when launching a Python script. The approach sets the 'XLA_FLAGS' variable inline and executes the Python script in the same command, ensuring XLA receives the flags prior to backend initialization. The input is the environment variable and a target script; the output is the execution of the script with modified XLA behavior. Requires a POSIX-compatible shell environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/xla_flags.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nXLA_FLAGS='--flag1=value1 --flag2=value2'  python3 source.py\n```\n\n----------------------------------------\n\nTITLE: Conceptual Use of Runtime and Compiler Tokens in JAX JIT\nDESCRIPTION: Shows a JIT-compiled function `f` that conceptually uses both `runtime_token` (passed in/out at the JIT boundary) and `compiler_token` (created and threaded internally). The `runtime_token` helps sequence JIT calls, while the `compiler_token` sequences effects within the compiled function body (like `jax.print`). Note the lack of direct dependency between the two token types in this simplified example, which relies on strict execution assumptions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(runtime_token, x):\n  compiler_token = new_compiler_token()\n  compiler_token = jax.print(compiler_token, \"hello\")\n  compiler_token = jax.print(compiler_token, \"world\")\n  return runtime_token, x\n```\n\n----------------------------------------\n\nTITLE: Building JAX with Custom Remote Python 3.13 using Bazel\nDESCRIPTION: This command demonstrates how to build a JAX target using Bazel with a specific, hermetically downloaded Python 3.13 interpreter fetched from a URL. It utilizes the default `requirements_lock_3_13.txt` for dependencies. Key environment variables are passed via `--repo_env`: `HERMETIC_PYTHON_VERSION` specifies the version, `HERMETIC_PYTHON_URL` provides the download link for the Python build, and `HERMETIC_PYTHON_SHA256` ensures the integrity of the downloaded archive. Bazel must be installed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\nbazel build <target> \\\n  --repo_env=HERMETIC_PYTHON_VERSION=3.13 \\\n  --repo_env=HERMETIC_PYTHON_URL=\"https://github.com/indygreg/python-build-standalone/releases/download/20241016/cpython-3.13.0+20241016-x86_64-unknown-linux-gnu-install_only.tar.gz\" \\\n  --repo_env=HERMETIC_PYTHON_SHA256=\"2c8cb15c6a2caadaa98af51df6fe78a8155b8471cb3dd7b9836038e0d3657fb4\"\n```\n\n----------------------------------------\n\nTITLE: ResNet50 Dimension Mismatch\nDESCRIPTION: ValueError when converting ResNet50 model due to dimension mismatch. Expected dimension 1 but got 8 for input tensor.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nValueError('Cannot set tensor: Dimension mismatch. Got 8 but expected 1 for dimension 0 of input 0.')\n```\n\n----------------------------------------\n\nTITLE: JAX Export Main Function Structure in MLIR\nDESCRIPTION: Illustrates the structure of the main function in the exported MLIR module, including platform indexing, token handling, and dimension variable processing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_12\n\nLANGUAGE: mlir\nCODE:\n```\n      func public main(\n            platform_index: i32 {jax.global_constant=\"_platform_index\"},\n            token_in: token,\n            arg: f32[?, ?]) {\n         arg_w = hlo.get_dimension_size(arg, 0)\n         dim1 = hlo.get_dimension_size(arg, 1)\n         arg_h = hlo.floordiv(dim1, 2)\n         call _check_shape_assertions(arg)  # See below\n         token = new_token()\n         token_out, res = call _wrapped_jax_export_main(platform_index,\n                                                        arg_h,\n                                                        arg_w,\n                                                        token_in,\n                                                        arg)\n         return token_out, res\n      }\n```\n\n----------------------------------------\n\nTITLE: Initializing Random PRNGKey in JAX Python\nDESCRIPTION: Example of initializing a random PRNG key in JAX, including how to recover previous behavior for negative seeds with x64 disabled outside of JIT compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nkey = random.PRNGKey(-1).at[0].set(0xFFFFFFFF)\n```\n\n----------------------------------------\n\nTITLE: Querying Default Export Platform - JAX - Python\nDESCRIPTION: This code snippet demonstrates how to retrieve the default export platform used by JAX via the export.default_export_platform() method. No external dependencies other than JAX are required. The snippet outputs the string name of the platform (e.g., 'cpu', 'tpu', etc.) for the machine on which the script is executed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from jax import export\n>>> export.default_export_platform()\n'cpu'\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Latency Hiding Scheduler for Manual PGLE\nDESCRIPTION: Command to set the XLA flag for enabling the latency hiding scheduler in the first step of manual PGLE workflow.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport XLA_FLAGS=\"--xla_gpu_enable_latency_hiding_scheduler=true\"\n```\n\n----------------------------------------\n\nTITLE: Two-step Process for Using PGLE with NVIDIA Nsight Systems\nDESCRIPTION: Commands showing how to use JAX compilation cache to enable both PGLE optimization and NVIDIA Nsight Systems profiling, which normally cannot coexist.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport JAX_ENABLE_COMPILATION_CACHE=yes          # not strictly needed, on by default\nexport JAX_COMPILATION_CACHE_DIR=/root/jax_cache\nJAX_ENABLE_PGLE=yes python my-model.py\n```\n\n----------------------------------------\n\nTITLE: JAX Out-of-Bounds Indexing Behavior\nDESCRIPTION: This snippet demonstrates JAX's behavior when indexing an array out of bounds, which returns the last value of the array instead of raising an error.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\njnp.arange(10)[11]\n```\n\n----------------------------------------\n\nTITLE: Testing JIT with Unimplemented Compilation Rule in JAX (Python)\nDESCRIPTION: Attempts to apply JAX's JIT decorator to a custom primitive before its XLA compilation/lowering rule is implemented. Uses an exception context to expect a NotImplementedError. Demonstrates that abstract evaluation is not sufficient; lowering must also be defined. Depends on 'api.jit', 'square_add_prim', and 'expectNotImplementedError'. Inputs are concrete floats, outputs in this case trigger an exception.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith expectNotImplementedError():\n  api.jit(square_add_prim)(2., 10.)\n\n```\n\n----------------------------------------\n\nTITLE: Redefining Cursed Identity (Example 3 Context)\nDESCRIPTION: This snippet repeats the definition of the `cursed_identity` function using `shmap` with unmapped input and output (`P()`) to provide context for evaluating the 'P-sum' solution's effect on it.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Example 3 again\ncursed_identity = shmap(lambda x: x, P(), P())\n```\n```\n\n----------------------------------------\n\nTITLE: Compiling JAX-Lowered Code with TensorFlow\nDESCRIPTION: Example showing how to compile only the JAX-lowered portion of a TensorFlow function when mixed with non-compilable TensorFlow code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef entire_tf_fun(x):\n  y = preprocess_tf_fun_not_compilable(x)\n  # Compile the code that is lowered from JAX\n  z = tf.function(jax2tf.convert(compute_jax_fn),\n                  autograph=False, jit_compile=True)(y)\n  return postprocess_tf_fun_not_compilable(z)\n```\n\n----------------------------------------\n\nTITLE: Testing Wave Equation Simulation Scaling on TPU Chips\nDESCRIPTION: Compares the performance of the wave equation simulation on a single TPU chip versus 8 TPU chips, demonstrating near-linear scaling.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%time\n# single TPU chip\nu_final, _ = multi_step_jit((u, v), count=2**13, c=c, dt=0.5)\n```\n\nLANGUAGE: python\nCODE:\n```\n%%time\n# 8x TPU chips, 4x more steps in roughly half the time!\nu_final, _ = multi_step_pmap(\n    (u, v), count=2**15, c=c, dt=0.5, exchange_interval=4, save_interval=2**15)\n```\n\nLANGUAGE: python\nCODE:\n```\n18.3 / (10.3 / 4)  # near linear scaling (8x would be perfect)\n```\n\n----------------------------------------\n\nTITLE: Optimization Barrier Implementation\nDESCRIPTION: Example showing how to use optimization barriers to enforce operation ordering in XLA.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  fut = ppermute_start(x)\n  x, fut = optimization_barrier((x, fut))  # x now depends on fut\n  z = x + 1\n  z, fut = optimization_barrier((z, fut)) # fut now depends on z\n  y = ppermute_done(fut)\n  return y, z\n```\n\n----------------------------------------\n\nTITLE: Generating JAX Primitives Coverage Documentation\nDESCRIPTION: Command to regenerate the primitives coverage documentation table. This should be run on a CPU machine with specific environment variables set.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/jax_primitives_coverage.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nJAX_OUTPUT_LIMITATIONS_DOC=1 JAX_ENABLE_X64=1 python jax/experimental/jax2tf/tests/jax_primitives_coverage_test.py JaxPrimitiveTest.test_generate_primitives_coverage_doc\n```\n\n----------------------------------------\n\nTITLE: JAX Dispatch Execution with Runtime Token Management (Conceptual)\nDESCRIPTION: Modifies the conceptual `_execute` function to manage runtime tokens. It retrieves a global `runtime_token`, passes it into the `compiled_computation.execute` call along with other arguments, receives an updated token back, updates the global token, and returns the original computation outputs. This enforces sequential execution between different JIT calls.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef _execute(compiled_computation, *args):\n  runtime_token = get_runtime_token() # Grab global token\n  runtime_token, *outputs = compiled_computation.execute(runtime_token, *args)\n  update_runtime_token(runtime_token) # Update global token\n  return outputs\n```\n\n----------------------------------------\n\nTITLE: Debugging JAX Exports Using Python Logging\nDESCRIPTION: Shows how to enable logging for exported modules in the open-source version of JAX using a command-line flag.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n# Log from python\npython tests/export_test.py JaxExportTest.test_basic -v=3\n```\n\n----------------------------------------\n\nTITLE: AOT Tracing with Static Arguments in JAX\nDESCRIPTION: Illustrates how static arguments in JAX's AOT compilation are specialized to their concrete values during lowering, getting baked into the compiled function rather than remaining as parameters.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/aot.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> lowered_with_x = jax.jit(f, static_argnums=0).trace(7, 8).lower()\n\n>>> # Lowered HLO, specialized to the *value* of the first argument (7)\n>>> print(lowered_with_x.as_text())\nmodule @jit_f attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<i32>) -> (tensor<i32> {jax.result_info = \"result\"}) {\n    %c = stablehlo.constant dense<14> : tensor<i32>\n    %0 = stablehlo.add %c, %arg0 : tensor<i32>\n    return %0 : tensor<i32>\n  }\n}\n\n>>> lowered_with_x.compile()(5)\nArray(19, dtype=int32, weak_type=True)\n```\n\n----------------------------------------\n\nTITLE: JAX Gather Function Signature\nDESCRIPTION: The signature of the JAX gather function that provides complex indexing capabilities. When enable_xla=False, it has partial support limited to scalar indexing, certain multi-dimensional indexing cases, and gather operations with a single batch dimension.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/no_xla_limitations.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlax.gather(\n    operand, start_indices, dimension_numbers, slice_sizes,\n    unique_indices=False, indices_are_sorted=False, mode=None,\n    fill_value=None\n)\n```\n\n----------------------------------------\n\nTITLE: Simplified JAX Dispatch Execution Function (Conceptual)\nDESCRIPTION: Presents a conceptual Python function `_execute` representing the core JAX dispatch mechanism. It takes a `compiled_computation` and arguments `*args`, executes the computation, and returns the outputs. This represents the baseline execution flow before token management is added.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef _execute(compiled_computation, *args):\n  outputs = compiled_computation.execute(*args)\n  return outputs\n```\n\n----------------------------------------\n\nTITLE: Simulating Unsplit Axes in `shard_map` with `jnp.tile` in JAX (Python)\nDESCRIPTION: Illustrates how the behavior of not splitting an input over a specific mesh axis in `shard_map` (as seen in the `f1` example) is conceptually equivalent to manually tiling the input and then fully splitting it. The input `x` is explicitly tiled along its second dimension by the size of the 'j' mesh axis using `jnp.tile`. This tiled input `x_` is then passed to `f2`, which is decorated with `shard_map` using `in_specs=P('i', 'j')` (splitting across both mesh axes). The resulting `x_block` shape inside `f2` is the same as in `f1`, demonstrating the implicit tiling performed by `shard_map` when an input spec omits a mesh axis. Dependencies are the same as the `f1` example.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@partial(shard_map, mesh=m, in_specs=P('i', 'j'), out_specs=P('i', 'j'))\ndef f2(x_block):\n  print(x_block.shape)\n  return x_block\n\nx = np.arange(12 * 12).reshape(12, 12)\nx_ = jnp.tile(x, (1, mesh.axis_size['j']))  # x_ has shape (12, 24)\ny = f2(x_)  # prints (3,12), and f1(x) == f2(x_)\n```\n\n----------------------------------------\n\nTITLE: Nested Pipeline Structure in JAX Pallas\nDESCRIPTION: Code skeleton demonstrating the structure of a program using nested pipelines, with an outer kernel handling remote HBM-HBM transfers and an inner kernel for HBM-VMEM transfers using emit_pipeline.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef outer_kernel(...):\n  # ... do work to pipeline remote HBM-HBM transfers (outer kernel)\n\n  def inner_kernel(...):\n    # ... do work (inner kernel)\n  pltpu.emit_pipeline(\n          inner_kernel,\n          grid=inner_grid,\n          in_specs=...,\n          out_specs=...,\n  )(inner_kernel_args)\n  # ... do more work (outer kernel)\n\npl.pallas_call(\n  outer_kernel,\n  grid=outer_grid,\n  in_specs=...\n  out_specs=...\n  scratch=inner_kernel_allocs\n)\n```\n\n----------------------------------------\n\nTITLE: Allowing weakly-typed promotions in strict mode\nDESCRIPTION: This example demonstrates that strict promotion mode still allows operations between JAX arrays and Python scalars, which is useful for maintaining compatibility with common code patterns.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> with jax.numpy_dtype_promotion('strict'):\n...   z = x + 1\n>>> print(z)\n2.0\n```\n\n----------------------------------------\n\nTITLE: Additional JAX Examples with Device Variance Annotations\nDESCRIPTION: Provides examples of various JAX operations rewritten with explicit device variance annotations, including pbroadcast, cursed_identity, and their transposes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Example 2 rewritten with explicit pbroadcast\nf2 = shmap(lambda x, y: pbroadcast(psum(g(x), 'i'), 'i') * y,\n           in_specs=(P('i'), P('i')), out_specs=P('i'))\n\n# Example 2 transpose using device variance types\nt(f2, 0) = shmap(lambda y, zbar: t(g)(pbroadcast(psum(zbar * y, 'i'), 'i')),\n                 in_specs=(P('i'), P('i')), out_specs=P('i'))\n\n\n# Example 3 again\ncursed_identity = shmap(lambda x: x, P(), P())\n# Notice here the body is `f32[...] -> f32[...]`, i.e. no device varying type.\n\n# Example 3 transpose using device variance types\nt(cursed_identity) = shmap(lambda x: x, P(), P())\nt(t(cursed_identity)) = shmap(lambda x: x, P(), P())\n```\n\n----------------------------------------\n\nTITLE: Implementing Helper Function for Partial Evaluation of JAX cond\nDESCRIPTION: Defines a helper function that performs partial evaluation on both jaxprs and ensures their compatibility. It determines which outputs are known/unknown and joins the jaxprs appropriately.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_87\n\nLANGUAGE: python\nCODE:\n```\ndef _cond_partial_eval(true_jaxpr: Jaxpr, false_jaxpr: Jaxpr, in_uks: list[bool]\n                       ) -> tuple[Jaxpr, Jaxpr, Jaxpr, Jaxpr, list[bool], int]:\n  _, _, t_out_uks, _ = partial_eval_jaxpr(true_jaxpr , in_uks)\n  _, _, f_out_uks, _ = partial_eval_jaxpr(false_jaxpr, in_uks)\n  out_uks = map(op.or_, t_out_uks, f_out_uks)\n\n  t_jaxpr1, t_jaxpr2, _, t_nres = partial_eval_jaxpr(true_jaxpr , in_uks, out_uks)\n  f_jaxpr1, f_jaxpr2, _, f_nres = partial_eval_jaxpr(false_jaxpr, in_uks, out_uks)\n\n  t_jaxpr1, f_jaxpr1 = _join_jaxpr_res(t_jaxpr1, f_jaxpr1, t_nres, f_nres)\n  t_jaxpr2, f_jaxpr2 = _join_jaxpr_consts(t_jaxpr2, f_jaxpr2, t_nres, f_nres)\n  assert typecheck_jaxpr(t_jaxpr1) == typecheck_jaxpr(f_jaxpr1)\n  assert typecheck_jaxpr(t_jaxpr2) == typecheck_jaxpr(f_jaxpr2)\n  num_res = t_nres + f_nres\n\n  return t_jaxpr1, f_jaxpr1, t_jaxpr2, f_jaxpr2, out_uks, num_res\n```\n\n----------------------------------------\n\nTITLE: Confirming CPU Device Type in JAX\nDESCRIPTION: Creates a random array using JAX and verifies that the device platform is CPU. This ensures the notebook is running on the intended CPU runtime.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_cpu.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jaxlib import xla_extension\nimport jax\nkey = jax.random.PRNGKey(1701)\narr = jax.random.normal(key, (1000,))\ndevice = list(arr.devices())[0]\nprint(f\"JAX device type: {device}\")\nassert device.platform == \"cpu\", f\"unexpected JAX device type: {device.platform}\"\n```\n\n----------------------------------------\n\nTITLE: Example of Potential Reordering Issues with Logging Effects in JAX\nDESCRIPTION: Defines a JIT-compiled function `f` that calls a hypothetical `log_value` function twice. If `log_value` has side-effects (like appending to a global list), compiler or runtime reordering could cause `log_value(y)` to execute before `log_value(x)`, leading to unexpected logging order compared to the sequential Python code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x, y):\n  log_value(x)\n  log_value(y)\nf(1, 2)\n```\n\n----------------------------------------\n\nTITLE: Google Cloud SSH Command for TensorBoard Port Forwarding\nDESCRIPTION: This gcloud command sets up SSH connection with local port forwarding for accessing TensorBoard on a Google Cloud machine.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ngcloud compute ssh <machine-name> -- -L 6006:localhost:6006\n```\n\n----------------------------------------\n\nTITLE: Pulling ROCm Ubuntu Docker Image\nDESCRIPTION: Command to pull the ROCm Ubuntu development image.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/rocm/README.md#2025-04-22_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ndocker pull rocm/dev-ubuntu-22.04:6.3-complete\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Bazel Build for ROCm GPU Tests\nDESCRIPTION: This command utilizes the JAX build script (`build/build.py`) to configure the Bazel build environment for testing on ROCm-enabled GPUs. It specifies the ROCm plugin via `--wheels=jax-rocm-plugin` and uses `--configure_only` to perform only the configuration step, preparing for subsequent ROCm-specific `bazel test` commands.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_32\n\nLANGUAGE: shell\nCODE:\n```\npython build/build.py build --wheels=jax-rocm-plugin --configure_only\n```\n\n----------------------------------------\n\nTITLE: IndexError for Dynamic Slice Indices in JAX2TF (Text)\nDESCRIPTION: This snippet represents the error text returned by JAX2TF and other converters when model code attempts to use dynamic (non-static) indices in NumPy-style slicing, which is not permitted in compiled contexts. There are no code dependencies; it explains a static limitation. Inputs are attempted array slice operations with symbolic limits; output is an IndexError explanation with guidance to use lax.dynamic_slice. The detail helps users understand and resolve issues with dynamic indexing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\nIndexError('Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(None, b, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).')\n```\n\n----------------------------------------\n\nTITLE: NVIDIA CUDA Requirements Specification\nDESCRIPTION: Detailed package requirements for NVIDIA CUDA components including version constraints, platform specifications, and SHA256 hashes for package verification. The requirements are specifically for Linux platforms and include CUPTI, NVCC, runtime libraries, cuDNN, cuFFT, cuSolver, and other CUDA-related packages.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13_ft.txt#2025-04-22_snippet_2\n\nLANGUAGE: requirements\nCODE:\n```\nnvidia-cuda-cupti-cu12==12.8.57 ; sys_platform == \"linux\" \\\n    --hash=sha256:8e0b2eb847de260739bee4a3f66fac31378f4ff49538ff527a38a01a9a39f950 \\\n    --hash=sha256:bbed719c52a476958a74cfc42f2b95a3fd6b3fd94eb40134acc4601feb4acac3 \\\n    --hash=sha256:ff154211724fd824e758ce176b66007b558eea19c9a5135fc991827ee147e317\nnvidia-cuda-nvcc-cu12==12.8.61 ; sys_platform == \"linux\" \\\n    --hash=sha256:171f605044ba17bc455d19cad289946c3dbea029a90c60dfa7b88e545bc8e329 \\\n    --hash=sha256:28604ec42aaa09035b0fb7111432e5121bc385580b30c55d2acfb7d644b16548 \\\n    --hash=sha256:4524739cfc080e9c9e53032912be8f020058e0a7186746d19acef3b6d916ea0b\n```\n\n----------------------------------------\n\nTITLE: Controlling Generated Test Cases in Pytest Tests\nDESCRIPTION: This command runs JAX tests using `pytest` with parallel execution (`-n auto`), setting the `JAX_NUM_GENERATED_CASES` environment variable to 25 beforehand. This controls the number of combinatorially generated cases for tests that support this feature, overriding the default value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_41\n\nLANGUAGE: shell\nCODE:\n```\n# pytest\nJAX_NUM_GENERATED_CASES=25 pytest -n auto tests\n```\n\n----------------------------------------\n\nTITLE: Automatic Parallelization with sharded_jit in JAX\nDESCRIPTION: This snippet introduces the new automatic parallelization feature in JAX using sharded_jit. It demonstrates how to partition a convolution operation across multiple devices for improved performance.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import sharded_jit, PartitionSpec as P\nfrom jax import lax\n\nconv = lambda image, kernel: lax.conv(image, kernel, (1, 1), 'SAME')\n\nimage = jnp.ones((1, 8, 2000, 1000)).astype(np.float32)\nkernel = jnp.array(np.random.random((8, 8, 5, 5)).astype(np.float32))\n\nnp.set_printoptions(edgeitems=1)\nconv(image, kernel)\n\n%timeit conv(image, kernel).block_until_ready()\n\nimage_partitions = P(1, 1, 4, 2)\nsharded_conv = sharded_jit(conv,\n                           in_parts=(image_partitions, None),\n                           out_parts=image_partitions)\n\nsharded_conv(image, kernel)\n\n%timeit -n 10 sharded_conv(image, kernel).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Limitation: AOT-Compiled Functions Cannot Be Transformed in JAX\nDESCRIPTION: Demonstrates the limitation that AOT-compiled functions cannot be used with JAX transformations like vmap, grad, or jit, while the standard jit-compiled functions can be transformed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/aot.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> def g(x):\n...   assert x.shape == (3, 2)\n...   return x @ jnp.ones(2)\n\n>>> def make_z(*shape):\n...   return jnp.arange(np.prod(shape)).reshape(shape)\n\n>>> z, zs = make_z(3, 2), make_z(4, 3, 2)\n\n>>> g_jit = jax.jit(g)\n>>> g_aot = jax.jit(g).trace(z).lower().compile()\n\n>>> jax.vmap(g_jit)(zs)\nArray([[ 1.,  5.,  9.],\n       [13., 17., 21.],\n       [25., 29., 33.],\n       [37., 41., 45.]], dtype=float32)\n\n>>> jax.vmap(g_aot)(zs)  # doctest: +SKIP\nTraceback (most recent call last):\nTypeError: Cannot apply JAX transformations to a function lowered and compiled for a particular signature. Detected argument of Tracer type <class 'jax._src.interpreters.batching.BatchTracer'>\n```\n\n----------------------------------------\n\nTITLE: Strategy 2: Incorrect Use of static_argnums with Class Methods in JAX\nDESCRIPTION: This example demonstrates a broken approach to using jit with class methods by marking self as static. The code initially works but breaks when the object is mutated because JAX caches the compiled function based on the object's identity.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n \n  # WARNING: this example is broken, as we'll see below. Don't copy & paste!\n  @partial(jit, static_argnums=0)\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n```\n\n----------------------------------------\n\nTITLE: Using JAX Devices in Python\nDESCRIPTION: Demonstrates the usage of jax.devices() and jax.local_devices() functions to access devices in a distributed job. The change affects how CPU devices are treated in distributed setups.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\njax.devices()  # includes all devices in a distributed job\njax.local_devices()  # only includes devices local to the current process\n```\n\n----------------------------------------\n\nTITLE: Evaluating Loss After Optimization\nDESCRIPTION: Prints the final loss value after running gradient descent optimization to show the improvement from the initial loss.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint(loss(params, batch))\n```\n\n----------------------------------------\n\nTITLE: Preparing Flax Models for jax2tf Conversion in Python\nDESCRIPTION: Shows the standard pattern for obtaining the parameters (`optimizer.target`) and creating a prediction function (`predict_fn`) from a trained Flax model (`MyModel`). The `predict_fn` takes parameters and input, applying them to the model using `model.apply`. This two-argument structure (`params`, `inputs`) is required by utilities like the `convert_and_save_model` example function for saving as a SavedModel with parameters as variables.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nclass MyModel(nn.Module):\n   ...\n\nmodel = MyModel(*config_args, **kwargs)  # Construct the model\noptimizer = ...  # Train the model\nparams = optimizer.target  # These are the parameters\npredict_fn = lambda params, input: model.apply({\"params\": params}, input)\n```\n```\n\n----------------------------------------\n\nTITLE: Start TensorBoard Server (Alternative Syntax, Shell)\nDESCRIPTION: This variant starts TensorBoard with a similar log directory but uses a trailing slash in the path. Either is acceptable; it depends on user convention and directory structure. Standard port and logdir semantics apply.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ntensorboard --logdir /tmp/tensorboard/\n```\n\n----------------------------------------\n\nTITLE: Visualizing Checkpointed Autodiff Computation\nDESCRIPTION: Uses the print_fwd_bwd utility to visualize how the computation changes when using checkpointing with a policy that saves only dot operations with no batch dimensions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# using jax.checkpoint with policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable:\nprint_fwd_bwd(f3, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Nightly TensorBoard and Profiler Installation (Shell)\nDESCRIPTION: This command installs nightly (development) releases of TensorFlow, TensorBoard, and the Profiler Plugin, which may be required for the latest features or bug fixes. Prerequisites: an environment where pip is available and upgraded. All packages are installed in their nightly variants. This approach is recommended when using nightly JAX or TensorFlow builds.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install tf-nightly tb-nightly tbp-nightly\n```\n\n----------------------------------------\n\nTITLE: Importing Module Documentation with Sphinx automodule (reStructuredText)\nDESCRIPTION: This reStructuredText snippet employs the Sphinx `automodule` directive to automatically include documentation from the specified Python module, `jax.experimental.pjit`. It renders the module's docstring and potentially other members based on the Sphinx configuration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.pjit.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: jax.experimental.pjit\n```\n\n----------------------------------------\n\nTITLE: Implementing the Concrete Behavior of a Custom Primitive\nDESCRIPTION: This code defines the concrete implementation of the multiply-add primitive and registers it with JAX using def_impl. The implementation function handles concrete values (not abstract JAX values) and can use standard NumPy for calculations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@trace(\"multiply_add_impl\")\ndef multiply_add_impl(x, y, z):\n  \"\"\"Concrete implementation of the primitive.\n\n  This function does not need to be JAX traceable.\n\n  Args:\n    x, y, z: The concrete arguments of the primitive. Will only be called with \n      concrete values.\n\n  Returns:\n    the concrete result of the primitive.\n  \"\"\"\n  # Note: you can use the ordinary (non-JAX) NumPy, which is not JAX-traceable.\n  return np.add(np.multiply(x, y), z)\n\n# Now, register the primal implementation with JAX:\nmultiply_add_p.def_impl(multiply_add_impl)\n```\n\n----------------------------------------\n\nTITLE: Adding Local Wheel Dependency and Updating Lock File (Shell)\nDESCRIPTION: This sequence of commands first appends the absolute path of a local JAXLib wheel file to the `build/requirements.in` file. Then, it runs the requirements update script to incorporate this local wheel into the dependency lock file (`build/requirements_lock_3_12.txt`) for Python 3.12.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\necho -e \"\\n$(realpath jaxlib-0.4.27.dev20240416-cp312-cp312-manylinux2014_x86_64.whl)\" >> build/requirements.in\npython build/build.py requirements_update --python_version=3.12\n```\n\n----------------------------------------\n\nTITLE: Testing Custom VJP with grad\nDESCRIPTION: Testing the custom VJP implementation by computing the gradient of the function with respect to its first argument.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(2., 3.))\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Analyzing Saved Residuals in Differentiation\nDESCRIPTION: Setting up JAX and defining a simple multi-layer network function to analyze what intermediate values (residuals) are saved during differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gradient-checkpointing.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n\ndef g(W, x):\n  y = jnp.dot(W, x)\n  return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n  x = g(W1, x)\n  x = g(W2, x)\n  x = g(W3, x)\n  return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\n# Inspect the 'residual' values to be saved on the forward pass\n# if you were to evaluate `jax.grad(f)(W1, W2, W3, x)`\nfrom jax.ad_checkpoint import print_saved_residuals\njax.ad_checkpoint.print_saved_residuals(f, W1, W2, W3, x)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JIT Compilation with Dynamic Python Code\nDESCRIPTION: Example showing how JAX can JIT-compile the linear part of computation even with dynamic Python control flow, featuring try-except blocks and conditional statements.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    try:\n        if x < 3:\n            return 2 * x ** 3\n        else:\n            raise ValueError\n    except ValueError:\n        return jnp.pi * x\n\ny, f_vjp = vjp(f, 4.)\nprint(jit(f_vjp)(1.))\n```\n\n----------------------------------------\n\nTITLE: Defining Primitive Operations and Base Interpreter in Python\nDESCRIPTION: Sets up the basic structure for JAX-like interpretation, including an enum for primitive operations, an Interpreter base class, and an EvalInterpreter for concrete evaluation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum, auto\nfrom contextlib import contextmanager\nfrom typing import Any\n\nclass Op(Enum):\n  add = auto()  # addition on floats\n  mul = auto()  # multiplication on floats\n\nclass Interpreter:\n  def interpret_op(self, op: Op, args: tuple[Any, ...]):\n    assert False, \"subclass should implement this\"\n\nclass EvalInterpreter:\n  def interpret_op(self, op, args):\n    assert all(isinstance(arg, float) for arg in args)\n    match op:\n      case Op.add:\n        x, y = args\n        return x + y\n      case Op.mul:\n        x, y = args\n        return x * y\n      case _:\n        raise ValueError(f\"Unrecognized primitive op: {op}\")\n\ncurrent_interpreter = EvalInterpreter()\n\n@contextmanager\ndef set_interpreter(new_interpreter):\n  global current_interpreter\n  prev_interpreter = current_interpreter\n  try:\n    current_interpreter = new_interpreter\n    yield\n  finally:\n    current_interpreter = prev_interpreter\n\ndef add(x, y): return current_interpreter.interpret_op(Op.add, (x, y))\ndef mul(x, y): return current_interpreter.interpret_op(Op.mul, (x, y))\n```\n\n----------------------------------------\n\nTITLE: Boxing and Raising Values for Primitive Interpretation - JAX Python\nDESCRIPTION: Implements helper functions to lower and raise values into Tracer objects for use in the interpreter stack. Depends on Tracer, Trace, and jax_types, and uses type-checking and assertion to determine the correct wrapping or lifting logic. Parameters include values to be boxed or lifted and the Trace instance; output is either a boxed Tracer or the input value. Exceptions are raised for unsupported lift scenarios or tracing inconsistencies.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef full_lower(val: Any):\\n  if isinstance(val, Tracer):\\n    return val.full_lower()\\n  else:\\n    return val\\n\\ndef full_raise(trace: Trace, val: Any) -> Tracer:\\n  if not isinstance(val, Tracer):\\n    assert type(val) in jax_types\\n    return trace.pure(val)\\n  level = trace.main.level\\n  if val._trace.main is trace.main:\\n    return val\\n  elif val._trace.main.level < level:\\n    return trace.lift(val)\\n  elif val._trace.main.level > level:\\n    raise Exception(f\\\"Can't lift level {val._trace.main.level} to {level}.\\\")\\n  else:  # val._trace.level == level\\n    raise Exception(f\\\"Different traces at same level: {val._trace}, {trace}.\\\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Abstract Evaluation for JAX cond Primitive\nDESCRIPTION: Defines the abstract evaluation rule for cond that determines the output types based on the input types and jaxprs. It validates that the predicate is a boolean scalar and that both jaxprs have compatible types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_84\n\nLANGUAGE: python\nCODE:\n```\ndef cond_abstract_eval(pred_type, *in_types, true_jaxpr, false_jaxpr):\n  if pred_type != ShapedArray((), np.dtype('bool')): raise TypeError\n  jaxpr_type = typecheck_jaxpr(true_jaxpr)\n  if jaxpr_type != typecheck_jaxpr(false_jaxpr):\n    raise TypeError\n  if not all(t1 == t2 for t1, t2 in zip(jaxpr_type.in_types, in_types)):\n    raise TypeError\n  return jaxpr_type.out_types\nabstract_eval_rules[cond_p] = cond_abstract_eval\n```\n\n----------------------------------------\n\nTITLE: Calculating 0th Order Derivative using nth_order_derivative in Python\nDESCRIPTION: This snippet demonstrates calculating the 0th order derivative (which is equivalent to evaluating the function itself) using the previously defined `nth_order_derivative` function. It calls the function with n=0 on a presumed function `foo` at point 2.0.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_3\n\nLANGUAGE: ipython3\nCODE:\n```\nprint(nth_order_derivative(0, foo, 2.0))\n```\n\n----------------------------------------\n\nTITLE: Illustrating Functional PRNG without Splitting in Python\nDESCRIPTION: This snippet presents a functional PRNG model where the state (`rng`) is explicitly passed into and returned from functions (`bar`, `baz`). While adhering to functional principles suitable for JAX, this explicit state threading introduces sequential dependencies (e.g., `baz` must run before `bar` can use its resulting state `rng_2`), hindering parallelization and making code less expressive as functions must manage state propagation even if they don't use randomness directly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef foo(rng_1):\n   y, rng_2 = baz(rng_1)\n   z, rng_3 = bar(rng_2)\n   return y + z, rng_3\n\ndef bar(x, rng):\n  val, new_rng = rand(rng, (3, 4))\n  return val, new_rng\n\ndef baz(x, rng):\n  val, new_rng = rand(rng, (3, 4))\n  return val, new_rng\n\ndef main():\n  foo(RandomState(0))\n```\n```\n\n----------------------------------------\n\nTITLE: Migrating Deprecated jax.tree.map Usage - JAX - Python\nDESCRIPTION: This Python snippet demonstrates how to migrate from deprecated usage of jax.tree.map where None is present in the data trees. The new approach uses a lambda function and the is_leaf argument to treat None as a leaf, preserving compatibility with upcoming JAX releases. It depends on the jax library and expects inputs a and b (possibly nested structures), where None should be handled explicitly as a leaf node. The key parameter is is_leaf, which is set to a function that returns True if x is None.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\njax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)\n```\n\n----------------------------------------\n\nTITLE: Configuring Bazel for Remote Custom Python with Custom Prefix (Shell)\nDESCRIPTION: These Bazel arguments configure the use of a remote Python archive where the interpreter files are located under a non-standard directory within the archive (e.g., 'my_python/install' instead of just 'python'). The `HERMETIC_PYTHON_PREFIX` variable specifies this internal path.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n# We assume that top-level folder in the tarbal is called \"python\", if it is\n# something different just pass additional HERMETIC_PYTHON_PREFIX parameter\n--repo_env=HERMETIC_PYTHON_URL=\"https://remote/url/to/my_python.tgz\"\n--repo_env=HERMETIC_PYTHON_SHA256=<file's_sha256_sum>\n--repo_env=HERMETIC_PYTHON_PREFIX=\"my_python/install\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Promotion Lattice Avoiding Precision Loss in Python\nDESCRIPTION: This code snippet creates and visualizes a graph representing a promotion lattice that avoids precision loss in mixed integer and floating-point operations. It promotes unsigned integers to float via their existing signed integer paths.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n  'i*': ['f*', 'u8', 'i8'], 'f*': ['c*', 'f16'], 'c*': ['c64'],\n  'u8': ['u16', 'i16'], 'u16': ['u32', 'i32'], 'u32': ['u64', 'i64'],\n  'i8': ['i16', 'f16'], 'i16': ['i32', 'f32'], 'i32': ['i64', 'f64'],\n  'f16': ['f32'], 'f32': ['f64', 'c64'], 'f64': ['c128'],\n  'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n  'i*': [-1.25, 0.5], 'f*': [-0.5, 2], 'c*': [0, 3],\n  'u8': [0.5, 0], 'u16': [1.5, 0], 'u32': [2.5, 0], 'u64': [3.5, 0],\n  'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\n  'f16': [0.5, 2], 'f32': [1.5, 2], 'f64': [2.5, 2],\n  'c64': [2, 3], 'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 5))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\n```\n\n----------------------------------------\n\nTITLE: Documenting Data Types and Helper Functions - Sphinx (Python)\nDESCRIPTION: This documentation snippet provides an autosummary table for the jax.dtypes Python module using Sphinx markup. It lists important types and helpers for working with JAX dtypes, leveraging Sphinx directives to automatically pull docstrings and signatures. This enables users to quickly access overviews and documentation for common data type utilities without directly viewing the source. \":toctree: _autosummary\" specifies that summaries and links to detailed documentation for each listed attribute (such as bfloat16, float0, canonicalize_dtype, issubdtype, etc.) are automatically generated in the autosummary tree.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.dtypes.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``jax.dtypes`` module\n=====================\n\n.. automodule:: jax.dtypes\n\n.. autosummary::\n  :toctree: _autosummary\n\n    bfloat16\n    canonicalize_dtype\n    float0\n    issubdtype\n    prng_key\n    result_type\n    scalar_type_of\n```\n\n----------------------------------------\n\nTITLE: Configuring Bazel to Use Remote Custom Python Archive (Shell)\nDESCRIPTION: These Bazel command-line arguments configure the build to download and use a custom Python interpreter from a remote URL. `HERMETIC_PYTHON_URL` specifies the download location, and `HERMETIC_PYTHON_SHA256` provides the expected checksum.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n# OR\n--repo_env=HERMETIC_PYTHON_URL=\"https://remote/url/to/my_python.tgz\"\n--repo_env=HERMETIC_PYTHON_SHA256=<file's_sha256_sum>\n```\n\n----------------------------------------\n\nTITLE: Visualizing Original Function and Gradient\nDESCRIPTION: Plotting a sine function and its gradient to establish a baseline before applying gradient clipping.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom jax import vmap\n\nt = jnp.linspace(0, 10, 1000)\n\nplt.plot(jnp.sin(t))\nplt.plot(vmap(grad(jnp.sin))(t))\n```\n\n----------------------------------------\n\nTITLE: Automating Hourly JAX Regression Testing within Container (Bash)\nDESCRIPTION: This Bash script orchestrates hourly JAX/XLA builds and tests within an existing JAX development container (e.g., `ghcr.io/nvidia/jax:nightly-2023-08-24`). It first updates the local git checkouts of XLA and JAX sources, installs dependencies, and clones the benchmark repository. It then loops through specified days (24-26) and hours (0-24), executing the `/dir/test2.sh` script for each timestamp (e.g., 'Aug 24 2023 10:00:00'). Output for each hour is saved to `OUT-mm-dd-hh`. This script requires running inside a container with `/opt/xla-source`, `/opt/jax-source`, and necessary build tools.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/investigating_a_regression.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n  # Execute this script inside the container:\n  # docker run -v $PWD:/dir --gpus=all ghcr.io/nvidia/jax:nightly-2023-08-24 /bin/bash\n  cd /opt/xla-source\n  git remote update\n  cd /opt/jax-source\n  git remote update\n  pip install jmp pyvista numpy matplotlib Rtree trimesh jmp termcolor orbax\n  cd /tmp\n  git clone https://github.com/Autodesk/XLB\n  cd XLB\n\n  for d in `seq -w 24 26`; do\n      for h in `seq -w 0 24`; do\n          echo $m $d $h\n          /bin/bash /dir/test2.sh Aug $d 2023 $h:00:00 &> OUT-08-${d}-$h\n      done\n  done\n```\n\n----------------------------------------\n\nTITLE: Checking NumPy Promotion of uint64 and int64 in Python\nDESCRIPTION: This code snippet demonstrates NumPy's promotion behavior when adding uint64 and int64 types. It shows that the result is promoted to float64.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n(np.uint64(1) + np.int64(1)).dtype\n```\n\n----------------------------------------\n\nTITLE: Migrating to static_argnums in jax.checkpoint for Static Argument Tracing - JAX (Python)\nDESCRIPTION: This updated example shows the preferred migration path: using static_argnums in jax.checkpoint to indicate arguments that should be treated as static during tracing. The function foo now specifies is_training (the second argument) as static, allowing conditional logic without the removed concrete option. Inputs include x (data) and is_training (static flag); the ... denotes implementation-dependent computation. Requires functools.partial and jax.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/11830-new-remat-checkpoint.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@partial(jax.checkpoint, static_argnums=(1,))  # NEW jax.checkpoint API\ndef foo(x, is_training):\n  if is_training:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Advanced Indexing with JAX NumPy\nDESCRIPTION: Shows JAX's support for complex indexing operations with matrix multiplication, including custom indices, adding dimensions, and reversing arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(jnp.dot(x, 2 * x)[[0, 2, 1, 0], ..., None, ::-1])\n```\n\n----------------------------------------\n\nTITLE: Setting Global JAX Rank Promotion Configuration\nDESCRIPTION: Demonstrates how to configure JAX rank promotion behavior globally using the jax.config API.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/rank_promotion_warning.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jax\njax.config.update(\"jax_numpy_rank_promotion\", \"warn\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Inverse Registry for JAX Primitives\nDESCRIPTION: Initializes an empty dictionary that will map JAX primitives to their inverse functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninverse_registry = {}\n```\n\n----------------------------------------\n\nTITLE: NumPy Vector Random Sampling\nDESCRIPTION: Demonstrates NumPy's sequential equivalence guarantee when sampling multiple random numbers.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/random-numbers.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(0)\nprint(\"individually:\", np.stack([np.random.uniform() for _ in range(3)]))\n\nnp.random.seed(0)\nprint(\"all at once: \", np.random.uniform(size=3))\n```\n\n----------------------------------------\n\nTITLE: SSH Tunnel Setup for Remote JAX Profiling Access (Bash)\nDESCRIPTION: This Bash command sets up an SSH tunnel forwarding local port 9001 to the remote server's port 9001, required to enable browser-based access to Perfetto's trace UI for profiling sessions on remote VMs. Fill in your <user> and <host> values. You must have SSH access to the remote machine. No external dependencies other than OpenSSH client are required. This approach allows local access to a remote profiling server interface.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ssh -L 9001:127.0.0.1:9001 <user>@<host>\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Related Libraries in Python\nDESCRIPTION: This snippet shows how to import JAX, NumPy, and various JAX modules required for the examples in the document.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom jax import jit\nfrom jax import lax\nfrom jax import random\nimport jax\nimport jax.numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies Specification with Hash Verification\nDESCRIPTION: A comprehensive list of Python package dependencies with their exact versions, hash values for verification, and comments indicating which requirement files need each package. This format is commonly used in pip requirements files with hash verification to ensure package integrity.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_10.txt#2025-04-22_snippet_7\n\nLANGUAGE: pip\nCODE:\n```\nportpicker==1.6.0 ; python_version < \"3.13\" \\\n    --hash=sha256:b2787a41404cf7edbe29b07b9e0ed863b09f2665dcc01c1eb0c2261c1e7d0755 \\\n    --hash=sha256:bd507fd6f96f65ee02781f2e674e9dc6c99bbfa6e3c39992e3916204c9d431fa\n    # via\n    #   -r build/nonfreethreading-requirements.txt\n    #   -r build/test-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining emit_pipeline Function Signature in JAX Pallas\nDESCRIPTION: Function signature for emit_pipeline which creates a nested pipeline given an inner kernel and BlockSpecs. It follows the standard pallas_call pattern by specifying a grid and BlockSpecs for inputs and outputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef emit_pipeline(\n    kernel: Callable,\n    grid: tuple[int],\n    in_specs: PyTree[BlockSpec] = None,\n    out_specs: PyTree[BlockSpec] = None,\n    should_accumulate_out: bool = False,\n    dimension_semantics: tuple[GridDimensionSemantics] = None,\n) -> Callable:\n  ... # Returns a custom pipeline given an inner kernel and BlockSpecs.\n```\n\n----------------------------------------\n\nTITLE: Illustrating XLA Inserting Copy Due to Perceived Aliasing with ppermute\nDESCRIPTION: This snippet shows how XLA might insert a defensive `copy` before calling `ppermute_start`. Because `ppermute_start` aliases its input `x` to an output `x2`, and the original `x` is used later in `z = x + 1`, XLA conservatively assumes `ppermute_start` might modify `x` (like an in-place op) and inserts a copy to protect the original value for the `z = x + 1` computation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  x2 = copy(x)\n  *sems, x2, y = ppermute_start(x2)\n  z = x + 1\n  y = ppermute_done((*sems, x2, y))\n  return y, z\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Trace and Tracer Base Classes\nDESCRIPTION: Implements base classes for transformation interpreters. Trace handles primitive applications while Tracer represents boxed values with transformation context.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Trace:\n  main: MainTrace\n\n  def __init__(self, main: MainTrace) -> None:\n    self.main = main\n\n  def pure(self, val): assert False  # must override\n  def lift(self, val): assert False  # must override\n\n  def process_primitive(self, primitive, tracers, params):\n    assert False  # must override\n\nclass Tracer:\n  _trace: Trace\n\n  __array_priority__ = 1000\n\n  @property\n  def aval(self):\n    assert False  # must override\n\n  def full_lower(self):\n    return self  # default implementation\n\n  def __neg__(self): return self.aval._neg(self)\n  def __add__(self, other): return self.aval._add(self, other)\n  def __radd__(self, other): return self.aval._radd(self, other)\n  def __mul__(self, other): return self.aval._mul(self, other)\n  def __rmul__(self, other): return self.aval._rmul(self, other)\n  def __gt__(self, other): return self.aval._gt(self, other)\n  def __lt__(self, other): return self.aval._lt(self, other)\n  def __bool__(self): return self.aval._bool(self)\n  def __nonzero__(self): return self.aval._nonzero(self)\n\n  def __getattr__(self, name):\n    try:\n      return getattr(self.aval, name)\n    except AttributeError:\n      raise AttributeError(f\"{self.__class__.__name__} has no attribute {name}\")\n\ndef swap(f): return lambda x, y: f(y, x)\n```\n\n----------------------------------------\n\nTITLE: Committing Changes and Syncing with Main JAX Repository\nDESCRIPTION: These Git commands show how to commit changes, sync with the main JAX repository, and push to a remote branch for creating a pull request.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/contributing.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit add file1.py file2.py ...\ngit commit -m \"Your commit message\"\ngit fetch upstream\ngit rebase upstream/main\ngit push --set-upstream origin name-of-change\n```\n\n----------------------------------------\n\nTITLE: Installing JAX for Google Cloud TPU VMs\nDESCRIPTION: Command to install JAX optimized for Google Cloud TPU virtual machines. This provides specialized support for Google's Tensor Processing Units.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"jax[tpu]\"\n```\n\n----------------------------------------\n\nTITLE: Using Regular Semaphores in Pallas for TPU\nDESCRIPTION: This code snippet demonstrates the usage of regular semaphores in Pallas for TPU. It shows the three main operations that can be performed on regular semaphores: signal, wait, and read. These are used for synchronization across multiple devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef semaphore_signal(\n    sem: Ref[SemaphoreType],\n    inc: int,\n    device_id: int | tuple[int, ...],\n    device_id_type: DeviceIdType\n) -> None:\n  ... # Increments the semaphore `sem` on the target device `device_id` by `inc`.\n  \ndef semaphore_wait(\n    semaphore: Ref[SemaphoreType],\n    value: int,\n) -> None:\n  ... # Blocks until the locally allocated copy of `sem` reaches `value`, then decrement by `value` and proceed.\n    \ndef semaphore_read(\n    sem: Ref[SemaphoreType],\n) -> jax.Array:\n  ...  # Returns the current value of `sem` as an `int32[]`.\n```\n\n----------------------------------------\n\nTITLE: Calculating 4th Order Derivative using nth_order_derivative in Python\nDESCRIPTION: This snippet demonstrates calculating the 4th order derivative using the `nth_order_derivative` function. Similar to the 3rd order derivative, this is expected to be zero for the presumed second-order polynomial `foo`. It calls the function with n=4 on `foo` at point 2.0.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_7\n\nLANGUAGE: ipython3\nCODE:\n```\nprint(nth_order_derivative(4, foo, 2.0))\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Gradient Handling for Integer Arguments\nDESCRIPTION: Demonstrates how TensorFlow handles gradients for integer arguments differently from JAX. TensorFlow returns None by default but can return zeros with the same dtype when using UnconnectedGradients.ZERO option.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef f_tf(x):  # x: int16\n  return tf.cast(x, tf.float32) * 2.\n\nxv = tf.Variable(x)\nwith tf.GradientTape(persistent=True) as tape:\n  print(tape.gradient(f_tf(xv), xv))\n  # returns None\n  print(tape.gradient(f_tf(xv), xv,\n                      unconnected_gradients=tf.UnconnectedGradients.ZERO))\n  # returns 0 with the same shape and dtype as x\n```\n\n----------------------------------------\n\nTITLE: Displaying Distributed Array Results\nDESCRIPTION: Shows the result of a pmap operation, demonstrating how the array is sharded across devices.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ny\n```\n\n----------------------------------------\n\nTITLE: Using pmap with static broadcast arguments in JAX (Python)\nDESCRIPTION: Example of using the new static_broadcast_argnums parameter in jax.pmap to specify arguments that should be treated as compile-time constants and broadcasted to all devices, introduced in JAX 0.1.60.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\njax.pmap(f, static_broadcast_argnums=(0,))\n```\n\n----------------------------------------\n\nTITLE: Setting Up New Notebooks for jupytext in JAX (Bash)\nDESCRIPTION: Command to set up a new Jupyter notebook for use with jupytext in the JAX project, enabling syncing between ipynb and md formats.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_53\n\nLANGUAGE: bash\nCODE:\n```\njupytext --set-formats ipynb,md:myst path/to/the/notebook.ipynb\n```\n\n----------------------------------------\n\nTITLE: Documenting the custom_partitioning Function\nDESCRIPTION: This Sphinx directive specifically targets the `custom_partitioning` function within the `jax.experimental.custom_partitioning` module (or the current module context if set by `automodule`). It generates documentation for this specific function, including its signature, parameters, and docstring.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.custom_partitioning.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autofunction:: custom_partitioning\n```\n\n----------------------------------------\n\nTITLE: Specifying NumPy Dependency for JAX Project\nDESCRIPTION: This snippet defines the required version of NumPy for the JAX project. It specifies a version approximately equal to 2.2.5, which is necessary for proper functioning under free-threading conditions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/freethreading-requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nnumpy~=2.2.5\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Accelerator GPU Tests with Bazel\nDESCRIPTION: This command runs JAX GPU tests tagged as 'multiaccelerator' using Bazel. It specifies `--local_test_jobs=4` to limit local parallelism, explicitly uses the preinstalled `jaxlib` (`--//jax:build_jaxlib=false`), and sets the XLA allocator via `--test_env=XLA_PYTHON_CLIENT_ALLOCATOR=platform`. This is suitable for tests designed to utilize multiple GPU devices simultaneously.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_37\n\nLANGUAGE: shell\nCODE:\n```\nbazel test //tests:gpu_tests --local_test_jobs=4 --test_tag_filters=multiaccelerator --//jax:build_jaxlib=false --test_env=XLA_PYTHON_CLIENT_ALLOCATOR=platform\n```\n\n----------------------------------------\n\nTITLE: Visualizing Promotion Lattice Avoiding Wider-than-Necessary Promotions in Python\nDESCRIPTION: This code snippet creates and visualizes a graph representing a promotion lattice that avoids most wider-than-necessary promotions in mixed integer and floating-point operations. It accepts some precision loss to achieve more efficient promotions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n  'i*': ['f*', 'u8', 'i8'], 'f*': ['c*', 'f16'], 'c*': ['c64'],\n  'u8': ['u16', 'i16'], 'u16': ['u32', 'i32'], 'u32': ['u64', 'i64'],\n  'i8': ['i16'], 'i16': ['f16', 'i32'], 'i32': ['f32', 'i64'], 'i64': ['f64'],\n  'f16': ['f32'], 'f32': ['f64', 'c64'], 'f64': ['c128'],\n  'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n  'i*': [-1.25, 0.5], 'f*': [-0.5, 2], 'c*': [0, 3],\n  'u8': [0.5, 0], 'u16': [1.5, 0], 'u32': [2.5, 0], 'u64': [3.5, 0],\n  'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\n  'f16': [1.5, 2], 'f32': [2.5, 2], 'f64': [3.5, 2],\n  'c64': [3, 3], 'c128': [4, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 5))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\n```\n\n----------------------------------------\n\nTITLE: Automatic pbroadcast Insertion in JAX Operations\nDESCRIPTION: This snippet describes the logic for automatically inserting pbroadcast operations in JAX when operands disagree in their device variance types. It aims to lift each operand to the resulting device variance type by taking the union of operands' device variance types' axis name sets.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Pseudo-code for automatic pbroadcast insertion\ndef insert_pbroadcasts(operands):\n    union_of_variance_types = get_union_of_variance_types(operands)\n    for operand in operands:\n        if operand.variance_type != union_of_variance_types:\n            operand = pbroadcast(operand, union_of_variance_types)\n    return operands\n```\n\n----------------------------------------\n\nTITLE: Creating one-hot vectors with JAX (Python)\nDESCRIPTION: Example of using the new jax.nn.one_hot utility function to create one-hot encoded vectors, introduced in JAX 0.1.60.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\njax.nn.one_hot(x, num_classes)\n```\n\n----------------------------------------\n\nTITLE: Flattening a 2D Array with Symbolic Dimensions in JAX\nDESCRIPTION: Demonstrates how to use jax2tf.convert with polymorphic shapes to reshape a 2D array into a 1D array, where one dimension is symbolically represented.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],)),\n                polymorphic_shapes=[\"(b, 4)\"])(np.ones((3, 4)))\n```\n\n----------------------------------------\n\nTITLE: Defining a Conditional Function for Differentiation\nDESCRIPTION: Creates a function with conditional logic that will be automatically differentiated by JAX, demonstrating JAX's ability to handle control flow in differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  if x > 0:\n    return 2 * x ** 3\n  else:\n    return 3 * x\n```\n\n----------------------------------------\n\nTITLE: Hypothetical Efficient Transpose of shmap with psum in JAX\nDESCRIPTION: This snippet presents a hypothetical, efficient transpose (`¿f1_transpose?`) for the function `f1` defined previously. It uses `shmap` to apply the transpose of `g` (`t(g)`) to an unmapped input (`in_specs=P()`) and produces an output sharded along axis 'i' (`out_specs=P('i')`). This structure avoids the redundant communication present in the actual current transpose.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\n# An efficient \"transpose\" of Example 1 (but don't transpose this again!)\n¿f1_transpose? = shmap(t(g), in_specs=P(), out_specs=P('i'))\n```\n```\n\n----------------------------------------\n\nTITLE: Importing JAX modules\nDESCRIPTION: Imports the necessary JAX modules for gradient computation, JIT compilation, and NumPy-like operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad, jit\nimport jax.numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Configuring Output Shapes and Grid Specifications for TPU Kernel in Python\nDESCRIPTION: Defines the output shapes and grid specifications needed for the Pallas TPU kernel call. This includes memory space specifications, semaphore configurations, and scratch buffer shapes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nout_shape = (\n    jax.ShapeDtypeStruct((block_size[0], block_size[1]), jnp.float32),  # output\n    # Shape: [working/recv, block[0], block[1]]\n    jax.ShapeDtypeStruct(\n        (2, block_size[0], block_size[1]), jnp.float32\n    ),  # hbm_scratch\n)\n\ngrid_spec = pltpu.PrefetchScalarGridSpec(\n    num_scalar_prefetch=0,\n    in_specs=[\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n    ],\n    out_specs=[\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n    ],\n    grid=(num_devices, 2),\n    scratch_shapes=(\n        [pltpu.SemaphoreType.DMA] * 5\n        + [pltpu.SemaphoreType.REGULAR] * 2  # Capacity semaphores\n        + [\n            pltpu.VMEM((block_size[0] // 2, block_size[1]), jnp.float32)\n        ]  # accum_scratch\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Error When Using Checks Without checkify Transformation in JAX\nDESCRIPTION: Demonstrates what happens when trying to use checkify.check without applying the checkify transformation, which causes a ValueError because the check cannot be abstractly evaluated.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\njax.jit(f)(jnp.ones((5,)), -1)  # checkify transformation not used\n# ValueError: Cannot abstractly evaluate a checkify.check which was not functionalized.\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Trace-Time Printing with Standard Python `print` in JAX `jit`\nDESCRIPTION: This snippet defines a function `f` decorated with `jax.jit`. Inside `f`, a standard Python `print` statement is used. When `f` is called, the `print` statement executes during JAX's tracing phase, printing the abstract representation (Traced<ShapedArray(...)>) of the intermediate value `y`, not its concrete runtime value.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n\n@jax.jit\ndef f(x):\n  y = x + 1\n  print(\"intermediate value: {}\".format(y))\n  return y * 2\n\nresult = f(2)\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing with Refs in Pallas Python Kernels\nDESCRIPTION: This snippet shows various ways to read from and write to Refs in Pallas kernels, including vanilla Python indexing, NumPy advanced indexing, and broadcasting techniques.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(x_ref, o_ref):\n  # Using vanilla Python indexing\n  x = x_ref[0, 2:5, :]\n  # Or via Numpy advanced int indexing\n  o_ref[jnp.arange(3), :] = x\n\n# Note that in order to use NumPy advanced int indexing, you need to broadcast the indices against each other into the desired multidimensional shape:\ndef f(x_ref):\n  # Assume x_ref is (8, 4) and we want to read out a (2, 3) slice\n  x = x_ref[jnp.arange(2)[..., None], jnp.arange(3)[None, ...]]\n```\n\n----------------------------------------\n\nTITLE: Error Example: Matrix Multiplication with Incompatible Dimensions in JAX\nDESCRIPTION: Shows an error case where matrix multiplication is attempted with incompatible dimensions in a shape-polymorphic setting.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: jnp.matmul(x, x),\n               polymorphic_shapes=[\"(v, 4)\"])(np.ones((4, 4)))\n```\n\n----------------------------------------\n\nTITLE: Testing XLA JIT Compilation with a Custom Function\nDESCRIPTION: This snippet demonstrates JAX's just-in-time (JIT) compilation by defining and applying a SELU activation function to random data. The block_until_ready() call ensures computation completes before proceeding.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_gpu.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef selu(x, alpha=1.67, lmbda=1.05):\n  return lmbda * jax.numpy.where(x > 0, x, alpha * jax.numpy.exp(x) - alpha)\nx = jax.random.normal(key, (5000,))\nresult = selu(x).block_until_ready()\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Showing Compiler Reordering of Side-Effects within JAX JIT\nDESCRIPTION: Defines a single JIT-compiled JAX function `f` containing two sequential `jax.print` calls. Because there is no data dependency between these print calls, a compiler like XLA is permitted to reorder their execution, potentially printing \"world\" before \"hello\", despite their textual order in the Python code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/10657-sequencing-effects.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef f(x):\n  jax.print(\"hello\")\n  jax.print(\"world\")\n  return x\n```\n\n----------------------------------------\n\nTITLE: Building and Packaging Custom Python Interpreter (Shell)\nDESCRIPTION: These commands illustrate how to build Python from source and package the installation into a compressed tar archive (`my_python.tgz`). This archive can then be used to provide a custom Python interpreter for the hermetic Bazel build process.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n./configure --prefix python\nmake -j12\nmake altinstall\ntar -czpf my_python.tgz python\n```\n\n----------------------------------------\n\nTITLE: Installing JAX for Google Cloud TPU VMs\nDESCRIPTION: Command to install JAX with appropriate versions of jaxlib and libtpu for Google Cloud TPU virtual machines.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install \"jax[tpu]\"\n```\n\n----------------------------------------\n\nTITLE: Manual Error Plumbing Alternative to checkify in JAX\nDESCRIPTION: Shows how to manually plumb error values through a function as an alternative to using checkify. This approach is functionally pure but requires more boilerplate code to handle error tracking and reporting.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef f_checked(x):\n  error = x <= 0.\n  result = jnp.log(x)\n  return error, result\n\nerr, y = jax.jit(f_checked)(0.)\nif err:\n  raise ValueError(\"must be positive!\")\n# ValueError: \"must be positive!\"\n```\n\n----------------------------------------\n\nTITLE: Using checkify with Automatic Error Detection in JAX\nDESCRIPTION: Shows how to use checkify to automatically instrument code with checks for common errors without adding manual check calls. The example demonstrates catching out-of-bounds indexing and NaN generation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/checkify_guide.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, i):\n  y = x[i]        # i could be out of bounds.\n  z = jnp.sin(y)  # z could become NaN\n  return z\n\nerrors = checkify.user_checks | checkify.index_checks | checkify.float_checks\nchecked_f = checkify.checkify(f, errors=errors)\n\nerr, z = checked_f(jnp.ones((5,)), 100)\nerr.throw()\n# ValueError: out-of-bounds indexing at <..>:7 (f)\n\nerr, z = checked_f(jnp.array([jnp.inf, 1]), 0)\nerr.throw()\n# ValueError: nan generated by primitive sin at <...>:8 (f)\n```\n\n----------------------------------------\n\nTITLE: Using Debug Print with TPU Log Recorder in Pallas\nDESCRIPTION: Example demonstrating how to enable the TPU log recorder for debug_print functionality in Pallas. This shows how to compile a kernel with the necessary XLA options to enable logging.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/g3doc/debugging.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nkernel = pl.pallas_call(...)\ncompiled_kernel = (\n       jax.jit(kernel)\n       .lower(x)\n       .compile({'xla_tpu_enable_log_recorder': 'true'})\n )\nresult = compiled_kernel(x)\n```\n\n----------------------------------------\n\nTITLE: Adding Local JAX/jaxlib Wheels to Hermetic Requirements\nDESCRIPTION: This script adds local wheel files for `jax` and `jaxlib` to the `build/requirements.in` file using their absolute paths obtained via `realpath`. It then runs `build.py requirements_update`, specifying the target Python version (e.g., 3.12), to update the lock file for that version, incorporating the local wheels into the hermetic environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_35\n\nLANGUAGE: shell\nCODE:\n```\necho -e \"\\n$(realpath jax-0.4.26-py3-none-any.whl)\" >> build/requirements.in\necho -e \"\\n$(realpath jaxlib-0.4.26-cp312-cp312-manylinux2014_x86_64.whl)\" >> build/requirements.in\npython build/build.py requirements_update --python_version=3.12\n```\n\n----------------------------------------\n\nTITLE: Initializing Test Parameters for JAX Network\nDESCRIPTION: Creates test parameters, input, and target values for demonstrating the checkpointing behavior. This snippet initializes weight matrices and vectors with ones for use in examples.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for JAX\nDESCRIPTION: This requirements file lists Python packages and their exact versions needed for the JAX project. It uses pip's hash-checking mode to ensure package integrity. The file includes dependencies for building, testing, and running JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_11.txt#2025-04-22_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nabsl-py==2.1.0 \\\n    --hash=sha256:526a04eadab8b4ee719ce68f204172ead1027549089702d99b9059f129ff1308 \\\n    --hash=sha256:7820790efbb316739cde8b4e19357243fc3608a152024288513dd968d7d959ff\n    # via -r build/test-requirements.txt\nattrs==23.2.0 \\\n    --hash=sha256:935dc3b529c262f6cf76e50877d35a4bd3c1de194fd41f47a2b7ae8f19971f30 \\\n    --hash=sha256:99b87a485a5820b23b879f04c2305b44b951b502fd64be915879d77a7e8fc6f1\n    # via hypothesis\nauditwheel==6.1.0 \\\n    --hash=sha256:3bdc686e774cf9e355e924b0fe5a562d55caa385d72234ffe7b81b378dba360f \\\n    --hash=sha256:e52f734861859e3743eb29fcac7da9c4921a1e4bea58f954b52f2926f8e9e364\n    # via -r build/test-requirements.txt\nbuild==1.2.1 \\\n    --hash=sha256:526263f4870c26f26c433545579475377b2b7588b6f1eac76a001e873ae3e19d \\\n    --hash=sha256:75e10f767a433d9a86e50d83f418e83efc18ede923ee5ff7df93b6cb0306c5d4\n    # via -r build/test-requirements.txt\ncloudpickle==3.0.0 \\\n    --hash=sha256:246ee7d0c295602a036e86369c77fecda4ab17b506496730f2f576d9016fd9c7 \\\n    --hash=sha256:996d9a482c6fb4f33c1a35335cf8afd065d2a56e973270364840712d9131a882\n    # via -r build/test-requirements.txt\ncolorama==0.4.6 \\\n    --hash=sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44 \\\n    --hash=sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6\n    # via -r build/test-requirements.txt\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Importing Function Documentation with Sphinx autofunction (reStructuredText)\nDESCRIPTION: This reStructuredText snippet utilizes the Sphinx `autofunction` directive to automatically insert documentation for the `pjit` function. It depends on the preceding `automodule` or `currentmodule` directive to establish the context for locating the function within the `jax.experimental.pjit` module.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.pjit.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autofunction:: pjit\n```\n\n----------------------------------------\n\nTITLE: Building jaxlib with CUDA Plugins\nDESCRIPTION: Command to build jaxlib with CUDA plugins, generating wheels for jaxlib without CUDA, jax-cuda-plugin, and jax-cuda-pjrt.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython build/build.py build --wheels=jaxlib,jax-cuda-plugin,jax-cuda-pjrt\n```\n\n----------------------------------------\n\nTITLE: Verifying Array Type Preservation with Python Scalar Operations\nDESCRIPTION: This code demonstrates that when performing operations between arrays of various integer types and Python scalars, the array's dtype is preserved in the result, regardless of its width.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor dtype in [np.int8, np.int16, np.int32, np.int64]:\n  x = np.arange(10, dtype=dtype)\n  assert (x + 2).dtype == dtype\n```\n\n----------------------------------------\n\nTITLE: Installing CPU-only JAX with pip\nDESCRIPTION: Detailed pip commands for installing CPU-only JAX. This ensures both pip is up-to-date and the latest version of JAX is installed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade pip\npip install --upgrade jax\n```\n\n----------------------------------------\n\nTITLE: Illustrating Potential Buffer Lifetime Issue in JAX Function\nDESCRIPTION: This Python function `f(x)` demonstrates a scenario using hypothetical asynchronous operations `ppermute_start` and `ppermute_done`. The potential issue is that the input buffer `x` might be freed by XLA after `z = x + 1` but before the asynchronous `ppermute` operation initiated by `ppermute_start(x)` has finished reading from `x`, leading to reading garbage memory.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  fut = ppermute_start(x)\n  z = x + 1\n  y = ppermute_done(fut)\n  return y, z\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Modulo Function for Circular Neighbor Calculation in Python\nDESCRIPTION: Defines a helper function for calculating modulo to handle circular neighbor indices in a mesh of devices, ensuring proper wraparound behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef mod(x, n):\n  return lax.rem(x + n, n)\n```\n\n----------------------------------------\n\nTITLE: Redefining shmap with psum (Example 1 Context)\nDESCRIPTION: This snippet repeats the definition of `f1` for context before introducing a potential solution. It shows a `shmap` applying `g`, then `psum`, and returning an unmapped output, leading to inefficient transposes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Example 1 again\nf1 = shmap(lambda x: psum(g(x), 'i'),\n           in_specs=P('i'), out_specs=P())\n```\n```\n\n----------------------------------------\n\nTITLE: Composing pmap with Differentiation\nDESCRIPTION: Shows how to combine pmap with JAX's automatic differentiation functionality.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@pmap\ndef f(x):\n  y = jnp.sin(x)\n  @pmap\n  def g(z):\n    return jnp.cos(z) * jnp.tan(y.sum()) * jnp.tanh(x).sum()\n  return grad(lambda w: jnp.sum(g(w)))(x)\n```\n\n----------------------------------------\n\nTITLE: Testing JAX Transformations of log1pexp\nDESCRIPTION: Demonstrating that the log1pexp function is JAX-transformable but has numerical stability issues when differentiated with large inputs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jit, grad, vmap\n\nprint(jit(log1pexp)(3.))\nprint(jit(grad(log1pexp))(3.))\nprint(vmap(jit(grad(log1pexp)))(jnp.arange(3.)))\n```\n\n----------------------------------------\n\nTITLE: Importing Pallas GPU Modules\nDESCRIPTION: Basic imports required for using Pallas and its Mosaic GPU backend functionality.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.experimental.pallas as pl\nimport jax.experimental.pallas.mosaic_gpu as plgpu\n```\n\n----------------------------------------\n\nTITLE: Implementing List Splitting and Merging Utilities for JAX\nDESCRIPTION: Utility functions for splitting lists in half and merging two lists according to a boolean selector list. These are used in linearize and partial evaluation implementations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ndef split_half(lst: list[Any]) -> tuple[list[Any], list[Any]]:\n  assert not len(lst) % 2\n  return split_list(lst, len(lst) // 2)\n\ndef merge_lists(which: list[bool], l1: list[Any], l2: list[Any]) -> list[Any]:\n  l1, l2 = iter(l1), iter(l2)\n  out = [next(l2) if b else next(l1) for b in which]\n  assert next(l1, None) is next(l2, None) is None\n  return out\n```\n\n----------------------------------------\n\nTITLE: Illustrating `shard_map`'s Rank-Preserving Behavior in JAX (Python)\nDESCRIPTION: Demonstrates the conceptual rank-preserving behavior of `shard_map` using a simple 1D mesh. It sets up a mesh `m` over 4 devices and shows that applying `shard_map` with corresponding input/output specs (`P('i')`) is conceptually equivalent to splitting the input `y` into 4 blocks (`y_blk`) using `jnp.split`, applying the function `f` to each block (which maintains the original rank), and then concatenating the results using `jnp.concatenate`. This highlights the key difference from `pmap` where the mapped function receives inputs and produces outputs with the same rank as the original arrays. Dependencies include `jax`, `jax.numpy`, `jax.sharding`, `jax.experimental.shard_map`, and `numpy`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndevices = np.array(jax.devices()[:4])\nm = Mesh(devices, ('i',))  # mesh.shape['i'] = 4\n\nshard_map(f, m, in_specs=P('i'), out_specs=P('i'))(y)\n==\njnp.concatenate([f(y_blk) for y_blk in jnp.split(y, 4)])\n```\n\n----------------------------------------\n\nTITLE: Displaying JAX Converter Error Message in Markdown\nDESCRIPTION: An example of how error messages are displayed for failed conversions, showing the specific error encountered for a particular model and converter combination.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n### Example: `flax/actor_critic_[(_, 4*b, 4*b, _)]` | Converter: `jax2tf_xla`\n```\nInconclusiveDimensionOperation(\"Cannot divide '-1*b' by '2'.\\nSee https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#division-of-shape-polynomials-is-partially-supported.\\n\\nThis error arises for arithmetic or comparison operations with shapes that\\nare non-constant, and the result of the operation cannot be represented as\\na polynomial of dimension variables, or a boolean constant (for comparisons).\\n\\nPlease see https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#computing-with-dimension-variables\\nfor more details.\\n\")\n```\n\n----------------------------------------\n\nTITLE: Using jax.scipy.integrate.trapezoid in Python\nDESCRIPTION: Example of using the newly added jax.scipy.integrate.trapezoid function for numerical integration using the trapezoidal rule.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nimport jax.numpy as jnp\nfrom jax.scipy import integrate\n\nx = jnp.linspace(0, 1, 100)\ny = x**2\nresult = integrate.trapezoid(y, x)\n```\n\n----------------------------------------\n\nTITLE: Calculating 1st Order Derivative using nth_order_derivative in Python\nDESCRIPTION: This snippet demonstrates calculating the 1st order derivative using the previously defined `nth_order_derivative` function. It calls the function with n=1 on a presumed function `foo` at point 2.0.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_4\n\nLANGUAGE: ipython3\nCODE:\n```\nprint(nth_order_derivative(1, foo, 2.0))\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Dependencies with Pre-release NumPy\nDESCRIPTION: Specifies the use of pre-release NumPy package from the scientific-python-nightly-wheels index. This allows JAX to work with the latest NumPy features before their official release.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_14_ft.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n--pre\n--extra-index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple\nnumpy\n```\n\n----------------------------------------\n\nTITLE: Demonstration: Constant-only Jaxpr Generation Limitation (Python)\nDESCRIPTION: Uses make_jaxpr_v1 to stage a computation with only constants, demonstrating a known limitation in earlier tracing logic due to find_top_trace. Calls the multiplication primitive with two constants, then prints the result. Shows the need for omnistaging or dynamic tracing to capture such computations properly.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\njaxpr, consts, _ = make_jaxpr_v1(lambda: mul(2., 2.))\nprint(jaxpr)\n```\n\n----------------------------------------\n\nTITLE: Using the custom_jvp function with pytree input\nDESCRIPTION: Shows how to create a Point namedtuple instance and pass it to the custom_jvp function defined earlier. This demonstrates the basic usage of the function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_58\n\nLANGUAGE: python\nCODE:\n```\npt = Point(1., 2.)\n\nprint(f(pt))\n```\n\n----------------------------------------\n\nTITLE: Using Python Control Flow with JAX Autodiff ('grad') without 'jit' in Python\nDESCRIPTION: Defines a Python function `f(x)` using standard `if/else` control flow. It then successfully computes the gradient of this function using `jax.grad` for inputs on both sides of the conditional branch, demonstrating that native Python control flow works with `grad` when `jit` is not used. Requires `jax.grad`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import grad\n\ndef f(x):\n  if x < 3:\n    return 3. * x ** 2\n  else:\n    return -4 * x\n\nprint(grad(f)(2.))  # ok!\nprint(grad(f)(4.))  # ok!\n```\n\n----------------------------------------\n\nTITLE: Pallas Call Function Definition\nDESCRIPTION: Core function signature for pallas_call showing required parameters for kernel execution including shape specifications, input/output specs, and grid configuration.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/design.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef pallas_call(\n    kernel: Callable,\n    out_shape: Sequence[jax.ShapeDtypeStruct],\n    *,\n    in_specs: Sequence[Spec],\n    out_specs: Sequence[Spec],\n    grid: Optional[Tuple[int, ...]] = None) -> Callable:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Initializing JAX and Pallas TPU Environment - Python\nDESCRIPTION: This snippet imports foundational JAX, NumPy, and Pallas modules for writing kernels on a TPU. It checks that the hardware device is a TPU, a required dependency, and prints the current device kind. All examples in this file assume correct import and TPU setup, and failure to detect a TPU will result in an explicit assertion error. There are no direct inputs or outputs, but it must be executed before running further code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\\nimport timeit\\nimport numpy as np\\nimport jax\\nfrom jax import numpy as jnp\\nfrom jax import lax\\nfrom jax.experimental import checkify\\nfrom jax.experimental import pallas as pl\\nfrom jax.experimental.pallas import tpu as pltpu\\n\\nassert \\\"TPU\\\" in jax.devices()[0].device_kind, \\\"Please run this notebook with TPU devices.\\\"\\nprint(\\\"Running on\\\", jax.devices()[0].device_kind)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Bayesian Inference in JAX\nDESCRIPTION: Imports necessary libraries including JAX, NumPy, SciPy, and Matplotlib for Bayesian inference and visualization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/vmapped_log_probs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nimport jax\n\nimport jax.numpy as jnp\nimport jax.scipy as jsp\nfrom jax import random\n\nimport numpy as np\nimport scipy as sp\n```\n\n----------------------------------------\n\nTITLE: Testing JAX with Custom Local Python 3.13 and Lock File using Bazel\nDESCRIPTION: This command runs a JAX test target using Bazel with a custom Python 3.13 interpreter provided from the local filesystem and a custom requirements lock file. Environment variables set via `--repo_env` specify the Python version (`HERMETIC_PYTHON_VERSION`), the local path to the Python archive (`HERMETIC_PYTHON_URL` using `file://`), an optional prefix to strip within the archive (`HERMETIC_PYTHON_PREFIX`), the archive's checksum (`HERMETIC_PYTHON_SHA256`), and the absolute path to the custom lock file (`HERMETIC_REQUIREMENTS_LOCK`). Assumes the custom lock file exists at the specified path.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\nbazel test <target> \\\n  --repo_env=HERMETIC_PYTHON_VERSION=3.13 \\\n  --repo_env=HERMETIC_PYTHON_URL=\"file:///path/to/cpython.tar.gz\" \\\n  --repo_env=HERMETIC_PYTHON_PREFIX=\"prefix/to/strip/in/cython/tar/gz/archive\" \\\n  --repo_env=HERMETIC_PYTHON_SHA256=<sha256_sum> \\\n  --repo_env=HERMETIC_REQUIREMENTS_LOCK=\"/absolute/path/to/build:custom_requirements_lock.txt\"\n```\n\n----------------------------------------\n\nTITLE: Local Testing of JAX Documentation Build (Bash)\nDESCRIPTION: Series of commands to set up a local environment and test the JAX documentation build process, mirroring the steps used on Read the Docs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_54\n\nLANGUAGE: bash\nCODE:\n```\nmkvirtualenv jax-docs\nmkdir jax-docs\ncd jax-docs\ngit clone --no-single-branch --depth 50 https://github.com/jax-ml/jax\ncd jax\ngit checkout --force origin/test-docs\ngit clean -d -f -f\nworkon jax-docs\n\npython -m pip install --upgrade --no-cache-dir pip\npython -m pip install --upgrade --no-cache-dir -I Pygments==2.3.1 setuptools==41.0.1 docutils==0.14 mock==1.0.1 pillow==5.4.1 alabaster>=0.7,<0.8,!=0.7.5 commonmark==0.8.1 recommonmark==0.5.0 'sphinx<2' 'sphinx-rtd-theme<0.5' 'readthedocs-sphinx-ext<1.1'\npython -m pip install --exists-action=w --no-cache-dir -r docs/requirements.txt\ncd docs\npython `which sphinx-build` -T -E -b html -d _build/doctrees-readthedocs -D language=en . _build/html\n```\n\n----------------------------------------\n\nTITLE: Installing JAX Nightly for CPU Only with Pip - Bash\nDESCRIPTION: Installs or upgrades to the latest nightly (pre-release) versions of JAX and jaxlib targeting CPU usage only, via pip. The '-U --pre' flags ensure both upgrade and acceptance of pre-release versions. The -f flag specifies the nightly builds find-links repository URL. Prerequisite: Python and pip must be installed. Input is the command; output is a CPU-only JAX setup, potentially with breaking changes due to nightly updates. These pre-release builds may be less stable than regular releases.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install -U --pre jax jaxlib -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html\n\n```\n\n----------------------------------------\n\nTITLE: Checking NVIDIA CUDA Version\nDESCRIPTION: Command to check the installed NVIDIA CUDA version, which is necessary to determine compatibility with JAX versions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnvcc --version\n```\n\n----------------------------------------\n\nTITLE: Illustrating XLA Inserting Copy Inside Loop\nDESCRIPTION: Shows how XLA might insert a defensive `copy` at the beginning of the loop body (`x = copy(x)`). Due to the combined aliasing from `fori_loop` (input/output) and the `ppermute` operations, XLA detects potential buffer conflicts where the buffer being written to (`y`) might alias with the buffer being read from (`x`) in the *next* iteration's `ppermute_start`. The copy breaks this aliasing, ensuring correctness but adding overhead.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  def body(i, x):\n    x = copy(x)\n    *sems, x, y = ppermute_start(x)\n    y = ppermute_done((*sems, x, y))\n    return y\n  return fori_loop(0, 8, body, x)\n```\n```\n\n----------------------------------------\n\nTITLE: JAX Export Wrapped Main Function Signature in MLIR\nDESCRIPTION: Shows the signature of the _wrapped_jax_export_main function, which includes platform index, dimension variables, tokens, and input arguments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_13\n\nLANGUAGE: mlir\nCODE:\n```\n      func private _wrapped_jax_export_main(\n          platform_index: i32 {jax.global_constant=\"_platform_index\"},\n          arg_h: i32 {jax.global_constant=\"h\"},\n          arg_w: i32 {jax.global_constant=\"w\"},\n          arg_token: stablehlo.token {jax.token=True},\n          arg: f32[?, ?]) -> (stablehlo.token, ...)\n```\n\n----------------------------------------\n\nTITLE: Comparing Module Sizes for Single-Platform and Multi-Platform Exports - JAX - Python\nDESCRIPTION: This code demonstrates and compares the size of the exported StableHLO module serialized blobs for single-platform and multi-platform exports of a computationally intensive function. It uses JAX's export and serialization utilities to show the effect of platform count on module size. The snippet requires JAX and computes and prints the serialized module length for both cases.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from jax import export\n>>> from jax import lax\n>>> # A largish function\n>>> def f(x):\n...   for i in range(1000):\n...     x = jnp.cos(x)\n...   return x\n\n>>> exp_single = export.export(jax.jit(f))(1.)\n>>> len(exp_single.mlir_module_serialized)  # doctest: +SKIP\n9220\n\n>>> exp_multi = export.export(jax.jit(f),\n...                           platforms=[\"cpu\", \"tpu\", \"cuda\"])(1.)\n>>> len(exp_multi.mlir_module_serialized)  # doctest: +SKIP\n9282\n\n```\n\n----------------------------------------\n\nTITLE: Disabling JAX GPU Memory Preallocation with Environment Variable\nDESCRIPTION: Sets an environment variable to disable JAX's default behavior of preallocating 75% of GPU memory, making JAX allocate GPU memory as needed instead. This can help avoid OOM errors but may increase memory fragmentation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_memory_allocation.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nXLA_PYTHON_CLIENT_PREALLOCATE=false\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Python Function for JAX Transformation\nDESCRIPTION: An example function that demonstrates primitive operations (sin, multiplication, addition, negation) that can be transformed by JAX. This represents the type of function that JAX transformations operate on.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n```\n\n----------------------------------------\n\nTITLE: Comparing Gradient Handling for Unused Arguments in JAX and TensorFlow\nDESCRIPTION: Shows differences in handling gradients for unused arguments between native TensorFlow and jax2tf.convert. JAX2TF returns zeros for unused float arguments but None for unused integer arguments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# x1 and x3 are not used. x3 has integer type.\ndef fn(x0, x1, x2, x3):\n  return x0 * 0. + x2 * 2.\n\nxs = [tf.Variable(x) for x in [10., 11., 12., 13]]\nwith tf.GradientTape(persistent=True) as tape:\n res = fn(*xs)\n\ng_tf_native = tape.gradient(res, xs)\n# Returns: 0., None, 2., None\n\ng_tf_native_0 = tape.gradient(res, xs,\n                              unconnected_gradients=tf.UnconnectedGradients.ZERO)\n# Returns: 0., 0., 2., 0\n\n# Now with jax2tf.convert\nwith tf.GradientTape() as tape:\n  res = jax2tf.convert(fn, with_gradient=True)(*xs)\n\ng_jax2tf = tape.gradient(res, xs)\n# Returns: 0., 0., 2., None\n# Note that the gradient for x1 is 0.\n\ng_jax2tf_0 = tape.gradient(res, xs,\n                            unconnected_gradients=tf.UnconnectedGradients.ZERO)\n# Returns: 0., 0., 2., 0\n# In this case we get the same result as for TF native.\n```\n\n----------------------------------------\n\nTITLE: Constructing and Printing a Jaxpr in Python\nDESCRIPTION: Demonstrates using the `build_jaxpr` function to convert a Python function `foo` (assumed to take one argument) into its corresponding `Jaxpr` representation. The resulting `Jaxpr` object is then printed to the console, utilizing the `__str__` method defined in the `Jaxpr` class.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(build_jaxpr(foo, 1))\n```\n\n----------------------------------------\n\nTITLE: HBM to VMEM Accumulation in Original Reduce-Scatter Kernel\nDESCRIPTION: Original implementation for copying data from HBM to VMEM accumulator, incrementing it, and copying results back to HBM using async copies in a reduce-scatter operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/distributed.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlocal_copy = pltpu.make_async_copy(\n    src_ref=hbm_scratch.at[working_slot, current_phase_slice],\n    dst_ref=accum_scratch,\n    sem=local_copy_sem,\n)\nlocal_copy.start()\nlocal_copy.wait()\n@pl.when(~last_iteration)\ndef _():\n  @pl.when(phase == LEFT)\n  def _():\n    accum_scratch[...] += x_ref[left_copy_device, left_copy_slice]\n  @pl.when(phase == RIGHT)\n  def _():\n    accum_scratch[...] += x_ref[right_copy_device, right_copy_slice]\nlocal_copy = pltpu.make_async_copy(\n    src_ref=accum_scratch,\n    dst_ref=hbm_scratch.at[working_slot, current_phase_slice],\n    sem=local_copy_sem,\n)\nlocal_copy.start()\nlocal_copy.wait()\n```\n\n----------------------------------------\n\nTITLE: Illustrating XLA's Defensive Copy for In-Place Operations\nDESCRIPTION: This Python function demonstrates how XLA resolves the conflict shown previously. When XLA detects that an input buffer (`x`) to an in-place operation (`add_one_inplace`) is also used later (`x * 2`), it inserts a `copy` operation. This ensures that `add_one_inplace` operates on a separate buffer (`x2`), preserving the original `x` for subsequent use.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  x2 = copy(x)\n  y = add_one_inplace(x2)\n  return y, x * 2\n```\n```\n\n----------------------------------------\n\nTITLE: GNN Convolution Segment Error\nDESCRIPTION: Runtime error in GNN convolution where segment IDs are out of valid range during UnsortedSegmentSum operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n{{function_node __wrapped__UnsortedSegmentSum_device_/job:localhost/replica:0/task:0/device:CPU:0}} segment_ids[0] = 92 is out of range [0, 52) [Op:UnsortedSegmentSum]\n```\n\n----------------------------------------\n\nTITLE: Using jax.jit with donate_argnames in Python\nDESCRIPTION: Example of using the new donate_argnames parameter in jax.jit to specify which arguments should be donated for memory optimization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nimport jax\n\n@jax.jit(donate_argnames=('x',))\ndef my_function(x, y):\n    return x + y\n\nresult = my_function(jax.numpy.array([1, 2, 3]), jax.numpy.array([4, 5, 6]))\n```\n\n----------------------------------------\n\nTITLE: Error Example: Invalid Shape Parameter in JAX Reshape Operation\nDESCRIPTION: Illustrates an error that occurs when using non-integer arrays as shape parameters in a JAX reshape operation within a shape-polymorphic context.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: jnp.reshape(x, (x.shape[0] * np.array([x.shape[1]]),)),\n                polymorphic_shapes=[\"(b, 4)\"])(np.ones((3, 4)))\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Verification\nDESCRIPTION: This snippet shows how Python package dependencies are specified in a requirements file, including version numbers and SHA256 hash values for verification. It demonstrates the format for multiple packages.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13.txt#2025-04-22_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nsix==1.16.0 \\\n    --hash=sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926 \\\n    --hash=sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\n    # via python-dateutil\nsortedcontainers==2.4.0 \\\n    --hash=sha256:25caa5a06cc30b6b83d11423433f65d1f9d76c4c6a0c90e3379eaa43b9bfdb88 \\\n    --hash=sha256:a163dcaede0f1c021485e957a39245190e74249897e2ae4b2aa38595db237ee0\n    # via hypothesis\ntensorstore==0.1.73 \\\n    --hash=sha256:03cec5141a27d2e65e4ff604641cfb1f7989d66c361534392e810b80cbda617d \\\n    --hash=sha256:0429bf781ce3ed45be761b46f4bc5979412dadf063f509cb7e9581981a1e097b\n```\n\n----------------------------------------\n\nTITLE: Using Pre-compiled Modules with Auto PGLE (Non-working Example)\nDESCRIPTION: Example demonstrating that Auto PGLE doesn't work with pre-compiled modules, as it requires recompilation during execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax._src import config\n\ntrain_step_compiled = train_step().lower().compile()\n\nwith config.enable_pgle(True), config.pgle_profiling_runs(1):\n  train_step_compiled()\n  # No effect since module was pre-compiled.\n  train_step_compiled()\n```\n\n----------------------------------------\n\nTITLE: Attempting to Run JIT-Compiled Conditional Function\nDESCRIPTION: Tries to execute the JIT-compiled conditional function, which will fail due to JAX's constraints on dynamic control flow in compiled code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    g(2)\nexcept Exception as e:\n    print(e)\n    pass\n```\n\n----------------------------------------\n\nTITLE: Testing JAX with Scenario-Specific Dependencies using Bazel (Scenario 2)\nDESCRIPTION: Similar to the previous example, this command runs a JAX test target using Bazel but selects the dependency set defined for 'scenario2' associated with Python 3.13. It sets `HERMETIC_PYTHON_VERSION` to `3.13-scenario2` using `--repo_env`, assuming a corresponding entry in the `WORKSPACE` file's `requirements` dictionary.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\n# To build with scenario2 dependendencies:\nbazel test <target> --repo_env=HERMETIC_PYTHON_VERSION=3.13-scenario2\n```\n\n----------------------------------------\n\nTITLE: Automating Nightly JAX Regression Testing with Docker (Bash)\nDESCRIPTION: This Bash script automates the process of testing JAX performance across multiple nightly builds. It iterates through specified months (7, 8, 9) and days (1-30), launching a specific NVIDIA JAX nightly Docker container for each date. Inside each container, it executes the `/dir/test.sh` script (mounted from the host) to run the performance test and redirects standard output and error to a dated file (`OUT-mm-dd`). It requires Docker, GPU access (`--gpus=all`), and the availability of the `ghcr.io/nvidia/jax:nightly-YYYY-MM-DD` images.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/investigating_a_regression.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n  for m in 7 8 9; do\n    for d in `seq -w 1 30`; do\n      docker run -v $PWD:/dir --gpus=all ghcr.io/nvidia/jax:nightly-2023-0${m}-${d} /bin/bash /dir/test.sh &> OUT-0${m}-${d}\n    done\n  Done\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Pallas Dependencies for TPU Pipelining\nDESCRIPTION: Sets up the required imports from JAX and its Pallas framework for TPU memory pipelining. Includes core JAX, Pallas TPU-specific modules, and NumPy dependencies.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/pipelining.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax.experimental import pallas as pl\nfrom jax.experimental.pallas import tpu as pltpu\nimport jax.numpy as jnp\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Pretty Printing Implementation for Conditionals in Python\nDESCRIPTION: Implements pretty printing functionality for conditional operations in JAX. Formats the conditional expression with proper indentation and parameter display.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_95\n\nLANGUAGE: python\nCODE:\n```\ndef pprint_cond(names: defaultdict[Var, str], eqn: JaxprEqn) -> PPrint:\n  true_jaxpr, false_jaxpr = eqn.params['true_jaxpr'], eqn.params['false_jaxpr']\n  new_params = {k:v for k, v in eqn.params.items() if not k.endswith('jaxpr')}\n  lhs = pp(' '.join(var_str(names, v) for v in eqn.out_binders))\n  rhs = (pp(eqn.primitive.name) >> pp_params(new_params) >>\n         pp(' '.join(names[x] if isinstance(x, Var) else str(x.val)\n                     for x in eqn.inputs)))\n  return vcat([lhs >> pp(' = ') >> rhs,\n               pp_jaxpr(true_jaxpr).indent(2),\n               pp_jaxpr(false_jaxpr).indent(2)])\npp_rules[cond_p] = pprint_cond\n```\n\n----------------------------------------\n\nTITLE: Adding Specific JAX/jaxlib Versions to Hermetic Requirements\nDESCRIPTION: This script adds specific version requirements for `jax` and `jaxlib` (e.g., >= 0.4.26) to the `build/requirements.in` file. Subsequently, it runs the `build.py requirements_update` command to regenerate the corresponding lock file, ensuring these versions are included in the hermetic Python environment used by Bazel.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_34\n\nLANGUAGE: shell\nCODE:\n```\necho -e \"\\njax >= 0.4.26\" >> build/requirements.in\necho -e \"\\njaxlib >= 0.4.26\" >> build/requirements.in\npython build/build.py requirements_update\n```\n\n----------------------------------------\n\nTITLE: Controlling Generated Test Cases in Bazel Tests\nDESCRIPTION: This command runs JAX tests using Bazel while controlling the number of combinatorially generated test cases. The `--test_env=JAX_NUM_GENERATED_CASES=25` flag passes the environment variable `JAX_NUM_GENERATED_CASES` with a value of 25 to the test execution environment, overriding the default number of generated cases.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_40\n\nLANGUAGE: shell\nCODE:\n```\n# Bazel\nbazel test //tests/... --test_env=JAX_NUM_GENERATED_CASES=25`\n```\n\n----------------------------------------\n\nTITLE: Listing exported IR modules\nDESCRIPTION: This snippet shows how to list the exported and JIT compiled IR modules that were dumped to a directory.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n$ ls -l /tmp/export.dumps/\ntotal 32\n-rw-rw-r--@ 1 necula  wheel  2316 Jun 19 11:04 jax_ir0_jit_sin_export.mlir\n-rw-rw-r--@ 1 necula  wheel  2279 Jun 19 11:04 jax_ir1_jit_sin_compile.mlir\n-rw-rw-r--@ 1 necula  wheel  3377 Jun 19 11:04 jax_ir2_jit_call_exported_compile.mlir\n-rw-rw-r--@ 1 necula  wheel  2333 Jun 19 11:04 jax_ir3_jit_my_fun_export.mlir\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: A comprehensive list of Python package dependencies with version specifications. Includes both direct dependencies and development tools needed for the JAX project. Notable constraints include matplotlib version pinning for Python 3.10 and conditional requirements based on Python versions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/test-requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py\nbuild\ncloudpickle\ncolorama>=0.4.4\nfilelock\nflatbuffers\nhypothesis\nmpmath>=1.3\npillow>=10.4.0\n# TODO(kanglan): Remove once psutil from portpicker supports python 3.13t\nportpicker; python_version<\"3.13\"\npytest-xdist\nwheel\nrich\nsetuptools\n# matplotlib 3.9.0 pins NumPy 1.23, which is incompatible with the requirement\n# below.\nmatplotlib~=3.8.4; python_version==\"3.10\"\nmatplotlib; python_version>=\"3.11\"\nopt-einsum\nauditwheel\n```\n\n----------------------------------------\n\nTITLE: Using nansum Outside of JAX Transforms\nDESCRIPTION: This snippet demonstrates the use of the nansum function outside of JAX transforms, where it works as expected.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nx = jnp.array([1, 2, jnp.nan, 3, 4])\nprint(nansum(x))\n```\n\n----------------------------------------\n\nTITLE: Waiting on a Barrier for Thread Synchronization in PLGPU (Python)\nDESCRIPTION: Shows canonical use of 'plgpu.barrier_wait' to ensure a thread blocks until a PTX barrier (allocated in SMEM) signals completion. The function expects as input a reference to a Barrier previously set up for synchronization. Correct use is required in any concurrent workflow using PLGPU barriers to avoid data structure corruption and ensure backpressure.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nplgpu.barrier_wait(barrier)\n```\n\n----------------------------------------\n\nTITLE: Array Type Syntax Grammar for Device Variance in JAX\nDESCRIPTION: Defines the grammar for array type syntax that includes device variance information. Arrays can be annotated with the mesh axes along which they may vary.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/17111-shmap-transpose.md#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\nshaped_array ::= <dtype>[<int_literal>, ...]<device_variance_type>\ndevice_variance_type ::= {<axis_name>, ...}\n```\n\n----------------------------------------\n\nTITLE: Controlling CUDA Async Allocator Preallocation\nDESCRIPTION: Sets an environment variable to configure preallocation size for the CUDA async allocator. Setting to -1 uses the default amount, or specify a byte size for custom preallocation to mitigate performance variability.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_memory_allocation.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nTF_CUDA_MALLOC_ASYNC_SUPPORTED_PREALLOC=N\n```\n\n----------------------------------------\n\nTITLE: Vmap Rule for XLA Call Primitive\nDESCRIPTION: Implements the vectorized mapping (vmap) transformation rule for the XLA call primitive, enabling vectorization of JIT-compiled functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndef xla_call_vmap_rule(axis_size, vals_in, dims_in, *, jaxpr, num_consts):\n  del num_consts  # Unused\n  new_jaxpr, new_consts = vmap_jaxpr(jaxpr, axis_size, tuple(dims_in))\n  outs = bind(xla_call_p, *new_consts, *vals_in, jaxpr=new_jaxpr,\n              num_consts=len(new_consts))\n  return outs, [0] * len(outs)\nvmap_rules[xla_call_p] = xla_call_vmap_rule\n\n@lru_cache\ndef vmap_jaxpr(jaxpr: Jaxpr, axis_size: int, bdims_in: tuple[BatchAxis, ...]\n               ) -> tuple[Jaxpr, list[Any]]:\n  vmap_traceable = vmap(jaxpr_as_fun(jaxpr), tuple(bdims_in))\n  in_avals = [unmapped_aval(axis_size, d, v.aval)\n              for v, d in zip(jaxpr.in_binders, bdims_in)]\n  new_jaxpr, new_consts, _ = make_jaxpr(vmap_traceable, *in_avals)\n  return new_jaxpr, new_consts\n\ndef unmapped_aval(axis_size: int, batch_dim: BatchAxis, aval: ShapedArray\n                  ) -> ShapedArray:\n  if batch_dim is not_mapped:\n    return aval\n  else:\n    shape = list(aval.shape)\n    shape.insert(batch_dim, axis_size)\n    return ShapedArray(tuple(shape), aval.dtype)\n```\n\n----------------------------------------\n\nTITLE: Referencing JAX implementation of scipy.special.lpmn_values\nDESCRIPTION: Mentions the JAX implementation of lpmn_values function, which has similar weaknesses to lpmn in terms of API divergence and limited use cases.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/18137-numpy-scipy-scope.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\njax.scipy.special.lpmn_values\n```\n\n----------------------------------------\n\nTITLE: Documenting JAX Scipy Submodules and API Overview - reStructuredText\nDESCRIPTION: These reStructuredText snippets utilize the Sphinx automodule and autosummary directives to document the jax.scipy module and its submodules in a structured fashion. Dependencies include Sphinx and appropriate configurations for autodoc/autosummary extensions. Each autosummary block lists public interface functions and classes for a submodule, with :toctree: directives for hierarchical navigation. Inputs are module and function/class names; outputs are structured documentation stubs. Designed primarily for Sphinx-based doc builds and not executed as code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.scipy.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``jax.scipy`` module\n====================\n\njax.scipy.cluster\n-----------------\n\n.. automodule:: jax.scipy.cluster.vq\n\n.. autosummary::\n   :toctree: _autosummary\n   \n   vq\n\njax.scipy.fft\n-------------\n\n.. automodule:: jax.scipy.fft\n\n.. autosummary::\n  :toctree: _autosummary\n\n   dct\n   dctn\n   idct\n   idctn\n\njax.scipy.integrate\n-------------------\n\n.. automodule:: jax.scipy.integrate\n\n.. autosummary::\n  :toctree: _autosummary\n\n   trapezoid\n\njax.scipy.interpolate\n---------------------\n\n.. automodule:: jax.scipy.interpolate\n\n.. autosummary::\n  :toctree: _autosummary\n\n   RegularGridInterpolator\n\n\njax.scipy.linalg\n----------------\n\n.. automodule:: jax.scipy.linalg\n\n.. autosummary::\n  :toctree: _autosummary\n\n   block_diag\n   cho_factor\n   cho_solve\n   cholesky\n   det\n   eigh\n   eigh_tridiagonal\n   expm\n   expm_frechet\n   funm\n   hessenberg\n   hilbert\n   inv\n   lu\n   lu_factor\n   lu_solve\n   pascal\n   polar\n   qr\n   rsf2csf\n   schur\n   solve\n   solve_triangular\n   sqrtm\n   svd\n   toeplitz\n\njax.scipy.ndimage\n-----------------\n\n.. automodule:: jax.scipy.ndimage\n\n.. autosummary::\n  :toctree: _autosummary\n\n   map_coordinates\n\njax.scipy.optimize\n------------------\n\n.. automodule:: jax.scipy.optimize\n\n.. autosummary::\n  :toctree: _autosummary\n\n   minimize\n   OptimizeResults\n\njax.scipy.signal\n----------------\n\n.. automodule:: jax.scipy.signal\n\n.. autosummary::\n  :toctree: _autosummary\n\n   fftconvolve\n   convolve\n   convolve2d\n   correlate\n   correlate2d\n   csd\n   detrend\n   istft\n   stft\n   welch\n\njax.scipy.spatial.transform\n---------------------------\n\n.. automodule:: jax.scipy.spatial.transform\n\n.. autosummary::\n  :toctree: _autosummary\n\n   Rotation\n   Slerp\n\njax.scipy.sparse.linalg\n-----------------------\n\n.. automodule:: jax.scipy.sparse.linalg\n\n.. autosummary::\n  :toctree: _autosummary\n\n   bicgstab\n   cg\n   gmres\n\njax.scipy.special\n-----------------\n\n.. automodule:: jax.scipy.special\n\n.. autosummary::\n  :toctree: _autosummary\n\n   bernoulli\n   beta\n   betainc\n   betaln\n   digamma\n   entr\n   erf\n   erfc\n   erfinv\n   exp1\n   expi\n   expit\n   expn\n   factorial\n   fresnel\n   gamma\n   gammainc\n   gammaincc\n   gammaln\n   gammasgn\n   hyp1f1\n   i0\n   i0e\n   i1\n   i1e\n   kl_div\n   log_ndtr\n   log_softmax\n   logit\n   logsumexp\n   lpmn\n   lpmn_values\n   multigammaln\n   ndtr\n   ndtri\n   poch\n   polygamma\n   rel_entr\n   softmax\n   spence\n   sph_harm\n   xlog1py\n   xlogy\n   zeta\n\n\njax.scipy.stats\n---------------\n\n.. automodule:: jax.scipy.stats\n\n.. autosummary::\n  :toctree: _autosummary\n\n   mode\n   rankdata\n   sem\n\njax.scipy.stats.bernoulli\n~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.bernoulli\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpmf\n   pmf\n   cdf\n   ppf\n\njax.scipy.stats.beta\n~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.beta\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n   cdf\n   logcdf\n   sf\n   logsf\n\njax.scipy.stats.betabinom\n~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.betabinom\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpmf\n   pmf\n\njax.scipy.stats.binom\n~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.binom\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpmf\n   pmf\n\njax.scipy.stats.cauchy\n~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.cauchy\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n   cdf\n   logcdf\n   sf\n   logsf\n   isf\n   ppf\n\njax.scipy.stats.chi2\n~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.chi2\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n   cdf\n   logcdf\n   sf\n   logsf\n\n\njax.scipy.stats.dirichlet\n~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.dirichlet\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n\njax.scipy.stats.expon\n~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.expon\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n   logcdf\n   cdf\n   logsf\n   sf\n   ppf\n\njax.scipy.stats.gamma\n~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.gamma\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n   cdf\n   logcdf\n   sf\n   logsf\n\njax.scipy.stats.gennorm\n~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.gennorm\n.. autosummary::\n  :toctree: _autosummary\n\n   cdf\n   logpdf\n   pdf\n\njax.scipy.stats.geom\n~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.geom\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpmf\n   pmf\n\njax.scipy.stats.laplace\n~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.laplace\n.. autosummary::\n  :toctree: _autosummary\n\n   cdf\n   logpdf\n   pdf\n\njax.scipy.stats.logistic\n~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.logistic\n.. autosummary::\n  :toctree: _autosummary\n\n   cdf\n   isf\n   logpdf\n   pdf\n   ppf\n   sf\n\njax.scipy.stats.multinomial\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.multinomial\n.. autosummary::\n  :toctree: _autosummary\n\n   logpmf\n   pmf\n\njax.scipy.stats.multivariate_normal\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.multivariate_normal\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n\njax.scipy.stats.nbinom\n~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.nbinom\n\n.. autosummary::\n  :toctree: _autosummary\n\n   logpmf\n   pmf\n\njax.scipy.stats.norm\n~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.norm\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n   cdf\n   logcdf\n   ppf\n   sf\n   logsf\n   isf\n\njax.scipy.stats.pareto\n~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.pareto\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n\njax.scipy.stats.poisson\n~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.poisson\n.. autosummary::\n  :toctree: _autosummary\n\n   logpmf\n   pmf\n   cdf\n\njax.scipy.stats.t\n~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.t\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n\njax.scipy.stats.truncnorm\n~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.truncnorm\n.. autosummary::\n  :toctree: _autosummary\n\n   cdf\n   logcdf\n   logpdf\n   logsf\n   pdf\n   sf\n\njax.scipy.stats.uniform\n~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.uniform\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n   cdf\n   ppf\n\njax.scipy.stats.gaussian_kde\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. currentmodule:: jax.scipy.stats\n.. autosummary::\n  :toctree: _autosummary\n\n   gaussian_kde\n   gaussian_kde.evaluate\n   gaussian_kde.integrate_gaussian\n   gaussian_kde.integrate_box_1d\n   gaussian_kde.integrate_kde\n   gaussian_kde.resample\n   gaussian_kde.pdf\n   gaussian_kde.logpdf\n\njax.scipy.stats.vonmises\n~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.vonmises\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n\njax.scipy.stats.wrapcauchy\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: jax.scipy.stats.wrapcauchy\n.. autosummary::\n  :toctree: _autosummary\n\n   logpdf\n   pdf\n\n```\n\n----------------------------------------\n\nTITLE: Workaround for tf.Module Magic Conversion Issues\nDESCRIPTION: Demonstrates a workaround for handling complex data structures with tf.Module where automatic container wrapping causes errors. This approach uses tree_flatten and tree_unflatten to preserve JAX pytree structure.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\ninput_data = #Any data object\n\nm = tf.Module()\nflat, tree_def = jax.tree_util.tree_flatten(input_data)\nm.input_data = {\"flat\": flat, \"tree_def\": tree_def}\n```\n\n----------------------------------------\n\nTITLE: Markdown Note About Documentation Status\nDESCRIPTION: A Markdown note block indicating this section is a placeholder for upcoming content in the JAX tutorials draft with references to existing documentation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/_tutorials/profiling-and-performance.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{note}\nThis is a placeholder for a section in the new {ref}`jax-tutorials-draft`.\n\nFor the time being, you may find some related content in the old documentation:\n- {doc}`../profiling`\n- {doc}`../device_memory_profiling`\n- {doc}`../transfer_guard`\n```\n```\n\n----------------------------------------\n\nTITLE: JAX to TF XLA Conversion Error - LM1B Model\nDESCRIPTION: Dimension polynomial comparison error when converting LM1B model with non-constant shapes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nInconclusiveDimensionOperation(\"Dimension polynomial comparison 'b' == '2' is inconclusive\\n\\nThis error arises for arithmetic or comparison operations with shapes that\\nare non-constant, and the result of the operation cannot be represented as\\na polynomial of dimension variables, or a boolean constant (for comparisons).\")\n```\n\n----------------------------------------\n\nTITLE: Setting JAX Persistent Cache Minimum Compile Time\nDESCRIPTION: Sets an environment variable to limit the number of cache entries written to the persistent cache based on compilation time.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_22\n\nLANGUAGE: Bash\nCODE:\n```\nJAX_PERSISTENT_CACHE_MIN_COMPILE_TIME_SECS=$N\n```\n\n----------------------------------------\n\nTITLE: Launching ROCm JAX Docker Container\nDESCRIPTION: Commands to launch and attach to a ROCm JAX Docker container with necessary device mappings and configurations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/rocm/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run -it -d --network=host --device=/dev/kfd --device=/dev/dri --ipc=host --shm-size 64G --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v $(pwd):/jax_dir --name rocm_jax rocm/jax-community:latest /bin/bash\n\ndocker attach rocm_jax\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Requirements with Bazel for Python 3.12\nDESCRIPTION: Bazel command for updating pinned requirements for JAX with Python 3.12. This command generates the requirements file automatically, ensuring consistent dependencies across development environments.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_12.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel run //build:requirements.update\n```\n\n----------------------------------------\n\nTITLE: Loading PGLE-optimized Functions from Cache for Nsight Systems Profiling\nDESCRIPTION: Command to load previously cached PGLE-optimized functions while running NVIDIA Nsight Systems profiler.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nJAX_COMPILATION_CACHE_EXPECT_PGLE=yes nsys profile python my-model.py\n```\n\n----------------------------------------\n\nTITLE: Running JAX Performance Benchmark within Docker Container (Bash)\nDESCRIPTION: This Bash script is designed to be executed inside a JAX nightly Docker container (invoked by `test_runner.sh`). It first installs required Python packages using pip. Then, it clones the 'XLB' repository from GitHub, sets the PYTHONPATH environment variable to include the current directory, limits execution to a single GPU (CUDA_VISIBLE_DEVICES=0), and finally runs a specific Python performance benchmark script (`examples/performance/MLUPS3d.py`) with arguments 256 and 200. It assumes `pip`, `git`, and Python 3 are available in the container.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/investigating_a_regression.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n  pip install jmp pyvista numpy matplotlib Rtree trimesh jmp termcolor orbax\n  git clone https://github.com/Autodesk/XLB\n  cd XLB\n  export PYTHONPATH=.\n  export CUDA_VISIBLE_DEVICES=0 # only 1 GPU is needed\n\n  python3 examples/performance/MLUPS3d.py 256 200\n```\n\n----------------------------------------\n\nTITLE: Testing JAX with Default Python 3.13 Dependencies using Bazel\nDESCRIPTION: This command executes a JAX test target using Bazel with the default Python 3.13 interpreter and its associated default dependency set. It explicitly sets `HERMETIC_PYTHON_VERSION` to `3.13` via `--repo_env`, ensuring the standard `requirements_lock_3_13.txt` (as defined in the `WORKSPACE` `requirements` dictionary) is used.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\n# To build with default dependendencies:\nbazel test <target> --repo_env=HERMETIC_PYTHON_VERSION=3.13\n```\n\n----------------------------------------\n\nTITLE: Displaying Escaped Tracer in JAX\nDESCRIPTION: Shows the result of an escaped tracer when accessing global state after jit transformation with omnistaging enabled.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/4410-omnistaging.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(key) # Traced<ShapedArray(uint32[2])>with<DynamicJaxprTrace(level=0/1)>\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple JAX Model Function\nDESCRIPTION: Demonstrates a basic JAX model function that may cause issues when directly converted to TensorFlow due to parameter embedding.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model_jax(inputs):\n  return param0 + param1 * inputs\n```\n\n----------------------------------------\n\nTITLE: Setting the Current Module Context to jax.stages in Sphinx\nDESCRIPTION: This Sphinx directive sets the default module context for subsequent directives like `autoclass`. It ensures that class names are resolved relative to `jax.stages` without needing the full path.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.stages.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: jax.stages\n```\n\n----------------------------------------\n\nTITLE: Testing JAX with Custom Python and Scenario-Specific Dependencies using Bazel\nDESCRIPTION: This command runs a JAX test target using Bazel, combining a custom Python 3.13 interpreter (specified via local path URL and SHA256) with a scenario-specific dependency set ('scenario1'). It sets `HERMETIC_PYTHON_VERSION` to `3.13-scenario1` to select the lock file, and provides `HERMETIC_PYTHON_URL` and `HERMETIC_PYTHON_SHA256` to define the custom interpreter, all via `--repo_env` flags.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\n# To build with scenario1 dependendencies and custom Python 3.13 interpreter:\nbazel test <target> \\\n  --repo_env=HERMETIC_PYTHON_VERSION=3.13-scenario1 \\\n  --repo_env=HERMETIC_PYTHON_URL=\"file:///path/to/cpython.tar.gz\" \\\n  --repo_env=HERMETIC_PYTHON_SHA256=<sha256_sum>\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Precision Example\nDESCRIPTION: Recommended approach for matrix multiplication with specific precision control using lax.dot with preferred element type setting.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/details.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlax.dot(x, y, preferred_element_type=jnp.float32)\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Requirements using Bazel\nDESCRIPTION: Shell command showing how to update package requirements using Bazel for the JAX-ML project. This command is used to auto-generate dependency specifications with Python 3.10.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_10.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbazel run //build:requirements.update\n```\n\n----------------------------------------\n\nTITLE: JAX Array Scatter Update\nDESCRIPTION: Example showing the deprecated unsafe implicit dtype casting in scatter operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\njnp.zeros(4, dtype=int).at[0].set(1.5)\n```\n\n----------------------------------------\n\nTITLE: Setting LD_LIBRARY_PATH for CUDA in Shell\nDESCRIPTION: This command adds the path to libcupti.so to the LD_LIBRARY_PATH environment variable to resolve CUDA library loading issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.1/extras/CUPTI/lib64/:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Synchronizing with commit_smem before WGMMA in PLGPU (Python)\nDESCRIPTION: Identical in intent to the previous snippet, but showing explicit synchronization before a WGMMA call. Writes to SMEM, performs commit, and then issues the TensorCore operation. Proper commits avoid subtle data handling bugs. Requires 'plgpu' and correct operand references.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/gpu/reference.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsmem_ref[...] = value\\nplgpu.commit_smem()\\nplgpu.wgmma(smem_ref, ...)\n```\n\n----------------------------------------\n\nTITLE: Setting NCCL Environment Variables for GPU Communication Optimization (Python)\nDESCRIPTION: Illustrates how to set Nvidia NCCL environment variables within a Python script using `os.environ.update`. The flags `NCCL_LL128_BUFFSIZE`, `NCCL_LL_BUFFSIZE`, and `NCCL_PROTO` are configured to potentially enhance communication speed for single-host, multi-GPU setups on Nvidia hardware. Setting buffer sizes to -2 might indicate automatic sizing or disabling certain limits, while tuning the protocol preference aims for faster communication paths (LL/LL128 often refer to low-latency protocols). These settings are noted as potentially less useful for multi-host communication.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nos.environ.update({\n  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n  \"NCCL_LL_BUFFSIZE\": \"-2\",\n   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n })\n```\n\n----------------------------------------\n\nTITLE: Adding Upstream Remote for JAX Repository in Git\nDESCRIPTION: This command adds the main JAX repository as an upstream remote to sync changes during development.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/contributing.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add upstream https://www.github.com/jax-ml/jax\n```\n\n----------------------------------------\n\nTITLE: Configuring Concise Tracebacks in Jupyter\nDESCRIPTION: This IPython magic command sets the exception reporting mode to 'minimal'. This is often used in notebooks to make tracebacks less verbose, focusing on the essential error information.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/external-callbacks.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# This ensures that code cell tracebacks appearing below will be concise.\n%xmode minimal\n```\n\n----------------------------------------\n\nTITLE: JAX Reduce Window Function Signature\nDESCRIPTION: The signature of the JAX reduce window function used for window-based reduction operations. It has partial support when enable_xla=False, limited to specific computation types, data types, and configuration parameters.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/no_xla_limitations.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlax.reduce_window(operand, init_value, computation: Callable,\n                  window_dimensions: core.Shape, window_strides: Sequence[int],\n                  padding: Union[str, Sequence[Tuple[int, int]]],\n                  base_dilation: Optional[Sequence[int]] = None,\n                  window_dilation: Optional[Sequence[int]] = None)\n)\n```\n\n----------------------------------------\n\nTITLE: Handling 64-bit Precision Differences in JAX and TensorFlow\nDESCRIPTION: These code snippets illustrate the differences in handling 32-bit vs. 64-bit values between JAX and TensorFlow, both with JAX_ENABLE_X64 flag unset and set.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# with JAX_ENABLE_X64=0\njnp.sin(3.14)  # Has type float32\ntf.math.sin(3.14)  # Has type float32\n\njnp.sin(np.float64(3.14))  # Also has type float32\ntf.math.sin(np.float64(3.14))  # Has type float64\n\n# The jax2tf.convert function behaves like the JAX function.\njax2tf.convert(jnp.sin)(3.14)  # Has type float32\njax2tf.convert(jnp.sin)(np.float64(3.14))  # Has type float32\n\n# The following will still compute `sin` in float32 (with a tf.cast on the argument).\ntf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64))\n```\n\nLANGUAGE: python\nCODE:\n```\n# with JAX_ENABLE_X64=1\njnp.sin(3.14)  # Has type float64\ntf.math.sin(3.14)  # Has type float32\n\n# The jax2tf.convert function behaves like the JAX function.\njax2tf.convert(jnp.sin)(3.14)  # Has type float64\n\n# The following will compute `sin` in float64.\ntf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64))\n\n# The following will compute `sin` in float32.\ntf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14))\n```\n\n----------------------------------------\n\nTITLE: Generating NumPy Type Promotion Table with Pandas Display - Python\nDESCRIPTION: This code defines a function to compute and display the implicit type promotion table for NumPy dtypes by combining NumPy arrays or scalars of all relevant types and observing the result type. It uses NumPy for mathematical operations, Pandas DataFrame for structuring the result, and IPython's display module to render HTML tables. Dependencies are numpy, pandas, and IPython; key parameters include the mapping of dtypes, and the table output lists promotion results for all type combinations. Limitation: the table does not handle 'bfloat16' (marked invalid) and ignores value-dependent promotions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\\nimport pandas as pd\\nfrom IPython import display\\n\\nnp_dtypes = {\\n  'b': np.bool_,\\n  'u8': np.uint8, 'u16': np.uint16, 'u32': np.uint32, 'u64': np.uint64,\\n  'i8': np.int8, 'i16': np.int16, 'i32': np.int32, 'i64': np.int64,\\n  'bf16': 'invalid', 'f16': np.float16, 'f32': np.float32, 'f64': np.float64,\\n  'c64': np.complex64, 'c128': np.complex128,\\n  'i*': int, 'f*': float, 'c*': complex}\\n\\nnp_dtype_to_code = {val: key for key, val in np_dtypes.items()}\\n\\ndef make_np_zero(dtype):\\n  if dtype in {int, float, complex}:\\n    return dtype(0)\\n  else:\\n    return np.zeros(1, dtype=dtype)\\n\\ndef np_result_code(dtype1, dtype2):\\n  try:\\n    out = np.add(make_np_zero(dtype1), make_np_zero(dtype2))\\n  except TypeError:\\n    return '-'\\n  else:\\n    if type(out) in {int, float, complex}:\\n      return np_dtype_to_code[type(out)]\\n    else:\\n      return np_dtype_to_code[out.dtype.type]\\n\\ngrid = [[np_result_code(dtype1, dtype2)\\n         for dtype2 in np_dtypes.values()]\\n        for dtype1 in np_dtypes.values()]\\ntable = pd.DataFrame(grid, index=np_dtypes.keys(), columns=np_dtypes.keys())\\ndisplay.HTML(table.to_html())\n```\n\n----------------------------------------\n\nTITLE: Installing Older JAXLIB (GPU Wheel) with Pip - Bash\nDESCRIPTION: Installs a specific older version of jaxlib built for GPU/CUDA environments by specifying the '+cuda' suffix and fetching the wheel from the CUDA releases find-links URL. Prerequisite: Compatible NVIDIA GPU, correct CUDA and cuDNN versions, Python, and pip. Input is the provided bash command; output is an old CUDA-enabled jaxlib version. Only works for versions present on the archive URL.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npip install jaxlib==0.3.25+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n```\n\n----------------------------------------\n\nTITLE: Replacing Deprecated `non_negative_dim` with `max_dim` in JAX Core (Python)\nDESCRIPTION: Shows the replacement for the deprecated `core.non_negative_dim(d)` API, introduced in JAX 0.4.24 changes related to shape polymorphism. The equivalent functionality, ensuring a symbolic dimension `d` is non-negative, can be achieved using `core.max_dim(d, 0)`. This change is relevant when working with symbolic dimensions in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Old, deprecated usage:\ncore.non_negative_dim(d)\n\n# New, recommended usage:\ncore.max_dim(d, 0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Wu's Antialiased Line Algorithm in JAX\nDESCRIPTION: Defines a JIT-compiled function 'drawline' that implements Wu's antialiased line algorithm for better quality line plotting. This function is used in subsequent plotting utilities.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef drawline(im, x0, y0, x1, y1):\n  \"\"\"An implementation of Wu's antialiased line algorithm.\n  \n  This functional version was adapted from here:\n    https://en.wikipedia.org/wiki/Xiaolin_Wu's_line_algorithm\n  \"\"\"\n\n  ipart = lambda x: jnp.floor(x).astype('int32')\n  round_ = lambda x: ipart(x + 0.5).astype('int32')\n  fpart = lambda x: x - jnp.floor(x)\n  rfpart = lambda x: 1 - fpart(x)\n\n  def plot(im, x, y, c):\n    return ops.index_add(im, ops.index[x, y], c)\n\n  steep = jnp.abs(y1 - y0) > jnp.abs(x1 - x0)\n  cond_swap = lambda cond, x: lax.cond(cond, x, lambda x: (x[1], x[0]), x, lambda x: x)\n  \n  (x0, y0) = cond_swap(steep, (x0, y0))\n  (x1, y1) = cond_swap(steep, (x1, y1))\n  \n  (y0, y1) = cond_swap(x0 > x1, (y0, y1))\n  (x0, x1) = cond_swap(x0 > x1, (x0, x1))\n\n  dx = x1 - x0\n  dy = y1 - y0\n  gradient = jnp.where(dx == 0.0, 1.0, dy/dx)\n\n  # handle first endpoint\n  xend = round_(x0)\n  yend = y0 + gradient * (xend - x0)\n  xgap = rfpart(x0 + 0.5)\n  xpxl1 = xend # this will be used in main loop\n  ypxl1 = ipart(yend)\n\n  def true_fun(im):\n    im = plot(im, ypxl1, xpxl1, rfpart(yend) * xgap)\n    im = plot(im, ypxl1+1, xpxl1,  fpart(yend) * xgap)\n    return im\n  def false_fun(im):\n    im = plot(im, xpxl1, ypxl1  , rfpart(yend) * xgap)\n    im = plot(im, xpxl1, ypxl1+1,  fpart(yend) * xgap)\n    return im\n  im = lax.cond(steep, im, true_fun, im, false_fun)\n  \n  intery = yend + gradient\n\n  # handle second endpoint\n  xend = round_(x1)\n  yend = y1 + gradient * (xend - x1)\n  xgap = fpart(x1 + 0.5)\n  xpxl2 = xend  # this will be used in the main loop\n  ypxl2 = ipart(yend)\n  def true_fun(im):\n    im = plot(im, ypxl2  , xpxl2, rfpart(yend) * xgap)\n    im = plot(im, ypxl2+1, xpxl2,  fpart(yend) * xgap)\n    return im\n  def false_fun(im):\n    im = plot(im, xpxl2, ypxl2,  rfpart(yend) * xgap)\n    im = plot(im, xpxl2, ypxl2+1, fpart(yend) * xgap)\n    return im\n  im = lax.cond(steep, im, true_fun, im, false_fun)\n  \n  def true_fun(arg):\n    im, intery = arg\n    def body_fun(x, arg):\n      im, intery = arg\n      im = plot(im, ipart(intery), x, rfpart(intery))\n      im = plot(im, ipart(intery)+1, x, fpart(intery))\n      intery = intery + gradient\n      return (im, intery)\n    im, intery = lax.fori_loop(xpxl1+1, xpxl2, body_fun, (im, intery))\n    return (im, intery)\n  def false_fun(arg):\n    im, intery = arg\n    def body_fun(x, arg):\n      im, intery = arg\n      im = plot(im, x, ipart(intery), rfpart(intery))\n      im = plot(im, x, ipart(intery)+1, fpart(intery))\n      intery = intery + gradient\n      return (im, intery)\n    im, intery = lax.fori_loop(xpxl1+1, xpxl2, body_fun, (im, intery))\n    return (im, intery)\n  im, intery = lax.cond(steep, (im, intery), true_fun, (im, intery), false_fun)\n  \n  return im\n```\n\n----------------------------------------\n\nTITLE: Using jax.numpy.ufunc in Python\nDESCRIPTION: Example of using the new jax.numpy.ufunc class and jax.numpy.frompyfunc function to convert a scalar-valued function into a numpy.ufunc-like object with methods like outer, reduce, accumulate, at, and reduceat.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nimport jax.numpy as jnp\n\n# Convert a scalar function to a ufunc-like object\nmy_func = jnp.frompyfunc(lambda x: x * 2, 1, 1)\n\n# Use ufunc methods\nresult = my_func.outer([1, 2, 3], [4, 5, 6])\n```\n\n----------------------------------------\n\nTITLE: Attempting Unsupported Operations on New-Style PRNG Keys (Python)\nDESCRIPTION: Shows that arithmetic operations (e.g., addition) are intentionally disallowed on typed PRNG keys in JAX. Attempting such operations raises a TypeError, enforcing safety by design. Input: typed key and an integer. Output: explicit error instead of unwanted behavior. This prevents accidental misuse of keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> key = key + 1  # doctest: +SKIP\nTraceback (most recent call last):\nTypeError: add does not accept dtypes key<fry>, int32.\n\n```\n\n----------------------------------------\n\nTITLE: Defining Exception Formatter for Sharding Errors in JAX\nDESCRIPTION: Creates a utility function to format and display exceptions with colored output, used to demonstrate error cases when arrays with incompatible shardings are combined in operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport textwrap\nfrom termcolor import colored\n\ndef print_exception(e):\n  name = colored(f'{type(e).__name__}', 'red', force_color=True)\n  print(textwrap.fill(f'{name}: {str(e)}'))\n```\n\n----------------------------------------\n\nTITLE: Generating JAX Primitives Limitations Documentation\nDESCRIPTION: This command runs a test to generate the documentation for JAX primitives with known numerical discrepancies when converted to TensorFlow. It enables 64-bit computations and outputs the limitations documentation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/primitives_with_limited_support.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nJAX_ENABLE_X64=1 JAX_OUTPUT_LIMITATIONS_DOC=1 python jax/experimental/jax2tf/tests/primitives_test.py JaxPrimitiveTest.test_generate_limitations_doc\n```\n\n----------------------------------------\n\nTITLE: shard_map API Specification\nDESCRIPTION: Formal API specification for the shard_map function, including type hints and parameter descriptions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/14273-shard-map.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.sharding import Mesh\nSpecs = PyTree[PartitionSpec]\n\ndef shard_map(f: Callable, mesh: Mesh, in_specs: Specs, out_specs: Specs\n          ) -> Callable:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Shape Polymorphism with call_tf\nDESCRIPTION: Example of using call_tf with shape polymorphism by specifying output shape and dtype information.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef fun_jax(x):\n  y_shape = (x.shape[0] * 2, y.shape[1:])\n  y = jax2tf.call_tf(\n      lambda x: tf.concat([x, x], axis=0),\n      output_shape_dype=jax.ShapeDtypeStruct(y_shape, x.dtype))(x)\n  # JAX will know the y.shape\n  return jnp.ones(y.shape, dtype=y.dtype) + y\n\njax2tf.convert(fun_jax, polymorphic_shapes=[\"b, ...\"])(x)\n```\n\n----------------------------------------\n\nTITLE: Auto-documenting the jax.stages.Wrapped Class using Sphinx\nDESCRIPTION: This Sphinx directive generates documentation for the `Wrapped` class within the `jax.stages` module. It specifically includes documentation for the `trace`, `lower` methods and the special `__call__` method.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.stages.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: Wrapped\n   :members: trace, lower\n   :special-members: __call__\n```\n\n----------------------------------------\n\nTITLE: Running JAX CPU and Backend-Independent Tests with Bazel\nDESCRIPTION: This command executes the JAX CPU-specific tests (`//tests:cpu_tests`) and backend-independent tests (`//tests:backend_independent_tests`) using Bazel. By default, this command will build `jaxlib` from source as part of the test execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_33\n\nLANGUAGE: shell\nCODE:\n```\nbazel test //tests:cpu_tests //tests:backend_independent_tests\n```\n\n----------------------------------------\n\nTITLE: PyTorch Type Promotion Implementation\nDESCRIPTION: Implements type promotion testing for PyTorch datatypes. Notable for lacking unsigned integer types above uint8. Creates a promotion table using pandas DataFrame.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport pandas as pd\nfrom IPython import display\n\ntorch_dtypes = {\n  'b': torch.bool,\n  'u8': torch.uint8, 'u16': 'invalid', 'u32': 'invalid', 'u64': 'invalid',\n  'i8': torch.int8, 'i16': torch.int16, 'i32': torch.int32, 'i64': torch.int64,\n  'bf16': torch.bfloat16, 'f16': torch.float16, 'f32': torch.float32, 'f64': torch.float64,\n  'c64': torch.complex64, 'c128': torch.complex128,\n  'i*': int, 'f*': float, 'c*': complex}\n\ntorch_dtype_to_code = {val: key for key, val in torch_dtypes.items()}\n\ndef make_torch_zero(dtype):\n  if dtype in {int, float, complex}:\n    return dtype(0)\n  else:\n    return torch.zeros(1, dtype=dtype)\n\ndef torch_result_code(dtype1, dtype2):\n  try:\n    out = torch.add(make_torch_zero(dtype1), make_torch_zero(dtype2))\n  except TypeError:\n    return '-'\n  else:\n    if type(out) in {int, float, complex}:\n      return torch_dtype_to_code[type(out)]\n    else:\n      return torch_dtype_to_code[out.dtype]\n\ngrid = [[torch_result_code(dtype1, dtype2)\n         for dtype2 in torch_dtypes.values()]\n        for dtype1 in torch_dtypes.values()]\ntable = pd.DataFrame(grid, index=torch_dtypes.keys(), columns=torch_dtypes.keys())\ndisplay.HTML(table.to_html())\n```\n\n----------------------------------------\n\nTITLE: Checking Extended Dtype Properties in JAX\nDESCRIPTION: Shows how to check if a dtype is a PRNG extended dtype using JAX's dtype checking functions. This helps identify typed PRNG keys.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> jax.dtypes.issubdtype(jax.dtypes.prng_key, jax.dtypes.extended)\nTrue\n```\n\n----------------------------------------\n\nTITLE: JavaScript for Controlling Sidebar Visibility\nDESCRIPTION: A script that hides the menu sidebar on the main page when the window width is at least 960 pixels by checking the primary checkbox.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/* Along with some CSS settings in style.css (look for `body:has(.hero)`)\n   this will ensure that the menu sidebar is hidden on the main page. */\nif (window.innerWidth >= 960) {\n   document.getElementById(\"__primary\").checked = true;\n}\n```\n\n----------------------------------------\n\nTITLE: JAX Convolution Function Signature (Conv_general_dilated)\nDESCRIPTION: The signature of the JAX convolution function that uses XlaConv under the hood. This function supports various convolution types with different parameter configurations, but has partial support when enable_xla=False.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/no_xla_limitations.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlax.conv_general_dilated(\n    lhs, rhs, window_strides, padding, lhs_dilation,\n    rhs_dilation, dimension_numbers, feature_group_count,\n    batch_group_count, precision, preferred_element_type\n)\n```\n\n----------------------------------------\n\nTITLE: Invoking Pallas Kernel with pallas_call in JAX\nDESCRIPTION: Demonstrates how to use pallas_call to integrate a Pallas kernel into a JAX computation, specifying output shape and jitting the function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef add_vectors(x: jax.Array, y: jax.Array) -> jax.Array:\n  return pl.pallas_call(\n      add_vectors_kernel,\n      out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)\n  )(x, y)\nadd_vectors(jnp.arange(8), jnp.arange(8))\n```\n\n----------------------------------------\n\nTITLE: Permissive JAX Sum Function\nDESCRIPTION: This snippet defines a permissive version of jnp.sum that allows list inputs by converting them to arrays.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef permissive_sum(x):\n  return jnp.sum(jnp.array(x))\n\nx = list(range(10))\npermissive_sum(x)\n```\n\n----------------------------------------\n\nTITLE: Creating a Development Branch in Git\nDESCRIPTION: This command creates a new branch for implementing changes in the JAX repository.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/contributing.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b name-of-change\n```\n\n----------------------------------------\n\nTITLE: Viewing Memory Profile with pprof\nDESCRIPTION: Shell command to visualize the saved memory profile using pprof's web interface.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/device_memory_profiling.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npprof --web memory.prof\n```\n\n----------------------------------------\n\nTITLE: Specifying JAX Python Dependencies with Version Constraints\nDESCRIPTION: This snippet defines Python package dependencies for the JAX project. It specifies different numpy version requirements based on Python version and lists additional dependencies with notes about compatibility issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/nonfreethreading-requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy~=2.0.0; python_version<=\"3.12\"\nnumpy~=2.1.0; python_version>=\"3.13\"\n\n# These packages have not released free-threaded wheels.\nzstandard\ntensorstore\n\n# portpicker is architecture independent, but it depends on psutil which has not\n# released a 3.13-ft wheel.\nportpicker\n```\n\n----------------------------------------\n\nTITLE: Defining NVIDIA CUDA Dependencies for JAX on Linux\nDESCRIPTION: This requirements file specifies the NVIDIA CUDA 12 dependencies needed for JAX on Linux platforms. The dependencies include core CUDA libraries, deep learning libraries (cuDNN), and various specialized CUDA components (cuBLAS, cuFFT, etc.). The comment indicates that these wheels are downloaded only when the targets in bazel command contain dependencies on them.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/gpu-test-requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# NVIDIA CUDA dependencies\n# Note that the wheels are downloaded only when the targets in bazel command\n# contain dependencies on these wheels.\nnvidia-cublas-cu12>=12.1.3.1 ; sys_platform == \"linux\"\nnvidia-cuda-cupti-cu12>=12.1.105 ; sys_platform == \"linux\"\nnvidia-cuda-nvcc-cu12>=12.6.85 ; sys_platform == \"linux\"\nnvidia-cuda-runtime-cu12>=12.1.105 ; sys_platform == \"linux\"\nnvidia-cudnn-cu12>=9.8,<10.0 ; sys_platform == \"linux\"\nnvidia-cufft-cu12>=11.0.2.54 ; sys_platform == \"linux\"\nnvidia-cusolver-cu12>=11.4.5.107 ; sys_platform == \"linux\"\nnvidia-cusparse-cu12>=12.1.0.106 ; sys_platform == \"linux\"\nnvidia-nccl-cu12>=2.18.1 ; sys_platform == \"linux\"\nnvidia-nvjitlink-cu12>=12.1.105 ; sys_platform == \"linux\"\n```\n\n----------------------------------------\n\nTITLE: Wrapping Custom Pytree Functions for SavedModel Compatibility\nDESCRIPTION: This snippet shows how to create a wrapper function for custom pytree containers to enable SavedModel compatibility and gradient computation in TensorFlow.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# wrapped TF function to use only standard containers\ndef f_tf_wrapped(a, b):\n  return f_tf(CustomPair(a, b))\n\n# Try to put into SavedModel\nmy_model = tf.Module()\n# Save a function that can take scalar inputs.\nmy_model.f = tf.function(f_tf_wrapped, autograph=False,\n                         input_signature=[tf.TensorSpec([], tf.float32),\n                                          tf.TensorSpec([], tf.float32)])\nmodel_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\ntf.saved_model.save(my_model, model_dir,\n                    options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n```\n\n----------------------------------------\n\nTITLE: Validating JIT-Compiled Gradient with JAX\nDESCRIPTION: Example of using JAX's JIT compilation with the grad function on a custom primitive, demonstrating that JIT works with custom primitive reverse differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nassert api.jit(api.grad(square_add_prim))(2., 10.) == 4.\n```\n\n----------------------------------------\n\nTITLE: Correctly Reusing Symbolic Scopes\nDESCRIPTION: Demonstrates how to correctly reuse a symbolic scope to ensure symbolic dimensions can be combined in operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\na, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\nb, = export.symbolic_shape(\"b,\", scope=a1.scope)\n\na + b  # Allowed\n```\n\n----------------------------------------\n\nTITLE: Example Output of Nightly JAX Performance Test Results\nDESCRIPTION: This text snippet shows example output lines obtained by running `grep MLUPS OUT*` after executing the nightly tests via `test_runner.sh`. Each line displays the MLUPS (Mega Lattice Updates Per Second) performance metric extracted from a specific dated output file (`OUT-mm-dd`). This output is used to identify the date range(s) where significant performance changes (regressions or improvements) occurred.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/investigating_a_regression.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nOUT-07-06:MLUPS: 587.9240990200157\nOUT-07-07:MLUPS: 587.8907972116419\nOUT-07-08:MLUPS: 587.3186499464459\nOUT-07-09:MLUPS: 587.3130127722537\nOUT-07-10:MLUPS: 587.8526619429658\nOUT-07-17:MLUPS: 570.1631097290182\nOUT-07-18:MLUPS: 570.2819775617064\nOUT-07-19:MLUPS: 570.1672213357352\nOUT-07-20:MLUPS: 587.437153685251\nOUT-07-21:MLUPS: 587.6702557143142\nOUT-07-25:MLUPS: 577.3063618431178\nOUT-07-26:MLUPS: 577.2362978080912\nOUT-07-27:MLUPS: 577.2101850145785\nOUT-07-28:MLUPS: 577.0716349809895\nOUT-07-29:MLUPS: 577.4223280707176\nOUT-07-30:MLUPS: 577.2255967221336\nOUT-08-01:MLUPS: 577.277685388252\nOUT-08-02:MLUPS: 577.0137874289354\nOUT-08-03:MLUPS: 577.1333281553946\nOUT-08-04:MLUPS: 577.305012020407\nOUT-08-05:MLUPS: 577.2143988866626\nOUT-08-06:MLUPS: 577.2409145495443\nOUT-08-07:MLUPS: 577.2602819927345\nOUT-08-08:MLUPS: 577.2823738293221\nOUT-08-09:MLUPS: 577.3453199728248\nOUT-08-11:MLUPS: 577.3161423260563\nOUT-08-12:MLUPS: 577.1697775786824\nOUT-08-13:MLUPS: 577.3049883393633\nOUT-08-14:MLUPS: 576.9051978525331\nOUT-08-15:MLUPS: 577.5331743016213\nOUT-08-16:MLUPS: 577.5117505070573\nOUT-08-18:MLUPS: 577.5930698237612\nOUT-08-19:MLUPS: 577.3539885757353\nOUT-08-20:MLUPS: 577.4190113959127\nOUT-08-21:MLUPS: 577.300394253605\nOUT-08-22:MLUPS: 577.4263792037783\nOUT-08-23:MLUPS: 577.4087536357031\nOUT-08-24:MLUPS: 577.1094728438082\nOUT-08-25:  File \"/XLB/examples/performance/MLUPS3d.py\", line 5, in <module>\nOUT-08-26:MLUPS: 537.0164618489928\nOUT-08-27:MLUPS: 536.9545448661609\nOUT-08-28:MLUPS: 536.2887650464874\nOUT-08-29:MLUPS: 536.7178471720636\nOUT-08-30:MLUPS: 536.6978912984252\nOUT-09-01:MLUPS: 536.7030899164106\nOUT-09-04:MLUPS: 536.5339818238837\nOUT-09-05:MLUPS: 536.6507808565617\nOUT-09-06:MLUPS: 536.7144494518315\nOUT-09-08:MLUPS: 536.7376612408998\nOUT-09-09:MLUPS: 536.7798324141778\nOUT-09-10:MLUPS: 536.726157440174\nOUT-09-11:MLUPS: 536.7446210750584\nOUT-09-12:MLUPS: 536.6707332269023\nOUT-09-13:MLUPS: 536.6777936517823\nOUT-09-14:MLUPS: 536.7581523280307\nOUT-09-15:MLUPS: 536.6156273667873\nOUT-09-16:MLUPS: 536.7320935035265\nOUT-09-17:MLUPS: 536.7104991444398\nOUT-09-18:MLUPS: 536.7492269469092\nOUT-09-19:MLUPS: 536.6760131792959\nOUT-09-20:MLUPS: 536.7361260076634\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables for JAX2TF model serving\nDESCRIPTION: Defining variables for model path, model type, batch sizes for saving and serving, and model version tracking to manage the SavedModel workflow.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Shortcuts\n# Where to save SavedModels\nMODEL_PATH=/tmp/jax2tf/saved_models\n# The example model. The options are \"mnist_flax\" and \"mnist_pure_jax\"\nMODEL=mnist_flax\n# The batch size for the SavedModel. Use -1 for batch-polymorphism,\n# or a strictly positive value for a fixed batch size.\nSERVING_BATCH_SIZE_SAVE=-1\n# The batch size to send to the model. Must be equal to SERVING_BATCH_SIZE_SAVE\n# if not -1.\nSERVING_BATCH_SIZE=16\n# Increment this when you make changes to the model parameters after the\n# initial model generation (Step 1 below).\nMODEL_VERSION=$(( 1 + ${MODEL_VERSION:-0} ))\n```\n\n----------------------------------------\n\nTITLE: Using Updated PRNGKey in JAX Random Module\nDESCRIPTION: Shows the new way of creating PRNG keys using the 'impl' argument instead of the deprecated named key constructors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport jax.random as random\n\n# New way to create keys\nkey1 = random.PRNGKey(seed, impl='threefry2x32')\nkey2 = random.PRNGKey(seed, impl='rbg')\nkey3 = random.PRNGKey(seed, impl='unsafe_rbg')\n```\n\n----------------------------------------\n\nTITLE: Installing Legacy Monolithic CUDA JAXLIB Nightly with Pip - Bash\nDESCRIPTION: Installs a legacy nightly build of monolithic CUDA-enabled jaxlib via pip, for historical use cases. The specified find-links URL points to the special legacy channel for CUDA 12. Prerequisite: Older NVIDIA GPU environments and proper CUDA setup; generally not recommended for new deployments as support will end after Sep 2024. Input is the bash pip command; output is an older style CUDA jaxlib installation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npip install -U --pre jaxlib -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_cuda12_releases.html\n\n```\n\n----------------------------------------\n\nTITLE: Tensor Dimension Mismatch ValueError (Different Axis) from TFLite/Flex Converter (Text)\nDESCRIPTION: This error showcases a case where TFLite (or Flex variant) conversion fails due to dimension 1 of the input tensor receiving 8 values instead of the expected 1 value. It demonstrates that specific axes may be checked for size compliance. The message is for documentation; its key parameter is dimension 1's size; it expects a 1-sized shape, but an 8 is given. This output helps clarify axis-specific shape expectations in TFLite conversions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nValueError('Cannot set tensor: Dimension mismatch. Got 8 but expected 1 for dimension 1 of input 0.')\n```\n\n----------------------------------------\n\nTITLE: Tensor Dimension Mismatch ValueError from TFLite/Flex Converter (Text)\nDESCRIPTION: This error output provides the detailed ValueError produced when attempting to set a tensor with an incorrect dimension, reflecting a common shape checking failure in TFLite (and TFLite+Flex) conversion. Dependencies include TFLite's shape constraints. The relevant parameter is the dimension value at a given axis; inputs are tensor values with mismatched dimensions, and outputs are explicit error messages. This serves as reference for debugging input tensor shape mismatches.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_15\n\nLANGUAGE: text\nCODE:\n```\nValueError('Cannot set tensor: Dimension mismatch. Got 2 but expected 1 for dimension 0 of input 0.')\n```\n\n----------------------------------------\n\nTITLE: Basic JAX Import and Setup\nDESCRIPTION: Imports JAX's NumPy interface for numerical operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Checking Out, Building, and Testing JAX/XLA Commits Hourly (Bash)\nDESCRIPTION: This Bash script, designed to be called by `test_runner2.sh`, performs the core hourly testing logic. It accepts a timestamp string (e.g., 'Aug 24 2023 10:00:00') as command-line arguments. It uses `git rev-list` to find and check out the latest commits in both the XLA and JAX source repositories that occurred *before* the given timestamp. It then removes any previous JAX wheel, rebuilds JAX using the `build-jax.sh` script (expected to be present in the container), sets environment variables, and runs the `MLUPS3d.py` performance benchmark. Dependencies include `git`, `build-jax.sh`, and the benchmark script/repository.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/investigating_a_regression.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n  echo \"param: $@\"\n  cd /opt/xla-source\n  git checkout `git rev-list -1 --before=\"$*\" origin/main`\n  git show -q\n  cd /opt/jax-source\n  git checkout `git rev-list -1 --before=\"$*\" origin/main`\n  git show -q\n\n  rm /opt/jax-source/dist/jax*.whl\n  build-jax.sh # The script is in the nightly container\n\n  export PYTHONPATH=.\n  export CUDA_VISIBLE_DEVICES=0 # only 1 GPU is needed\n\n  python3 examples/performance/MLUPS3d.py 256 200\n```\n\n----------------------------------------\n\nTITLE: Sphinx Directive for Automatic Module Documentation\nDESCRIPTION: This Sphinx directive `automodule` instructs the documentation generator to automatically pull documentation from the specified Python module, `jax.experimental.mesh_utils`, including its docstrings.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.mesh_utils.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: jax.experimental.mesh_utils\n```\n\n----------------------------------------\n\nTITLE: Cloning the JAX Repository\nDESCRIPTION: Commands to clone the JAX source code repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/jax-ml/jax\ncd jax\n```\n\n----------------------------------------\n\nTITLE: SSH Port Forwarding for Remote TensorBoard Access\nDESCRIPTION: This SSH command sets up local port forwarding to access a remote TensorBoard server on the default port 6006.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nssh -L 6006:localhost:6006 <remote server address>\n```\n\n----------------------------------------\n\nTITLE: Calculating Neural Network Loss in JAX\nDESCRIPTION: Computes the initial loss value for the neural network model using the parameters and batch data.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(loss(params, batch))\n```\n\n----------------------------------------\n\nTITLE: Warning for Shape/Type Mismatch in Buffer Donation (Python)\nDESCRIPTION: Shows a `UserWarning` that occurs when a donated buffer's shape or dtype does not match any of the output buffers. Here, `y` has shape `(1, 3)`, while the output of `add(x, y)` (where `x` is `(2, 3)`) will have shape `(2, 3)`. Because the donated buffer from `y` cannot be used for the output due to the shape mismatch, JAX issues a warning. Requires `jax`, `numpy`, and the `add`, `x` variables from previous examples.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/faq.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Assume add, x are defined as before\n# x = jax.device_put(np.ones((2, 3)))\nimport jax\nimport numpy as np\n\ny = jax.device_put(np.ones((1, 3)))  # `y` has different shape than the output\n# Execute `add` with donation of the buffer for `y`.\nz = jax.jit(add, donate_argnums=(1,))(x, y)\n# >> UserWarning: Some donated buffers were not usable: f32[1,3]{1,0}\n```\n\n----------------------------------------\n\nTITLE: Installing jaxlib Using pip\nDESCRIPTION: Command to install the prebuilt jaxlib wheel using pip, recommended when only modifying Python portions of JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install jaxlib\n```\n\n----------------------------------------\n\nTITLE: Demonstrating FLOPs Intensity Calculation for Matrix Multiplication - JAX - Python\nDESCRIPTION: Prints the calculated arithmetic intensity (FLOPs/byte) for a specific matrix multiplication size and type, using the previously defined matmul_flops_intensity function. Dependencies: jax (as jnp). Parameters: matrix sizes, dtype. Shows numeric result for a 1024x1024x1024 float32 matmul. Useful for comparing model sizes to hardware capabilities; output is textual, not programmatic.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"{matmul_flops_intensity(1024, 1024, 1024, jnp.float32)} flops/byte\")\n```\n\n----------------------------------------\n\nTITLE: Exporting JAX function with IR dumping\nDESCRIPTION: This snippet demonstrates how to export a JAX function while dumping the IR modules to a specified directory using an environment variable.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nJAX_DUMP_IR_TO=/tmp/export.dumps pytest tests/export_test.py -k test_basic --log-level=3 --log-file=/tmp/mylog.txt\n```\n\n----------------------------------------\n\nTITLE: Generating Module Documentation with Sphinx Automodule (rst)\nDESCRIPTION: This reStructuredText directive instructs the Sphinx documentation generator to automatically create documentation for the Python module `jax.example_libraries.optimizers`. The `:members:` option includes documentation for all public members, `:undoc-members:` includes members without docstrings, and `:show-inheritance:` displays the class inheritance hierarchy. Requires Sphinx and the target Python module (`jax.example_libraries.optimizers`) to be correctly configured and accessible.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.example_libraries.optimizers.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: jax.example_libraries.optimizers\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Referencing JAX Distributed Module in reStructuredText\nDESCRIPTION: This RST code sets up the documentation structure for the jax.distributed module, including the current module reference, module automodule directive, and an autosummary that lists the initialize and shutdown functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.distributed.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``jax.distributed`` module\n==========================\n\n.. currentmodule:: jax.distributed\n\n.. automodule:: jax.distributed\n\n.. autosummary::\n    :toctree: _autosummary\n\n    initialize\n    shutdown\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Zero JVP\nDESCRIPTION: Shows the effect of using None to indicate zero JVP in defjvps. The example demonstrates that the gradient with respect to the corresponding argument is zero.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(2., 3.))\nprint(grad(f, 0)(2., 3.))  # same as above\nprint(grad(f, 1)(2., 3.))\n```\n\n----------------------------------------\n\nTITLE: Testing JAX with Scenario-Specific Dependencies using Bazel (Scenario 1)\nDESCRIPTION: This command executes a JAX test target using Bazel, specifically selecting the dependency set defined for 'scenario1' associated with Python 3.13. It achieves this by setting the `HERMETIC_PYTHON_VERSION` environment variable to `3.13-scenario1` via the `--repo_env` flag. This assumes a corresponding entry exists in the `requirements` dictionary in the `WORKSPACE` file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\n# To build with scenario1 dependendencies:\nbazel test <target> --repo_env=HERMETIC_PYTHON_VERSION=3.13-scenario1\n```\n\n----------------------------------------\n\nTITLE: Demonstration: Tracing and Typechecking a Jaxpr with make_jaxpr_v1 (Python)\nDESCRIPTION: Shows usage of make_jaxpr_v1 to trace a simple function (lambda x: 2. * x) into its Jaxpr representation and then prints both the Jaxpr and results of typechecking. Demonstrates parameter preparation and retrieval of output structure. Requires implementation of supporting infrastructure (multiplication primitive, get_aval, raise_to_shaped).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\njaxpr, consts, _ = make_jaxpr_v1(lambda x: 2. * x, raise_to_shaped(get_aval(3.)))\nprint(jaxpr)\nprint(typecheck_jaxpr(jaxpr))\n```\n\n----------------------------------------\n\nTITLE: Using Saved Profile for PGLE-optimized Compilation\nDESCRIPTION: Command to run the workload again with the profile file provided to the compilation process using XLA flags.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/gpu_performance_tips.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport XLA_FLAGS=\"--xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_pgle_profile_file_or_directory_path=/path/to/profile/profile.pb\"\n```\n\n----------------------------------------\n\nTITLE: Reporting Inconclusive Dimension Operations in JAX2TF (Text)\nDESCRIPTION: This snippet provides the error text returned when a shape-based comparison such as 'b == 2' cannot be conclusively resolved during model conversion using jax2tf or similar tools. It does not require any code dependencies but refers users to written documentation. The key parameters are shape dimension variables; the expected input is a dynamic shape, and the output is an error explanation. This message is informational and intended to guide users toward resolution when using model converters.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nInconclusiveDimensionOperation(\"Dimension polynomial comparison 'b' == '2' is inconclusive\\n\\nThis error arises for arithmetic or comparison operations with shapes that\\nare non-constant, and the result of the operation cannot be represented as\\na polynomial of dimension variables, or a boolean constant (for comparisons).\\n\\nPlease see https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#computing-with-dimension-variables\\nfor more details.\\n\")\n```\n\n----------------------------------------\n\nTITLE: Using DTypeLike Type Annotation in JAX\nDESCRIPTION: Demonstrates the usage of the new jax.typing.DTypeLike type annotation for objects convertible to JAX dtypes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.typing import DTypeLike\n\ndef function_with_dtype(dtype: DTypeLike):\n    # Function implementation\n```\n\n----------------------------------------\n\nTITLE: Installing Older JAXLIB (CPU Wheel) with Pip - Bash\nDESCRIPTION: Demonstrates how to install a specific older version of JAX with CPU support and its dependencies using pip. Both the full package and jaxlib are installed from an archived release index, which may not be available on PyPI. Prerequisite: Python and pip. Input is the bash command; output is the requested archive version of JAX or jaxlib installed. Limited by the availability of the wheel on the specified URL.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/installation.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# Install jaxlib on CPU via the wheel archive\npip install \"jax[cpu]==0.3.25\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n# Install the jaxlib 0.3.25 CPU wheel directly\npip install jaxlib==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Dependency Lock File using Bazel (Shell)\nDESCRIPTION: This command directly invokes the Bazel target (`//build:requirements.update`) responsible for updating the Python dependency lock file. It achieves the same result as the `build.py` script method. The target Python version is specified using the `HERMETIC_PYTHON_VERSION` environment variable passed via `--repo_env`.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nbazel run //build:requirements.update --repo_env=HERMETIC_PYTHON_VERSION=3.12\n```\n\n----------------------------------------\n\nTITLE: Highlighting XLA's Potential Buffer Free Point\nDESCRIPTION: This snippet shows the same function `f(x)` as before, but includes a comment explicitly marking the point after `z = x + 1` where XLA's analysis might incorrectly decide it's safe to free the buffer `x`, potentially causing errors because the asynchronous `ppermute` might still be using it.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  fut = ppermute_start(x)\n  z = x + 1\n  # XLA can free x here!\n  y = ppermute_done(fut)\n  return y, z\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing Neural Network Dependencies in Python\nDESCRIPTION: Imports required JAX libraries for neural network implementation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Building JAX from Source\nDESCRIPTION: Commands to clone and build JAX with ROCm support from source code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/rocm/README.md#2025-04-22_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/ROCm/jax -b <branch_name>\ncd jax\n\npython3 ./build/build.py build --wheels=jaxlib,jax-rocm-plugin,jax-rocm-pjrt \\\n    --rocm_version=60 --rocm_path=/opt/rocm-[version]\n\npython3 setup.py develop --user && pip3 -m pip install dist/*.whl\n```\n\n----------------------------------------\n\nTITLE: Specifying NVIDIA CUDA Library Dependencies\nDESCRIPTION: This snippet demonstrates how to specify NVIDIA CUDA library dependencies with version constraints and platform-specific conditions. It includes examples for cublas, cudnn, and cuda-nvcc.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_10.txt#2025-04-22_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nnvidia-cublas-cu12==12.8.3.14 ; sys_platform == \"linux\" \\\n    --hash=sha256:3f0e05e7293598cf61933258b73e66a160c27d59c4422670bf0b79348c04be44 \\\n    --hash=sha256:93a4e0e386cc7f6e56c822531396de8170ed17068a1e18f987574895044cd8c3 \\\n    --hash=sha256:9ae5eae500aead01fc4bdfc458209df638b1a3551557ce11a78eea9ece602ae9\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Dependency Lock File using Python Script (Shell)\nDESCRIPTION: This command executes a Python script (`build/build.py`) to update the pinned Python dependencies for a specific Python version (e.g., 3.12). It reads direct dependencies from `build/requirements.in`, resolves the full transitive closure using `pip-compile`, and writes the pinned versions and hashes to the corresponding `build/requirements_lock_<python version>.txt` file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython build/build.py requirements_update --python_version=3.12\n```\n\n----------------------------------------\n\nTITLE: Implementing partial evaluation for xla_call primitive in JAX\nDESCRIPTION: Defines the partial evaluation rule for the xla_call primitive in JAX. It handles the 'unzipping' of jaxprs into two separate jaxprs for known and unknown parts.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_66\n\nLANGUAGE: Python\nCODE:\n```\ndef xla_call_partial_eval(trace, tracers, *, jaxpr, num_consts):\n  del num_consts  # Unused\n  in_unknowns = [not t.pval.is_known for t in tracers]\n  jaxpr1, jaxpr2, out_unknowns, num_res = partial_eval_jaxpr(jaxpr, in_unknowns)\n  known_tracers, unknown_tracers = partition_list(in_unknowns, tracers)\n  known_vals = [t.pval.const for t in known_tracers]\n  outs1_res = bind(xla_call_p, *known_vals, jaxpr=jaxpr1, num_consts=0)\n  outs1, res = split_list(outs1_res, len(jaxpr1.outs) - num_res)\n  res_tracers = [trace.instantiate_const(full_raise(trace, x)) for x in res]\n  outs2 = [PartialEvalTracer(trace, PartialVal.unknown(v.aval), None)\n           for v in jaxpr2.outs]\n  eqn = JaxprEqnRecipe(xla_call_p, res_tracers + unknown_tracers,\n                       dict(jaxpr=jaxpr2, num_consts=0),\n                       [v.aval for v in jaxpr2.outs], map(ref, outs2))\n  for t in outs2: t.recipe = eqn\n  return merge_lists(out_unknowns, outs1, outs2)\npartial_eval_rules[xla_call_p] = xla_call_partial_eval\n\ndef partial_eval_jaxpr(jaxpr: Jaxpr, in_unknowns: list[bool],\n                       instantiate: list[bool] | None = None,\n                       ) -> tuple[Jaxpr, Jaxpr, list[bool], int]:\n  env: dict[Var, bool] = {}\n  residuals: set[Var] = set()\n\n  def read(x: Atom) -> bool:\n    return type(x) is Var and env[x]\n\n  def write(unk: bool, v: Var) -> None:\n    env[v] = unk\n\n  def new_res(x: Atom) -> Atom:\n    if type(x) is Var: residuals.add(x)\n    return x\n\n  eqns1, eqns2 = [], []\n  map(write, in_unknowns, jaxpr.in_binders)\n  for eqn in jaxpr.eqns:\n    unks_in = map(read, eqn.inputs)\n    rule = partial_eval_jaxpr_rules.get(eqn.primitive)\n    if rule:\n      eqn1, eqn2, unks_out, res = rule(unks_in, eqn)\n      eqns1.append(eqn1); eqns2.append(eqn2); residuals.update(res)\n      map(write, unks_out, eqn.out_binders)\n    elif any(unks_in):\n      inputs = [v if unk else new_res(v) for unk, v in zip(unks_in, eqn.inputs)]\n      eqns2.append(JaxprEqn(eqn.primitive, inputs, eqn.params, eqn.out_binders))\n      map(partial(write, True), eqn.out_binders)\n    else:\n      eqns1.append(eqn)\n      map(partial(write, False), eqn.out_binders)\n  out_unknowns = map(read, jaxpr.outs)\n  if instantiate is not None:\n    for v, uk, inst in zip(jaxpr.outs, out_unknowns, instantiate):\n      if inst and not uk: new_res(v)\n    out_unknowns = map(op.or_, out_unknowns, instantiate)\n\n  residuals, num_res = list(residuals), len(residuals)\n  assert all(type(v) is Var for v in residuals), residuals\n\n  ins1, ins2 = partition_list(in_unknowns, jaxpr.in_binders)\n  outs1, outs2 = partition_list(out_unknowns, jaxpr.outs)\n\n  jaxpr1 = Jaxpr(ins1, eqns1, outs1 + residuals)\n  jaxpr2 = Jaxpr(residuals + ins2, eqns2, outs2)\n  typecheck_partial_eval_jaxpr(jaxpr, in_unknowns, out_unknowns, jaxpr1, jaxpr2)\n\n  return jaxpr1, jaxpr2, out_unknowns, num_res\n```\n\n----------------------------------------\n\nTITLE: Running Pytest Tests with 64-bit Precision Enabled\nDESCRIPTION: This command executes JAX tests using `pytest` in parallel (`-n auto`), with specific configuration set via environment variables. `JAX_ENABLE_X64=1` enables the use of 64-bit floats and integers within JAX during the tests. `JAX_NUM_GENERATED_CASES=25` sets the number of generated test cases.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_42\n\nLANGUAGE: shell\nCODE:\n```\nJAX_ENABLE_X64=1 JAX_NUM_GENERATED_CASES=25 pytest -n auto tests\n```\n\n----------------------------------------\n\nTITLE: Syncing Jupyter Notebooks with jupytext in JAX (Bash)\nDESCRIPTION: Commands to install jupytext and sync Jupyter notebooks between ipynb and md formats in the JAX project.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_51\n\nLANGUAGE: bash\nCODE:\n```\npip install jupytext==1.16.4\njupytext --sync docs/notebooks/thinking_in_jax.ipynb\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test Targets within a JAX Test File\nDESCRIPTION: This command runs a specific Python test file (`tests/lax_numpy_test.py`) but only executes tests whose names match the provided target string or regular expression. The `--test_targets=\"testPad\"` flag directs the test runner within the file to execute only tests related to `jax.numpy.pad` (assuming test methods are named like `testPad*`).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\npython tests/lax_numpy_test.py --test_targets=\"testPad\"\n```\n\n----------------------------------------\n\nTITLE: Defining Plotting Utilities for Lorentz ODE Solver\nDESCRIPTION: Implements various plotting utilities including image adjustment, conversion to RGBA, and functions for displaying and packing images. These utilities are used for visualizing the ODE solutions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef img_adjust(data):\n  oim = np.array(data)\n  hist, bin_edges = np.histogram(oim.flat, bins=256*256)\n  bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n  cdf = hist.cumsum()\n  cdf = cdf / float(cdf[-1])\n  return np.interp(oim.flat, bin_centers, cdf).reshape(oim.shape)\n\ndef imify(arr, vmin=None, vmax=None, cmap=None, origin=None):\n  arr = img_adjust(arr)\n  sm = cm.ScalarMappable(cmap=cmap)\n  sm.set_clim(vmin, vmax)\n  if origin is None:\n    origin = mpl.rcParams[\"image.origin\"]\n  if origin == \"lower\":\n    arr = arr[::-1]\n  rgba = sm.to_rgba(arr, bytes=True)\n  return rgba\n\ndef plot_image(array, **kwargs):\n  f = io.BytesIO()\n  imarray = imify(array, **kwargs)\n  plt.imsave(f, imarray, format=\"png\")\n  f.seek(0)\n  dat = f.read()\n  f.close()\n  display_png(dat, raw=True)\n\ndef pack_images(images, rows, cols):\n  shape = np.shape(images)\n  width, height, depth = shape[-3:]\n  images = np.reshape(images, (-1, width, height, depth))\n  batch = np.shape(images)[0]\n  rows = np.minimum(rows, batch)\n  cols = np.minimum(batch // rows, cols)\n  images = images[:rows * cols]\n  images = np.reshape(images, (rows, cols, width, height, depth))\n  images = np.transpose(images, [0, 2, 1, 3, 4])\n  images = np.reshape(images, [rows * width, cols * height, depth])\n  return images\n```\n\n----------------------------------------\n\nTITLE: Handling Aliasing Issues in Asynchronous Permutation Loop\nDESCRIPTION: This snippet addresses the aliasing issue between x and out by introducing a copy operation. However, this approach can lead to incorrect results due to buffer lifetime issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  fut = ppermute_start(x)\n  def body(i, fut):\n    *sems, x, out = fut\n    out = copy(out)\n    x = ppermute_done((*sems, x, out))\n    (*sems, x, out) = ppermute_start(x)\n    return (*sems, x, out)\n  (*sems, x, out) = fori_loop(0, 7, body, x)\n  return ppermute_done((*sems, x, out))\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Uncompiled Function in JAX\nDESCRIPTION: Measures the execution time of the uncompiled function, using block_until_ready to ensure accurate timing by waiting for accelerator completion.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n%timeit f(x).block_until_ready()\n```\n\n----------------------------------------\n\nTITLE: Attempting to Run JIT-Compiled Loop Function\nDESCRIPTION: Tries to execute the JIT-compiled loop function, which will fail due to JAX's constraints on dynamic loops in compiled code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    g(jnp.array([1., 2., 3.]), 5)\nexcept Exception as e:\n    print(e)\n    pass\n```\n\n----------------------------------------\n\nTITLE: Documenting Multihost Utility APIs - reStructuredText\nDESCRIPTION: This snippet sets up auto-documentation for the \\`jax.experimental.multihost_utils\\` module and its main functions using Sphinx's reStructuredText directives. Dependencies include Sphinx with the autodoc and autosummary extensions enabled, and the referenced Python module must be importable. The code lists the functions that will have individual API summaries generated in the documentation output, targeting users who want easily navigable and up-to-date documentation for distributed and multihost utilities in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.multihost_utils.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n``jax.experimental.multihost_utils`` module\n===========================================\n\n.. automodule:: jax.experimental.multihost_utils\n\n.. currentmodule:: jax.experimental.multihost_utils\n\nMultihost Utils API Reference\n-----------------------------\n.. autosummary::\n   :toctree: _autosummary\n\n   broadcast_one_to_all\n   sync_global_devices\n   process_allgather\n   assert_equal\n   host_local_array_to_global_array\n   global_array_to_host_local_array\n\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for JAX RMS Normalization Custom Op\nDESCRIPTION: Sets up a CMake build configuration for a JAX custom operation implementing RMS normalization. It determines the minimum CMake version, finds Python dependencies, locates JAX include directories via FFI, and configures the shared library target with appropriate compile settings.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/ffi/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18...3.27)\nproject(rms_norm LANGUAGES CXX)\n\nfind_package(Python 3.8 COMPONENTS Interpreter Development.Module REQUIRED)\nexecute_process(\n  COMMAND \"${Python_EXECUTABLE}\"\n          \"-c\" \"from jax.extend import ffi; print(ffi.include_dir())\"\n  OUTPUT_STRIP_TRAILING_WHITESPACE OUTPUT_VARIABLE XLA_DIR)\nmessage(STATUS \"XLA include directory: ${XLA_DIR}\")\n\nadd_library(rms_norm SHARED \"rms_norm.cc\")\ntarget_include_directories(rms_norm PUBLIC ${XLA_DIR})\ntarget_compile_features(rms_norm PUBLIC cxx_std_17)\ninstall(TARGETS rms_norm LIBRARY DESTINATION ${CMAKE_CURRENT_LIST_DIR})\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in RST Documentation\nDESCRIPTION: RST markup for embedding a YouTube video introduction to JAX by Jake VanderPlas using raw HTML iframe.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/beginner_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/WdTeDXsOSj4\"\n title=\"Intro to JAX: Accelerating Machine Learning research\"\nframeborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" \nallowfullscreen></iframe>\n```\n\n----------------------------------------\n\nTITLE: Logging output for JAX function export\nDESCRIPTION: This snippet displays the log output when exporting a JAX function, showing the function name, version, and lowering platforms.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/export.md#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nI0619 10:54:18.978733 8299482112 _export.py:606] Exported JAX function: fun_name=sin version=9 lowering_platforms=('cpu',) disabled_checks=()\nI0619 10:54:18.978767 8299482112 _export.py:607] Define JAX_DUMP_IR_TO to dump the module.\n```\n\n----------------------------------------\n\nTITLE: Fixing Shape Compatibility with Equality Constraints\nDESCRIPTION: Demonstrates how to fix broadcasting incompatibility by adding an explicit equality constraint between the dimensions involved in the operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x, y: x + y[:y.shape[0] // 2],\n               polymorphic_shapes=(\"a\", \"b\"),\n               polymorphic_constraints=(\"floordiv(b, 2) == a\",))(x, y)\n```\n\n----------------------------------------\n\nTITLE: LSTM Sequence Model Unsupported Ops\nDESCRIPTION: Conversion error for sequence-to-sequence LSTM model due to unsupported bitwise operations in TensorFlow.js.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nValueError('Unsupported Ops in the model before optimization\\nBitwiseOr, BitwiseAnd, BitwiseXor, LeftShift, Bitcast, RightShift')\n```\n\n----------------------------------------\n\nTITLE: Using bitwise_count Function in JAX NumPy\nDESCRIPTION: Example of using the new jax.numpy.bitwise_count function to count the number of set bits in an integer array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n\narray = jnp.array([0, 1, 2, 3, 4])\nbit_counts = jnp.bitwise_count(array)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow serving Docker image\nDESCRIPTION: Command to pull the TensorFlow serving nightly Docker image for model serving.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nDOCKER_IMAGE=tensorflow/serving:nightly\ndocker pull ${DOCKER_IMAGE}\n```\n\n----------------------------------------\n\nTITLE: Defining and Pinning Python Dependencies for GPU Stack - requirements file - Plaintext\nDESCRIPTION: This snippet specifies a set of Python package dependencies, pinning both versions and hashes to ensure consistent and secure installations. The list is formatted in a requirements.txt style, with package selectors for Linux and inline comments indicating source files and dependency trees. Key inputs include various nvidia-cuda* libraries with explicit versioning, opt-einsum, packaging, pillow, pluggy, and portpicker, ensuring robust GPU and numerical computing support.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_12.txt#2025-04-22_snippet_1\n\nLANGUAGE: Plaintext\nCODE:\n```\n--hash=sha256:28604ec42aaa09035b0fb7111432e5121bc385580b30c55d2acfb7d644b16548 \\\n--hash=sha256:4524739cfc080e9c9e53032912be8f020058e0a7186746d19acef3b6d916ea0b\n# via -r build/gpu-test-requirements.txt\nnvidia-cuda-runtime-cu12==12.8.57 ; sys_platform == \"linux\" \\\n--hash=sha256:534ccebd967b6a44292678fa5da4f00666029cb2ed07a79515ea41ef31fe3ec7 \\\n--hash=sha256:75342e28567340b7428ce79a5d6bb6ca5ff9d07b69e7ce00d2c7b4dc23eff0be \\\n--hash=sha256:89be637e3ee967323865b85e0f147d75f9a5bd98360befa37481b02dd57af8f5\n# via -r build/gpu-test-requirements.txt\nnvidia-cudnn-cu12==9.8.0.87 ; sys_platform == \"linux\" \\\n--hash=sha256:b4b5cfddc32aa4180f9d390ee99e9a9f55a89e7087329b41aba4319327e22466 \\\n--hash=sha256:b883faeb2f6f15dba7bbb6756eab6a0d9cecb59db5b0fa07577b9cfa24cd99f4 \\\n--hash=sha256:d6b02cd0e3e24aa31d0193a8c39fec239354360d7d81055edddb69f35d53a4c8\n# via -r build/gpu-test-requirements.txt\nnvidia-cufft-cu12==11.3.3.41 ; sys_platform == \"linux\" \\\n--hash=sha256:68509dcd7e3306e69d0e2d8a6d21c8b25ed62e6df8aac192ce752f17677398b5 \\\n--hash=sha256:da650080ab79fcdf7a4b06aa1b460e99860646b176a43f6208099bdc17836b6a \\\n--hash=sha256:f9760612886786601d27a0993bb29ce1f757e6b8b173499d0ecfa850d31b50f8\n# via -r build/gpu-test-requirements.txt\nnvidia-cusolver-cu12==11.7.2.55 ; sys_platform == \"linux\" \\\n--hash=sha256:0fd9e98246f43c15bee5561147ad235dfdf2d037f5d07c9d41af3f7f72feb7cc \\\n--hash=sha256:4d1354102f1e922cee9db51920dba9e2559877cf6ff5ad03a00d853adafb191b \\\n--hash=sha256:a5a516c55da5c5aba98420d9bc9bcab18245f21ec87338cc1f930eb18dd411ac\n# via -r build/gpu-test-requirements.txt\nnvidia-cusparse-cu12==12.5.7.53 ; sys_platform == \"linux\" \\\n--hash=sha256:3c1b61eb8c85257ea07e9354606b26397612627fdcd327bfd91ccf6155e7c86d \\\n--hash=sha256:82c201d6781bacf6bb7c654f0446728d0fe596dfdd82ef4a04c204ce3e107441 \\\n--hash=sha256:d869c6146ca80f4305b62e02d924b4aaced936f8173e3cef536a67eed2a91af1\n# via\n#   -r build/gpu-test-requirements.txt\n#   nvidia-cusolver-cu12\nnvidia-nccl-cu12==2.25.1 ; sys_platform == \"linux\" \\\n--hash=sha256:362aed5963fb9ea2ed2f264409baae30143498fd0e5c503aeaa1badd88cdc54a \\\n--hash=sha256:4ab428bc915785cc66e8c57cb34c7a64cf739c46702b8db748b6ad6cc7180cf8\n# via -r build/gpu-test-requirements.txt\nnvidia-nvjitlink-cu12==12.8.61 ; sys_platform == \"linux\" \\\n--hash=sha256:1166a964d25fdc0eae497574d38824305195a5283324a21ccb0ce0c802cbf41c \\\n--hash=sha256:45fd79f2ae20bd67e8bc411055939049873bfd8fac70ff13bd4865e0b9bdab17 \\\n--hash=sha256:9b80ecab31085dda3ce3b41d043be0ec739216c3fc633b8abe212d5a30026df0\n# via\n#   -r build/gpu-test-requirements.txt\n#   nvidia-cufft-cu12\n#   nvidia-cusolver-cu12\n#   nvidia-cusparse-cu12\nopt-einsum==3.3.0 \\\n--hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n--hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\n# via -r build/test-requirements.txt\npackaging==24.0 \\\n--hash=sha256:2ddfb553fdf02fb784c234c7ba6ccc288296ceabec964ad2eae3777778130bc5 \\\n--hash=sha256:eb82c5e3e56209074766e6885bb04b8c38a0c015d0a30036ebe7ece34c9989e9\n# via\n#   auditwheel\n#   build\n#   matplotlib\n#   pytest\npillow==11.0.0 \\\n--hash=sha256:00177a63030d612148e659b55ba99527803288cea7c75fb05766ab7981a8c1b7 \\\n--hash=sha256:006bcdd307cc47ba43e924099a038cbf9591062e6c50e570819743f5607404f5 \\\n--hash=sha256:084a07ef0821cfe4858fe86652fffac8e187b6ae677e9906e192aafcc1b69903 \\\n--hash=sha256:0ae08bd8ffc41aebf578c2af2f9d8749d91f448b3bfd41d7d9ff573d74f2a6b2 \\\n--hash=sha256:0e038b0745997c7dcaae350d35859c9715c71e92ffb7e0f4a8e8a16732150f38 \\\n--hash=sha256:1187739620f2b365de756ce086fdb3604573337cc28a0d3ac4a01ab6b2d2a6d2 \\\n--hash=sha256:16095692a253047fe3ec028e951fa4221a1f3ed3d80c397e83541a3037ff67c9 \\\n--hash=sha256:1a61b54f87ab5786b8479f81c4b11f4d61702830354520837f8cc791ebba0f5f \\\n--hash=sha256:1c1d72714f429a521d8d2d018badc42414c3077eb187a59579f28e4270b4b0fc \\\n--hash=sha256:1e2688958a840c822279fda0086fec1fdab2f95bf2b717b66871c4ad9859d7e8 \\\n--hash=sha256:20ec184af98a121fb2da42642dea8a29ec80fc3efbaefb86d8fdd2606619045d \\\n--hash=sha256:21a0d3b115009ebb8ac3d2ebec5c2982cc693da935f4ab7bb5c8ebe2f47d36f2 \\\n--hash=sha256:224aaa38177597bb179f3ec87eeefcce8e4f85e608025e9cfac60de237ba6316 \\\n--hash=sha256:2679d2258b7f1192b378e2893a8a0a0ca472234d4c2c0e6bdd3380e8dfa21b6a \\\n--hash=sha256:27a7860107500d813fcd203b4ea19b04babe79448268403172782754870dac25 \\\n--hash=sha256:290f2cc809f9da7d6d622550bbf4c1e57518212da51b6a30fe8e0a270a5b78bd \\\n--hash=sha256:2e46773dc9f35a1dd28bd6981332fd7f27bec001a918a72a79b4133cf5291dba \\\n--hash=sha256:3107c66e43bda25359d5ef446f59c497de2b5ed4c7fdba0894f8d6cf3822dafc \\\n--hash=sha256:375b8dd15a1f5d2feafff536d47e22f69625c1aa92f12b339ec0b2ca40263273 \\\n--hash=sha256:45c566eb10b8967d71bf1ab8e4a525e5a93519e29ea071459ce517f6b903d7fa \\\n--hash=sha256:499c3a1b0d6fc8213519e193796eb1a86a1be4b1877d678b30f83fd979811d1a \\\n--hash=sha256:4ad70c4214f67d7466bea6a08061eba35c01b1b89eaa098040a35272a8efb22b \\\n--hash=sha256:4b60c9520f7207aaf2e1d94de026682fc227806c6e1f55bba7606d1c94dd623a \\\n--hash=sha256:5178952973e588b3f1360868847334e9e3bf49d19e169bbbdfaf8398002419ae \\\n--hash=sha256:52a2d8323a465f84faaba5236567d212c3668f2ab53e1c74c15583cf507a0291 \\\n--hash=sha256:598b4e238f13276e0008299bd2482003f48158e2b11826862b1eb2ad7c768b97 \\\n--hash=sha256:5bd2d3bdb846d757055910f0a59792d33b555800813c3b39ada1829c372ccb06 \\\n--hash=sha256:5c39ed17edea3bc69c743a8dd3e9853b7509625c2462532e62baa0732163a904 \\\n--hash=sha256:5d203af30149ae339ad1b4f710d9844ed8796e97fda23ffbc4cc472968a47d0b \\\n--hash=sha256:5ddbfd761ee00c12ee1be86c9c0683ecf5bb14c9772ddbd782085779a63dd55b \\\n--hash=sha256:607bbe123c74e272e381a8d1957083a9463401f7bd01287f50521ecb05a313f8 \\\n--hash=sha256:61b887f9ddba63ddf62fd02a3ba7add935d053b6dd7d58998c630e6dbade8527 \\\n--hash=sha256:6619654954dc4936fcff82db8eb6401d3159ec6be81e33c6000dfd76ae189947 \\\n--hash=sha256:674629ff60030d144b7bca2b8330225a9b11c482ed408813924619c6f302fdbb \\\n--hash=sha256:6ec0d5af64f2e3d64a165f490d96368bb5dea8b8f9ad04487f9ab60dc4bb6003 \\\n--hash=sha256:6f4dba50cfa56f910241eb7f883c20f1e7b1d8f7d91c750cd0b318bad443f4d5 \\\n--hash=sha256:70fbbdacd1d271b77b7721fe3cdd2d537bbbd75d29e6300c672ec6bb38d9672f \\\n--hash=sha256:72bacbaf24ac003fea9bff9837d1eedb6088758d41e100c1552930151f677739 \\\n--hash=sha256:7326a1787e3c7b0429659e0a944725e1b03eeaa10edd945a86dead1913383944 \\\n--hash=sha256:73853108f56df97baf2bb8b522f3578221e56f646ba345a372c78326710d3830 \\\n--hash=sha256:73e3a0200cdda995c7e43dd47436c1548f87a30bb27fb871f352a22ab8dcf45f \\\n--hash=sha256:75acbbeb05b86bc53cbe7b7e6fe00fbcf82ad7c684b3ad82e3d711da9ba287d3 \\\n--hash=sha256:8069c5179902dcdce0be9bfc8235347fdbac249d23bd90514b7a47a72d9fecf4 \\\n--hash=sha256:846e193e103b41e984ac921b335df59195356ce3f71dcfd155aa79c603873b84 \\\n--hash=sha256:8594f42df584e5b4bb9281799698403f7af489fba84c34d53d1c4bfb71b7c4e7 \\\n--hash=sha256:86510e3f5eca0ab87429dd77fafc04693195eec7fd6a137c389c3eeb4cfb77c6 \\\n--hash=sha256:8853a3bf12afddfdf15f57c4b02d7ded92c7a75a5d7331d19f4f9572a89c17e6 \\\n--hash=sha256:88a58d8ac0cc0e7f3a014509f0455248a76629ca9b604eca7dc5927cc593c5e9 \\\n--hash=sha256:8ba470552b48e5835f1d23ecb936bb7f71d206f9dfeee64245f30c3270b994de \\\n--hash=sha256:8c676b587da5673d3c75bd67dd2a8cdfeb282ca38a30f37950511766b26858c4 \\\n--hash=sha256:8ec4a89295cd6cd4d1058a5e6aec6bf51e0eaaf9714774e1bfac7cfc9051db47 \\\n--hash=sha256:94f3e1780abb45062287b4614a5bc0874519c86a777d4a7ad34978e86428b8dd \\\n--hash=sha256:9a0f748eaa434a41fccf8e1ee7a3eed68af1b690e75328fd7a60af123c193b50 \\\n--hash=sha256:a5629742881bcbc1f42e840af185fd4d83a5edeb96475a575f4da50d6ede337c \\\n--hash=sha256:a65149d8ada1055029fcb665452b2814fe7d7082fcb0c5bed6db851cb69b2086 \\\n--hash=sha256:b3c5ac4bed7519088103d9450a1107f76308ecf91d6dabc8a33a2fcfb18d0fba \\\n--hash=sha256:b4fd7bd29610a83a8c9b564d457cf5bd92b4e11e79a4ee4716a63c959699b306 \\\n--hash=sha256:bcd1fb5bb7b07f64c15618c89efcc2cfa3e95f0e3bcdbaf4642509de1942a699 \\\n--hash=sha256:c12b5ae868897c7338519c03049a806af85b9b8c237b7d675b8c5e089e4a618e \\\n--hash=sha256:c26845094b1af3c91852745ae78e3ea47abf3dbcd1cf962f16b9a5fbe3ee8488 \\\n--hash=sha256:c6a660307ca9d4867caa8d9ca2c2658ab685de83792d1876274991adec7b93fa \\\n--hash=sha256:c809a70e43c7977c4a42aefd62f0131823ebf7dd73556fa5d5950f5b354087e2 \\\n--hash=sha256:c8b2351c85d855293a299038e1f89db92a2f35e8d2f783489c6f0b2b5f3fe8a3 \\\n--hash=sha256:cb929ca942d0ec4fac404cbf520ee6cac37bf35be479b970c4ffadf2b6a1cad9 \\\n--hash=sha256:d2c0a187a92a1cb5ef2c8ed5412dd8d4334272617f532d4ad4de31e0495bd923 \\\n--hash=sha256:d69bfd8ec3219ae71bcde1f942b728903cad25fafe3100ba2258b973bd2bc1b2 \\\n--hash=sha256:daffdf51ee5db69a82dd127eabecce20729e21f7a3680cf7cbb23f0829189790 \\\n--hash=sha256:e58876c91f97b0952eb766123bfef372792ab3f4e3e1f1a2267834c2ab131734 \\\n--hash=sha256:eda2616eb2313cbb3eebbe51f19362eb434b18e3bb599466a1ffa76a033fb916 \\\n--hash=sha256:ee217c198f2e41f184f3869f3e485557296d505b5195c513b2bfe0062dc537f1 \\\n--hash=sha256:f02541ef64077f22bf4924f225c0fd1248c168f86e4b7abdedd87d6ebaceab0f \\\n--hash=sha256:f1b82c27e89fffc6da125d5eb0ca6e68017faf5efc078128cfaa42cf5cb38798 \\\n--hash=sha256:fba162b8872d30fea8c52b258a542c5dfd7b235fb5cb352240c8d63b414013eb \\\n--hash=sha256:fbbcb7b57dc9c794843e3d1258c0fbf0f48656d46ffe9e09b63bbd6e8cd5d0a2 \\\n--hash=sha256:fcb4621042ac4b7865c179bb972ed0da0218a076dc1820ffc48b1d74c1e37fe9\n# via\n#   -r build/test-requirements.txt\n#   matplotlib\npluggy==1.5.0 \\\n--hash=sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1 \\\n--hash=sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669\n# via pytest\nportpicker==1.6.0 ; python_version < \"3.13\" \\\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Documentation in Markdown\nDESCRIPTION: Markdown documentation that outlines JAX's CI system architecture, including build workflows, testing configurations, and artifact storage mechanisms. The documentation covers presubmit checks, continuous builds, nightly builds, and release processes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/ci/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# JAX Continuous Integration\n\nThis folder contains the configuration files and scripts used to build and test\nJAX. It is typically used by continuous integration (CI) jobs to automate builds\nand run comprehensive tests across various platforms and configurations. This\npage provides an overview of the JAX CI system, its components, and the\ndifferent workflows it supports.\n```\n\n----------------------------------------\n\nTITLE: Building jaxlib from Source for CPU/TPU\nDESCRIPTION: Commands to build jaxlib from source for CPU or TPU support and install the resulting wheel.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython build/build.py build --wheels=jaxlib --verbose\npip install dist/*.whl  # installs jaxlib (includes XLA)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for JAX Pallas Matmul\nDESCRIPTION: Imports necessary libraries for the matrix multiplication implementation. This includes JAX core functionalities, the Pallas experimental library, Pallas TPU extensions, JAX random number generation, JAX NumPy, and standard NumPy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/matmul.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#@title Imports\nimport functools\nfrom typing import Callable\n\nimport jax\nfrom jax.experimental import pallas as pl\nfrom jax.experimental.pallas import tpu as pltpu\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Running JAX Doctests for Markdown and reStructuredText Files\nDESCRIPTION: This command runs `pytest` to execute doctests embedded within Markdown (`.md`) and reStructuredText (`.rst`) files located in the `docs` directory. It uses parallel execution (`-n auto`), concise traceback format (`--tb=short`), specifies file globs for doctests, continues even if some doctests fail (`--doctest-continue-on-failure`), and ignores a specific file (`docs/multi_process.md`). Environment variables `JAX_TRACEBACK_FILTERING=off` and XLA flags are also set.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_45\n\nLANGUAGE: shell\nCODE:\n```\nJAX_TRACEBACK_FILTERING=off XLA_FLAGS=--xla_force_host_platform_device_count=8 pytest -n auto --tb=short --doctest-glob='*.md' --doctest-glob='*.rst' docs --doctest-continue-on-failure --ignore=docs/multi_process.md\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Pallas Libraries for Pipelining\nDESCRIPTION: This snippet imports the necessary libraries for working with JAX and Pallas, which are used for implementing software pipelining techniques.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/pipelining.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nfrom jax import numpy as jnp\nfrom jax.experimental import pallas as pl\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Installing JAX for CPU using pip\nDESCRIPTION: This command uses pip, the Python package installer, to install the latest version of JAX optimized for CPU execution. The `-U` flag ensures that if JAX is already installed, it will be upgraded to the latest version.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install -U jax\n```\n\n----------------------------------------\n\nTITLE: Checking Notebook Sync with pre-commit in JAX (Bash)\nDESCRIPTION: Commands to install pre-commit and check if markdown and ipynb files are properly synced using jupytext in the JAX project.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_52\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit run jupytext --all-files\n```\n\n----------------------------------------\n\nTITLE: Importing Pallas and JAX Dependencies\nDESCRIPTION: Import statements for using Pallas with JAX, including necessary functions and modules.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/quickstart.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\nimport jax\nfrom jax.experimental import pallas as pl\nimport jax.numpy as jnp\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Generating Block-Sparse Matrix and Block-COO Representation - Python\nDESCRIPTION: This utility function samples a dense matrix and its block-sparse (block-COO) representation for use in validation and sparse kernel tutorials. It takes desired dimensions, block size, nonzero block probability, and dtype as parameters. Dependencies: JAX random, JAX/NumPy. The function returns a tuple consisting of the dense matrix, data blocks, and their indices in both axes. Limitation: block sizes must exactly divide major dimensions for correct reshaping and zero-padding is not handled.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/sparse.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate_block_sparse_mat(key, M, N, blk_M, blk_N, p=0.2, dtype=jnp.float32):\\n  \\\"\\\"\\\"Returns a sampled matrix and its block-sparse representation.\\n\\n  Args:\\n    key: RNG Key.\\n    M: Major array dimension.\\n    N: Minor array dimension.\\n    blk_M: Block size along M dimension.\\n    blk_N: Block size along N dimension.\\n    p: Probability that a block will be non-zero.\\n    dtype: dtype of the sampled matrix.\\n\\n  Returns:\\n    dense_mat: A (M, N) dense sampled array.\\n    block_data: A (num_blocks, blk_M, blk_N) array of data blocks representing\\n      the non-zero blocks of the matrix.\\n    indices_i: A (num_blocks,) array of block indices for the first axis.\\n    indices_j: A (num_blocks,) array of block indices for the second axis.\\n  \\\"\\\"\\\"\\n  mask_key, blocks_key = jax.random.split(key)\\n  num_blocks = (M // blk_M, N // blk_N)\\n  # We first sample a block mask, denoting which blocks are nonzero.\\n  block_mask = jax.random.bernoulli(mask_key, p=p, shape=num_blocks)\\n  num_blocks = jnp.sum(block_mask)\\n  indices = jnp.where(block_mask)\\n  # For each non-zero block, we sample a block of random values.\\n  block_data = jax.random.uniform(blocks_key,\\n                                  shape=(num_blocks, blk_M, blk_N),\\n                                  dtype=dtype)\\n  # For checking purposes, create the dense version of the sparse matrix.\\n  dense_mat = jnp.zeros((M, N), dtype=dtype)\\n  for blk in range(num_blocks):\\n    idx_i = indices[0][blk]\\n    idx_j = indices[1][blk]\\n    slice_i = slice(idx_i * blk_M, (idx_i + 1) * blk_M)\\n    slice_j = slice(idx_j * blk_N, (idx_j + 1) * blk_N)\\n    dense_mat = dense_mat.at[slice_i, slice_j].set(block_data[blk])\\n  return dense_mat, block_data, indices[0], indices[1]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Equal Dimension Assumption in Shape Polymorphism\nDESCRIPTION: Example showing how jax2tf assumes equal sizes for dimensions represented by the same variable, which can lead to incorrect behavior when dimensions actually differ in size.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef f_jax(x):\n  return 0 if x.shape[0] != x.shape[1] else 1\n\nx45 = np.ones((4, 5), dtype=np.float32)\nself.assertEqual(0, f_jax(x45))  # JAX seems that x.shape[0] != x.shape[1]\n\n# jax2tf catches the broken assumption x.shape[0] == x.shape[1] if the lowered\n# function is executed eagerly.\n# Raises: ValueError: polymorphic shape ('b, b',) has dimension variable 'b' corresponding to multiple values {4, 5}, for argument shapes (TensorShape([4, 5]),)\njax2tf.convert(f_jax, polymorphic_shapes=[\"b, b\"])(x45)\n\n# However, if we first trace to a TensorFlow graph, we may miss the broken assumption.\nf_tf = tf.function(\n    jax2tf.convert(f_jax, polymorphic_shapes=[\"b, b\"]),\n    autograph=False).get_concrete_function(tf.TensorSpec([None, None], dtype=np.float32))\nself.assertEqual(1, f_tf(x45))\n```\n\n----------------------------------------\n\nTITLE: Using JAX JIT Without Shard Map (Problematic Approach)\nDESCRIPTION: Example of using jax.jit directly with a function containing custom_partitioning primitives, which generates different cache keys on each execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/persistent_compilation_cache.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlayernorm_matmul_without_shard_map = jax.jit(F, in_shardings=(...), out_sharding=(...))(x1, x2, gamma, beta)\n```\n\n----------------------------------------\n\nTITLE: Training the Quickdraw CNN Model with Python\nDESCRIPTION: Command to train a CNN on the Quickdraw dataset using Flax. This script downloads the dataset if needed, trains the model for 5 epochs by default, and exports both the model and its TF.js-compatible version.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/tf_js/quickdraw/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 quickdraw.py\n```\n\n----------------------------------------\n\nTITLE: Specifying package dependencies with hashes\nDESCRIPTION: This snippet shows how package dependencies are specified with their versions and SHA256 hashes. It includes examples for mdurl, ml-dtypes, mpmath, and numpy packages.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_11.txt#2025-04-22_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nmdurl==0.1.2 \\\n    --hash=sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8 \\\n    --hash=sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba\n    # via markdown-it-py\nml-dtypes==0.5.1 \\\n    --hash=sha256:023ce2f502efd4d6c1e0472cc58ce3640d051d40e71e27386bed33901e201327 \\\n    --hash=sha256:05f23447a1c20ddf4dc7c2c661aa9ed93fcb2658f1017c204d1e758714dc28a8 \\\n    --hash=sha256:12651420130ee7cc13059fc56dac6ad300c3af3848b802d475148c9defd27c23\n    # via\n    #   -r build/requirements.in\n    #   tensorstore\nmpmath==1.4.0a1 \\\n    --hash=sha256:78884400f439f500fa76be0121a8f9598313d87664863a192e1185ddbd7ae97f \\\n    --hash=sha256:f8b7b5a3a1726ab6e8c898eb2157426b82c482ab1ab8ffed9f88bb9e07c6e9c1\n    # via -r build/test-requirements.txt\nnumpy==2.0.0 ; python_version <= \"3.12\" \\\n    --hash=sha256:04494f6ec467ccb5369d1808570ae55f6ed9b5809d7f035059000a37b8d7e86f \\\n    --hash=sha256:0a43f0974d501842866cc83471bdb0116ba0dffdbaac33ec05e6afed5b615238 \\\n    --hash=sha256:0e50842b2295ba8414c8c1d9d957083d5dfe9e16828b37de883f51fc53c4016f\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Core Dependencies with Exact Versions\nDESCRIPTION: Lists the core dependencies required for JAX operation with exact version pins. These include utility libraries for command-line flags, data structures, testing, serialization, data types, and mathematical operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_14_ft.txt#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==2.1.0\n\nattrs==24.3.0\n\nhypothesis==6.123.9\n\nsortedcontainers==2.4.0\n\nflatbuffers==24.12.23\n\nml-dtypes==0.5.1\n\nopt-einsum==3.4.0\n```\n\n----------------------------------------\n\nTITLE: Using Deprecated custom_transforms for Custom Differentiation in JAX (Python)\nDESCRIPTION: Demonstrates the use of the old custom_transforms API in JAX to define a function with a custom VJP rule. Highlights the limitations of the previous approach: tracing errors occur with this method because custom_transforms forms the jaxpr up-front, losing Python flexibility for control flow or debugging. Requires the JAX library (import jax) and the grad transformation for differentiation. The function f has a conditional returning either x or 0, and an undefined VJP rule placeholder; grad(f)(1.) triggers an error.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/2026-custom-derivatives.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# old custom_transforms api to be replaced\n@jax.custom_transforms\ndef f(x):\n  if x > 0:\n    return x\n  else:\n    return 0.\n\ndef f_vjp(x):\n  return ...\n\njax.defvjp_all(f, f_vjp)\n\ngrad(f)(1.)  # Error!\n```\n\n----------------------------------------\n\nTITLE: Updating JAX Dependencies Including Pre-releases using Bazel (Shell)\nDESCRIPTION: This Bazel command updates the dependency lock file for a specific Python version, similar to the previous example, but includes pre-release versions in the dependency resolution. The `-- --pre` argument is passed through to the underlying `pip-compile` tool.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nbazel run //build:requirements.update --repo_env=HERMETIC_PYTHON_VERSION=3.12 -- --pre\n```\n\n----------------------------------------\n\nTITLE: JAX Named Scope Usage\nDESCRIPTION: Example of the named_scope context manager added for profiler metadata in version 0.3.14.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\njax.named_scope\n```\n\n----------------------------------------\n\nTITLE: Accessing JAX Default Device\nDESCRIPTION: Example function reference showing the addition of jax.default_device() in version 0.3.14.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\njax.default_device()\n```\n\n----------------------------------------\n\nTITLE: Pulling ROCm JAX Docker Image\nDESCRIPTION: Command to pull the latest ROCm JAX Docker image from Docker Hub.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/rocm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ndocker pull rocm/jax-community:latest\n```\n\n----------------------------------------\n\nTITLE: Rewriting Function to Avoid Unnecessary Copy with ppermute\nDESCRIPTION: This modified version of `f(x)` avoids the unnecessary copy introduced by XLA. Instead of using the original input `x` for `z = x + 1`, it uses the aliased output `x2` from `ppermute_start`. Since `x2` is known to be identical to `x` (because `ppermute_start` just forwards it), this is semantically correct. Now, XLA no longer sees a separate consumer of the original `x` buffer after `ppermute_start`, eliminating the need for the defensive copy.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  *sems, x2, y = ppermute_start(x)\n  z = x2 + 1\n  y = ppermute_done((*sems, x2, y))\n  return y, z\n```\n```\n\n----------------------------------------\n\nTITLE: Using Custom JVP with Non-Differentiation Transforms\nDESCRIPTION: Shows that when using non-differentiation transforms like vmap and jit, only the original function is called, not the custom JVP rule. This demonstrates JAX's transformation behavior with custom differentiation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import vmap, jit\n\nprint(f(3.))\n```\n\n----------------------------------------\n\nTITLE: Running a Specific JAX Test File with Custom Case Count\nDESCRIPTION: This command directly executes a specific Python test file (`tests/lax_numpy_test.py`) using the Python interpreter. The `JAX_NUM_GENERATED_CASES=5` environment variable is set to limit the number of generated test cases specifically for this run to 5. This is useful for debugging or getting detailed output for a single test file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nJAX_NUM_GENERATED_CASES=5 python tests/lax_numpy_test.py\n```\n\n----------------------------------------\n\nTITLE: Running Bazel Tests Using Preinstalled JAX/jaxlib\nDESCRIPTION: This command executes the JAX CPU and backend-independent tests using Bazel, but instructs Bazel not to build `jaxlib` from source. The flag `--//jax:build_jaxlib=false` ensures that the tests use the versions of `jax` and `jaxlib` already installed within the hermetic Python environment (presumably installed via previous steps like modifying `requirements.in`).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_36\n\nLANGUAGE: shell\nCODE:\n```\nbazel test --//jax:build_jaxlib=false //tests:cpu_tests //tests:backend_independent_tests\n```\n\n----------------------------------------\n\nTITLE: Handling Inconclusive Dimension Comparison in JAX to TensorFlow XLA Conversion\nDESCRIPTION: This error occurs when converting a JAX BiLSTM model with dynamic shapes to TensorFlow XLA. The conversion fails due to an inconclusive dimension comparison between 'b' and '2', which cannot be represented as a polynomial or boolean constant.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nInconclusiveDimensionOperation(\"Dimension polynomial comparison 'b' == '2' is inconclusive\\n\\nThis error arises for arithmetic or comparison operations with shapes that\\nare non-constant, and the result of the operation cannot be represented as\\na polynomial of dimension variables, or a boolean constant (for comparisons).\\n\\nPlease see https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#computing-with-dimension-variables\\nfor more details.\\n\")\n```\n\n----------------------------------------\n\nTITLE: Docker Environment Variables Table in Markdown\nDESCRIPTION: Markdown table defining three environment variables (JAXCI_DOCKER_WORK_DIR, JAXCI_DOCKER_ARGS, JAXCI_DOCKER_IMAGE) used in JAX's Docker container configuration, including their default values, behaviors, and usage links.\nSOURCE: https://github.com/jax-ml/jax/blob/main/ci/envs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nName                    | Default Value                                                                                                | Behavior                                                                                             | Usage\n----------------------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- | -----\n`JAXCI_DOCKER_WORK_DIR` | \"/jax\"                                                                                                       | The path on the container where the JAX Git repository is mounted to.                                | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_WORK_DIR&type=code)\n`JAXCI_DOCKER_ARGS`     | Empty String                                                                                                 | Space seprated string of additional arguments that will be passed when starting the Docker container | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_ARGS&type=code)\n`JAXCI_DOCKER_IMAGE`    | Depends on the system (see [ci/envs/docker.env](https://github.com/jax-ml/jax/blob/main/ci/envs/docker.env)) | Docker image to pull                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_IMAGE&type=code)\n```\n\n----------------------------------------\n\nTITLE: Inspecting the exported SavedModel\nDESCRIPTION: Command to examine the structure and details of the exported SavedModel using the saved_model_cli tool.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsaved_model_cli show --all --dir ${MODEL_PATH}/${MODEL}/${MODEL_VERSION}\n```\n\n----------------------------------------\n\nTITLE: Auto-documenting the jax.stages.Lowered Class using Sphinx\nDESCRIPTION: This Sphinx directive generates documentation for the `Lowered` class within the `jax.stages` module. It includes documentation for the `in_tree`, `out_tree`, `compile`, `as_text`, `compiler_ir`, and `cost_analysis` members.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.stages.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: Lowered\n   :members: in_tree, out_tree, compile, as_text, compiler_ir, cost_analysis\n```\n\n----------------------------------------\n\nTITLE: Creating New-Style Typed PRNG Keys with JAX (Python)\nDESCRIPTION: Shows creation of a new-style, typed PRNG key using jax.random.key. Unlike raw keys, typed keys are scalars with a dedicated dtype (e.g., key<fry>). Requires JAX 0.4.24+ and proper import. Input: seed (integer). Output: Array with dtype set to key<fry>. This method is compatible with planned future customizations and type enforcement.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9263-typed-keys.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> key = jax.random.key(0)\n>>> key\nArray((), dtype=key<fry>) overlaying:\n[0 0]\n>>> key.shape\n()\n>>> key.dtype\nkey<fry>\n\n```\n\n----------------------------------------\n\nTITLE: Saving TensorFlow SavedModel with Custom Gradients\nDESCRIPTION: Demonstrates how to save a TensorFlow SavedModel with custom gradients, which is necessary when using jax2tf conversion with gradients.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptions = tf.saved_model.SaveOptions(experimental_custom_gradients=True)\ntf.saved_model.save(model, path, options=options)\n```\n\n----------------------------------------\n\nTITLE: Citing JAX in Publications - BibTeX Entry - BibTeX\nDESCRIPTION: This snippet provides a BibTeX entry for citing the JAX repository in academic work. It specifies the authors, title, URL, version, and year for reproducibility and standardized referencing. Replace the version number with the value from jax/version.py if a more updated citation is needed.\nSOURCE: https://github.com/jax-ml/jax/blob/main/README.md#2025-04-22_snippet_17\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{jax2018github,\\n  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},\\n  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},\\n  url = {http://github.com/jax-ml/jax},\\n  version = {0.3.13},\\n  year = {2018},\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: JAX to TFJS Conversion Error - Seq2seq LSTM\nDESCRIPTION: Error message indicating unsupported operations when converting to TensorFlow.js, specifically bitwise operations and bit manipulation functions.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nConversion error\nValueError('Unsupported Ops in the model before optimization\nBitwiseOr, BitwiseAnd, BitwiseXor, LeftShift, Bitcast, RightShift')\n```\n\n----------------------------------------\n\nTITLE: Analyzing Doubly Vectorized Dot Product\nDESCRIPTION: Uses make_jaxpr to show the intermediate representation of a dot product vectorized across two dimensions, demonstrating nested vmap operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nmake_jaxpr(vmap(vmap(jnp.dot)))(jnp.ones((10, 10, 8)), jnp.ones((10, 10, 8)))\n```\n\n----------------------------------------\n\nTITLE: Building JAX Documentation with Sphinx (Bash)\nDESCRIPTION: Commands to build JAX documentation using Sphinx, with options for executing notebooks or building without execution.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_50\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-build -b html docs docs/build/html -j auto\n```\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-build -b html -D nb_execution_mode=off docs docs/build/html -j auto\n```\n\n----------------------------------------\n\nTITLE: Importing jax.experimental.custom_partitioning Module Documentation\nDESCRIPTION: This Sphinx directive automatically imports and formats documentation for the entire `jax.experimental.custom_partitioning` Python module. It pulls docstrings and signatures from the source code.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.custom_partitioning.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: jax.experimental.custom_partitioning\n```\n\n----------------------------------------\n\nTITLE: JAX Cache Initialization\nDESCRIPTION: Function reference showing the updated cache initialization without max_cache_size_bytes parameter.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\njax.experimental.compilation_cache.initialize_cache\n```\n\n----------------------------------------\n\nTITLE: Sphinx Directive for API Function Summary\nDESCRIPTION: This Sphinx `autosummary` directive generates a concise summary table linking to the documentation of the listed functions (`create_device_mesh`, `create_hybrid_device_mesh`) within the `jax.experimental.mesh_utils` module. The `:toctree: _autosummary` option specifies the directory where individual function documentation pages will be generated.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.mesh_utils.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n  :toctree: _autosummary\n\n  create_device_mesh\n  create_hybrid_device_mesh\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Dependencies with Hash Values\nDESCRIPTION: This snippet shows how package dependencies are specified with version numbers and SHA256 hash values for security and reproducibility. It includes examples for fsspec, hypothesis, importlib-resources, and other packages.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13_ft.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nfsspec==2024.12.0 \\\n    --hash=sha256:670700c977ed2fb51e0d9f9253177ed20cbde4a3e5c0283cc5385b5870c8533f \\\n    --hash=sha256:b520aed47ad9804237ff878b504267a3b0b441e97508bd6d2d8774e3db85cee2\n    # via etils\nhypothesis==6.123.9 \\\n    --hash=sha256:0f924bd9513daa9ecddbfe8abe8b3f7598d4d09234fe1027b19b4cd717adba05 \\\n    --hash=sha256:aca6a2f7aeef85e5201079ab93156fca137a8cabcf5cc39ea2a3b7147432fe89\n    # via -r build/test-requirements.txt\nimportlib-resources==6.5.2 \\\n    --hash=sha256:185f87adef5bcc288449d98fb4fba07cea78bc036455dd44c5fc4a2fe78fed2c \\\n    --hash=sha256:789cfdc3ed28c78b67a06acb8126751ced69a3d5f79c095a98298cd8a760ccec\n    # via etils\n```\n\n----------------------------------------\n\nTITLE: Referencing JAX implementation of scipy.special.lpmn\nDESCRIPTION: Mentions the JAX implementation of lpmn function, which generates Legendre polynomials using a fori_loop. The implementation differs from scipy's API and has limited discoverable uses.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/18137-numpy-scipy-scope.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\njax.scipy.special.lpmn\n```\n\n----------------------------------------\n\nTITLE: Setuptools Configuration with SHA-256 Hashes\nDESCRIPTION: Setuptools package configuration marked as unsafe in requirements file. It specifies version 76.0.0 with SHA-256 hashes and indicates it's included via multiple requirements files.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_11.txt#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nsetuptools==76.0.0 \\\n    --hash=sha256:199466a166ff664970d0ee145839f5582cb9bca7a0a3a2e795b6a9cb2308e9c6 \\\n    --hash=sha256:43b4ee60e10b0d0ee98ad11918e114c70701bc6051662a9a675a0496c1a158f4\n    # via\n    #   -r build/requirements.in\n    #   -r build/test-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Executing Build Scripts Directly on macOS\nDESCRIPTION: Command to run a JAX build script directly on macOS systems without using Docker. This example demonstrates running the jaxlib artifact build script.\nSOURCE: https://github.com/jax-ml/jax/blob/main/ci/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./ci/build_artifacts.sh jaxlib\n```\n\n----------------------------------------\n\nTITLE: JAX to TFLite Conversion Error - Seq2seq LSTM\nDESCRIPTION: Error log showing unsupported TFLite runtime operations for seq2seq LSTM model conversion, including bitwise operations, tensor manipulation, and selection operations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nConversion error\nSome ops are not supported by the native TFLite runtime\n\ttf.Bitcast(tensor<1x4xui32>) -> (tensor<1x4xf32>) : {device = \"\"}\n\ttf.BitwiseOr(tensor<1x4xui32>, tensor<ui32>) -> (tensor<1x4xui32>) : {device = \"\"}\n\ttf.BitwiseOr(tensor<1xui32>, tensor<1xui32>) -> (tensor<1xui32>) : {device = \"\"}\n```\n\n----------------------------------------\n\nTITLE: Testing Gradient at Non-differentiable Point\nDESCRIPTION: Showing that standard autodiff produces NaN for the gradient at the non-differentiable point (zero).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint(grad(f)(0.))\n```\n\n----------------------------------------\n\nTITLE: Hosting the Quickdraw Demo Locally\nDESCRIPTION: Command to start a local HTTP server for interacting with the trained Quickdraw model. This allows users to test the model's drawing recognition capabilities through a web interface.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/examples/tf_js/quickdraw/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 -m http.server <port>\n```\n\n----------------------------------------\n\nTITLE: Using fill_diagonal in JAX NumPy\nDESCRIPTION: Example of using the newly added jax.numpy.fill_diagonal function to fill the diagonal of an array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\n\narray = jnp.zeros((4, 4))\njnp.fill_diagonal(array, 5)\n```\n\n----------------------------------------\n\nTITLE: Documenting JAX Profiler Module Features with Sphinx Directives - reStructuredText\nDESCRIPTION: This snippet uses reStructuredText, specifically Sphinx autodoc directives, to structure module documentation for jax.profiler. It employs directives like automodule for module-level docs, autosummary for generating API summaries and toctree navigation, and doc references for linking to additional information. No Python code is executed; all content is for static documentation generation, dependent on Sphinx and its extensions for processing. Sections outline high-level features for user guidance.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.profiler.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: jax.profiler\n\n``jax.profiler`` module\n=======================\n\n.. automodule:: jax.profiler\n\nTracing and time profiling\n--------------------------\n\n:doc:`profiling` describes how to make use of JAX's tracing and time profiling\nfeatures.\n\n.. autosummary::\n  :toctree: _autosummary\n\n  start_server\n  start_trace\n  stop_trace\n  trace\n  annotate_function\n  TraceAnnotation\n  StepTraceAnnotation\n\n\nDevice memory profiling\n-----------------------\n\nSee :doc:`device_memory_profiling` for an introduction to JAX's device memory\nprofiling features.\n\n.. autosummary::\n  :toctree: _autosummary\n\n  device_memory_profile\n  save_device_memory_profile\n\n```\n\n----------------------------------------\n\nTITLE: Dynamic Shape Example: nansum Function\nDESCRIPTION: This snippet defines a nansum function that uses dynamic shapes, which works outside of JAX transforms but fails within them.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef nansum(x):\n  mask = ~jnp.isnan(x)  # boolean mask selecting non-nan values\n  x_without_nans = x[mask]\n  return x_without_nans.sum()\n```\n\n----------------------------------------\n\nTITLE: Configuring NVIDIA Kernel Module for Profiling in Shell\nDESCRIPTION: These commands modify NVIDIA kernel module options to allow non-admin profiling, update the initramfs, and reboot the system.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/profiling.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\necho 'options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"' | sudo tee -a /etc/modprobe.d/nvidia-kernel-common.conf\nsudo update-initramfs -u\nsudo reboot now\n```\n\n----------------------------------------\n\nTITLE: Installing JAX with ROCm Support\nDESCRIPTION: Command to install JAX with ROCm support using pip.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/rocm/README.md#2025-04-22_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\npip3 install jax[rocm]\n```\n\n----------------------------------------\n\nTITLE: Auto-documenting the jax.stages Module using Sphinx\nDESCRIPTION: This Sphinx directive automatically imports and documents the `jax.stages` Python module. It serves as the entry point for documenting the contents of this module.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.stages.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: jax.stages\n```\n\n----------------------------------------\n\nTITLE: Referencing JAX implementation of scipy.special.sph_harm\nDESCRIPTION: Mentions the JAX implementation of sph_harm function, which is built on lpmn and has an API that diverges from the corresponding scipy function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/18137-numpy-scipy-scope.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\njax.scipy.special.sph_harm\n```\n\n----------------------------------------\n\nTITLE: Testing JAX ROCm Device Detection\nDESCRIPTION: Python command to verify JAX can detect ROCm devices in the environment.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/rocm/README.md#2025-04-22_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npython -c \"import jax; print(jax.devices())\"\n```\n\n----------------------------------------\n\nTITLE: Running Single-Accelerator GPU Tests in Parallel with Bazel\nDESCRIPTION: This script configures and runs JAX GPU tests (excluding multi-accelerator tests) in parallel across multiple GPUs. It sets environment variables `NB_GPUS` and `JOBS_PER_ACC` to define the number of GPUs and parallel jobs per GPU. These are used to calculate total jobs (`J`) and passed to Bazel via `--test_env` flags along with a custom run script (`build/parallel_accelerator_execute.sh`) specified using `--run_under`. It also disables XLA preallocation and filters out multi-accelerator tests.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/developer.md#2025-04-22_snippet_38\n\nLANGUAGE: shell\nCODE:\n```\nNB_GPUS=2\nJOBS_PER_ACC=4\nJ=$((NB_GPUS * JOBS_PER_ACC))\nMULTI_GPU=\"--run_under $PWD/build/parallel_accelerator_execute.sh --test_env=JAX_ACCELERATOR_COUNT=${NB_GPUS} --test_env=JAX_TESTS_PER_ACCELERATOR=${JOBS_PER_ACC} --local_test_jobs=$J\"\nbazel test //tests:gpu_tests //tests:backend_independent_tests --test_env=XLA_PYTHON_CLIENT_PREALLOCATE=false --test_tag_filters=-multiaccelerator $MULTI_GPU\n```\n\n----------------------------------------\n\nTITLE: Specifying JAX Dependencies with Hashes in Python\nDESCRIPTION: This snippet shows how to specify Python package dependencies with precise versions and SHA256 hash checksums for verification. It includes examples for numpy, ml-dtypes, mpmath, and NVIDIA CUDA libraries.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13_ft.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nnumpy==2.2.5 \\\n    --hash=sha256:0255732338c4fdd00996c0421884ea8a3651eea555c3a56b84892b66f696eb70 \\\n    --hash=sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a \\\n    # ... (additional hashes omitted for brevity)\n\nml-dtypes==0.5.1 \\\n    --hash=sha256:023ce2f502efd4d6c1e0472cc58ce3640d051d40e71e27386bed33901e201327 \\\n    --hash=sha256:05f23447a1c20ddf4dc7c2c661aa9ed93fcb2658f1017c204d1e758714dc28a8 \\\n    # ... (additional hashes omitted for brevity)\n\nmpmath==1.3.0 \\\n    --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n    --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n    # via -r build/test-requirements.txt\n\nnvidia-cublas-cu12==12.8.3.14 ; sys_platform == \"linux\" \\\n    --hash=sha256:3f0e05e7293598cf61933258b73e66a160c27d59c4422670bf0b79348c04be44 \\\n    --hash=sha256:93a4e0e386cc7f6e56c822531396de8170ed17068a1e18f987574895044cd8c3 \\\n    --hash=sha256:9ae5eae500aead01fc4bdfc458209df638b1a3551557ce11a78eea9ece602ae9\n```\n\n----------------------------------------\n\nTITLE: Testing JAX Transformations of Stable log1pexp\nDESCRIPTION: Confirming that the numerically stable log1pexp with custom JVP still works correctly with other JAX transformations.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(jit(log1pexp)(3.))\nprint(jit(grad(log1pexp))(3.))\nprint(vmap(jit(grad(log1pexp)))(jnp.arange(3.)))\n```\n\n----------------------------------------\n\nTITLE: Including External Markdown Content in Sphinx Documentation\nDESCRIPTION: A Sphinx directive for including external markdown content. This specific directive loads the CHANGELOG.md file from the parent directory into the current documentation file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/changelog.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{include} ../CHANGELOG.md\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Pinned Python Dependencies with Hashes (pip)\nDESCRIPTION: This configuration lists Python package dependencies required for the JAX project. It uses the pip requirements file format, specifying exact versions (`==`) and including SHA256 hashes (`--hash=`) for verifying package integrity during installation. Comments (`# via ...`) provide context on why each dependency is included, often indicating which other package requires it or which requirements file it originates from (e.g., `build/test-requirements.txt`). The file also flags `setuptools` as potentially unsafe in requirements files.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13_ft.txt#2025-04-22_snippet_3\n\nLANGUAGE: pip\nCODE:\n```\n--hash=sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1 \\\n    --hash=sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669\n    # via pytest\npyelftools==0.31 \\\n    --hash=sha256:c774416b10310156879443b81187d182d8d9ee499660380e645918b50bc88f99 \\\n    --hash=sha256:f52de7b3c7e8c64c8abc04a79a1cf37ac5fb0b8a49809827130b858944840607\n    # via auditwheel\npygments==2.19.1 \\\n    --hash=sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f \\\n    --hash=sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c\n    # via rich\npyparsing==3.2.1 \\\n    --hash=sha256:506ff4f4386c4cec0590ec19e6302d3aedb992fdc02c761e90416f158dacf8e1 \\\n    --hash=sha256:61980854fd66de3a90028d679a954d5f2623e83144b5afe5ee86f43d762e5f0a\n    # via matplotlib\npyproject-hooks==1.2.0 \\\n    --hash=sha256:1e859bd5c40fae9448642dd871adf459e5e2084186e8d2c2a79a824c970da1f8 \\\n    --hash=sha256:9e5c6bfa8dcc30091c74b0cf803c81fdd29d94f01992a7707bc97babb1141913\n    # via build\npytest==8.3.4 \\\n    --hash=sha256:50e16d954148559c9a74109af1eaf0c945ba2d8f30f0a3d3335edde19788b6f6 \\\n    --hash=sha256:965370d062bce11e73868e0335abac31b4d3de0e82f4007408d242b4f8610761\n    # via pytest-xdist\npytest-xdist==3.6.1 \\\n    --hash=sha256:9ed4adfb68a016610848639bb7e02c9352d5d9f03d04809919e2dafc3be4cca7 \\\n    --hash=sha256:ead156a4db231eec769737f57668ef58a2084a34b2e55c4a8fa20d861107300d\n    # via -r build/test-requirements.txt\npython-dateutil==2.9.0.post0 \\\n    --hash=sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3 \\\n    --hash=sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427\n    # via matplotlib\nrich==13.9.4 \\\n    --hash=sha256:439594978a49a09530cff7ebc4b5c7103ef57baf48d5ea3184f21d9a2befa098 \\\n    --hash=sha256:6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90\n    # via -r build/test-requirements.txt\nscipy==1.15.2 ; python_version >= \"3.13\" \\\n    --hash=sha256:01edfac9f0798ad6b46d9c4c9ca0e0ad23dbf0b1eb70e96adb9fa7f525eff0bf \\\n    --hash=sha256:03205d57a28e18dfd39f0377d5002725bf1f19a46f444108c29bdb246b6c8a11 \\\n    --hash=sha256:08b57a9336b8e79b305a143c3655cc5bdbe6d5ece3378578888d2afbb51c4e37 \\\n    --hash=sha256:11e7ad32cf184b74380f43d3c0a706f49358b904fa7d5345f16ddf993609184d \\\n    --hash=sha256:28a0d2c2075946346e4408b211240764759e0fabaeb08d871639b5f3b1aca8a0 \\\n    --hash=sha256:2b871df1fe1a3ba85d90e22742b93584f8d2b8e6124f8372ab15c71b73e428b8 \\\n    --hash=sha256:302093e7dfb120e55515936cb55618ee0b895f8bcaf18ff81eca086c17bd80af \\\n    --hash=sha256:42dabaaa798e987c425ed76062794e93a243be8f0f20fff6e7a89f4d61cb3d40 \\\n    --hash=sha256:447ce30cee6a9d5d1379087c9e474628dab3db4a67484be1b7dc3196bfb2fac9 \\\n    --hash=sha256:4c6676490ad76d1c2894d77f976144b41bd1a4052107902238047fb6a473e971 \\\n    --hash=sha256:54c462098484e7466362a9f1672d20888f724911a74c22ae35b61f9c5919183d \\\n    --hash=sha256:597a0c7008b21c035831c39927406c6181bcf8f60a73f36219b69d010aa04737 \\\n    --hash=sha256:5a6fd6eac1ce74a9f77a7fc724080d507c5812d61e72bd5e4c489b042455865e \\\n    --hash=sha256:5ea7ed46d437fc52350b028b1d44e002646e28f3e8ddc714011aaf87330f2f32 \\\n    --hash=sha256:601881dfb761311045b03114c5fe718a12634e5608c3b403737ae463c9885d53 \\\n    --hash=sha256:62ca1ff3eb513e09ed17a5736929429189adf16d2d740f44e53270cc800ecff1 \\\n    --hash=sha256:69ea6e56d00977f355c0f84eba69877b6df084516c602d93a33812aa04d90a3d \\\n    --hash=sha256:6a8e34cf4c188b6dd004654f88586d78f95639e48a25dfae9c5e34a6dc34547e \\\n    --hash=sha256:6d0194c37037707b2afa7a2f2a924cf7bac3dc292d51b6a925e5fcb89bc5c776 \\\n    --hash=sha256:6f223753c6ea76983af380787611ae1291e3ceb23917393079dcc746ba60cfb5 \\\n    --hash=sha256:6f5e296ec63c5da6ba6fa0343ea73fd51b8b3e1a300b0a8cae3ed4b1122c7462 \\\n    --hash=sha256:7cd5b77413e1855351cdde594eca99c1f4a588c2d63711388b6a1f1c01f62274 \\\n    --hash=sha256:869269b767d5ee7ea6991ed7e22b3ca1f22de73ab9a49c44bad338b725603301 \\\n    --hash=sha256:87994da02e73549dfecaed9e09a4f9d58a045a053865679aeb8d6d43747d4df3 \\\n    --hash=sha256:888307125ea0c4466287191e5606a2c910963405ce9671448ff9c81c53f85f58 \\\n    --hash=sha256:92233b2df6938147be6fa8824b8136f29a18f016ecde986666be5f4d686a91a4 \\\n    --hash=sha256:9412f5e408b397ff5641080ed1e798623dbe1ec0d78e72c9eca8992976fa65aa \\\n    --hash=sha256:9b18aa747da280664642997e65aab1dd19d0c3d17068a04b3fe34e2559196cb9 \\\n    --hash=sha256:9de9d1416b3d9e7df9923ab23cd2fe714244af10b763975bea9e4f2e81cebd27 \\\n    --hash=sha256:a2ec871edaa863e8213ea5df811cd600734f6400b4af272e1c011e69401218e9 \\\n    --hash=sha256:a5080a79dfb9b78b768cebf3c9dcbc7b665c5875793569f48bf0e2b1d7f68f6f \\\n    --hash=sha256:a8bf5cb4a25046ac61d38f8d3c3426ec11ebc350246a4642f2f315fe95bda655 \\\n    --hash=sha256:b09ae80010f52efddb15551025f9016c910296cf70adbf03ce2a8704f3a5ad20 \\\n    --hash=sha256:b5e025e903b4f166ea03b109bb241355b9c42c279ea694d8864d033727205e65 \\\n    --hash=sha256:bad78d580270a4d32470563ea86c6590b465cb98f83d760ff5b0990cb5518a93 \\\n    --hash=sha256:bae43364d600fdc3ac327db99659dcb79e6e7ecd279a75fe1266669d9a652828 \\\n    --hash=sha256:c4697a10da8f8765bb7c83e24a470da5797e37041edfd77fd95ba3811a47c4fd \\\n    --hash=sha256:c90ebe8aaa4397eaefa8455a8182b164a6cc1d59ad53f79943f266d99f68687f \\\n    --hash=sha256:cd58a314d92838f7e6f755c8a2167ead4f27e1fd5c1251fd54289569ef3495ec \\\n    --hash=sha256:cf72ff559a53a6a6d77bd8eefd12a17995ffa44ad86c77a5df96f533d4e6c6bb \\\n    --hash=sha256:def751dd08243934c884a3221156d63e15234a3155cf25978b0a668409d45eb6 \\\n    --hash=sha256:e7c68b6a43259ba0aab737237876e5c2c549a031ddb7abc28c7b47f22e202ded \\\n    --hash=sha256:ecf797d2d798cf7c838c6d98321061eb3e72a74710e6c40540f0e8087e3b499e \\\n    --hash=sha256:f031846580d9acccd0044efd1a90e6f4df3a6e12b4b6bd694a7bc03a89892b28 \\\n    --hash=sha256:fb530e4794fc8ea76a4a21ccb67dea33e5e0e60f07fc38a49e821e1eae3b71a0 \\\n    --hash=sha256:fe8a9eb875d430d81755472c5ba75e84acc980e4a8f6204d402849234d3017db\n    # via -r build/requirements.in\nsix==1.17.0 \\\n    --hash=sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274 \\\n    --hash=sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81\n    # via python-dateutil\nsortedcontainers==2.4.0 \\\n    --hash=sha256:25caa5a06cc30b6b83d11423433f65d1f9d76c4c6a0c90e3379eaa43b9bfdb88 \\\n    --hash=sha256:a163dcaede0f1c021485e957a39245190e74249897e2ae4b2aa38595db237ee0\n    # via hypothesis\ntyping-extensions==4.12.2 \\\n    --hash=sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d \\\n    --hash=sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8\n    # via etils\nwheel==0.45.1 \\\n    --hash=sha256:661e1abd9198507b1409a20c02106d9670b2576e916d58f520316666abca6729 \\\n    --hash=sha256:708e7481cc80179af0e556bbf0cc00b8444c7321e2700b8d8580231d13017248\n    # via -r build/test-requirements.txt\nzipp==3.21.0 \\\n    --hash=sha256:2c9958f6430a2040341a52eb608ed6dd93ef4392e02ffe219417c1b28b5dd1f4 \\\n    --hash=sha256:ac1bbe05fd2991f160ebce24ffbac5f6d11d83dc90891255885223d42b3cd931\n    # via etils\n\n# The following packages are considered to be unsafe in a requirements file:\nsetuptools==70.3.0 \\\n    --hash=sha256:f171bab1dfbc86b132997f26a119f6056a57950d058587841a0082e8830f9dc5 \\\n    --hash=sha256:fe384da74336c398e0d956d1cae0669bc02eed936cdb1d49b57de1990dc11ffc\n    # via\n    #   -r build/requirements.in\n    #   -r build/test-requirements.txt\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating JAX Symbolic Reasoning Limitations Leading to Error\nDESCRIPTION: Shows a scenario where JAX's symbolic comparison mechanism fails to prove a mathematically true inequality ('b >= mod(b, 3)') for positive integers. This limitation results in an `InconclusiveDimensionOperation` error during the export of a function that uses this comparison implicitly (in `lax.slice_in_dim`).\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import lax\n>>> b, = export.symbolic_shape(\"b\")\n>>> f = lambda x: lax.slice_in_dim(x, 0, x.shape[0] % 3)\n>>> export.export(jax.jit(f))(\n...     jax.ShapeDtypeStruct((b,), dtype=np.int32))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\njax._src.export.shape_poly.InconclusiveDimensionOperation: Symbolic dimension comparison 'b' >= 'mod(b, 3)' is inconclusive.\nThis error arises for comparison operations with shapes that\nare non-constant, and the result of the operation cannot be represented as\na boolean value for all values of the symbolic dimensions involved.\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating In-Place Updates in NumPy\nDESCRIPTION: This example shows how in-place updates are typically performed in NumPy, which is not possible in the same way with JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnumpy_array = np.zeros((3,3), dtype=np.float32)\nprint(\"original array:\")\nprint(numpy_array)\n\n# In place, mutating update\nnumpy_array[1, :] = 1.0\nprint(\"updated array:\")\nprint(numpy_array)\n```\n\n----------------------------------------\n\nTITLE: Styling HTML Table for Type Promotion Display\nDESCRIPTION: This CSS code defines styles for an HTML table identified by the ID 'types'. It sets borders, padding, background colors (including alternating row colors and specific colors for highlighted cells), and font weight to enhance the readability of the type promotion comparison table.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/type_promotion.rst#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<style>\n    #types table {\n      border: 2px solid #aaa;\n    }\n\n    #types td, #types th {\n      border: 1px solid #ddd;\n      padding: 3px;\n    }\n    #types th {\n      border-bottom: 1px solid #aaa;\n    }\n    #types tr:nth-child(even){background-color: #f2f2f2;}\n    #types .d {\n      background-color: #ccf2cc;\n    }\n    #types td:first-child{\n      background-color: #f2f2f2;\n      border-right: 1px solid #aaa;\n      font-weight: bold;\n    }\n    #types tr:first-child{background-color: #f2f2f2;}\n</style>\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies and Hashes - Requirements File - Text\nDESCRIPTION: Defines exact versions of Python packages to be installed, each with one or more corresponding SHA256 hashes to ensure package integrity and reproducibility. The lines are written in the requirements.txt format, which is standard for pip-based Python environments, and can be used directly with pip install -r requirements.txt --require-hashes. Inline comments indicate transitive dependencies and the source files from which requirements are drawn. Input consists of text lines with package requirements, hashes, and comments, with no code execution or output produced from this static definition.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_12.txt#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:b2787a41404cf7edbe29b07b9e0ed863b09f2665dcc01c1eb0c2261c1e7d0755 \\\n--hash=sha256:bd507fd6f96f65ee02781f2e674e9dc6c99bbfa6e3c39992e3916204c9d431fa\n# via\n#   -r build/nonfreethreading-requirements.txt\n#   -r build/test-requirements.txt\npsutil==5.9.8 \\\n--hash=sha256:02615ed8c5ea222323408ceba16c60e99c3f91639b07da6373fb7e6539abc56d \\\n--hash=sha256:05806de88103b25903dff19bb6692bd2e714ccf9e668d050d144012055cbca73 \\\n--hash=sha256:26bd09967ae00920df88e0352a91cff1a78f8d69b3ecabbfe733610c0af486c8 \\\n--hash=sha256:27cc40c3493bb10de1be4b3f07cae4c010ce715290a5be22b98493509c6299e2 \\\n--hash=sha256:36f435891adb138ed3c9e58c6af3e2e6ca9ac2f365efe1f9cfef2794e6c93b4e \\\n--hash=sha256:50187900d73c1381ba1454cf40308c2bf6f34268518b3f36a9b663ca87e65e36 \\\n--hash=sha256:611052c4bc70432ec770d5d54f64206aa7203a101ec273a0cd82418c86503bb7 \\\n--hash=sha256:6be126e3225486dff286a8fb9a06246a5253f4c7c53b475ea5f5ac934e64194c \\\n--hash=sha256:7d79560ad97af658a0f6adfef8b834b53f64746d45b403f225b85c5c2c140eee \\\n--hash=sha256:8cb6403ce6d8e047495a701dc7c5bd788add903f8986d523e3e20b98b733e421 \\\n--hash=sha256:8db4c1b57507eef143a15a6884ca10f7c73876cdf5d51e713151c1236a0e68cf \\\n--hash=sha256:aee678c8720623dc456fa20659af736241f575d79429a0e5e9cf88ae0605cc81 \\\n--hash=sha256:bc56c2a1b0d15aa3eaa5a60c9f3f8e3e565303b465dbf57a1b730e7a2b9844e0 \\\n--hash=sha256:bd1184ceb3f87651a67b2708d4c3338e9b10c5df903f2e3776b62303b26cb631 \\\n--hash=sha256:d06016f7f8625a1825ba3732081d77c94589dca78b7a3fc072194851e88461a4 \\\n--hash=sha256:d16bbddf0693323b8c6123dd804100241da461e41d6e332fb0ba6058f630f8c8\n# via portpicker\npyelftools==0.31 \\\n--hash=sha256:c774416b10310156879443b81187d182d8d9ee499660380e645918b50bc88f99 \\\n--hash=sha256:f52de7b3c7e8c64c8abc04a79a1cf37ac5fb0b8a49809827130b858944840607\n# via auditwheel\npygments==2.18.0 \\\n--hash=sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199 \\\n--hash=sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\n# via rich\npyparsing==3.1.2 \\\n--hash=sha256:a1bac0ce561155ecc3ed78ca94d3c9378656ad4c94c1270de543f621420f94ad \\\n--hash=sha256:f9db75911801ed778fe61bb643079ff86601aca99fcae6345aa67292038fb742\n# via matplotlib\npyproject-hooks==1.1.0 \\\n--hash=sha256:4b37730834edbd6bd37f26ece6b44802fb1c1ee2ece0e54ddff8bfc06db86965 \\\n--hash=sha256:7ceeefe9aec63a1064c18d939bdc3adf2d8aa1988a510afec15151578b232aa2\n# via build\npytest==8.2.0 \\\n--hash=sha256:1733f0620f6cda4095bbf0d9ff8022486e91892245bb9e7d5542c018f612f233 \\\n--hash=sha256:d507d4482197eac0ba2bae2e9babf0672eb333017bcedaa5fb1a3d42c1174b3f\n# via pytest-xdist\npytest-xdist==3.6.1 \\\n--hash=sha256:9ed4adfb68a016610848639bb7e02c9352d5d9f03d04809919e2dafc3be4cca7 \\\n--hash=sha256:ead156a4db231eec769737f57668ef58a2084a34b2e55c4a8fa20d861107300d\n# via -r build/test-requirements.txt\npython-dateutil==2.9.0.post0 \\\n--hash=sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3 \\\n--hash=sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427\n# via matplotlib\nrich==13.7.1 \\\n--hash=sha256:4edbae314f59eb482f54e9e30bf00d33350aaa94f4bfcd4e9e3110e64d0d7222 \\\n--hash=sha256:9be308cb1fe2f1f57d67ce99e95af38a1e2bc71ad9813b0e247cf7ffbcc3a432\n# via -r build/test-requirements.txt\nscipy==1.13.1 ; python_version <= \"3.12\" \\\n--hash=sha256:017367484ce5498445aade74b1d5ab377acdc65e27095155e448c88497755a5d \\\n--hash=sha256:095a87a0312b08dfd6a6155cbbd310a8c51800fc931b8c0b84003014b874ed3c \\\n--hash=sha256:20335853b85e9a49ff7572ab453794298bcf0354d8068c5f6775a0eabf350aca \\\n--hash=sha256:27e52b09c0d3a1d5b63e1105f24177e544a222b43611aaf5bc44d4a0979e32f9 \\\n--hash=sha256:2831f0dc9c5ea9edd6e51e6e769b655f08ec6db6e2e10f86ef39bd32eb11da54 \\\n--hash=sha256:2ac65fb503dad64218c228e2dc2d0a0193f7904747db43014645ae139c8fad16 \\\n--hash=sha256:392e4ec766654852c25ebad4f64e4e584cf19820b980bc04960bca0b0cd6eaa2 \\\n--hash=sha256:436bbb42a94a8aeef855d755ce5a465479c721e9d684de76bf61a62e7c2b81d5 \\\n--hash=sha256:45484bee6d65633752c490404513b9ef02475b4284c4cfab0ef946def50b3f59 \\\n--hash=sha256:54f430b00f0133e2224c3ba42b805bfd0086fe488835effa33fa291561932326 \\\n--hash=sha256:5713f62f781eebd8d597eb3f88b8bf9274e79eeabf63afb4a737abc6c84ad37b \\\n--hash=sha256:5d72782f39716b2b3509cd7c33cdc08c96f2f4d2b06d51e52fb45a19ca0c86a1 \\\n--hash=sha256:637e98dcf185ba7f8e663e122ebf908c4702420477ae52a04f9908707456ba4d \\\n--hash=sha256:8335549ebbca860c52bf3d02f80784e91a004b71b059e3eea9678ba994796a24 \\\n--hash=sha256:949ae67db5fa78a86e8fa644b9a6b07252f449dcf74247108c50e1d20d2b4627 \\\n--hash=sha256:a014c2b3697bde71724244f63de2476925596c24285c7a637364761f8710891c \\\n--hash=sha256:a78b4b3345f1b6f68a763c6e25c0c9a23a9fd0f39f5f3d200efe8feda560a5fa \\\n--hash=sha256:cdd7dacfb95fea358916410ec61bbc20440f7860333aee6d882bb8046264e949 \\\n--hash=sha256:cfa31f1def5c819b19ecc3a8b52d28ffdcc7ed52bb20c9a7589669dd3c250989 \\\n--hash=sha256:d533654b7d221a6a97304ab63c41c96473ff04459e404b83275b60aa8f4b7004 \\\n--hash=sha256:d605e9c23906d1994f55ace80e0125c587f96c020037ea6aa98d01b4bd2e222f \\\n--hash=sha256:de3ade0e53bc1f21358aa74ff4830235d716211d7d077e340c7349bc3542e884 \\\n--hash=sha256:e89369d27f9e7b0884ae559a3a956e77c02114cc60a6058b4e5011572eea9299 \\\n--hash=sha256:eccfa1906eacc02de42d70ef4aecea45415f5be17e72b61bafcfd329bdc52e94 \\\n--hash=sha256:f26264b282b9da0952a024ae34710c2aff7d27480ee91a2e82b7b7073c24722f\n# via -r build/requirements.in\nsix==1.16.0 \\\n--hash=sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926 \\\n--hash=sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\n# via python-dateutil\nsortedcontainers==2.4.0 \\\n--hash=sha256:25caa5a06cc30b6b83d11423433f65d1f9d76c4c6a0c90e3379eaa43b9bfdb88 \\\n--hash=sha256:a163dcaede0f1c021485e957a39245190e74249897e2ae4b2aa38595db237ee0\n# via hypothesis\ntensorstore==0.1.73 \\\n--hash=sha256:03cec5141a27d2e65e4ff604641cfb1f7989d66c361534392e810b80cbda617d \\\n--hash=sha256:0429bf781ce3ed45be761b46f4bc5979412dadf063f509cb7e9581981a1e097b \\\n--hash=sha256:05f7fdcb063f08f40f74c49f92c0f0136c5b715d49e111950bf025b12a72a907 \\\n--hash=sha256:0eb83a2526e211a721842c3e98293e4bc9e1fdb9dac37ecf37d6ccbde84b8ee3 \\\n--hash=sha256:192feb8a8fd0f37fa298588d037d4889d2f9d07b18b3295488f05ee268f57b70 \\\n--hash=sha256:2aed43498b00d37df583da9e06328751cfe695bb166043aa9ef7183174cf7e29 \\\n--hash=sha256:421a3f87864a0a8837b4f9f0c8ee86079b46b112de902496d3b90c72f51d02ea \\\n--hash=sha256:440569458b91974e0ffa210654a01f2721758476c48240f7c925fc0d107056be \\\n--hash=sha256:4433dcfcb943e100b90b0fc8e0b1d174e8c2c1cedb1fcc86e6d20b6a2e961831 \\\n--hash=sha256:44d70dd0c000db8c0d2386e788c5e91d3b37ebee8f629f3848d7a012c85d1e11 \\\n--hash=sha256:5fc9feab09de9e99c381145adeef5ff9e01f898e509b851ff2edd940c8b2384a \\\n--hash=sha256:70d57b63706de4a3a9c1c217b338658fa160b2d41f5b399e6926f9eaf29b2a4d \\\n--hash=sha256:7a812e8297a4ed70109057628b767c1a12b535f2db657635f0ed1517b23b990b \\\n--hash=sha256:7b4e08bfa61880863bedb90499a23c63d9493cf9310207c230086b0a3700c75d \\\n--hash=sha256:83c6ca5cb39ffeeb4a562942e3b9e2f32b026f362b2b7266c44201bd7c3116a5 \\\n--hash=sha256:87fb7879af73a5b7ded9c9de3e2014baf6468d9d7c47edfc19490907b346e0a6 \\\n--hash=sha256:a11d2e496d7442c68b35cd222a8c8df3fdee9e30fb2984c91546d81faff8bf61 \\\n--hash=sha256:be3f5ef6f359486ee52785e8a302819152e51286c50181c6c35f316b7568ce60 \\\n--hash=sha256:dd7fa6d7e9579a1a75e6185d7df10e28fcc7db2e14190ed60261a71b9c09e1df \\\n--hash=sha256:e99ae99ac48f41c4e36b1e3717c6dbdab96dd27fc91618dd01afb9ad848a9293 \\\n--hash=sha256:f24b325385fd30be612ab8494a29d3bfef37b9444357912ba184f30f325f093b\n# via -r build/nonfreethreading-requirements.txt\ntyping-extensions==4.12.0rc1 \\\n--hash=sha256:be199d06d8f09ca2c9425e3aa04a9afba33e892fe079dea959e72df7f8442343 \\\n--hash=sha256:f933a7b288a919ca97adbff656e52ff81f7ff25d98a2aabb9355ca4090f772fe\n# via etils\nwheel==0.43.0 \\\n--hash=sha256:465ef92c69fa5c5da2d1cf8ac40559a8c940886afcef87dcf14b9470862f1d85 \\\n--hash=sha256:55c570405f142630c6b9f72fe09d9b67cf1477fcf543ae5b8dcb1f5b7377da81\n# via -r build/test-requirements.txt\nzipp==3.18.2 \\\n--hash=sha256:6278d9ddbcfb1f1089a88fde84481528b07b0e10474e09dcfe53dad4069fa059 \\\n--hash=sha256:dce197b859eb796242b0622af1b8beb0a722d52aa2f57133ead08edd5bf5374e\n# via etils\nzstandard==0.22.0 \\\n--hash=sha256:11f0d1aab9516a497137b41e3d3ed4bbf7b2ee2abc79e5c8b010ad286d7464bd \\\n--hash=sha256:1958100b8a1cc3f27fa21071a55cb2ed32e9e5df4c3c6e661c193437f171cba2 \\\n--hash=sha256:1a90ba9a4c9c884bb876a14be2b1d216609385efb180393df40e5172e7ecf356 \\\n--hash=sha256:1d43501f5f31e22baf822720d82b5547f8a08f5386a883b32584a185675c8fbf \\\n--hash=sha256:23d2b3c2b8e7e5a6cb7922f7c27d73a9a615f0a5ab5d0e03dd533c477de23004 \\\n--hash=sha256:2612e9bb4977381184bb2463150336d0f7e014d6bb5d4a370f9a372d21916f69 \\\n--hash=sha256:275df437ab03f8c033b8a2c181e51716c32d831082d93ce48002a5227ec93019 \\\n--hash=sha256:2ac9957bc6d2403c4772c890916bf181b2653640da98f32e04b96e4d6fb3252a \\\n--hash=sha256:2b11ea433db22e720758cba584c9d661077121fcf60ab43351950ded20283440 \\\n--hash=sha256:2fdd53b806786bd6112d97c1f1e7841e5e4daa06810ab4b284026a1a0e484c0b \\\n--hash=sha256:33591d59f4956c9812f8063eff2e2c0065bc02050837f152574069f5f9f17775 \\\n--hash=sha256:36a47636c3de227cd765e25a21dc5dace00539b82ddd99ee36abae38178eff9e \\\n--hash=sha256:39b2853efc9403927f9065cc48c9980649462acbdf81cd4f0cb773af2fd734bc \\\n--hash=sha256:3db41c5e49ef73641d5111554e1d1d3af106410a6c1fb52cf68912ba7a343a0d \\\n--hash=sha256:445b47bc32de69d990ad0f34da0e20f535914623d1e506e74d6bc5c9dc40bb09 \\\n--hash=sha256:466e6ad8caefb589ed281c076deb6f0cd330e8bc13c5035854ffb9c2014b118c \\\n--hash=sha256:48f260e4c7294ef275744210a4010f116048e0c95857befb7462e033f09442fe \\\n\n```\n\n----------------------------------------\n\nTITLE: Specifying NVIDIA cuDNN Dependency for Linux\nDESCRIPTION: Defines the NVIDIA cuDNN package version and hash for Linux platforms. This is a GPU-specific requirement for deep learning operations in JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_10.txt#2025-04-22_snippet_5\n\nLANGUAGE: Text\nCODE:\n```\nnvidia-cudnn-cu12==9.8.0.87 ; sys_platform == \"linux\" \\\n    --hash=sha256:b4b5cfddc32aa4180f9d390ee99e9a9f55a89e7087329b41aba4319327e22466 \\\n    --hash=sha256:b883faeb2f6f15dba7bbb6756eab6a0d9cecb59db5b0fa07577b9cfa24cd99f4 \\\n    --hash=sha256:d6b02cd0e3e24aa31d0193a8c39fec239354360d7d81055edddb69f35d53a4c8\n```\n\n----------------------------------------\n\nTITLE: Importing and Documenting a Python Module with Sphinx Automodule\nDESCRIPTION: This reStructuredText directive tells the Sphinx documentation generator to automatically pull documentation from the specified Python module (`jax.experimental.key_reuse`). It processes the module's docstrings to create the documentation page content.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax.experimental.key_reuse.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: jax.experimental.key_reuse\n```\n\n----------------------------------------\n\nTITLE: Migrating jax.xla_computation to AOT APIs - Python\nDESCRIPTION: This code demonstrates how to migrate from the deprecated 'jax.xla_computation(fn)(*args, **kwargs)' to the recommended AOT APIs using JAX's just-in-time compilation. Replace calls to 'jax.xla_computation(fn)(*args, **kwargs)' with 'jax.jit(fn).lower(*args, **kwargs).compiler_ir('hlo')'. Dependencies: JAX >= 0.4.30. Parameters include 'fn' (the function to be compiled) and its respective arguments. Returns the HLO compiler IR corresponding to the lowered function. The primary constraint is that this pattern should be followed after the removal of 'jax.xla_computation.'\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\njax.jit(fn).lower(*args, **kwargs).compiler_ir('hlo')\n```\n\n----------------------------------------\n\nTITLE: Detecting Non-finite Values with JAX Breakpoints and Conditional Execution in Python\nDESCRIPTION: This snippet demonstrates how to create a conditional breakpoint that only triggers when non-finite values (NaN or infinity) are detected in JAX computations. It combines jax.lax.cond with jax.debug.breakpoint to pause execution when division by zero occurs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/debugging/print_breakpoint.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef breakpoint_if_nonfinite(x):\n  is_finite = jnp.isfinite(x).all()\n  def true_fn(x):\n    pass\n  def false_fn(x):\n    jax.debug.breakpoint()\n  lax.cond(is_finite, true_fn, false_fn, x)\n\n@jax.jit\ndef f(x, y):\n  z = x / y\n  breakpoint_if_nonfinite(z)\n  return z\nf(2., 0.) # ==> Pauses during execution!\n```\n\n----------------------------------------\n\nTITLE: Defining JAX Project Dependencies\nDESCRIPTION: Specifies required Python packages and version constraints for documentation building, CI testing, and notebook execution in the JAX project. Includes core dependencies like Sphinx for documentation, testing frameworks like pytest, and scientific computing libraries.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py\nipython>=8.8.0  # 8.7.0 has ipython3 lexer error\npydata-sphinx-theme==0.14.4  # v0.15 breaks sidebar toggling\nsphinx>=7.3.2,<8.0  # 7.3.0 breaks sphinx-book-theme; 8.0 breaks myst-nb 1.1\nsphinx-book-theme==1.1.1  # v1.1.2 requires pydata-sphinx-theme v0.15\nsphinx-copybutton>=0.5.0\nsphinx-remove-toctrees\nsphinx-design\nsphinxext-rediraffe\nmyst-nb>=1.0.0\n\n# Packages used for CI tests.\nflatbuffers\npytest\npytest-xdist\n\n# Packages used for notebook execution\nmatplotlib\nscikit-learn\npooch\nnumpy\nrich[jupyter]\ncmake\n.[ci]  # Install jax from the current directory; jaxlib from pypi.\n```\n\n----------------------------------------\n\nTITLE: Python Package Hash Requirements\nDESCRIPTION: SHA256 hash definitions for Python package dependencies, including setuptools v76.0.0 and various other packages referenced in nonfreethreading-requirements.txt and test-requirements.txt.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13.txt#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:a0817825b900fcd43ac5d05b8b3079937073d2b1ff9cf89427590718b70dd840 \\\n--hash=sha256:a4ae99c57668ca1e78597d8b06d5af837f377f340f4cce993b551b2d7731778d \\\n--hash=sha256:a8c86881813a78a6f4508ef9daf9d4995b8ac2d147dcb1a450448941398091c9 \\\n--hash=sha256:a8fffdbd9d1408006baaf02f1068d7dd1f016c6bcb7538682622c556e7b68e35 \\\n--hash=sha256:a9b07268d0c3ca5c170a385a0ab9fb7fdd9f5fd866be004c4ea39e44edce47dd \\\n--hash=sha256:ab19a2d91963ed9e42b4e8d77cd847ae8381576585bad79dbd0a8837a9f6620a \\\n--hash=sha256:ac184f87ff521f4840e6ea0b10c0ec90c6b1dcd0bad2f1e4a9a1b4fa177982ea \\\n--hash=sha256:b0e166f698c5a3e914947388c162be2583e0c638a4703fc6a543e23a88dea3c1 \\\n--hash=sha256:b2170c7e0367dde86a2647ed5b6f57394ea7f53545746104c6b09fc1f4223573 \\\n--hash=sha256:b2d8c62d08e7255f68f7a740bae85b3c9b8e5466baa9cbf7f57f1cde0ac6bc09 \\\n--hash=sha256:b4567955a6bc1b20e9c31612e615af6b53733491aeaa19a6b3b37f3b65477094 \\\n--hash=sha256:b69bb4f51daf461b15e7b3db033160937d3ff88303a7bc808c67bbc1eaf98c78 \\\n--hash=sha256:b8c0bd73aeac689beacd4e7667d48c299f61b959475cdbb91e7d3d88d27c56b9 \\\n--hash=sha256:be9b5b8659dff1f913039c2feee1aca499cfbc19e98fa12bc85e037c17ec6ca5\n```\n\n----------------------------------------\n\nTITLE: Defining Package Dependencies with Hash Values for JAX Project\nDESCRIPTION: This snippet lists package dependencies with their corresponding SHA256 hash values for version verification. Each line represents a package version with multiple hash options for integrity checking.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_12.txt#2025-04-22_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\n    --hash=sha256:4ac59d5d6910b220141c1737b79d4a5aa9e57466e7469a012ed42ce2d3995e88 \\\n    --hash=sha256:53866a9d8ab363271c9e80c7c2e9441814961d47f88c9bc3b248142c32141d94 \\\n    --hash=sha256:589402548251056878d2e7c8859286eb91bd841af117dbe4ab000e6450987e08 \\\n    --hash=sha256:68953dc84b244b053c0d5f137a21ae8287ecf51b20872eccf8eaac0302d3e3b0 \\\n    --hash=sha256:6c25b8eb733d4e741246151d895dd0308137532737f337411160ff69ca24f93a \\\n    --hash=sha256:7034d381789f45576ec3f1fa0e15d741828146439228dc3f7c59856c5bcd3292 \\\n    --hash=sha256:73a1d6bd01961e9fd447162e137ed949c01bdb830dfca487c4a14e9742dccc93 \\\n    --hash=sha256:8226a33c542bcb54cd6bd0a366067b610b41713b64c9abec1bc4533d69f51e70 \\\n    --hash=sha256:888196c9c8893a1e8ff5e89b8f894e7f4f0e64a5af4d8f3c410f0319128bb2f8 \\\n    --hash=sha256:88c5b4b47a8a138338a07fc94e2ba3b1535f69247670abfe422de4e0b344aae2 \\\n    --hash=sha256:8a1b2effa96a5f019e72874969394edd393e2fbd6414a8208fea363a22803b45 \\\n    --hash=sha256:93e1856c8313bc688d5df069e106a4bc962eef3d13372020cc6e3ebf5e045202 \\\n    --hash=sha256:9501f36fac6b875c124243a379267d879262480bf85b1dbda61f5ad4d01b75a3 \\\n    --hash=sha256:959665072bd60f45c5b6b5d711f15bdefc9849dd5da9fb6c873e35f5d34d8cfb \\\n    --hash=sha256:a1d67d0d53d2a138f9e29d8acdabe11310c185e36f0a848efa104d4e40b808e4 \\\n    --hash=sha256:a493d470183ee620a3df1e6e55b3e4de8143c0ba1b16f3ded83208ea8ddfd91d \\\n    --hash=sha256:a7ccf5825fd71d4542c8ab28d4d482aace885f5ebe4b40faaa290eed8e095a4c \\\n    --hash=sha256:a88b7df61a292603e7cd662d92565d915796b094ffb3d206579aaebac6b85d5f \\\n    --hash=sha256:a97079b955b00b732c6f280d5023e0eefe359045e8b83b08cf0333af9ec78f26 \\\n    --hash=sha256:d22fdef58976457c65e2796e6730a3ea4a254f3ba83777ecfc8592ff8d77d303 \\\n    --hash=sha256:d75f693bb4e92c335e0645e8845e553cd09dc91616412d1d4650da835b5449df \\\n    --hash=sha256:d8593f8464fb64d58e8cb0b905b272d40184eac9a18d83cf8c10749c3eafcd7e \\\n    --hash=sha256:d8fff0f0c1d8bc5d866762ae95bd99d53282337af1be9dc0d88506b340e74b73 \\\n    --hash=sha256:de20a212ef3d00d609d0b22eb7cc798d5a69035e81839f549b538eff4105d01c \\\n    --hash=sha256:e9e9d4e2e336c529d4c435baad846a181e39a982f823f7e4495ec0b0ec8538d2 \\\n    --hash=sha256:f058a77ef0ece4e210bb0450e68408d4223f728b109764676e1a13537d056bb0 \\\n    --hash=sha256:f1a4b358947a65b94e2501ce3e078bbc929b039ede4679ddb0460829b12f7375 \\\n    --hash=sha256:f9b2cde1cd1b2a10246dbc143ba49d942d14fb3d2b4bccf4618d475c65464912 \\\n    --hash=sha256:fe3390c538f12437b859d815040763abc728955a52ca6ff9c5d4ac707c4ad98e\n    # via -r build/nonfreethreading-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dimension Variable Assumption Issues in Shape Polymorphism\nDESCRIPTION: Example showing how jax2tf makes assumptions about dimension variables being strictly positive, which can lead to incorrect behavior when converting functions that handle empty arrays differently.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef f_jax(x):\n  return 0 if x.shape[0] == 0 else 1\n\nx0 = np.array([], np.float32)\nself.assertEqual(0, f_jax(x0))  # JAX sees that the x.shape[0] == 0\n\n# jax2tf catches the broken assumption b >= 1 if the lowered function is executed\n# eagerly.\n# Raises: ValueError: Dimension variable b must have integer value >= 1. Found value 0 when solving b == 0\njax2tf.convert(f_jax, polymorphic_shapes=[\"b\"])(x0)\n\n# However, if we first trace to a TensorFlow graph, we may miss the broken assumption:\nf_tf = tf.function(\n        jax2tf.convert(f_jax, polymorphic_shapes=[\"b\"]), autograph=False\n       ).get_concrete_function(tf.TensorSpec([None], dtype=np.float32))\nself.assertEqual(1, f_tf(x0))\n```\n\n----------------------------------------\n\nTITLE: For-Loop Implementation with lax.fori_loop in JAX (Python)\nDESCRIPTION: This snippet illustrates the use of jax.lax.fori_loop to implement a loop with a fixed number of iterations, in Python, suitable for JAX transformations. It requires JAX and jnp, and optionally NumPy for inputs. The loop body receives an index and carry, updating carry each iteration based on array-shape broadcasting. Inputs are an array and an integer number of steps; output is the final carry. Using jnp.ones as a constant demonstrates closure capture inside the loop.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jaxpr.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef func10(arg, n):\n  ones = jnp.ones(arg.shape)  # A constant.\n  return lax.fori_loop(0, n,\n                       lambda i, carry: carry + ones * 3. + arg,\n                       arg + ones)\n\nprint(make_jaxpr(func10)(np.ones(16), 5))\n```\n\n----------------------------------------\n\nTITLE: Specifying Pillow Image Processing Library Dependency\nDESCRIPTION: Defines the Pillow package version and multiple hash values. Pillow is used for image processing tasks in the JAX ecosystem.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_10.txt#2025-04-22_snippet_6\n\nLANGUAGE: Text\nCODE:\n```\npillow==11.0.0 \\\n    --hash=sha256:00177a63030d612148e659b55ba99527803288cea7c75fb05766ab7981a8c1b7 \\\n    --hash=sha256:006bcdd307cc47ba43e924099a038cbf9591062e6c50e570819743f5607404f5 \\\n    --hash=sha256:084a07ef0821cfe4858fe86652fffac8e187b6ae677e9906e192aafcc1b69903 \\\n    --hash=sha256:0ae08bd8ffc41aebf578c2af2f9d8749d91f448b3bfd41d7d9ff573d74f2a6b2 \\\n    --hash=sha256:0e038b0745997c7dcaae350d35859c9715c71e92ffb7e0f4a8e8a16732150f38 \\\n    --hash=sha256:1187739620f2b365de756ce086fdb3604573337cc28a0d3ac4a01ab6b2d2a6d2 \\\n    --hash=sha256:16095692a253047fe3ec028e951fa4221a1f3ed3d80c397e83541a3037ff67c9 \\\n    --hash=sha256:1a61b54f87ab5786b8479f81c4b11f4d61702830354520837f8cc791ebba0f5f \\\n    --hash=sha256:1c1d72714f429a521d8d2d018badc42414c3077eb187a59579f28e4270b4b0fc \\\n    --hash=sha256:1e2688958a840c822279fda0086fec1fdab2f95bf2b717b66871c4ad9859d7e8 \\\n    --hash=sha256:20ec184af98a121fb2da42642dea8a29ec80fc3efbaefb86d8fdd2606619045d \\\n    --hash=sha256:21a0d3b115009ebb8ac3d2ebec5c2982cc693da935f4ab7bb5c8ebe2f47d36f2 \\\n    --hash=sha256:224aaa38177597bb179f3ec87eeefcce8e4f85e608025e9cfac60de237ba6316 \\\n    --hash=sha256:2679d2258b7f1192b378e2893a8a0a0ca472234d4c2c0e6bdd3380e8dfa21b6a \\\n    --hash=sha256:27a7860107500d813fcd203b4ea19b04babe79448268403172782754870dac25 \\\n    --hash=sha256:290f2cc809f9da7d6d622550bbf4c1e57518212da51b6a30fe8e0a270a5b78bd \\\n    --hash=sha256:2e46773dc9f35a1dd28bd6981332fd7f27bec001a918a72a79b4133cf5291dba \\\n    --hash=sha256:3107c66e43bda25359d5ef446f59c497de2b5ed4c7fdba0894f8d6cf3822dafc \\\n    --hash=sha256:375b8dd15a1f5d2feafff536d47e22f69625c1aa92f12b339ec0b2ca40263273 \\\n    --hash=sha256:45c566eb10b8967d71bf1ab8e4a525e5a93519e29ea071459ce517f6b903d7fa \\\n    --hash=sha256:499c3a1b0d6fc8213519e193796eb1a86a1be4b1877d678b30f83fd979811d1a \\\n    --hash=sha256:4ad70c4214f67d7466bea6a08061eba35c01b1b89eaa098040a35272a8efb22b \\\n    --hash=sha256:4b60c9520f7207aaf2e1d94de026682fc227806c6e1f55bba7606d1c94dd623a \\\n    --hash=sha256:5178952973e588b3f1360868847334e9e3bf49d19e169bbbdfaf8398002419ae \\\n    --hash=sha256:52a2d8323a465f84faaba5236567d212c3668f2ab53e1c74c15583cf507a0291 \\\n    --hash=sha256:598b4e238f13276e0008299bd2482003f48158e2b11826862b1eb2ad7c768b97 \\\n    --hash=sha256:5bd2d3bdb846d757055910f0a59792d33b555800813c3b39ada1829c372ccb06 \\\n    --hash=sha256:5c39ed17edea3bc69c743a8dd3e9853b7509625c2462532e62baa0732163a904 \\\n    --hash=sha256:5d203af30149ae339ad1b4f710d9844ed8796e97fda23ffbc4cc472968a47d0b \\\n    --hash=sha256:5ddbfd761ee00c12ee1be86c9c0683ecf5bb14c9772ddbd782085779a63dd55b \\\n    --hash=sha256:607bbe123c74e272e381a8d1957083a9463401f7bd01287f50521ecb05a313f8 \\\n    --hash=sha256:61b887f9ddba63ddf62fd02a3ba7add935d053b6dd7d58998c630e6dbade8527 \\\n    --hash=sha256:6619654954dc4936fcff82db8eb6401d3159ec6be81e33c6000dfd76ae189947 \\\n    --hash=sha256:674629ff60030d144b7bca2b8330225a9b11c482ed408813924619c6f302fdbb \\\n    --hash=sha256:6ec0d5af64f2e3d64a165f490d96368bb5dea8b8f9ad04487f9ab60dc4bb6003 \\\n    --hash=sha256:6f4dba50cfa56f910241eb7f883c20f1e7b1d8f7d91c750cd0b318bad443f4d5 \\\n    --hash=sha256:70fbbdacd1d271b77b7721fe3cdd2d537bbbd75d29e6300c672ec6bb38d9672f \\\n    --hash=sha256:72bacbaf24ac003fea9bff9837d1eedb6088758d41e100c1552930151f677739 \\\n    --hash=sha256:7326a1787e3c7b0429659e0a944725e1b03eeaa10edd945a86dead1913383944 \\\n    --hash=sha256:73853108f56df97baf2bb8b522f3578221e56f646ba345a372c78326710d3830 \\\n    --hash=sha256:73e3a0200cdda995c7e43dd47436c1548f87a30bb27fb871f352a22ab8dcf45f \\\n    --hash=sha256:75acbbeb05b86bc53cbe7b7e6fe00fbcf82ad7c684b3ad82e3d711da9ba287d3 \\\n    --hash=sha256:8069c5179902dcdce0be9bfc8235347fdbac249d23bd90514b7a47a72d9fecf4 \\\n    --hash=sha256:846e193e103b41e984ac921b335df59195356ce3f71dcfd155aa79c603873b84 \\\n    --hash=sha256:8594f42df584e5b4bb9281799698403f7af489fba84c34d53d1c4bfb71b7c4e7 \\\n    --hash=sha256:86510e3f5eca0ab87429dd77fafc04693195eec7fd6a137c389c3eeb4cfb77c6 \\\n    --hash=sha256:8853a3bf12afddfdf15f57c4b02d7ded92c7a75a5d7331d19f4f9572a89c17e6 \\\n    --hash=sha256:88a58d8ac0cc0e7f3a014509f0455248a76629ca9b604eca7dc5927cc593c5e9 \\\n    --hash=sha256:8ba470552b48e5835f1d23ecb936bb7f71d206f9dfeee64245f30c3270b994de \\\n    --hash=sha256:8c676b587da5673d3c75bd67dd2a8cdfeb282ca38a30f37950511766b26858c4 \\\n    --hash=sha256:8ec4a89295cd6cd4d1058a5e6aec6bf51e0eaaf9714774e1bfac7cfc9051db47 \\\n    --hash=sha256:94f3e1780abb45062287b4614a5bc0874519c86a777d4a7ad34978e86428b8dd \\\n    --hash=sha256:9a0f748eaa434a41fccf8e1ee7a3eed68af1b690e75328fd7a60af123c193b50 \\\n    --hash=sha256:a5629742881bcbc1f42e840af185fd4d83a5edeb96475a575f4da50d6ede337c \\\n    --hash=sha256:a65149d8ada1055029fcb665452b2814fe7d7082fcb0c5bed6db851cb69b2086 \\\n    --hash=sha256:b3c5ac4bed7519088103d9450a1107f76308ecf91d6dabc8a33a2fcfb18d0fba \\\n    --hash=sha256:b4fd7bd29610a83a8c9b564d457cf5bd92b4e11e79a4ee4716a63c959699b306 \\\n    --hash=sha256:bcd1fb5bb7b07f64c15618c89efcc2cfa3e95f0e3bcdbaf4642509de1942a699 \\\n    --hash=sha256:c12b5ae868897c7338519c03049a806af85b9b8c237b7d675b8c5e089e4a618e \\\n    --hash=sha256:c26845094b1af3c91852745ae78e3ea47abf3dbcd1cf962f16b9a5fbe3ee8488 \\\n    --hash=sha256:c6a660307ca9d4867caa8d9ca2c2658ab685de83792d1876274991adec7b93fa \\\n    --hash=sha256:c809a70e43c7977c4a42aefd62f0131823ebf7dd73556fa5d5950f5b354087e2 \\\n    --hash=sha256:c8b2351c85d855293a299038e1f89db92a2f35e8d2f783489c6f0b2b5f3fe8a3 \\\n    --hash=sha256:cb929ca942d0ec4fac404cbf520ee6cac37bf35be479b970c4ffadf2b6a1cad9 \\\n    --hash=sha256:d2c0a187a92a1cb5ef2c8ed5412dd8d4334272617f532d4ad4de31e0495bd923 \\\n    --hash=sha256:d69bfd8ec3219ae71bcde1f942b728903cad25fafe3100ba2258b973bd2bc1b2 \\\n    --hash=sha256:daffdf51ee5db69a82dd127eabecce20729e21f7a3680cf7cbb23f0829189790 \\\n    --hash=sha256:e58876c91f97b0952eb766123bfef372792ab3f4e3e1f1a2267834c2ab131734 \\\n    --hash=sha256:eda2616eb2313cbb3eebbe51f19362eb434b18e3bb599466a1ffa76a033fb916 \\\n    --hash=sha256:ee217c198f2e41f184f3869f3e485557296d505b5195c513b2bfe0062dc537f1 \\\n    --hash=sha256:f02541ef64077f22bf4924f225c0fd1248c168f86e4b7abdedd87d6ebaceab0f \\\n    --hash=sha256:f1b82c27e89fffc6da125d5eb0ca6e68017faf5efc078128cfaa42cf5cb38798 \\\n    --hash=sha256:fba162b8872d30fea8c52b258a542c5dfd7b235fb5cb352240c8d63b414013eb \\\n    --hash=sha256:fbbcb7b57dc9c794843e3d1258c0fbf0f48656d46ffe9e09b63bbd6e8cd5d0a2\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Jaxpr using an Interpreter in Python\nDESCRIPTION: Defines the `eval_jaxpr` function, which takes a `Jaxpr` object and input arguments (`args`) and evaluates the represented computation. It maintains an environment (`env`) mapping variable names to their runtime values. It iterates through the equations, evaluates arguments, executes operations using `current_interpreter.interpret_op`, updates the environment, and finally returns the evaluated result.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef eval_jaxpr(jaxpr, args):\n  # An environment mapping variables to values\n  env = dict(zip(jaxpr.parameters, args))\n  def eval_atom(x): return env[x] if isinstance(x, Var) else x\n  for eqn in jaxpr.equations:\n    args = tuple(eval_atom(x) for x in eqn.args)\n    env[eqn.var] = current_interpreter.interpret_op(eqn.op, args)\n  return eval_atom(jaxpr.return_val)\n\nprint(eval_jaxpr(build_jaxpr(foo, 1), (2.0,)))\n```\n\n----------------------------------------\n\nTITLE: Inlining Constant Literals into Jaxpr (Python)\nDESCRIPTION: Implements the _inline_literals function to rewrite a JAXPR by replacing eligible constant binders with literal values, improving compactness and reducing unnecessary constant variables. Utilizes supporting functions (split_list, partition_list, typecheck_jaxpr) and data structures (Jaxpr, ShapedArray, Lit). Key parameters are the Jaxpr to transform and the list of constants. Returns the updated Jaxpr and its new constants, expects input Jaxpr to be typecheckable.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax.md#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef _inline_literals(jaxpr: Jaxpr, consts: list[Any]) -> tuple[Jaxpr, list[Any]]:\n  const_binders, other_binders = split_list(jaxpr.in_binders, len(consts))\n  scalars = [type(x) in jax_types and not get_aval(x).shape for x in consts]\n  new_const_binders, lit_binders = partition_list(scalars, const_binders)\n  new_consts, lit_vals = partition_list(scalars, consts)\n  literals = dict(zip(lit_binders, map(Lit, lit_vals)))\n  new_eqns = [JaxprEqn(eqn.primitive, [literals.get(x, x) for x in eqn.inputs],\n                       eqn.params, eqn.out_binders) for eqn in jaxpr.eqns]\n  new_outs = [literals.get(x, x) for x in jaxpr.outs]\n  new_jaxpr = Jaxpr(new_const_binders + other_binders, new_eqns, new_outs)\n  typecheck_jaxpr(new_jaxpr)\n  return new_jaxpr, new_consts\n```\n\n----------------------------------------\n\nTITLE: Specifying NVIDIA CUDA Runtime Dependency for Linux\nDESCRIPTION: Defines the NVIDIA CUDA Runtime package version and hash for Linux platforms. This is a GPU-specific requirement for JAX.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_10.txt#2025-04-22_snippet_4\n\nLANGUAGE: Text\nCODE:\n```\nnvidia-cuda-runtime-cu12==12.8.57 ; sys_platform == \"linux\" \\\n    --hash=sha256:534ccebd967b6a44292678fa5da4f00666029cb2ed07a79515ea41ef31fe3ec7 \\\n    --hash=sha256:75342e28567340b7428ce79a5d6bb6ca5ff9d07b69e7ce00d2c7b4dc23eff0be \\\n    --hash=sha256:89be637e3ee967323865b85e0f147d75f9a5bd98360befa37481b02dd57af8f5\n```\n\n----------------------------------------\n\nTITLE: Enable Double Precision in JAX\nDESCRIPTION: Example showing how to enable double precision calculations in JAX using configuration settings.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Common_Gotchas_in_JAX.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\njax.config.update(\"jax_enable_x64\", True)\nx = random.uniform(random.key(0), (1000,), dtype=jnp.float64)\nx.dtype # --> dtype('float64')\n```\n\n----------------------------------------\n\nTITLE: Error when Using JVP with Custom VJP\nDESCRIPTION: Demonstrates that forward-mode autodiff cannot be used on a custom_vjp function. The example shows the error raised when attempting to use jvp with a custom VJP function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jvp\n\ntry:\n  jvp(f, (3.,), (1.,))\nexcept TypeError as e:\n  print('ERROR! {}'.format(e))\n```\n\n----------------------------------------\n\nTITLE: Illustrating InconclusiveDimensionOperation in JAX Export\nDESCRIPTION: Demonstrates how comparing symbolic dimensions (e.g., 'a + 1 >= b') during JAX export can lead to an `InconclusiveDimensionOperation` error if the comparison cannot be resolved to a definite boolean value for all possible positive integer values of the symbolic variables. The example attempts to export a JIT-compiled function whose conditional logic depends on such an inconclusive comparison.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport jax\n>>> export.export(jax.jit(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1))(\n...     jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"), dtype=np.int32))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\njax._src.export.shape_poly.InconclusiveDimensionOperation: Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive.\nThis error arises for comparison operations with shapes that\nare non-constant, and the result of the operation cannot be represented as\na boolean value for all values of the symbolic dimensions involved.\n\n```\n\n----------------------------------------\n\nTITLE: Restoring and Using a Saved JAX Model in TensorFlow\nDESCRIPTION: Demonstrates how to load a JAX model saved as a TensorFlow SavedModel and use it for inference. The restored model doesn't require JAX to run, only XLA, and can be used with custom data types.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# Restoring (note: the restored model does *not* require JAX to run, just XLA).\nrestored_model = tf.saved_model.load(model_dir)\ndef restored_f(pair: CustomPair):\n  return restored_model.f(pair.a, pair.b)\n\nres_tf_3 = restored_f(x)\nself.assertAllClose(res_jax, res_tf_3)\ngrad_jax = jax.grad(f_jax)(x)\n\nx_v = [tf.Variable(x.a), tf.Variable(x.b)]\nwith tf.GradientTape() as tape:\n  res = f_tf_wrapped(*x_v)\n  grad_tf = tape.gradient(res, x_v)\n\nself.assertAllClose(grad_jax.a, grad_tf[0])\nself.assertAllClose(grad_jax.b, grad_tf[1])\n```\n\n----------------------------------------\n\nTITLE: JAX to TFLite+Flex Conversion Error - Seq2seq LSTM\nDESCRIPTION: Runtime error showing buffer size mismatch and tensor copying failures when using TFLite with Flex delegate.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nRuntimeError('TF Lite does not support TensorFlow data type: uint32FlexDelegate: Tensor jax2tf_apply_with_vars_/Seq2seq/Decoder_0/scan/while/body/DecoderLSTM_0/random_bits/jit__threefry_random_bits_original_/jit_threefry_2x32_/strided_slice_2(68) buffer size mismatch 8(2) != 4(1)failed to copy data from TF tensorNode number 662 (TfLiteFlexDelegate) failed to invoke.Node number 25 (WHILE) failed to invoke.')\n```\n\n----------------------------------------\n\nTITLE: JIT-compiling a function with a loop\nDESCRIPTION: Demonstrates a successful JIT compilation of a function containing a for loop that doesn't depend on input values.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/control-flow.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@jit\ndef f(x):\n  for i in range(3):\n    x = 2 * x\n  return x\n\nprint(f(3))\n```\n\n----------------------------------------\n\nTITLE: Defining the Jaxpr Data Structure in Python\nDESCRIPTION: Defines the `Jaxpr` dataclass, which represents a JAX expression (intermediate representation). It holds the function's formal parameters (`parameters`), a list of computational steps (`equations`), and the return value (`return_val`). A `__str__` method is provided for human-readable printing.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# We call an IR function a \"Jaxpr\", for \"JAX expression\"\n@dataclass\nclass Jaxpr:\n  parameters : list[Var]      # The function's formal parameters (arguments)\n  equations  : list[Equation] # The body of the function, a list of instructions/equations\n  return_val : Atom           # The function's return value\n\n  def __str__(self):\n    lines = []\n    lines.append(', '.join(b for b in self.parameters) + ' ->')\n    for eqn in self.equations:\n      args_str = ', '.join(str(arg) for arg in eqn.args)\n      lines.append(f'  {eqn.var} = {eqn.op}({args_str})')\n    lines.append(self.return_val)\n    return '\\n'.join(lines)\n```\n\n----------------------------------------\n\nTITLE: Jaxpr Evaluation Implementation\nDESCRIPTION: Implementation of a basic Jaxpr interpreter that evaluates Jaxpr equations in order, maintaining variable bindings in an environment dictionary.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef eval_jaxpr(jaxpr, consts, *args):\n  # Mapping from variable -> value\n  env = {}\n\n  def read(var):\n    # Literals are values baked into the Jaxpr\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # Bind args and consts to environment\n  safe_map(write, jaxpr.invars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Loop through equations and evaluate primitives using `bind`\n  for eqn in jaxpr.eqns:\n    # Read inputs to equation from environment\n    invals = safe_map(read, eqn.invars)\n    # `bind` is how a primitive is called\n    outvals = eqn.primitive.bind(*invals, **eqn.params)\n    # Primitives may return multiple outputs or not\n    if not eqn.primitive.multiple_results:\n      outvals = [outvals]\n    # Write the results of the primitive into the environment\n    safe_map(write, eqn.outvars, outvals)\n  # Read the final result of the Jaxpr from the environment\n  return safe_map(read, jaxpr.outvars)\n\nclosed_jaxpr = jax.make_jaxpr(f)(jnp.ones(5))\neval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, jnp.ones(5))\n```\n\n----------------------------------------\n\nTITLE: Building Complex JAX Transformations with Forward and Reverse-mode Differentiation\nDESCRIPTION: Implements forward-mode (jacfwd) and reverse-mode (jacrev) Jacobian computation and combines them with other JAX transformations to create a Hessian function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import jvp, vjp  # forward and reverse-mode\n\ncurry = lambda f: partial(partial, f)\n\n@curry\ndef jacfwd(fun, x):\n  pushfwd = partial(jvp, fun, (x,))  # jvp!\n  std_basis = jnp.eye(np.size(x)).reshape((-1,) + jnp.shape(x)),\n  y, jac_flat = vmap(pushfwd, out_axes=(None, -1))(std_basis)  # vmap!\n  return jac_flat.reshape(jnp.shape(y) + jnp.shape(x))\n\n@curry\ndef jacrev(fun, x):\n  y, pullback = vjp(fun, x)  # vjp!\n  std_basis = jnp.eye(np.size(y)).reshape((-1,) + jnp.shape(y))\n  jac_flat, = vmap(pullback)(std_basis)  # vmap!\n  return jac_flat.reshape(jnp.shape(y) + jnp.shape(x))\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))  # jit!\n```\n\n----------------------------------------\n\nTITLE: Cross-backend Lowering with Specified Platforms - Python\nDESCRIPTION: This snippet provides a migration path for cases where previously 'jax.xla_computation' was used for cross-backend lowering, e.g., by specifying a backend (such as 'tpu'). Now, you use 'jax.jit(fn).trace(*args, **kwargs).lower(lowering_platforms=(\"tpu\",)).compiler_ir(\"hlo\")'. This requires JAX >= 0.4.30 and an understanding of the corresponding function and backend arguments. The output is the HLO compiler IR for the specified backend.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\njax.jit(fn).trace(*args, **kwargs).lower(lowering_platforms=(\"tpu\",)).compiler_ir(\"hlo\")\n```\n\n----------------------------------------\n\nTITLE: Displaying JAX Converter Evaluation Results Table in Markdown\nDESCRIPTION: A markdown table summarizing the evaluation results for different JAX converters on various Flax model examples. It shows which converters succeeded (YES) or failed (NO) for each example.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/g3doc/convert_models_results.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Example | jax2tf_xla | jax2tf_noxla | jax2tfjs | jax2tflite | jax2tflite+flex |\n| --- | --- | --- | --- | --- | --- |\n| `flax/actor_critic` | YES | YES | YES | YES | YES |\n| `flax/actor_critic_[(b, ...)]` | YES | YES | YES | YES | YES |\n| `flax/actor_critic_[(_, 4*b, 4*b, _)]` | NO | NO | NO | NO | NO |\n| `flax/bilstm` | YES | YES | YES | NO | NO |\n| `flax/bilstm_[(b, _), (_,)]` | NO | NO | NO | NO | NO |\n| `flax/bilstm_[(_, _), (b,)]` | NO | NO | NO | NO | NO |\n```\n\n----------------------------------------\n\nTITLE: Revisiting Potential Copy Issue with ppermute\nDESCRIPTION: This snippet revisits the original `f(x)` function using `ppermute_start` and `ppermute_done`. It sets the stage for explaining how XLA's defensive copy mechanism might apply here, even with the lifetime fix, due to potential aliasing perceived by XLA.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/design/async_note.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n```py\ndef f(x):\n  fut = ppermute_start(x)\n  z = x + 1\n  y = ppermute_done(fut)\n  return y, z\n```\n```\n\n----------------------------------------\n\nTITLE: Using explicit_axes in JAX Python\nDESCRIPTION: Shows how to use the explicit_axes decorator to transition from Auto mode to Explicit mode for specific mesh axes within a function.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/explicit-sharding.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nauto_mesh = jax.make_mesh((2, 4), (\"X\", \"Y\"),\n                           axis_types=(AxisType.Auto, AxisType.Auto))\n\n@functools.partial(explicit_axes, axes=('X', 'Y'))\ndef explicit_g(y):\n  print(f'mesh inside g: {get_abstract_mesh()}')\n  print(f'y.sharding inside g: {jax.typeof(y) = }')\n  z = y * 2\n  print(f'z.sharding inside g: {jax.typeof(z) = }', end='\\n\\n')\n  return z\n\n@jax.jit\ndef f(arr1):\n  print(f'mesh inside f: {get_abstract_mesh()}', end='\\n\\n')\n  x = jnp.sin(arr1)\n\n  z = explicit_g(x, in_shardings=P(\"X\", \"Y\"))\n\n  return z + 1\n\nwith jax.sharding.use_mesh(auto_mesh):\n  some_x = jax.device_put(np.arange(16).reshape(4, 4), P(\"X\", \"Y\"))\n  f(some_x)\n```\n\n----------------------------------------\n\nTITLE: Replacing Deprecated `device_buffers` with `addressable_shards` in JAX (Python)\nDESCRIPTION: Describes the replacement for the deprecated `arr.device_buffers` property of JAX arrays, mentioned in the JAX 0.4.22 release notes. To get the data from all addressable shards, use a list comprehension like `[x.data for x in arr.addressable_shards]`. This reflects the shift towards the explicit array sharding interface.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Old, deprecated property:\narr.device_buffers\n\n# New, recommended approach:\n[x.data for x in arr.addressable_shards]\n```\n\n----------------------------------------\n\nTITLE: Reusing a JAX Symbolic Scope for Consistent Dimensions\nDESCRIPTION: Shows how to ensure symbolic dimensions share the same scope by explicitly reusing the scope of a previously defined symbolic dimension (`a.scope`) when creating another (`b`). This allows arithmetic operations (`a + b`) between them without causing scope mismatch errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> a, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\n>>> b, = export.symbolic_shape(\"b,\", scope=a.scope)  # Reuse the scope of `a`\n\n>>> a + b  # Allowed\nb + a\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Shape Assertion Errors in JAX\nDESCRIPTION: Shows how JAX generates shape assertion errors when the actual input shapes do not match the expected polymorphic shapes specification.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> def f(x):  # x: f32[b, b, 2*d]\n...   return x\n>>> exp = export.export(jax.jit(f))(\n...     jax.ShapeDtypeStruct(export.symbolic_shape(\"b, b, 2*d\"), dtype=np.int32))   \n>>> exp.call(np.ones((3, 3, 5), dtype=np.int32))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: Input shapes do not match the polymorphic shapes specification.\nDivision had remainder 1 when computing the value of 'd'.\nUsing the following polymorphic shapes specifications:\n  args[0].shape = (b, b, 2*d).\nObtained dimension variables: 'b' = 3 from specification 'b' for dimension args[0].shape[0] (= 3), .\nPlease see https://docs.jax.dev/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\n\n```\n\n----------------------------------------\n\nTITLE: Creating Animation of Wave Equation Simulation\nDESCRIPTION: Generates an animated GIF of the wave equation simulation results, showing the propagation of the wave over time.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.cm\nimport matplotlib.colors\nfrom PIL import Image\n\ndef make_images(data, cmap='RdBu', vmax=None):\n  images = []\n  for frame in data:\n    if vmax is None:\n      this_vmax = np.max(abs(frame))\n    else:\n      this_vmax = vmax\n    norm = matplotlib.colors.Normalize(vmin=-this_vmax, vmax=this_vmax)\n    mappable = matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap)\n    rgba = mappable.to_rgba(frame, bytes=True)\n    image = Image.fromarray(rgba, mode='RGBA')\n    images.append(image)\n  return images\n\ndef save_movie(images, path, duration=100, loop=0, **kwargs):\n  images[0].save(path, save_all=True, append_images=images[1:],\n                 duration=duration, loop=loop, **kwargs)\n\nimages = make_images(u_final[::, ::8, ::8].transpose(0, 2, 1))\n```\n\nLANGUAGE: python\nCODE:\n```\n# Show Movie\nproglog.default_bar_logger = partial(proglog.default_bar_logger, None)\nImageSequenceClip([np.array(im) for im in images], fps=25).ipython_display()\n```\n\nLANGUAGE: python\nCODE:\n```\n# Save GIF.\nsave_movie(images,'wave_movie.gif', duration=[2000]+[200]*(len(images)-2)+[2000])\n# The movie sometimes takes a second before showing up in the file system.\nimport time; time.sleep(1)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Download animation.\ntry:\n    from google.colab import files\nexcept ImportError:\n    pass\nelse:\n    files.download('wave_movie.gif')\n```\n\n----------------------------------------\n\nTITLE: Applying JAX Transformations (jvp) to Jaxpr Evaluation in Python\nDESCRIPTION: Demonstrates that the result of evaluating a Jaxpr via `eval_jaxpr` remains a standard Python function that can be further processed by JAX transformations like `jvp` (Jacobian-vector product for forward-mode automatic differentiation). It wraps the `eval_jaxpr` call in a lambda and computes its JVP.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/autodidax2_part1.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(jvp(lambda x: eval_jaxpr(build_jaxpr(foo, 1), (x,)), 2.0, 1.0))\n```\n\n----------------------------------------\n\nTITLE: Dumping Jaxprs and Mosaic Code for Debugging Pallas\nDESCRIPTION: Example showing how to use the debug parameter to print out the Jaxpr and lowered Mosaic code for a Pallas kernel. This is useful for understanding the compilation process and diagnosing issues.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/g3doc/debugging.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef kernel(x_ref, y_ref, o_ref):\n  o_ref[...] = x_ref[...] + y_ref[...]\n\nx = jnp.ones((8, 128), dtype=jnp.float32)\npl.pallas_call(\n  kernel,\n  out_shape=jax.ShapeDTypeStruct((8, 128), jnp.float32),\n  debug=True,\n  name=\"my_call\"\n)(x, x)\n```\n\n----------------------------------------\n\nTITLE: Handling Non-finite Values in NVIDIA GPU SVD Computation\nDESCRIPTION: Example of passing an array with non-finite values to a non-symmetric eigendecomposition. The fix ensures that such inputs produce arrays full of NaNs as outputs instead of causing errors or hangs.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Array with non-finite values\nnon_finite_array = jnp.array([jnp.inf, jnp.nan, 1.0])\n\n# Non-symmetric eigendecomposition (now produces NaNs instead of error)\nresult = jnp.linalg.eig(non_finite_array)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Sharding Pattern Change with Constraint in JAX\nDESCRIPTION: Displays the sharding of the input array and the output array after applying the JIT-compiled function with a sharding constraint. This shows how the sharding pattern was transformed from ('x', 'y') to ('y', 'x').\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\njax.debug.visualize_array_sharding(x)\ny = f(x)\njax.debug.visualize_array_sharding(y)\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for TPU Documentation in reStructuredText\nDESCRIPTION: This snippet creates a table of contents using reStructuredText syntax. It sets up a toctree directive with a caption and specifies the maximum depth of the table. The toctree includes links to various guide documents related to TPU usage and optimization.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/pallas/tpu/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :caption: Guides\n   :maxdepth: 2\n\n   details\n   pipelining\n   matmul\n   sparse\n   distributed\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Non-Linear Dimension Variable Error in JAX\nDESCRIPTION: Illustrates an error case where JAX cannot solve for dimension variables due to non-linear expressions in the input shapes.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> a, = export.symbolic_shape(\"a\")\n>>> export.export(jax.jit(lambda x: x.shape[0]))(\n...    jax.ShapeDtypeStruct((a * a,), dtype=np.int32))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: Cannot solve for values of dimension variables {'a'}.\nWe can only solve linear uni-variate constraints.\nUsing the following polymorphic shapes specifications: args[0].shape = (a^2,).\nUnprocessed specifications: 'a^2' for dimension size args[0].shape[0].\n\n```\n\n----------------------------------------\n\nTITLE: Verifying Correctness of Sharded Matrix Multiplication in JAX\nDESCRIPTION: Confirms that the result of a matrix multiplication performed on a single device matches the result of the same operation performed with sharded inputs across multiple devices, validating that sharding doesn't affect numerical correctness.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnp.allclose(jnp.dot(x_single, x_single),\n            jnp.dot(y, z))\n```\n\n----------------------------------------\n\nTITLE: Implementing JVP Tangent Calculation in Python\nDESCRIPTION: Example of how JAX computes a JVP (Jacobian-Vector Product) tangent calculation for the function f(x, y) = x * y + y at point (2., 4.), showing the linear operations performed on input tangents.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jax-primitives.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n   a = xt * 4.\n   b = 2. * yt\n   c = a + b\n   ft = c + yt\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Array-Scalar Type Promotion in NumPy\nDESCRIPTION: This example shows that when operations involve a NumPy array and a Python scalar, the scalar defers to the dtype of the array. The result of adding an int8 array and a Python integer is an int8 array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/9407-type-promotion.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nx = np.zeros(1, dtype='int8')  # int8 array\ny = 1  # Python int = int64 scalar\n(x + y).dtype\n```\n\n----------------------------------------\n\nTITLE: Using Alternative to Deprecated solve Function with Batched 1D Arguments\nDESCRIPTION: A replacement pattern for jax.numpy.linalg.solve with batched 1D arguments on the right-hand side, which is no longer supported in JAX 0.5.0. This adds a dimension and then squeezes it out after solving.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsolve(a, b[..., None]).squeeze(-1)\n```\n\n----------------------------------------\n\nTITLE: Defining a Function for JIT Compilation\nDESCRIPTION: Creates a function with a loop that performs multiple operations, returning a subset of the result - this will be used to demonstrate JIT compilation benefits.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  y = x\n  for _ in range(10):\n    y = y - 0.1 * y + 3.\n  return y[:100, :100]\n\nf(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hard Assertions with Checkify in Pallas\nDESCRIPTION: Example showing how to use checkify.check for hard assertions in Pallas kernels. This method will halt the TPU if the assertion fails and requires the --jax_pallas_enable_runtime_assert flag.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/g3doc/debugging.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom jax.experimental import checkify\n\ndef kernel(...):\n  checkify.check(x > y, \"Check x > y failed\")  # Will halt if x <= y\n```\n\n----------------------------------------\n\nTITLE: Symbolic Args Specs Usage in Python\nDESCRIPTION: Shows how to use symbolic_args_specs to construct pytrees of ShapeDtypeStruct objects with polymorphic shape specifications.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f1(x, y): # x: f32[a, 1], y : f32[a, 4]\n return x + y\n\n# Assuming you have some actual args with concrete shapes\nx = np.ones((3, 1), dtype=np.int32)\ny = np.ones((3, 4), dtype=np.int32)\nargs_specs = export.symbolic_args_specs((x, y), \"a, ...\")\nexp = export.export(jax.jit(f1))(* args_specs)\nexp.in_avals\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Test Case Example in Python\nDESCRIPTION: Example showing how to configure JAX test cases with the numpy rank promotion option using a decorator. This demonstrates the recommended way to customize test behavior after deprecation of JaxTestCase.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n@jtu.with_config(jax_numpy_rank_promotion='allow')\nclass MyTestCase(jtu.JaxTestCase):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Using Symbolic Constraints for Stride Operations\nDESCRIPTION: Example showing how to work around limitations in JAX's reasoning for strided operations by adding a symbolic constraint for a floor division operation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\njax2tf.convert(lambda x: x[: 4*(x.shape[0] // 4)],\n               polymorphic_shapes=(\"b, ...\",))\n```\n\n----------------------------------------\n\nTITLE: Using static_argnums with JIT Compilation\nDESCRIPTION: Recompiles the function with static_argnums to indicate that the loop bound should be treated as static during compilation, allowing the function to compile successfully.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ng = jit(f, static_argnums=(1,))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Saved Residuals with Named Checkpoints\nDESCRIPTION: Demonstrates how to analyze which values are saved when using named checkpoints in the prediction function. Shows the effect of naming on JAX's default behavior.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_remat.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprint_saved_residuals(loss, params, x, y)\n```\n\n----------------------------------------\n\nTITLE: Using the custom_vjp function with pytree input\nDESCRIPTION: Shows how to create a Point namedtuple instance and pass it to the custom_vjp function. This demonstrates the basic usage of the custom_vjp implementation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb#2025-04-22_snippet_61\n\nLANGUAGE: python\nCODE:\n```\npt = Point(1., 2.)\n\nprint(f(pt))\n```\n\n----------------------------------------\n\nTITLE: Visualizing JAX Arrays with Matplotlib\nDESCRIPTION: This snippet shows how to visualize JAX arrays using Matplotlib. It plots the first row of the previously created random array.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nplt.plot(x[0])\n```\n\n----------------------------------------\n\nTITLE: Runtime Instance Checking with JAX Arrays\nDESCRIPTION: Example demonstrating runtime type checking behavior with both regular arrays and traced arrays under JIT compilation.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/jep/12049-type-annotations.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return isinstance(x, ArrayInstance)\nx = jnp.array([1, 2, 3])\nassert f(x)       # x will be an array\nassert jit(f)(x)  # x will be a tracer\n```\n\n----------------------------------------\n\nTITLE: Jupytext Configuration in YAML\nDESCRIPTION: Metadata configuration for Jupytext document formatting, specifying Python kernel settings and file format details.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/_tutorials/advanced-debugging.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\njupytext:\n  formats: md:myst\n  text_representation:\n    extension: .md\n    format_name: myst\n    format_version: 0.13\n    jupytext_version: 1.16.4\nkernelspec:\n  display_name: Python 3\n  language: python\n  name: python3\n```\n\n----------------------------------------\n\nTITLE: Checking JAX Version and Environment Information\nDESCRIPTION: Imports JAX and JAXlib modules, displays the hostname of the Colab environment, and prints the version information for both JAX and JAXlib.\nSOURCE: https://github.com/jax-ml/jax/blob/main/tests/notebooks/colab_cpu.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jaxlib\n\n!cat /var/colab/hostname\nprint(jax.__version__)\nprint(jaxlib.__version__)\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Shape Polymorphism\nDESCRIPTION: Shows common error cases when working with shape polymorphism, including broadcasting and matrix multiplication shape mismatches.\nSOURCE: https://github.com/jax-ml/jax/blob/main/docs/export/shape_poly.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nv, = export.symbolic_shape(\"v,\")\nexport.export(jax.jit(lambda x, y: x + y))(\n    jax.ShapeDtypeStruct((v,), dtype=np.int32),\n    jax.ShapeDtypeStruct((4,), dtype=np.int32))\n```\n\n----------------------------------------\n\nTITLE: Running Bazel Command for JAX Requirements Update\nDESCRIPTION: Command to update Python package requirements for the JAX project using Bazel. This leverages pip-compile with Python 3.13 to generate an updated requirements file.\nSOURCE: https://github.com/jax-ml/jax/blob/main/build/requirements_lock_3_13.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel run //build:requirements.update\n```\n\n----------------------------------------\n\nTITLE: Using item Method with Index Arguments in JAX\nDESCRIPTION: Demonstrates the updated jax.Array.item method which now supports optional index arguments for element access.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\narray = jax.numpy.array([[1, 2], [3, 4]])\nvalue = array.item(1, 1)  # Returns 4\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with JAX NumPy\nDESCRIPTION: Performs matrix multiplication using JAX's NumPy implementation and prints a specific value from the result.\nSOURCE: https://github.com/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ny = jnp.dot(x, x)\nprint(y[0, 0])\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Platforms for TPU Initialization\nDESCRIPTION: Sets environment variables to control JAX platform initialization behavior, particularly for TPU usage.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_23\n\nLANGUAGE: Bash\nCODE:\n```\nJAX_PLATFORMS=tpu,cpu\n```\n\nLANGUAGE: Bash\nCODE:\n```\nJAX_PLATFORMS=''\n```\n\nLANGUAGE: Bash\nCODE:\n```\nJAX_PLATFORMS=cpu\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Persistent Cache Error Handling\nDESCRIPTION: Sets an environment variable to control whether JAX raises exceptions on persistent cache errors.\nSOURCE: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#2025-04-22_snippet_24\n\nLANGUAGE: Bash\nCODE:\n```\nJAX_RAISE_PERSISTENT_CACHE_ERRORS=true\n```"
  }
]
[
  {
    "owner": "apache",
    "repo": "hadoop-site",
    "content": "TITLE: Configuring Variable Expansion in Hadoop Configuration Files\nDESCRIPTION: This snippet demonstrates the new variable expansion feature added to Hadoop configuration files. It allows the use of ${variable} syntax to reference values from the configuration or Java system properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_37\n\nLANGUAGE: XML\nCODE:\n```\n${hadoop.tmp.dir}\n```\n\n----------------------------------------\n\nTITLE: Configuring Capacity Scheduler Queue Ordering Policies\nDESCRIPTION: Configuration example for queue ordering policies in capacity-scheduler.xml. This snippet demonstrates how to set up FIFO (First In, First Out) application ordering for the default queue.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>yarn.scheduler.capacity.root.ordering-policy</name>\n  <value>priority-utilization</value>\n  <description>The ordering policy inside the root queue</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.root.default.ordering-policy</name>\n  <value>fifo</value>\n  <description>The ordering policy inside the default queue.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Implementing Crypto Streams with Hadoop Stream Interfaces\nDESCRIPTION: Implementation of crypto input and output streams that conform to Hadoop stream interfaces (HADOOP-10603). Contributed by Yi Liu and Charles Lamb.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Configuring Speculative Execution in Hadoop XML\nDESCRIPTION: This snippet shows configuration changes in hadoop-default.xml to allow finer-grained control over speculative execution for map and reduce tasks independently.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_32\n\nLANGUAGE: XML\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Configuring Alternate Hadoop Configuration Directory\nDESCRIPTION: This change adds a --config option to Hadoop shell scripts, allowing users to specify an alternate configuration directory. This provides more flexibility in managing Hadoop configurations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Bash\nCODE:\n```\n--config\n```\n\n----------------------------------------\n\nTITLE: Find Command Implementation - Bash\nDESCRIPTION: New filesystem find command implementation allowing advanced file searching capabilities.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nHADOOP-8989. hadoop fs -find feature\n```\n\n----------------------------------------\n\nTITLE: Truncate API Addition - Java\nDESCRIPTION: New truncate API exposed through FileSystem interface and shell command.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nHADOOP-11490. Expose truncate API via FileSystem and shell command.\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Streaming Field Separators\nDESCRIPTION: Configuration settings for specifying field separators in Hadoop Streaming jobs for map and reduce operations. Default value is tab character.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Properties\nCODE:\n```\nstream.map.input.field.separator=\\t\nstream.map.output.field.separator=\\t\nstream.reduce.input.field.separator=\\t\nstream.reduce.output.field.separator=\\t\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration XML Changes for Task Limits\nDESCRIPTION: Configuration changes to hadoop-default.xml that modify the maximum number of map and reduce tasks per TaskTracker. Adds separate limits for map and reduce tasks, replacing the deprecated single task limit.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: xml\nCODE:\n```\nmapred.tasktracker.map.tasks.maximum (default value of 2)\nmapred.tasktracker.reduce.tasks.maximum (default value of 2)\nmapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Release Version in Bash\nDESCRIPTION: Renames the branch-2.3 release version from 2.4.0-SNAPSHOT to 2.3.0-SNAPSHOT using a shell command.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\n# Implied bash command\nsed -i 's/2.4.0-SNAPSHOT/2.3.0-SNAPSHOT/g' pom.xml\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration Parameters for Map-Reduce JVM and Environment Settings\nDESCRIPTION: This snippet introduces new configuration parameters for setting JVM options, environment variables, and ulimit separately for map and reduce tasks. It also deprecates some older, combined parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Properties\nCODE:\n```\nmapred.map.child.java.opts\nmapred.reduce.child.java.opts\nmapred.map.child.env\nmapred.reduce.child.ulimit\nmapred.map.child.env\nmapred.reduce.child.ulimit\nmapred.child.java.opts (deprecated)\nmapred.child.env (deprecated)\nmapred.child.ulimit (deprecated)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop SASL Properties in core-site.xml\nDESCRIPTION: XML configuration snippet showing how to set the Hadoop SASL properties for QOP settings in the core-site.xml file, which affects security configuration across Hadoop components.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.security.saslproperties.resolver.class</name>\n  <value>org.apache.hadoop.security.SaslPropertiesResolver</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Adding getDouble() and setDouble() methods to Configuration class in Java\nDESCRIPTION: This snippet adds support for getting and setting double values in the Hadoop Configuration class. It improves the flexibility of configuration handling in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8415. Add getDouble() and setDouble() in \norg.apache.hadoop.conf.Configuration (Jan van der Lugt via harsh)\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Job Resource Allocation in Java\nDESCRIPTION: Configuration properties for controlling MapReduce job resource allocation, including memory and CPU settings for the Application Master and tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nmapreduce.job.am.resource.mb\nmapreduce.job.am.resource.cpu-vcores\nyarn.app.am.resource.mb\nyarn.app.am.resource.cpu-vcores\n```\n\n----------------------------------------\n\nTITLE: Adding RPC Timeout Option in Java\nDESCRIPTION: Introduces an option to set a timeout for Remote Procedure Calls (RPC) in Hadoop. This allows for better control over network operations and prevents indefinite waiting on unresponsive servers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// New RPC method with timeout option\nrpc.callWithTimeout(timeout)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Capacity Scheduler Properties in XML\nDESCRIPTION: Example showing how to define various capacity scheduler properties in the capacity-scheduler.xml file. This includes setting properties like queue capacities, user limits, and maximum application settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>yarn.scheduler.capacity.maximum-applications</name>\n  <value>10000</value>\n  <description>Maximum number of applications that can be pending and running.</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>\n  <value>0.1</value>\n  <description>Maximum percent of resources in the cluster which can be used to run application masters i.e. controls number of concurrent running applications.</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.resource-calculator</name>\n  <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>\n  <description>The ResourceCalculator implementation to be used to compare Resources in the scheduler.\n  The default i.e. DefaultResourceCalculator only uses Memory while DominantResourceCalculator uses dominant-resource to compare multi-dimensional resources such as Memory, CPU etc.</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.root.queues</name>\n  <value>default</value>\n  <description>The queues at the this level (root is the root queue).</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Optimizing Edit Log Preservation in HDFS\nDESCRIPTION: This optimization improves performance and addresses timeout issues by using hard-links to preserve old edit logs instead of copying them. It was implemented in version 2.6.1.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-8480. Fix performance and timeout issues in HDFS-7929 by using hard-links to preserve old edit logs, instead of copying them. (Zhe Zhang via Colin P. McCabe)\n```\n\n----------------------------------------\n\nTITLE: Fixing RPC client timeout in Hadoop (Java)\nDESCRIPTION: Addresses HADOOP-11252 where the RPC client does not time out by default. This fix implements proper timeout behavior for RPC clients.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11252. RPC client does not time out by default.\n```\n\n----------------------------------------\n\nTITLE: Configuring Field Separators for Streaming Jobs in Hadoop\nDESCRIPTION: New configuration options allow specifying custom field separators for map and reduce input/output in Hadoop streaming jobs. These options default to tab (\"\\t\") if not set.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: XML\nCODE:\n```\nstream.map.input.field.separator\nstream.map.output.field.separator\nstream.reduce.input.field.separator\nstream.reduce.output.field.separator\n```\n\n----------------------------------------\n\nTITLE: FileSystem Wildcard Syntax - File Pattern\nDESCRIPTION: New wildcard input syntax support added in HADOOP-1968 for FileSystem operations using curly braces.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: plain\nCODE:\n```\n{ }\n```\n\n----------------------------------------\n\nTITLE: Defining YARN Configuration Constants in Java\nDESCRIPTION: This code snippet defines multiple static final fields in the YarnConfiguration class. These fields represent default values for various YARN configuration parameters, covering aspects like ResourceManager settings, Router configurations, Timeline Service properties, and general YARN behaviors.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n  public static final int DEFAULT_RM_PLACEMENT_CONSTRAINTS_SCHEDULER_POOL_SIZE;\n  public static final int DEFAULT_RM_PROXY_CONNECTION_TIMEOUT;\n  public static final boolean DEFAULT_RM_PUBLISH_CONTAINER_EVENTS_ENABLED;\n  public static final long DEFAULT_RM_RESERVATION_SYSTEM_MAX_PERIODICITY;\n  public static final boolean DEFAULT_RM_RESOURCE_PROFILES_ENABLED;\n  public static final String DEFAULT_RM_RESOURCE_PROFILES_SOURCE_FILE;\n  // ... (many more fields)\n  public static final boolean DEFAULT_YARN_WEBAPP_UI2_ENABLE;\n}\n```\n\n----------------------------------------\n\nTITLE: Using Wildcard Input Syntax in Hadoop FileSystem\nDESCRIPTION: Adds support for wildcard input syntax using curly braces in Hadoop FileSystem operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.globStatus(new Path(\"/user/{alice,bob}/*.txt\"));\n```\n\n----------------------------------------\n\nTITLE: Improving getSplits Performance in FileInputFormat\nDESCRIPTION: Optimization to improve getSplits performance in FileInputFormat by using listLocatedStatus. This change is part of MAPREDUCE-1981.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-1981. Improve getSplits performance by using listLocatedStatus\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Script Environment Variables in Shell\nDESCRIPTION: Allows HADOOP_PREFIX to be overridden in Hadoop scripts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP-7801. HADOOP_PREFIX cannot be overriden. (Bruno Mahé via tomwhite)\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Hadoop Task Management\nDESCRIPTION: Configuration changes to hadoop-default.xml that add mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum to replace the deprecated mapred.tasktracker.tasks.maximum parameter. This allows for better configuration of heterogeneous clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.tasktracker.map.tasks.maximum (default value of 2)\nadd mapred.tasktracker.reduce.tasks.maximum (default value of 2)\nremove mapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Making Shell Path with Optional Canonicalization\nDESCRIPTION: Addition of a new parameter to FileUtil.makeShellPath API to control whether file paths should be canonicalized or not.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: java\nCODE:\n```\nFileUtil.makeShellPath(path, canonicalize)\n```\n\n----------------------------------------\n\nTITLE: Updating WritableUtils.clone Method Signature in Java\nDESCRIPTION: Changes the WritableUtils.clone method to accept a Configuration object instead of a JobConf object, improving flexibility.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_30\n\nLANGUAGE: Java\nCODE:\n```\nWritableUtils.clone(Configuration conf)\n```\n\n----------------------------------------\n\nTITLE: Enabling Bash Tab Completion for Hadoop CLI\nDESCRIPTION: A new contrib module provides bash tab completion for the Hadoop command-line interface. Installation instructions are provided in the README file of the contrib directory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: Bash\nCODE:\n```\nbin/hadoop\n```\n\n----------------------------------------\n\nTITLE: IO Enhancement - NativeIO Support\nDESCRIPTION: Added support for fadvise and sync_file_range in NativeIO with ReadaheadPool infrastructure for HDFS and MapReduce performance improvements.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nReadaheadPool\n```\n\n----------------------------------------\n\nTITLE: Adding New SequenceFile.createWriter API in Java\nDESCRIPTION: Introduces a new API for SequenceFile.createWriter that allows specifying blocksize, replication factor, and buffer size for the underlying HDFS file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_31\n\nLANGUAGE: Java\nCODE:\n```\nSequenceFile.createWriter(Configuration conf, Path file, int blockSize, short replication, int bufferSize)\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Manager Queue Properties in Hadoop\nDESCRIPTION: Demonstrates the naming convention for configuring queue properties in Hadoop's resource manager. This allows for fine-grained control over scheduling parameters for different queues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Properties\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Adding LZO Compressed Text File Splitter in Java\nDESCRIPTION: This improvement adds an input format capable of splitting LZO compressed text files. It enables more efficient processing of LZO compressed data in Hadoop jobs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4640. Adds an input format that can split lzo compressed text files.\n```\n\n----------------------------------------\n\nTITLE: Implementing DNSToSwitchMapping in Java\nDESCRIPTION: Implements a base class for DNSToSwitchMapping that can offer extra topology information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7777 Implement a base class for DNSToSwitchMapping implementations \nthat can offer extra topology information. (stevel)\n```\n\n----------------------------------------\n\nTITLE: Adding FileSystem API for Closing All File Systems for a UGI in Java\nDESCRIPTION: Adds a new FileSystem API closeAllForUGI() for closing all file systems associated with a particular UserGroupInformation (UGI). This allows for better resource management when dealing with multiple file systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.closeAllForUGI(UserGroupInformation ugi)\n```\n\n----------------------------------------\n\nTITLE: Handling Insufficient Space for Map Output Fetching\nDESCRIPTION: Bug fix to prevent the reducer from implicating map attempts when there is insufficient space to fetch map output. This addresses MAPREDUCE-5251.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5251. Reducer should not implicate map attempt if it has insufficient space to fetch map output\n```\n\n----------------------------------------\n\nTITLE: Modifying BlockUnderConstructionFeature.truncateBlock type (Java)\nDESCRIPTION: Improvement to change the type of BlockUnderConstructionFeature.truncateBlock to BlockInfo for better type consistency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-12884. BlockUnderConstructionFeature.truncateBlock should be of type BlockInfo.\n```\n\n----------------------------------------\n\nTITLE: Adding copyBytes Method to Text and BytesWritable Classes in Java\nDESCRIPTION: Introduces a new copyBytes method to both Text and BytesWritable classes, providing a way to efficiently copy byte data between these Writable types.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\npublic void copyBytes(byte[] bytes, int start, int len)\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Logging Levels in Java\nDESCRIPTION: New configuration options for setting logging levels separately for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapred.map.child.log.level \n  add mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Status Persistence in Hadoop XML\nDESCRIPTION: This snippet shows configuration changes to hadoop-default.xml for persisting job statuses in HDFS. It adds new properties to control persistence activation, duration, and storage location.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                                /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Job Status Persistence in Hadoop MapReduce\nDESCRIPTION: Configuration properties added to hadoop-default.xml for persisting completed job statuses in HDFS. These settings control whether persistence is active, how long job information is retained, and where job information is stored.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                              /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Implementing High-Performance Secure Random Number Sources in Java\nDESCRIPTION: This code implements high-performance secure random number sources in Java, likely for use in cryptographic operations within Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10734. Implement high-performance secure random number sources. (Yi Liu via Colin Patrick McCabe)\n```\n\n----------------------------------------\n\nTITLE: Triggering Full Block Reports in HDFS DataNode\nDESCRIPTION: Adds a command to allow system administrators to manually trigger full block reports from a DataNode. This improves diagnostic and maintenance capabilities.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7278. Add a command that allows sysadmins to manually trigger full block reports from a DN (cmccabe)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Common with hadoop-policy.xml\nDESCRIPTION: Example of the hadoop-policy.xml configuration file that defines ACL settings for Hadoop services. This file controls which users or groups have access to specific Hadoop services through Access Control Lists.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>security.client.protocol.acl</name>\n  <value>administrator group</value>\n  <description>ACL for ClientProtocol, which is used by user code\n  via the DistributedFileSystem.\n  The ACL is a comma-separated list of user and group names. The user and\n  group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n  A special value of \"*\" means all users are allowed.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN Application Classpath in Java\nDESCRIPTION: Sets a default value for the YARN_APPLICATION_CLASSPATH configuration property in YarnConfiguration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-4422. YARN_APPLICATION_CLASSPATH needs a documented default value in YarnConfiguration.\n```\n\n----------------------------------------\n\nTITLE: File System Wildcard Support\nDESCRIPTION: Implementation of globStatus method in FileSystem interface to support wildcard patterns, replacing older globPath and listPath methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.globStatus\n```\n\n----------------------------------------\n\nTITLE: Implementing ONCRPC and XDR Protocols (Java)\nDESCRIPTION: Implements the ONCRPC (Open Network Computing Remote Procedure Call) and XDR (External Data Representation) protocols in Hadoop. This adds support for these standard protocols, enhancing Hadoop's interoperability.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nONCRPC\nXDR\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Job Initialization Threads in Java\nDESCRIPTION: Introduced a configuration parameter to control the number of job initialization threads. This allows for parallel job initialization, potentially improving cluster startup times.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nmapred.jobinit.threads\n```\n\n----------------------------------------\n\nTITLE: Configuring IPC client connection retries in Hadoop core-default.xml\nDESCRIPTION: Adds a new configuration property 'ipc.client.connect.max.retries.on.timeouts' to the core-default.xml file to control the number of retries for IPC client connections on socket timeouts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>ipc.client.connect.max.retries.on.timeouts</name>\n  <value>45</value>\n  <description>Indicates the number of retries a client will make to establish\n               a server connection when the socket times out.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Setting MapReduce Job Speculative Execution Threshold in Java\nDESCRIPTION: Configuration property for controlling the threshold at which speculative execution is triggered for slow tasks in MapReduce jobs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nmapreduce.job.speculative.slowtaskthreshold\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop hdfs-site.xml\nDESCRIPTION: XML configuration for the hdfs-site.xml file that sets up HDFS parameters including replication factor and filesystem directories.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n    <property>\n        <name>dfs.namenode.name.dir</name>\n        <value>/home/hadoop/hdfs/namenode</value>\n    </property>\n    <property>\n        <name>dfs.datanode.data.dir</name>\n        <value>/home/hadoop/hdfs/datanode</value>\n    </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Improving Lease Recovery in HDFS NameNode (Java)\nDESCRIPTION: Fixes an issue where the NameNode lease manager could not handle certain characters in filenames, causing errors in Secondary NameNode and NameNode restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-6017. Lease Manager in NameNode does not handle certain characters\nin filenames. This results in fatal errors in Secondary NameNode and while\nrestrating NameNode. (Tsz Wo (Nicholas), SZE via rangadi)\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Levels for Map and Reduce Tasks in Hadoop MapReduce (Java)\nDESCRIPTION: New configuration options added to set logging levels separately for map and reduce tasks. This allows for more granular control over task logging.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapred.map.child.log.level \n  add mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenSSL Cipher in Java\nDESCRIPTION: This snippet updates the OpensslCipher#getInstance method to accept CipherSuite#name format for initializing OpenSSL ciphers in Java.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10803. Update OpensslCipher#getInstance to accept CipherSuite#name format. (Yi Liu)\n```\n\n----------------------------------------\n\nTITLE: Implementing 4-Layer NetworkTopology (Java)\nDESCRIPTION: Adds a new NetworkTopologyWithNodeGroup class that implements a 4-layer network topology. This allows for more granular network topology representation in Hadoop clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nNetworkTopologyWithNodeGroup\n```\n\n----------------------------------------\n\nTITLE: Using Custom Counters in Hadoop MapReduce\nDESCRIPTION: Allows users to specify counters via strings instead of enumerations, providing more flexibility in counter creation and usage.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\ncontext.getCounter(\"MyGroup\", \"MyCounter\").increment(1);\n```\n\n----------------------------------------\n\nTITLE: Java Library Path Configuration in Hadoop Scripts\nDESCRIPTION: Shell script update to add Java library path to system library path for proper native library loading.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n# HADOOP-8781\nhadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Defining YARN Configuration Constants in Java\nDESCRIPTION: This snippet shows the declaration of various configuration constants for Apache Hadoop YARN. These constants define default values and property names for settings related to resource management, container execution, and cluster operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.6/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n    public static final String AMRM_PROXY_HA_ENABLED;\n    public static final String AMRM_PROXY_INTERCEPTOR_CLASS_PIPELINE;\n    public static final String APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC;\n    public static final String APP_FINAL_VALUE_RETENTION_THRESHOLD;\n    public static final String APP_NAME_PLACEMENT_RULE;\n    public static final String APPLICATION_TAG_BASED_PLACEMENT_ENABLED;\n    public static final String APPLICATION_TAG_BASED_PLACEMENT_USER_WHITELIST;\n    public static final String ATS_APP_COLLECTOR_LINGER_PERIOD_IN_MS;\n    public static final String AUTOMATICALLY_DISCOVER_GPU_DEVICES;\n    public static final String CENTRALIZED_NODELABEL_CONFIGURATION_TYPE;\n    public static final String CLIENT_FAILOVER_NO_HA_PROXY_PROVIDER;\n    public static final String CONFIG_NODE_DESCRIPTOR_PROVIDER;\n    public static final String CURATOR_LEADER_ELECTOR;\n    public static final String DEFALUT_RM_PROXY_TIMEOUT_ENABLED;\n    // ... (additional constant declarations)\n}\n```\n\n----------------------------------------\n\nTITLE: Adding RunningJob.getJobName() Method in Hadoop\nDESCRIPTION: Introduces a new method getJobName() to the RunningJob interface to retrieve the name of a running job.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_34\n\nLANGUAGE: Java\nCODE:\n```\nRunningJob#getJobName()\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Log4j Backend in Java\nDESCRIPTION: This snippet refers to the addition of a Log4j backend in Java that can output JSON data, one entry per line.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nlog4j backend\n```\n\n----------------------------------------\n\nTITLE: Field Separator Configuration for Streaming Jobs\nDESCRIPTION: Configuration properties added to specify field separators for map and reduce input/output in Hadoop Streaming jobs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\nstream.map.input.field.separator\nstream.map.output.field.separator\nstream.reduce.input.field.separator\nstream.reduce.output.field.separator\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Recursive create() in FileSystem and SequenceFile.Writer in Java\nDESCRIPTION: This change adds support for non-recursive file creation in Hadoop's FileSystem and SequenceFile.Writer classes. It allows creating files without automatically creating parent directories.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-6840. Support non-recursive create() in FileSystem and \nSequenceFile.Writer. (jitendra and eli via eli)\n```\n\n----------------------------------------\n\nTITLE: Fixing NodeManager Resource Relocalization Issue in Java\nDESCRIPTION: Addresses a bug where the NodeManager mistakenly loses resources and unnecessarily relocalizes them. This optimization prevents redundant resource transfers and improves efficiency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1386. NodeManager mistakenly loses resources and relocalizes them\n```\n\n----------------------------------------\n\nTITLE: Configuring IPC client idle connection management in Hadoop\nDESCRIPTION: The ipc.client.maxidletime property is removed and replaced with ipc.client.connection.maxidletime. The max idle time is now defined as twice the connection max idle time.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>ipc.client.connection.maxidletime</name>\n  <value>30000</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring DFS Block Size in Hadoop\nDESCRIPTION: Sets the DFS block size as a multiple of the checksum size. This change requires an upgrade from previous versions due to layout and protocol changes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_31\n\nLANGUAGE: Text\nCODE:\n```\ndfs.block.size must now be a multiple of io.byte.per.checksum\n```\n\n----------------------------------------\n\nTITLE: Defining Default YARN Configuration Constants in Java\nDESCRIPTION: This code snippet defines numerous static final fields representing default values for various YARN configuration parameters. These constants cover aspects such as Node Manager health checks, log aggregation, resource management, network settings, and Resource Manager operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n  public static final long DEFAULT_NM_HEALTH_CHECK_TIMEOUT_MS;\n  public static final int DEFAULT_NM_LOG_AGGREGATION_NUM_LOG_FILES_SIZE_PER_APP;\n  public static final int DEFAULT_NM_LOG_AGGREGATION_THREAD_POOL_SIZE;\n  public static final boolean DEFAULT_NM_LOG_CONTAINER_DEBUG_INFO;\n  public static final boolean DEFAULT_NM_LOG_CONTAINER_DEBUG_INFO_ON_ERROR;\n  public static final double DEFAULT_NM_LOG_DELETE_THRESHOLD;\n  public static final boolean DEFAULT_NM_LOG_TRIGGER_DELETE_BY_SIZE_ENABLED;\n  public static final int DEFAULT_NM_MEMORY_RESOURCE_CGROUPS_SOFT_LIMIT_PERCENTAGE;\n  public static final int DEFAULT_NM_MEMORY_RESOURCE_CGROUPS_SWAPPINESS;\n  public static final boolean DEFAULT_NM_MEMORY_RESOURCE_ENABLED;\n  public static final boolean DEFAULT_NM_MEMORY_RESOURCE_ENFORCED;\n  public static final boolean DEFAULT_NM_NETWORK_RESOURCE_ENABLED;\n  public static final String DEFAULT_NM_NETWORK_RESOURCE_INTERFACE;\n  public static final long DEFAULT_NM_NETWORK_RESOURCE_OUTBOUND_BANDWIDTH_MBIT;\n  public static final String DEFAULT_NM_NETWORK_RESOURCE_TAG_MAPPING_FILE_PATH;\n  public static final boolean DEFAULT_NM_NETWORK_TAG_HANDLER_ENABLED;\n  public static final long DEFAULT_NM_NODE_ATTRIBUTES_PROVIDER_FETCH_INTERVAL_MS;\n  public static final long DEFAULT_NM_NODE_ATTRIBUTES_PROVIDER_FETCH_TIMEOUT_MS;\n  public static final long DEFAULT_NM_NODE_ATTRIBUTES_RESYNC_INTERVAL;\n  public static final long DEFAULT_NM_NODE_LABELS_PROVIDER_FETCH_INTERVAL_MS;\n  public static final long DEFAULT_NM_NODE_LABELS_PROVIDER_FETCH_TIMEOUT_MS;\n  public static final long DEFAULT_NM_NODE_LABELS_RESYNC_INTERVAL;\n  public static final boolean DEFAULT_NM_NUMA_AWARENESS_ENABLED;\n  public static final String DEFAULT_NM_NUMA_AWARENESS_NUMACTL_CMD;\n  public static final boolean DEFAULT_NM_NUMA_AWARENESS_READ_TOPOLOGY;\n  public static final int DEFAULT_NM_OPPORTUNISTIC_CONTAINERS_MAX_QUEUE_LENGTH;\n  public static final float DEFAULT_NM_PCORES_VCORES_MULTIPLIER;\n  public static final boolean DEFAULT_NM_PLUGGABLE_DEVICE_FRAMEWORK_ENABLED;\n  public static final boolean DEFAULT_NM_PUBLISH_CONTAINER_EVENTS_ENABLED;\n  public static final long DEFAULT_NM_REAP_RUNC_LAYER_MOUNTS_INTERVAL;\n  public static final int DEFAULT_NM_RECOVERY_COMPACTION_INTERVAL_SECS;\n  public static final boolean DEFAULT_NM_RECOVERY_SUPERVISED;\n  public static final boolean DEFAULT_NM_REMOTE_APP_LOG_DIR_INCLUDE_OLDER;\n  public static final int DEFAULT_NM_RESOURCE_MON_INTERVAL_MS;\n  public static final boolean DEFAULT_NM_RESOURCE_PLUGINS_FAIL_FAST;\n  public static final boolean DEFAULT_NM_RUNC_ALLOW_HOST_PID_NAMESPACE;\n  public static final boolean DEFAULT_NM_RUNC_ALLOW_PRIVILEGED_CONTAINERS;\n  public static final String DEFAULT_NM_RUNC_ALLOWED_CONTAINER_NETWORKS;\n  public static final String DEFAULT_NM_RUNC_ALLOWED_CONTAINER_RUNTIMES;\n  public static final long DEFAULT_NM_RUNC_CACHE_REFRESH_INTERVAL;\n  public static final String DEFAULT_NM_RUNC_IMAGE_TAG_TO_MANIFEST_PLUGIN;\n  public static final String DEFAULT_NM_RUNC_IMAGE_TOPLEVEL_DIR;\n  public static final int DEFAULT_NM_RUNC_LAYER_MOUNTS_TO_KEEP;\n  public static final String DEFAULT_NM_RUNC_MANIFEST_TO_RESOURCES_PLUGIN;\n  public static final String DEFAULT_NM_RUNC_PRIVILEGED_CONTAINERS_ACL;\n  public static final long DEFAULT_NM_RUNC_STAT_CACHE_TIMEOUT;\n  public static final String DEFAULT_NODELABEL_CONFIGURATION_TYPE;\n  public static final int DEFAULT_NUM_MANIFESTS_TO_CACHE;\n  public static final int DEFAULT_NUMBER_OF_ASYNC_ENTITIES_TO_MERGE;\n  public static final String DEFAULT_NVIDIA_DOCKER_PLUGIN_V1_ENDPOINT;\n  public static final int DEFAULT_OPP_CONTAINER_ALLOCATION_NODES_NUMBER_USED;\n  public static final int DEFAULT_OPP_CONTAINER_MAX_ALLOCATIONS_PER_AM_HEARTBEAT;\n  public static final boolean DEFAULT_OPPORTUNISTIC_CONTAINER_ALLOCATION_ENABLED;\n  public static final String DEFAULT_QUEUE_FULL_NAME;\n  public static final int DEFAULT_RM_ACTIVITIES_MANAGER_APP_ACTIVITIES_MAX_QUEUE_LENGTH;\n  public static final long DEFAULT_RM_ACTIVITIES_MANAGER_APP_ACTIVITIES_TTL_MS;\n  public static final long DEFAULT_RM_ACTIVITIES_MANAGER_CLEANUP_INTERVAL_MS;\n  public static final long DEFAULT_RM_ACTIVITIES_MANAGER_SCHEDULER_ACTIVITIES_TTL_MS;\n  public static final String DEFAULT_RM_APPLICATION_HTTPS_POLICY;\n  public static final int DEFAULT_RM_APPLICATION_MAX_TAG_LENGTH;\n  public static final int DEFAULT_RM_APPLICATION_MAX_TAGS;\n  public static final long DEFAULT_RM_APPLICATION_MONITOR_INTERVAL_MS;\n  public static final boolean DEFAULT_RM_AUTO_UPDATE_CONTAINERS;\n  public static final String DEFAULT_RM_CLUSTER_ID;\n  public static final long DEFAULT_RM_DECOMMISSIONING_NODES_WATCHER_POLL_INTERVAL;\n  public static final boolean DEFAULT_RM_DELEGATION_TOKEN_ALWAYS_CANCEL;\n  public static final int DEFAULT_RM_DELEGATION_TOKEN_MAX_CONF_SIZE_BYTES;\n  public static final long DEFAULT_RM_DT_RENEWER_THREAD_RETRY_INTERVAL;\n  public static final int DEFAULT_RM_DT_RENEWER_THREAD_RETRY_MAX_ATTEMPTS;\n  public static final long DEFAULT_RM_DT_RENEWER_THREAD_TIMEOUT;\n  public static final boolean DEFAULT_RM_ENABLE_NODE_UNTRACKED_WITHOUT_INCLUDE_PATH;\n  public static final long DEFAULT_RM_EPOCH;\n  public static final long DEFAULT_RM_EPOCH_RANGE;\n  public static final int DEFAULT_RM_LEVELDB_COMPACTION_INTERVAL_SECS;\n  public static final int DEFAULT_RM_MAX_LOG_AGGREGATION_DIAGNOSTICS_IN_MEMORY;\n  public static final int DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MAX_MS;\n  public static final int DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MIN_MS;\n  public static final boolean DEFAULT_RM_NM_HEARTBEAT_INTERVAL_SCALING_ENABLE;\n  public static final float DEFAULT_RM_NM_HEARTBEAT_INTERVAL_SLOWDOWN_FACTOR;\n  public static final float DEFAULT_RM_NM_HEARTBEAT_INTERVAL_SPEEDUP_FACTOR;\n  public static final boolean DEFAULT_RM_NM_REGISTRATION_IP_HOSTNAME_CHECK_KEY;\n  public static final long DEFAULT_RM_NODE_GRACEFUL_DECOMMISSION_TIMEOUT;\n  public static final int DEFAULT_RM_NODE_IP_CACHE_EXPIRY_INTERVAL_SECS;\n  public static final long DEFAULT_RM_NODE_LABELS_PROVIDER_FETCH_INTERVAL_MS;\n  public static final long DEFAULT_RM_NODE_LABELS_PROVIDER_UPDATE_NEWLY_REGISTERED_INTERVAL_MS;\n  public static final int DEFAULT_RM_NODEMANAGER_CONNECT_RETRIES;\n  public static final String DEFAULT_RM_NODEMANAGER_UNTRACKED_NODE_SELECTIVE_STATES_TO_REMOVE;\n  public static final long DEFAULT_RM_NODEMANAGER_UNTRACKED_REMOVAL_TIMEOUT_MSEC;\n  public static final int DEFAULT_RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE;\n  public static final int DEFAULT_RM_PLACEMENT_CONSTRAINTS_RETRY_ATTEMPTS;\n  // ... other fields\n}\n```\n\n----------------------------------------\n\nTITLE: MapReduce Distributed Cache Support\nDESCRIPTION: Support for the distributed cache system allowing MR jobs to create temporary files during task execution using java.io.File.createTempFile.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\njava.io.File.createTempFile\n```\n\n----------------------------------------\n\nTITLE: Adding createNonRecursive API to LocalFileSystem in Java\nDESCRIPTION: This improvement adds a new API createNonRecursive to the LocalFileSystem class in Hadoop. It allows creating files without automatically creating parent directories in the local file system.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-6886. LocalFileSystem Needs createNonRecursive API.\n(Nicolas Spiegelberg and eli via eli)\n```\n\n----------------------------------------\n\nTITLE: Limiting QuickSort Recursion Depth (Java)\nDESCRIPTION: Limits the recursion depth for QuickSort to prevent StackOverflowErrors. Switches to HeapSort when partitioning depth exceeds a multiple of log(n) to avoid O(n*n) cases.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3442. Limit recursion depth on the stack for QuickSort to prevent\nStackOverflowErrors. To avoid O(n*n) cases, when partitioning depth exceeds\na multiple of log(n), change to HeapSort. (cdouglas)\n```\n\n----------------------------------------\n\nTITLE: Configuring Out-of-Band Heartbeat for TaskTracker in MapReduce\nDESCRIPTION: Configuration setting to enable out-of-band heartbeats from the TaskTracker to the JobTracker upon task completion, which helps improve job latency by notifying the JobTracker about completed tasks more promptly.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: configuration\nCODE:\n```\nmapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN Application Classpath\nDESCRIPTION: Default configuration value for YARN application classpath in YarnConfiguration, referenced in MAPREDUCE-4422.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nYARN_APPLICATION_CLASSPATH\n```\n\n----------------------------------------\n\nTITLE: Supporting Snappy Compression in Hadoop Java API\nDESCRIPTION: This feature adds support for Snappy compression in Hadoop, providing an additional option for data compression in the framework.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nCompressionCodec.createCompressor(\"snappy\")\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Speculative Execution\nDESCRIPTION: Updates to hadoop-default.xml configuration to provide separate controls for speculative execution of map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: XML\nCODE:\n```\ndeprecated mapred.speculative.execution\nmapred.map.tasks.speculative.execution\nmapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Fixing TestLocalFsFCStatistics timeout (Java)\nDESCRIPTION: Addresses HADOOP-12706 where TestLocalFsFCStatistics#testStatisticsThreadLocalDataCleanUp times out occasionally. This fix improves the reliability of the test.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-12706. TestLocalFsFCStatistics#testStatisticsThreadLocalDataCleanUp times out occasionally\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Limits in Hadoop XML\nDESCRIPTION: This snippet shows configuration changes in hadoop-default.xml to support different numbers of mappers and reducers per TaskTracker, allowing better utilization of heterogeneous clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_31\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.tasktracker.map.tasks.maximum (default value of 2)\nadd mapred.tasktracker.reduce.tasks.maximum (default value of 2)\nremove mapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Implementing listStatusIterator and listLocatedStatus for ViewFs (Java)\nDESCRIPTION: Adds implementations for listStatusIterator and listLocatedStatus methods in ViewFs, improving file system operations for federated namespaces.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// Code not provided in change log\n```\n\n----------------------------------------\n\nTITLE: MapReduce Child Process Configuration Properties\nDESCRIPTION: Configuration properties for separately controlling map and reduce JVM parameters, environment variables, and ulimit settings. Introduces new specific properties while deprecating generic ones.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\nmapred.map.child.java.opts\nmapred.reduce.child.java.opts\nmapred.map.child.env\nmapred.reduce.child.ulimit\nmapred.map.child.env\nmapred.reduce.child.ulimit\n# deprecated\nmapred.child.java.opts\nmapred.child.env\nmapred.child.ulimit\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Common Data Format Options\nDESCRIPTION: Example of Hadoop Common configuration property for controlling data format options. This snippet shows the configuration for compression codecs, which specifies implementations for different compression formats.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>io.compression.codecs</name>\n  <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,\n      org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,\n      org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec\n  </value>\n  <description>A list of the compression codec classes that can be used \n               for compression/decompression. In addition to any classes specified\n               with this property (which take precedence), codec classes on the\n               classpath are discovered using a Java ServiceLoader.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring LDAP Group Mapping Provider in core-site.xml\nDESCRIPTION: XML configuration for setting up LDAP Group Mapping Service in Hadoop. This configuration specifies the LDAP provider class that maps users to groups for Hadoop security authorization.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.security.group.mapping</name>\n  <value>org.apache.hadoop.security.LdapGroupsMapping</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Adding JVM Reuse Configuration in Hadoop (Java/XML)\nDESCRIPTION: Adds a new configuration property 'mapred.job.reuse.jvm.num.tasks' to hadoop-default.xml to enable JVM reuse across Map-Reduce tasks for improved performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: XML\nCODE:\n```\nConfiguration changes to hadoop-default.xml:\n  add mapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Line Length in LineRecordReader (Java)\nDESCRIPTION: Sets a maximum line length for LineRecordReader to avoid reading too far into the following split. The limit is defined by the 'mapred.linerecordreader.maxlength' configuration property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration conf = new Configuration();\nconf.setLong(\"mapred.linerecordreader.maxlength\", maxLineLength);\n```\n\n----------------------------------------\n\nTITLE: Adding mapred.tasktracker Configuration Properties in hadoop-default.xml\nDESCRIPTION: Configuration changes that replace mapred.tasktracker.tasks.maximum with separate properties for map and reduce tasks, allowing better configuration for heterogeneous clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.tasktracker.map.tasks.maximum (default value of 2)\nadd mapred.tasktracker.reduce.tasks.maximum (default value of 2)\nremove mapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Using WritableComparator in Hadoop\nDESCRIPTION: Modified to only create key type instances if no WritableComparator is defined. Calling superclass compare throws NullPointerException. Defines RawComparator for NullWritable and allows it as SequenceFile key.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\npublic class MyKeyComparator extends WritableComparator {\n  protected MyKeyComparator() {\n    super(MyKey.class, true);\n  }\n  \n  @Override\n  public int compare(WritableComparable a, WritableComparable b) {\n    // Custom comparison logic\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Block Size Limit and Minimum in HDFS Configuration\nDESCRIPTION: Adds configuration options to set a limit on the number of blocks per file and a minimum block size in HDFS. This provides more control over block allocation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-4305. Add a configurable limit on number of blocks per file, and min block size.\n```\n\n----------------------------------------\n\nTITLE: Configuring LongBytes Handling in Hadoop Configuration (Java)\nDESCRIPTION: Adds support for handling human readable byte size values in Hadoop Configuration using the new getLongBytes method.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration.getLongBytes\n```\n\n----------------------------------------\n\nTITLE: Setting Fair Scheduler Map and Reduce Assignment in Java\nDESCRIPTION: Configures the Fair scheduler to assign both a map and a reduce task on each heartbeat by default. This improves task assignment efficiency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nsetAssignMultiple(true)\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration XML Changes for Job Status Persistence\nDESCRIPTION: Configuration additions to hadoop-default.xml for persisting job statuses in HDFS, enabling query of decommissioned jobs and survival across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_29\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.tracker.persist.jobstatus.active (default value of false)\nmapred.job.tracker.persist.jobstatus.hours (default value of 0)\nmapred.job.tracker.persist.jobstatus.dir (default value of /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Configuring IO Sort Parameters in Hadoop (Java)\nDESCRIPTION: This snippet introduces two new configuration variables for optimizing map output sorting in Hadoop. 'io.sort.spill.percent' defines the percentage of io.sort.mb that triggers a spill, while 'io.sort.record.percent' specifies the percentage of io.sort.mb allocated for key/value indexes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nio.sort.spill.percent - the percentages of io.sort.mb that should\n                            cause a spill (default 80%)\nio.sort.record.percent - the percent of io.sort.mb that should\n                             hold key/value indexes (default 5%)\n```\n\n----------------------------------------\n\nTITLE: MapReduce Configuration Properties Sample\nDESCRIPTION: Configuration properties for setting separate JVM parameters, environment variables and ulimit for map and reduce tasks. Introduces new configuration parameters while deprecating older ones.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nmapred.map.child.java.opts\nmapred.reduce.child.java.opts\nmapred.map.child.env\nmapred.reduce.child.ulimit\nmapred.map.child.env\nmapred.reduce.child.ulimit\n# Deprecated:\nmapred.child.java.opts\nmapred.child.env\nmapred.child.ulimit\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Logging Levels in Java\nDESCRIPTION: This snippet introduces new configuration options to set logging levels for map and reduce tasks separately in MapReduce jobs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nadd mapred.map.child.log.level \nadd mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Implementing RetryCache for RPC Retries (Java)\nDESCRIPTION: Adds a RetryCache utility for implementing RPC retries. This helps in improving the reliability of remote procedure calls in Hadoop by providing a standardized retry mechanism.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nRetryCache\n```\n\n----------------------------------------\n\nTITLE: MapReduce Task Logging Configuration Properties\nDESCRIPTION: Configuration properties for controlling logging levels of map and reduce tasks independently.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: properties\nCODE:\n```\nmapred.map.child.log.level\nmapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Improving FairScheduler Shutdown (Java)\nDESCRIPTION: Addresses a problem where the ContinuousSchedulingThread in FairScheduler could fail to shutdown properly.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/yarn/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3697. FairScheduler: ContinuousSchedulingThread can fail to shutdown.\\n(Zhihai Xu via kasha)\n```\n\n----------------------------------------\n\nTITLE: Setting Hadoop SSL Configuration Properties\nDESCRIPTION: Properties for configuring SSL/TLS encryption for Hadoop services. These settings enable secure communication channels between Hadoop components.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.5/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nhadoop.ssl.enabled=true\nhadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory\nhadoop.ssl.require.client.cert=false\nhadoop.ssl.hostname.verifier=DEFAULT\n```\n\n----------------------------------------\n\nTITLE: Adding Manual Block Report Trigger in HDFS\nDESCRIPTION: This new feature allows system administrators to manually trigger full block reports from a DataNode. It was added in version 2.6.1.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7278. Add a command that allows sysadmins to manually trigger full block reports from a DN (cmccabe)\n```\n\n----------------------------------------\n\nTITLE: Implementing RetryCache Utility in Java\nDESCRIPTION: Adds a RetryCache utility class for implementing RPC retries.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nRetryCache\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration Parameter for Out-of-Band Heartbeat in MapReduce\nDESCRIPTION: Configuration change to add the mapreduce.tasktracker.outofband.heartbeat parameter, which enables the tasktracker to optionally send an out-of-band heartbeat on task-completion for better job-latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nmapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes to hadoop-default.xml for Job Status Persistence\nDESCRIPTION: Updates to hadoop-default.xml that add configuration properties for persisting job statuses in HDFS, including activation flag, retention period, and storage directory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_30\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                                /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Configuring Speculative Execution in Hadoop XML\nDESCRIPTION: This snippet shows configuration changes to hadoop-default.xml for controlling speculative execution. It deprecates an old property and adds new properties for finer-grained control over map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: XML\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Configuration Property Definition for MALLOC_ARENA_MAX\nDESCRIPTION: Configuration code snippet for setting MALLOC_ARENA_MAX environment variable to address memory allocation issues in MapReduce tasks\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nMALLOC_ARENA_MAX\n```\n\n----------------------------------------\n\nTITLE: Optimizing HDFS Edits Log Creation\nDESCRIPTION: Tool for generating realistic edit logs for testing and development purposes. This optimization improves the performance of OP_ADD operations during edits loading.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nHDFS-2886. CreateEditLogs should generate a realistic edit log.\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration XML Changes for Speculative Execution\nDESCRIPTION: Configuration updates to hadoop-default.xml that provide finer-grained control over speculative execution for map and reduce tasks separately.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_28\n\nLANGUAGE: xml\nCODE:\n```\nmapred.speculative.execution (deprecated)\nmapred.map.tasks.speculative.execution\nmapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: JVM Reuse Configuration in Hadoop Map-Reduce\nDESCRIPTION: Configuration parameter added to hadoop-default.xml that controls the number of tasks for which a JVM can be reused in Map-Reduce operations, improving resource utilization.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: config\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Configuring Field Separators for Streaming Jobs in Hadoop\nDESCRIPTION: Defines new configuration values for specifying field separators in Hadoop streaming jobs. These settings allow customization of input and output field separators for both map and reduce phases.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Properties\nCODE:\n```\nstream.map.input.field.separator\nstream.map.output.field.separator\nstream.reduce.input.field.separator\nstream.reduce.output.field.separator\n```\n\n----------------------------------------\n\nTITLE: Adding Generation Stamp to Block Objects (Java)\nDESCRIPTION: Modifies the Block object to include a generation stamp, with existing blocks getting a stamp of 0. This change is necessary to support append operations in HDFS.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-2656.  The Block object has a generation stamp inside it.\nExisting blocks get a generation stamp of 0. This is needed to support\nappends.\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Manager Queue Properties in Hadoop\nDESCRIPTION: Defines a naming convention for configuring resource manager queue properties in Hadoop. Queue-specific properties follow the pattern 'hadoop.rm.queue.queue-name.property-name'.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: XML\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Setting MALLOC_ARENA_MAX Environment Variable in Hadoop\nDESCRIPTION: Sets the MALLOC_ARENA_MAX environment variable in the hadoop-env.sh script to optimize memory allocation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\nexport MALLOC_ARENA_MAX=4\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Hadoop Components\nDESCRIPTION: Example of new environment variable HADOOP_NAMENODE_OPTS for component-specific configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: Bash\nCODE:\n```\nHADOOP_NAMENODE_OPTS\n```\n\n----------------------------------------\n\nTITLE: Implementing FileSystem.newInstance() in Java\nDESCRIPTION: Adds a new method FileSystem.newInstance() that always returns a newly allocated FileSystem object. This allows creating fresh FileSystem instances.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.newInstance()\n```\n\n----------------------------------------\n\nTITLE: Improving StringUtils Methods in Java\nDESCRIPTION: This snippet addresses race conditions and unnecessary synchronization in StringUtils methods. It improves the humanReadableInt() method to avoid race conditions and removes synchronization from limitDecimalTo2(double).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9252. In StringUtils, humanReadableInt(..) has a race condition and\nthe synchronization of limitDecimalTo2(double) can be avoided.  (szetszwo)\n```\n\n----------------------------------------\n\nTITLE: MapReduce output file naming conventions in new API\nDESCRIPTION: This snippet explains the file naming convention for MapReduce output files in the new API. Reducer outputs use 'part-r-NNNNN' format while mapper outputs use 'part-m-NNNNN' format.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: text\nCODE:\n```\npart-r-00000\n```\n\nLANGUAGE: text\nCODE:\n```\npart-m-00000\n```\n\n----------------------------------------\n\nTITLE: Fixing BZip2 record handling for small input splits (Java)\nDESCRIPTION: Resolves an issue where BZip2 was dropping and duplicating records when input split size is small. This fix improves data integrity when processing BZip2 compressed files.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n// Code not provided in change log\n```\n\n----------------------------------------\n\nTITLE: Adding Avro jar to Eclipse classpath in Hadoop MapReduce\nDESCRIPTION: Adds the Avro jar file to the Eclipse classpath for the MapReduce project to ensure proper compilation and execution within the Eclipse IDE.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: XML\nCODE:\n```\n<classpath>\n    <!-- ... other classpath entries ... -->\n    <classpathentry kind=\"lib\" path=\"path/to/avro.jar\"/>\n    <!-- ... -->\n</classpath>\n```\n\n----------------------------------------\n\nTITLE: Configuring Capacity Scheduler Default Queue Settings\nDESCRIPTION: Example configuration for the default queue properties in capacity-scheduler.xml. This includes capacity, maximum capacity, user-limit-factor, and state settings for the default queue.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>yarn.scheduler.capacity.root.default.capacity</name>\n  <value>100</value>\n  <description>Default queue target capacity.</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>\n  <value>1</value>\n  <description>Default queue user limit a percentage from 0.0 to 1.0.</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>\n  <value>100</value>\n  <description>The maximum capacity of the default queue.</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.root.default.state</name>\n  <value>RUNNING</value>\n  <description>The state of the default queue. State can be one of RUNNING or STOPPED.</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>\n  <value>*</value>\n  <description>The ACL of who can submit jobs to the default queue.</description>\n</property>\n\n<property>\n  <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>\n  <value>*</value>\n  <description>The ACL of who can administer jobs on the default queue.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Implementing NetworkTopologyWithNodeGroup in Java\nDESCRIPTION: Adds a new 4-layer implementation of NetworkTopology called NetworkTopologyWithNodeGroup to support more granular network topology configurations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nNetworkTopologyWithNodeGroup\n```\n\n----------------------------------------\n\nTITLE: StringUtils Split Implementation\nDESCRIPTION: Addition of split method in StringUtils for handling non-escaped single-character separator\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nStringUtils.split(String str, char separator)\n```\n\n----------------------------------------\n\nTITLE: Adding LZ4 Compression Support in Java\nDESCRIPTION: Adds support for LZ4 compression to Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7657. Add support for LZ4 compression. (Binglin Chang via todd)\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Job Status Persistence\nDESCRIPTION: Configuration properties added to hadoop-default.xml for persisting job statuses in HDFS. Includes settings for activation, retention period, and storage directory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.tracker.persist.jobstatus.active (default value of false)\nmapred.job.tracker.persist.jobstatus.hours (default value of 0)\nmapred.job.tracker.persist.jobstatus.dir (default value of /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Implementing Truncate API in Java\nDESCRIPTION: Addition of truncate functionality to HDFS, including support for snapshots and exposing the API through Web HDFS, HDFS httpfs, and libhdfs interfaces.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-3107. Introduce truncate. (Plamen Jeliazkov via shv)\\n\\nHDFS-7056. Snapshot support for truncate. (Plamen Jeliazkov and shv)\\n\\nHDFS-7655. Expose truncate API for Web HDFS. (yliu)\\n\\nHDFS-7656. Expose truncate API for HDFS httpfs. (yliu)\\n\\nHDFS-7838. Expose truncate API for libhdfs. (yliu)\n```\n\n----------------------------------------\n\nTITLE: Configuration Change for CapacityScheduler Maximum Resource Percentage\nDESCRIPTION: Configuration addition for the CapacityScheduler to cap concurrently running applications per-queue and per-user. The parameter sets the maximum percentage of cluster resources that can be allocated to ApplicationMasters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nadd yarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Implementing Pluggable Trash Policies in Java\nDESCRIPTION: Adds support for pluggable trash policies, allowing custom implementations of trash behavior. This provides more flexibility in how deleted files are handled.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\npublic interface TrashPolicy {\n  void initialize(Configuration conf, FileSystem fs, Path home);\n  void moveToTrash(Path path) throws IOException;\n  void createCheckpoint() throws IOException;\n  void deleteCheckpoint() throws IOException;\n  Path getCurrentTrashDir();\n  boolean isEnabled();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes to hadoop-default.xml for Speculative Execution\nDESCRIPTION: Updates to hadoop-default.xml that deprecate the general speculative execution property and add specific properties for map and reduce tasks speculative execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_29\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes to hadoop-default.xml for Speculative Execution\nDESCRIPTION: Updates to hadoop-default.xml that deprecate the general speculative execution property and add specific properties for map and reduce tasks speculative execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_29\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Setting Executable Bit on Generated Files in Bash\nDESCRIPTION: Ensures that configure files generated as part of the released tarball have the executable bit set.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\nconfigure files that are generated as part of the released\ntarball need to have executable bit set.\n```\n\n----------------------------------------\n\nTITLE: Improving DataNode Directory Addition in HDFS\nDESCRIPTION: This improvement makes adding a new data directory to the DataNode an atomic operation and enhances error handling. It was implemented in version 2.6.1.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7035. Make adding a new data directory to the DataNode an atomic operation and improve error handling (Lei Xu via Colin P. McCabe)\n```\n\n----------------------------------------\n\nTITLE: Implementing BZip2 Codec for MapReduce Processing\nDESCRIPTION: Adds a BZip2 compatible codec to allow processing of BZip2 compressed data in MapReduce jobs, expanding the types of compressed input that can be handled efficiently.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nAdd a bzip2 compatible codec, so bzip compressed data may be processed by map/reduce.\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Limits in Hadoop\nDESCRIPTION: Configuration changes to hadoop-default.xml to support different numbers of mappers and reducers per TaskTracker. This allows administrators to better configure and utilize heterogeneous clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.tasktracker.map.tasks.maximum (default value of 2)\nadd mapred.tasktracker.reduce.tasks.maximum (default value of 2)\nremove mapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for JNI and Library Path Dependencies\nDESCRIPTION: Several CMake configuration updates to fix JNI dependencies, ARM builds, and version compatibility issues. Includes changes to properly locate and link against libjvm.so, jni.h, and handle platform-specific requirements.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# HADOOP-8737\nalways use JAVA_HOME to find libjvm.so, jni.h, jni_md.h\n\n# HADOOP-8747\nSyntax error fix on cmake version 2.6 patch 2 in JNIFlags.cmake\n\n# HADOOP-8764\nFix for ARM build compatibility\n```\n\n----------------------------------------\n\nTITLE: Configuring Map-Reduce Slow Start Parameter in mapred-default.xml\nDESCRIPTION: Configuration change to mapred-default.xml that adds a new parameter 'mapred.reduce.slowstart.completed.maps' which ensures reduces aren't scheduled until a sufficient number of maps are completed, preventing reduce tasks from swamping the cluster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.reduce.slowstart.completed.maps\n```\n\n----------------------------------------\n\nTITLE: Improving Configuration.getClassByName Performance in Java\nDESCRIPTION: This snippet improves the performance of Configuration.getClassByName when the class is not found by caching negative results. It addresses HADOOP-6502.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-6502. Improve the performance of Configuration.getClassByName when\nthe class is not found by caching negative results.\n(sharad, todd via todd)\n```\n\n----------------------------------------\n\nTITLE: Adding SequenceFileAsBinaryInputFormat in Java\nDESCRIPTION: Introduces a new InputFormat that reads sequence files as BytesWritable/BytesWritable regardless of the key and value types used to write the file. This allows for flexible processing of sequence files.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nSequenceFileAsBinaryInputFormat\n```\n\n----------------------------------------\n\nTITLE: Supplying MapReduce JARs in Classpath from Application Master in Java\nDESCRIPTION: Modifies the MapReduce Application Master to provide MapReduce JARs in the classpath instead of relying on YARN, improving dependency management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-4638. MR AM supplies MapReduce jars in classpath rather than rely on YARN.\n```\n\n----------------------------------------\n\nTITLE: Fixing MapReduce Task Status String Length\nDESCRIPTION: Support for limiting task status string length and number of block locations in MapReduce branch-2, referenced in MAPREDUCE-4146.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nmapreduce.job.counters.limit\n```\n\n----------------------------------------\n\nTITLE: Addressing Dual Active RM State (Java)\nDESCRIPTION: Fixes an issue where both Resource Managers could be in active state when Admin#transitionToActive fails during refreshAll().\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/yarn/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3893. Both RM in active state when Admin#transitionToActive failure \\nfrom refeshAll() (Bibin A Chundatt via rohithsharmaks)\n```\n\n----------------------------------------\n\nTITLE: Configuring Variable Expansion in Hadoop Configuration Files\nDESCRIPTION: Adds support for variable expansion in Hadoop configuration files. Variables are referenced using ${variable} syntax and are resolved from the configuration or Java system properties. The default configuration is modified to use ${hadoop.tmp.dir} for temporary directories.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: XML\nCODE:\n```\n${hadoop.tmp.dir}\n```\n\n----------------------------------------\n\nTITLE: Deprecating Configuration.getObject() Method in Hadoop\nDESCRIPTION: Deprecates the getObject() method in Configuration class and adds getRaw() method to skip variable expansion. This change is part of HADOOP-1197 to ensure Configurations only contain strings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\n// Deprecated\n// Configuration.getObject()\n\n// New method\nConfiguration.getRaw()\n```\n\n----------------------------------------\n\nTITLE: Performing Quick Check of Apache Hadoop Release with SHA-512 in Bash\nDESCRIPTION: This snippet shows the command to perform a quick integrity check of an Apache Hadoop release using SHA-512 checksum. It verifies the downloaded tarball against the provided checksum file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/releases.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nshasum -a 512 hadoop-X.Y.Z-src.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Status Persistence in Hadoop\nDESCRIPTION: Configuration changes to hadoop-default.xml to persist statuses of completed jobs in HDFS. This allows the JobClient to query information about decommissioned jobs and across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                                /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Updating ShuffleHandler Service Name in YARN Configuration\nDESCRIPTION: Changes the ShuffleHandler auxiliary service name from 'mapreduce.shuffle' to 'mapreduce_shuffle' to define constraints on Auxiliary Service names.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>yarn.nodemanager.aux-services</name>\n  <value>mapreduce_shuffle</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: YARN Configuration Fields with Missing Documentation\nDESCRIPTION: Collection of Java configuration fields from YarnConfiguration class that are missing proper documentation blocks or @since version tags. These fields control various aspects of YARN including Node Manager settings, Resource Manager behavior, and system configurations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n// Examples of undocumented fields\norg.apache.hadoop.yarn.conf.YarnConfiguration {\n    NM_SCRIPT_BASED_NODE_ATTRIBUTES_PROVIDER_OPTS\n    NM_SCRIPT_BASED_NODE_ATTRIBUTES_PROVIDER_PATH\n    NODE_RESOURCES_CONFIGURATION_FILE\n    RM_APPLICATION_MASTER_SERVICE_PROCESSORS\n    RM_SCHEDULER_MUTATION_ACL_POLICY_CLASS\n}\n```\n\n----------------------------------------\n\nTITLE: Referencing Configuration Variables in Hadoop\nDESCRIPTION: This snippet demonstrates how to reference variables in Hadoop configuration files. Variables are enclosed in ${} syntax and can be sourced from the configuration or Java system properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_34\n\nLANGUAGE: Properties\nCODE:\n```\n${hadoop.tmp.dir}\n```\n\n----------------------------------------\n\nTITLE: Referencing Configuration Variables in Hadoop\nDESCRIPTION: Demonstrates how to reference variables in Hadoop configuration files using the ${variable} syntax. Variables are resolved first from the configuration, then from Java system properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_31\n\nLANGUAGE: Properties\nCODE:\n```\n${hadoop.tmp.dir}\n```\n\n----------------------------------------\n\nTITLE: Shell environment configuration\nDESCRIPTION: Update to hadoop-config.sh to add Java library path to LD_LIBRARY_PATH for proper native library loading.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\n# HADOOP-8781\nhadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: MapReduce Configuration Change for Out-of-band Heartbeat\nDESCRIPTION: Configuration property added to optionally enable out-of-band heartbeat on task completion for better job latency in TaskTracker.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nadd mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Updating FileSystem Cache Key in Java\nDESCRIPTION: Updates the FileSystem cache key after a FileSystem object is created. This change affects how FileSystem instances are cached and retrieved.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.CACHE.put(key, newValue);\n```\n\n----------------------------------------\n\nTITLE: Improving Protobuf version handling and detection in Hadoop build\nDESCRIPTION: This enhancement improves how Hadoop detects and handles different versions of Protocol Buffers during the build process. It makes the build system more robust and flexible.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: XML\nCODE:\n```\nHADOOP-9872. Improve protoc version handling and detection. (tucu)\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Block Size with Size Prefixes\nDESCRIPTION: Adds support for specifying the HDFS block size using size-indicating prefixes like K, M, G etc. This improves the readability and usability of the dfs.blocksize configuration parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-1314. Make dfs.blocksize accept size-indicating prefixes.\n```\n\n----------------------------------------\n\nTITLE: Implementing fsync for HDFS Files (Java)\nDESCRIPTION: Enhances fsync operations on HDFS files to ensure data persistence. Datanodes now move blocks from the tmp directory to the real block directory on restart.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3113. An fsync invoked on a HDFS file really really\npersists data! The datanode moves blocks in the tmp directory to \nthe real block directory on a datanode-restart.\n```\n\n----------------------------------------\n\nTITLE: Implementing Merge API for MapFile in Java\nDESCRIPTION: Provides a Merge API for MapFile to merge multiple similar MapFiles into one MapFile. This new feature was introduced in Hadoop 2.7.0.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11569. Provide Merge API for MapFile to merge multiple similar MapFiles to one MapFile. (Vinayakumar B via ozawa)\n```\n\n----------------------------------------\n\nTITLE: Terminating Hadoop Task Attempts via Shell Commands\nDESCRIPTION: These commands allow terminating specific task attempts in a Hadoop job. The -fail-task option fails a task, while -kill-task forcefully kills it.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\nbin/hadoop job [-fail-task|-kill-task]\n```\n\n----------------------------------------\n\nTITLE: Deploying Hadoop Site to Staging with Maven Command\nDESCRIPTION: Command to deploy the built Hadoop website to the staging area. This requires write access to the SVN repository.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean site-deploy -Dstaging\n```\n\n----------------------------------------\n\nTITLE: Configuration for JobTracker Job Status Persistence\nDESCRIPTION: Configuration changes to hadoop-default.xml that enable persisting completed job statuses in HDFS. This allows JobClient to query information about decommissioned jobs and retrieve job information across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                                /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Adding isRunning() Method to AbstractDelegationTokenSecretManager in Hadoop\nDESCRIPTION: This change adds an isRunning() method to the AbstractDelegationTokenSecretManager class in Hadoop. It's part of improvements related to delegation token management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nisRunning()\n```\n\n----------------------------------------\n\nTITLE: Improving errno reporting in libhdfs (C)\nDESCRIPTION: Enhances error reporting in libhdfs by providing more meaningful errno values. Specifically, EACCES is now returned for permission-related issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: C\nCODE:\n```\nHADOOP-3549. Give more meaningful errno's in libhdfs. In particular, \\nEACCES is returned for permission problems. (Ben Slusky via omalley)\n```\n\n----------------------------------------\n\nTITLE: Speculative Execution Configuration Parameters\nDESCRIPTION: New configuration parameters in hadoop-default.xml for controlling speculative execution separately for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\nmapred.speculative.execution (deprecated)\nmapred.map.tasks.speculative.execution\nmapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Configuring File Listing in Java\nDESCRIPTION: Adds utility methods in FileUtil to handle directory listing and avoid NPEs when using File.list() and File.listFiles(). This improves robustness when working with file system operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nFileUtil.list(File dir)\nFileUtil.listFiles(File dir)\n```\n\n----------------------------------------\n\nTITLE: Updating Protobuf version in Hadoop build\nDESCRIPTION: This change updates the Protocol Buffers (protobuf) library used in Hadoop from version 2.4.x to 2.5. It ensures Hadoop is using the latest features and improvements in protobuf.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: XML\nCODE:\n```\nHADOOP-9845. Update protobuf to 2.5 from 2.4.x. (tucu)\n```\n\n----------------------------------------\n\nTITLE: Reading Files with Conditional Return in Java\nDESCRIPTION: Implementation of SocketInputStream.read() method that returns -1 when reaching the end of file (EOF). This change corrects behavior to follow standard Java InputStream conventions.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nSocketInputStream.read() should return -1 in case EOF.\n```\n\n----------------------------------------\n\nTITLE: Configuring mapred.reduce.slowstart.completed.maps in Java\nDESCRIPTION: Added a configuration parameter to control when reduce tasks start based on the number of completed map tasks. This helps ensure that reduces aren't started too early, which could swamp the cluster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\nmapred.reduce.slowstart.completed.maps\n```\n\n----------------------------------------\n\nTITLE: Code Reference in Commit Messages\nDESCRIPTION: This content is a changelog and doesn't contain actual code snippets. It contains references to code changes but no actual code blocks are present.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_11\n\n\n\n----------------------------------------\n\nTITLE: Setting JAR from Distributed Cache in MapReduce Job (Java)\nDESCRIPTION: Allows jobs to set a JAR that is in the distributed cache, improving flexibility for job configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\njob.setJar(\"path/to/jar/in/distributed/cache.jar\");\n```\n\n----------------------------------------\n\nTITLE: Implementing getDelegationTokens API in FileSystem for Java\nDESCRIPTION: This new API in the FileSystem class checks for known tokens in the passed Credentials object. It enhances token management and security in Hadoop file systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7933. Add a getDelegationTokens api to FileSystem which checks\nfor known tokens in the passed Credentials object. (sseth)\n```\n\n----------------------------------------\n\nTITLE: Modifying CMake Build Process for Java Libraries\nDESCRIPTION: Updates the CMake build process to consistently use JAVA_HOME for locating libjvm.so, jni.h, and jni_md.h, ensuring proper Java library integration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ncmake: always use JAVA_HOME to find libjvm.so, jni.h, jni_md.h.\n```\n\n----------------------------------------\n\nTITLE: Configuring MiniCluster ResourceManager Port (Java)\nDESCRIPTION: Allows users to specify the port used by the ResourceManager in MiniCluster for testing purposes. This provides more control over the test environment setup.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nMiniCluster ResourceManager\n```\n\n----------------------------------------\n\nTITLE: Supporting HTTP REST in HttpServer\nDESCRIPTION: Adds support for HTTP REST operations in the HttpServer class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7594. Support HTTP REST in HttpServer.\n```\n\n----------------------------------------\n\nTITLE: Configuring Node Manager Debug Delay (Java)\nDESCRIPTION: Allows setting the yarn.nodemanager.delete.debug-delay-sec property to -1 for easier debugging of containers. This prevents containers from being deleted immediately after completion.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nyarn.nodemanager.delete.debug-delay-sec\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Levels for Map and Reduce Tasks in Hadoop MapReduce\nDESCRIPTION: Adds new configuration options to set logging levels separately for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Properties\nCODE:\n```\nmapred.map.child.log.level\nmapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Implementing HsftpFileSystem in Java\nDESCRIPTION: Adds HsftpFileSystem to enable file transfers over SSL, enhancing security for file operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n// Implementation of HsftpFileSystem for SSL file transfers\n```\n\n----------------------------------------\n\nTITLE: Enhancing FsShell Text Command for Avro Files\nDESCRIPTION: This improvement allows the FsShell '-text' command to read Avro files stored in HDFS and other filesystems, expanding its functionality to support this data format.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9740. Fix FsShell '-text' command to be able to read Avro\n    files stored in HDFS and other filesystems. (Allan Yan via jlowe)\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Client-to-ApplicationMaster Communication Retries in Java\nDESCRIPTION: Configuration property for setting the maximum number of retries for IPC communication between the MapReduce client and the ApplicationMaster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nMRJobConfig.MR_CLIENT_TO_AM_IPC_MAX_RETRIES_ON_TIMEOUTS\n```\n\n----------------------------------------\n\nTITLE: Implementing HsftpFileSystem in Java\nDESCRIPTION: Adds HsftpFileSystem to enable file transfers over SSL, enhancing security for file operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n// Implementation of HsftpFileSystem for SSL file transfers\n```\n\n----------------------------------------\n\nTITLE: Optimizing JobTracker and JobInProgress Counter Retrieval in Java\nDESCRIPTION: This optimization reduces the lock acquisition time for retrieving job counters in JobTracker.getJobCounters() and JobInProgress.getCounters() methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nMake JobTracker.getJobCounters() and JobInProgress.getCounters() aquire locks in a shorter time period.\n```\n\n----------------------------------------\n\nTITLE: Deprecating Configuration.getObject() and Adding getRaw() in Java\nDESCRIPTION: Deprecates the getObject() method in Configuration class and adds a new getRaw() method that skips variable expansion. This change aims to ensure that Configurations only contain strings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_28\n\nLANGUAGE: Java\nCODE:\n```\n// Deprecated\nConfiguration.getObject()\n\n// New method\nConfiguration.getRaw()\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Configuration in core-default.xml\nDESCRIPTION: Adds new configuration properties 'rpc.metrics.quantile.enable' and 'rpc.metrics.percentiles.intervals' to the core-default.xml file for RPC metrics.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<!-- Implied XML snippet -->\n<property>\n  <name>rpc.metrics.quantile.enable</name>\n  <value>true</value>\n</property>\n<property>\n  <name>rpc.metrics.percentiles.intervals</name>\n  <value>60,300,900</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Implementing AccessControlList as Writable in Java\nDESCRIPTION: Makes the AccessControlList class implement the Writable interface, allowing it to be easily serialized and deserialized. This change is part of updating the Job ACLs feature.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\npublic class AccessControlList implements Writable {\n  // Implementation details\n}\n```\n\n----------------------------------------\n\nTITLE: Removing Duplicate FileSystem.listStatus() Call in FsShell Java Class\nDESCRIPTION: Eliminates a redundant call to FileSystem.listStatus() in the FsShell class to improve performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: java\nCODE:\n```\n// Removed duplicate call\n// FileSystem.listStatus(path);\n```\n\n----------------------------------------\n\nTITLE: Configuring Out-of-Band Heartbeat for TaskTracker in Java\nDESCRIPTION: Adds a new configuration property to optionally enable out-of-band heartbeats from TaskTracker to JobTracker on task completion, improving job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nmapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: YARN Configuration Field References\nDESCRIPTION: List of YARN configuration fields from YarnConfiguration class that are missing proper documentation blocks or @since tags. These fields control various aspects of YARN including Timeline Service, security settings, container management, and resource allocation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.conf.YarnConfiguration.SCHEDULER_CONFIGURATION_FS_MAX_VERSION\norg.apache.hadoop.yarn.conf.YarnConfiguration.SCHEDULER_CONFIGURATION_FS_PATH\norg.apache.hadoop.yarn.conf.YarnConfiguration.SCHEDULER_CONFIGURATION_STORE_CLASS\norg.apache.hadoop.yarn.conf.YarnConfiguration.SCHEDULER_RM_PLACEMENT_CONSTRAINTS_HANDLER\norg.apache.hadoop.yarn.conf.YarnConfiguration.SCRIPT_NODE_DESCRIPTOR_PROVIDER\n```\n\n----------------------------------------\n\nTITLE: Default Temp Directory Configuration\nDESCRIPTION: Shows the default temporary directory configuration pattern using variables. The temp directory is set under ${hadoop.tmp.dir} which defaults to /tmp/hadoop-${user.name}.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_31\n\nLANGUAGE: Properties\nCODE:\n```\n/tmp/hadoop-${user.name}\n```\n\n----------------------------------------\n\nTITLE: Optimizing Local Renames in Java\nDESCRIPTION: Uses rename operation instead of copy for local file renames, potentially improving performance for local file operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n// Optimization: Use rename instead of copy for local renames\n```\n\n----------------------------------------\n\nTITLE: Configuring WebHDFS Retry Policy in Java\nDESCRIPTION: Adds configuration keys for setting the retry policy in WebHDFSFileSystem. This allows customizing how WebHDFS retries failed operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-5219. Add configuration keys for retry policy in WebHDFSFileSystem.\n```\n\n----------------------------------------\n\nTITLE: Implementing High Performance Secure Random Number Generation\nDESCRIPTION: This component implements high-performance secure random number sources, identified by HADOOP-10734. The change was contributed by Yi Liu and reviewed by Colin Patrick McCabe.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Implementing LevelDB-based RMStateStore in YARN\nDESCRIPTION: This new feature adds a LevelDB-based implementation for the RMStateStore, providing an alternative storage option for ResourceManager state.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nYARN-2765. Added leveldb-based implementation for RMStateStore. (Jason Lowe\nvia jianhe)\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration Properties in hadoop-env.sh\nDESCRIPTION: Environment variable settings for the hadoop-env.sh file that configures JVM options and memory allocation for Hadoop processes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_9\n\nLANGUAGE: properties\nCODE:\n```\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport HADOOP_HEAPSIZE=1000\nexport HADOOP_NAMENODE_OPTS=\"-Xmx1024m\"\nexport HADOOP_DATANODE_OPTS=\"-Xmx512m\"\nexport HADOOP_LOG_DIR=/var/log/hadoop\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Setting for Hadoop Client\nDESCRIPTION: Command setting HADOOP_CLIENT_OPTS environment variable in hadoop-env.cmd\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhadoop-env.cmd\n```\n\n----------------------------------------\n\nTITLE: Adding Generic Type Parameter to RetryInvocationHandler in Hadoop\nDESCRIPTION: Adds a generic type parameter to the RetryInvocationHandler class to improve type safety.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9803. Add a generic type parameter to RetryInvocationHandler.\n```\n\n----------------------------------------\n\nTITLE: Using Maven Version Plugin to Update Project Version\nDESCRIPTION: This command uses the Maven versions plugin to set a new version number for the project. It's useful for updating version numbers across multiple Maven modules consistently.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmvn versions:set -DnewVersion=VERSION\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Slow Start in Java\nDESCRIPTION: Adds a configuration parameter to control when reduce tasks start based on the number of completed map tasks. This helps prevent reduce tasks from overwhelming the cluster before sufficient map tasks are complete.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.reduce.slowstart.completed.maps\n```\n\n----------------------------------------\n\nTITLE: Fixing CryptoOutputStream Synchronization in Java\nDESCRIPTION: This bug fix makes CryptoOutputStream behave like DFSOutputStream with respect to synchronization. It addresses concurrency issues in the encrypted output stream implementation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11710. Make CryptoOutputStream behave like DFSOutputStream wrt synchronization. (Sean Busbey via yliu)\n```\n\n----------------------------------------\n\nTITLE: MapReduce Counter Enum Support\nDESCRIPTION: Implementation for supporting backward compatibility for mapred.Task.Counter and mapred.JobInProgress.Counter enums. This modification ensures compatibility of counters after MAPREDUCE-901.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-3794. Support mapred.Task.Counter and mapred.JobInProgress.Counter enums for compatibility\n```\n\n----------------------------------------\n\nTITLE: Build Script Updates\nDESCRIPTION: Updates to build configuration scripts including CMake changes for Mac OS X and shared library handling. Referenced in HADOOP-11154 and HADOOP-11250.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Referenced in HADOOP-11154 and HADOOP-11250 but actual code not shown in log\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for JVM Parameters\nDESCRIPTION: New configuration parameters introduced for separately controlling map and reduce JVM options, environment variables and ulimit settings. Deprecates older unified parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\nmapred.map.child.java.opts\nmapred.reduce.child.java.opts\nmapred.map.child.env\nmapred.reduce.child.ulimit\nmapred.map.child.env\nmapred.reduce.child.ulimit\n# Deprecated:\nmapred.child.java.opts\nmapred.child.env\nmapred.child.ulimit\n```\n\n----------------------------------------\n\nTITLE: Fixing race condition in MetricsSourceAdapter (Java)\nDESCRIPTION: Resolves a race condition in MetricsSourceAdapter.updateJmxCache to prevent potential concurrent modification issues in metrics collection.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n// Code not provided in change log\n```\n\n----------------------------------------\n\nTITLE: Configuring maximum line length for LineRecordReader in MapReduce\nDESCRIPTION: A new configuration property mapred.linerecordreader.maxlength is introduced to set the maximum allowed line length for LineRecordReader, improving its robustness.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>mapred.linerecordreader.maxlength</name>\n  <value>10000000</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring Variable Expansion in Hadoop Configuration Files\nDESCRIPTION: This change allows for variable expansion in Hadoop configuration files. Variables are referenced using ${variable} syntax and are resolved from the configuration or Java system properties. The default configuration is modified to use ${hadoop.tmp.dir} for temporary directories.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Properties\nCODE:\n```\n${hadoop.tmp.dir}\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Speculative Execution Control in Hadoop MapReduce\nDESCRIPTION: Configuration changes to hadoop-default.xml that provide finer-grained control over speculative execution in Hadoop MapReduce. Allows separate configuration for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Deprecating SequenceFile Compression Type Setting in Hadoop Java API\nDESCRIPTION: Deprecates the SequenceFile.setCompressionType method in favor of new methods for setting compression type on SequenceFile writers, output formats and job configurations. Also removes a related configuration property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nSequenceFile.setCompressionType // Deprecated\n\n// New methods:\nSequenceFile.createWriter\nSequenceFileOutputFormat.setCompressionType \nJobConf.setMapOutputCompressionType\n```\n\nLANGUAGE: XML\nCODE:\n```\n<!-- Deprecated configuration -->\n<property>\n  <name>io.seqfile.compression.type</name>\n  <value>deprecated</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Optimizing UTF8 string/byte conversions (Java)\nDESCRIPTION: Optimization to improve the performance of UTF8 string and byte conversions in HDFS operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-10662. Optimize UTF8 string/byte conversions.\n```\n\n----------------------------------------\n\nTITLE: Modifying CLASSPATH Export in Hadoop Shell Scripts\nDESCRIPTION: Changes the way CLASSPATH is set by removing it from the command line and exporting it in the environment instead.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n# Old way\n# hadoop command $CLASSPATH\n\n# New way\nexport CLASSPATH\nhadoop command\n```\n\n----------------------------------------\n\nTITLE: Optimizing BlockInfo Array Capacity Management\nDESCRIPTION: Uses System.arraycopy to improve performance when ensuring capacity in BlockInfo arrays. This optimization can speed up operations that manipulate block metadata.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-554. Use System.arraycopy in BlockInfo.ensureCapacity.\n```\n\n----------------------------------------\n\nTITLE: Fixing CMake Build for ARM Architecture in Java\nDESCRIPTION: This change modifies the CMake build system to always use JAVA_HOME to find libjvm.so, jni.h, and jni_md.h. This fixes build issues on ARM architecture.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncmake: always use JAVA_HOME to find libjvm.so, jni.h, jni_md.h.\n```\n\n----------------------------------------\n\nTITLE: Implementing Path Normalization Method\nDESCRIPTION: Code reference to using StringUtils instead of String#replace for path normalization, mentioned in HADOOP-6490 improvement.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nStringUtils.replace(path, \"//\", \"/\")\n```\n\n----------------------------------------\n\nTITLE: Fixing Lock Access in FSLeafQueue (Java)\nDESCRIPTION: Addresses an issue where app lists in FSLeafQueue were being accessed without required locks, potentially causing concurrency issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nYARN-2975. FSLeafQueue app lists are accessed without required locks. (kasha)\n```\n\n----------------------------------------\n\nTITLE: Deploying Hadoop Site to Production with Maven Command\nDESCRIPTION: Command to deploy the built Hadoop website to production. This requires write access to the SVN repository.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean site-deploy\n```\n\n----------------------------------------\n\nTITLE: Fixing File Handle Leak in HarFileSystem\nDESCRIPTION: This code fixes a file handle leak in the HarMetaData.parseMetaData() method of the HarFileSystem class. It improves resource management and prevents potential memory issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9278. Fix the file handle leak in HarMetaData.parseMetaData() in\nHarFileSystem. (Chris Nauroth via szetszwo)\n```\n\n----------------------------------------\n\nTITLE: Serializing Protocol Buffers with ObjectWritable in Java\nDESCRIPTION: Adds support for serializing and deserializing Protocol Buffer objects using ObjectWritable. This enables Protocol Buffers to be used more easily with Hadoop's serialization framework.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nObjectWritable.writeObject(DataOutput out, Object instance, Class<?> declaredClass, Configuration conf)\nObject ObjectWritable.readObject(DataInput in, ObjectWritable objectWritable, Configuration conf)\n```\n\n----------------------------------------\n\nTITLE: Handling Multibyte Delimiters in LineRecordReader for Java\nDESCRIPTION: This code snippet addresses an issue with LineRecordReader not handling multibyte record delimiters correctly. It improves the handling of uncompressed input and fixes issues with incomplete records and wrong position/key information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.mapred.LineRecordReader\n```\n\n----------------------------------------\n\nTITLE: Enhancing distcp functionality in Java\nDESCRIPTION: Modifies distcp to allow meaningful combination of -update and -delete options.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5227. Fix distcp so -update and -delete can be meaningfully\ncombined. (Tsz Wo (Nicholas), SZE via cdouglas)\n```\n\n----------------------------------------\n\nTITLE: Handling Multibyte Delimiters in LineRecordReader for Java\nDESCRIPTION: This code snippet addresses an issue with LineRecordReader not handling multibyte record delimiters correctly. It improves the handling of uncompressed input and fixes issues with incomplete records and wrong position/key information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.mapred.LineRecordReader\n```\n\n----------------------------------------\n\nTITLE: Path Configuration Change Example\nDESCRIPTION: Change in configuration format for dfs.data.dir and mapred.local.dir from space-separated to comma-separated lists to fix Windows compatibility issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_36\n\nLANGUAGE: Properties\nCODE:\n```\n# Old format\ndfs.data.dir=/data1 /data2 /data3\n\n# New format\ndfs.data.dir=/data1,/data2,/data3\n```\n\n----------------------------------------\n\nTITLE: Configuring Deprecated SequenceFile Compression Type in Java\nDESCRIPTION: This snippet shows a deprecated configuration setting for SequenceFile compression type in Hadoop. It demonstrates how to set the compression type, which has been replaced by newer methods in SequenceFile, SequenceFileOutputFormat, and JobConf classes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nio.seqfile.compression.type\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Recursive Directories in Hadoop FileSystem (Java)\nDESCRIPTION: Adds support for non-recursive directory creation in Hadoop FileSystem and SequenceFile.Writer APIs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.createNonRecursive()\n```\n\nLANGUAGE: Java\nCODE:\n```\nSequenceFile.Writer.createNonRecursive()\n```\n\n----------------------------------------\n\nTITLE: Fixing Javadoc in WritableUtils.java (Java)\nDESCRIPTION: Corrects incorrect javadoc in the WritableUtils.java file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-12465. Incorrect javadoc in WritableUtils.java. (Jagadesh Kiran N via aajisaka)\n```\n\n----------------------------------------\n\nTITLE: Fixing FileUtil File Leaks in Java\nDESCRIPTION: Addresses potential file leaks in the FileUtil class by ensuring proper file closure. This improves resource management in file operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3592. Fix a couple of possible file leaks in FileUtil\n(Bill de hOra via rangadi)\n```\n\n----------------------------------------\n\nTITLE: Listing FileSystem API Addition - Java\nDESCRIPTION: New iterator-based listing API added to FileSystem class to improve filesystem operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nHADOOP-10987. Provide an iterator-based listing API for FileSystem\n```\n\n----------------------------------------\n\nTITLE: Adding YARN Capacity Scheduler Configuration\nDESCRIPTION: This snippet shows the configuration change needed for the CapacityScheduler to cap concurrently running applications per-queue and per-user, as part of MAPREDUCE-2697.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\nConfiguration changes:\n      add yarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Script Addition - Hadoop Application Directory Setup\nDESCRIPTION: Added hadoop-setup-application.sh script for creating application directories with configuration features including support for dfs.support.append, webhdfs and hadoop proxy user settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhadoop-setup-application.sh\n```\n\n----------------------------------------\n\nTITLE: Adding RPC Timeout Option in Java\nDESCRIPTION: Modifies RPC to have an option to timeout.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: java\nCODE:\n```\nMake RPC to have an option to timeout.\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated Configuration in Hadoop XML\nDESCRIPTION: Removes support for the deprecated mapred.child.heap.size configuration property from the hadoop-default.xml file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_28\n\nLANGUAGE: XML\nCODE:\n```\nConfiguration changes to hadoop-default.xml:\n  remove mapred.child.heap.size\n```\n\n----------------------------------------\n\nTITLE: Optimizing DataNode Writes in Java\nDESCRIPTION: Optimization to improve DataNode write performance for small writes and flushes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-8722. Optimize datanode writes for small writes and flushes (kihwal)\n```\n\n----------------------------------------\n\nTITLE: Native Code Implementation - CRC32C Using SSE4.2\nDESCRIPTION: Implementation of CRC32C checksum verification using SSE4.2 instructions for improved performance in native code.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nPureJavaCrc32\n```\n\n----------------------------------------\n\nTITLE: File Path Construction - Windows Hard Link Creation\nDESCRIPTION: Implementation for creating hard links on Windows systems using the fsutil hardlink command.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\nfsutil hardlink\n```\n\n----------------------------------------\n\nTITLE: Extending LightWeightGSet for Eviction in Java\nDESCRIPTION: Extends the LightWeightGSet class to support eviction of expired elements.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nLightWeightGSet\n```\n\n----------------------------------------\n\nTITLE: Implementing ONCRPC and XDR in Java\nDESCRIPTION: Implements Open Network Computing Remote Procedure Call (ONCRPC) and External Data Representation (XDR) protocols.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nONCRPC\n```\n\nLANGUAGE: Java\nCODE:\n```\nXDR\n```\n\n----------------------------------------\n\nTITLE: Adding NativeIO Support via JNI in Java\nDESCRIPTION: Adds support for NativeIO using JNI (Java Native Interface).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: java\nCODE:\n```\nAdds support for NativeIO using JNI.\n```\n\n----------------------------------------\n\nTITLE: Removing unnecessary job.setNumReduceTasks call in SleepJob\nDESCRIPTION: Optimization to remove an unnecessary call to job.setNumReduceTasks in the SleepJob.createJob method, improving job creation performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5775. Remove unnecessary job.setNumReduceTasks in SleepJob.createJob \n(jhanver chand sharma via devaraj)\n```\n\n----------------------------------------\n\nTITLE: Configuring Profiling String in Java\nDESCRIPTION: Makes the profiling string configurable for better customization of profiling output.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n// Code to make profiling string configurable\n```\n\n----------------------------------------\n\nTITLE: Fixing Map-Side Sort Corner Case (Java)\nDESCRIPTION: Fixes an issue in the map-side sort where some values were incorrectly counted as too large, causing premature spills to disk. Also addresses values incorrectly bypassing the combiner.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3931. Fix corner case in the map-side sort that causes some values \nto be counted as too large and cause pre-mature spills to disk. Some values\nwill also bypass the combiner incorrectly. (cdouglas via omalley)\n```\n\n----------------------------------------\n\nTITLE: Fixing Prototype in OpensslSecureRandom.c\nDESCRIPTION: This C code snippet addresses an incorrect prototype in the OpensslSecureRandom.c file, likely improving the implementation of secure random number generation using OpenSSL.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: C\nCODE:\n```\nHADOOP-10871. incorrect prototype in OpensslSecureRandom.c (cmccabe)\n```\n\n----------------------------------------\n\nTITLE: Fixing CapacityScheduler Lock Acquisition in YARN\nDESCRIPTION: This bug fix removes unnecessary lock acquisition in the CapacityScheduler when calling getQueue, which could improve performance by reducing lock contention.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3487. CapacityScheduler scheduler lock obtained unnecessarily when \ncalling getQueue (Jason Lowe via wangda)\n```\n\n----------------------------------------\n\nTITLE: Missing @Since Tags in YARN Container Resource Management APIs\nDESCRIPTION: Methods related to container resource management in NMClient and NMClientAsync classes that require version annotation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.NMClient.updateContainerResource(org.apache.hadoop.yarn.api.records.Container)\norg.apache.hadoop.yarn.client.api.async.NMClientAsync.updateContainerResourceAsync(org.apache.hadoop.yarn.api.records.Container)\n```\n\n----------------------------------------\n\nTITLE: Implementing JobTracker restart behavior in Java\nDESCRIPTION: Modifies JobTracker to wait for TaskTrackers to rejoin before scheduling new tasks after a restart, fixing race conditions with greedy scheduling.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5337. JobTracker, upon restart, now waits for the TaskTrackers to\njoin back before scheduling new tasks. This fixes race conditions associated\nwith greedy scheduling as was the case earlier. (Amar Kamat via ddas)\n```\n\n----------------------------------------\n\nTITLE: Enhancing hadoop classpath command\nDESCRIPTION: Improves the 'hadoop classpath' command to expand wildcards and optionally write the classpath into a jar manifest, enhancing classpath management and deployment.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10903. Enhance hadoop classpath command to expand wildcards or write classpath into jar manifest. (cnauroth)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Hadoop Instances with hadoop.tmp.dir\nDESCRIPTION: Configuration example showing how to set the temporary directory base for Hadoop. This property defines where temporary files are stored and is important when running multiple Hadoop instances on the same machine.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.tmp.dir</name>\n  <value>/tmp/hadoop-${user.name}</value>\n  <description>A base for other temporary directories.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Adding MutableRate metrics for FSNamesystemLock operations (Java)\nDESCRIPTION: Improvement to add MutableRate metrics for FSNamesystemLock operations, providing better monitoring capabilities.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-10872. Add MutableRate metrics for FSNamesystemLock operations.\n```\n\n----------------------------------------\n\nTITLE: Configuration Setting for TaskTracker Out-of-band Heartbeat\nDESCRIPTION: Configuration parameter to enable the tasktracker to send an out-of-band heartbeat on task-completion for better job-latency. This setting is part of the MAPREDUCE-270 optimization.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: config\nCODE:\n```\nadd mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: File Location Optimization for HDFS Splits\nDESCRIPTION: Optimization for FileInputFormat that provides locations for splits based on the rack/host that has the most number of bytes, improving data locality for MapReduce jobs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: java\nCODE:\n```\nFixes FileInputFormat to do provide locations for splits based on the rack/host that has the most number of bytes.\n```\n\n----------------------------------------\n\nTITLE: Executing Shell Command for Hadoop Classpath\nDESCRIPTION: Adds a new 'classpath' command to the hadoop shell script to print the classpath used by Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\nhadoop classpath\n```\n\n----------------------------------------\n\nTITLE: Fixing MR AppMaster HTTPS Configuration in Java\nDESCRIPTION: Fixed MR AppMaster's webapp to use a new config mapreduce.ssl.enabled to enable HTTPS, disabled by default as MR AM needs to set up its own certificates.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nmapreduce.ssl.enabled\n```\n\n----------------------------------------\n\nTITLE: HDFS Address Configuration Parameters\nDESCRIPTION: Configuration parameters for various HDFS and MapReduce service addresses.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\ndfs.secondary.http.address\ndfs.datanode.address\ndfs.datanode.http.address\ndfs.http.address\nmapred.job.tracker.http.address\nmapred.task.tracker.report.address\nmapred.task.tracker.http.address\n```\n\n----------------------------------------\n\nTITLE: Setting JVM Options for Heap Dump on OutOfMemoryError in Java\nDESCRIPTION: Adds the -XX:+HeapDumpOnOutOfMemoryError JVM option to the forked tests to generate a heap dump when an OutOfMemoryError occurs. This helps in diagnosing memory-related issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n-XX:+HeapDumpOnOutOfMemoryError\n```\n\n----------------------------------------\n\nTITLE: Improving SortedMapWritable Compliance with Map Interface\nDESCRIPTION: This change addresses violations of the Map interface contract in SortedMapWritable for the equals() and hashCode() methods. It ensures proper implementation of these methods for consistency with the Java Collections framework.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9124. SortedMapWritable violates contract of Map interface for\nequals() and hashCode(). (Surenkumar Nihalani via tomwhite)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Job Initialization Threads in Hadoop MapReduce\nDESCRIPTION: Introduces a configurable number of threads for job initialization in Hadoop MapReduce. The number of threads can be set using the 'mapred.jobinit.threads' configuration parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nmapred.jobinit.threads\n```\n\n----------------------------------------\n\nTITLE: IPC Server Configuration Parameter\nDESCRIPTION: Hidden configuration option for controlling maximum IPC handler response buffer size.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\nipc.server.max.response.size\n```\n\n----------------------------------------\n\nTITLE: Fixing FileInputFormat to Read Files Recursively\nDESCRIPTION: Bug fix for FileInputFormat to read files recursively in the input path directory. This addresses MAPREDUCE-3193.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-3193. FileInputFormat doesn't read files recursively in the input path dir\n```\n\n----------------------------------------\n\nTITLE: Enhancing NetUtils in Java for Socket Address Creation\nDESCRIPTION: Added a new method in NetUtils class to create socket addresses with more informative exception messages for better error handling.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\n// New NetUtils method for creating socket addresses\n```\n\n----------------------------------------\n\nTITLE: Undocumented AppInfo Methods\nDESCRIPTION: Collection of getter methods in AppInfo class lacking documentation blocks. These methods handle resource allocation, container management, and application properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\ngetAggregatePreemptedResourceAllocation()\ngetAggregateResourceAllocation()\ngetAllocatedCpuVcores()\ngetAllocatedMemoryMB()\ngetAmNodeLabelExpression()\ngetAppNodeLabelExpression()\ngetLaunchTime()\ngetPriority()\ngetReservedCpuVcores()\ngetReservedMemoryMB()\ngetRunningContainers()\nisUnmanagedApp()\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes - Hadoop Server Addresses\nDESCRIPTION: List of deprecated and new configuration variables for server bind addresses and ports across Hadoop components.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: Properties\nCODE:\n```\n# Deprecated configuration variables:\ndfs.info.bindAddress\ndfs.info.port\ndfs.datanode.bindAddress\ndfs.datanode.port\ndfs.datanode.info.bindAdress\ndfs.datanode.info.port\ndfs.secondary.info.bindAddress\ndfs.secondary.info.port\nmapred.job.tracker.info.bindAddress\nmapred.job.tracker.info.port\nmapred.task.tracker.report.bindAddress\ntasktracker.http.bindAddress\ntasktracker.http.port\n\n# New configuration variables:\ndfs.secondary.http.address\ndfs.datanode.address\ndfs.datanode.http.address\ndfs.http.address\nmapred.job.tracker.http.address\nmapred.task.tracker.report.address\nmapred.task.tracker.http.address\n```\n\n----------------------------------------\n\nTITLE: Fixing DistCp Failure Reporting\nDESCRIPTION: Bug fix to ensure DistCp reports failure correctly instead of reporting success on failure. This addresses MAPREDUCE-5315.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5315.  DistCp reports success even on failure.\n```\n\n----------------------------------------\n\nTITLE: Deprecated Method Removal in JobConf\nDESCRIPTION: List of deprecated methods removed from JobConf class to clean up the API, including methods related to input/output paths and compression settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\naddInputPath(Path)\ngetInputPaths()\ngetMapOutputCompressionType()\ngetOutputPath()\ngetSystemDir()\nsetInputPath(Path)\nsetMapOutputCompressionType(CompressionType style)\nsetOutputPath(Path)\n```\n\n----------------------------------------\n\nTITLE: Configuration File Inclusion in Hadoop\nDESCRIPTION: Implementation that enables a configuration file to include other configuration files, allowing for modular configuration management in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nA configuration file can include other configuration files.\n```\n\n----------------------------------------\n\nTITLE: Enhancing Block Report RPC Failure Logging in HDFS\nDESCRIPTION: Improves log reporting during block report RPC failures, providing better diagnostic information for troubleshooting network or communication issues between DataNodes and NameNodes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7579. Improve log reporting during block report rpc failure. (Charles Lamb via cnauroth)\n```\n\n----------------------------------------\n\nTITLE: Fixing Version Annotation in Python Script\nDESCRIPTION: Modifies the saveVersion.py script to include the branch information in the version annotation for Hadoop builds.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nHADOOP-9011. saveVersion.py does not include branch in version annotation.\n```\n\n----------------------------------------\n\nTITLE: Fixing Version Annotation in Python Script\nDESCRIPTION: Modifies the saveVersion.py script to include the branch information in the version annotation for Hadoop builds.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nHADOOP-9011. saveVersion.py does not include branch in version annotation.\n```\n\n----------------------------------------\n\nTITLE: Specifying Network Interface for Hadoop Node Hostname\nDESCRIPTION: This change allows specifying the network interface and nameserver to be used when determining the local hostname advertised by datanodes and tasktrackers. It improves network configuration flexibility in Hadoop clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Bash\nCODE:\n```\n/bin/sh\n```\n\n----------------------------------------\n\nTITLE: String Switch Statement Improvement in RegistrySecurity.java\nDESCRIPTION: Improvement to use JDK7's switch statement for String instead of if-else statements in RegistrySecurity.java for better code readability and performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Changed from if-else statements to switch statement\nswitch(stringValue) {\n    case \"option1\":\n        // handle option1\n        break;\n    case \"option2\":\n        // handle option2\n        break;\n    default:\n        // default handling\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Success File for Completed MapReduce Jobs in Java\nDESCRIPTION: Enhances FileOutputCommitter to create a _SUCCESS file upon successful job completion, providing an easy way to verify job status.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n_SUCCESS\n```\n\n----------------------------------------\n\nTITLE: Enhancing Text class for reading with known length\nDESCRIPTION: Improves the Text class to support reading with a known length, potentially optimizing performance for certain text processing operations in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10855. Allow Text to be read with a known Length. (todd)\n```\n\n----------------------------------------\n\nTITLE: Missing @SINCE Tag for NMClient and NMClientAsync Methods\nDESCRIPTION: The updateContainerResource method in NMClient and updateContainerResourceAsync method in NMClientAsync are missing @SINCE tags. These methods are likely used to update resources for a container in YARN.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.NMClient Method updateContainerResource(org.apache.hadoop.yarn.api.records.Container)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.NMClientAsync Method updateContainerResourceAsync(org.apache.hadoop.yarn.api.records.Container)\n```\n\n----------------------------------------\n\nTITLE: Deprecating Configuration.set(String,Object) and Implementing Iterable in Java\nDESCRIPTION: Deprecates the set(String,Object) method in Configuration class and implements the Iterable interface. This change further enforces the principle of storing only strings in Configurations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\n// Deprecated\nConfiguration.set(String, Object)\n\n// New implementation\nConfiguration implements Iterable\n```\n\n----------------------------------------\n\nTITLE: Updating getSplits Method to Use listLocatedStatus in Java\nDESCRIPTION: Improved getSplits performance by using listLocatedStatus instead of listStatus.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n// Use listLocatedStatus instead of listStatus in getSplits\nlistLocatedStatus()\n```\n\n----------------------------------------\n\nTITLE: Configuring JobClient retry for JobStatus calls\nDESCRIPTION: Added a new configuration option for JobClient to retry JobStatus calls. This prevents failures on history-server backed by DFSes with weaker consistency guarantees.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6251. Added a new config for JobClient to retry JobStatus calls so\\nthat they don't fail on history-server backed by DFSes with not so strong\\nguarantees. (Craig Welch via vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Implementing ViewFileSystem listLocatedStatus in Java\nDESCRIPTION: This improvement implements the listLocatedStatus method for ViewFileSystem to speed up split calculation. It enhances performance for operations that need to list and get status of files.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11812. Implement listLocatedStatus for ViewFileSystem to speed up split calculation (gera)\n```\n\n----------------------------------------\n\nTITLE: Updating Block Replication Map in Java\nDESCRIPTION: Fix to ensure proper removal of blocks from excess replication tracking and decrement of associated metrics when a block is removed.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-6945. BlockManager should remove a block from excessReplicateMap and\\ndecrement ExcessBlocks metric when the block is removed. (aajisaka)\n```\n\n----------------------------------------\n\nTITLE: Setting up Hadoop Environment Variables in .bashrc\nDESCRIPTION: Configuration of environment variables in the user's .bashrc file to set up Hadoop paths and Java home directory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Add these lines to ~/.bashrc\nexport HADOOP_HOME=/opt/hadoop\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n```\n\n----------------------------------------\n\nTITLE: Setting Hadoop Daemon Scheduling Priority\nDESCRIPTION: This snippet shows how to set the scheduling priority for Hadoop daemons using the HADOOP_NICENESS environment variable.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_38\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP_NICENESS\n```\n\n----------------------------------------\n\nTITLE: Optimizing RMNodeFinishedContainersPulledByAMEvent Sending\nDESCRIPTION: Improves efficiency by optimizing the sending of RMNodeFinishedContainersPulledByAMEvent for every AM heartbeat.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nYARN-5262. Optimize sending RMNodeFinishedContainersPulledByAMEvent for every AM heartbeat (Rohith Sharma K S via jlowe)\n```\n\n----------------------------------------\n\nTITLE: Configuration Pattern - Jetty Port Setting Fix\nDESCRIPTION: Fix for WebServer to prevent port number increases in case of negative port settings caused by Jetty race conditions.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nif (port < 0) {\n  // Do not increment port\n}\n```\n\n----------------------------------------\n\nTITLE: Configuration Pattern - Null Check Validation\nDESCRIPTION: Exception handling improvement in Configuration.set() to handle null key or value parameters with better error messages\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration.set(String key, String value)\n```\n\n----------------------------------------\n\nTITLE: Deprecated Method Removals in JobConf\nDESCRIPTION: List of deprecated methods removed from JobConf class for version 0.19.0, including input/output path handling and compression methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\naddInputPath(Path)\ngetInputPaths()\ngetMapOutputCompressionType()\ngetOutputPath()\ngetSystemDir()\nsetInputPath(Path)\nsetMapOutputCompressionType(CompressionType style)\nsetOutputPath(Path)\n```\n\n----------------------------------------\n\nTITLE: Adding HTTPS support to Hadoop web UIs in Java\nDESCRIPTION: Implements HTTPS support for Hadoop web user interfaces to improve security.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8581. add support for HTTPS to the web UIs.\n```\n\n----------------------------------------\n\nTITLE: Updating Block Replication Map in Java\nDESCRIPTION: Bug fix to properly remove blocks from excess replicate map and decrement ExcessBlocks metric when blocks are removed.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-6945. BlockManager should remove a block from excessReplicateMap and\ndecrement ExcessBlocks metric when the block is removed. (aajisaka)\n```\n\n----------------------------------------\n\nTITLE: Reducing Client Failures During DataNode Restart in HDFS\nDESCRIPTION: Implements improvements to reduce client failures that occur during datanode restart.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hdfs/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-9574. Reduce client failures during datanode restart\n```\n\n----------------------------------------\n\nTITLE: Setting Custom JAR in Distributed Cache for MapReduce Jobs in Java\nDESCRIPTION: Allows jobs to set a custom JAR file that is in the distributed cache, providing more flexibility for job configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-4408. allow jobs to set a JAR that is in the distributed cached\n```\n\n----------------------------------------\n\nTITLE: Fixing RawLocalFileSystem.listStatus() for UNIX pipefile (Java)\nDESCRIPTION: Addresses HDFS-8767 where RawLocalFileSystem.listStatus() returns null for UNIX pipefile. This fix ensures proper handling of UNIX pipefiles.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-8767. RawLocalFileSystem.listStatus() returns null for UNIX pipefile.\n```\n\n----------------------------------------\n\nTITLE: Implementing ONCRPC and XDR protocols in Java\nDESCRIPTION: Implements the ONCRPC (Open Network Computing Remote Procedure Call) and XDR (External Data Representation) protocols to support interoperability with other systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nONCRPC\nXDR\n```\n\n----------------------------------------\n\nTITLE: Removing Main-Class from MapReduce tools JAR in build.xml\nDESCRIPTION: Removes the incorrectly set Main-Class attribute from the tools JAR and renames the target from \"tools-jar\" to \"tools\".\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: XML\nCODE:\n```\n<target name=\"tools\" depends=\"compile\">\n  <jar jarfile=\"${build.dir}/${name}-tools-${version}.jar\">\n    <fileset dir=\"${build.classes}\">\n      <include name=\"org/apache/hadoop/tools/**/*.class\"/>\n    </fileset>\n  </jar>\n</target>\n```\n\n----------------------------------------\n\nTITLE: Adding Double Support to Hadoop Configuration in Java\nDESCRIPTION: Adds getDouble() and setDouble() methods to the org.apache.hadoop.conf.Configuration class to support reading and writing double values.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.conf.Configuration\n```\n\n----------------------------------------\n\nTITLE: Adding JSON Log4j Backend in Hadoop\nDESCRIPTION: This snippet describes the addition of a new Log4j backend that outputs JSON data, one entry per line. This enhancement improves logging capabilities in Hadoop, potentially making log analysis easier.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nAdd a log4j back end that can push out JSON data, one per line.\n```\n\n----------------------------------------\n\nTITLE: Configuration for Job Status Persistence\nDESCRIPTION: Configuration settings in hadoop-default.xml for persisting job statuses in HDFS, allowing query of decommissioned jobs across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: XML\nCODE:\n```\nmapred.job.tracker.persist.jobstatus.active (default value of false)\nmapred.job.tracker.persist.jobstatus.hours (default value of 0)\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Information in Hadoop YARN (Java)\nDESCRIPTION: Method to set resource information for a specific resource type in the Resource class. This is used for defining custom resource types in YARN.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.1/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nResource.setResourceInformation(String resource, ResourceInformation resourceInformation)\n```\n\n----------------------------------------\n\nTITLE: Checking container-executor for vulnerability using readelf command in Linux\nDESCRIPTION: This code demonstrates how to determine if a Hadoop container-executor binary is vulnerable to CVE-2023-26031 by examining its RUNPATH or RPATH settings. A vulnerable binary will have '../lib/native/' in its runpath, while a safe one will only have '$ORIGIN/'.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/cve_list.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ readelf -d container-executor|grep 'RUNPATH\\|RPATH'\n0x000000000000001d (RUNPATH)            Library runpath: [$ORIGIN/:../lib/native/]\n```\n\n----------------------------------------\n\nTITLE: Enabling TCP_NODELAY for Hadoop IPC\nDESCRIPTION: Enables the TCP_NODELAY option by default for Hadoop's Inter-Process Communication (IPC), which can reduce latency in network communications.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8069. Enable TCP_NODELAY by default for IPC. (Todd Lipcon via Arpit Agarwal)\n```\n\n----------------------------------------\n\nTITLE: Adding ByteBufferReadable interface to FSDataInputStream in Java\nDESCRIPTION: Adds a new ByteBufferReadable interface to FSDataInputStream to improve I/O performance for certain use cases.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8135. Add ByteBufferReadable interface to FSDataInputStream.\n```\n\n----------------------------------------\n\nTITLE: Removing JobClient Static Configuration Methods in Java\nDESCRIPTION: Removes static Configuration-related methods from the JobClient class to eliminate dependency on concrete filesystems. This change removes setCommandLineConfig and getCommandLineConfig methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\nJobClient::setCommandLineConfig\nJobClient::getCommandLineConfig\n```\n\n----------------------------------------\n\nTITLE: Preventing infinite increase of reducer resource requests\nDESCRIPTION: Fixes an issue where MapReduce jobs could infinitely increase the number of reducer resource requests.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6689. MapReduce job can infinitely increase number of reducer\nresource requests (Wangda Tan via jlowe)\n```\n\n----------------------------------------\n\nTITLE: Configuration Change - Sequence File Compression Type\nDESCRIPTION: Deprecation of io.seqfile.compression.type configuration property in hadoop-default.xml in favor of using SequenceFile.createWriter, SequenceFileOutputFormat.setCompressionType, and JobConf.setMapOutputCompressionType methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: XML\nCODE:\n```\nio.seqfile.compression.type\n```\n\n----------------------------------------\n\nTITLE: Configuration Change - Sequence File Compression Type\nDESCRIPTION: Deprecation of io.seqfile.compression.type configuration property in hadoop-default.xml in favor of using SequenceFile.createWriter, SequenceFileOutputFormat.setCompressionType, and JobConf.setMapOutputCompressionType methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: XML\nCODE:\n```\nio.seqfile.compression.type\n```\n\n----------------------------------------\n\nTITLE: Implementing CRC32C Checksum in Java\nDESCRIPTION: Adds CRC32C as a new DataChecksum implementation for improved checksum calculation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7443. Add CRC32C as another DataChecksum implementation\n```\n\n----------------------------------------\n\nTITLE: Deprecating LineReader in Java\nDESCRIPTION: Extends the new LineReader class to the old name and deprecates it. Updates related tests to use the new class. This maintains backwards compatibility while encouraging use of the new implementation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4269. Fix the deprecation of LineReader by extending the new class\ninto the old name and deprecating it. Also update the tests to test the \nnew class. (cdouglas via omalley)\n```\n\n----------------------------------------\n\nTITLE: Hadoop Command for File Removal\nDESCRIPTION: Hadoop filesystem shell command for removing files, which was fixed in HADOOP-1207 to continue when non-existent files are encountered rather than stopping the operation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nbin/hadoop fs -rm\n```\n\n----------------------------------------\n\nTITLE: Updating FSNamesystem Method Signatures in Java\nDESCRIPTION: Changes to FSNamesystem method signatures as part of moving the HTTP server from FSNamesystem into NameNode. Removes getNameNodeInfoPort() and replaces getDFSNameNodeMachine() and getDFSNameNodePort() with getDFSNameNodeAddress().\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\nFSNamesystem.getNameNodeInfoPort()\nFSNamesystem.getDFSNameNodeMachine()\nFSNamesystem.getDFSNameNodePort()\nFSNamesystem.getDFSNameNodeAddress()\n```\n\n----------------------------------------\n\nTITLE: Patching Jetty to disable SSLv3 in Java\nDESCRIPTION: Modifies the Jetty web server configuration to disable the SSLv3 protocol. This change enhances the security of web services in Hadoop by removing support for an outdated and vulnerable protocol.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nPatch up Jetty to disable SSLv3.\n```\n\n----------------------------------------\n\nTITLE: Configuring ulimits for Streaming/Pipes Tasks in Shell\nDESCRIPTION: Adds functionality to specify ulimits for streaming and pipes tasks, allowing better resource control.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\n# Shell commands to set ulimits for streaming/pipes tasks\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Limits in Hadoop XML\nDESCRIPTION: This snippet shows configuration changes to hadoop-default.xml for setting maximum map and reduce tasks per TaskTracker. It adds new properties for map and reduce tasks and removes a deprecated property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.tasktracker.map.tasks.maximum (default value of 2)\nadd mapred.tasktracker.reduce.tasks.maximum (default value of 2)\nremove mapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Accessing Application Resource Allocation in YARN (Java)\nDESCRIPTION: These methods in the AppInfo class retrieve information about allocated and reserved resources for a YARN application, including CPU cores and memory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppInfo {\n    public int getAllocatedCpuVcores() { /* ... */ }\n    public int getAllocatedMemoryMB() { /* ... */ }\n    public int getReservedCpuVcores() { /* ... */ }\n    public int getReservedMemoryMB() { /* ... */ }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Out-of-Band Heartbeat for TaskTracker in MapReduce\nDESCRIPTION: This configuration change adds a new property to enable out-of-band heartbeats from TaskTracker to JobTracker on task completion for better job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nadd mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Implementing NetworkTopology Plugin (Java)\nDESCRIPTION: Makes the NetworkTopology class pluggable, allowing for custom implementations of network topology. This enables more flexible and customized rack awareness in Hadoop clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nNetworkTopology\n```\n\n----------------------------------------\n\nTITLE: Implementing chmod with JNI in Java\nDESCRIPTION: Adds a native implementation of the chmod (change mode) operation using Java Native Interface (JNI). This allows for more efficient and platform-specific file permission changes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\npublic static native int chmod(String path, int mode) throws IOException;\n```\n\n----------------------------------------\n\nTITLE: Adding Speculative Execution Configuration Properties in hadoop-default.xml\nDESCRIPTION: Configuration changes to allow finer-grained control over speculative execution by replacing the general property with separate controls for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Adding JobTracker Job Status Persistence Configuration in hadoop-default.xml\nDESCRIPTION: Configuration changes to support persisting job statuses in HDFS, allowing the JobClient to query information about completed jobs across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                                /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Preventing mapper rescheduling after reducer completion\nDESCRIPTION: Fixes an issue where mappers were getting rescheduled on node transition even after all reducers had completed.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5817. Mappers get rescheduled on node transition even after all\nreducers are completed. (Sangjin Lee via kasha)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Setup Script in Bash\nDESCRIPTION: This snippet indicates modifications to the Hadoop setup configuration script to add toggles for DFS append support, WebHDFS, and Hadoop proxy user settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsetup config script\n```\n\n----------------------------------------\n\nTITLE: TaskLog URL Change Documentation\nDESCRIPTION: Documents the change in TaskLog URL format and parameters, showing the transition from tasklog.jsp to a simpler tasklog endpoint with start and end parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_34\n\nLANGUAGE: text\nCODE:\n```\nhttp://<tasktracker>/tasklog.jsp -> http://<tasktracker>tasklog with\n         parameters limited to start and end, which may be positive (from\n```\n\n----------------------------------------\n\nTITLE: Preventing mapper rescheduling after reducer completion\nDESCRIPTION: Fixes an issue where mappers were getting rescheduled on node transition even after all reducers had completed.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5817. Mappers get rescheduled on node transition even after all\nreducers are completed. (Sangjin Lee via kasha)\n```\n\n----------------------------------------\n\nTITLE: Removing unused BlockLocation serialization in Java\nDESCRIPTION: Removes unused BlockLocation serialization code to simplify the codebase. This is an incompatible change.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8388. Remove unused BlockLocation serialization.\n```\n\n----------------------------------------\n\nTITLE: Handling RMContainer State Transitions (Java)\nDESCRIPTION: Ensures that the Scheduler re-requests container resources when an RMContainer transitions from ALLOCATED to KILLED state.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/yarn/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3535. Scheduler must re-request container resources when RMContainer transitions\\nfrom ALLOCATED to KILLED. (rohithsharma and peng.zhang via asuresh)\n```\n\n----------------------------------------\n\nTITLE: Handling RMContainer State Transitions (Java)\nDESCRIPTION: Ensures that the Scheduler re-requests container resources when an RMContainer transitions from ALLOCATED to KILLED state.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/yarn/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3535. Scheduler must re-request container resources when RMContainer transitions\\nfrom ALLOCATED to KILLED. (rohithsharma and peng.zhang via asuresh)\n```\n\n----------------------------------------\n\nTITLE: Fixing MapTask Serialization (Java)\nDESCRIPTION: Corrects the serialization data structures in MapTask where value lengths were incorrectly calculated.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3550. Fix the serialization data structures in MapTask where the\nvalue lengths are incorrectly calculated. (cdouglas)\n```\n\n----------------------------------------\n\nTITLE: Implementing Windows mlock equivalent in Hadoop Native Code\nDESCRIPTION: Adds Windows-specific implementation for memory locking, equivalent to the mlock function on Unix systems. This allows Hadoop to lock memory pages on Windows platforms.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10815. Implement Windows equivalent of mlock. (cnauroth)\n```\n\n----------------------------------------\n\nTITLE: Adding general interface for NFS and Mount in Java\nDESCRIPTION: Adds a general interface for NFS (Network File System) and Mount protocols, enabling Hadoop to interact with NFS-based storage systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nNFS\nMount\n```\n\n----------------------------------------\n\nTITLE: YARN Resource Management Methods\nDESCRIPTION: Factory and builder methods for creating YARN resource management objects like ResourceRequest, ContainerId, and Resource instances.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nResource.newInstance(long, int)\nResourceRequest.newInstance(Priority, String, Resource, int, boolean, String, ExecutionTypeRequest)\nContainerId.newContainerId(ApplicationAttemptId, long)\nApplicationAttemptId.newInstance(ApplicationId, int)\n```\n\n----------------------------------------\n\nTITLE: Updating Library Path in hadoop-config.sh Script\nDESCRIPTION: A modification to hadoop-config.sh to add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH, ensuring native libraries can be properly located at runtime.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH.\n```\n\n----------------------------------------\n\nTITLE: Setting Hadoop Daemon Scheduling Priority in Shell Scripts\nDESCRIPTION: Adds the HADOOP_NICENESS environment variable to set scheduling priority for Hadoop daemons in shell scripts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP_NICENESS\n```\n\n----------------------------------------\n\nTITLE: Clearing FileSystem statistics between tasks in Java\nDESCRIPTION: Clears FileSystem statistics between tasks when JVM reuse is enabled.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5585. Clear FileSystem statistics between tasks when jvm-reuse\nis enabled. (omalley)\n```\n\n----------------------------------------\n\nTITLE: Using fadvise in NodeManager Shuffle Handler in Java\nDESCRIPTION: Implements fadvise functionality in the NodeManager's shuffle handler to optimize file access patterns and improve performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-3289. Make use of fadvise in the NM's shuffle handler.\n```\n\n----------------------------------------\n\nTITLE: Configuring NFS Gateway via core-site.xml\nDESCRIPTION: XML configuration for NFS Gateway including port settings, mount directory, and export point configurations for the NFSv3 server.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>nfs.exports.allowed.hosts</name>\n  <value>* rw</value>\n  <description>\n    Comma separated list of the hosts/networks that are allowed to mount the\n    given directory. The format is\n    host1|network1 [rw,ro] host2|network2 [rw,ro].\n    If the option is not specified, then \"* rw\" is assumed, i.e. anyone can mount with read-write access.\n  </description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Adding NFS and Mount Interfaces (Java)\nDESCRIPTION: Adds general interfaces for NFS (Network File System) and Mount protocols. This lays the groundwork for implementing NFS support in Hadoop, allowing for better integration with NFS-based systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nNFS\nMount\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce JVM Parameters\nDESCRIPTION: Configuration changes for separately setting map and reduce JVM parameters, environment variables and ulimit settings. Introduces new configuration properties while deprecating older ones.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nmapred.map.child.java.opts\nmapred.reduce.child.java.opts\nmapred.map.child.env\nmapred.reduce.child.ulimit\nmapred.map.child.env\nmapred.reduce.child.ulimit\n# deprecated\nmapred.child.java.opts\nmapred.child.env\nmapred.child.ulimit\n```\n\n----------------------------------------\n\nTITLE: Implementing String switch in RegistrySecurity.java\nDESCRIPTION: Replaces if-else statements with a switch statement for String comparisons in the RegistrySecurity class, taking advantage of JDK7's new feature.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nswitch (stringValue) {\n  case \"option1\":\n    // handle option1\n    break;\n  case \"option2\":\n    // handle option2\n    break;\n  default:\n    // handle default case\n}\n```\n\n----------------------------------------\n\nTITLE: Deprecating Configuration.set(String,Object) Method in Hadoop\nDESCRIPTION: Deprecates the set(String,Object) method in Configuration class to ensure only strings are put in Configurations. This change is part of HADOOP-1343 and also implements Iterable interface for Configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\n// Deprecated\n// Configuration.set(String, Object)\n\n// Configuration now implements Iterable\n```\n\n----------------------------------------\n\nTITLE: Removing Static Configuration Methods from JobClient in Java\nDESCRIPTION: Removes static Configuration-related methods from JobClient as part of removing static Configuration dependency. The removed methods are setCommandLineConfig and getCommandLineConfig. Also removes JobShell and TestJobShell classes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nJobClient::setCommandLineConfig\nJobClient::getCommandLineConfig\n```\n\n----------------------------------------\n\nTITLE: YARN Application Management Methods\nDESCRIPTION: Methods for managing YARN applications, including submission, monitoring, and configuration of application contexts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nApplicationReport.setAmNodeLabelExpression(String)\nApplicationSubmissionContext.setApplicationSchedulingPropertiesMap(Map)\nApplicationSubmissionContext.setApplicationTimeouts(Map)\nKillApplicationRequest.setDiagnostics(String)\n```\n\n----------------------------------------\n\nTITLE: Configuring Core Operations in Hadoop\nDESCRIPTION: Various configuration changes including log4j properties setup, directory ownership checks, proxy user configurations in core-site.xml, and environment variable fixes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhadoop-setup-conf.sh\n```\n\n----------------------------------------\n\nTITLE: Updating FsShell Command Help in Hadoop Common\nDESCRIPTION: Updates the help text for the 'ls' command in FsShell to remove outdated format information from Hadoop 0.16.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10314. The ls command help still shows outdated 0.16 format.\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Job Status Persistence in hadoop-default.xml\nDESCRIPTION: Configuration parameters to enable persistence of completed job statuses in HDFS, allowing access to information about decommissioned jobs across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.tracker.persist.jobstatus.active (default value of false)\nmapred.job.tracker.persist.jobstatus.hours (default value of 0)\nmapred.job.tracker.persist.jobstatus.dir (default value of /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Adding getMsb and getLsb Methods to ClientId in Java\nDESCRIPTION: Adds getMsb() and getLsb() methods to the ClientId class to retrieve the most and least significant bits of the client ID.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n// HADOOP-9821\npublic class ClientId {\n    public long getMsb() {\n        // Implementation\n    }\n    \n    public long getLsb() {\n        // Implementation\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Code Reference: Filesystem Access Path\nDESCRIPTION: Example of filesystem library path structure for pipes, utils and libhdfs under HADOOP-3344.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nc++/<os_osarch_jvmdatamodel>/lib\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Configuration Properties for Server Addresses\nDESCRIPTION: Lists new configuration variables for specifying server addresses and ports, replacing deprecated individual address and port properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Properties\nCODE:\n```\ndfs.secondary.http.address\ndfs.datanode.address\ndfs.datanode.http.address\ndfs.http.address\nmapred.job.tracker.http.address\nmapred.task.tracker.report.address\nmapred.task.tracker.http.address\n```\n\n----------------------------------------\n\nTITLE: Fixing start-all.sh script reference in Hadoop\nDESCRIPTION: Corrects an incorrect reference to start-dfs.sh in the start-all.sh script for starting YARN services.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nif [ -f \"${HADOOP_YARN_HOME}/sbin/start-yarn.sh\" ]; then\n  \"${HADOOP_YARN_HOME}/sbin/start-yarn.sh\"\nfi\n```\n\n----------------------------------------\n\nTITLE: Initializing WritableComparator in Java 6\nDESCRIPTION: This code change ensures WritableComparator initializes classes when looking for their raw comparator, addressing issues with Java 6 class initialization. It's part of the bug fixes for release 0.20.3.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-6881. Make WritableComparator intialize classes when\nlooking for their raw comparator, as classes often register raw\ncomparators in initializers, which are no longer automatically run\nin Java 6 when a class is referenced. (cutting via omalley)\n```\n\n----------------------------------------\n\nTITLE: Correcting HarFsInputStream Read Method\nDESCRIPTION: Fixes an issue where the HarFsInputStream.read(byte[]) method was incorrectly updating the position.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10440. HarFsInputStream.read(byte[]) updates position incorrectly.\n```\n\n----------------------------------------\n\nTITLE: Missing DAO Method Documentation in ContainerInfo Class\nDESCRIPTION: Missing documentation for ContainerInfo class method that retrieves node identifier information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\ngetNodeId()\n```\n\n----------------------------------------\n\nTITLE: MapReduce File Output Committer Credentials Bug Fix\nDESCRIPTION: Bug fix note showing the MAPREDUCE-5240 issue where initialized Credentials cache appears empty in FileOutputCommitter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: plain text\nCODE:\n```\nMAPREDUCE-5240 inside of FileOutputCommitter the initialized Credentials cache appears to be empty. (vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Reintroducing isSplittable Method for Hadoop 1 Compatibility in Java\nDESCRIPTION: Fixed compatibility with Hadoop 1 in mapred.lib.CombinFileInputFormat by reintroducing isSplittable(FileSystem, Path) API and ensuring semantic compatibility.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nmapred.lib.CombinFileInputFormat.isSplittable(FileSystem, Path)\n```\n\n----------------------------------------\n\nTITLE: Resolving snapshot diff corruption after concat (Java)\nDESCRIPTION: Bug fix to prevent snapshot diff corruption that could occur after concatenation operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-13120. Snapshot diff could be corrupted after concat.\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in Configuration Class Methods\nDESCRIPTION: Methods in org.apache.hadoop.conf.Configuration lacking proper documentation blocks for resource handling and system properties configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class Configuration {\n    void addResource(InputStream in, boolean restrict)\n    void addResource(InputStream in, String name, boolean restrict)\n    void addResource(String name, boolean restrict)\n    void addResource(URL url, boolean restrict)\n    void addResource(Path path, boolean restrict)\n    void dumpConfiguration(Configuration config, String name, Writer writer)\n    void setRestrictSystemProperties(boolean restrict)\n    void setRestrictSystemPropertiesDefault(boolean restrict)\n    void setRestrictSystemProps(boolean restrict)\n    void writeXml(String name, Writer writer)\n}\n```\n\n----------------------------------------\n\nTITLE: Missing JavaDoc in UserGroupInformation.isInitialized()\nDESCRIPTION: Method isInitialized() in UserGroupInformation class lacks required documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.5/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.security.UserGroupInformation.isInitialized()\n```\n\n----------------------------------------\n\nTITLE: Adding Variable Length Block Support in Java\nDESCRIPTION: Implementation of support for variable length blocks in HDFS, which allows for more flexible storage allocation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n```\n\n----------------------------------------\n\nTITLE: Appending to Files in Hadoop FileSystem API\nDESCRIPTION: Extends FileSystem API to allow appending to existing files, enabling more flexible file operations in HDFS and other supported filesystems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\nFSDataOutputStream out = fs.append(new Path(\"/path/to/file\"));\n// Write data to out\nout.close();\n```\n\n----------------------------------------\n\nTITLE: Fixing Bzip2Factory Thread Safety Issue in Hadoop (Java)\nDESCRIPTION: Addresses a thread safety issue in the Bzip2Factory class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-12191. Bzip2Factory is not thread safe. (Brahma Reddy Battula via ozawa)\n```\n\n----------------------------------------\n\nTITLE: Adding checkOperation(WRITE) checks in FSNamesystem (Java)\nDESCRIPTION: Improvement to add checkOperation(WRITE) checks in the FSNamesystem class. This enhances write operation validation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-13602. Add checkOperation(WRITE) checks in FSNamesystem.\n```\n\n----------------------------------------\n\nTITLE: Container Executor Configuration Path Definition\nDESCRIPTION: Configuration setting for container-executor.conf.dir to specify relative path for container executor binary\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\ncontainer-executor.conf.dir\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameter Addition for TaskTracker Heartbeat\nDESCRIPTION: Configuration change to add a new parameter mapreduce.tasktracker.outofband.heartbeat for controlling task completion heartbeat behavior.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Properties\nCODE:\n```\nmapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Validating Resource Requests in YARN ResourceManager\nDESCRIPTION: This snippet improves the efficiency of validating resource requests in the YARN ResourceManager by only obtaining queue info once. It's part of an optimization in the RMServerUtils class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3489. RMServerUtils.validateResourceRequests should only obtain queue \ninfo once. (Varun Saxena via wangda)\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Links for JIRA Issues in Perl Script\nDESCRIPTION: This change updates the changes2html.pl script to generate links to HADOOP, HDFS, and MAPREDUCE JIRA issues in the HTML output.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Perl\nCODE:\n```\n$line =~ s/(HADOOP|HDFS|MAPREDUCE)-\\d+/\"<a href=\\\"https://issues.apache.org/jira/browse/$1\\\">$1<\\/a>\"/ge;\n```\n\n----------------------------------------\n\nTITLE: Optimizing Container Pulling in RMAppAttempt\nDESCRIPTION: Optimizes the pullJustFinishedContainers method in RMAppAttempt to improve performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nYARN-5483. Optimize RMAppAttempt#pullJustFinishedContainers (sandflee via jlowe)\n```\n\n----------------------------------------\n\nTITLE: Updating FileSystem API in Hadoop Java Classes\nDESCRIPTION: Removes deprecated FileSystem methods and updates API calls in various Hadoop classes to use new methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: java\nCODE:\n```\n// Removed deprecated methods\n// FileSystem.getBlockSize(Path f)\n// FileSystem.getLength(Path f)\n// FileSystem.getReplication(Path src)\n// FileSystem.delete(Path f)\n// FileSystem.getName()\n// FileSystem.getNamed(String name, Configuration conf)\n```\n\n----------------------------------------\n\nTITLE: Changing Hadoop Shell Scripts to Use /bin/sh\nDESCRIPTION: Update to Hadoop shell scripts to use /bin/sh instead of /bin/bash for improved portability across different Unix-like systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: Bash\nCODE:\n```\n#!/bin/sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Bash Shell in Shell Script\nDESCRIPTION: Updates the bin/slaves.sh script to explicitly use /bin/bash instead of /bin/sh for better compatibility across systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_30\n\nLANGUAGE: Shell\nCODE:\n```\n/bin/bash\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in Container Management\nDESCRIPTION: Container management and status related methods missing proper documentation or @since tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.api.records.ContainerId.newContainerId(org.apache.hadoop.yarn.api.records.ApplicationAttemptId, long)\norg.apache.hadoop.yarn.api.ContainerManagementProtocol.localize(org.apache.hadoop.yarn.api.protocolrecords.ResourceLocalizationRequest)\n```\n\n----------------------------------------\n\nTITLE: Fixing FileSystem API in Java\nDESCRIPTION: Adds a new API getFiles to FileSystem and FileContext that lists all files under the input path or subtree, returning block locations with each file's status.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: java\nCODE:\n```\nAdd a new FileSystem API getFiles to FileSystem and FileContext that\nlists all files under the input path or the subtree rooted at the\ninput path if recursive is true. Block locations are returned together\nwith each file's status.\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Log4j Backend in Java\nDESCRIPTION: Added a new Log4j backend that can output JSON data, one entry per line, for improved logging capabilities in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n// New Log4j backend for JSON output\n```\n\n----------------------------------------\n\nTITLE: Extending Findbugs Analysis to tools.jar in Java\nDESCRIPTION: Expands the scope of Findbugs static analysis to include the tools.jar file. This enhances code quality checks across more of the Hadoop codebase.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4259. Findbugs should run over tools.jar also. (cdouglas via \nomalley)\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated RawLocalFileSystem Methods in Java\nDESCRIPTION: Removes deprecated methods from RawLocalFileSystem as part of fixing warnings generated by findbugs. The removed methods are getName(), lock(Path p, boolean shared), and release(Path p).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\npublic String getName()\npublic void lock(Path p, boolean shared)\npublic void release(Path p)\n```\n\n----------------------------------------\n\nTITLE: JVM Reuse Configuration Parameter\nDESCRIPTION: Configuration parameter added to hadoop-default.xml to control the number of tasks for which a JVM can be reused.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Setting DFS Umask Configuration in Java\nDESCRIPTION: This code snippet demonstrates how to set the umask used by the Distributed File System (DFS) in Hadoop. The umask controls default permissions for newly created files and directories in HDFS.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\ndfs.umask\n```\n\n----------------------------------------\n\nTITLE: Asynchronous MapCompletion Events in Reduce Shuffle\nDESCRIPTION: Performance optimization that modifies the Reduce shuffle scheduler to invoke getMapCompletionEvents in a separate thread, improving shuffle performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: java\nCODE:\n```\nFixes Reduce shuffle scheduler to invoke getMapCompletionEvents in a separate thread.\n```\n\n----------------------------------------\n\nTITLE: MapReduce Command Path Configuration\nDESCRIPTION: Implementation of mapred classpath command to print classpath for MapReduce applications.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nmapred classpath\n```\n\n----------------------------------------\n\nTITLE: Serializing Protocol Buffers with ObjectWritable in Java\nDESCRIPTION: This improvement adds the ability to serialize and deserialize protocol buffers using ObjectWritable in Hadoop, enhancing data interchange capabilities.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nObjectWritable.writeObject(ProtocolBuffer)\n```\n\n----------------------------------------\n\nTITLE: Updating Application State in ZKRMStateStore\nDESCRIPTION: Updated the ZKRMStateStore to handle updating application state when it hasn't been stored before. This fixes an issue with the updateApplication(Attempt)StateInternal method.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1924. Made ZKRMStateStore updateApplication(Attempt)StateInternal work\nwhen Application(Attempt) state hasn't been stored before.\n```\n\n----------------------------------------\n\nTITLE: Fixing duplicate records with multibyte delimiters and compressed input\nDESCRIPTION: Resolves an issue where multibyte delimiters with compressed input files were generating duplicate records.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6558. multibyte delimiters with compressed input files generate\nduplicate records (Wilfred Spiegelenburg via jlowe)\n```\n\n----------------------------------------\n\nTITLE: Resource Manager Queue Configuration Pattern\nDESCRIPTION: Naming convention for configuring resource manager queue properties in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Properties\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Fixing Hadoop Authentication Cookie Format\nDESCRIPTION: This code matches the hadoop.auth cookie format to the Jetty output, ensuring compatibility between Hadoop authentication and Jetty.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11068. Match hadoop.auth cookie format to jetty output.\\n    (Gregory Chanan via cnauroth)\n```\n\n----------------------------------------\n\nTITLE: Filesystem fix - HADOOP-4014 Windows Hard Links\nDESCRIPTION: Creates hard links using 'fsutil hardlink' command on Windows systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_11\n\n\n\n----------------------------------------\n\nTITLE: Implementing JDK7 Process File Descriptor Close Workaround\nDESCRIPTION: Adds a workaround for a bug in JDK7 related to closing process file descriptors.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nWorkaround JDK7 Process fd close bug\n```\n\n----------------------------------------\n\nTITLE: Default Temporary Directory Configuration in Hadoop\nDESCRIPTION: Shows the default configuration for Hadoop's temporary directory, which uses variable expansion to set the path based on the current user.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_32\n\nLANGUAGE: Properties\nCODE:\n```\n/tmp/hadoop-${user.name}\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation for ContainerInfo.getNodeId Method\nDESCRIPTION: Getter method in ContainerInfo class that returns the node ID. Missing proper JavaDoc documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic String getNodeId()\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in YARN Client APIs\nDESCRIPTION: Various missing documentation blocks and @since tags in YARN client API classes including AMRMClientAsync, NMClientAsync, and YarnClient. These issues need to be addressed to improve API documentation quality.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.0/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Constructors missing documentation\nAMRMClientAsync(int, AbstractCallbackHandler)\nAMRMClientAsync(AMRMClient, int, AbstractCallbackHandler)\nNMClientAsync(String, NMClient, AbstractCallbackHandler)\nNMClientAsync(String, AbstractCallbackHandler)\nNMClientAsync(AbstractCallbackHandler)\n\n// Methods missing @since tags\nAMRMClientAsync.createAMRMClientAsync(int, AbstractCallbackHandler)\nAMRMClientAsync.createAMRMClientAsync(AMRMClient, int, AbstractCallbackHandler)\nNMClientAsync.createNMClientAsync(AbstractCallbackHandler)\nYarnClient.createReservation()\nYarnClient.failApplicationAttempt(ApplicationAttemptId)\nYarnClient.getApplications(Set, Set, Set, EnumSet)\nNMClient.increaseContainerResource(Container)\nNMClientAsync.increaseContainerResourceAsync(Container)\nYarnClient.killApplication(ApplicationId, String)\nYarnClient.listReservations(ReservationListRequest)\nAMRMClient.requestContainerResourceChange(Container, Resource)\nAMRMClientAsync.requestContainerResourceChange(Container, Resource)\nYarnClient.signalToContainer(ContainerId, SignalContainerCommand)\nYarnClient.updateApplicationPriority(ApplicationId, Priority)\n```\n\n----------------------------------------\n\nTITLE: Adding closeAllForUGI Method to FileSystem in Java\nDESCRIPTION: Adds a new FileSystem API closeAllForUGI(..) for closing all file systems associated with a particular UserGroupInformation (UGI).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: java\nCODE:\n```\nAdd a new FileSystem API closeAllForUGI(..) for closing all\nfile systems associated with a particular UGI.\n```\n\n----------------------------------------\n\nTITLE: Upgrading Tomcat Dependency in Hadoop (Java)\nDESCRIPTION: Upgrades the Tomcat dependency to version 6.0.44 in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-12232. Upgrade Tomcat dependency to 6.0.44. (cnauroth)\n```\n\n----------------------------------------\n\nTITLE: Configuration Path Reference\nDESCRIPTION: Configuration path reference showing the removal of a deprecated heap size parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: xml\nCODE:\n```\nmapred.child.heap.size\n```\n\n----------------------------------------\n\nTITLE: Updating JobClient Constructor in Hadoop\nDESCRIPTION: Changes the JobClient constructor to accept a JobConf object instead of a Configuration object for better type safety.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_33\n\nLANGUAGE: Java\nCODE:\n```\n// Old constructor:\nJobClient(Configuration config)\n\n// New constructor:\nJobClient(JobConf conf)\n```\n\n----------------------------------------\n\nTITLE: Updating Block Generation Stamp in Datanode (Java)\nDESCRIPTION: Adds an RPC server to the Datanode with two RPCs: one to retrieve block metadata and another to set the generation stamp of an existing block. This supports append operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3283. The Datanode has a RPC server. It currently supports\ntwo RPCs: the first RPC retrives the metadata about a block and the\nsecond RPC sets the generation stamp of an existing block.\n```\n\n----------------------------------------\n\nTITLE: Deprecated Method Removal in JobConf\nDESCRIPTION: List of deprecated methods removed from JobConf class for version 0.19.0, including input/output path handling and map output compression methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\naddInputPath(Path)\ngetInputPaths()\ngetMapOutputCompressionType()\ngetOutputPath()\ngetSystemDir()\nsetInputPath(Path)\nsetMapOutputCompressionType(CompressionType style)\nsetOutputPath(Path)\n```\n\n----------------------------------------\n\nTITLE: Updating Block Generation Stamp in Datanode (Java)\nDESCRIPTION: Adds an RPC server to the Datanode with two RPCs: one to retrieve block metadata and another to set the generation stamp of an existing block. This supports append operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3283. The Datanode has a RPC server. It currently supports\ntwo RPCs: the first RPC retrives the metadata about a block and the\nsecond RPC sets the generation stamp of an existing block.\n```\n\n----------------------------------------\n\nTITLE: Bash Shell Command Quoting Update\nDESCRIPTION: Reference to adding bash quoting functionality to Shell class for improved command handling\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nHADOOP-13434. Add bash quoting to Shell class.\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration for Capacity Scheduler in Java\nDESCRIPTION: This snippet shows the addition of a new configuration parameter for the CapacityScheduler to cap concurrently running applications per-queue and per-user.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nadd yarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Example of Setting Java Library Path in MapReduce Child JVM\nDESCRIPTION: Shows how to specify the java.library.path system property within the mapred.child.java.opts configuration property for MapReduce tasks, as implemented in HADOOP-1493.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: java\nCODE:\n```\nmapred.child.java.opts\n```\n\n----------------------------------------\n\nTITLE: Configuring LZO Text File Input Format\nDESCRIPTION: Code reference showing LZO compressed text file handling functionality that was added. Enables splitting of LZO compressed files for processing.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4640\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Library Path in Hadoop Shell Script\nDESCRIPTION: Adds JAVA_LIBRARY_PATH to LD_LIBRARY_PATH in the hadoop-config.sh script to ensure native libraries are properly loaded.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP-8781. hadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH.\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Environment Variables in Bash\nDESCRIPTION: Modifies the hadoop-config.sh script to add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH, ensuring proper library loading for Hadoop processes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nhadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH.\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Library Path in Hadoop Shell Script\nDESCRIPTION: Adds JAVA_LIBRARY_PATH to LD_LIBRARY_PATH in the hadoop-config.sh script to ensure native libraries are properly loaded.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP-8781. hadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH.\n```\n\n----------------------------------------\n\nTITLE: Handling Concurrent CopySucceeded and CopyFailed in Shuffle for Java\nDESCRIPTION: This fix addresses a null pointer exception (NPE) issue in the shuffle phase caused by concurrent calls to copySucceeded() and copyFailed() methods on the same host. It improves thread safety in the shuffle handler.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\ncopySucceeded()\ncopyFailed()\n```\n\n----------------------------------------\n\nTITLE: Configuring LZO Text File Input Format\nDESCRIPTION: Code reference showing LZO compressed text file handling functionality that was added. Enables splitting of LZO compressed files for processing.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4640\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Library Path in Hadoop Shell Script\nDESCRIPTION: Adds JAVA_LIBRARY_PATH to LD_LIBRARY_PATH in the hadoop-config.sh script to ensure native libraries are properly loaded.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP-8781. hadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH.\n```\n\n----------------------------------------\n\nTITLE: Preserving Resource Request Order in YARN\nDESCRIPTION: This improvement uses LinkedHashMap to maintain the order of resource requests, which is important for certain scheduling decisions and consistency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3465. Use LinkedHashMap to preserve order of resource requests. \n(Zhihai Xu via kasha)\n```\n\n----------------------------------------\n\nTITLE: Fixing HttpServer Startup Without Hostname\nDESCRIPTION: Allows HttpServer to start even when a hostname is not specified.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nHttpServer can't start if hostname is not specified.\n```\n\n----------------------------------------\n\nTITLE: Native Library Build Configuration\nDESCRIPTION: Ant build configuration to enable optional native library compilation. Specifies -Dcompile.native flag to build libhadoop.so for improved zlib compression performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\n-Dcompile.native\n```\n\n----------------------------------------\n\nTITLE: AppInfo Class Methods for Resource and Label Management\nDESCRIPTION: Collection of getter methods in AppInfo class for retrieving allocated resources (CPU, memory), node label expressions, priority, container count and application management status.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppInfo {\n    private boolean unmanagedApplication;\n    private int priority;\n    private int runningContainers;\n    \n    public int getAllocatedCpuVcores()\n    public int getAllocatedMemoryMB()\n    public String getAmNodeLabelExpression()\n    public String getAppNodeLabelExpression()\n    public int getPriority()\n    public int getRunningContainers()\n    public boolean isUnmanagedApp()\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Out-of-Band Heartbeat in MapReduce TaskTracker\nDESCRIPTION: Adds a new configuration property to optionally enable out-of-band heartbeats from the TaskTracker to the JobTracker upon task completion, improving job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nadd mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated Heap Size Configuration in Hadoop\nDESCRIPTION: This snippet shows the removal of a deprecated configuration property for child heap size in Hadoop's MapReduce. It also mentions fixing indentation issues in the TaskRunner class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: XML\nCODE:\n```\nConfiguration changes to hadoop-default.xml:\n  remove mapred.child.heap.size\n```\n\n----------------------------------------\n\nTITLE: Configuration Property Example\nDESCRIPTION: Example of Hadoop configuration property for LDAP search filter\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nhadoop.security.group.mapping.ldap.search.filter.user\n```\n\n----------------------------------------\n\nTITLE: Creating File Status API Change\nDESCRIPTION: Example of API deprecation where FileSystem.listPaths was replaced with listStatus and FileStatus became a concrete class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.listStatus\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in YarnConfiguration Class\nDESCRIPTION: Method calls related to configuration settings in YarnConfiguration class that lack proper documentation blocks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.conf.YarnConfiguration.isAclEnabled(org.apache.hadoop.conf.Configuration)\norg.apache.hadoop.yarn.conf.YarnConfiguration.isDistSchedulingEnabled(org.apache.hadoop.conf.Configuration)\norg.apache.hadoop.yarn.conf.YarnConfiguration.isOpportunisticContainerAllocationEnabled(org.apache.hadoop.conf.Configuration)\norg.apache.hadoop.yarn.conf.YarnConfiguration.main(java.lang.String[])\n```\n\n----------------------------------------\n\nTITLE: Updating CMake Build Configuration for Java Paths\nDESCRIPTION: Modifies CMake configuration to always use JAVA_HOME for finding libjvm.so, jni.h, and jni_md.h, improving build consistency across environments.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nHADOOP-8737. cmake: always use JAVA_HOME to find libjvm.so, jni.h, jni_md.h.\n```\n\n----------------------------------------\n\nTITLE: Serializing Task encrypted spill key\nDESCRIPTION: Modified the serialization of the Task#encryptedSpillKey field to occur at the end of the serialization process. This change affects how task data is serialized.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6387. Serialize the recently added Task#encryptedSpillKey field at \\nthe end. (Arun Suresh via kasha)\n```\n\n----------------------------------------\n\nTITLE: Missing @SINCE Tag for AMRMClient and AMRMClientAsync Methods\nDESCRIPTION: The updateTrackingUrl and waitFor methods in both AMRMClient and AMRMClientAsync classes are missing @SINCE tags. These methods are used for updating tracking URLs and implementing wait conditions in YARN applications.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method updateTrackingUrl(java.lang.String)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method updateTrackingUrl(java.lang.String)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method waitFor(java.util.function.Supplier)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method waitFor(java.util.function.Supplier, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method waitFor(java.util.function.Supplier, int, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Optimizing PureJavaCrc32 Performance in Java\nDESCRIPTION: Improves the performance of the PureJavaCrc32 implementation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7333. Performance improvement in PureJavaCrc32.\n```\n\n----------------------------------------\n\nTITLE: Implementing AES-CTR CryptoCodec using JNI to OpenSSL in Java\nDESCRIPTION: This code implements the AES-CTR CryptoCodec using Java Native Interface (JNI) to interface with OpenSSL for improved encryption performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10693. Implementation of AES-CTR CryptoCodec using JNI to OpenSSL. (Yi Liu via cmccabe)\n```\n\n----------------------------------------\n\nTITLE: Implementing AES-CTR CryptoCodec using JNI to OpenSSL in Java\nDESCRIPTION: This code implements the AES-CTR CryptoCodec using Java Native Interface (JNI) to interface with OpenSSL for improved encryption performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10693. Implementation of AES-CTR CryptoCodec using JNI to OpenSSL. (Yi Liu via cmccabe)\n```\n\n----------------------------------------\n\nTITLE: Executing Hadoop Setup Application Script in Bash\nDESCRIPTION: This snippet shows the addition of a new Bash script 'hadoop-setup-application.sh' for creating application directories in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhadoop-setup-application.sh\n```\n\n----------------------------------------\n\nTITLE: Disabling Filesystem Instance Caching in Hadoop\nDESCRIPTION: This bug fix allows caching of filesystem instances to be disabled on a per-instance basis. It's part of the bug fixes for release 0.20.2.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-6231. Allow caching of filesystem instances to be disabled on a\nper-instance basis. (tomwhite)\n```\n\n----------------------------------------\n\nTITLE: Adding StringUtils.split Method for Non-escaped Single-character Separator in Java\nDESCRIPTION: Introduces a new method in StringUtils class to split strings using a non-escaped single-character separator. This provides a more efficient way to split strings in certain scenarios.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nStringUtils.split(String input, char separator)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Constructor - ContainerContext\nDESCRIPTION: Constructor for ContainerContext class with missing documentation block. Takes container identification and resource parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nConstructor (java.lang.String, org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.Resource, org.apache.hadoop.yarn.server.api.ContainerType)\n```\n\n----------------------------------------\n\nTITLE: Configuration Addition for Out-of-Band Heartbeat in TaskTracker\nDESCRIPTION: Configuration property added for enabling out-of-band heartbeats from TaskTracker to JobTracker upon task completion, which helps improve job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nadd mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: DFSClient Method Deprecations - Java\nDESCRIPTION: Documentation of removed deprecated DFSClient methods including getHints() and isDirectory().\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ngetHints(..)\nisDirectory(..)\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Logging Levels in Java\nDESCRIPTION: This snippet shows the addition of two new configuration parameters to control the logging level of map and reduce tasks in MapReduce jobs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nadd mapred.map.child.log.level \nadd mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Updating JobClient Constructor in Hadoop\nDESCRIPTION: Changes the JobClient constructor to accept a JobConf object instead of a Configuration object.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\npublic JobClient(JobConf conf) { ... }\n```\n\n----------------------------------------\n\nTITLE: Correcting Locking in FsVolumeList for HDFS Java DataNodes\nDESCRIPTION: Fixes incorrect locking in the FsVolumeList#checkDirs method that could cause DataNodes to hang. This improvement enhances the reliability of DataNode operations involving volume checks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7489. Incorrect locking in FsVolumeList#checkDirs can hang datanodes\n```\n\n----------------------------------------\n\nTITLE: Updating Maven POM for Staging URL in distributionManagement\nDESCRIPTION: Parameterizes the staging URL in the Maven POM file's distributionManagement section for consistency across builds.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<distributionManagement>\n  <site>\n    <id>apache-website</id>\n    <url>${staging.url}</url>\n  </site>\n</distributionManagement>\n```\n\n----------------------------------------\n\nTITLE: Configuration Path Reference\nDESCRIPTION: File path reference for hadoop-hdfs test directory showing location of bit-rotted unit tests that needed rehabilitation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhadoop-hdfs-project/hadoop-hdfs/src/test/unit/\n```\n\n----------------------------------------\n\nTITLE: Updating Commons Net Version in Java\nDESCRIPTION: This snippet updates the version of the Commons Net library to 3.1.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8211. Update commons-net version to 3.1. (eli)\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag in JournalNode getStorageInfos Method\nDESCRIPTION: The getStorageInfos() method in JournalNodeMXBean interface is missing the @since JavaDoc tag that documents the version when this method was introduced.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean.getStorageInfos()\n```\n\n----------------------------------------\n\nTITLE: Setting JVM Option for Heap Dump on OutOfMemoryError\nDESCRIPTION: Adds the JVM option to generate a heap dump file when an OutOfMemoryError occurs, improving debugging capabilities for memory-related issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\n-XX:+HeapDumpOnOutOfMemoryError\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter for Hadoop 2.8.2 Release Announcement\nDESCRIPTION: YAML frontmatter defining the title and date of the release announcement for Apache Hadoop 2.8.2. This metadata is typically used by static site generators to process and display page information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/2.8.2.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Release 2.8.2 available\ndate: 2017-10-24\n---\n```\n\n----------------------------------------\n\nTITLE: File Path Separator Usage in Windows\nDESCRIPTION: Reference to path separator handling specifically for Windows environments, from HADOOP-8164.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nBack slash as path separator is handled for Windows only\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration Methods in Java\nDESCRIPTION: Adds Configuration.getStrings(name, default-value) and setStrings methods for improved configuration handling.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration.getStrings(name, default-value);\nConfiguration.setStrings(name, values);\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter for Release Announcement\nDESCRIPTION: YAML front matter defining metadata for the release announcement page including title, date, and content type.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/2.6.5.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Release 2.6.5 available\ndate: \"2016-10-08\"\ntype: release\n---\n```\n\n----------------------------------------\n\nTITLE: Using Counters in MapReduce Streaming Environment\nDESCRIPTION: This bash example shows how to increment counters in a Hadoop streaming job using stderr messages. It demonstrates the syntax for incrementing custom counters in non-Java MapReduce implementations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nreporter:counter:<group>,<counter>,<amount>\n```\n\n----------------------------------------\n\nTITLE: Fixing Maven Site Generation\nDESCRIPTION: Resolves issues with generating the Maven site for Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nFix 'mvn site'.\n```\n\n----------------------------------------\n\nTITLE: Defining YARN Configuration Fields in Java\nDESCRIPTION: This snippet shows the declaration of various configuration fields used in the YarnConfiguration class. These fields are used to set different parameters for YARN operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nNO DOC BLOCK: org.apache.hadoop.yarn.conf.YarnConfiguration Field AM_SCHEDULING_NODE_BLACKLISTING_DISABLE_THRESHOLD\nNO DOC BLOCK: org.apache.hadoop.yarn.conf.YarnConfiguration Field AM_SCHEDULING_NODE_BLACKLISTING_ENABLED\nNO DOC BLOCK: org.apache.hadoop.yarn.conf.YarnConfiguration Field AMRM_PROXY_ADDRESS\nNO DOC BLOCK: org.apache.hadoop.yarn.conf.YarnConfiguration Field AMRM_PROXY_CLIENT_THREAD_COUNT\nMISSING @SINCE TAG: org.apache.hadoop.yarn.conf.YarnConfiguration Field AMRM_PROXY_ENABLED\nNO DOC BLOCK: org.apache.hadoop.yarn.conf.YarnConfiguration Field AMRM_PROXY_HA_ENABLED\nNO DOC BLOCK: org.apache.hadoop.yarn.conf.YarnConfiguration Field AMRM_PROXY_INTERCEPTOR_CLASS_PIPELINE\nNO DOC BLOCK: org.apache.hadoop.yarn.conf.YarnConfiguration Field APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC\n```\n\n----------------------------------------\n\nTITLE: Refactoring DelegationTokenAuthenticationFilter in Java\nDESCRIPTION: Small refactoring of DelegationTokenAuthenticationFilter class to allow code sharing. This improvement was added in Hadoop 2.7.1.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-12103. Small refactoring of DelegationTokenAuthenticationFilter to allow code sharing. (Yongjun Zhang)\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: HTML comment block containing the Apache License 2.0 header information for the documentation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/mailing_lists.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!---\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n```\n\n----------------------------------------\n\nTITLE: Fixing NPE in TestCapacityScheduler in Java\nDESCRIPTION: Resolves a null pointer exception in the TestCapacityScheduler class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5068. Fix NPE in TestCapacityScheduler.  (Vinod Kumar Vavilapalli\nvia szetszwo)\n```\n\n----------------------------------------\n\nTITLE: Configuring Speculative Execution in Hadoop MapReduce\nDESCRIPTION: Configuration changes to hadoop-default.xml to allow finer-grained control over speculative execution. Users can now set it for maps and reduces independently.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: XML\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Optimizing Datanode Writes in Java\nDESCRIPTION: Optimization to improve datanode write performance for small writes and flushes. This change likely involves tuning internal buffers or write algorithms.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-8722. Optimize datanode writes for small writes and flushes (kihwal)\n```\n\n----------------------------------------\n\nTITLE: Adding Maximum AM Resource Configuration for CapacityScheduler\nDESCRIPTION: Configuration change for MAPREDUCE-2697 that enhances CapacityScheduler to cap concurrently running applications per-queue & per-user by adding a parameter to control maximum Application Master resource percentage.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: configuration\nCODE:\n```\nadd yarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Improving BlockPlacementPolicy in Java for HDFS\nDESCRIPTION: Abstract BlockManager's rack policy into BlockPlacementPolicy to improve block placement flexibility.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-8647. Abstract BlockManager's rack policy into BlockPlacementPolicy.\n```\n\n----------------------------------------\n\nTITLE: Versioning ShuffleHandler DB Schema in Java\nDESCRIPTION: This improvement versions the ShuffleHandler database schema to handle compatible and incompatible changes. It enhances the maintainability and upgradeability of the ShuffleHandler component.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nShuffleHandler.getSchema()\n```\n\n----------------------------------------\n\nTITLE: HDFS NameNode Port Separation\nDESCRIPTION: Enhancement to allow NameNode to use separate ports for service and client requests, improving security and resource management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nHDFS-599. Allow NameNode to have a seprate port for service requests from client requests.\n```\n\n----------------------------------------\n\nTITLE: HTML Apache License Comment Block\nDESCRIPTION: Standard Apache License 2.0 comment block used in HTML/Markdown files to specify licensing terms.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/0.15.3.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!---\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n```\n\n----------------------------------------\n\nTITLE: Extending LightWeightGSet for expired element eviction in Java\nDESCRIPTION: Extends the LightWeightGSet class to support eviction of expired elements, improving memory management for caches and other data structures.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nLightWeightGSet\n```\n\n----------------------------------------\n\nTITLE: Missing JavaDoc for DelegationTokenAuthenticator\nDESCRIPTION: Field SERVICE_PARAM in DelegationTokenAuthenticator class lacks required documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.5/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.SERVICE_PARAM\n```\n\n----------------------------------------\n\nTITLE: Defining YARN Configuration Constants in Java\nDESCRIPTION: This snippet shows multiple field declarations for YARN configuration constants. These constants are likely used throughout the YARN codebase to reference specific configuration properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n  public static final String DEFAULT_YARN_WORKFLOW_ID_TAG_PREFIX;\n  public static final String DEFAULT_ZK_APPID_NODE_SPLIT_INDEX;\n  public static final String DEFAULT_ZK_DELEGATION_TOKEN_NODE_SPLIT_INDEX;\n  public static final String DELEGATED_CENTALIZED_NODELABEL_CONFIGURATION_TYPE;\n  public static final String DISABLED_RM_PLACEMENT_CONSTRAINTS_HANDLER;\n  public static final String DISK_VALIDATOR;\n  public static final String DISPLAY_APPS_FOR_LOGGED_IN_USER;\n  public static final String DIST_SCHEDULING_ENABLED;\n  // ... many more field declarations\n}\n```\n\n----------------------------------------\n\nTITLE: Closing FileReader in MiniKdc\nDESCRIPTION: Ensures that the FileReader created in MiniKdc's main method is properly closed.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nMiniKdc#main() should close the FileReader it creates.\n```\n\n----------------------------------------\n\nTITLE: Deprecated Configuration Property in hadoop-default.xml\nDESCRIPTION: Shows the deprecation of the io.seqfile.compression.type configuration property in hadoop-default.xml as part of HADOOP-2869 changes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_29\n\nLANGUAGE: xml\nCODE:\n```\nio.seqfile.compression.type\n```\n\n----------------------------------------\n\nTITLE: Verifying Release Checksums Using gpg in Linux/Unix\nDESCRIPTION: Command to verify the integrity of downloaded Hadoop release files by checking the SHA-256 checksum with GPG. This ensures the download matches the official release and has not been tampered with.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpg --print-md SHA256 hadoop-X.Y.Z.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Fixing task scheduling issue in JobTracker in Java\nDESCRIPTION: Addresses an issue where JobTracker might schedule two attempts of the same task with the same attempt ID across restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5394. JobTracker might schedule 2 attempts of the same task \nwith the same attempt id across restarts. (Amar Kamat via sharad)\n```\n\n----------------------------------------\n\nTITLE: Resolving Missing cglib Dependency\nDESCRIPTION: Addresses a missing cglib dependency when building RPMs against Google Guice versions above 3.0.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nmvn-rpmbuild against google-guice > 3.0 yields missing cglib dependency\n```\n\n----------------------------------------\n\nTITLE: Adding New MapReduce Configuration Properties\nDESCRIPTION: New configuration properties are added to allow separate settings for map and reduce JVM parameters, environment variables, and ulimit. Some existing properties are deprecated.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nadd mapred.map.child.java.opts\nadd mapred.reduce.child.java.opts\nadd mapred.map.child.env\nadd mapred.reduce.child.ulimit\nadd mapred.map.child.env\nadd mapred.reduce.child.ulimit\ndeprecated mapred.child.java.opts\ndeprecated mapred.child.env\ndeprecated mapred.child.ulimit\n```\n\n----------------------------------------\n\nTITLE: Fixing Block Deletion in HDFS DataNode (Java)\nDESCRIPTION: Prevents the block receiver from removing a block that's being created or written by other threads.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5192. Block receiver should not remove a block that's created or\nbeing written by other threads. (hairong)\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Speculative Execution in hadoop-default.xml\nDESCRIPTION: Updates to provide finer-grained control over speculative execution, allowing independent configuration for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Missing @Since Tags in YARN Tracking URL Update Methods\nDESCRIPTION: Methods for updating tracking URLs in AMRMClient and AMRMClientAsync classes that need version documentation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient.updateTrackingUrl(java.lang.String)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.updateTrackingUrl(java.lang.String)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Application Information in YARN (Java)\nDESCRIPTION: This snippet demonstrates methods for accessing various details of a YARN application, including resource allocation, priority, launch time, and node label expressions. It also includes methods to check if the application is unmanaged.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppInfo {\n    public Resource getAggregatePreemptedResourceAllocation() { /* ... */ }\n    public Resource getAggregateResourceAllocation() { /* ... */ }\n    public int getAllocatedCpuVcores() { /* ... */ }\n    public int getAllocatedGpus() { /* ... */ }\n    public long getAllocatedMemoryMB() { /* ... */ }\n    public String getAmNodeLabelExpression() { /* ... */ }\n    public String getAppNodeLabelExpression() { /* ... */ }\n    public long getLaunchTime() { /* ... */ }\n    public int getPriority() { /* ... */ }\n    public int getReservedCpuVcores() { /* ... */ }\n    public int getReservedGpus() { /* ... */ }\n    public long getReservedMemoryMB() { /* ... */ }\n    public int getRunningContainers() { /* ... */ }\n    public boolean isUnmanagedApp() { /* ... */ }\n    private int priority;\n    private int runningContainers;\n    private boolean unmanagedApplication;\n}\n```\n\n----------------------------------------\n\nTITLE: Updating FSNamesystem Method Signatures in Java\nDESCRIPTION: Changes method signatures in the FSNamesystem class. Removes getNameNodeInfoPort() and replaces getDFSNameNodeMachine() and getDFSNameNodePort() with getDFSNameNodeAddress().\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nFSNamesystem.getNameNodeInfoPort()\nFSNamesystem.getDFSNameNodeMachine()\nFSNamesystem.getDFSNameNodePort()\nFSNamesystem.getDFSNameNodeAddress()\n```\n\n----------------------------------------\n\nTITLE: Improving Block Report Error Handling in HDFS Java\nDESCRIPTION: Enhances logging for block report RPC failures in HDFS. This improvement helps with debugging issues related to communication between DataNodes and NameNodes during block reporting.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7579. Improve log reporting during block report rpc failure.\n```\n\n----------------------------------------\n\nTITLE: FileOutputCommitter Credentials Cache Fix - Java\nDESCRIPTION: Bug fix for MAPREDUCE-5240 where the initialized Credentials cache appears to be empty inside FileOutputCommitter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nMAPREDUCE-5240 inside of FileOutputCommitter the initialized Credentials cache appears to be empty.\n```\n\n----------------------------------------\n\nTITLE: Java Package Structure Changes - HDFS Refactoring\nDESCRIPTION: Breaking change that restructures the hadoop.dfs package into separate packages under hadoop.hdfs to better organize client, server and protocol components. This change makes DistributedFileSystem and DFSClient package private.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nhadoop.dfs.* -> hadoop.hdfs.[client|server|protocol].*\n```\n\n----------------------------------------\n\nTITLE: Configuring MapFileOutputFormat Index Interval\nDESCRIPTION: Configuration options for map-reduce jobs regarding sort buffer management and spill thresholds\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: properties\nCODE:\n```\nio.sort.spill.percent=80\\nio.sort.record.percent=5\n```\n\n----------------------------------------\n\nTITLE: FileOutputCommitter Credentials Bug Fix\nDESCRIPTION: Code snippet mentioned in MAPREDUCE-5240 bug fix where initialized Credentials cache appears empty in FileOutputCommitter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nFileOutputCommitter the initialized Credentials cache appears to be empty\n```\n\n----------------------------------------\n\nTITLE: Adding getJobName() Method to RunningJob Interface in Java\nDESCRIPTION: Adds a new getJobName() method to the RunningJob interface, allowing retrieval of the job name for a running Hadoop job.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_30\n\nLANGUAGE: Java\nCODE:\n```\nRunningJob#getJobName()\n```\n\n----------------------------------------\n\nTITLE: Fixing unsafe long to int conversion in UncompressedSplitLineReader\nDESCRIPTION: Fixes an unsafe long to int conversion in UncompressedSplitLineReader that was causing an IndexOutOfBoundsException.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6635. Unsafe long to int conversion in UncompressedSplitLineReader\nand IndexOutOfBoundsException. (Junping Du via vvasudev)\n```\n\n----------------------------------------\n\nTITLE: HDFS URL Configuration\nDESCRIPTION: Addition of hdfs:// protocol to fs.default.name configuration parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: xml\nCODE:\n```\nhdfs://\n```\n\n----------------------------------------\n\nTITLE: Implementing RetryCache utility for RPC retries in Java\nDESCRIPTION: Adds a new RetryCache utility class to implement RPC retries, improving the reliability of remote procedure calls in distributed systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nRetryCache\n```\n\n----------------------------------------\n\nTITLE: Adding Hadoop Setup Application Script in Bash\nDESCRIPTION: Added a new Bash script 'hadoop-setup-application.sh' for creating application directories in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nhadoop-setup-application.sh\n```\n\n----------------------------------------\n\nTITLE: Fixing resource update in MapReduce ApplicationMaster\nDESCRIPTION: Fixes the MapReduce ApplicationMaster to properly update resource asks after ramping down all reducers, avoiding job hangs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6514. Fixed MapReduce ApplicationMaster to properly updated\nresources ask after ramping down of all reducers avoiding job hangs. (Varun\nSaxena and Wangda Tan via vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Configuring Speculative Execution in Hadoop XML\nDESCRIPTION: Updates to hadoop-default.xml to deprecate mapred.speculative.execution and add new properties for map and reduce task speculative execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: XML\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Updating AM-RM tokens in MapReduce uber jobs\nDESCRIPTION: Fixed MapReduce uber jobs to not fail the update of AM-RM tokens when they roll-over. This addresses an issue with token refresh in uber job execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6324. Fixed MapReduce uber jobs to not fail the udpate of AM-RM\\ntokens when they roll-over. (Jason Lowe via vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Constructor - ContainerTerminationContext\nDESCRIPTION: Constructor for container termination context with missing documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nConstructor (java.lang.String, org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.Resource, org.apache.hadoop.yarn.server.api.ContainerType)\n```\n\n----------------------------------------\n\nTITLE: Setting Default YARN WebApp HTTP Address\nDESCRIPTION: Adds the webapp.http.address property to yarn-default.xml to ensure the default installation with HTTPS enabled doesn't have a broken link on the NodeManager UI.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>yarn.nodemanager.webapp.http.address</name>\n  <value>0.0.0.0:8042</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Implementing ONCRPC and XDR protocols in Java\nDESCRIPTION: This change implements the Open Network Computing Remote Procedure Call (ONCRPC) protocol and External Data Representation (XDR) in Hadoop. It enhances Hadoop's networking capabilities.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9509. Implement ONCRPC and XDR. (brandonli)\n```\n\n----------------------------------------\n\nTITLE: Configuration Property Example - Configuration#getLongBytes\nDESCRIPTION: Example of a configuration property method being added to handle human readable byte size values, referenced in HADOOP-7910.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\nConfiguration.getLongBytes\n```\n\n----------------------------------------\n\nTITLE: Configuring Mini YARN Cluster Property (Java)\nDESCRIPTION: Fixes an incorrect property name for configuring a mini YARN cluster in YarnConfiguration. The correct property name is IS_MINI_YARN_CLUSTER.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nIS_MINI_YARN_CLUSTER\n```\n\n----------------------------------------\n\nTITLE: Addressing NameNode stability issues in Java\nDESCRIPTION: Fixes stability issues with NameNode on clusters with little available space.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3810. NameNode seems unstable on a cluster with little space left.\n(hairong)\n```\n\n----------------------------------------\n\nTITLE: Configuring Mini YARN Cluster Property (Java)\nDESCRIPTION: Fixes an incorrect property name for configuring a mini YARN cluster in YarnConfiguration. The correct property name is IS_MINI_YARN_CLUSTER.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nIS_MINI_YARN_CLUSTER\n```\n\n----------------------------------------\n\nTITLE: Configuration XML Changes for Job Status Persistence in HDFS\nDESCRIPTION: Configuration additions to hadoop-default.xml for persisting job statuses in HDFS, allowing JobClient to query information about decommissioned jobs across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                              /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Fixing CMake Shared Library Version Detection\nDESCRIPTION: This change fixes an issue in the CMakeLists.txt file related to detecting shared library versions without version numbers, improving the build process for Hadoop native libraries.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nHADOOP-11250. fix endmacro of set_find_shared_library_without_version in\n    CMakeLists (Yi Liu via Colin P. McCabe)\n```\n\n----------------------------------------\n\nTITLE: Project Configuration Change - Version Update\nDESCRIPTION: Document entry showing configuration changes to Hadoop project. The update involves removing legacy Avro RPC system and implementing Protocol Buffer based RPC engine.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nHADOOP-7920. Remove Avro Rpc. (suresh)\nHADOOP-7773. Add support for protocol buffer based RPC engine.\n```\n\n----------------------------------------\n\nTITLE: Configuration Addition for JVM Reuse in MapReduce\nDESCRIPTION: Adds a new configuration parameter 'mapred.job.reuse.jvm.num.tasks' to hadoop-default.xml to enable JVM reuse across Map-Reduce tasks for improved performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Bash Script Path Configuration\nDESCRIPTION: Fix for bin/slaves.sh script to explicitly specify /bin/bash instead of /bin/sh as the shell interpreter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n/bin/bash rather than /bin/sh\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Logging Levels in Java\nDESCRIPTION: Adds new configuration parameters to control the logging level of map and reduce tasks independently.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nadd mapred.map.child.log.level \nadd mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Fixing Eclipse Classpath for MapReduce\nDESCRIPTION: Adds Avro jar to the Eclipse classpath for MapReduce development.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: XML\nCODE:\n```\n<classpathentry kind=\"lib\" path=\"lib/avro-${avro.version}.jar\"/>\n```\n\n----------------------------------------\n\nTITLE: Initializing Configuration for Hadoop FileSystem in Java\nDESCRIPTION: This snippet shows how to initialize a Configuration object for the Hadoop FileSystem. It demonstrates setting the verification of checksums.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nViewFileSystem.setVerifyChecksum()\n```\n\n----------------------------------------\n\nTITLE: Adding IFile Readahead in MapReduce in Java\nDESCRIPTION: Implements readahead functionality for IFile to improve I/O performance in MapReduce jobs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-4511. Add IFile readahead\n```\n\n----------------------------------------\n\nTITLE: Improving Concurrent Access on FsVolumeList in HDFS\nDESCRIPTION: Enhances concurrent access handling for the FsVolumeList in HDFS, likely improving performance and reducing contention in multi-threaded scenarios.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7531. Improve the concurrent access on FsVolumeList (Lei Xu via Colin P. McCabe)\n```\n\n----------------------------------------\n\nTITLE: CMake build configuration update\nDESCRIPTION: Update to CMake build system to use JAVA_HOME for finding JVM dependencies like libjvm.so, jni.h, and jni_md.h.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# HADOOP-8737\ncmake: always use JAVA_HOME to find libjvm.so, jni.h, jni_md.h\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Method References in Hadoop YARN API\nDESCRIPTION: List of Java method signatures with missing @SINCE tags or documentation blocks in the YARN API. These methods belong to various classes handling application reports, container management, resource allocation, and cluster metrics.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.api.records.ApplicationReport.getAmNodeLabelExpression()\norg.apache.hadoop.yarn.api.protocolrecords.AllocateResponse.getApplicationPriority()\norg.apache.hadoop.yarn.api.records.ApplicationSubmissionContext.getApplicationSchedulingPropertiesMap()\norg.apache.hadoop.yarn.api.records.ApplicationReport.getApplicationTimeouts()\norg.apache.hadoop.yarn.api.records.ApplicationSubmissionContext.getApplicationTimeouts()\n```\n\n----------------------------------------\n\nTITLE: File System Creation API\nDESCRIPTION: Implementation of non-recursive file creation functionality mentioned in HADOOP-6840\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.createNonRecursive()\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test Methods in Bash\nDESCRIPTION: This snippet shows how to run a set of specific test methods within a single test class in the Hadoop project. It's likely part of a testing framework or script used in the development process.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nRunning a set of methods in a Single Test Class.\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Front Matter for Hadoop Release Announcement\nDESCRIPTION: Specifies the title and date of the Hadoop 2.10.0 release announcement using YAML front matter in a markdown file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/2.10.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Release 2.10.0 available\ndate: 2019-10-29\n---\n```\n\n----------------------------------------\n\nTITLE: MapReduce Admin Command Options Addition\nDESCRIPTION: Added new administrative command options for MapReduce Application Master as part of MAPREDUCE-4810.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-4810. Added new admin command options for MR AM. (Jerry Chen via\\nvinodkv)\n```\n\n----------------------------------------\n\nTITLE: Disabling Memory Monitoring in MiniMRYarnCluster\nDESCRIPTION: Disables memory monitoring by default in MiniMRYarnCluster to avoid failures in downstream tests.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5094. Disabled memory monitoring by default in MiniMRYarnCluster to avoid some downstream tests failing. (Siddharth Seth via vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Configuration File Include Mechanism\nDESCRIPTION: Enhancement allowing configuration files to include other configuration files, providing a modular approach to Hadoop configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration.include\n```\n\n----------------------------------------\n\nTITLE: Configuration File Include Mechanism\nDESCRIPTION: Enhancement allowing configuration files to include other configuration files, providing a modular approach to Hadoop configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration.include\n```\n\n----------------------------------------\n\nTITLE: Undocumented Methods and Fields in YARN DAO Classes\nDESCRIPTION: List of Java methods and fields that are missing proper documentation blocks in the org.apache.hadoop.yarn.server.webapp.dao package. These elements need documentation for better code maintainability and API documentation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// AppInfo class methods\ngetAllocatedCpuVcores()\ngetAllocatedMemoryMB()\ngetAmNodeLabelExpression()\ngetAppNodeLabelExpression()\ngetPriority()\ngetReservedCpuVcores()\ngetReservedMemoryMB()\ngetRunningContainers()\nisUnmanagedApp()\n\n// AppAttemptInfo class methods\ngetFinishedTime()\ngetStartedTime()\n\n// ContainerInfo class methods\ngetAllocatedResources()\ngetNodeId()\n\n// Fields\nAppInfo.priority\nAppInfo.runningContainers\nAppInfo.unmanagedApplication\nAppAttemptInfo.finishedTime\nAppAttemptInfo.startedTime\nContainerInfo.allocatedResources\nContainerInfo.nodeId\n```\n\n----------------------------------------\n\nTITLE: MapReduce Configuration Property Addition\nDESCRIPTION: Configuration change to support out-of-band heartbeat functionality in the TaskTracker for better job latency\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nmapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in FileOutputCommitter Fields\nDESCRIPTION: Constants and configuration fields in FileOutputCommitter class missing documentation blocks, including cleanup and failure handling parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nFILEOUTPUTCOMMITTER_CLEANUP_FAILURES_IGNORED\nFILEOUTPUTCOMMITTER_CLEANUP_FAILURES_IGNORED_DEFAULT\nFILEOUTPUTCOMMITTER_CLEANUP_SKIPPED\nFILEOUTPUTCOMMITTER_CLEANUP_SKIPPED_DEFAULT\nFILEOUTPUTCOMMITTER_FAILURE_ATTEMPTS\nFILEOUTPUTCOMMITTER_FAILURE_ATTEMPTS_DEFAULT\nFILEOUTPUTCOMMITTER_TASK_CLEANUP_ENABLED\nFILEOUTPUTCOMMITTER_TASK_CLEANUP_ENABLED_DEFAULT\n```\n\n----------------------------------------\n\nTITLE: Configuring IO Sort Parameters in Hadoop Java\nDESCRIPTION: Sets configuration variables for controlling map output sorting behavior in Hadoop. Defines thresholds for spilling and key/value index allocation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nio.sort.spill.percent - the percentages of io.sort.mb that should\n                            cause a spill (default 80%)\nio.sort.record.percent - the percent of io.sort.mb that should\n                             hold key/value indexes (default 5%)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in YARN API Methods\nDESCRIPTION: A comprehensive list of methods from Hadoop YARN API that are missing proper documentation tags or blocks. These methods belong to various classes handling container management, application resources, node management, and cluster operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\npublic interface ApplicationClientProtocol {\n    GetAttributesToNodesResponse getAttributesToNodes(GetAttributesToNodesRequest request);\n    GetClusterNodeAttributesResponse getClusterNodeAttributes(GetClusterNodeAttributesRequest request);\n    GetNewReservationResponse getNewReservation(GetNewReservationRequest request);\n    GetNodesToAttributesResponse getNodesToAttributes(GetNodesToAttributesRequest request);\n    GetResourceProfileResponse getResourceProfile(GetResourceProfileRequest request);\n    GetAllResourceProfilesResponse getResourceProfiles(GetAllResourceProfilesRequest request);\n    GetAllResourceTypeInfoResponse getResourceTypeInfo(GetAllResourceTypeInfoRequest request);\n    ReservationListResponse listReservations(ReservationListRequest request);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop NFS Gateway in hdfs-site.xml\nDESCRIPTION: XML configuration snippet for setting up the HDFS NFS Gateway, including the registration of the NFS protocol and configuring the number of server threads to handle NFS requests.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.proxyuser.nfsserver.groups</name>\n  <value>*</value>\n  <description>The 'nfsserver' user is allowed to proxy all members of all groups.\n  The value '*' means all groups.</description>\n</property>\n<property>\n  <name>hadoop.proxyuser.nfsserver.hosts</name>\n  <value>*</value>\n  <description>The 'nfsserver' user is allowed to proxy all hosts.\n  The value '*' means all hosts.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuration XML Changes for Task Tracker Settings\nDESCRIPTION: Configuration changes in hadoop-default.xml to support different numbers of mappers and reducers per TaskTracker. Adds new parameters for maximum map and reduce tasks while deprecating the old combined parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\nmapred.tasktracker.map.tasks.maximum (default value of 2)\nmapred.tasktracker.reduce.tasks.maximum (default value of 2)\nmapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: YARN Configuration Field Constants - Federation Settings\nDESCRIPTION: Constants defining federation-related configuration parameters including enable flags, timeouts, and policy settings\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic static final String FEDERATION_ENABLED;\npublic static final String FEDERATION_FAILOVER_ENABLED;\npublic static final String FEDERATION_CACHE_TIME_TO_LIVE_SECS;\npublic static final String FEDERATION_POLICY_MANAGER;\npublic static final String FEDERATION_POLICY_MANAGER_PARAMS;\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce JVM Parameters in Java\nDESCRIPTION: Code snippet showing the configuration changes for setting separate JVM parameters for map and reduce tasks. New configuration options are added for Java options, environment variables, and ulimit settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapred.map.child.java.opts\n  add mapred.reduce.child.java.opts\n  add mapred.map.child.env\n  add mapred.reduce.child.ulimit\n  add mapred.map.child.env\n  add mapred.reduce.child.ulimit\n  deprecated mapred.child.java.opts\n  deprecated mapred.child.env\n  deprecated mapred.child.ulimit\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Reuse for MapReduce Tasks in Hadoop\nDESCRIPTION: This configuration option allows reusing JVMs across multiple MapReduce tasks to improve performance. It specifies the number of tasks that can reuse a JVM.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>mapred.job.reuse.jvm.num.tasks</name>\n  <value>[number of tasks]</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Reuse in MapReduce via hadoop-default.xml\nDESCRIPTION: Configuration property added to hadoop-default.xml for controlling JVM reuse across Map-Reduce tasks. This parameter allows MapReduce jobs to reuse JVMs for multiple tasks, improving performance by avoiding JVM startup costs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: XML\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation for YARN AppInfo and ContainerInfo Methods\nDESCRIPTION: This snippet highlights multiple methods in AppInfo and ContainerInfo classes that are missing documentation blocks. These methods are related to resource allocation, node labels, and application properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.3/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// AppInfo methods\ngetAggregatePreemptedResourceAllocation()\ngetAggregateResourceAllocation()\ngetAllocatedCpuVcores()\ngetAllocatedGpus()\ngetAllocatedMemoryMB()\ngetAmNodeLabelExpression()\ngetAppNodeLabelExpression()\ngetLaunchTime()\ngetPriority()\ngetReservedCpuVcores()\ngetReservedGpus()\ngetReservedMemoryMB()\ngetRunningContainers()\nisUnmanagedApp()\n\n// ContainerInfo methods\ngetAllocatedResources() // Missing @since tag\ngetNodeId()\nhasCustomResources()\n```\n\n----------------------------------------\n\nTITLE: Configuring Core-Site.xml for HDFS URI\nDESCRIPTION: XML configuration for core-site.xml that specifies the NameNode URI for the HDFS cluster. This sets up the default filesystem name to point to the NameNode's hostname and port.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://namenode:9000</value>\n  </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Configuring Field Separators in Hadoop Streaming Jobs\nDESCRIPTION: Configuration properties to specify field separators for map and reduce input/output in Hadoop Streaming jobs. These properties allow customization of the delimiter used when processing data, with all defaults set to tab (\"\\t\").\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nstream.map.input.field.separator\nstream.map.output.field.separator\nstream.reduce.input.field.separator\nstream.reduce.output.field.separator\n```\n\n----------------------------------------\n\nTITLE: Adding task-based limit capability to JHS job cache\nDESCRIPTION: Adds the capability to set a task-based limit on the Job History Server job cache.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6622. Add capability to set JHS job cache to a\ntask-based limit (rchiang via rkanter)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in AppAttemptInfo Class Methods and Fields\nDESCRIPTION: Shows undocumented timing-related methods and fields in the AppAttemptInfo class for tracking attempt start and finish times.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.2/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass AppAttemptInfo {\n    private long startedTime;\n    private long finishedTime;\n    \n    public long getStartedTime()\n    public long getFinishedTime()\n}\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in ContainerInfo Class Methods and Fields\nDESCRIPTION: Documents missing documentation for container resource allocation and node identification in the ContainerInfo class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.2/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nclass ContainerInfo {\n    private String allocatedResources;\n    private String nodeId;\n    \n    public String getAllocatedResources()\n    public String getNodeId()\n}\n```\n\n----------------------------------------\n\nTITLE: Reintroducing Partition File Methods for Hadoop 1 Compatibility in Java\nDESCRIPTION: Fixed compatibility with Hadoop 1 in mapred.TotalOrderPartitioner by reintroducing (get,set)PartitionFile methods that take JobConf parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nmapred.TotalOrderPartitioner.(get,set)PartitionFile(JobConf)\n```\n\n----------------------------------------\n\nTITLE: AppAttemptInfo Class Methods for Timing Information\nDESCRIPTION: Methods and fields in AppAttemptInfo class for managing application attempt timing information including start and finish timestamps.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppAttemptInfo {\n    private long startedTime;\n    private long finishedTime;\n    \n    public long getStartedTime()\n    public long getFinishedTime()\n}\n```\n\n----------------------------------------\n\nTITLE: MapReduce Configuration Addition - OutOfBand Heartbeat\nDESCRIPTION: Configuration change to add a new property for out-of-band heartbeat functionality in TaskTracker to improve job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nmapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation for AppInfo.getReservedCpuVcores Method\nDESCRIPTION: Getter method in AppInfo class that returns the number of reserved CPU virtual cores. Missing proper JavaDoc documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic int getReservedCpuVcores()\n```\n\n----------------------------------------\n\nTITLE: Fixing NPE in Unmanaged Application Submission (Java)\nDESCRIPTION: Resolves a Null Pointer Exception that occurred when submitting an Unmanaged application.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/yarn/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nYARN-4452. NPE when submit Unmanaged application. (Naganarasimha G R\\nvia junping_du)\n```\n\n----------------------------------------\n\nTITLE: Implementing Configuration.getLongBytes for Human Readable Byte Sizes in Java\nDESCRIPTION: This snippet adds support for handling human readable byte size values in Hadoop's Configuration class. It implements a new method getLongBytes to parse and convert string representations of byte sizes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7910. Add Configuration.getLongBytes to handle human readable byte size values. (Sho Shimauchi via harsh)\n```\n\n----------------------------------------\n\nTITLE: Example of Adding JobName Retrieval Method in Hadoop API\nDESCRIPTION: Implementation of the HADOOP-1344 feature which adds the getJobName() method to the RunningJob interface. This allows retrieving the name of a running job.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: java\nCODE:\n```\nRunningJob#getJobName()\n```\n\n----------------------------------------\n\nTITLE: Updating MapReduce JobHistory server UGI usage\nDESCRIPTION: Fixed the MapReduce JobHistory server to use the correct (login) UserGroupInformation to refresh log and cleaner settings. This addresses issues with security and configuration updates.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-6410. Fixed MapReduce JobHistory server to use the right (login)\\nUGI to refresh log and cleaner settings. (Varun Saxena via vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Undocumented Fields in YARN DAO Classes\nDESCRIPTION: This snippet lists various fields in AppInfo, AppAttemptInfo, and ContainerInfo classes that lack documentation blocks. These fields represent important properties of YARN applications and containers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.3/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n// ContainerInfo fields\nallocatedResources\nnodeId\n\n// AppAttemptInfo fields\nfinishedTime\nstartedTime\n\n// AppInfo fields\npriority\nrunningContainers\nunmanagedApplication\n```\n\n----------------------------------------\n\nTITLE: Initializing FileSystem Home Directory in Java\nDESCRIPTION: Adds a method to get the user's home directory in a FileSystem as a fully-qualified path. Also changes getWorkingDirectory() to return a fully-qualified path.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem#getHomeDirectory()\n```\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem#getWorkingDirectory()\n```\n\n----------------------------------------\n\nTITLE: Fixing file appending in HDFS (Java)\nDESCRIPTION: Adds support for appending to files in HDFS. This change allows clients to append data to existing files in the Hadoop Distributed File System.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-1700.  Support appending to file in HDFS. (dhruba)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Constructor - ContainerContext with ExecutionType\nDESCRIPTION: Extended constructor for ContainerContext class that includes execution type parameter. Missing documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nConstructor (java.lang.String, org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.Resource, org.apache.hadoop.yarn.server.api.ContainerType, org.apache.hadoop.yarn.api.records.ExecutionType)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Reserved Memory in AppInfo Class\nDESCRIPTION: This method retrieves the amount of reserved memory in megabytes for an application. It is part of the AppInfo class in the YARN server webapp DAO.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic int getReservedMemoryMB()\n```\n\n----------------------------------------\n\nTITLE: Fixing Race Condition in LocalJobRunner\nDESCRIPTION: Bug fix to address a race condition in LocalJobRunner that was resulting in job failures. This resolves MAPREDUCE-5001.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5001. LocalJobRunner has race condition resulting in job failures\n```\n\n----------------------------------------\n\nTITLE: Missing @since tag in waitFor method with three parameters in AMRMClientAsync class\nDESCRIPTION: Documentation highlighting a missing @since JavaDoc tag in the waitFor method with Supplier and two integer parameters in the AMRMClientAsync class of the Hadoop YARN client API.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.1.4/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: File Path Addition to .gitignore\nDESCRIPTION: Simple addition of patchprocess/ directory to .gitignore file to exclude it from version control\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: text\nCODE:\n```\npatchprocess/\n```\n\n----------------------------------------\n\nTITLE: Setting Default MapReduce Shuffle Port (XML)\nDESCRIPTION: Adds the default mapreduce.shuffle.port property to mapred-default.xml. This specifies the default port used for the MapReduce shuffle process.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: XML\nCODE:\n```\nmapreduce.shuffle.port\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag in AMRMClientAsync waitFor Method\nDESCRIPTION: Java method signature for waitFor with two parameters requiring documentation tag. The method accepts a Supplier and an integer parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.1.3/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int)\n```\n\n----------------------------------------\n\nTITLE: Setting Default MapReduce Shuffle Port (XML)\nDESCRIPTION: Adds the default mapreduce.shuffle.port property to mapred-default.xml. This specifies the default port used for the MapReduce shuffle process.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: XML\nCODE:\n```\nmapreduce.shuffle.port\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in ContainerInfo Class - Container Properties\nDESCRIPTION: Methods and fields related to container properties that lack documentation blocks or @since tags in ContainerInfo class\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.6/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic class ContainerInfo {\n    private String nodeId;\n    private Map<String, Object> allocatedResources;\n\n    public String getNodeId()\n    public Map<String, Object> getAllocatedResources()\n    public Map<String, String> getExposedPorts()\n    public boolean hasCustomResources()\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS NFS Gateway in nfs-site.xml\nDESCRIPTION: XML configuration for the HDFS NFS Gateway, specifying permissions, port settings, dump directory location, and allowed client hosts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>nfs.dump.dir</name>\n  <value>/tmp/.hdfs-nfs</value>\n</property>\n\n<property>\n  <name>nfs.rtmax</name>\n  <value>1048576</value>\n  <description>This is the maximum size in bytes of a READ request supported by the NFS gateway.</description>\n</property>\n\n<property>\n  <name>nfs.wtmax</name>\n  <value>1048576</value>\n  <description>This is the maximum size in bytes of a WRITE request supported by the NFS gateway.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Fixing Block Replication in HDFS (Java)\nDESCRIPTION: Fixes an issue where blocks could remain under-replicated by providing synchronized modification to the counter xmitsInProgress in DataNode.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5465. Fix the problem of blocks remaining under-replicated by\nproviding synchronized modification to the counter xmitsInProgress in\nDataNode. (hairong)\n```\n\n----------------------------------------\n\nTITLE: Configuring Streaming Job Field Separators in Hadoop\nDESCRIPTION: Configuration parameters for specifying field separators in streaming jobs. These settings allow customization of input and output field delimiters for map and reduce operations, all defaulting to tab character.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: config\nCODE:\n```\nstream.map.input.field.separator\nstream.map.output.field.separator\nstream.reduce.input.field.separator\nstream.reduce.output.field.separator\n```\n\n----------------------------------------\n\nTITLE: Adding Pipes Facility for C++ MapReduce in Hadoop\nDESCRIPTION: Introduces the pipes facility to allow writing MapReduce programs in C++.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: C++\nCODE:\n```\n// Example C++ MapReduce program using Hadoop Pipes\n#include \"hadoop/Pipes.hh\"\n#include \"hadoop/TemplateFactory.hh\"\n#include \"hadoop/StringUtils.hh\"\n\nclass WordCountMap: public HadoopPipes::Mapper {\npublic:\n  void map(HadoopPipes::MapContext& context) {\n    // Map implementation\n  }\n};\n\nclass WordCountReduce: public HadoopPipes::Reducer {\npublic:\n  void reduce(HadoopPipes::ReduceContext& context) {\n    // Reduce implementation\n  }\n};\n\nint main(int argc, char *argv[]) {\n  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, \n                              WordCountReduce>());\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Hadoop Website YAML\nDESCRIPTION: This YAML configuration defines the structure and settings for the Apache Hadoop project website. It includes site metadata, menu items, and build parameters for generating the site.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.1.2/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License. See accompanying LICENSE file.\n\nproject_name: Apache Hadoop\nproject_description: Framework for distributed processing of large data sets across clusters of computers using simple programming models\nproject_snapshot: true\ndecoration: /WEB/hadoop.apache.org/\nstandard_notices: /WEB/hadoop.apache.org/notices/\nbase: /hadoop\nrelease_version: 3.3.6\nrelease_date: 25 July, 2023\nrelease_hash: 265ab66a93a0f78b94e3a79ee4bd70a21ecda150\ndoap_file: project.rdf\ndecorator_target: &decorator_target hadoop\nmenus:\n  - hadoop\npopulate_versions:\n  - module: hadoop\n    from: 3.3.0\n    to: 3.3.6\n    releases:\n      - 3.3.6\n      - 3.3.5\n      - 3.3.4\n      - 3.3.3\n      - 3.3.2\n      - 3.3.1\n      - 3.3.0\n    branches:\n      - branch-3.3\n    latest:\n      3.3.6:\n        release: true\nversions:\n  - module: hadoop\n    version: 3.3.6\n    default: true\n  - module: hadoop\n    version: trunk\n    unstable: true\n\nbuild:\n  plugins:\n    - asciidoc\n    - scalate\n    - assets\n    - versions2\n  assets:\n    - css\n    - images\n  directives:\n    - \"docs/current:docs/3.3.6\"\n  versions:\n    - module: hadoop\n      start: 3.3.0\n      end: 3.3.6\nproduction_url: https://hadoop.apache.org\nurl: https://hadoop.apache.org\nbase_url: /hadoop\ndestination: content\nshow_drafts: false\nfuture: true\nlsi: false\nhighligher: rouge\nmarkdown: kramdown\nstrict_front_matter: true\nkramdown:\n  input: GFM\n  syntax_highlighter: rouge\n  syntax_highlighter_opts:\n    block:\n      line_numbers: true\n      start_line: 1\nplugins:\n  - jekyll-asciidoc\n  - jekyll-redirect-from\n\nasciidoc: {}\nasciidoctor:\n  base_dir: :docdir\n  safe: unsafe\n  attributes:\n    imagesdir: /images\n    icons: font\n    source-highlighter: rouge\n    rouge-css: style\n```\n\n----------------------------------------\n\nTITLE: Configuring Capacity Scheduler Default Values in Java\nDESCRIPTION: This code snippet allows configuring default values for the Capacity Scheduler. It improves flexibility by making the default settings customizable.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4178. Make the capacity scheduler's default values configurable.\n```\n\n----------------------------------------\n\nTITLE: Updating Configuration Class Resource Loading in Java\nDESCRIPTION: Modifies the Configuration class to use URL.openStream() for finding embedded .jar resources, addressing an issue where it failed to locate these resources.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration class fails to find embedded .jar resources; should use URL.openStream()\n```\n\n----------------------------------------\n\nTITLE: Closing BufferedOutputStream in FileUtil#unpackEntries() (Java)\nDESCRIPTION: Ensures that the BufferedOutputStream in FileUtil#unpackEntries() is closed in a finally block to prevent resource leaks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10365. BufferedOutputStream in FileUtil#unpackEntries() should be closed in finally block. (Kiran Kumar M R and Sanghyun Yun via ozawa)\n```\n\n----------------------------------------\n\nTITLE: Missing JavaDoc Block in FSLimitException Class\nDESCRIPTION: The FSLimitException class in Hadoop HDFS protocol package is missing its class-level documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.hdfs.protocol.FSLimitException\n```\n\n----------------------------------------\n\nTITLE: Retrieving Container Resource Allocation in YARN (Java)\nDESCRIPTION: This method in the ContainerInfo class returns information about the resources allocated to a specific container in a YARN application.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class ContainerInfo {\n    public Object getAllocatedResources() { /* ... */ }\n    private Object allocatedResources;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Levels for MapReduce Tasks in Java\nDESCRIPTION: Code snippet showing the configuration changes for setting separate logging levels for map and reduce tasks. New configuration options are added to control log levels.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapred.map.child.log.level \n  add mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Setting MapReduce Job Counter Limit (Java)\nDESCRIPTION: Adds support for the deprecated mapreduce.job.counters.limit property in MapReduce 2. This allows configuring the maximum number of counters allowed per job.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nmapreduce.job.counters.limit\n```\n\n----------------------------------------\n\nTITLE: Running TestDFSIO Read Benchmark in Hadoop\nDESCRIPTION: This command runs the TestDFSIO benchmark in read mode, reading 10 files each with 1000MB size. It shows how to execute a file read performance test using the same files created in the write test.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ hadoop jar hadoop-*test*.jar TestDFSIO -read -nrFiles 10 -fileSize 1000\n```\n\n----------------------------------------\n\nTITLE: Fixing Resource Computation in CapacityScheduler\nDESCRIPTION: Updated the capacity scheduler to consider the maximum capacity of the queue when computing the ideal capacity for preemption.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/yarn/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1957. Consider the max capacity of the queue when computing the ideal\ncapacity for preemption.\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN Application Classpath (Shell)\nDESCRIPTION: Sets the default YARN application classpath, ensuring necessary libraries are available to YARN applications.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nexport YARN_APPLICATION_CLASSPATH=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,$HADOOP_COMMON_HOME/share/hadoop/common/lib/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,$HADOOP_YARN_HOME/share/hadoop/yarn/*,$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*\n```\n\n----------------------------------------\n\nTITLE: Downloading Hadoop SHA-512 checksum file using wget\nDESCRIPTION: Command to download the SHA-512 checksum file for the Hadoop binary release. This file contains the cryptographic hash used to verify the integrity of the download.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://www.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz.sha512\n```\n\n----------------------------------------\n\nTITLE: Handling Token Renewal Exception in MR Client\nDESCRIPTION: Addresses an issue where the MR Client gets a renewer token exception while Oozie is submitting a job.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5088. MR Client gets an renewer token exception while Oozie is submitting a job (Daryn Sharp via cos)\n```\n\n----------------------------------------\n\nTITLE: Improving Java Serialization in Hadoop (Java)\nDESCRIPTION: Fixes the Java serialization implementation to clear the state of the serializer between objects. This change is not enabled by default.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3565. Fix the Java serialization, which is not enabled by\ndefault, to clear the state of the serializer between objects.\n(tomwhite via omalley)\n```\n\n----------------------------------------\n\nTITLE: Adding isCorrupt Field to BlockLocation in Java\nDESCRIPTION: Adds a boolean field 'isCorrupt' to the BlockLocation class. This allows for quick identification of corrupted blocks without additional API calls.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\npublic class BlockLocation {\n  private boolean isCorrupt;\n  // ... other fields and methods\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop HTTP Web UI Authentication\nDESCRIPTION: Configuration example for Hadoop's HTTP authentication settings. This property controls the authentication type used for the web interfaces of Hadoop services, with options for simple or Kerberos authentication.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.http.authentication.type</name>\n  <value>simple</value>\n  <description>\n    Defines authentication used for Hadoop HTTP web-consoles.\n    Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#\n  </description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring the Project Version in Maven POM File\nDESCRIPTION: The Maven POM file configuration showing how to set the project version. This is the core configuration for version management in Maven-based projects like Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<project>\n  ...\n  <version>2.0.0-SNAPSHOT</version>\n  ...\n</project>\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Task Tracker in hadoop-default.xml\nDESCRIPTION: Configuration updates to support different number of mappers and reducers per TaskTracker for heterogeneous clusters. Adds new parameters for maximum map and reduce tasks while deprecating the old combined parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: xml\nCODE:\n```\nmapred.tasktracker.map.tasks.maximum (default value of 2)\nmapred.tasktracker.reduce.tasks.maximum (default value of 2)\nmapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Setting JVM Options for Heap Dump on OutOfMemoryError in Java\nDESCRIPTION: Java Virtual Machine option to generate a heap dump when an OutOfMemoryError occurs. This is useful for diagnosing memory-related issues in Hadoop processes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n-XX:+HeapDumpOnOutOfMemoryError\n```\n\n----------------------------------------\n\nTITLE: Installing Hadoop Prerequisites using APT\nDESCRIPTION: Installing Java and other dependencies required for Hadoop using the APT package manager on Debian-based systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt update\nsudo apt install default-jdk openssh-server openssh-client\n```\n\n----------------------------------------\n\nTITLE: SSH Setup for HDFS Cluster Nodes\nDESCRIPTION: Shell commands to set up SSH key-based authentication between Hadoop cluster nodes. This allows the Hadoop processes to communicate between nodes without password prompting.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\n```\n\n----------------------------------------\n\nTITLE: Setting Node Manager Address Property (Java)\nDESCRIPTION: Updates the description of the yarn.nodemanager.address configuration property in YARN. This property specifies the address on which the Node Manager listens.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nyarn.nodemanager.address\n```\n\n----------------------------------------\n\nTITLE: Hadoop License Header Comment Block\nDESCRIPTION: Standard Apache License 2.0 header comment block used in Hadoop project files to specify licensing terms.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/0.20.204.0.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!---\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n```\n\n----------------------------------------\n\nTITLE: Adding NFS and Mount Interfaces in Java\nDESCRIPTION: Adds general interfaces for Network File System (NFS) and Mount protocols.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nNFS\n```\n\nLANGUAGE: Java\nCODE:\n```\nMount\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata for Hadoop 0.23.6 Release Announcement in Markdown\nDESCRIPTION: This code snippet defines the metadata for the Hadoop 0.23.6 release announcement page using YAML front matter in Markdown. It specifies the title of the page and the date of the release.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/0.23.6.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 0.23.6 available\ndate: 2013-02-07\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Child JVM Parameters in Java\nDESCRIPTION: Adds new configuration options to set JVM parameters, environment variables, and ulimit separately for map and reduce tasks. Deprecates previous combined settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapred.map.child.java.opts\n  add mapred.reduce.child.java.opts\n  add mapred.map.child.env\n  add mapred.reduce.child.ulimit\n  add mapred.map.child.env\n  add mapred.reduce.child.ulimit\n  deprecated mapred.child.java.opts\n  deprecated mapred.child.env\n  deprecated mapred.child.ulimit\n```\n\n----------------------------------------\n\nTITLE: Fixing Streaming Tests for MapReduce\nDESCRIPTION: Resolves failing streaming tests due to changes introduced in MAPREDUCE-4994.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5006. Fix failing streaming tests due to MAPREDUCE-4994. (Sandy Ryza via tomwhite)\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Setting Double Values in Configuration Class\nDESCRIPTION: Methods added to org.apache.hadoop.conf.Configuration to support getting and setting double values. This enhancement allows configuration parameters to be stored and retrieved as double-precision floating point values.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\ngetDouble() and setDouble()\n```\n\n----------------------------------------\n\nTITLE: Implementing Serialization Conversion Interface in Java\nDESCRIPTION: Adds a new Interface and default implementation to convert and restore serializations of objects to/from strings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n// New Interface for serialization conversion\n// Default implementation for object serialization\n```\n\n----------------------------------------\n\nTITLE: Fixing WritableDeserializer in Java\nDESCRIPTION: This fix ensures that the WritableDeserializer sets the Configuration on deserialized Writables, addressing a potential issue with configuration handling.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nWritableDeserializer\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Manager Queue Properties in Hadoop\nDESCRIPTION: This snippet demonstrates the naming convention for configuring resource manager queue properties in Hadoop. It allows setting various parameters related to scheduling and queue properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Properties\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Missing DAO Method Documentation in AppInfo Class\nDESCRIPTION: Missing documentation for key methods in AppInfo class that handle resource allocation, node labels, launch time, containers and application metadata.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\ngetAggregatePreemptedResourceAllocation()\ngetAggregateResourceAllocation()\ngetAllocatedCpuVcores()\ngetAllocatedGpus()\ngetAllocatedMemoryMB()\ngetAmNodeLabelExpression()\ngetAppNodeLabelExpression()\ngetLaunchTime()\ngetPriority()\ngetReservedCpuVcores()\ngetReservedGpus()\ngetReservedMemoryMB()\ngetRunningContainers()\nisUnmanagedApp()\n```\n\n----------------------------------------\n\nTITLE: Updating Shell Script Portability in Hadoop\nDESCRIPTION: Makes Hadoop shell scripts more portable by explicitly depending on bash without requiring it to be the default shell. This improves script compatibility across different Unix-like systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n```\n\n----------------------------------------\n\nTITLE: Logging Resource Localization in Java NodeManager\nDESCRIPTION: Enhances the NodeManager logging to include information about where a resource was localized. This improves traceability and debugging of resource localization.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nYARN-985. Nodemanager should log where a resource was localized\n```\n\n----------------------------------------\n\nTITLE: Configuration Key Reference - FileSystem Buffer Directory\nDESCRIPTION: Code reference showing the removal of deprecated FileSystem client buffer directory configuration key FS_CLIENT_BUFFER_DIR_KEY from CommonConfigurationKeys.java\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nFS_CLIENT_BUFFER_DIR_KEY = \"fs.client.buffer.dir\"\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Frontmatter for Hadoop 3.0.0-beta1 Release Post\nDESCRIPTION: Sets the title and date for the Hadoop 3.0.0-beta1 release announcement post using YAML frontmatter in Markdown.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/3.0.0-beta1.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 3.0.0-beta1 available\ndate: 2017-10-03\n---\n```\n\n----------------------------------------\n\nTITLE: HDFS Release Notes Entry Format\nDESCRIPTION: Standard format used for documenting changes in HDFS releases, including categorization of changes into INCOMPATIBLE CHANGES, NEW FEATURES, IMPROVEMENTS, OPTIMIZATIONS, and BUG FIXES sections.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nRelease 2.6.x - YYYY-MM-DD\n\n  INCOMPATIBLE CHANGES\n\n  NEW FEATURES\n\n  IMPROVEMENTS\n\n  OPTIMIZATIONS\n\n  BUG FIXES\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Manager Queue Properties in Hadoop\nDESCRIPTION: This snippet demonstrates the naming convention for configuring resource manager queue properties in Hadoop. It allows setting various parameters related to scheduling and queue properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Properties\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Protobuf Namespace Declaration\nDESCRIPTION: Reference to adding namespace declarations in HDFS .proto files for non-Java languages\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Protobuf\nCODE:\n```\n.proto\n```\n\n----------------------------------------\n\nTITLE: FileSystem API Changes - Java\nDESCRIPTION: Code changes mentioned in HADOOP-2566 that adds globStatus method to FileSystem interface and deprecates globPath and listPath methods for improved file listing functionality.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: plain\nCODE:\n```\nFileSystem.globStatus\nFileSystem.globPath (deprecated)\nFileSystem.listPath (deprecated)\n```\n\n----------------------------------------\n\nTITLE: Fixing DFSClient Deadlock in HDFS\nDESCRIPTION: This bug fix addresses a deadlock issue in DFSClient when closing a file and failing to renew a lease. It was fixed in version 2.6.4.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-9294. DFSClient deadlock when close file and failed to renew lease.\n```\n\n----------------------------------------\n\nTITLE: Replacing String Equality Checks with isEmpty() in Hadoop\nDESCRIPTION: Replaces string equality checks with empty string (\"\") using the String#isEmpty() method for improved readability and potentially better performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8814. Replace string equals \"\" by String#isEmpty().\n```\n\n----------------------------------------\n\nTITLE: Refactoring InputSampler in Java MapReduce\nDESCRIPTION: Moves the InputSampler class to org.apache.hadoop.mapred.lib to resolve dependency issues between examples.jar and tools.jar. This improves the project's modular structure.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4247. Move InputSampler into org.apache.hadoop.mapred.lib, so that\nexamples.jar doesn't depend on tools.jar. (omalley)\n```\n\n----------------------------------------\n\nTITLE: Setting Default YARN ResourceManager ZooKeeper Timeout in XML Configuration\nDESCRIPTION: This XML configuration snippet sets the default timeout value for the YARN ResourceManager's ZooKeeper connection. It is part of the yarn-default.xml configuration file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>yarn.resourcemanager.zk-timeout-ms</name>\n  <value>default_value</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: No code snippets present\nDESCRIPTION: This file contains only release notes and JIRA updates with no actual code snippets.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Verifying Hadoop binary release with PGP signature\nDESCRIPTION: Command to verify the authenticity of the Hadoop binary release using the downloaded PGP signature. This uses gpg (GNU Privacy Guard) to check if the signature matches the file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify hadoop-3.3.2.tar.gz.asc hadoop-3.3.2.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Configuration for YARN in Shell Script\nDESCRIPTION: Shell script snippet showing how to set YARN_APPLICATION_CLASSPATH with a default value in YarnConfiguration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nexport YARN_APPLICATION_CLASSPATH=\"$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*\"\n```\n\n----------------------------------------\n\nTITLE: Renaming DFSOutputStream flush Method in Java\nDESCRIPTION: This change renames the DFSOutputStream.flush method to DFSOutputStream.fsync for better clarity of its functionality.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\nDFSOutputStream.flush\n```\n\nLANGUAGE: Java\nCODE:\n```\nDFSOutputStream.fsync\n```\n\n----------------------------------------\n\nTITLE: YARN Configuration Field Constants - Router Settings\nDESCRIPTION: Constants for YARN Router configuration including default ports, addresses and interceptor classes\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\npublic static final String DEFAULT_ROUTER_CLIENTRM_ADDRESS;\npublic static final String DEFAULT_ROUTER_WEBAPP_ADDRESS;\npublic static final String DEFAULT_ROUTER_WEBAPP_HTTPS_ADDRESS;\npublic static final String DEFAULT_ROUTER_WEBAPP_PORT;\n```\n\n----------------------------------------\n\nTITLE: Adding Truncate API Support in Java\nDESCRIPTION: New feature to add truncate API support for Web HDFS, HDFS httpfs, and libhdfs interfaces.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7655. Expose truncate API for Web HDFS. (yliu)\n\nHDFS-7656. Expose truncate API for HDFS httpfs. (yliu)\n\nHDFS-7838. Expose truncate API for libhdfs. (yliu)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Constructor - ContainerInitializationContext\nDESCRIPTION: Constructor for container initialization context with missing documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nConstructor (java.lang.String, org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.Resource, org.apache.hadoop.yarn.server.api.ContainerType)\n```\n\n----------------------------------------\n\nTITLE: Splitting getAdditionalBlock() Method in Java\nDESCRIPTION: Improvement to split getAdditionalBlock() into two separate methods for better modularity.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-8081. Split getAdditionalBlock() into two methods. (shv)\n```\n\n----------------------------------------\n\nTITLE: YARN State Store Service Implementation\nDESCRIPTION: Implementation fix to ensure proper closure of LeveldbIterator in state store service within a finally block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nNMLeveldbStateStoreService#loadLocalizationState()\n```\n\n----------------------------------------\n\nTITLE: Configuration Property for Enabling Web UI Job Control\nDESCRIPTION: Configuration property that enables the ability to kill jobs from the Hadoop web UI. This feature is disabled by default and must be explicitly enabled.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: java\nCODE:\n```\nwebinterface.private.actions\n```\n\n----------------------------------------\n\nTITLE: Invalid JSON Output Bug Fix in FifoScheduler\nDESCRIPTION: Bug fix for FifoScheduler web service REST API that was generating invalid JSON output.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: JSON\nCODE:\n```\nMAPREDUCE-3680\n```\n\n----------------------------------------\n\nTITLE: Including Missing Header in C++ Code\nDESCRIPTION: Added unistd.h header file inclusion to HadoopPipes.cc to fix compilation issues\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n#include <unistd.h>\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in Resource Management Methods\nDESCRIPTION: Resource management related methods missing version tags across YarnClient and AMRMClient implementations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\ncreateReservation()\ngetApplications(Set, Set, Set, EnumSet)\nincreaseContainerResource(Container)\nrequestContainerResourceChange(Container, Resource)\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Configuration Script in Bash\nDESCRIPTION: Modified the Hadoop setup configuration script to add toggles for dfs.support.append, webhdfs, and hadoop proxy user settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\n# Modified setup config script\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN ResourceManager ZooKeeper Timeout\nDESCRIPTION: Sets the default value for the YARN ResourceManager ZooKeeper timeout in yarn-default.xml.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>yarn.resourcemanager.zk-timeout-ms</name>\n  <value>10000</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Fixing RpcRequestHeaderProto.callId Type in Hadoop RPC\nDESCRIPTION: Changes the type of RpcRequestHeaderProto.callId from uint32 to sint32 to match the signed nature of ipc.Client.CONNECTION_CONTEXT_CALL_ID (-3).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9944. Fix RpcRequestHeaderProto.callId to be sint32 rather than uint32 since ipc.Client.CONNECTION_CONTEXT_CALL_ID is signed (i.e. -3)\n```\n\n----------------------------------------\n\nTITLE: Deprecated JobConf Methods (Java)\nDESCRIPTION: List of deprecated methods in the JobConf class that were removed. However, the actual code snippet is incomplete in the source text.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_28\n\nLANGUAGE: java\nCODE:\n```\n// Removed methods from JobConf class\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Manager Queue Properties\nDESCRIPTION: Naming convention for configuring queue properties in the Hadoop Resource Manager. Properties follow the pattern 'hadoop.rm.queue.queue-name.property-name' to define scheduling parameters for different queues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: java\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Implementing Quota Support for Storage Types in Java\nDESCRIPTION: Addition of quota support for different storage types in HDFS, allowing for more granular resource management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7584. Enable Quota Support for Storage Types (See breakdown of\\ntasks below)\n```\n\n----------------------------------------\n\nTITLE: User Authentication and Security\nDESCRIPTION: Implementation of Kerberos authentication, SSL encryption for web services, and token-based security measures\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nUserGroupInformation.getCurrentUser()\n```\n\n----------------------------------------\n\nTITLE: C/C++ Enum Naming Fix\nDESCRIPTION: Change to rename ChecksumTypeProto enum NULL since it is illegal in C/C++\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nChecksumTypeProto\n```\n\n----------------------------------------\n\nTITLE: Adding HBase User Parameter in Hadoop Config Script\nDESCRIPTION: Updated the Hadoop setup configuration script to include a parameter for specifying the HBase user.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\n# Added HBase user parameter to setup config script\n```\n\n----------------------------------------\n\nTITLE: Updating MapReduce to Use yarn-client Module in Java\nDESCRIPTION: Modifies MapReduce to utilize the yarn-client module, aligning it with YARN's client-side functionality.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-4580. Change MapReduce to use the yarn-client module.\n```\n\n----------------------------------------\n\nTITLE: Reference to Hadoop file path locations for native libraries\nDESCRIPTION: This snippet documents the location of C++ libraries for pipes, utils, and libhdfs in the Hadoop file system. Libraries are organized by operating system, architecture, and JVM data model.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nc++/<os_osarch_jvmdatamodel>/lib\n```\n\n----------------------------------------\n\nTITLE: Updating Eclipse Build Path for Java\nDESCRIPTION: Modifies the Eclipse project configuration to include the hsqldb.jar in the build path. This ensures proper compilation and execution in the Eclipse IDE.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4249. Fix eclipse path to include the hsqldb.jar. (szetszwo via\nomalley)\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Pull with Automatic Rebase\nDESCRIPTION: Sets up Git to automatically use rebase instead of merge when pulling changes. This helps maintain a cleaner commit history by avoiding unnecessary merge commits.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: gitconfig\nCODE:\n```\ngit config --global pull.rebase true\n```\n\n----------------------------------------\n\nTITLE: Client Protocol Version Update\nDESCRIPTION: Version change in ClientProtocol from 17 to 18 to support DatanodeReport functionality for reporting live, dead or all datanodes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\nClientProtocol.getContentLength\n```\n\n----------------------------------------\n\nTITLE: Implementing MapFile getClosest Method in Java\nDESCRIPTION: Adds a method to MapFile that returns the key that comes just before if the key is not present. This enhances the functionality of MapFile for key lookup operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\nMapFile.getClosest()\n```\n\n----------------------------------------\n\nTITLE: Removing Duplicate stax-api JARs\nDESCRIPTION: Removes duplicate versions of stax-api JARs from the Hadoop tarball.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nHadoop tarball has 2 versions of stax-api JARs.\n```\n\n----------------------------------------\n\nTITLE: Updating ExcessBlocks Metric in BlockManager for HDFS\nDESCRIPTION: Ensures BlockManager removes a block from excessReplicateMap and decrements ExcessBlocks metric when the block is removed.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hdfs/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-6945. BlockManager should remove a block from excessReplicateMap and decrement ExcessBlocks metric when the block is removed.\n```\n\n----------------------------------------\n\nTITLE: Optimizing Configuration Variable Expansion in Java\nDESCRIPTION: Improves the performance of regex-based configuration variable expansion, particularly for long values. This optimization reduces processing time for large configuration strings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11506. Configuration variable expansion regex expensive for long values.\n```\n\n----------------------------------------\n\nTITLE: Optimizing Excess Replica Selection in BlockPlacementPolicyDefault for HDFS\nDESCRIPTION: Improves the algorithm for selecting excess replicas in BlockPlacementPolicyDefault.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-9314. Improve BlockPlacementPolicyDefault's picking of excess replicas.\n```\n\n----------------------------------------\n\nTITLE: Improving Bash Tab Completion for Hadoop Commands\nDESCRIPTION: Updates the Bash tab completion script to look in PATH for Hadoop executables, allowing for more flexible command completion.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n# Look for hadoop executable in PATH\nfor cmd in $(compgen -c hadoop); do\n  if [ -x \"$cmd\" ]; then\n    # Tab completion logic here\n  fi\ndone\n```\n\n----------------------------------------\n\nTITLE: Fixing Hadoop Authentication Cookie Format\nDESCRIPTION: This change modifies the hadoop.auth cookie format to match the output from Jetty, improving compatibility between Hadoop's authentication system and the Jetty web server.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11068. Match hadoop.auth cookie format to jetty output.\n    (Gregory Chanan via cnauroth)\n```\n\n----------------------------------------\n\nTITLE: Setting Java IPv4 Preference in Hadoop Configuration\nDESCRIPTION: This snippet shows how to configure Hadoop to prefer IPv4 over IPv6 using the java.net.preferIPv4Stack system property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nSystem.setProperty(\"java.net.preferIPv4Stack\", \"true\");\n```\n\n----------------------------------------\n\nTITLE: Configuring serialFilter in KeyProvider (Java)\nDESCRIPTION: Configures serialFilter in KeyProvider to avoid UnrecoverableKeyException caused by JDK-8189997. This improves security by restricting deserialization.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Code not provided in change log\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated DistributedFileSystem Methods in Java\nDESCRIPTION: Removes deprecated constructor and methods reportChecksumFailure, getDelegationToken(Text), renewDelegationToken and cancelDelegationToken from DistributedFileSystem class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Removed code:\n// Deprecated constructor\n// public DistributedFileSystem() { ... }\n\n// Deprecated methods\n// public void reportChecksumFailure(String f, FSDataInputStream in, long inPos, FSDataInputStream sums, long sumsPos) { ... }\n// public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException { ... }\n// public long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException { ... }\n// public void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException { ... }\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Logging Levels in Java\nDESCRIPTION: Adds new configuration options to set logging levels separately for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapred.map.child.log.level \n  add mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Updating InputStream Handling in VersionInfo Constructor\nDESCRIPTION: Fixes a resource leak by ensuring the InputStream is properly closed in the VersionInfo constructor.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-10368. InputStream is not closed in VersionInfo ctor.\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Speculative Execution Control\nDESCRIPTION: Updates to hadoop-default.xml that provide finer-grained control over speculative execution by allowing separate settings for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_32\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Deprecating Configuration Methods in Hadoop\nDESCRIPTION: Deprecates getObject() and set(String,Object) methods in Configuration class to ensure only strings are stored in Configurations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_32\n\nLANGUAGE: Java\nCODE:\n```\n// Deprecated:\nConfiguration.getObject()\nConfiguration.set(String, Object)\n\n// New method:\nConfiguration.getRaw()\n```\n\n----------------------------------------\n\nTITLE: Handling NullPointerException in BlockManager for HDFS\nDESCRIPTION: Addresses a potential NullPointerException in BlockManager when no excess replica can be chosen.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-9313. Possible NullPointerException in BlockManager if no excess replica can be chosen.\n```\n\n----------------------------------------\n\nTITLE: Configuring End-of-Record Delimiter for TextInputFormat in Java\nDESCRIPTION: This code allows setting the end-of-record delimiter for TextInputFormat in Hadoop, providing more flexibility in handling input data formats.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nTextInputFormat.setEndOfRecordDelimiter()\n```\n\n----------------------------------------\n\nTITLE: Unit Testing File Path in HDFS\nDESCRIPTION: A file path mentioned in test method TestBalancerWithEncryptedTransfer showing the location of unit tests in the Hadoop HDFS project structure. This indicates the directory structure used for unit tests in the project.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhadoop-hdfs-project/hadoop-hdfs/src/test/unit/\n```\n\n----------------------------------------\n\nTITLE: Hadoop Command for Retrieving Files\nDESCRIPTION: Hadoop filesystem shell command for retrieving files from HDFS to the local filesystem, mentioned in HADOOP-1292 which was updating its behavior.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nbin/hadoop fs -get\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Initialization Threads in Hadoop MapReduce\nDESCRIPTION: Code that introduces multiple job initialization threads in MapReduce, with the thread count configurable via the mapred.jobinit.threads parameter to improve job startup performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nIntroduces multiple job initialization threads, where the number of threads are configurable via mapred.jobinit.threads.\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in YarnConfiguration Class Fields\nDESCRIPTION: Lists fields in YarnConfiguration that are missing either documentation blocks or @since version tags. These are configuration constants used for YARN settings related to resource management, container execution, federation, and other YARN features.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.conf.YarnConfiguration\n```\n\n----------------------------------------\n\nTITLE: Adding isCorrupt Field to BlockLocation in Java\nDESCRIPTION: Adds a boolean field isCorrupt to the BlockLocation class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: java\nCODE:\n```\nAdd boolean field isCorrupt to BlockLocation.\n```\n\n----------------------------------------\n\nTITLE: Updating NetworkTopology for efficient node management (Java)\nDESCRIPTION: Improves the efficiency of adding, getting, and removing nodes in NetworkTopology. This optimization enhances cluster topology management performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// Code not provided in change log\n```\n\n----------------------------------------\n\nTITLE: Fixing output stream closure with IOUtils (Java)\nDESCRIPTION: Bug fix to address an issue where output streams closed with IOUtils were suppressing write errors.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-12881. Output streams closed with IOUtils suppressing write errors.\n```\n\n----------------------------------------\n\nTITLE: Disabling SSLv3 in KMS for Hadoop\nDESCRIPTION: This code snippet disables SSLv3 in the Key Management Service (KMS) for Hadoop to address security vulnerabilities.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11217. Disable SSLv3 in KMS. (Robert Kanter via kasha)\n```\n\n----------------------------------------\n\nTITLE: Modifying saveVersion.sh to handle non-English locales\nDESCRIPTION: Script change to unset LANG and LC_CTYPE environment variables in saveVersion.sh to ensure compatibility with non-English locales. This fix prevents character encoding issues when saving version information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nunset LANG\n```\n\nLANGUAGE: bash\nCODE:\n```\nunset LC_CTYPE\n```\n\n----------------------------------------\n\nTITLE: Updating POM XML Configuration for YARN-888\nDESCRIPTION: XML configuration updates to applicationhistoryservice sub-project according to YARN-888 changes\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\nYARN-1594. Updated pom.xml of applicationhistoryservice sub-project according to YARN-888.\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation for YARN AppAttemptInfo Methods\nDESCRIPTION: This snippet shows AppAttemptInfo methods that are missing documentation blocks. These methods are related to timing information for application attempts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.3/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// AppAttemptInfo methods\ngetFinishedTime()\ngetStartedTime()\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Container Executor in YARN\nDESCRIPTION: Implementation of Docker analog of LinuxContainerExecutor in YARN (YARN-1964)\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Installing Hadoop Bash Tab Completion\nDESCRIPTION: Reference to the bash-tab-completion contrib module which enables tab completion for the bin/hadoop script. The module requires installation according to instructions in the README file in the contrib directory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nbin/hadoop\n```\n\n----------------------------------------\n\nTITLE: Updating FsShell test command (Java)\nDESCRIPTION: Modifies the FsShell -test command to be consistent with Unix semantics. The command now returns zero for true and non-zero for false.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3792. Make FsShell -test consistent with unix semantics, returning\\nzero for true and non-zero for false. (Ben Slusky via cdouglas)\n```\n\n----------------------------------------\n\nTITLE: Using FileSystem.deleteOnExit() for temporary HDFS files in Java\nDESCRIPTION: A new API method FileSystem.deleteOnExit() is introduced to handle temporary files in HDFS more easily.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem fs = FileSystem.get(conf);\nPath tempFile = new Path(\"/tmp/myTempFile\");\nfs.create(tempFile);\nfs.deleteOnExit(tempFile);\n```\n\n----------------------------------------\n\nTITLE: Implementing Halton Sequence for PiEstimator in Java\nDESCRIPTION: This change replaces the use of java.util.Random with a Halton sequence in the PiEstimator class. It likely improves the distribution of random numbers used in the estimation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4437. Use Halton sequence instead of java.util.Random in PiEstimator.\n```\n\n----------------------------------------\n\nTITLE: Executing Shell Command to Get User Groups in Hadoop\nDESCRIPTION: This snippet shows a method name change for getting user groups in Hadoop's util.Shell class. It improves naming consistency for the command that retrieves groups for a user.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\ngetGROUPS_FOR_USER_COMMAND\n```\n\n----------------------------------------\n\nTITLE: Fixing Ivy Configuration for Avro Dependency in MapReduce (XML)\nDESCRIPTION: Modifies the Ivy configuration to look for the Avro JAR file from the Maven repository. This change improves dependency management for the MapReduce project.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: XML\nCODE:\n```\n<ivy-module version=\"2.0\">\n  <dependencies>\n    <dependency org=\"org.apache.avro\" name=\"avro\" rev=\"${avro.version}\" conf=\"*->default\"/>\n  </dependencies>\n</ivy-module>\n```\n\n----------------------------------------\n\nTITLE: Implementing Halton Sequence for PiEstimator in Java\nDESCRIPTION: This change replaces the use of java.util.Random with a Halton sequence in the PiEstimator class. It likely improves the distribution of random numbers used in the estimation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4437. Use Halton sequence instead of java.util.Random in PiEstimator.\n```\n\n----------------------------------------\n\nTITLE: Executing Hadoop Job List Command\nDESCRIPTION: Adds a new sub-command to list Hadoop jobs using the command line interface.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\nbin/hadoop job -list\n```\n\n----------------------------------------\n\nTITLE: Handling Null Pointer Exception in FilterFileSystem in Java\nDESCRIPTION: This code snippet refers to handling a Null Pointer Exception (NPE) in the FilterFileSystem class of Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nFilterFileSystem\n```\n\n----------------------------------------\n\nTITLE: Updating Configuration in hadoop-default.xml for SequenceFile Compression\nDESCRIPTION: Deprecates the 'io.seqfile.compression.type' configuration property in hadoop-default.xml as part of changes to SequenceFile compression API.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: XML\nCODE:\n```\ndeprecated io.seqfile.compression.type\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration Parameters for Map-Reduce Task Logging Levels\nDESCRIPTION: This snippet adds new configuration parameters to allow setting the logging level for map and reduce tasks separately.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Properties\nCODE:\n```\nmapred.map.child.log.level\nmapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: System Changes in YARN Container Executor\nDESCRIPTION: Added support for system user whitelisting in YARN container-executor.c for improved security and access control.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: C\nCODE:\n```\nYARN-1137. Add support whitelist for system users to Yarn container-executor.c\n```\n\n----------------------------------------\n\nTITLE: Enhancing Shell Commands in Hadoop FileSystem\nDESCRIPTION: Adds new options to 'ls' command for improved file listing functionality.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP-7485. Add -h option to ls to list file sizes in human readable format.\n```\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP-7378. Add -d option to ls to not expand directories.\n```\n\n----------------------------------------\n\nTITLE: Initializing ResourceRequest in Hadoop YARN (Java)\nDESCRIPTION: Static factory method for creating a new ResourceRequest instance with specified parameters. This method is part of the YARN API for resource allocation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.1/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nResourceRequest.newInstance(Priority priority, String resourceName, Resource capability, int numContainers, boolean relaxLocality, String nodeLabelsExpression, ExecutionTypeRequest executionTypeRequest)\n```\n\n----------------------------------------\n\nTITLE: Implementing New MapReduce API\nDESCRIPTION: Major API change introducing new MapReduce interfaces in org.apache.hadoop.mapreduce package. Includes Context objects, simplified Mapper/Reducer implementations, and updated file naming conventions.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.mapreduce\n```\n\n----------------------------------------\n\nTITLE: Using Native Hadoop Packaging Script in Bash\nDESCRIPTION: Reference to a script that packages all native library files for Hadoop. This script was modified as part of HADOOP-6173 to ensure it includes all necessary native library files.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npackageNativeHadoop.sh\n```\n\n----------------------------------------\n\nTITLE: FileSystem Method Deprecations - Java\nDESCRIPTION: Documentation of removed deprecated FileSystem methods in Hadoop including getBlockSize(), getLength(), getReplication() and delete().\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\ngetBlockSize(Path f)\ngetLength(Path f)\ngetReplication(Path src)\ndelete(Path f)\n```\n\n----------------------------------------\n\nTITLE: Constructor Missing Documentation\nDESCRIPTION: Multiple constructors in ContainerTokenIdentifier class lacking documentation blocks. These constructors handle container initialization with varying parameters including ContainerId, Resource allocation, and execution parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-yarn/hadoop-yarn-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.security.ContainerTokenIdentifier Constructor (org.apache.hadoop.yarn.api.records.ContainerId, int, java.lang.String, java.lang.String, org.apache.hadoop.yarn.api.records.Resource, long, int, long, org.apache.hadoop.yarn.api.records.Priority, long, org.apache.hadoop.yarn.api.records.LogAggregationContext, java.lang.String, org.apache.hadoop.yarn.server.api.ContainerType, org.apache.hadoop.yarn.api.records.ExecutionType)\n```\n\n----------------------------------------\n\nTITLE: Updating SSLFactory to disable SSLv3 in Java\nDESCRIPTION: Disables the SSLv3 protocol in the SSLFactory class to address security vulnerabilities. This change improves the security of SSL/TLS connections in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nSSLFactory shouldn't allow SSLv3.\n```\n\n----------------------------------------\n\nTITLE: Adding Task's Current Working Directory to LD_LIBRARY_PATH (Shell)\nDESCRIPTION: Enhances the task execution environment by adding the task's current working directory to its LD_LIBRARY_PATH, allowing for easier access to native libraries.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD\n```\n\n----------------------------------------\n\nTITLE: Fixing Syntax Error in CMake JNIFlags File\nDESCRIPTION: This change fixes a syntax error in the JNIFlags.cmake file that was causing issues with CMake version 2.6 patch 2.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nSyntax error on cmake version 2.6 patch 2 in JNIFlags.cmake.\n```\n\n----------------------------------------\n\nTITLE: Defining YarnConfiguration Constants in Java\nDESCRIPTION: This snippet shows a collection of constant field definitions for the YarnConfiguration class. These constants represent various configuration options for YARN components such as the Router, Timeline Service, and Scheduler.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n    public static final String ROUTER_RMADMIN_ADDRESS;\n    public static final String ROUTER_RMADMIN_INTERCEPTOR_CLASS_PIPELINE;\n    public static final String ROUTER_RMADMIN_PREFIX;\n    public static final String ROUTER_USER_CLIENT_THREADS_SIZE;\n    public static final String ROUTER_WEBAPP_ADDRESS;\n    public static final String ROUTER_WEBAPP_DEFAULT_INTERCEPTOR_CLASS;\n    public static final String ROUTER_WEBAPP_HTTPS_ADDRESS;\n    public static final String ROUTER_WEBAPP_INTERCEPTOR_CLASS_PIPELINE;\n    public static final String ROUTER_WEBAPP_PARTIAL_RESULTS_ENABLED;\n    public static final String ROUTER_WEBAPP_PREFIX;\n    public static final String SCHEDULER_CONFIGURATION_FS_MAX_VERSION;\n    public static final String SCHEDULER_CONFIGURATION_FS_PATH;\n    public static final String SCHEDULER_CONFIGURATION_STORE_CLASS;\n    public static final String SCHEDULER_RM_PLACEMENT_CONSTRAINTS_HANDLER;\n    public static final String SCRIPT_NODE_DESCRIPTOR_PROVIDER;\n    public static final String SYSTEM_METRICS_PUBLISHER_ENABLED;\n    public static final String TIMELINE_CSRF_CUSTOM_HEADER;\n    public static final String TIMELINE_CSRF_ENABLED;\n    public static final String TIMELINE_CSRF_METHODS_TO_IGNORE;\n    public static final String TIMELINE_CSRF_PREFIX;\n    // ... (many more constant definitions)\n}\n```\n\n----------------------------------------\n\nTITLE: Using Protocol Buffers for RPC Headers in Java\nDESCRIPTION: These changes implement the use of Protocol Buffers for RPC payload and response headers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8285 Use ProtoBuf for RpcPayLoadHeader (sanjay radia)\n\nHADOOP-8366 Use ProtoBuf for RpcResponseHeader (sanjay radia)\n```\n\n----------------------------------------\n\nTITLE: Defining YarnConfiguration Constants in Java\nDESCRIPTION: This snippet shows the declaration of various constant fields in the YarnConfiguration class. These constants are likely used for configuration and security settings in YARN.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n  public static final String YARN_SECURITY_SERVICE_AUTHORIZATION_COLLECTOR_NODEMANAGER_PROTOCOL;\n  public static final String YARN_SECURITY_SERVICE_AUTHORIZATION_DISTRIBUTEDSCHEDULING_PROTOCOL;\n  public static final String YARN_WEBAPP_UI2_ENABLE;\n  public static final String YARN_WEBAPP_UI2_WARFILE_PATH;\n  public static final String YARN_WORKFLOW_ID_TAG_PREFIX;\n  public static final String YARN_XFS_ENABLED;\n  public static final String ZK_APPID_NODE_SPLIT_INDEX;\n  public static final String ZK_CONFIGURATION_STORE;\n  public static final String ZK_DELEGATION_TOKEN_NODE_SPLIT_INDEX;\n}\n```\n\n----------------------------------------\n\nTITLE: Removing mkdir Command from Hadoop Config Script\nDESCRIPTION: Removes an unnecessary mkdir command from the hadoop-config.cmd script used for Hadoop configuration on Windows systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Batch\nCODE:\n```\nHADOOP-9313. Remove spurious mkdir from hadoop-config.cmd.\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Reuse for MapReduce Tasks in Hadoop\nDESCRIPTION: Adds a new configuration parameter to control JVM reuse across MapReduce tasks, potentially improving performance by reducing JVM startup overhead.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: XML\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Setting Hadoop Authorization System Properties\nDESCRIPTION: Configuration properties for enabling service-level authorization in Hadoop. This includes setting the authorization method and enabling service access control.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.5/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nhadoop.security.authorization=true\nhadoop.security.service.authorization.manager=org.apache.hadoop.security.authorize.ServiceAuthorizationManager\n```\n\n----------------------------------------\n\nTITLE: Bash Script Path Fix for slaves.sh\nDESCRIPTION: Fixed the slaves.sh script to explicitly use /bin/bash instead of /bin/sh as the shell interpreter to ensure proper functionality.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\n/bin/bash\n```\n\n----------------------------------------\n\nTITLE: AES-CTR CryptoCodec Implementation with OpenSSL JNI\nDESCRIPTION: Implementation of AES-CTR CryptoCodec using JNI bindings to OpenSSL (HADOOP-10693). The implementation provides native crypto operations through OpenSSL.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Configuring Double Values in Hadoop Configuration (Java)\nDESCRIPTION: Adds support for getting and setting double values in Hadoop's Configuration class. This allows for more precise numeric configuration options.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\ngetDouble()\nsetDouble()\n```\n\n----------------------------------------\n\nTITLE: Fixing Gzip Codec Decompressor in Hadoop\nDESCRIPTION: This bug fix ensures that the Gzip codec does not return null decompressors. It's part of the bug fixes for release 0.20.203.0.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7258. The Gzip codec should not return null decompressors. (omalley)\n```\n\n----------------------------------------\n\nTITLE: YARN Logs Command Implementation\nDESCRIPTION: Fix for the yarn logs command line interface and improved error messages for mapred job -logs command.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nyarn logs\n```\n\n----------------------------------------\n\nTITLE: Initializing Job Threads in Java\nDESCRIPTION: Introduces multiple job initialization threads to improve job startup performance. The number of threads is configurable through a new parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nmapred.jobinit.threads\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Block for YarnClient Method\nDESCRIPTION: The updateApplicationTimeouts method in YarnClient class is missing a documentation block. This method likely updates timeouts for a YARN application.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.YarnClient Method updateApplicationTimeouts(org.apache.hadoop.yarn.api.protocolrecords.UpdateApplicationTimeoutsRequest)\n```\n\n----------------------------------------\n\nTITLE: Protocol Buffer File Path\nDESCRIPTION: File path showing location of .proto files that needed namespace declarations added for non-Java languages.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhdfs .proto\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag in AMRMClientAsync waitFor Method Overload\nDESCRIPTION: Java method signature for waitFor with three parameters requiring documentation tag. The method accepts a Supplier and two integer parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.1.3/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Fixing Syntax Error in test-patch.sh Bash Script\nDESCRIPTION: Corrects a syntax error in the test-patch.sh script used for Hadoop development and testing.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n# Fixed syntax error in test-patch.sh\n# (Actual fix not provided in the release notes)\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags for AMRMClient and AMRMClientAsync Methods\nDESCRIPTION: Several methods in AMRMClient and AMRMClientAsync classes are missing @since tags. These methods include updateTrackingUrl and various overloads of waitFor, which likely handle application master operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method updateTrackingUrl(java.lang.String)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method updateTrackingUrl(java.lang.String)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method waitFor(java.util.function.Supplier)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method waitFor(java.util.function.Supplier, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method waitFor(java.util.function.Supplier, int, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Tracking Application Attempt Timing in YARN (Java)\nDESCRIPTION: These methods and fields in the AppAttemptInfo class manage the start and finish times of a YARN application attempt, crucial for monitoring and debugging.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppAttemptInfo {\n    public long getFinishedTime() { /* ... */ }\n    public long getStartedTime() { /* ... */ }\n    private long finishedTime;\n    private long startedTime;\n}\n```\n\n----------------------------------------\n\nTITLE: Maven Artifact Version Change\nDESCRIPTION: Change to hadoop core jar file naming convention to include version number for better identification. Renamed to hadoop-${version}-core.jar format.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: text\nCODE:\n```\nhadoop-${version}-core.jar\n```\n\n----------------------------------------\n\nTITLE: Updating GetApplicationsRequestPBImpl in YARN\nDESCRIPTION: Fixed a bug in GetApplicationsRequestPBImpl to add missing fields to the protocol buffer implementation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/yarn/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nYARN-2016. Fix a bug in GetApplicationsRequestPBImpl to add the missed fields\nto proto.\n```\n\n----------------------------------------\n\nTITLE: Updating MapReduce Classpath Handling in Java\nDESCRIPTION: Changes MapReduce to supply MapReduce jars in the classpath rather than relying on YARN for classpath management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-4638. MR AM supplies MapReduce jars in classpath rather than rely on YARN.\n```\n\n----------------------------------------\n\nTITLE: Timeline Utils Constants\nDESCRIPTION: Undocumented constants in TimelineUtils class used for flow management and versioning in YARN's timeline service.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-yarn/hadoop-yarn-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.util.timeline.TimelineUtils Field DEFAULT_FLOW_VERSION\norg.apache.hadoop.yarn.util.timeline.TimelineUtils Field FLOW_NAME_TAG_PREFIX\norg.apache.hadoop.yarn.util.timeline.TimelineUtils Field FLOW_RUN_ID_TAG_PREFIX\norg.apache.hadoop.yarn.util.timeline.TimelineUtils Field FLOW_VERSION_TAG_PREFIX\n```\n\n----------------------------------------\n\nTITLE: Undocumented AppInfo Fields\nDESCRIPTION: Undocumented field members in AppInfo class related to application properties and status.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nField priority\nField runningContainers\nField unmanagedApplication\n```\n\n----------------------------------------\n\nTITLE: Retrieving Node ID in ContainerInfo Class\nDESCRIPTION: This method retrieves the Node ID associated with a container. It is part of the ContainerInfo class in the YARN server webapp DAO.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic String getNodeId()\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Library Path in Hadoop Shell Script\nDESCRIPTION: This snippet adds the JAVA_LIBRARY_PATH to the LD_LIBRARY_PATH in the hadoop-config.sh script. This allows native libraries to be found at runtime.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nhadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Configuring DFS Block Size in Hadoop\nDESCRIPTION: Sets the DFS block size as a multiple of the checksum bytes. This change requires an upgrade from previous versions due to layout changes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\ndfs.block.size = n * io.byte.per.checksum\n```\n\n----------------------------------------\n\nTITLE: Fixing TypedBytesInput readRaw() method in Hadoop MapReduce\nDESCRIPTION: Modifies the readRaw() method in TypedBytesInput to preserve custom type codes, ensuring correct handling of typed bytes input in MapReduce jobs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\npublic class TypedBytesInput {\n    // ...\n    public byte[] readRaw() throws IOException {\n        // Modified implementation to preserve custom type codes\n        // ...\n    }\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Job ID in Hadoop Java API\nDESCRIPTION: Introduces new JobID, TaskID and TaskAttemptID classes to replace string identifiers. Applications can use xxxID.toString() and xxxID.forName() methods to convert between objects and strings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nJobID jobId = JobID.forName(jobIdString);\nString jobIdString = jobId.toString();\n```\n\n----------------------------------------\n\nTITLE: HDFS XML Configuration Property Reference\nDESCRIPTION: XML configuration property references mentioned in various JIRA tickets including dfs.data.dir, io.bytes.per.checksum, and fs.checkpoint changes\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\ndfs.data.dir permissions=\"700\"\nio.bytes.per.checksum\nfs.checkpoint.*\ndfs.namenode.checkpoint.*\n```\n\n----------------------------------------\n\nTITLE: Adding MapReduce TaskTracker Out-of-Band Heartbeat Configuration in Java\nDESCRIPTION: Adds a new configuration property 'mapreduce.tasktracker.outofband.heartbeat' to optionally enable out-of-band heartbeats from TaskTrackers to JobTracker on task completion for better job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nadd mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Terminating Hadoop Task Attempts\nDESCRIPTION: Introduces new sub-commands to fail or kill specific task attempts in a Hadoop job.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: Shell\nCODE:\n```\nbin/hadoop job -fail-task <task-id>\nbin/hadoop job -kill-task <task-id>\n```\n\n----------------------------------------\n\nTITLE: Extending DistCp to accept custom CopyListing\nDESCRIPTION: Enhancement to the DistCp utility allowing it to accept a custom CopyListing implementation, providing more flexibility for copy operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5014. Extend Distcp to accept a custom CopyListing. \n(Srikanth Sundarrajan via amareshwari)\n```\n\n----------------------------------------\n\nTITLE: Configuring Out-of-Band Heartbeat for TaskTracker in MapReduce\nDESCRIPTION: Adds a new configuration property to optionally enable out-of-band heartbeat on task completion for better job latency in TaskTracker.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nmapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Managing Container Collections in YARN (Java)\nDESCRIPTION: This snippet shows a method for adding all elements from a collection to another collection, likely used for managing sets of containers in the YARN server web application.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic class ContainersInfo {\n    public boolean addAll(java.util.Collection c) { /* ... */ }\n}\n```\n\n----------------------------------------\n\nTITLE: Hadoop YARN Configuration Constants\nDESCRIPTION: Configuration field declarations missing documentation blocks in YarnConfiguration class. These fields define important configuration parameters for YARN functionality.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nAM_SCHEDULING_NODE_BLACKLISTING_DISABLE_THRESHOLD\nAM_SCHEDULING_NODE_BLACKLISTING_ENABLED\nAMRM_PROXY_ADDRESS\nAMRM_PROXY_CLIENT_THREAD_COUNT\nAMRM_PROXY_ENABLED\nAMRM_PROXY_HA_ENABLED\nAMRM_PROXY_INTERCEPTOR_CLASS_PIPELINE\nAPP_ATTEMPT_DIAGNOSTICS_LIMIT_KC\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in AppInfo Class Methods and Fields\nDESCRIPTION: Lists undocumented methods and fields in the AppInfo class including resource allocation, node label expressions, priority, and container management methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.2/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nclass AppInfo {\n    private int priority;\n    private int runningContainers;\n    private boolean unmanagedApplication;\n    \n    public int getAllocatedCpuVcores()\n    public int getAllocatedMemoryMB()\n    public String getAmNodeLabelExpression()\n    public String getAppNodeLabelExpression()\n    public int getPriority()\n    public int getReservedCpuVcores()\n    public int getReservedMemoryMB()\n    public int getRunningContainers()\n    public boolean isUnmanagedApp()\n}\n```\n\n----------------------------------------\n\nTITLE: Optimizing FileSystem StatisticsData instances (Java)\nDESCRIPTION: Addresses HADOOP-12107 where long running apps may have a huge number of StatisticsData instances under FileSystem. This optimization improves memory usage for long-running applications.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-12107. long running apps may have a huge number of StatisticsData instances under FileSystem\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Containers in Java\nDESCRIPTION: Sets the MALLOC_ARENA_MAX environment variable for all daemons and containers to optimize memory allocation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nMALLOC_ARENA_MAX\n```\n\n----------------------------------------\n\nTITLE: Implementing File Appending for SequenceFiles in Java\nDESCRIPTION: Adds support for appending to existing SequenceFiles, allowing data to be added to the end of an existing file rather than overwriting it.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7139. Allow appending to existing SequenceFiles\n```\n\n----------------------------------------\n\nTITLE: FileSystem API for UGI Closure\nDESCRIPTION: New FileSystem API closeAllForUGI for closing all file systems associated with a particular UserGroupInformation (UGI)\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\ncloseAllForUGI(UserGroupInformation ugi)\n```\n\n----------------------------------------\n\nTITLE: HDFS Java Build Configuration\nDESCRIPTION: Build.xml modification to fix references to hadoop-common version and configuration settings\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\nhadoop-common-${version}\n```\n\n----------------------------------------\n\nTITLE: Default Hadoop Temporary Directory Configuration (Shell)\nDESCRIPTION: Shows the default configuration for Hadoop's temporary directory, which is set to /tmp/hadoop-${user.name}. This uses variable expansion to include the current user's name in the path.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\n/tmp/hadoop-${user.name}\n```\n\n----------------------------------------\n\nTITLE: Fixing NMToken Handling in ApplicationMasterService\nDESCRIPTION: Fixed a bug in the ApplicationMasterService to prevent adding null NMTokens to the NMTokens list from previous attempts during work-preserving ApplicationMaster restart.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/yarn/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nYARN-2053. Fixed a bug in AMS to not add null NMToken into NMTokens list from\nprevious attempts for work-preserving AM restart.\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in AppInfo DAO Class\nDESCRIPTION: Methods and fields requiring documentation in AppInfo class for managing YARN application information including resource allocation, node labels, containers and application properties\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.4/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppInfo {\n    private int priority;\n    private int runningContainers;\n    private boolean unmanagedApplication;\n\n    public int getAllocatedCpuVcores()\n    public int getAllocatedMemoryMB()\n    public String getAmNodeLabelExpression()\n    public String getAppNodeLabelExpression()\n    public int getPriority()\n    public int getRunningContainers()\n    public boolean isUnmanagedApp()\n}\n```\n\n----------------------------------------\n\nTITLE: Building Hadoop Site with Maven Command\nDESCRIPTION: Command to build the Hadoop website using Maven. The site is built into the target/staging directory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean site\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in Shared Cache Methods\nDESCRIPTION: Methods related to shared cache functionality in org.apache.hadoop.mapreduce.Job class missing version documentation tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\naddArchiveToSharedCache(URI, Configuration)\naddFileToSharedCache(URI, Configuration)\naddFileToSharedCacheAndClasspath(URI, Configuration)\ngetArchiveSharedCacheUploadPolicies(Configuration)\ngetFileSharedCacheUploadPolicies(Configuration)\nsetArchiveSharedCacheUploadPolicies(Configuration, Map)\nsetFileSharedCacheUploadPolicies(Configuration, Map)\n```\n\n----------------------------------------\n\nTITLE: Enabling TCP No Delay for TaskTracker Child Processes in Java\nDESCRIPTION: Enables the TCP_NODELAY option for IPC clients in TaskTracker child processes to improve network performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nipc.client.tcpnodelay\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Logging Levels\nDESCRIPTION: Configuration properties for setting separate logging levels for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nmapred.map.child.log.level\nmapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Fixing Typo in HDFS Explorer JavaScript\nDESCRIPTION: Bug fix for a typo in the HDFS web interface explorer JavaScript file\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// HDFS-6472: fix typo in webapps/hdfs/explorer.js\n```\n\n----------------------------------------\n\nTITLE: Fixing Curator Dependencies in Hadoop Project (Java)\nDESCRIPTION: Resolves duplicate and conflicting curator dependencies declared in the hadoop-project.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-12230. hadoop-project declares duplicate, conflicting curator dependencies. (Rakesh R via aajisaka)\n```\n\n----------------------------------------\n\nTITLE: Configuration Fix in YARN QueuePlacementRule Class\nDESCRIPTION: Bug fix for string comparison in QueuePlacementRule's NestedUserQueue class to properly compare strings using equals() instead of ==.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nQueuePlacementRule#NestedUserQueue#getQueueForApp\n```\n\n----------------------------------------\n\nTITLE: Configuring Cache-Control Header for Dynamic Content in HTTP\nDESCRIPTION: Sets the Cache-Control header to 'no-cache' for all dynamic content, ensuring that clients do not cache potentially changing data.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nCache-Control no-cache\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Cache on HDFS NameNode\nDESCRIPTION: Implements a retry cache on the NameNode to improve idempotency of operations. This helps handle duplicate requests in case of retries.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-4979. Implement retry cache on Namenode.\n```\n\n----------------------------------------\n\nTITLE: Adding getDouble() and setDouble() methods to Configuration class in Java\nDESCRIPTION: Adds new getDouble() and setDouble() methods to the org.apache.hadoop.conf.Configuration class to support reading and writing double values.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.conf.Configuration\n```\n\n----------------------------------------\n\nTITLE: Using System Counters in MapReduce Configuration\nDESCRIPTION: This example demonstrates how to access system counter statistics like file read/write bytes in a MapReduce program. It shows retrieving counter values for monitoring I/O operations in a Hadoop job.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// FileSystemCounterGroup\nSystem.out.println(\"HDFS bytes read: \" + \n               counters.findCounter(\"FileSystemCounters\", \"HDFS_BYTES_READ\").getValue());\nSystem.out.println(\"HDFS bytes written: \" + \n               counters.findCounter(\"FileSystemCounters\", \"HDFS_BYTES_WRITTEN\").getValue());\n```\n\n----------------------------------------\n\nTITLE: Randomizing Cluster Creation in MiniMRCluster\nDESCRIPTION: Modifies MiniMRCluster to use a random component when creating an actual cluster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5083. MiniMRCluster should use a random component when creating an actual cluster (Siddharth Seth via hitesh)\n```\n\n----------------------------------------\n\nTITLE: Defining User Counters in MapReduce Java Implementation\nDESCRIPTION: This code demonstrates how to define and use a custom counter in a MapReduce job. It shows how to increment a counter within the map method of a Hadoop MapReduce program using the Context object.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic static enum COUNTRY {\n  INDIA,\n  USA,\n  CHINA\n}\n...\npublic void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n  String country = // ... extract the country field from the input\n  if(country.equals(\"India\") {\n    context.getCounter(COUNTRY.INDIA).increment(1);\n  }\n  // etc\n}\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration Property\nDESCRIPTION: Default MapReduce shuffle port configuration property in mapred-default.xml\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\nmapreduce.shuffle.port\n```\n\n----------------------------------------\n\nTITLE: Configuring Out-of-Band Heartbeat for TaskTracker in Java\nDESCRIPTION: Adds a new configuration property to optionally enable out-of-band heartbeats from TaskTracker to JobTracker on task completion, improving job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nadd mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: YarnConfiguration Missing Documentation Fields\nDESCRIPTION: List of class fields in org.apache.hadoop.yarn.conf.YarnConfiguration that are missing proper documentation. These fields represent various configuration parameters for YARN components including NodeManager, ResourceManager, and container management settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.conf.YarnConfiguration\nField DEFAULT_NM_DOCKER_USER_REMAPPING_GID_THRESHOLD\nField DEFAULT_NM_DOCKER_USER_REMAPPING_UID_THRESHOLD\nField DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED\nField DEFAULT_NM_ELASTIC_MEMORY_CONTROL_OOM_TIMEOUT_SEC\nField DEFAULT_NM_ENABLE_HARDWARE_CAPABILITY_DETECTION\nField DEFAULT_NM_FPGA_VENDOR_PLUGIN\nField DEFAULT_NM_GPU_DOCKER_PLUGIN_IMPL\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration Property - Java\nDESCRIPTION: Configuration property changes showing the default IPC client timeout setting format.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nmapred.line.input.format.linespermap\n```\n\n----------------------------------------\n\nTITLE: Implementing DNSToSwitchMapping Base Class in Java\nDESCRIPTION: This snippet implements a base class for DNSToSwitchMapping implementations that can offer extra topology information. It addresses HADOOP-7777.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7777 Implement a base class for DNSToSwitchMapping implementations \nthat can offer extra topology information. (stevel)\n```\n\n----------------------------------------\n\nTITLE: Implementing Native CRC32C Using SSE4.2 in C++\nDESCRIPTION: Implements CRC32C checksum calculation using SSE4.2 instructions for improved performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nHADOOP-7446. Implement CRC32C native code using SSE4.2 instructions.\n```\n\n----------------------------------------\n\nTITLE: YARN Timeline Web Services Implementation\nDESCRIPTION: Fix for type mismatch in Map containsKey check of TimelineWebServices injectOwnerInfo method.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/yarn/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nTimelineWebServices#injectOwnerInfo\n```\n\n----------------------------------------\n\nTITLE: Missing JavaDoc for KeyProvider JCEKS Fields\nDESCRIPTION: Multiple fields (JCEKS_KEY_SERIAL_FILTER and JCEKS_KEY_SERIALFILTER_DEFAULT) in KeyProvider class lack required documentation blocks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.5/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.crypto.key.KeyProvider.JCEKS_KEY_SERIAL_FILTER\norg.apache.hadoop.crypto.key.KeyProvider.JCEKS_KEY_SERIALFILTER_DEFAULT\n```\n\n----------------------------------------\n\nTITLE: API Addition in FileSystem and FileContext\nDESCRIPTION: New API getFiles added to FileSystem and FileContext that lists all files under an input path with optional recursion. Returns block locations along with file status.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\ngetFiles(Path path, boolean recursive)\n```\n\n----------------------------------------\n\nTITLE: Incomplete Documentation for HADOOP_SECURITY_CRYPTO_JCEKS_KEY_SERIALFILTER\nDESCRIPTION: Field HADOOP_SECURITY_CRYPTO_JCEKS_KEY_SERIALFILTER in CommonConfigurationKeysPublic class is missing required @since tag in JavaDoc.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.5/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_SECURITY_CRYPTO_JCEKS_KEY_SERIALFILTER\n```\n\n----------------------------------------\n\nTITLE: Fixing NullPointerException in MapReduce Deprecated Key Handling (Java)\nDESCRIPTION: Addresses a NullPointerException that occurs when handling deprecated configuration keys in MapReduce. This fix improves the robustness of the configuration system.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\npublic class Configuration {\n  public String get(String name) {\n    String[] names = handleDeprecation(deprecatedKeyMap.get(name));\n    String result = null;\n    for(String n : names) {\n      result = getProperty(n);\n      if(result != null) {\n        break;\n      }\n    }\n    return result;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating MapReduce Configuration in Java\nDESCRIPTION: Code snippet showing how to set the 'yarn.nodemanager.delete.debug-delay-sec' property to '-1' for easier container debugging in MapReduce.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nyarn.nodemanager.delete.debug-delay-sec = \"-1\"\n```\n\n----------------------------------------\n\nTITLE: TaskTracker Configuration Updates\nDESCRIPTION: Configuration changes for TaskTracker to support different numbers of mappers and reducers, replacing the single tasks.maximum parameter with separate maximums for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: xml\nCODE:\n```\n<!-- New configuration properties -->\nmapred.tasktracker.map.tasks.maximum (default value of 2)\nmapred.tasktracker.reduce.tasks.maximum (default value of 2)\n<!-- Removed/deprecated configuration -->\nmapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: OpenSSL Cipher Implementation\nDESCRIPTION: C implementation for AES-CTR CryptoCodec using JNI calls to OpenSSL library. Part of HADOOP-10693.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: c\nCODE:\n```\n// Referenced in HADOOP-10693 but actual code not shown in log\n```\n\n----------------------------------------\n\nTITLE: Updating XML Configuration for Hadoop Streaming in XML\nDESCRIPTION: Fixes the command line argument handling to support multiple -cacheArchive options in Hadoop streaming configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_32\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>stream.cachearchive</name>\n  <value>hdfs://archive1.jar,hdfs://archive2.jar</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Getting Job Status in MapReduce (Java)\nDESCRIPTION: Adds support for retrieving job status through the RunningJob interface, providing more detailed information about job progress.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nJobStatus status = runningJob.getJobStatus();\n```\n\n----------------------------------------\n\nTITLE: Improving Error Handling in libhdfs\nDESCRIPTION: Enhances error messaging in libhdfs when the shared library file 'libhdfs.so' is not found, providing better diagnostics for users.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: C\nCODE:\n```\nlibhdfs: better error message if llibhdfs.so doesn't exist.\n```\n\n----------------------------------------\n\nTITLE: Resolving Memory Leak in ResourceManager (Java)\nDESCRIPTION: Fixes a memory leak in ResourceManager when running in SIMPLE security mode.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3857. Memory leak in ResourceManager with SIMPLE mode. (mujunchao via zxu)\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Server Addresses\nDESCRIPTION: Lists deprecated and new configuration variables for server bind addresses and ports in Hadoop. Changes consolidate address format from separate host and port to 'host:port' format.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: properties\nCODE:\n```\n# Deprecated configuration variables:\ndfs.info.bindAddress\ndfs.info.port\ndfs.datanode.bindAddress\ndfs.datanode.port\ndfs.datanode.info.bindAdress\ndfs.datanode.info.port\ndfs.secondary.info.bindAddress\ndfs.secondary.info.port\nmapred.job.tracker.info.bindAddress\nmapred.job.tracker.info.port\nmapred.task.tracker.report.bindAddress\ntasktracker.http.bindAddress\ntasktracker.http.port\n\n# New configuration variables:\ndfs.secondary.http.address\ndfs.datanode.address\ndfs.datanode.http.address\ndfs.http.address\nmapred.job.tracker.http.address\nmapred.task.tracker.report.address\nmapred.task.tracker.http.address\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Authentication with Kerberos\nDESCRIPTION: XML configuration for enabling Kerberos authentication in Hadoop. This snippet shows how to set the authentication method to 'kerberos' and configure a service principal for a specific service.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.5/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.security.authentication</name>\n  <value>kerberos</value>\n</property>\n\n<property>\n  <name>dfs.datanode.kerberos.principal</name>\n  <value>hdfs/_HOST@YOUR-REALM.COM</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Custom ConfigRedactor in core-site.xml\nDESCRIPTION: XML configuration for defining a custom ConfigRedactor class implementation in Hadoop. This is used to redact sensitive configuration information in logs and UI to enhance security.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.security.credential.clear-text-fallback</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop IPC Timeouts\nDESCRIPTION: Removes ipc.client.timeout property and sets ipc.client.maxidletime to twice ipc.client.connection.maxidletime. Connections with outstanding requests are not considered idle.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\nconf.set(\"ipc.client.connection.maxidletime\", \"60000\");\n// ipc.client.maxidletime is automatically set to 120000\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce JVM Reuse\nDESCRIPTION: Configuration parameter added to hadoop-default.xml to control the number of tasks that can reuse a JVM instance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_30\n\nLANGUAGE: properties\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Deprecating MapReduce Output Compression Type (Java)\nDESCRIPTION: Deprecates the 'mapred.map.output.compression.type' configuration property as part of improvements to the Map-Reduce shuffle/merge process.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\n// Deprecated:\n// conf.set(\"mapred.map.output.compression.type\", compressionType);\n```\n\n----------------------------------------\n\nTITLE: Updating Capacity Scheduler Documentation in Java\nDESCRIPTION: Updates the Capacity Scheduler documentation to include configuration types. This improves the clarity of the configuration options available.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1180. Update capacity scheduler docs to include types on the configs\n```\n\n----------------------------------------\n\nTITLE: Adding releasenotes.html check-in location in Java\nDESCRIPTION: Establishes a location to check in releasenotes.html after HADOOP-4920.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_28\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5645. After HADOOP-4920 we need a place to checkin\nreleasenotes.html. (nigel)\n```\n\n----------------------------------------\n\nTITLE: Container Executor Configuration Path\nDESCRIPTION: Default configuration directory path for container-executor relative to binary\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\ncontainer-executor.conf.dir\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Memory in settings.xml\nDESCRIPTION: XML configuration for Maven settings to allocate sufficient memory for building documentation, which helps prevent memory-related failures.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<profile>\n  <id>maven-memory</id>\n  <activation>\n    <activeByDefault>true</activeByDefault>\n  </activation>\n  <properties>\n    <jvm>-Xmx4g -XX:MaxMetaspaceSize=2g</jvm>\n  </properties>\n</profile>\n```\n\n----------------------------------------\n\nTITLE: Java Method Signatures Missing Documentation\nDESCRIPTION: Collection of Java method signatures from Hadoop YARN API that require proper documentation. These methods are from various classes including ApplicationReport, ContainerStatus, NodeReport, and other YARN API classes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Sample of affected methods:\ngetAmNodeLabelExpression()\ngetApplicationPriority()\ngetApplicationSchedulingPropertiesMap()\ngetApplicationTimeouts()\ngetAppNodeLabelExpression()\ngetAttributesToNodes(GetAttributesToNodesRequest)\ngetAuxiliaryLocalPathHandler()\n```\n\n----------------------------------------\n\nTITLE: European Commission OneLab Copyright Notice for Bloom Filter Classes\nDESCRIPTION: Copyright and license notice for org.apache.hadoop.util.bloom.* classes from the European Commission OneLab project. Specifies redistribution terms and disclaimers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hdfs/LICENSE.txt#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n/**\n *\n * Copyright (c) 2005, European Commission project OneLab under contract\n * 034819 (http://www.one-lab.org)\n * All rights reserved.\n * Redistribution and use in source and binary forms, with or \n * without modification, are permitted provided that the following \n * conditions are met:\n *  - Redistributions of source code must retain the above copyright \n *    notice, this list of conditions and the following disclaimer.\n *  - Redistributions in binary form must reproduce the above copyright \n *    notice, this list of conditions and the following disclaimer in \n *    the documentation and/or other materials provided with the distribution.\n *  - Neither the name of the University Catholique de Louvain - UCL\n *    nor the names of its contributors may be used to endorse or \n *    promote products derived from this software without specific prior \n *    written permission.\n *    \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT \n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS \n * FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE \n * COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, \n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, \n * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER \n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT \n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN \n * ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n * POSSIBILITY OF SUCH DAMAGE.\n */\n```\n\n----------------------------------------\n\nTITLE: Creating HDFS URL in Hadoop Java API\nDESCRIPTION: Adds default port 8020 for HDFS namenode, allowing simplified URIs like \"hdfs://example.com/dir/file\" without specifying the port.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\nURI hdfsUri = new URI(\"hdfs://example.com/dir/file\");\nFileSystem fs = FileSystem.get(hdfsUri, conf);\n```\n\n----------------------------------------\n\nTITLE: Removed Deprecated Methods in RawLocalFileSystem\nDESCRIPTION: List of deprecated methods removed from RawLocalFileSystem as part of the HADOOP-4253 fix for various warnings generated by findbugs.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: java\nCODE:\n```\npublic String getName()\npublic void lock(Path p, boolean shared)\npublic void release(Path p)\n```\n\n----------------------------------------\n\nTITLE: File Permission Documentation\nDESCRIPTION: Documentation update regarding ChecksumFileSystem's handling of crc file permissions\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\nHADOOP-13052. ChecksumFileSystem mishandles crc file permissions.\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation for Another CommonConfigurationKeys Field (Java)\nDESCRIPTION: The IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY field in the org.apache.hadoop.fs.CommonConfigurationKeys class is also missing its documentation block. This documentation is essential for developers to understand the purpose and usage of this configuration key.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r1.2.1/jdiff/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.fs.CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY\n```\n\n----------------------------------------\n\nTITLE: Preventing Negative Active Node Count in Java Resource Manager\nDESCRIPTION: Fixes a bug where the count of active nodes could be decremented below 0. This ensures the integrity of cluster metrics and prevents potential issues arising from invalid node counts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1101. Active nodes can be decremented below 0\n```\n\n----------------------------------------\n\nTITLE: Fixing BytesWritable toString Method in Java\nDESCRIPTION: This snippet fixes the BytesWritable.toString method to avoid extending the sign bit when converting bytes to a string representation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nBytesWritable.toString\n```\n\n----------------------------------------\n\nTITLE: Removing Vendor Names from Codebase\nDESCRIPTION: Cleans up vendor names from the Hadoop codebase.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nCleanup vendor names from the code base.\n```\n\n----------------------------------------\n\nTITLE: Setting Default MapReduce Shuffle Port in XML\nDESCRIPTION: Adding the default mapreduce.shuffle.port property to mapred-default.xml configuration file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>mapreduce.shuffle.port</name>\n  <value>13562</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Disabling SSLv3 in KMS for Hadoop\nDESCRIPTION: This code snippet disables the SSLv3 protocol in the Key Management Server (KMS) for Hadoop, likely as a security measure to prevent vulnerabilities associated with the older SSL protocol version.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11217. Disable SSLv3 in KMS. (Robert Kanter via kasha)\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: YAML frontmatter configuration for the blog post specifying title, date and linked status.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/news/2018-10-01-ozone-0.2.1-alpha.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Ozone release 0.2.1-alpha available\ndate: 2018-10-01\nlinked: true\n---\n```\n\n----------------------------------------\n\nTITLE: Missing DAO Field Documentation\nDESCRIPTION: Missing documentation for various fields across DAO classes including timing, node, priority and container information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nAppAttemptInfo.finishedTime\nContainerInfo.nodeId\nAppInfo.priority\nAppInfo.runningContainers\nAppAttemptInfo.startedTime\nAppInfo.unmanagedApplication\n```\n\n----------------------------------------\n\nTITLE: Hadoop Release 3.3.1 Frontmatter in Markdown\nDESCRIPTION: YAML frontmatter defining metadata for the Hadoop 3.3.1 release announcement page, including title and publication date.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/3.3.1.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 3.3.1 available\ndate: 2021-06-15\n---\n```\n\n----------------------------------------\n\nTITLE: Adding LZ4 Compression Support in Java\nDESCRIPTION: This snippet adds support for LZ4 compression to Hadoop. It addresses HADOOP-7657.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7657. Add support for LZ4 compression. (Binglin Chang via todd)\n```\n\n----------------------------------------\n\nTITLE: Bash Script Path Fix\nDESCRIPTION: Fix to explicitly specify /bin/bash instead of /bin/sh in the slaves.sh script to ensure proper execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\n/bin/bash\n```\n\n----------------------------------------\n\nTITLE: Implementing NetworkTopology Plugin in Java\nDESCRIPTION: Makes the NetworkTopology class pluggable to allow custom implementations of network topology.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nNetworkTopology\n```\n\n----------------------------------------\n\nTITLE: Managing Node Label Expressions for YARN Applications (Java)\nDESCRIPTION: These methods in the AppInfo class handle node label expressions for both the application and its Application Master, which are used for node selection in YARN.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppInfo {\n    public String getAmNodeLabelExpression() { /* ... */ }\n    public String getAppNodeLabelExpression() { /* ... */ }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Data Transfer Version in Java\nDESCRIPTION: This snippet updates the DATA_TRANSFER_VERSION constant, likely in response to changes in the data transfer protocol.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\npublic static final int DATA_TRANSFER_VERSION = X; // Where X is the new version number\n```\n\n----------------------------------------\n\nTITLE: Fixing Intermittent TestRPC Failures\nDESCRIPTION: Resolves intermittent failures of TestRPC on JDK 7.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nTestRPC fails intermittently on jkd7\n```\n\n----------------------------------------\n\nTITLE: Executing YARN Script in Bash\nDESCRIPTION: Modification to the bin/yarn script to not print the command line unnecessarily. This improves the script's output by reducing verbosity.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/yarn\n```\n\n----------------------------------------\n\nTITLE: Formatting the HDFS Namenode\nDESCRIPTION: Shell command to format the NameNode, which initializes the HDFS filesystem. This should only be run once when setting up a new HDFS cluster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nhdfs namenode -format\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in AppInfo Class - Resource Methods\nDESCRIPTION: Methods related to resource allocation and management that lack documentation blocks in AppInfo class\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.6/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppInfo {\n    private boolean unmanagedApplication;\n    private int priority;\n    private int runningContainers;\n\n    public int getAggregatePreemptedResourceAllocation()\n    public int getAggregateResourceAllocation()\n    public int getAllocatedCpuVcores()\n    public int getAllocatedGpus()\n    public int getAllocatedMemoryMB()\n    public String getAmNodeLabelExpression()\n    public String getAppNodeLabelExpression()\n    public long getLaunchTime()\n    public int getPriority()\n    public int getReservedCpuVcores()\n    public int getReservedGpus()\n    public int getReservedMemoryMB()\n    public int getRunningContainers()\n    public boolean isUnmanagedApp()\n}\n```\n\n----------------------------------------\n\nTITLE: Defining HTML Structure for Apache Hadoop Website\nDESCRIPTION: This HTML snippet defines the structure of the Apache Hadoop project website. It includes the DOCTYPE declaration, head section with meta tags and title, and the beginning of the body content with navigation menus.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.2/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!DOCTYPE html>\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n  <title>Apache Hadoop</title>\n</head>\n\n<body>\n<div id=\"container\">\n  <div id=\"header\">\n    <div id=\"headerButtons\">\n      <a href=\"http://www.apache.org/\">Apache Software Foundation</a> |\n      <a href=\"http://www.apache.org/foundation/sponsorship.html\">Sponsorship</a> |\n      <a href=\"http://www.apache.org/foundation/thanks.html\">Thanks</a>\n    </div>\n    <div id=\"headerLogo\">\n      <a href=\"/\">\n        <img alt=\"Apache Hadoop\" src=\"/images/hadoop-logo.jpg\" />\n      </a>\n    </div>\n  </div>\n\n  <div id=\"navigation\">\n    <ul>\n      <li><a href=\"/\">Home</a></li>\n      <li><a href=\"/releases.html\">Releases</a></li>\n      <li><a href=\"/documentation.html\">Documentation</a></li>\n      <li><a href=\"/community.html\">Community</a></li>\n      <li><a href=\"/support.html\">Support</a></li>\n      <li><a href=\"/trademarks.html\">Trademarks</a></li>\n    </ul>\n  </div>\n\n  <div id=\"content\">\n```\n\n----------------------------------------\n\nTITLE: Adding new FileSystem API for specifying checksum type in Java\nDESCRIPTION: Adds a new API to allow users to specify a checksum type when creating files using FileSystem.create()\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-8240. Add a new API to allow users to specify a checksum type on FileSystem.create(..).\n```\n\n----------------------------------------\n\nTITLE: Constants from YarnConfiguration Class\nDESCRIPTION: Collection of undocumented configuration constants for various YARN components including Resource Manager, Timeline Service, Federation settings, and other core YARN configurations. Many fields are missing proper documentation blocks or @since tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n// Example field pattern (these are field declarations in YarnConfiguration.java)\nDEFAULT_RM_PLACEMENT_CONSTRAINTS_RETRY_ATTEMPTS\nDEFAULT_RM_PLACEMENT_CONSTRAINTS_SCHEDULER_POOL_SIZE\nDEFAULT_RM_PROXY_CONNECTION_TIMEOUT\n// ... and many other similar constant field declarations\n```\n\n----------------------------------------\n\nTITLE: Setting Reduce Task Slow Start in Hadoop MapReduce\nDESCRIPTION: Introduces a slow-start for scheduling reduce tasks to ensure that reduces aren't started until a sufficient number of map tasks are completed. This prevents reduce tasks of jobs with unscheduled maps from overwhelming the cluster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>mapred.reduce.slowstart.completed.maps</name>\n  <value>0.05</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Updating AMRMToken Service Name in RMContainerAllocator for Java\nDESCRIPTION: This code fix updates the new AMRMToken service name properly in the RMContainerAllocator class. It addresses an issue related to token management in MapReduce application masters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nRMContainerAllocator.updateAMRMToken()\n```\n\n----------------------------------------\n\nTITLE: HDFS Security Enhancement\nDESCRIPTION: Implementation of block access token functionality to conform to the generic Token interface in Hadoop Common, improving security architecture.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nHDFS-992. Re-factor block access token implementation to conform to the generic Token interface in Common\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Speculative Execution\nDESCRIPTION: Updated configuration properties in hadoop-default.xml to provide finer-grained control over speculative execution for map and reduce tasks independently.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Configuration - HADOOP-4400 HDFS URL Update\nDESCRIPTION: Adds 'hdfs://' prefix to fs.default.name parameter in quickstart documentation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_12\n\n\n\n----------------------------------------\n\nTITLE: Running MapReduce Job List Command in Bash\nDESCRIPTION: Fix for the 'bin/mapred job -list' command to display all jobs instead of only the jobs owned by the current user. This improves visibility into all running jobs in the cluster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/mapred job -list\n```\n\n----------------------------------------\n\nTITLE: Removing Extra Semicolon in FSDirectory Java Class\nDESCRIPTION: Removes an extraneous semicolon in the FSDirectory class that was causing compilation issues in some IDEs. This is a minor syntax fix.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4242. Remove extra \";\" in FSDirectory that blocks compilation\nin some IDE's. (szetszwo via omalley)\n```\n\n----------------------------------------\n\nTITLE: Defining Streaming Field Separators in Hadoop\nDESCRIPTION: Configuration properties for specifying field separators in Hadoop Streaming jobs. Allows customization of input and output field separators for map and reduce operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Properties\nCODE:\n```\nstream.map.input.field.separator\nstream.map.output.field.separator\nstream.reduce.input.field.separator\nstream.reduce.output.field.separator\n```\n\n----------------------------------------\n\nTITLE: Missing Constructor Documentation in NMClientAsync\nDESCRIPTION: Constructors for the NMClientAsync class lacking documentation blocks. These constructors initialize async node manager clients with various parameter combinations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.2/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nNMClientAsync(String, NMClient, AbstractCallbackHandler)\nNMClientAsync(String, AbstractCallbackHandler)\nNMClientAsync(AbstractCallbackHandler)\n```\n\n----------------------------------------\n\nTITLE: Configuration XML Changes for Speculative Execution\nDESCRIPTION: Updates to hadoop-default.xml configuration to allow independent control of speculative execution for map and reduce tasks. Deprecates the combined parameter in favor of separate map and reduce controls.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\nmapred.speculative.execution (deprecated)\nmapred.map.tasks.speculative.execution\nmapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Default Temporary Directory Configuration Path\nDESCRIPTION: Shows the default configuration for Hadoop's temporary directory, which is set to /tmp/hadoop-${user.name}, demonstrating variable expansion in configuration values.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: plain\nCODE:\n```\n${hadoop.tmp.dir}\n```\n\n----------------------------------------\n\nTITLE: Updating Shell Scripts for Portability in Hadoop\nDESCRIPTION: This change improves the portability of Hadoop shell scripts by using /bin/sh instead of /bin/bash. This allows the scripts to run on a wider range of Unix-like systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_35\n\nLANGUAGE: Shell\nCODE:\n```\n/bin/sh\n```\n\n----------------------------------------\n\nTITLE: Adding ClusterMetrics Checks to Java TestRMNodeTransitions\nDESCRIPTION: Improves the TestRMNodeTransitions test suite by adding checks for ClusterMetrics. This enhances the test coverage and reliability of the ResourceManager node transition logic.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1119. Add ClusterMetrics checks to tho TestRMNodeTransitions tests\n```\n\n----------------------------------------\n\nTITLE: Adding ClusterMetrics Checks to Java TestRMNodeTransitions\nDESCRIPTION: Improves the TestRMNodeTransitions test suite by adding checks for ClusterMetrics. This enhances the test coverage and reliability of the ResourceManager node transition logic.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1119. Add ClusterMetrics checks to tho TestRMNodeTransitions tests\n```\n\n----------------------------------------\n\nTITLE: Variable Expansion Default Path\nDESCRIPTION: Example showing the default temporary directory path construction using variable expansion.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n/tmp/hadoop-${user.name}\n```\n\n----------------------------------------\n\nTITLE: Improving Shuffling to Memory for Compressed Map Outputs\nDESCRIPTION: Bug fix to improve shuffling to memory when fetching multiple compressed map outputs. This addresses MAPREDUCE-5308.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5308. Shuffling to memory can get out-of-sync when fetching multiple compressed map outputs\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in Application Management Methods\nDESCRIPTION: Application lifecycle management methods missing version tags in YarnClient.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nfailApplicationAttempt(ApplicationAttemptId)\nkillApplication(ApplicationId, String)\nupdateApplicationPriority(ApplicationId, Priority)\n```\n\n----------------------------------------\n\nTITLE: Configuration Change - SequenceFile Compression Type\nDESCRIPTION: Deprecation notice for SequenceFile.setCompressionType in favor of new methods SequenceFile.createWriter, SequenceFileOutputFormat.setCompressionType, and JobConf.setMapOutputCompressionType. Includes configuration change to hadoop-default.xml.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: xml\nCODE:\n```\nio.seqfile.compression.type\n```\n\n----------------------------------------\n\nTITLE: Configuration XML Update for JVM Reuse\nDESCRIPTION: Configuration change to hadoop-default.xml adding a new property mapred.job.reuse.jvm.num.tasks to enable JVM reuse across Map-Reduce Tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Fixing typo in YARN configuration property name\nDESCRIPTION: Corrects a typo in a property name within the yarn-default.xml configuration file. This improves the accuracy of YARN's default configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\nYARN-4241. Fix typo of property name in yarn-default.xml.\n```\n\n----------------------------------------\n\nTITLE: Including Unhealthy Nodes in ClusterMetricsInfo Total Node Count\nDESCRIPTION: Corrects the ClusterMetricsInfo to include unhealthy nodes in the total node count reported by RM web services. This provides a more accurate representation of the cluster state.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1176. RM web services ClusterMetricsInfo total nodes doesn't include unhealthy nodes\n```\n\n----------------------------------------\n\nTITLE: Including Unhealthy Nodes in ClusterMetricsInfo Total Node Count\nDESCRIPTION: Corrects the ClusterMetricsInfo to include unhealthy nodes in the total node count reported by RM web services. This provides a more accurate representation of the cluster state.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nYARN-1176. RM web services ClusterMetricsInfo total nodes doesn't include unhealthy nodes\n```\n\n----------------------------------------\n\nTITLE: MapReduce Configuration Change - CapacityScheduler\nDESCRIPTION: Configuration change to add parameter for controlling maximum AM resource percentage in CapacityScheduler\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nyarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Updating Datanode RPC Protocol Version in Hadoop\nDESCRIPTION: Changes the Datanode RPC protocol version from 7 to 8 as part of HADOOP-1134. This modification affects the communication protocol between Datanodes and other Hadoop components.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\n// Datanode RPC protocol version changed from 7 to 8\n```\n\n----------------------------------------\n\nTITLE: Adding Java Version to Startup Message\nDESCRIPTION: Improves logging by adding the Java version to the Hadoop startup message, helping with troubleshooting and version compatibility issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nSystem.getProperty(\"java.version\")\n```\n\n----------------------------------------\n\nTITLE: Native Library Compilation\nDESCRIPTION: Native code compilation changes mentioned in 'HADOOP-10810. Clean up native code compilation warnings'\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: C\nCODE:\n```\nNative code compilation\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop UMask Mode\nDESCRIPTION: Adds a new configuration option 'dfs.umaskmode' to set the umask in octal or symbolic format instead of decimal.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nconf.set(\"dfs.umaskmode\", \"0022\") // Octal\nconf.set(\"dfs.umaskmode\", \"u=rwx,g=r-x,o=r-x\") // Symbolic\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Task Tracker Configuration\nDESCRIPTION: Introduces new configuration properties for specifying the maximum number of map and reduce tasks per TaskTracker, replacing the deprecated single property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Properties\nCODE:\n```\n# New properties:\nmapred.tasktracker.map.tasks.maximum=2\nmapred.tasktracker.reduce.tasks.maximum=2\n\n# Removed property:\nmapred.tasktracker.tasks.maximum\n```\n\n----------------------------------------\n\nTITLE: Updating Hadoop Script for Deprecated Commands in Bash\nDESCRIPTION: This change modifies the hadoop script to recognize a full set of deprecated commands.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nHADOOP-8214. make hadoop script recognize a full set of deprecated commands (rvs via tucu)\n```\n\n----------------------------------------\n\nTITLE: Adding Java Version to Startup Message\nDESCRIPTION: Improves logging by adding the Java version to the Hadoop startup message, helping with troubleshooting and version compatibility issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nSystem.getProperty(\"java.version\")\n```\n\n----------------------------------------\n\nTITLE: Starting Hadoop Services\nDESCRIPTION: Commands to start various Hadoop services including HDFS (NameNode and DataNode) and YARN (ResourceManager and NodeManager).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# Start HDFS services\nstart-dfs.sh\n\n# Start YARN services\nstart-yarn.sh\n\n# Verify services are running\njps\n```\n\n----------------------------------------\n\nTITLE: Removing Java 5 Dependencies in Hadoop Build\nDESCRIPTION: This code snippet removes Java 5 dependencies from the Hadoop build process. It's part of the bug fixes for release 0.20.3.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7072. Remove java5 dependencies from build. (cos)\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in AMRMClientAsync waitFor Methods\nDESCRIPTION: Documentation issue report for two overloaded waitFor methods in the AMRMClientAsync class that are missing @since version tags. Methods take a Supplier and timeout parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.1.2/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int)\n```\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Adding Generic Type Parameter to RetryInvocationHandler in Java\nDESCRIPTION: Adds a generic type parameter to the RetryInvocationHandler class to improve type safety.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n// HADOOP-9803\npublic class RetryInvocationHandler<T> implements InvocationHandler {\n    // Implementation\n}\n```\n\n----------------------------------------\n\nTITLE: Removing Unused DFS Client Buffer Directory Configuration in Hadoop XML\nDESCRIPTION: Removes the unused configuration property 'dfs.client.buffer.dir' from hadoop-default.xml to clean up obsolete settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: XML\nCODE:\n```\nRemove unused dfs.client.buffer.dir from hadoop-default.xml\n```\n\n----------------------------------------\n\nTITLE: Deprecated Method Removals in RawLocalFileSystem\nDESCRIPTION: Lists deprecated methods that were removed from RawLocalFileSystem as part of fixing findbugs warnings\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: java\nCODE:\n```\npublic String getName()\npublic void lock(Path p, boolean shared)\npublic void release(Path p)\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Speculative Execution Control\nDESCRIPTION: Configuration changes to hadoop-default.xml that provide finer-grained control over speculative execution. The changes allow users to set speculative execution independently for maps and reduces rather than using a single global setting.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Speculative Execution Control\nDESCRIPTION: Configuration changes to hadoop-default.xml that provide finer-grained control over speculative execution. The changes allow users to set speculative execution independently for maps and reduces rather than using a single global setting.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Fixing Time Conversion in C and Java for File Access Times\nDESCRIPTION: Corrects the conversion between seconds in C and milliseconds in Java for file access times. This ensures consistency in time representations across languages.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nHADOOP-4280. Fix conversions between seconds in C and milliseconds in \nJava for access times for files. (Pete Wyckoff via rangadi)\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Status Persistence in Hadoop XML\nDESCRIPTION: This snippet shows configuration changes in hadoop-default.xml to persist statuses of completed jobs in HDFS, allowing queries about decommissioned jobs and across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_33\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                                /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Updating CodecFactory in Java\nDESCRIPTION: Allows CodecFactory to return a codec object given a codec's class name.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: java\nCODE:\n```\nAllow CodecFactory to return a codec object given a codec'\nclass name.\n```\n\n----------------------------------------\n\nTITLE: Managing Application Properties in YARN (Java)\nDESCRIPTION: These methods and fields in the AppInfo class handle various properties of a YARN application, including priority, running containers, and whether it's an unmanaged application.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppInfo {\n    public int getPriority() { /* ... */ }\n    public int getRunningContainers() { /* ... */ }\n    public boolean isUnmanagedApp() { /* ... */ }\n    private int priority;\n    private int runningContainers;\n    private boolean unmanagedApplication;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Java Preference for IPv4 Stack in Hadoop\nDESCRIPTION: Configures Hadoop to use IPv4 instead of IPv6 by setting the java.net.preferIPv4Stack system property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nSystem.setProperty(\"java.net.preferIPv4Stack\", \"true\");\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated RawLocalFileSystem Methods in Java\nDESCRIPTION: Removes deprecated methods from the RawLocalFileSystem class as part of fixing findbugs warnings. This change removes the getName(), lock(), and release() methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\npublic String getName()\npublic void lock(Path p, boolean shared)\npublic void release(Path p)\n```\n\n----------------------------------------\n\nTITLE: Improving Space Quota Parsing in Java\nDESCRIPTION: Fixes the conversion of \"TB\" extension to terabytes in the -setSpaceQuota command. Now uses StringUtils for parsing, improving accuracy and consistency in space quota management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4254. -setSpaceQuota command does not convert \"TB\" extenstion to\nterabytes properly. Implementation now uses StringUtils for parsing this.\n(Raghu Angadi)\n```\n\n----------------------------------------\n\nTITLE: Configuration Change for Task Limits in Hadoop\nDESCRIPTION: Changes to hadoop-default.xml that replace the single task limit with separate limits for map and reduce tasks, allowing better configuration of heterogeneous clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_31\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.tasktracker.map.tasks.maximum (default value of 2)\nadd mapred.tasktracker.reduce.tasks.maximum (default value of 2)\nremove mapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Adding logging level configuration for map/reduce tasks in Java\nDESCRIPTION: Introduces new configuration parameters to allow setting logging levels separately for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nadd mapred.map.child.log.level \nadd mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Task Limits\nDESCRIPTION: Configuration changes in hadoop-default.xml that add separate limits for map and reduce tasks per TaskTracker, replacing the deprecated single task limit.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: XML\nCODE:\n```\nmapred.tasktracker.map.tasks.maximum (default value of 2)\nmapred.tasktracker.reduce.tasks.maximum (default value of 2)\nmapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Updating Credentials Cache in FileOutputCommitter\nDESCRIPTION: Fixes an issue where the initialized Credentials cache appears to be empty inside FileOutputCommitter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5240 inside of FileOutputCommitter the initialized Credentials cache appears to be empty. (vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Using ServiceLoader for FileSystem Implementations in Java\nDESCRIPTION: This change implements the JDK ServiceLoader mechanism to discover FileSystem implementations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7549. Use JDK ServiceLoader mechanism to find FileSystem implementations. (tucu)\n```\n\n----------------------------------------\n\nTITLE: Bash Script Path Fix\nDESCRIPTION: Fix to specify /bin/bash instead of /bin/sh in the slaves.sh script to ensure proper execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n/bin/bash\n```\n\n----------------------------------------\n\nTITLE: Code Reference: MapReduce Mapper/Reducer API Changes\nDESCRIPTION: Example of new MapReduce API changes under HADOOP-1230 showing output file naming conventions.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\npart-r-00000 for the output of reduce 0\npart-m-00000 for the output of map 0\n```\n\n----------------------------------------\n\nTITLE: Adding synchronization to JobTracker methods in Java\nDESCRIPTION: Adds synchronization to JobTracker methods in the RecoveryManager class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5548. Add synchronization for JobTracker methods in RecoveryManager.\n(Amareshwari Sriramadasu via sharad)\n```\n\n----------------------------------------\n\nTITLE: Text Instance Access Pattern\nDESCRIPTION: Example showing broken Text class implementation where private members are accessed directly in compareTo and set methods\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: java\nCODE:\n```\nText.set(Text) // Broken - accesses private members directly\\nText.compareTo(Object) // Broken - accesses private members directly\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in CommonConfigurationKeysPublic Fields\nDESCRIPTION: Constants in org.apache.hadoop.fs.CommonConfigurationKeysPublic missing @since tags for KMS client configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class CommonConfigurationKeysPublic {\n    String KMS_CLIENT_FAILOVER_MAX_RETRIES_KEY\n    long KMS_CLIENT_FAILOVER_SLEEP_BASE_MILLIS_DEFAULT\n    String KMS_CLIENT_FAILOVER_SLEEP_BASE_MILLIS_KEY\n    long KMS_CLIENT_FAILOVER_SLEEP_MAX_MILLIS_DEFAULT\n    String KMS_CLIENT_FAILOVER_SLEEP_MAX_MILLIS_KEY\n    int KMS_CLIENT_TIMEOUT_DEFAULT\n    String KMS_CLIENT_TIMEOUT_SECONDS\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling FsShell to Read Avro Files\nDESCRIPTION: This improvement allows the FsShell '-text' command to read Avro files stored in HDFS and other filesystems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9740. Fix FsShell '-text' command to be able to read Avro\\n    files stored in HDFS and other filesystems. (Allan Yan via jlowe)\n```\n\n----------------------------------------\n\nTITLE: Configuration XML Changes for Speculative Execution in Hadoop MapReduce\nDESCRIPTION: Configuration changes in hadoop-default.xml to allow finer-grained control over speculative execution. Replaces the general mapred.speculative.execution property with separate controls for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration.getLongBytes Method in Java\nDESCRIPTION: This snippet adds a new method Configuration.getLongBytes to handle human-readable byte size values. It addresses HADOOP-7910.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7910. Add Configuration.getLongBytes to handle human readable byte size values. (Sho Shimauchi via harsh)\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration.getLongBytes Method in Java\nDESCRIPTION: This snippet adds a new method Configuration.getLongBytes to handle human-readable byte size values. It addresses HADOOP-7910.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7910. Add Configuration.getLongBytes to handle human readable byte size values. (Sho Shimauchi via harsh)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation for CommonConfigurationKeys Field (Java)\nDESCRIPTION: The IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT field in the org.apache.hadoop.fs.CommonConfigurationKeys class is missing its documentation block. This documentation is crucial for understanding the purpose and usage of the configuration key.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r1.2.1/jdiff/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.fs.CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT\n```\n\n----------------------------------------\n\nTITLE: Adding Job Counters for I/O Operations in Java\nDESCRIPTION: Implements job counters to measure the number of bytes read and written to HDFS, S3, KFS, and local file systems.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// New job counters for measuring I/O operations\n```\n\n----------------------------------------\n\nTITLE: Setting YARN Application Classpath in YarnConfiguration (Java)\nDESCRIPTION: Sets the default value for the YARN application classpath configuration in YarnConfiguration. This allows specifying the classpath used for YARN applications.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nYARN_APPLICATION_CLASSPATH\n```\n\n----------------------------------------\n\nTITLE: Adding Local Host Check Method in NetUtils Java Class\nDESCRIPTION: Implemented a new method in NetUtils class to determine if an InetAddress belongs to the local host, enhancing network-related functionality in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n// New NetUtils method to check if InetAddress is local\n```\n\n----------------------------------------\n\nTITLE: Default MapReduce Shuffle Port Property\nDESCRIPTION: Property definition for default MapReduce shuffle port in mapred-default.xml\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nmapreduce.shuffle.port\n```\n\n----------------------------------------\n\nTITLE: Resolving Connection Leak in Jets3tNativeFileSystemStore\nDESCRIPTION: Fixes a connection leak in the retrieveMetadata method of Jets3tNativeFileSystemStore.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nConnection leak in Jets3tNativeFileSystemStore#retrieveMetadata.\n```\n\n----------------------------------------\n\nTITLE: Resolving Connection Leak in Jets3tNativeFileSystemStore\nDESCRIPTION: Fixes a connection leak in the retrieveMetadata method of Jets3tNativeFileSystemStore.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nConnection leak in Jets3tNativeFileSystemStore#retrieveMetadata.\n```\n\n----------------------------------------\n\nTITLE: Setting End-of-Record Delimiter for TextInputFormat in Java\nDESCRIPTION: Allows configuring a custom end-of-record delimiter for TextInputFormat. This provides more flexibility when parsing input data with non-standard record separators.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nTextInputFormat.setRecordDelimiter(Job job, String delimiter)\n```\n\n----------------------------------------\n\nTITLE: Setting End-of-Record Delimiter for TextInputFormat in Java\nDESCRIPTION: Allows configuring a custom end-of-record delimiter for TextInputFormat. This provides more flexibility when parsing input data with non-standard record separators.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nTextInputFormat.setRecordDelimiter(Job job, String delimiter)\n```\n\n----------------------------------------\n\nTITLE: Updating DATA_TRANSFER_VERSION in Java\nDESCRIPTION: Updates the DATA_TRANSFER_VERSION constant to reflect changes made in HADOOP-3981. This likely affects data transfer protocols in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-4197. Update DATA_TRANSFER_VERSION for HADOOP-3981. (szetszwo)\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Header\nDESCRIPTION: Standard Apache License 2.0 header used at the beginning of the file to specify usage terms and conditions.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/news/NOTE.txt#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License. See accompanying LICENSE file.\n#\n```\n\n----------------------------------------\n\nTITLE: Removing Main-Class from build.xml in MapReduce Tools (XML)\nDESCRIPTION: Removes the incorrectly set Main-Class attribute from the build.xml file for MapReduce tools. Also renames the 'tools-jar' target to 'tools' for clarity.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: XML\nCODE:\n```\n<project name=\"hadoop-mapreduce-tools\" default=\"jar\">\n  <target name=\"tools\" depends=\"compile\">\n    <jar jarfile=\"${build.dir}/${name}-${version}.jar\">\n      <fileset dir=\"${build.classes}\">\n        <include name=\"**/*.class\"/>\n      </fileset>\n    </jar>\n  </target>\n</project>\n```\n\n----------------------------------------\n\nTITLE: Configuring TaskTracker Map/Reduce Tasks\nDESCRIPTION: Configuration entries in hadoop-default.xml to support different numbers of mappers and reducers per TaskTracker. Adds new parameters for maximum map and reduce tasks while deprecating the old combined parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\nmapred.tasktracker.map.tasks.maximum (default value of 2)\nmapred.tasktracker.reduce.tasks.maximum (default value of 2)\nmapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Including Configuration Files in Hadoop\nDESCRIPTION: Allows a Hadoop configuration file to include other configuration files, enabling modular and reusable configuration setups.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: XML\nCODE:\n```\n<configuration>\n  <include>\n    <file>included-config.xml</file>\n  </include>\n  <!-- Other configuration properties -->\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Cygpath Error Handling for Log Directory in Hadoop Scripts\nDESCRIPTION: Bug fix for a Cygwin compatibility issue that ensures the script handles the case when the log directory does not exist, preventing cygpath errors.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nFix cygpath error if the log directory does not exist.\n```\n\n----------------------------------------\n\nTITLE: Initializing SaslRpcServer Without Secret Manager\nDESCRIPTION: Ensures SaslRpcServer is initialized even when no secret manager is present.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nSaslRpcServer should be initialized even when no secret manager present.\n```\n\n----------------------------------------\n\nTITLE: Managing Application Attempt Information in YARN (Java)\nDESCRIPTION: This snippet shows methods for retrieving timing information for a YARN application attempt, including the start and finish times. These methods are part of the AppAttemptInfo class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppAttemptInfo {\n    public long getFinishedTime() { /* ... */ }\n    public long getStartedTime() { /* ... */ }\n    private long finishedTime;\n    private long startedTime;\n}\n```\n\n----------------------------------------\n\nTITLE: Removing Main-Class from build.xml in Hadoop MapReduce\nDESCRIPTION: Removes the incorrectly set Main-Class attribute from the tools target in build.xml and renames the target from 'tools-jar' to 'tools'.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: XML\nCODE:\n```\n<target name=\"tools\" depends=\"compile\">\n    <!-- Remove Main-Class attribute -->\n    <jar jarfile=\"${build.dir}/${name}-tools-${version}.jar\">\n        <!-- ... other jar contents ... -->\n    </jar>\n</target>\n```\n\n----------------------------------------\n\nTITLE: Configuration Method Deprecations in Hadoop\nDESCRIPTION: List of deprecated methods removed from Configuration.java including object handling and iteration methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\ngetObject(String name)\nsetObject(String name, Object value)\nget(String name, Object defaultValue)\nset(String name, Object value)\nIterator entries()\n```\n\n----------------------------------------\n\nTITLE: Setting Mini YARN Cluster Property\nDESCRIPTION: Configuration property name for identifying Mini YARN Cluster mode, referenced in MAPREDUCE-4484.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/mapreduce/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nIS_MINI_YARN_CLUSTER\n```\n\n----------------------------------------\n\nTITLE: Configuring HADOOP_OPTS in Java\nDESCRIPTION: Adds better control of HADOOP_OPTS for each Hadoop component.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// No specific code provided, but implies changes to how HADOOP_OPTS are handled\n```\n\n----------------------------------------\n\nTITLE: MIT CRC Implementation Copyright Notice\nDESCRIPTION: Copyright notice for native implementation of slicing-by-8 CRC calculation code from MIT.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/LICENSE.txt#2025-04-08_snippet_1\n\nLANGUAGE: C\nCODE:\n```\n/**\n *   Copyright 2008,2009,2010 Massachusetts Institute of Technology.\n *   All rights reserved. Use of this source code is governed by a\n *   BSD-style license that can be found in the LICENSE file.\n */\n```\n\n----------------------------------------\n\nTITLE: Using JobID, TaskID and TaskAttemptID classes in Java\nDESCRIPTION: New JobID, TaskID and TaskAttemptID classes are introduced to replace string identifiers. Applications can convert between objects and strings using toString() and forName() methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nJobID jobId = JobID.forName(jobIdString);\nString jobIdString = jobId.toString();\n\nTaskID taskId = TaskID.forName(taskIdString);\nString taskIdString = taskId.toString();\n\nTaskAttemptID attemptId = TaskAttemptID.forName(attemptIdString);\nString attemptIdString = attemptId.toString();\n```\n\n----------------------------------------\n\nTITLE: Extending LightWeightGSet for Element Eviction (Java)\nDESCRIPTION: Extends the LightWeightGSet class to support eviction of expired elements. This improvement helps in managing memory usage more efficiently in Hadoop components.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nLightWeightGSet\n```\n\n----------------------------------------\n\nTITLE: Accessing Container and Node Information in YARN (Java)\nDESCRIPTION: This method and field in the ContainerInfo class provide access to the node ID where a container is running, essential for resource tracking and management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\npublic class ContainerInfo {\n    public String getNodeId() { /* ... */ }\n    private String nodeId;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting DFS umask Configuration Variable in Java\nDESCRIPTION: Allows setting the 'dfs.umask' configuration variable to control the umask used by DFS (Distributed File System).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\ndfs.umask\n```\n\n----------------------------------------\n\nTITLE: Fixing OsSecureRandom reservoir filling in Java\nDESCRIPTION: Modifies OsSecureRandom to lazily fill its reservoir. This bug fix was implemented in Hadoop 2.7.1.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-11891. OsSecureRandom should lazily fill its reservoir (asuresh)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Resource Usage Calculation in FSAppAttempt\nDESCRIPTION: Skips object allocation in FSAppAttempt#getResourceUsage when possible to improve performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nYARN-4690. Skip object allocation in FSAppAttempt#getResourceUsage when possible (Ming Ma via sjlee)\n```\n\n----------------------------------------\n\nTITLE: Niels Provos Copyright Notice for tree.h\nDESCRIPTION: Copyright and license notice for src/main/native/util/tree.h component. Details redistribution terms and liability disclaimers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hdfs/LICENSE.txt#2025-04-08_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n/*-\n * Copyright 2002 Niels Provos <provos@citi.umich.edu>\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\n * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\n * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n```\n\n----------------------------------------\n\nTITLE: Removing Explicit Log4JLogger Reference\nDESCRIPTION: Removes an explicit reference to Log4JLogger that was causing issues for non-log4j users.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\nExplicit reference to Log4JLogger breaks non-log4j users\n```\n\n----------------------------------------\n\nTITLE: Configuring DFS Umask in Hadoop\nDESCRIPTION: Allows setting a configuration variable named dfs.umask to set the umask used by DFS. This provides more control over file permissions in HDFS.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nconf.set(\"dfs.umask\", \"022\");\n```\n\n----------------------------------------\n\nTITLE: DFS Umask Configuration\nDESCRIPTION: Configuration variable that allows setting the umask used by DFS for file permissions.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: Properties\nCODE:\n```\ndfs.umask\n```\n\n----------------------------------------\n\nTITLE: Adding Namespace Declarations in Protobuf Files\nDESCRIPTION: Includes namespace declarations in .proto files for languages other than Java, improving compatibility and organization of generated code.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Protobuf\nCODE:\n```\npackage hadoop.common;\n\noption java_package = \"org.apache.hadoop.common.proto\";\noption java_outer_classname = \"SecurityProtos\";\n```\n\n----------------------------------------\n\nTITLE: No code snippets found\nDESCRIPTION: This document contains release notes and change descriptions but no actual code snippets\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_9\n\n\n\n----------------------------------------\n\nTITLE: XML Configuration Fix for Line Endings\nDESCRIPTION: Fix for format strings to use platform independent line separators in XML edits file\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n// HDFS-6162: Format strings should use platform independent line separator\n```\n\n----------------------------------------\n\nTITLE: European Commission OneLab Bloom Filter Copyright Notice\nDESCRIPTION: Copyright and license notice for org.apache.hadoop.util.bloom.* classes from the European Commission OneLab project. Specifies redistribution terms and warranty disclaimers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/LICENSE.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n/**\n *\n * Copyright (c) 2005, European Commission project OneLab under contract\n * 034819 (http://www.one-lab.org)\n * All rights reserved.\n * Redistribution and use in source and binary forms, with or \n * without modification, are permitted provided that the following \n * conditions are met:\n *  - Redistributions of source code must retain the above copyright \n *    notice, this list of conditions and the following disclaimer.\n *  - Redistributions in binary form must reproduce the above copyright \n *    notice, this list of conditions and the following disclaimer in \n *    the documentation and/or other materials provided with the distribution.\n *  - Neither the name of the University Catholique de Louvain - UCL\n *    nor the names of its contributors may be used to endorse or \n *    promote products derived from this software without specific prior \n *    written permission.\n *    \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT \n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS \n * FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE \n * COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, \n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, \n * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER \n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT \n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN \n * ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n * POSSIBILITY OF SUCH DAMAGE.\n */\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delegation Tokens in Hadoop FileSystem (Java)\nDESCRIPTION: Adds a new API to FileSystem for retrieving delegation tokens and checking for known tokens in a Credentials object.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem.getDelegationTokens(Credentials credentials)\n```\n\n----------------------------------------\n\nTITLE: Adding getMsb/getLsb Methods to ClientId in Hadoop\nDESCRIPTION: Adds getMsb (most significant bits) and getLsb (least significant bits) methods to the ClientId class for retrieving parts of the client identifier.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9821. ClientId should have getMsb/getLsb methods.\n```\n\n----------------------------------------\n\nTITLE: Configuring Timeline Service in Hadoop YARN (Java)\nDESCRIPTION: Methods to check if various versions of the YARN Timeline Service are enabled in the configuration. These are part of the YarnConfiguration class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.1/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nYarnConfiguration.timelineServiceEnabled(Configuration conf)\nYarnConfiguration.timelineServiceV15Enabled(Configuration conf)\nYarnConfiguration.timelineServiceV1Enabled(Configuration conf)\nYarnConfiguration.timelineServiceV2Enabled(Configuration conf)\n```\n\n----------------------------------------\n\nTITLE: Client ID Generation for RPC Requests\nDESCRIPTION: Renamed Client#uuid to Client#clientId for better semantic clarity in RPC client implementation. This field provides a globally unique identifier for RPC clients.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nClient#uuid to Client#clientId\n```\n\n----------------------------------------\n\nTITLE: Native Code Fix in NativeIO.c\nDESCRIPTION: Fix for uninitialized variables in the NativeIO.c native implementation file. Part of HADOOP-11193.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: c\nCODE:\n```\n// Referenced in HADOOP-11193 but actual code not shown in log\n```\n\n----------------------------------------\n\nTITLE: Fixing TestTimedOutTestsListener timeout (Java)\nDESCRIPTION: Addresses HADOOP-12736 where TestTimedOutTestsListener#testThreadDumpAndDeadlocks sometimes times out. This fix improves the reliability of the test.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-12736. TestTimedOutTestsListener#testThreadDumpAndDeadlocks sometimes times out.\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Frontmatter for Hadoop Issue Tracking Page\nDESCRIPTION: YAML frontmatter defining the title and menu structure for the Hadoop Issue Tracking page. It sets the page title and positions the page within the development section of the main menu.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/issue_tracking.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Hadoop Issue Tracking\nmenu:\n   main:\n      name: \"Issue Tracking\"\n      parent: \"development\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Socket Factories in Hadoop\nDESCRIPTION: Allows specialization and configuration of socket factories in Hadoop, introducing StandardSocketFactory and SocksSocketFactory for SOCKS proxy support.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration conf = new Configuration();\nconf.setClass(\"hadoop.rpc.socket.factory.class.default\", StandardSocketFactory.class, SocketFactory.class);\n// Or for SOCKS proxy\nconf.setClass(\"hadoop.rpc.socket.factory.class.default\", SocksSocketFactory.class, SocketFactory.class);\n```\n\n----------------------------------------\n\nTITLE: Hadoop Speculative Execution Configuration\nDESCRIPTION: Configuration properties for controlling speculative execution separately for map and reduce tasks, deprecating the original single control property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: properties\nCODE:\n```\nmapred.speculative.execution\nmapred.map.tasks.speculative.execution\nmapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for ARM Build with JAVA_HOME\nDESCRIPTION: A CMake configuration that always uses JAVA_HOME to locate libjvm.so, jni.h, and jni_md.h. This is mentioned in the context of fixing ARM builds.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake: always use JAVA_HOME to find libjvm.so, jni.h, jni_md.h.\n```\n\n----------------------------------------\n\nTITLE: Resource Manager Queue Configuration\nDESCRIPTION: Configuration pattern for defining resource manager queue properties using the hadoop.rm.queue prefix.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Removed Deprecated RawLocalFileSystem Methods\nDESCRIPTION: Code showing the removed deprecated methods from RawLocalFileSystem as part of HADOOP-4253 findbugs fixes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: java\nCODE:\n```\npublic String getName()\npublic void lock(Path p, boolean shared)\npublic void release(Path p)\n```\n\n----------------------------------------\n\nTITLE: FileOutputCommitter Credentials Bug Fix\nDESCRIPTION: Code fix in FileOutputCommitter where initialized Credentials cache appears empty, identified in MAPREDUCE-5240.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5240 inside of FileOutputCommitter the initialized Credentials cache\\nappears to be empty. (vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Removed Deprecated RawLocalFileSystem Methods\nDESCRIPTION: Code showing the removed deprecated methods from RawLocalFileSystem as part of HADOOP-4253 findbugs fixes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: java\nCODE:\n```\npublic String getName()\npublic void lock(Path p, boolean shared)\npublic void release(Path p)\n```\n\n----------------------------------------\n\nTITLE: Missing @Since Tags in YARN Wait Methods\nDESCRIPTION: Various overloaded waitFor methods in AMRMClient and AMRMClientAsync classes requiring version tags. These methods accept Supplier and timeout parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient.waitFor(java.util.function.Supplier)\norg.apache.hadoop.yarn.client.api.AMRMClient.waitFor(java.util.function.Supplier, int)\norg.apache.hadoop.yarn.client.api.AMRMClient.waitFor(java.util.function.Supplier, int, int)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Configuration Properties for Hadoop System\nDESCRIPTION: List of configuration properties for DFS and MapReduce components including addresses for secondary namenode, datanode, and job/task trackers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: properties\nCODE:\n```\ndfs.secondary.http.address\ndfs.datanode.address\ndfs.datanode.http.address\ndfs.http.address\nmapred.job.tracker.http.address\nmapred.task.tracker.report.address\nmapred.task.tracker.http.address\n```\n\n----------------------------------------\n\nTITLE: HDFS URL Configuration Update\nDESCRIPTION: Update to the default filesystem name configuration to include the hdfs:// protocol prefix.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: xml\nCODE:\n```\nfs.default.name=hdfs://\n```\n\n----------------------------------------\n\nTITLE: Adding MapReduce Task Logging Level Configuration\nDESCRIPTION: New configuration properties are added to allow setting the logging level for map and reduce tasks separately.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nadd mapred.map.child.log.level\nadd mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Checking safe container-executor permissions configuration with ls command\nDESCRIPTION: This code shows a safe configuration of container-executor without the SUID bit set. Though this configuration doesn't support Yarn Secure Containers, it prevents the privilege escalation vulnerability by showing standard executable permissions.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/cve_list.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -laF /opt/hadoop/bin/container-executor\n-rwxr-xr-x. 1 yarn hadoop 802968 May 9 20:21 /opt/hadoop/bin/container-executor\n```\n\n----------------------------------------\n\nTITLE: Defining HTML Structure for Apache Hadoop Website\nDESCRIPTION: This HTML snippet defines the basic structure of the Apache Hadoop project website. It includes meta tags for character encoding and viewport, links to external stylesheets, and sets up the main layout with a header, navigation, and content area.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.1.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Apache Hadoop</title>\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css\"\n        integrity=\"sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65\" crossorigin=\"anonymous\">\n    <link rel=\"stylesheet\" href=\"/css/site.css\">\n</head>\n\n<body>\n    <header>\n        <nav>\n            <div class=\"logo-container\">\n                <a href=\"/\">\n                    <img width=\"210\" height=\"51\" class=\"logo\" src=\"/hadoop-logo.jpg\" alt=\"Apache Hadoop logo\">\n                </a>\n            </div>\n        </nav>\n    </header>\n    <main class=\"content-container\">\n        <div class=\"container\">\n            ${content}\n        </div>\n    </main>\n</body>\n\n</html>\n```\n\n----------------------------------------\n\nTITLE: Adding JVM Reuse Configuration in Hadoop XML\nDESCRIPTION: Adds a new configuration property 'mapred.job.reuse.jvm.num.tasks' to hadoop-default.xml to enable JVM reuse across Map-Reduce tasks for improved performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in AppAttemptInfo Class - Timing Methods\nDESCRIPTION: Methods and fields related to timing information that lack documentation blocks in AppAttemptInfo class\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.6/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppAttemptInfo {\n    private long startedTime;\n    private long finishedTime;\n\n    public long getStartedTime()\n    public long getFinishedTime()\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Merges for Reduce Phase in Java\nDESCRIPTION: Enables Reducers to perform merges for on-disk map output files in parallel with their copying, improving performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n// Implementation of parallel merges for Reducers\n```\n\n----------------------------------------\n\nTITLE: Enhancing EC2 Script Error Handling\nDESCRIPTION: Improves EC2 scripts to display an error message when the AWS_ACCOUNT_ID environment variable is not set, helping users identify configuration issues more easily.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nMake EC2 scripts show an error message if AWS_ACCOUNT_ID is unset.\n```\n\n----------------------------------------\n\nTITLE: Node Manager Address Property\nDESCRIPTION: YARN configuration property for setting Node Manager address\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nyarn.nodemanager.address\n```\n\n----------------------------------------\n\nTITLE: FileOutputCommitter Credentials Bug Fix in MAPREDUCE-5240\nDESCRIPTION: This note describes a bug fix where the initialized Credentials cache appears to be empty in FileOutputCommitter, affecting OutputCommitter's ability to access credentials set by the user.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: plain text\nCODE:\n```\nMAPREDUCE-5240 inside of FileOutputCommitter the initialized Credentials cache\nappears to be empty. (vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Merges for Reduce Phase in Java\nDESCRIPTION: Enables Reducers to perform merges for on-disk map output files in parallel with their copying, improving performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n// Implementation of parallel merges for Reducers\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration Changes - XML and Properties\nDESCRIPTION: Changes to Hadoop's default configuration files including removal and modification of parameters in hadoop-default.xml and log4j.properties related to user logs and task tracking.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: xml\nCODE:\n```\nremove mapred.userlog.num.splits\nremove mapred.userlog.purge.splits\nchange default mapred.userlog.limit.kb to 0 (no limit)\nchange default mapred.userlog.retain.hours to 24\n```\n\nLANGUAGE: properties\nCODE:\n```\nremove log4j.appender.TLA.noKeepSplits\nremove log4j.appender.TLA.purgeLogSplits\nremove log4j.appender.TLA.logsRetainHours\n```\n\n----------------------------------------\n\nTITLE: Hadoop YARN API Method References\nDESCRIPTION: List of Java method signatures missing proper documentation blocks or @since tags in the YARN API. These methods handle core YARN functionality like resource allocation, container management, and application lifecycle.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.api.records.ResourceRequest.newBuilder()\norg.apache.hadoop.yarn.api.records.ContainerId.newContainerId(ApplicationAttemptId, long)\norg.apache.hadoop.yarn.api.records.Resource.newInstance(long, int)\norg.apache.hadoop.yarn.api.records.ApplicationAttemptId.newInstance(ApplicationId, int)\n```\n\n----------------------------------------\n\nTITLE: Setting up SSH for Hadoop\nDESCRIPTION: Commands to generate and configure SSH keys for passwordless authentication between Hadoop nodes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\nssh localhost\n```\n\n----------------------------------------\n\nTITLE: Updating MapWritable.readFields Method in Java\nDESCRIPTION: Modifies the MapWritable.readFields method to clear the instance field variable every time it is called, ensuring proper deserialization.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_33\n\nLANGUAGE: Java\nCODE:\n```\npublic void readFields(DataInput in) throws IOException {\n  this.clear();\n  // ... existing deserialization logic\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Task Logging Levels in Java\nDESCRIPTION: Adds new configuration options to set logging levels separately for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapred.map.child.log.level \n  add mapred.reduce.child.log.level\n```\n\n----------------------------------------\n\nTITLE: Optimizing disk access for last partial chunk checksum (Java)\nDESCRIPTION: Optimization to improve disk access efficiency for the last partial chunk checksum of finalized replicas.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-11187. Optimize disk access for last partial chunk checksum of Finalized replica.\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN ResourceManager Options in Windows Batch File\nDESCRIPTION: This XML snippet shows the configuration for setting YARN ResourceManager options in a Windows batch file. It demonstrates the correct environment variable name to use for ResourceManager options.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>yarn.resourcemanager.opts</name>\n  <value>%YARN_RESOURCEMANAGER_OPTS%</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Tracking Individual RPC Metrics in Java\nDESCRIPTION: Implements functionality to track individual RPC metrics, providing more detailed performance information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n// Code to track individual RPC metrics\n```\n\n----------------------------------------\n\nTITLE: Tracking Individual RPC Metrics in Java\nDESCRIPTION: Implements functionality to track individual RPC metrics, providing more detailed performance information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n// Code to track individual RPC metrics\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in AMRMClientAsync waitFor Methods\nDESCRIPTION: Multiple overloads of the waitFor method in the AMRMClientAsync class are missing @since tags. These methods take various combinations of Supplier and integer parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.4/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Formatting Hadoop HDFS Filesystem\nDESCRIPTION: Command to format the Hadoop Distributed File System (HDFS) namenode, which initializes the storage directories and creates the filesystem structure.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nhdfs namenode -format\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Block for YarnClient Method\nDESCRIPTION: The updateApplicationTimeouts method in the YarnClient class is missing a documentation block. This method likely updates timeouts for a YARN application.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.YarnClient Method updateApplicationTimeouts(org.apache.hadoop.yarn.api.protocolrecords.UpdateApplicationTimeoutsRequest)\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag in AMRMClient Method\nDESCRIPTION: The waitFor method in org.apache.hadoop.yarn.client.api.AMRMClient is missing the @since tag. This method takes a Supplier and two integer parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient.waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Implementing Container ID Generation with Epochs\nDESCRIPTION: Added epoch numbers to container IDs to ensure uniqueness after ResourceManager restarts. Part of YARN-2052 implementation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nYARN-2052. Embedded an epoch number in container id to ensure the uniqueness of container id after RM restarts.\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop core-site.xml\nDESCRIPTION: XML configuration for the core-site.xml file that defines filesystem properties and default Hadoop parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://localhost:9000</value>\n    </property>\n    <property>\n        <name>hadoop.tmp.dir</name>\n        <value>/home/hadoop/tmp</value>\n    </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Log Level Command Addition - Bash\nDESCRIPTION: New command line option added to change log levels dynamically via hadoop --loglevel command.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nHADOOP-7984. Add hadoop --loglevel option to change log level.\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in Task and Job Fields\nDESCRIPTION: Fields related to task IDs and job configuration missing documentation blocks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nTASK_ID_REGEX\ntaskIdPattern\nUSE_WILDCARD_FOR_LIBJARS\nDEFAULT_USE_WILDCARD_FOR_LIBJARS\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop core-site.xml\nDESCRIPTION: XML configuration for the core-site.xml file that defines filesystem properties and default Hadoop parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://localhost:9000</value>\n    </property>\n    <property>\n        <name>hadoop.tmp.dir</name>\n        <value>/home/hadoop/tmp</value>\n    </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Making Configuration.getProps() Protected in Java\nDESCRIPTION: Changes the visibility of Configuration.getProps() method to protected to support meaningful subclassing of Configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nprotected Properties getProps() {\n  return properties;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing BytesWritable Constructor in Java\nDESCRIPTION: Adds an additional constructor to the BytesWritable class to improve flexibility.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-310. Additional constructor requested in BytesWritable.\n```\n\n----------------------------------------\n\nTITLE: AMRMClient Tracking URL Methods Missing @since Tags\nDESCRIPTION: Methods in AMRMClient and AMRMClientAsync classes for updating tracking URLs that lack proper version documentation tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.6/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient.updateTrackingUrl(String)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.updateTrackingUrl(String)\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Speculative Execution in Hadoop XML\nDESCRIPTION: Updates to hadoop-default.xml to provide finer-grained control over speculative execution, allowing independent configuration for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in Shared Cache Methods\nDESCRIPTION: Methods in Job class related to shared cache operations missing version documentation tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\naddArchiveToSharedCache(URI, Configuration)\naddFileToSharedCache(URI, Configuration)\naddFileToSharedCacheAndClasspath(URI, Configuration)\nclose()\ngetArchiveSharedCacheUploadPolicies(Configuration)\ngetFileSharedCacheUploadPolicies(Configuration)\nsetArchiveSharedCacheUploadPolicies(Configuration, Map)\nsetFileSharedCacheUploadPolicies(Configuration, Map)\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS-Site.xml for Replication Factor\nDESCRIPTION: XML configuration for hdfs-site.xml that sets the replication factor for HDFS blocks. This example sets the replication factor to 3, which is recommended for production clusters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n  <property>\n    <name>dfs.replication</name>\n    <value>3</value>\n  </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Blocks for Constants\nDESCRIPTION: Class fields in Job class lacking documentation blocks for wildcard configuration constants.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nDEFAULT_USE_WILDCARD_FOR_LIBJARS\nUSE_WILDCARD_FOR_LIBJARS\n```\n\n----------------------------------------\n\nTITLE: Variable Reference Pattern\nDESCRIPTION: Documents the syntax for variable references in configuration files. Variables use ${variable} syntax and are resolved first from configuration then Java system properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_30\n\nLANGUAGE: Properties\nCODE:\n```\n${variable}\n```\n\n----------------------------------------\n\nTITLE: DFS Command Syntax\nDESCRIPTION: Various DFS command examples including du, mv, and new count commands.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Bash\nCODE:\n```\ndfs du\ndfs mv\ndf -count\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in JobStatus Constructors\nDESCRIPTION: Constructor signatures for JobStatus class in both mapred and mapreduce packages lacking proper version documentation tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.mapred.JobStatus(JobID, float, float, float, float, int, JobPriority, String, String, String, String, boolean, String)\norg.apache.hadoop.mapred.JobStatus(JobID, float, float, float, float, int, JobPriority, String, String, String, String, String, boolean, String)\norg.apache.hadoop.mapreduce.JobStatus(JobID, float, float, float, float, State, JobPriority, String, String, String, String, String, boolean, String)\n```\n\n----------------------------------------\n\nTITLE: Cleaning up TestDFSIO Benchmark Files in Hadoop\nDESCRIPTION: This command runs the TestDFSIO benchmark with the clean option, which removes all the test files created during previous benchmark runs to free up space and prepare for fresh tests.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ hadoop jar hadoop-*test*.jar TestDFSIO -clean\n```\n\n----------------------------------------\n\nTITLE: Configuration Update - MapReduce Default Settings\nDESCRIPTION: Configuration changes to mapred-default.xml introducing mapred.reduce.slowstart.completed.maps parameter to control when reduce tasks start based on map task completion.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: XML\nCODE:\n```\nmapred.reduce.slowstart.completed.maps\n```\n\n----------------------------------------\n\nTITLE: Implementing Closeable for MRClientProtocolPBClientImpl\nDESCRIPTION: Changes MRClientProtocolPBClientImpl to be closeable, fixing failures in renewal of HistoryServer's delegation tokens.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-5117. Changed MRClientProtocolPBClientImpl to be closeable and thus fix failures in renewal of HistoryServer's delegations tokens. (Siddharth Seth via vinodkv)\n```\n\n----------------------------------------\n\nTITLE: NMClient Container Resource Methods Missing @since Tags\nDESCRIPTION: Methods in NMClient and NMClientAsync classes for updating container resources that lack proper version documentation tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.6/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.NMClient.updateContainerResource(Container)\norg.apache.hadoop.yarn.client.api.async.NMClientAsync.updateContainerResourceAsync(Container)\n```\n\n----------------------------------------\n\nTITLE: Capacity Scheduler Configuration Update\nDESCRIPTION: Configuration change to add maximum AM resource percentage parameter for the Capacity Scheduler\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/mapreduce/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nyarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation - ContainerInfo.nodeId Field\nDESCRIPTION: Undocumented field nodeId in ContainerInfo class of YARN server webapp DAO\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nnodeId\n```\n\n----------------------------------------\n\nTITLE: Running TestDFSIO Write Benchmark in Hadoop\nDESCRIPTION: This command runs the TestDFSIO benchmark in write mode, creating 10 files each with 1000MB size. It demonstrates how to execute a file write performance test in Hadoop using the hadoop jar command.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ hadoop jar hadoop-*test*.jar TestDFSIO -write -nrFiles 10 -fileSize 1000\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Reuse for Map-Reduce Tasks\nDESCRIPTION: Adds a new configuration parameter to control JVM reuse across Map-Reduce tasks. This can improve performance by reducing JVM startup overhead for multiple tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>mapred.job.reuse.jvm.num.tasks</name>\n  <value>-1</value>\n  <description>How many tasks to run per jvm. If set to -1, there is no limit.</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Setting Cache-Control Header for Dynamic Content\nDESCRIPTION: Adds a Cache-Control header with no-cache directive to all dynamic content responses, ensuring clients always fetch the latest version of dynamic resources.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nresponse.setHeader(\"Cache-Control\", \"no-cache\")\n```\n\n----------------------------------------\n\nTITLE: Updating Findbugs Heap Size in build.xml for MapReduce\nDESCRIPTION: Fixes a findbugs heap size problem in build.xml and adds a new property findbugs.heap.size.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: XML\nCODE:\n```\n<property name=\"findbugs.heap.size\" value=\"512m\"/>\n```\n\n----------------------------------------\n\nTITLE: Configuration Addition for Capacity Scheduler\nDESCRIPTION: Configuration change to add yarn.capacity-scheduler.maximum-am-resource-percent parameter for the CapacityScheduler to cap concurrently running applications per-queue & per-user.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nyarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Perl Script Update for JIRA Links\nDESCRIPTION: Updates to changes2html.pl script to generate links to HADOOP, HDFS, and MAPREDUCE JIRA issues\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Perl\nCODE:\n```\nchanges2html.pl\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation for AppInfo.getReservedMemoryMB Method\nDESCRIPTION: Getter method in AppInfo class that returns the amount of reserved memory in megabytes. Missing proper JavaDoc documentation block.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic int getReservedMemoryMB()\n```\n\n----------------------------------------\n\nTITLE: Accessing MapReduce Counters in Java Driver Code\nDESCRIPTION: This example shows how to access and read counter values from a completed MapReduce job in the driver program. It demonstrates retrieving counter groups, counters, and their values after job completion.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nCounters counters = job.getCounters();\nfor(CounterGroup group : counters) {\n  System.out.println(\"* Counter Group: \" + group.getDisplayName() + \" (\" + group.getName() + \")\");\n  System.out.println(\"  number of counters in this group: \" + group.size());\n  for(Counter counter : group) {\n    System.out.println(\"  - \" + counter.getDisplayName() + \": \" + counter.getName() + \": \" + counter.getValue());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating FSNamesystem Methods in Java\nDESCRIPTION: This snippet shows changes to FSNamesystem methods, including the removal of getNameNodeInfoPort() and replacement of getDFSNameNodeMachine() and getDFSNameNodePort() with getDFSNameNodeAddress().\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nFSNamesystem.getNameNodeInfoPort() // removed\nFSNamesystem.getDFSNameNodeMachine() // replaced\nFSNamesystem.getDFSNameNodePort() // replaced\nFSNamesystem.getDFSNameNodeAddress() // new method\n```\n\n----------------------------------------\n\nTITLE: Configuration Updates for Job Status Persistence in Hadoop\nDESCRIPTION: Configuration changes to hadoop-default.xml for persisting job statuses in HDFS, including settings for activation, retention period, and storage directory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_26\n\nLANGUAGE: XML\nCODE:\n```\nmapred.job.tracker.persist.jobstatus.active (default value of false)\nmapred.job.tracker.persist.jobstatus.hours (default value of 0)\nmapred.job.tracker.persist.jobstatus.dir (default value of /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Configuring Per-User Trash Directory in HDFS\nDESCRIPTION: Change to trash feature to use a per-user .Trash directory in the user's home directory instead of a global trash root.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n\".Trash\"\n```\n\n----------------------------------------\n\nTITLE: JVM Reuse Configuration\nDESCRIPTION: Configuration parameter added to hadoop-default.xml for controlling JVM reuse across Map-Reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Properties\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Secure Access Control\nDESCRIPTION: XML configuration for HDFS access control and permissions. These properties define how permissions are enforced and which users can access specific HDFS resources.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.5/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>dfs.permissions.enabled</name>\n  <value>true</value>\n</property>\n\n<property>\n  <name>dfs.block.access.token.enable</name>\n  <value>true</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Downloading Hadoop binary release using curl\nDESCRIPTION: Command to download the Hadoop binary release tarball using curl. This is the main package that contains the Hadoop distribution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O https://www.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Markdown Image and Header\nDESCRIPTION: Markdown syntax for displaying the Hadoop logo and project title\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/description.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n![hadoop-logo](hadoop-logo.jpg) Apache Hadoop\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Task Tracker in Hadoop XML\nDESCRIPTION: Changes to hadoop-default.xml configuration to support different numbers of mappers and reducers per TaskTracker. Adds new parameters for maximum map and reduce tasks while deprecating the old maximum tasks parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: xml\nCODE:\n```\nmapred.tasktracker.map.tasks.maximum (default value of 2)\nmapred.tasktracker.reduce.tasks.maximum (default value of 2)\nmapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Optimizing HDFS Edit Log Creation - Java\nDESCRIPTION: Code changes for generating realistic edit logs and optimizing OP_ADD operation during edits loading to improve HDFS performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-2718. Optimize OP_ADD in edits loading. (shv)\n\nHDFS-2886. CreateEditLogs should generate a realistic edit log. (shv)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Job Initialization Threads in Java\nDESCRIPTION: This change introduces multiple job initialization threads in Hadoop MapReduce. The number of threads is configurable via the 'mapred.jobinit.threads' parameter. This allows for parallel initialization of multiple jobs, potentially improving job startup times.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nmapred.jobinit.threads\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters for Default Hadoop Settings\nDESCRIPTION: Configuration parameters in hadoop-default.xml for setting maximum map and reduce tasks per TaskTracker. Replaces deprecated mapred.tasktracker.tasks.maximum parameter.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\nmapred.tasktracker.map.tasks.maximum (default value of 2)\nmapred.tasktracker.reduce.tasks.maximum (default value of 2)\nmapred.tasktracker.tasks.maximum (deprecated for 0.16.0)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Job Initialization Threads in Java\nDESCRIPTION: This change introduces multiple job initialization threads in Hadoop MapReduce. The number of threads is configurable via the 'mapred.jobinit.threads' parameter. This allows for parallel initialization of multiple jobs, potentially improving job startup times.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nmapred.jobinit.threads\n```\n\n----------------------------------------\n\nTITLE: Verifying Apache Hadoop Release with GPG in Bash\nDESCRIPTION: This snippet outlines the commands to verify an Apache Hadoop release using GPG. It includes steps for importing keys and verifying the signature of the release tarball.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/releases.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import KEYS\ngpg --verify hadoop-X.Y.Z-src.tar.gz.asc\n```\n\n----------------------------------------\n\nTITLE: Creating Git Summary Alias for Commit Log Overview\nDESCRIPTION: Defines a Git alias called 'summary' that provides a compact overview of the commit history. This command lists commit authors, message topics, and shows a summary of changes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: gitconfig\nCODE:\n```\ngit config --global alias.summary \"log --pretty=format:'* %s' --reverse\"\n```\n\n----------------------------------------\n\nTITLE: Removing deprecated configuration in Hadoop XML\nDESCRIPTION: Removes the deprecated 'mapred.child.heap.size' configuration property from the hadoop-default.xml file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: XML\nCODE:\n```\nremove mapred.child.heap.size\n```\n\n----------------------------------------\n\nTITLE: Defining YarnConfiguration Constants in Java\nDESCRIPTION: This snippet defines multiple constant fields for the YarnConfiguration class in Apache Hadoop's YARN module. These constants represent configuration keys and default values for various YARN settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n  public static final String FEDERATION_PREFIX = \"yarn.federation.\";\n  public static final String FEDERATION_REGISTRY_BASE_KEY = FEDERATION_PREFIX + \"registry.base-dir\";\n  public static final String FEDERATION_STATESTORE_CLIENT_CLASS = FEDERATION_PREFIX + \"state-store.class\";\n  public static final String FEDERATION_STATESTORE_HEARTBEAT_INTERVAL_SECS = FEDERATION_PREFIX + \"state-store.heartbeat-interval-secs\";\n  public static final String FEDERATION_STATESTORE_SQL_JDBC_CLASS = FEDERATION_PREFIX + \"state-store.sql.jdbc-class\";\n  public static final String FEDERATION_STATESTORE_SQL_MAXCONNECTIONS = FEDERATION_PREFIX + \"state-store.sql.max-connections\";\n  public static final String FEDERATION_STATESTORE_SQL_PASSWORD = FEDERATION_PREFIX + \"state-store.sql.password\";\n  public static final String FEDERATION_STATESTORE_SQL_URL = FEDERATION_PREFIX + \"state-store.sql.url\";\n  public static final String FEDERATION_STATESTORE_SQL_USERNAME = FEDERATION_PREFIX + \"state-store.sql.username\";\n  public static final String FEDERATION_STATESTORE_ZK_PARENT_PATH = FEDERATION_PREFIX + \"state-store.zk.parent-path\";\n  public static final String FEDERATION_STATESTORE_ZK_PREFIX = FEDERATION_PREFIX + \"state-store.zk.\";\n  // ... (more constants)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuration Addition for JVM Reuse in Hadoop\nDESCRIPTION: Added new configuration parameter mapred.job.reuse.jvm.num.tasks to hadoop-default.xml to enable JVM reuse across Map-Reduce Tasks for improved performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: JVM Reuse Configuration in Hadoop MapReduce\nDESCRIPTION: Configuration property added to hadoop-default.xml for controlling JVM reuse across MapReduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Properties\nCODE:\n```\nmapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Implementing ShortWritable in Java\nDESCRIPTION: Adds a new ShortWritable class to the Hadoop Common project.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7493. Add ShortWritable.\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Configuration\nDESCRIPTION: YAML front matter defining the page title and publication date for the Hadoop 3.1.2 release announcement\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/3.1.2.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 3.1.2 available\ndate: 2019-02-06\n---\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag in AMRMClient waitFor Method\nDESCRIPTION: The waitFor method in the AMRMClient class is missing the @since tag. This method takes a Supplier and two integer parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.4/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter Configuration for Release Post\nDESCRIPTION: YAML frontmatter configuration block defining the title, date, and linked status for the Hadoop 3.4.1 release announcement page.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/3.4.1.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Release 3.4.1 available\ndate: 2024-10-18\nlinked: true\n---\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag for getJNStartedTimeInMillis() Method in JournalNodeMXBean\nDESCRIPTION: The getJNStartedTimeInMillis() method in the JournalNodeMXBean interface is missing the @since Javadoc tag. This method likely returns the start time of the Journal Node in milliseconds.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean Method getJNStartedTimeInMillis()\n```\n\n----------------------------------------\n\nTITLE: Verifying Delegation Tokens in Hadoop Security (Java)\nDESCRIPTION: Adds a verifyToken method to AbstractDelegationTokenSecretManager for verifying delegation tokens.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nAbstractDelegationTokenSecretManager.verifyToken()\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration Properties for Task Control\nDESCRIPTION: Configuration properties defining maximum number of map and reduce tasks per TaskTracker, replacing the deprecated tasks.maximum property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: properties\nCODE:\n```\nmapred.tasktracker.map.tasks.maximum\nmapred.tasktracker.reduce.tasks.maximum\nmapred.tasktracker.tasks.maximum\n```\n\n----------------------------------------\n\nTITLE: Code fix - HADOOP-4280 Time Conversion between C and Java\nDESCRIPTION: Fixes issues with time conversion between seconds in C and milliseconds in Java for file access times.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_10\n\n\n\n----------------------------------------\n\nTITLE: Defining YARN Configuration Constants in Java\nDESCRIPTION: This snippet shows multiple field declarations for default configuration values in the YarnConfiguration class. These constants are used to set default behaviors for various YARN components and features.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n    public static final String DEFAULT_FEDERATION_STATESTORE_CONN_IDLE_TIMEOUT_TIME;\n    public static final String DEFAULT_FEDERATION_STATESTORE_CONN_MAX_LIFE_TIME;\n    public static final String DEFAULT_FEDERATION_STATESTORE_CONNECTION_TIMEOUT_TIME;\n    public static final String DEFAULT_FEDERATION_STATESTORE_HEARTBEAT_INITIAL_DELAY;\n    public static final String DEFAULT_FEDERATION_STATESTORE_HEARTBEAT_INTERVAL_SECS;\n    public static final String DEFAULT_FEDERATION_STATESTORE_MAX_APPLICATIONS;\n    public static final String DEFAULT_FEDERATION_STATESTORE_POOL_NAME;\n    public static final String DEFAULT_FEDERATION_STATESTORE_SQL_JDBC_CLASS;\n    public static final String DEFAULT_FEDERATION_STATESTORE_SQL_MAXCONNECTIONS;\n    public static final String DEFAULT_FEDERATION_STATESTORE_SQL_MINIMUMIDLE;\n    public static final String DEFAULT_FEDERATION_STATESTORE_ZK_PARENT_PATH;\n    public static final String DEFAULT_FEDERATION_YARN_CLIENT_FAILOVER_RANDOM_ORDER;\n    public static final String DEFAULT_FILTER_INVALID_XML_CHARS;\n    public static final String DEFAULT_FLOW_VERSION;\n    public static final String DEFAULT_FS_NODE_LABELS_STORE_IMPL_CLASS;\n    public static final String DEFAULT_FS_STORE_FILE_REPLICATION;\n    // ... many more field declarations ...\n}\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in AppAttemptInfo DAO Class\nDESCRIPTION: Methods and fields requiring documentation in AppAttemptInfo class for managing YARN application attempt timing information\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.4/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class AppAttemptInfo {\n    private long startedTime;\n    private long finishedTime;\n\n    public long getStartedTime()\n    public long getFinishedTime()\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics2 Record Filter in Hadoop XML\nDESCRIPTION: XML configuration snippet for setting up the Metrics2 record filter to check both name and tags. This improves the filtering capabilities of the metrics system.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>net.topology.table.file.name</name>\n  <value>*</value>\n  <description>File name for net topology table</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop yarn-site.xml\nDESCRIPTION: XML configuration for yarn-site.xml that sets up YARN resource manager parameters for resource allocation and scheduling.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.env-whitelist</name>\n        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n    </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag for getStorageInfos() Method in JournalNodeMXBean\nDESCRIPTION: The getStorageInfos() method in the JournalNodeMXBean interface is missing the @since Javadoc tag. This method probably retrieves storage information related to the Journal Node.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean Method getStorageInfos()\n```\n\n----------------------------------------\n\nTITLE: Managing Hadoop Services\nDESCRIPTION: Scripts for managing Hadoop services including job history server and secondary namenode initialization\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ninit.d\n```\n\n----------------------------------------\n\nTITLE: Adding Avro Dependencies to MapReduce Ivy Configuration\nDESCRIPTION: Updates the Ivy configuration to include Avro and its dependencies for Hadoop common.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/mapreduce/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: XML\nCODE:\n```\n<dependency org=\"org.apache.avro\" name=\"avro\" rev=\"${avro.version}\" conf=\"common->default\"/>\n```\n\n----------------------------------------\n\nTITLE: Documentation Issues in Hadoop File System Classes\nDESCRIPTION: List of classes and methods with missing @SINCE tags and documentation blocks in org.apache.hadoop.fs package\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nMISSING @SINCE TAG: org.apache.hadoop.fs.Options.OpenFileOptions Class\nMISSING @SINCE TAG: org.apache.hadoop.fs.FSDataInputStream Method getWrappedStream()\nNO DOC BLOCK: org.apache.hadoop.fs.FSDataInputStream Method maxReadSizeForVectorReads()\nMISSING @SINCE TAG: org.apache.hadoop.fs.PositionedReadable Method maxReadSizeForVectorReads()\nNO DOC BLOCK: org.apache.hadoop.fs.FSDataInputStream Method minSeekForVectorReads()\nMISSING @SINCE TAG: org.apache.hadoop.fs.PositionedReadable Method minSeekForVectorReads()\nMISSING @SINCE TAG: org.apache.hadoop.fs.FSBuilder Method must(java.lang.String, long)\nMISSING @SINCE TAG: org.apache.hadoop.fs.FSBuilder Method opt(java.lang.String, long)\n```\n\n----------------------------------------\n\nTITLE: YARN Configuration Field Constants - Timeline Service Settings\nDESCRIPTION: Default configuration values for YARN Timeline Service including webapp addresses, schema settings, and storage parameters\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\npublic static final String DEFAULT_TIMELINE_SERVICE_VERSION;\npublic static final String DEFAULT_TIMELINE_SERVICE_READER_WEBAPP_ADDRESS;\npublic static final String DEFAULT_TIMELINE_SERVICE_READER_WEBAPP_HTTPS_ADDRESS;\npublic static final String DEFAULT_TIMELINE_SERVICE_WRITER_CLASS;\n```\n\n----------------------------------------\n\nTITLE: Bash Environment Requirement\nDESCRIPTION: Environment specification requiring bash version 2 or later and tail command for script execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\nrequire bash (v2 or later) and tail\n```\n\n----------------------------------------\n\nTITLE: Defining Eclipse Classpath Variables for Hadoop Development\nDESCRIPTION: This XML snippet demonstrates how to configure Eclipse classpath variables for Hadoop development. It includes the M2_REPO variable pointing to the local Maven repository which is essential for resolving dependencies in an Eclipse-based Hadoop development environment.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.3/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<classpath>\n  <classpathentry kind=\"var\" path=\"M2_REPO/junit/junit/4.11/junit-4.11.jar\"/>\n  ...\n</classpath>\n```\n\n----------------------------------------\n\nTITLE: Configuration Addition for Out-of-Band Heartbeat in MapReduce\nDESCRIPTION: Configuration property addition that allows TaskTracker to send optional out-of-band heartbeats when tasks complete, improving job latency tracking.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: config\nCODE:\n```\nadd mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Addressing Flaky TestUserGroupInformation\nDESCRIPTION: Fixes flaky behavior in the testLogin method of TestUserGroupInformation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nTestUserGroupInformation#testLogin is flaky\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Fields in YarnConfiguration Class\nDESCRIPTION: List of class fields in YarnConfiguration that are missing proper documentation blocks and/or @since version tags. These represent configuration parameters for various YARN features including federation, container management, docker settings, and system defaults.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.conf.YarnConfiguration\n// Fields with missing @since tags:\nField APP_FINAL_VALUE_RETENTION_THRESHOLD\nField CURATOR_LEADER_ELECTOR\nField DEFAULT_APP_FINAL_VALUE_RETENTION_THRESHOLD\nField DEFAULT_FLOW_VERSION\nField DEFAULT_HDFS_LOCATION_FLOW_RUN_COPROCESSOR_JAR\nField DEFAULT_LINUX_CONTAINER_RUNTIME_ALLOWED_RUNTIMES\nField DEFAULT_NM_DISK_RESOURCE_ENABLED\nField DEFAULT_NM_DOCKER_ALLOW_DELAYED_REMOVAL\nField DEFAULT_NM_DOCKER_ALLOW_HOST_PID_NAMESPACE\nField DEFAULT_NM_DOCKER_ALLOW_PRIVILEGED_CONTAINERS\nField DEFAULT_NM_DOCKER_ALLOWED_CONTAINER_NETWORKS\nField DEFAULT_NM_DOCKER_ALLOWED_CONTAINER_RUNTIMES\nField DEFAULT_NM_DOCKER_CONTAINER_CAPABILITIES\nField DEFAULT_NM_DOCKER_DEFAULT_CONTAINER_NETWORK\nField DEFAULT_NM_DOCKER_ENABLE_USER_REMAPPING\nField DEFAULT_NM_DOCKER_PRIVILEGED_CONTAINERS_ACL\nField DEFAULT_NM_DOCKER_STOP_GRACE_PERIOD\n```\n\n----------------------------------------\n\nTITLE: Defining Default YARN Configuration Constants in Java\nDESCRIPTION: This snippet shows the declaration of multiple constant fields in the YarnConfiguration class. These constants define default values for various YARN configuration parameters, including AM scheduling, federation settings, container management, and resource allocation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n    public static final boolean DEFAULT_AM_SCHEDULING_NODE_BLACKLISTING_ENABLED;\n    public static final String DEFAULT_AMRM_PROXY_ADDRESS;\n    public static final int DEFAULT_AMRM_PROXY_CLIENT_THREAD_COUNT;\n    public static final boolean DEFAULT_AMRM_PROXY_ENABLED;\n    public static final boolean DEFAULT_AMRM_PROXY_HA_ENABLED;\n    public static final String DEFAULT_AMRM_PROXY_INTERCEPTOR_CLASS_PIPELINE;\n    public static final int DEFAULT_AMRM_PROXY_PORT;\n    public static final int DEFAULT_APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC;\n    public static final int DEFAULT_APP_FINAL_VALUE_RETENTION_THRESHOLD;\n    public static final long DEFAULT_ATS_APP_COLLECTOR_LINGER_PERIOD_IN_MS;\n    public static final int DEFAULT_CLUSTER_LEVEL_APPLICATION_PRIORITY;\n    public static final String DEFAULT_CONFIGURATION_STORE;\n    public static final boolean DEFAULT_CURATOR_LEADER_ELECTOR_ENABLED;\n    public static final String DEFAULT_DISK_VALIDATOR;\n    public static final boolean DEFAULT_DISPLAY_APPS_FOR_LOGGED_IN_USER;\n    public static final boolean DEFAULT_DIST_SCHEDULING_ENABLED;\n    public static final boolean DEFAULT_ENABLE_REST_APP_SUBMISSIONS;\n    // ... more constants\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Property in JAAS Configuration for Hadoop\nDESCRIPTION: Adds the ability to enable the 'debug' property in the JAAS (Java Authentication and Authorization Service) configuration for improved debugging.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration.setConfiguration(new Configuration() {\n  // Implementation to enable 'debug' property\n});\n```\n\n----------------------------------------\n\nTITLE: Getting Reserved CPU VCores in AppInfo Class\nDESCRIPTION: This method returns the number of reserved CPU virtual cores for an application. It is part of the AppInfo class in the YARN server webapp DAO.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic int getReservedCpuVcores()\n```\n\n----------------------------------------\n\nTITLE: Node ID Field in ContainerInfo Class\nDESCRIPTION: This field stores the Node ID associated with a container. It is a member of the ContainerInfo class in the YARN server webapp DAO.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nprivate String nodeId;\n```\n\n----------------------------------------\n\nTITLE: Fixing Typo in IPC Client Response Check\nDESCRIPTION: Corrects a typo in the checkResponse method of the o.a.h.ipc.Client class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nFix typo in o.a.h.ipc.Client#checkResponse.\n```\n\n----------------------------------------\n\nTITLE: Defining YARN Configuration Constants in Java\nDESCRIPTION: This snippet shows the declaration of multiple constant fields in the YarnConfiguration class. These constants represent default values, property names, and configuration options for various YARN components and features.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n  public static final String DEFAULT_ROUTER_WEBAPP_DEFAULT_INTERCEPTOR_CLASS;\n  public static final String DEFAULT_ROUTER_WEBAPP_HTTPS_ADDRESS;\n  public static final int DEFAULT_ROUTER_WEBAPP_HTTPS_PORT;\n  public static final String DEFAULT_ROUTER_WEBAPP_INTERCEPTOR_CLASS;\n  public static final boolean DEFAULT_ROUTER_WEBAPP_PARTIAL_RESULTS_ENABLED;\n  public static final int DEFAULT_ROUTER_WEBAPP_PORT;\n  public static final boolean DEFAULT_SYSTEM_METRICS_PUBLISHER_ENABLED;\n  public static final String DEFAULT_TIMELINE_SERVICE_BIND_HOST;\n  public static final int DEFAULT_TIMELINE_SERVICE_CLIENT_INTERNAL_ATTEMPT_DIR_CACHE_SIZE;\n  public static final String DEFAULT_TIMELINE_SERVICE_COLLECTOR_WEBAPP_ADDRESS;\n  public static final String DEFAULT_TIMELINE_SERVICE_COLLECTOR_WEBAPP_HTTPS_ADDRESS;\n  public static final String DEFAULT_TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETRY_POLICY_SPEC;\n  public static final String DEFAULT_TIMELINE_SERVICE_HBASE_SCHEMA_PREFIX;\n  public static final long DEFAULT_TIMELINE_SERVICE_LEVELDB_CACHE_READ_CACHE_SIZE;\n  public static final int DEFAULT_TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES;\n  public static final int DEFAULT_TIMELINE_SERVICE_LEVELDB_WRITE_BATCH_SIZE;\n  public static final int DEFAULT_TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE;\n  public static final String DEFAULT_TIMELINE_SERVICE_READ_ALLOWED_USERS;\n  public static final boolean DEFAULT_TIMELINE_SERVICE_READ_AUTH_ENABLED;\n  public static final String DEFAULT_TIMELINE_SERVICE_READER_CLASS;\n  public static final String DEFAULT_TIMELINE_SERVICE_READER_WEBAPP_ADDRESS;\n  public static final String DEFAULT_TIMELINE_SERVICE_READER_WEBAPP_HTTPS_ADDRESS;\n  public static final long DEFAULT_TIMELINE_SERVICE_ROLLING_PERIOD;\n  public static final String DEFAULT_TIMELINE_SERVICE_SCHEMA_CREATOR_CLASS;\n  public static final float DEFAULT_TIMELINE_SERVICE_VERSION;\n  public static final String DEFAULT_TIMELINE_SERVICE_WRITER_CLASS;\n  public static final int DEFAULT_TIMELINE_SERVICE_WRITER_FLUSH_INTERVAL_SECONDS;\n  public static final long DEFAULT_TIMELINE_V2_CLIENT_DRAIN_TIME_MILLIS;\n  public static final boolean DEFAULT_YARN_CLIENT_LOAD_RESOURCETYPES_FROM_SERVER;\n  public static final String DEFAULT_YARN_REGISTRY_CLASS;\n  public static final boolean DEFAULT_YARN_RESERVATION_ACL_ENABLE;\n  public static final boolean DEFAULT_YARN_WEBAPP_UI2_ENABLE;\n  public static final String DEFAULT_YARN_WORKFLOW_ID_TAG_PREFIX;\n  public static final int DEFAULT_ZK_APPID_NODE_SPLIT_INDEX;\n  public static final int DEFAULT_ZK_DELEGATION_TOKEN_NODE_SPLIT_INDEX;\n  public static final String DELEGATED_CENTALIZED_NODELABEL_CONFIGURATION_TYPE;\n  public static final String DISK_VALIDATOR;\n  public static final String DISPLAY_APPS_FOR_LOGGED_IN_USER;\n  public static final boolean DIST_SCHEDULING_ENABLED;\n  public static final String DISTRIBUTED_NODELABEL_CONFIGURATION_TYPE;\n  public static final String DOCKER_CONTAINER_RUNTIME_PREFIX;\n  public static final String DR_CONFIGURATION_FILE;\n  public static final boolean ENABLE_REST_APP_SUBMISSIONS;\n  public static final String EXCLUSIVE_ENFORCED_PARTITIONS;\n  public static final String EXCLUSIVE_ENFORCED_PARTITIONS_SUFFIX;\n  public static final String EXPIRED_ENTITY;\n  public static final long FEDERATION_AMRMPROXY_HB_MAX_WAIT_MS;\n  public static final long FEDERATION_AMRMPROXY_SUBCLUSTER_TIMEOUT;\n  public static final long FEDERATION_CACHE_TIME_TO_LIVE_SECS;\n  public static final String FEDERATION_CLUSTER_RESOLVER_CLASS;\n  public static final boolean FEDERATION_ENABLED;\n  public static final boolean FEDERATION_FAILOVER_ENABLED;\n  public static final boolean FEDERATION_FLUSH_CACHE_FOR_RM_ADDR;\n  public static final String FEDERATION_MACHINE_LIST;\n  public static final String FEDERATION_POLICY_MANAGER;\n  public static final String FEDERATION_POLICY_MANAGER_PARAMS;\n  public static final String FEDERATION_PREFIX;\n  public static final String FEDERATION_REGISTRY_BASE_KEY;\n  public static final String FEDERATION_STATESTORE_CLIENT_CLASS;\n  public static final long FEDERATION_STATESTORE_HEARTBEAT_INTERVAL_SECS;\n  public static final String FEDERATION_STATESTORE_SQL_JDBC_CLASS;\n  public static final int FEDERATION_STATESTORE_SQL_MAXCONNECTIONS;\n  public static final String FEDERATION_STATESTORE_SQL_PASSWORD;\n  public static final String FEDERATION_STATESTORE_SQL_URL;\n  public static final String FEDERATION_STATESTORE_SQL_USERNAME;\n  public static final String FEDERATION_STATESTORE_ZK_PARENT_PATH;\n  public static final String FEDERATION_STATESTORE_ZK_PREFIX;\n  public static final String FILE_CONFIGURATION_STORE;\n  public static final String FLOW_RUN_COPROCESSOR_JAR_HDFS_LOCATION;\n  public static final String FS_NODE_LABELS_STORE_IMPL_CLASS;\n  public static final int GLOBAL_RM_AM_MAX_ATTEMPTS;\n  public static final int KILLED_BY_CONTAINER_SCHEDULER;\n  public static final String LEVELDB_CONFIGURATION_STORE;\n  public static final String LINUX_CONTAINER_RUNTIME_ALLOWED_RUNTIMES;\n  public static final String LINUX_CONTAINER_RUNTIME_PREFIX;\n  public static final String LOG_AGGREGATION_FILE_CONTROLLER_FMT;\n  public static final String LOG_AGGREGATION_FILE_FORMATS;\n  public static final String LOG_AGGREGATION_REMOTE_APP_LOG_DIR_FMT;\n  public static final String LOG_AGGREGATION_REMOTE_APP_LOG_DIR_SUFFIX_FMT;\n  public static final long LOG_AGGREGATION_STATUS_TIME_OUT_MS;\n  public static final int MAX_CLUSTER_LEVEL_APPLICATION_PRIORITY;\n  public static final String MEMORY_CONFIGURATION_STORE;\n  public static final int MEMORY_INDEX;\n  public static final String NM_AUX_SERVICE_REMOTE_CLASSPATH;\n  public static final String NM_AUX_SERVICES_CLASSPATH;\n  public static final String NM_AUX_SERVICES_SYSTEM_CLASSES;\n  public static final String NM_COLLECTOR_SERVICE_ADDRESS;\n  public static final int NM_COLLECTOR_SERVICE_THREAD_COUNT;\n  public static final String NM_CONFIGURATION_FILES;\n  public static final int NM_CONTAINER_DIAGNOSTICS_MAXIMUM_SIZE;\n  public static final String NM_CONTAINER_LOCALIZER_JAVA_OPTS_DEFAULT;\n  public static final String NM_CONTAINER_LOCALIZER_JAVA_OPTS_KEY;\n  public static final String NM_CONTAINER_LOCALIZER_LOG_LEVEL;\n  public static final String NM_CONTAINER_LOCALIZER_LOG_LEVEL_DEFAULT;\n  public static final boolean NM_CONTAINER_MONITOR_ENABLED;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Adding MapReduce Configuration Property in Java\nDESCRIPTION: This snippet shows the addition of a new configuration property for MapReduce to control out-of-band heartbeats from TaskTrackers. It's part of an optimization to improve job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Using WritableComparator for Custom Key Types in Java\nDESCRIPTION: WritableComparator is modified to only create instances of the keytype if the type does not define a WritableComparator. This change is part of HADOOP-3665.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\npublic class CustomKeyComparator extends WritableComparator {\n  protected CustomKeyComparator() {\n    super(CustomKey.class, true);\n  }\n\n  @Override\n  public int compare(WritableComparable a, WritableComparable b) {\n    CustomKey ka = (CustomKey)a;\n    CustomKey kb = (CustomKey)b;\n    // Implement comparison logic\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in Resource Management Classes\nDESCRIPTION: Resource allocation and management related method calls that lack proper @since tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.api.protocolrecords.AllocateRequest.newInstance(int, float, java.util.List, java.util.List, java.util.List, org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest)\norg.apache.hadoop.yarn.api.protocolrecords.AllocateRequest.newInstance(int, float, java.util.List, java.util.List, org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest, java.lang.String)\n```\n\n----------------------------------------\n\nTITLE: Optimizing RPC Request Processing\nDESCRIPTION: Eliminates unnecessary processing of RPC requests when the client connection has been dropped.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nno need to process RPC request if the client connection has been dropped\n```\n\n----------------------------------------\n\nTITLE: Java Package Path Update\nDESCRIPTION: Code reference showing the movement of SnapshotException and SnapshotAccessControlException classes to the o.a.h.hdfs.protocol package.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hdfs/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\no.a.h.hdfs.protocol\n```\n\n----------------------------------------\n\nTITLE: YARN API and Configuration Documentation Issues\nDESCRIPTION: Various API methods and configuration fields missing proper documentation tags and blocks in the Hadoop YARN codebase. Includes protocol records, client interfaces, and configuration parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nMISSING @SINCE TAG: org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest Method setUpdateRequests(java.util.List)\\nMISSING @SINCE TAG: org.apache.hadoop.yarn.api.ApplicationClientProtocol Method signalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest)\\nNO DOC BLOCK: org.apache.hadoop.yarn.api.ContainerManagementProtocol Method signalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest)\n```\n\n----------------------------------------\n\nTITLE: Hadoop DFS File Deletion Commands in Shell\nDESCRIPTION: Commands for deleting files in Hadoop Distributed File System. The 'dfs -rm' command operates on a single file, while 'dfs -rmr' operates recursively on directories. This change was part of HADOOP-431.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\ndfs -rm\n```\n\nLANGUAGE: shell\nCODE:\n```\ndfs -rmr\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration for CapacityScheduler in Java\nDESCRIPTION: Adds a new configuration property for the CapacityScheduler to cap the percentage of cluster resources that can be used to run application masters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nadd yarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation YarnConfiguration Fields\nDESCRIPTION: Lists of class fields in YarnConfiguration that are missing proper documentation blocks or @since tags. These fields represent various configuration constants for YARN services, federation settings, timeline services, and runtime parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.1/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.conf.YarnConfiguration\n\n// Example fields missing documentation:\nDEFAULT_RM_PLACEMENT_CONSTRAINTS_RETRY_ATTEMPTS\nDEFAULT_RM_PLACEMENT_CONSTRAINTS_SCHEDULER_POOL_SIZE\nDEFAULT_RM_PUBLISH_CONTAINER_EVENTS_ENABLED\nDEFAULT_RM_RESERVATION_SYSTEM_MAX_PERIODICITY\n// ... additional fields omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Java Method Reference\nDESCRIPTION: Reference to locale-specific string operations that caused issues in SecurityUtil class\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\ntoUpperCase(Locale.getDefault())\nlowerCase(Locale.getDefault())\n```\n\n----------------------------------------\n\nTITLE: Configuration for Job Status Persistence in HDFS\nDESCRIPTION: Changes to hadoop-default.xml to enable persistence of completed job statuses in HDFS, allowing the JobClient to query information about decommissioned jobs and across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_33\n\nLANGUAGE: xml\nCODE:\n```\nadd mapred.job.tracker.persist.jobstatus.active (default value of false)\nadd mapred.job.tracker.persist.jobstatus.hours (default value of 0)\nadd mapred.job.tracker.persist.jobstatus.dir (default value of\n                                                /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in Client Methods\nDESCRIPTION: Various client methods missing @since tags in their documentation, including operations for resource management, container handling, and application management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.2/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\ncreateAMRMClientAsync(int, AbstractCallbackHandler)\ncreateReservation()\nfailApplicationAttempt(ApplicationAttemptId)\ngetApplications(Set, Set, Set, EnumSet)\nincreaseContainerResource(Container)\nkillApplication(ApplicationId, String)\nlistReservations(ReservationListRequest)\nrequestContainerResourceChange(Container, Resource)\nsignalToContainer(ContainerId, SignalContainerCommand)\nupdateApplicationPriority(ApplicationId, Priority)\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Frontmatter for Ozone Release Announcement\nDESCRIPTION: This snippet defines the frontmatter for a Markdown file announcing the Ozone 0.4.1-alpha release. It specifies the title, date, and sets the 'linked' property to true.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/news/2019-10-13-ozone-0.4.1-alpha.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Ozone 0.4.1-alpha is released\ndate: 2019-10-13\nlinked: true\n---\n```\n\n----------------------------------------\n\nTITLE: MapReduce Job History Configuration\nDESCRIPTION: Principal configuration property for JobHistory server authentication\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nmapreduce.jobhistory.principal\n```\n\n----------------------------------------\n\nTITLE: Adding MapReduce JVM Reuse Configuration in hadoop-default.xml\nDESCRIPTION: Adds a new configuration parameter 'mapred.job.reuse.jvm.num.tasks' to hadoop-default.xml to enable JVM reuse across Map-Reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: LZ4 Compression Algorithm Copyright Notice\nDESCRIPTION: Copyright and license notice for LZ4 fast compression algorithm implementation including terms of redistribution and warranty disclaimers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/LICENSE.txt#2025-04-08_snippet_2\n\nLANGUAGE: C\nCODE:\n```\n/*\n   LZ4 - Fast LZ compression algorithm\n   Header File\n   Copyright (C) 2011-2013, Yann Collet.\n   BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)\n\n   Redistribution and use in source and binary forms, with or without\n   modification, are permitted provided that the following conditions are\n   met:\n\n       * Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n       * Redistributions in binary form must reproduce the above\n   copyright notice, this list of conditions and the following disclaimer\n   in the documentation and/or other materials provided with the\n   distribution.\n\n   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n   \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n   You can contact the author at :\n   - LZ4 homepage : http://fastcompression.blogspot.com/p/lz4.html\n   - LZ4 source repository : http://code.google.com/p/lz4/\n*/\n```\n\n----------------------------------------\n\nTITLE: XML Configuration Example in Hadoop MapReduce Tutorial\nDESCRIPTION: Reference to mapred_tutorial.xml containing WordCount examples that required minor error corrections. Part of tutorial documentation fixes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.0/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: xml\nCODE:\n```\nmapred_tutorial.xml\n```\n\n----------------------------------------\n\nTITLE: Class Reference Deprecation\nDESCRIPTION: Reference showing deprecated class mappings for Shell command utilities.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.fs.ShellCommand for org.apache.hadoop.util.Shell\norg.apache.hadoop.util.ShellUtil for \n  org.apache.hadoop.util.Shell.ShellCommandExecutor\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Manager Queue Properties in Hadoop\nDESCRIPTION: Naming convention for specifying queue properties in the Hadoop Resource Manager configuration. Properties follow a specific format that includes the queue name and property name.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: config\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Downloading Hadoop binary release PGP signature using wget\nDESCRIPTION: Command to download the PGP signature file (.asc) for a Hadoop binary release using wget. This signature file is used to verify the authenticity of the downloaded binary.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://www.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz.asc\n```\n\n----------------------------------------\n\nTITLE: Missing @Since Tags in YARN Client Methods\nDESCRIPTION: Lists Java methods with missing @Since tags in the YARN client API. These methods are part of the AMRMClient and AMRMClientAsync classes and provide waitFor functionality with various parameter combinations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.1/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient.waitFor(java.util.function.Supplier, int, int)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Accessing TaskTracker Logs via HTTP\nDESCRIPTION: Shows the URL format change for accessing task logs through the TaskTracker's web interface. The new format accepts start and end parameters which can be positive (from start) or negative (from end).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_13\n\nLANGUAGE: http\nCODE:\n```\nhttp://<tasktracker>/tasklog.jsp -> http://<tasktracker>tasklog\n```\n\n----------------------------------------\n\nTITLE: Retrieving Home Directory in Hadoop FileSystem\nDESCRIPTION: New FileSystem method to get the user's home directory as a fully-qualified path. This change affects how working directories are handled.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\nFileSystem#getHomeDirectory()\n```\n\n----------------------------------------\n\nTITLE: Implementing User Counters in Hadoop Streaming\nDESCRIPTION: Support for user-defined counters is added to Hadoop Streaming. This allows custom metrics to be tracked and reported in streaming jobs. Part of HADOOP-1328.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n// In streaming mapper/reducer script\nprint \"reporter:counter:CounterGroup,CounterName,1\"\n```\n\n----------------------------------------\n\nTITLE: Optimizing UGI group lookups (Java)\nDESCRIPTION: Optimizes User Group Information (UGI) group lookups to improve performance of user authentication and group resolution in Hadoop security.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// Code not provided in change log\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation - AppInfo.getReservedMemoryMB()\nDESCRIPTION: Undocumented getter method for retrieving reserved memory in megabytes in AppInfo class of YARN server webapp DAO\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\ngetReservedMemoryMB()\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Child JVM Parameters in Java\nDESCRIPTION: Adds new configuration options for setting JVM parameters, environment variables, and ulimit separately for map and reduce tasks. Deprecates some older, combined configuration options.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes:\n  add mapred.map.child.java.opts\n  add mapred.reduce.child.java.opts\n  add mapred.map.child.env\n  add mapred.reduce.child.ulimit\n  add mapred.map.child.env\n  add mapred.reduce.child.ulimit\n  deprecated mapred.child.java.opts\n  deprecated mapred.child.env\n  deprecated mapred.child.ulimit\n```\n\n----------------------------------------\n\nTITLE: Removing Duplicate Surefire Plugin Config in Hadoop Common (Java)\nDESCRIPTION: Eliminates duplicate surefire plugin configuration in the hadoop-common module.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-9242. Duplicate surefire plugin config in hadoop-common. (Andrey Klochkov via suresh)\n```\n\n----------------------------------------\n\nTITLE: MapReduce API Method Deprecations\nDESCRIPTION: Removal of deprecated methods from JobConf class including input/output path handling and compression settings. This represents an API cleanup to remove legacy methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\naddInputPath(Path)\ngetInputPaths()\ngetMapOutputCompressionType()\ngetOutputPath()\ngetSystemDir()\nsetInputPath(Path)\nsetMapOutputCompressionType(CompressionType style)\nsetOutputPath(Path)\n```\n\n----------------------------------------\n\nTITLE: JobClient Static Configuration Method Removals\nDESCRIPTION: Removal of static configuration methods and related classes from JobClient to improve design.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nJobClient::setCommandLineConfig is removed\nJobClient::getCommandLineConfig is removed\nJobShell, TestJobShell classes are removed\n```\n\n----------------------------------------\n\nTITLE: Undocumented ContainerInfo Members\nDESCRIPTION: Undocumented members in ContainerInfo class for container node identification.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\ngetNodeId()\nField nodeId\n```\n\n----------------------------------------\n\nTITLE: Configuration Property Changes for RPC Server\nDESCRIPTION: Lists deprecated and new configuration properties for RPC server addressing and ports as part of HADOOP-2185.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_30\n\nLANGUAGE: config\nCODE:\n```\nDeprecated:\ndfs.info.bindAddress\ndfs.info.port\ndfs.datanode.bindAddress\ndfs.datanode.port\ndfs.datanode.info.bindAdress\ndfs.datanode.info.port\ndfs.secondary.info.bindAddress\ndfs.secondary.info.port\nmapred.job.tracker.info.bindAddress\nmapred.job.tracker.info.port\nmapred.task.tracker.report.bindAddress\ntasktracker.http.bindAddress\ntasktracker.http.port\n\nNew:\ndfs.secondary.http.address\ndfs.datanode.address\ndfs.datanode.http.address\ndfs.http.address\nmapred.job.tracker.http.address\nmapred.task.tracker.report.address\nmapred.task.tracker.http.address\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation - AppInfo.getReservedCpuVcores()\nDESCRIPTION: Undocumented getter method for retrieving reserved CPU vcores in AppInfo class of YARN server webapp DAO\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\ngetReservedCpuVcores()\n```\n\n----------------------------------------\n\nTITLE: Adding New MapReduce API in Java\nDESCRIPTION: This snippet introduces the new MapReduce API in org.apache.hadoop.mapreduce package, deprecating the old API in org.apache.hadoop.mapred. It highlights key differences including the use of Context objects, changes to Mapper and Reducer, and modifications to output file naming conventions.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n// New API package\norg.apache.hadoop.mapreduce\n\n// Old API package (deprecated)\norg.apache.hadoop.mapred\n\n// Example of new Mapper with run method\npublic class NewMapper extends Mapper<K1, V1, K2, V2> {\n  public void run(Context context) throws IOException, InterruptedException {\n    // Control loop for the task\n  }\n}\n\n// New output file naming convention\npart-r-00000 // for reduce 0 output\npart-m-00000 // for map 0 output\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce Slow-Start for Reduce Tasks in Java\nDESCRIPTION: This configuration change adds a new parameter 'mapred.reduce.slowstart.completed.maps' to mapred-default.xml. It introduces a slow-start for scheduling reduce tasks to ensure that reduces aren't started until a sufficient number of map tasks are completed, preventing reduce tasks of jobs with unscheduled maps from overwhelming the cluster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration changes to mapred-default.xml:\n  add mapred.reduce.slowstart.completed.maps\n```\n\n----------------------------------------\n\nTITLE: Configuration Method Deprecation Notice\nDESCRIPTION: List of deprecated Configuration class methods that were removed in this release, including object manipulation and iterator methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\ngetObject(String name)\nsetObject(String name, Object value)\nget(String name, Object defaultValue)\nset(String name, Object value)\nIterator entries()\n```\n\n----------------------------------------\n\nTITLE: Configuration Property Fix - Hadoop Default Settings\nDESCRIPTION: Configuration fix removing unsupported memory-related configuration variables from Hadoop settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_21\n\nLANGUAGE: xml\nCODE:\n```\nmapred.tasktracker.tasks.maxmemory\nmapred.task.max.memory\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation - ContainerInfo.getNodeId()\nDESCRIPTION: Undocumented getter method for nodeId in ContainerInfo class of YARN server webapp DAO\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\ngetNodeId()\n```\n\n----------------------------------------\n\nTITLE: Updating Shell Script Interpreter\nDESCRIPTION: Fix for bin/slaves.sh script to explicitly use /bin/bash instead of /bin/sh for better compatibility.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n```\n\n----------------------------------------\n\nTITLE: Configuring Initial Block Report Delay (Java)\nDESCRIPTION: Changes the default initial block report delay to 0 seconds. This can be configured using the appropriate Hadoop configuration property.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\nConfiguration conf = new Configuration();\nconf.setLong(\"dfs.blockreport.initialDelay\", 0);\n```\n\n----------------------------------------\n\nTITLE: Configuration Method Deprecation Notice\nDESCRIPTION: List of deprecated Configuration class methods that were removed in this release, including object manipulation and iterator methods.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\ngetObject(String name)\nsetObject(String name, Object value)\nget(String name, Object defaultValue)\nset(String name, Object value)\nIterator entries()\n```\n\n----------------------------------------\n\nTITLE: Improving ZooKeeper Watch Setting in YARN ResourceManager\nDESCRIPTION: This optimization avoids setting unnecessary watches in the ZKRMStateStore class of the YARN ResourceManager, potentially improving performance and reducing ZooKeeper load.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-yarn/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nYARN-3469. ZKRMStateStore: Avoid setting watches that are not required. \n(Jun Gong via kasha)\n```\n\n----------------------------------------\n\nTITLE: Converting Between JobID/TaskID Objects and Strings in Java\nDESCRIPTION: New classes JobID, TaskID, and TaskAttemptID are introduced to replace string counterparts. Applications can use toString() and forName() methods for conversion. This change is part of HADOOP-544.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nJobID jobId = JobID.forName(jobIdString);\nString jobIdString = jobId.toString();\n\nTaskID taskId = TaskID.forName(taskIdString);\nString taskIdString = taskId.toString();\n```\n\n----------------------------------------\n\nTITLE: Defining YARN Configuration Constants in Java\nDESCRIPTION: This snippet represents a collection of constant field declarations in the YarnConfiguration class. These constants define default values for various YARN settings related to NodeManager, ResourceManager, and container management.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n    public static final String DEFAULT_NM_DOCKER_STOP_GRACE_PERIOD;\n    public static final String DEFAULT_NM_DOCKER_USER_REMAPPING_GID_THRESHOLD;\n    public static final String DEFAULT_NM_DOCKER_USER_REMAPPING_UID_THRESHOLD;\n    public static final boolean DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED;\n    public static final int DEFAULT_NM_ELASTIC_MEMORY_CONTROL_OOM_TIMEOUT_SEC;\n    public static final boolean DEFAULT_NM_ENABLE_HARDWARE_CAPABILITY_DETECTION;\n    public static final String DEFAULT_NM_FPGA_VENDOR_PLUGIN;\n    public static final String DEFAULT_NM_GPU_DOCKER_PLUGIN_IMPL;\n    public static final boolean DEFAULT_NM_HEALTH_CHECK_RUN_BEFORE_STARTUP;\n    public static final String DEFAULT_NM_HEALTH_CHECK_SCRIPTS;\n    public static final long DEFAULT_NM_HEALTH_CHECK_TIMEOUT_MS;\n    public static final long DEFAULT_NM_LOG_AGGREGATION_NUM_LOG_FILES_SIZE_PER_APP;\n    public static final int DEFAULT_NM_LOG_AGGREGATION_THREAD_POOL_SIZE;\n    public static final boolean DEFAULT_NM_LOG_CONTAINER_DEBUG_INFO;\n    public static final int DEFAULT_NM_MEMORY_RESOURCE_CGROUPS_SOFT_LIMIT_PERCENTAGE;\n    public static final int DEFAULT_NM_MEMORY_RESOURCE_CGROUPS_SWAPPINESS;\n    public static final boolean DEFAULT_NM_MEMORY_RESOURCE_ENABLED;\n    public static final boolean DEFAULT_NM_MEMORY_RESOURCE_ENFORCED;\n    public static final boolean DEFAULT_NM_NETWORK_RESOURCE_ENABLED;\n    // ... (many more field declarations)\n}\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag in AMRMClient waitFor Method\nDESCRIPTION: The waitFor method in AMRMClient class is missing the @since tag. This method takes a Supplier and two integer parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.0/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient Method waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: File System Path Configuration - Java\nDESCRIPTION: File system path handling changes in Hadoop including new output format for dfs -ls command and implementation of new file system interfaces.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nperm repl owner group size date name\n```\n\n----------------------------------------\n\nTITLE: Required Environment for Hadoop Task Logs\nDESCRIPTION: Specifies the environment requirements for the task log functionality. Bash version 2 or later and the tail command are required for proper operation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.3/common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nrequire bash (v2 or later) and tail\n```\n\n----------------------------------------\n\nTITLE: Configuring Mount Table Entries in core-site.xml\nDESCRIPTION: XML configuration example for setting mount table entries in core-site.xml, used for ViewFS configuration in Hadoop which allows for creating a global namespace across multiple HDFS namespaces.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>fs.viewfs.mounttable.global.link./user</name>\n  <value>hdfs://nn1:8020/user</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Updating HDFS Configuration Templates\nDESCRIPTION: Removes references to deprecated properties in HDFS configuration template and default files. This helps keep the configuration files up-to-date and reduces confusion for users.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\nHDFS-2574. Remove references to some deprecated properties in conf templates and defaults files.\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in AMRMClientAsync Methods\nDESCRIPTION: Multiple waitFor methods in org.apache.hadoop.yarn.client.api.async.AMRMClientAsync are missing @since tags. These methods have different parameter combinations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Setting Default Socket Buffer Size for DataNodes in Java\nDESCRIPTION: Sets the default socket buffer size for DataNodes to 128K. This configuration change aims to optimize network performance for data transfers.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\ndfs.datanode.socket.write.timeout = 128 * 1024; // 128K\n```\n\n----------------------------------------\n\nTITLE: Variable Expansion Configuration Example\nDESCRIPTION: Shows the syntax for variable expansion in configuration files where values can contain ${variable} expressions. Variables are resolved from configuration first, then Java system properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n${hadoop.tmp.dir}\n```\n\n----------------------------------------\n\nTITLE: Java Security Configuration\nDESCRIPTION: Implementation of debug property in JAAS configuration for enhanced security debugging\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nJAAS.configuration\n```\n\n----------------------------------------\n\nTITLE: Adding Configuration for CapacityScheduler in Java\nDESCRIPTION: Adds a new configuration parameter for the CapacityScheduler to cap the maximum percentage of resources that can be used for application masters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nadd yarn.capacity-scheduler.maximum-am-resource-percent\n```\n\n----------------------------------------\n\nTITLE: Pattern Implementation - WritableComparator Fix for Java 6\nDESCRIPTION: Fix to make WritableComparator initialize classes when looking for raw comparators, since Java 6 no longer automatically runs initializers when classes are referenced.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nWritableComparator.initialize(classObj);\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Author Identity for Apache Hadoop Contributions\nDESCRIPTION: Sets up the Git user name and email for creating properly attributed commits to the Apache Hadoop project. Contributors should use their real name and Apache email address.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: gitconfig\nCODE:\n```\ngit config --global user.name \"Your Real Name\"\ngit config --global user.email \"your_id@apache.org\"\n```\n\n----------------------------------------\n\nTITLE: YARN Configuration Field References - Java\nDESCRIPTION: List of configuration fields from YarnConfiguration class and related classes that are missing proper documentation or @since tags. These fields control various aspects of YARN including timeline services, container management, federation settings, and logging configurations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.3/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Sample field references from YarnConfiguration\nDEFAULT_TIMELINE_SERVICE_READER_WEBAPP_HTTPS_ADDRESS\nDEFAULT_TIMELINE_SERVICE_ROLLING_PERIOD\nDEFAULT_TIMELINE_SERVICE_STORAGE_MONITOR_INTERVAL_MS\nDEFAULT_TIMELINE_SERVICE_VERSION\n// ... additional fields\n```\n\n----------------------------------------\n\nTITLE: European Commission OneLab Bloom Filter License Header\nDESCRIPTION: Copyright and license notice for org.apache.hadoop.util.bloom.* classes from the European Commission OneLab project. Specifies redistribution terms and disclaimers under a BSD-style license.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/LICENSE.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n/**\n *\n * Copyright (c) 2005, European Commission project OneLab under contract\n * 034819 (http://www.one-lab.org)\n * All rights reserved.\n * Redistribution and use in source and binary forms, with or \n * without modification, are permitted provided that the following \n * conditions are met:\n *  - Redistributions of source code must retain the above copyright \n *    notice, this list of conditions and the following disclaimer.\n *  - Redistributions in binary form must reproduce the above copyright \n *    notice, this list of conditions and the following disclaimer in \n *    the documentation and/or other materials provided with the distribution.\n *  - Neither the name of the University Catholique de Louvain - UCL\n *    nor the names of its contributors may be used to endorse or \n *    promote products derived from this software without specific prior \n *    written permission.\n *    \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT \n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS \n * FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE \n * COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, \n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, \n * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER \n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT \n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN \n * ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n * POSSIBILITY OF SUCH DAMAGE.\n */\n```\n\n----------------------------------------\n\nTITLE: Closing HTML Structure for Apache Hadoop Website\nDESCRIPTION: This HTML snippet closes the content div, adds a footer with copyright information, and closes the remaining open tags to complete the HTML structure of the Apache Hadoop project website.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.2/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n  </div>\n\n  <div id=\"footer\">\n    <div class=\"copyright\">\n      Copyright &copy; 2006-2023 <a href=\"http://www.apache.org/\">The Apache Software Foundation</a>.\n      Apache Hadoop, Hadoop, and the Hadoop elephant logo are trademarks of the Apache Software Foundation.\n    </div>\n    <div class=\"trademark\">\n      <a href=\"/trademarks.html\">Apache Hadoop, Hadoop, and their related projects are trademarks of the Apache Software Foundation. For a list of Apache project trademarks, see www.apache.org/foundation/marks</a>\n    </div>\n  </div>\n</div>\n\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Starting HDFS Services\nDESCRIPTION: Shell commands to start the Hadoop HDFS services including the NameNode, DataNodes, and other related services. These commands are typically run from the Hadoop scripts directory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nstart-dfs.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring NUMA Awareness in Hadoop YARN (Java)\nDESCRIPTION: Method to check if NUMA awareness is enabled in the YARN configuration. This is part of the YarnConfiguration class for managing YARN-specific settings.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.1/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nYarnConfiguration.numaAwarenessEnabled(Configuration conf)\n```\n\n----------------------------------------\n\nTITLE: Incrementing Counters in MapReduce Streaming with Python\nDESCRIPTION: This example demonstrates how to increment a custom counter in a Python script that's part of a Hadoop streaming job. It shows the stderr message format used to update counters in streaming applications.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsys.stderr.write(\"reporter:counter:Custom,MyCounter,1\\n\")\n```\n\n----------------------------------------\n\nTITLE: Locale Environment Variable Configuration in Shell Script\nDESCRIPTION: Shell script modification to unset LANG and LC_CTYPE environment variables for compatibility with non-English locales in saveVersion.sh\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\nunset LANG\nunset LC_CTYPE\n```\n\n----------------------------------------\n\nTITLE: Adding Tracing to DFSInputStream in Java\nDESCRIPTION: Implementation of tracing functionality for DFSInputStream to improve debugging and performance analysis capabilities.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-7055. Add tracing to DFSInputStream (cmccabe)\n```\n\n----------------------------------------\n\nTITLE: HDFS Security Token Implementation - Java\nDESCRIPTION: Implementation of block access tokens conforming to generic Token interface in Hadoop Common, enabling secure access to HDFS blocks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hdfs/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-992. Re-factor block access token implementation to conform to the generic Token interface in Common (Kan Zhang and Jitendra Pandey via jghoman)\n```\n\n----------------------------------------\n\nTITLE: Verifying Hadoop binary release with SHA-512 checksum\nDESCRIPTION: Command to verify the integrity of the Hadoop binary release by computing its SHA-512 hash and comparing it to the expected checksum. This ensures the file hasn't been corrupted or tampered with.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nshasum -a 512 hadoop-3.3.2.tar.gz | diff - hadoop-3.3.2.tar.gz.sha512\n```\n\n----------------------------------------\n\nTITLE: Building and Serving the Hadoop Site with Jekyll\nDESCRIPTION: Command to build and preview the Hadoop website locally using Jekyll's built-in server. The site is served at http://localhost:4000/ with live reload capabilities for development.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbundle exec jekyll serve\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: YAML frontmatter configuration for the release announcement page, specifying the title and publication date.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/3.0.2.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Release 3.0.2 available\ndate: 2018-04-21\n---\n```\n\n----------------------------------------\n\nTITLE: Java Test Suite Updates\nDESCRIPTION: Various test class updates and fixes for HDFS test suite\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nHDFS-441. Remove TestFTPFileSystem\n```\n\n----------------------------------------\n\nTITLE: Page Configuration in YAML Front Matter\nDESCRIPTION: YAML configuration block that defines the page title and menu structure, placing this page under the community section of the main navigation.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/who.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Who We are\nmenu:\n   main:\n      parent: community\n```\n\n----------------------------------------\n\nTITLE: Documentation Issues in Hadoop Statistics Classes\nDESCRIPTION: Missing @SINCE tags in statistics-related classes and fields\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nMISSING @SINCE TAG: org.apache.hadoop.fs.statistics.StoreStatisticNames Field ACTION_FILE_OPENED\nMISSING @SINCE TAG: org.apache.hadoop.fs.statistics.StoreStatisticNames Field OP_CREATE_FILE\nMISSING @SINCE TAG: org.apache.hadoop.fs.statistics.StoreStatisticNames Field OP_MSYNC\nMISSING @SINCE TAG: org.apache.hadoop.fs.statistics.StoreStatisticNames Field OP_OPENFILE\nMISSING @SINCE TAG: org.apache.hadoop.fs.statistics.StoreStatisticNames Field STORE_IO_RATE_LIMITED\n```\n\n----------------------------------------\n\nTITLE: AMRMClient Wait Methods Missing @since Tags\nDESCRIPTION: Various overloaded waitFor methods in AMRMClient and AMRMClientAsync classes that lack proper version documentation tags. Methods accept Supplier and timeout parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.6/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient.waitFor(Supplier)\norg.apache.hadoop.yarn.client.api.AMRMClient.waitFor(Supplier, int)\norg.apache.hadoop.yarn.client.api.AMRMClient.waitFor(Supplier, int, int)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(Supplier)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(Supplier, int)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.waitFor(Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter for Release Announcement\nDESCRIPTION: YAML front matter defining the title and date metadata for the release announcement page.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/2.9.0.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 2.9.0 available\ndate: 2017-12-17\n---\n```\n\n----------------------------------------\n\nTITLE: Adding MapReduce Configuration for Out-of-Band Heartbeat\nDESCRIPTION: This snippet adds a new configuration property for enabling out-of-band heartbeats from TaskTrackers to improve job latency.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.2/mapreduce/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nConfiguration changes:\n      add mapreduce.tasktracker.outofband.heartbeat\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License 2.0 Boilerplate Notice\nDESCRIPTION: This snippet provides the boilerplate notice to be included when applying the Apache License 2.0 to a work. It includes placeholders for the copyright year and owner's name.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/LICENSE.txt#2025-04-08_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Improving NMLeveldbStateStoreService Recovery Time\nDESCRIPTION: Addresses the issue of NMLeveldbStateStoreService database growth leading to longer recovery times.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nYARN-5009. NMLeveldbStateStoreService database can grow substantially leading to longer recovery times (jlowe)\n```\n\n----------------------------------------\n\nTITLE: Markdown Page Header with License\nDESCRIPTION: Page header in markdown format with title and Apache 2.0 license declaration in HTML comments\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/description.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Description\n---\n<!---\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n```\n\n----------------------------------------\n\nTITLE: Markdown Release Header\nDESCRIPTION: YAML frontmatter header defining the title and date of the release announcement post\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/3.0.0-alpha4.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 3.0.0-alpha4 available\ndate: 2017-07-07\n---\n```\n\n----------------------------------------\n\nTITLE: Adding MapReduce JVM Reuse Configuration in Hadoop XML\nDESCRIPTION: Adds a new configuration property 'mapred.job.reuse.jvm.num.tasks' to hadoop-default.xml to enable JVM reuse across Map-Reduce tasks, improving performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Configuration\nDESCRIPTION: YAML front matter block defining the page title and publication date for the Hadoop 2.6.4 release announcement.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/2.6.4.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 2.6.4 available\ndate: 2016-02-11\n---\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License 2.0 to Your Work\nDESCRIPTION: This snippet provides a template for applying the Apache License 2.0 to a project. It includes placeholders for the copyright year and owner, and the standard Apache 2.0 license text.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/mapreduce/LICENSE.txt#2025-04-08_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: File System Path Configuration\nDESCRIPTION: Configuration parameter for default file system path in libhdfs test.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\nfs.default.name\n```\n\n----------------------------------------\n\nTITLE: Starting HDFS with Upgrade Flag in Hadoop 0.16.0\nDESCRIPTION: Command to start HDFS with the upgrade flag when upgrading an existing HDFS filesystem to Hadoop 0.16.x from an earlier release. This is a required step in the upgrade process.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/0.16.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-dfs.sh -upgrade\n```\n\n----------------------------------------\n\nTITLE: Executing Hadoop Job List Command in Shell\nDESCRIPTION: This snippet shows how to list Hadoop jobs using the command line interface. It demonstrates the new 'job -list' subcommand added to the Hadoop CLI.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/CHANGES.txt#2025-04-08_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\nbin/hadoop job -list\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry - Windows Compatibility Fix\nDESCRIPTION: Example changelog entry showing a fix for Windows-specific test failures.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nMAPREDUCE-5824. Fixed test-failure of TestPipesNonJavaInputFormat in\nWindows. (Xuan Gong via vinodkv)\n```\n\n----------------------------------------\n\nTITLE: Fix typo in HDFS Explorer JavaScript\nDESCRIPTION: HDFS-6472 fix addressing a typo in the HDFS web interface explorer JavaScript file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nexplorer.js\n```\n\n----------------------------------------\n\nTITLE: JVM Configuration Parameter\nDESCRIPTION: JVM option to enable heap dump on out of memory errors for test execution\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\n-XX:+HeapDumpOnOutOfMemoryError\n```\n\n----------------------------------------\n\nTITLE: Apache License Comment Header in HTML\nDESCRIPTION: Standard Apache License 2.0 header comment included at the beginning of the file to specify licensing terms for the content.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/news/2009-03-xx-apachecon-eu.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!---\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n```\n\n----------------------------------------\n\nTITLE: Adding MapReduce JVM Reuse Configuration in Hadoop XML\nDESCRIPTION: Adds a new configuration property 'mapred.job.reuse.jvm.num.tasks' to hadoop-default.xml to allow reusing JVMs across Map-Reduce tasks for improved performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Generating Apache Hadoop Site with Hugo\nDESCRIPTION: Command to generate the Apache Hadoop site using Hugo static site generator. The generated site will be placed in the 'content' subdirectory.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/README.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhugo\n```\n\n----------------------------------------\n\nTITLE: Creating New Release Content in Markdown\nDESCRIPTION: Example of creating a new release file in Markdown format. The file should be named '<version>.md' and placed in the 'src/release' directory. It includes metadata and release information.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/README.md#2025-04-08_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 2.7.3 available\ndate: 2016-08-26\nlinked: true\n---\n\nPlease see the [Hadoop 2.7.3 Release\nNotes](https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/releasenotes.html)\nfor the list of 221 bug fixes and patches since the previous release\n2.7.2.\n```\n\n----------------------------------------\n\nTITLE: Configuring Variable Expansion in Hadoop Configuration Files\nDESCRIPTION: This snippet demonstrates how to use variable expansion in Hadoop configuration files. Variables are referenced using ${variable} syntax and can be defined in the configuration or Java system properties. The example shows setting the hadoop.tmp.dir variable.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_34\n\nLANGUAGE: Properties\nCODE:\n```\n${hadoop.tmp.dir}, which is, by default,\\n/tmp/hadoop-${user.name}\n```\n\n----------------------------------------\n\nTITLE: Adding MapReduce JVM Reuse Configuration in Hadoop XML\nDESCRIPTION: Adds a new configuration property 'mapred.job.reuse.jvm.num.tasks' to hadoop-default.xml to allow reusing JVMs across Map-Reduce tasks for improved performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: XML\nCODE:\n```\nadd mapred.job.reuse.jvm.num.tasks\n```\n\n----------------------------------------\n\nTITLE: Starting Hugo Development Server\nDESCRIPTION: Command to start a standalone auto-refreshed web server for improving and developing the Apache Hadoop site.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/README.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhugo server\n```\n\n----------------------------------------\n\nTITLE: Deprecating File Locking API in Hadoop\nDESCRIPTION: Marks the file locking API as deprecated in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\n@Deprecated\npublic void lockFile() { ... }\n```\n\n----------------------------------------\n\nTITLE: Starting HDFS with Upgrade Option in Hadoop 0.15.x\nDESCRIPTION: Command to start HDFS when upgrading from an earlier release to Hadoop 0.15.x. This initializes the upgrade process for the HDFS filesystem.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/0.15.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-dfs.sh -upgrade\n```\n\n----------------------------------------\n\nTITLE: Fixing NPE in TestCapacityScheduler in Java\nDESCRIPTION: Fixes a null pointer exception in the TestCapacityScheduler class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5607. Fix NPE in TestCapacityScheduler. (cdouglas)\n```\n\n----------------------------------------\n\nTITLE: Adding Bash Tab Completion for Hadoop CLI\nDESCRIPTION: Introduces a new contrib module that enables bash tab completion for the Hadoop command-line interface. This improves usability by providing auto-completion for Hadoop commands.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n# See the README file in the contrib directory for installation instructions\n```\n\n----------------------------------------\n\nTITLE: MIT CRC Implementation License Header\nDESCRIPTION: Copyright and BSD-style license notice for the native implementation of slicing-by-8 CRC calculation code from MIT.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/common/LICENSE.txt#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n/**\n *   Copyright 2008,2009,2010 Massachusetts Institute of Technology.\n *   All rights reserved. Use of this source code is governed by a\n *   BSD-style license that can be found in the LICENSE file.\n */\n```\n\n----------------------------------------\n\nTITLE: Improving TestCopyFiles in Java\nDESCRIPTION: Increases the number of files and adds debug messages in the TestCopyFiles class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-5305. Increase number of files and print debug messages in\nTestCopyFiles.  (szetszwo)\n```\n\n----------------------------------------\n\nTITLE: Updating HDFS URL Prefix in Java\nDESCRIPTION: Updates the HDFS URL prefix to use 'hdfs://' instead of a previous format. This change affects how HDFS paths are referenced in code.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/common/CHANGES.txt#2025-04-08_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\n\"hdfs://\"\n```\n\n----------------------------------------\n\nTITLE: Correcting Multithreaded Issues in ActiveStandbyElector\nDESCRIPTION: Resolves multithreaded correctness warnings in the ActiveStandbyElector class.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nFix multithreaded correctness warnings in ActiveStandbyElector\n```\n\n----------------------------------------\n\nTITLE: Supporting Symlinks in LocalJobRunner DistributedCache in Java\nDESCRIPTION: Adds support for symbolic links in the DistributedCache when using LocalJobRunner, improving flexibility for local job execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-3871. Allow symlinking in LocalJobRunner DistributedCache.\n```\n\n----------------------------------------\n\nTITLE: Linux Container Executor Script Configuration\nDESCRIPTION: Build script modification to prevent compiling 32-bit container-executor binary by default on all platforms.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.6/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nMAPREDUCE-3922\n```\n\n----------------------------------------\n\nTITLE: Setting up Hadoop mapred-site.xml\nDESCRIPTION: XML configuration for mapred-site.xml that defines the MapReduce framework and its parameters for job execution.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n    <property>\n        <name>mapreduce.application.classpath</name>\n        <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>\n    </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Upgrading Commons-Logging to Avoid Deadlock\nDESCRIPTION: Upgrades commons-logging to version 1.1.3 to prevent potential deadlocks in MiniDFSCluster.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nUpgrade to commons-logging 1.1.3 to avoid potential deadlock in MiniDFSCluster\n```\n\n----------------------------------------\n\nTITLE: Setting JobTracker Persistence Configuration Values in Hadoop\nDESCRIPTION: Configuration properties for enabling JobTracker job status persistence across restarts. These settings control whether job status is persisted, how long the information is retained, and where it is stored on disk.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.5.2/common/CHANGES.txt#2025-04-08_snippet_19\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.tracker.persist.jobstatus.active (default value of false)\nmapred.job.tracker.persist.jobstatus.hours (default value of 0)\nmapred.job.tracker.persist.jobstatus.dir (default value of\n                                                /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Fixing ArithmeticException in LocalDirAllocator\nDESCRIPTION: Addresses an ArithmeticException occurring when there is no available space on configured local directories.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nLocalDirAllocator throws \"ArithmeticException: / by zero\"\n```\n\n----------------------------------------\n\nTITLE: Structuring HTML Template for Apache Hadoop Website\nDESCRIPTION: This HTML template sets up the basic structure for the Apache Hadoop project website. It includes meta tags for character encoding and viewport, links to stylesheets, and script references. The body contains placeholder divs for header, menu, and main content areas.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!DOCTYPE html>\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Apache Hadoop</title>\n    <link rel=\"stylesheet\" href=\"/css/styles.css\" type=\"text/css\" />\n    <link rel=\"shortcut icon\" href=\"/images/favicon.ico\" />\n</head>\n<body>\n<div id=\"header\">\n</div>\n<div id=\"menu\">\n</div>\n<div id=\"content\">\n</div>\n<script type=\"text/javascript\" src=\"/js/apache-hadoop.js\"></script>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Property for Hadoop Native Library\nDESCRIPTION: Java property configuration that tells Hadoop whether to use native libraries for performance improvement. When set to true, Hadoop will attempt to use platform-specific implementations of certain components.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\n-Dhadoop.native.lib=true\n```\n\n----------------------------------------\n\nTITLE: Hadoop Version Command Usage in Shell\nDESCRIPTION: A command to display version and revision information about the Hadoop installation. This was added in version 0.6.0 (change HADOOP-567) and displays build information including version, date, user, and SVN revision.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.5/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\nbin/hadoop version\n```\n\n----------------------------------------\n\nTITLE: Setting RPC Protection In Hadoop Configuration File\nDESCRIPTION: XML property configuration that specifies the QoP (Quality of Protection) for Hadoop RPC protocols. This determines whether authentication, integrity checking, or privacy protection should be used for RPC connections.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.rpc.protection</name>\n  <value>authentication</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag for NMClient Method\nDESCRIPTION: The updateContainerResource method in NMClient class is missing the @since tag. This method probably updates the resources allocated to a container.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.NMClient Method updateContainerResource(org.apache.hadoop.yarn.api.records.Container)\n```\n\n----------------------------------------\n\nTITLE: Fixing BigDecimal Division in Pi Example (Java)\nDESCRIPTION: Corrects the usage of BigDecimal.divide() method in the pi example, likely addressing precision or rounding issues.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nMAPREDUCE-1880. Fix BigDecimal.divide(..) in the pi example.  (szetszwo)\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Xceivers in DataNode (Java)\nDESCRIPTION: Allows configuring the maximum number of xceivers (transfer threads) in the Hadoop DataNode. This improves flexibility in tuning DataNode performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3859. Allow the maximum number of xceivers in the data node to\nbe configurable. (Johan Oskarsson via omalley)\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Xceivers in DataNode (Java)\nDESCRIPTION: Allows configuring the maximum number of xceivers (transfer threads) in the Hadoop DataNode. This improves flexibility in tuning DataNode performance.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-3859. Allow the maximum number of xceivers in the data node to\nbe configurable. (Johan Oskarsson via omalley)\n```\n\n----------------------------------------\n\nTITLE: Updating Native Library Build Configuration\nDESCRIPTION: Changes to build configuration for native libraries including pipes, utils and libhdfs to use autoconf and match JVM architecture (32/64 bit).\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nc++/<os_osarch_jvmdatamodel>/lib\n```\n\n----------------------------------------\n\nTITLE: Adjusting Logging Levels in Java IPC, Metrics, and HTTP\nDESCRIPTION: Reduces logging verbosity by changing some info logs to debug level in IPC, metrics, and HTTP components.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nHADOOP-7858. Drop some info logging to DEBUG level in IPC,\nmetrics, and HTTP. (todd via eli)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in YARN Configuration Fields\nDESCRIPTION: Collection of YARN configuration fields that are missing proper documentation blocks or @since tags. These fields are part of YarnConfiguration, ApplicationConstants, and other YARN related classes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.4/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.conf.YarnConfiguration\n  TIMELINE_SERVICE_READER_WEBAPP_HTTPS_ADDRESS\n  TIMELINE_SERVICE_ROLLING_PERIOD\n  TIMELINE_SERVICE_SCHEMA_CREATOR_CLASS\n  TIMELINE_SERVICE_VERSION\n  TIMELINE_SERVICE_VERSIONS\n  TIMELINE_SERVICE_WRITER_ASYNC_QUEUE_CAPACITY\n  TIMELINE_SERVICE_WRITER_CLASS\n  TIMELINE_SERVICE_WRITER_FLUSH_INTERVAL_SECONDS\n  TIMELINE_V2_CLIENT_DRAIN_TIME_MILLIS\n  TIMELINE_XFS_OPTIONS\n```\n\n----------------------------------------\n\nTITLE: Variable Expansion Syntax in Hadoop Configuration Files\nDESCRIPTION: This shows the syntax for variable references in Hadoop configuration files. Variables are referenced using '${variable}' syntax, with values found first in the configuration and then in Java system properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/common/CHANGES.txt#2025-04-08_snippet_17\n\nLANGUAGE: plain\nCODE:\n```\n${variable}\n```\n\n----------------------------------------\n\nTITLE: Configuration Field Analysis - YarnConfiguration Class\nDESCRIPTION: Collection of configuration fields from YarnConfiguration class that are missing proper documentation or @since tags. These fields define various aspects of YARN's behavior including federation settings, container management, storage configurations, and runtime parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\npublic class YarnConfiguration {\n  // Federation related fields\n  FEDERATION_MACHINE_LIST\n  FEDERATION_POLICY_MANAGER\n  FEDERATION_POLICY_MANAGER_PARAMS\n  FEDERATION_PREFIX\n  // Storage related fields\n  FILE_CONFIGURATION_STORE\n  FS_CONFIGURATION_STORE\n  LEVELDB_CONFIGURATION_STORE\n  MEMORY_CONFIGURATION_STORE\n  // Container management fields\n  NM_CONTAINER_EXECUTOR_EXIT_FILE_TIMEOUT\n  NM_CONTAINER_LOCALIZER_JAVA_OPTS_DEFAULT\n  NM_CONTAINER_STDERR_BYTES\n  NM_CONTAINER_STDERR_PATTERN\n  // ... other fields omitted for brevity\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying Hadoop Script Source Path in Shell\nDESCRIPTION: Changes how Hadoop scripts source the hadoop-config.sh file.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/common/CHANGES.txt#2025-04-08_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\nHADOOP-7802. Hadoop scripts unconditionally source\n\"$bin\"/../libexec/hadoop-config.sh. (Bruno Mahé via tomwhite)\n```\n\n----------------------------------------\n\nTITLE: Referencing Configuration Variables in Java\nDESCRIPTION: Demonstrates how to reference configuration variables in Hadoop property values using the ${variable} syntax. Variables are resolved from the configuration or Java system properties.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.3/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\n${hadoop.tmp.dir}\n```\n\nLANGUAGE: Java\nCODE:\n```\n/tmp/hadoop-${user.name}\n```\n\n----------------------------------------\n\nTITLE: Updating POM XML Configuration for ApplicationHistoryServer\nDESCRIPTION: XML configuration changes to correctly build the applicationhistoryserver module\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\nYARN-935. Correcting pom.xml to build applicationhistoryserver module successfully.\n```\n\n----------------------------------------\n\nTITLE: Displaying Queue ACLs in MapReduce using Bash\nDESCRIPTION: Modification to the 'bin/mapred queue -showacl' command output to clarify ACLs for users. This improves the readability and understanding of queue access control lists.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.7/hadoop-project-dist/hadoop-mapreduce/CHANGES.txt#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/mapred queue -showacl\n```\n\n----------------------------------------\n\nTITLE: Bash Command Fix in UnixLocalWrapperScriptBuilder\nDESCRIPTION: Code reference showing the issue where using bash -c can cause 'Text file busy' errors in UnixLocalWrapperScriptBuilder\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbash -c\n```\n\n----------------------------------------\n\nTITLE: Bash Command Fix in UnixLocalWrapperScriptBuilder\nDESCRIPTION: Code reference showing the issue where using bash -c can cause 'Text file busy' errors in UnixLocalWrapperScriptBuilder\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbash -c\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag for NMClientAsync Method\nDESCRIPTION: The updateContainerResourceAsync method in NMClientAsync class is missing the @since tag. This is likely an asynchronous version of the container resource update method.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.0/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.NMClientAsync Method updateContainerResourceAsync(org.apache.hadoop.yarn.api.records.Container)\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry - MapReduce Enhancement\nDESCRIPTION: Example changelog entry showing the addition of API for task log URL access.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/CHANGES.txt#2025-04-08_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nMAPREDUCE-5830. Added back the private API HostUtil.getTaskLogUrl(..) for\nbinary compatibility with older clients like Hive 0.13. (Akira Ajisaka via\nvinodkv)\n```\n\n----------------------------------------\n\nTITLE: Implementing NativeIO Support Using JNI in Java\nDESCRIPTION: Adds support for Native I/O operations using Java Native Interface (JNI). This allows for improved performance by leveraging platform-specific optimizations for I/O operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\npublic class NativeIO {\n  static {\n    System.loadLibrary(\"hadoop\");\n  }\n  \n  public static native void someNativeMethod();\n  // ... other native method declarations\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing NativeIO Support Using JNI in Java\nDESCRIPTION: Adds support for Native I/O operations using Java Native Interface (JNI). This allows for improved performance by leveraging platform-specific optimizations for I/O operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.0/common/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\npublic class NativeIO {\n  static {\n    System.loadLibrary(\"hadoop\");\n  }\n  \n  public static native void someNativeMethod();\n  // ... other native method declarations\n}\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry - MapReduce Bug Fix\nDESCRIPTION: Example changelog entry showing a bug fix that removed forceful JVM exit in shutDownJob.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nMAPREDUCE-5714. Removed forceful JVM exit in shutDownJob.  \n(Jinghui Wang via Eric Yang)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in YARN Client API Methods\nDESCRIPTION: Lists of Java methods and constructors in the YARN client API that are missing proper documentation, including @since tags and documentation blocks. These issues span across multiple client interfaces including AMRMClient, NMClientAsync, YarnClient, and related classes.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.3/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.AMRMClient.addSchedulingRequests(java.util.Collection)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync.addSchedulingRequests(java.util.Collection)\norg.apache.hadoop.yarn.client.api.NMClient.commitLastReInitialization(org.apache.hadoop.yarn.api.records.ContainerId)\n// Additional methods omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes for Job Status Persistence in Hadoop XML\nDESCRIPTION: New configuration parameters in hadoop-default.xml for persisting job statuses in HDFS, enabling job information queries across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_18\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.tracker.persist.jobstatus.active (default value of false)\nmapred.job.tracker.persist.jobstatus.hours (default value of 0)\nmapred.job.tracker.persist.jobstatus.dir (default value of /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in YARN Client APIs\nDESCRIPTION: Documentation issues in Hadoop YARN client APIs including missing constructor documentation blocks and @since tags in method declarations. Affects async client implementations, resource management, and container operations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.4/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Constructor (int, org.apache.hadoop.yarn.client.api.async.AMRMClientAsync.AbstractCallbackHandler)\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Constructor (org.apache.hadoop.yarn.client.api.AMRMClient, int, org.apache.hadoop.yarn.client.api.async.AMRMClientAsync.AbstractCallbackHandler)\norg.apache.hadoop.yarn.client.api.async.NMClientAsync Constructor (java.lang.String, org.apache.hadoop.yarn.client.api.NMClient, org.apache.hadoop.yarn.client.api.async.NMClientAsync.AbstractCallbackHandler)\n```\n\n----------------------------------------\n\nTITLE: Configuring Speculative Execution Settings\nDESCRIPTION: Configuration changes in hadoop-default.xml to allow independent control of speculative execution for map and reduce tasks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\ndeprecated mapred.speculative.execution\nadd mapred.map.tasks.speculative.execution\nadd mapred.reduce.tasks.speculative.execution\n```\n\n----------------------------------------\n\nTITLE: Building Hadoop Documentation with Maven\nDESCRIPTION: Maven command to build Hadoop documentation while skipping tests and using different profile configurations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean site site:stage -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Status Persistence\nDESCRIPTION: Configuration settings in hadoop-default.xml for persisting job statuses in HDFS, allowing JobClient to query information about decommissioned jobs across JobTracker restarts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r0.23.11/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\nmapred.job.tracker.persist.jobstatus.active (default value of false)\nmapred.job.tracker.persist.jobstatus.hours (default value of 0)\nmapred.job.tracker.persist.jobstatus.dir (default value of /jobtracker/jobsInfo)\n```\n\n----------------------------------------\n\nTITLE: Documenting HDFS Administrative Properties\nDESCRIPTION: Adds documentation for the dfs.cluster.administrators and dfs.permissions.superusergroup properties in HDFS.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.4/hdfs/CHANGES.txt#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nHDFS-9415. Document dfs.cluster.administrators and dfs.permissions.superusergroup.\n```\n\n----------------------------------------\n\nTITLE: Publishing Content to Apache Hadoop Site\nDESCRIPTION: Command to publish content to the live Hadoop website after making changes. The script handles the necessary steps to update the site in the Apache CMS.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./publish.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Jekyll Dependencies for Hadoop Site\nDESCRIPTION: Commands for installing Jekyll and its dependencies required for local development of the Hadoop website. This ensures all necessary Ruby gems are installed based on the project's Gemfile.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.3/hadoop-mapreduce-client/hadoop-mapreduce-client-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngem install bundler\nbundle install\n```\n\n----------------------------------------\n\nTITLE: Undocumented AppAttemptInfo Members\nDESCRIPTION: Undocumented fields and methods in AppAttemptInfo class related to timing information for application attempts.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.0/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\ngetFinishedTime()\ngetStartedTime()\nField finishedTime\nField startedTime\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation in Input/Output Format Fields\nDESCRIPTION: Configuration fields in various input/output format classes missing documentation blocks, including separators and input directory handling.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nINPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS\nKEY_VALUE_SEPARATOR\nSEPARATOR\nDATA_FIELD_SEPARATOR\n```\n\n----------------------------------------\n\nTITLE: Documentation Issues in Hadoop Utility Classes\nDESCRIPTION: Missing @SINCE tags and documentation blocks in utility and functional classes\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.5/hadoop-project-dist/hadoop-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nMISSING @SINCE TAG: org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter Class\nMISSING @SINCE TAG: org.apache.hadoop.util.functional.FutureIO Method eval(org.apache.hadoop.util.functional.CallableRaisingIOE)\nMISSING @SINCE TAG: org.apache.hadoop.util.functional.FutureIO Method propagateOptions(org.apache.hadoop.fs.FSBuilder, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String)\n```\n\n----------------------------------------\n\nTITLE: Missing @since tag in waitFor method with two parameters in AMRMClientAsync class\nDESCRIPTION: Documentation highlighting a missing @since JavaDoc tag in the waitFor method with Supplier and integer parameters in the AMRMClientAsync class of the Hadoop YARN client API.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.1.4/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int)\n```\n\n----------------------------------------\n\nTITLE: Updating Build XML Configuration\nDESCRIPTION: Fix to correct jar name typo in build.xml configuration file\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/hadoop-project-dist/hadoop-hdfs/CHANGES.txt#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\nHDFS-480. Fix a typo in the jar name in build.xml\n```\n\n----------------------------------------\n\nTITLE: Missing Documentation Blocks for Job Fields\nDESCRIPTION: Class fields in org.apache.hadoop.mapreduce.Job missing complete documentation blocks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nDEFAULT_USE_WILDCARD_FOR_LIBJARS\nUSE_WILDCARD_FOR_LIBJARS\n```\n\n----------------------------------------\n\nTITLE: YARN Configuration Constants\nDESCRIPTION: Configuration constants used in Hadoop YARN for AM scheduling, AMRM proxy settings, and application diagnostics limits.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nAM_SCHEDULING_NODE_BLACKLISTING_DISABLE_THRESHOLD\nAM_SCHEDULING_NODE_BLACKLISTING_ENABLED\nAMRM_PROXY_ADDRESS\nAMRM_PROXY_CLIENT_THREAD_COUNT\nAMRM_PROXY_ENABLED\nAMRM_PROXY_HA_ENABLED\nAMRM_PROXY_INTERCEPTOR_CLASS_PIPELINE\nAPP_ATTEMPT_DIAGNOSTICS_LIMIT_KC\n```\n\n----------------------------------------\n\nTITLE: YARN Configuration Constants\nDESCRIPTION: Configuration constants used in Hadoop YARN for AM scheduling, AMRM proxy settings, and application diagnostics limits.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.3.2/hadoop-yarn/hadoop-yarn-api/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nAM_SCHEDULING_NODE_BLACKLISTING_DISABLE_THRESHOLD\nAM_SCHEDULING_NODE_BLACKLISTING_ENABLED\nAMRM_PROXY_ADDRESS\nAMRM_PROXY_CLIENT_THREAD_COUNT\nAMRM_PROXY_ENABLED\nAMRM_PROXY_HA_ENABLED\nAMRM_PROXY_INTERCEPTOR_CLASS_PIPELINE\nAPP_ATTEMPT_DIAGNOSTICS_LIMIT_KC\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in JobStatus Constructors\nDESCRIPTION: Constructors in both org.apache.hadoop.mapred.JobStatus and org.apache.hadoop.mapreduce.JobStatus classes missing version documentation tags.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.9.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.mapred.JobStatus(JobID, float, float, float, float, int, JobPriority, String, String, String, String, boolean, String)\norg.apache.hadoop.mapred.JobStatus(JobID, float, float, float, float, int, JobPriority, String, String, String, String, String, boolean, String)\norg.apache.hadoop.mapreduce.JobStatus(JobID, float, float, float, float, State, JobPriority, String, String, String, String, String, boolean, String)\n```\n\n----------------------------------------\n\nTITLE: Checking non-vulnerable container-executor configuration using readelf in Linux\nDESCRIPTION: This code shows the expected output when examining a non-vulnerable container-executor binary where the RUNPATH doesn't include the '../lib/native/' relative path that enables the privilege escalation vulnerability.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/cve_list.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ readelf -d container-executor|grep 'RUNPATH\\|RPATH'\n0x000000000000001d (RUNPATH)            Library runpath: [$ORIGIN/]\n```\n\n----------------------------------------\n\nTITLE: Missing Constructor Documentation in NMClientAsync\nDESCRIPTION: Constructors for the asynchronous Node Manager client class lacking documentation blocks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nNMClientAsync(String, NMClient, NMClientAsync.AbstractCallbackHandler)\nNMClientAsync(String, NMClientAsync.AbstractCallbackHandler)\nNMClientAsync(NMClientAsync.AbstractCallbackHandler)\n```\n\n----------------------------------------\n\nTITLE: XML Configuration Path for Windows\nDESCRIPTION: Configuration path setting that doesn't work properly on Windows systems, showing the $HADOOP_CONF_DIR variable usage\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.5/yarn/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: XML\nCODE:\n```\nyarn.application.classpath=$HADOOP_CONF_DIR\n```\n\n----------------------------------------\n\nTITLE: Missing Constructor Documentation in AMRMClientAsync\nDESCRIPTION: Constructors for the asynchronous Application Master Resource Manager client class lacking documentation blocks.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.3/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nAMRMClientAsync(int, AMRMClientAsync.AbstractCallbackHandler)\nAMRMClientAsync(AMRMClient, int, AMRMClientAsync.AbstractCallbackHandler)\n```\n\n----------------------------------------\n\nTITLE: Missing Constructor Documentation in AMRMClientAsync\nDESCRIPTION: Constructors for the AMRMClientAsync class lacking documentation blocks. These constructors initialize async application master resource manager clients with different parameter combinations.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.8.2/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nAMRMClientAsync(int, AbstractCallbackHandler)\nAMRMClientAsync(AMRMClient, int, AbstractCallbackHandler)\n```\n\n----------------------------------------\n\nTITLE: Fixing Java Package Names in Hadoop\nDESCRIPTION: This change updates package names from 'dfs' to 'hdfs' to reflect the new naming convention in Hadoop.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nimport org.apache.hadoop.hdfs.*;\n```\n\n----------------------------------------\n\nTITLE: Unset Environment Variables in Shell Script\nDESCRIPTION: This snippet unsets the LANG and LC_CTYPE environment variables in the saveVersion.sh script to ensure compatibility with non-English locales.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nunset LANG LC_CTYPE\n```\n\n----------------------------------------\n\nTITLE: Code Reference: JobClient API Changes\nDESCRIPTION: Lists removed JobClient API methods and classes as part of HADOOP-3986 to remove static Configuration.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.6.1/hadoop-project-dist/hadoop-common/CHANGES.txt#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nJobClient::setCommandLineConfig is removed\nJobClient::getCommandLineConfig is removed\nJobShell, TestJobShell classes are removed\n```\n\n----------------------------------------\n\nTITLE: Resource Manager Queue Configuration Pattern\nDESCRIPTION: Naming convention for configuring queue properties in Hadoop Resource Manager.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Properties\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Resource Manager Queue Configuration Pattern\nDESCRIPTION: Naming convention for configuring queue properties in Hadoop Resource Manager.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/common/CHANGES.txt#2025-04-08_snippet_9\n\nLANGUAGE: Properties\nCODE:\n```\nhadoop.rm.queue.queue-name.property-name\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag in FileSystem Method (Java)\nDESCRIPTION: The getCacheSize() method in the org.apache.hadoop.fs.FileSystem class is missing the @since tag in its documentation. This tag is important for versioning and tracking when the method was introduced.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r1.2.1/jdiff/missingSinces.txt#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.fs.FileSystem.getCacheSize()\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tag in JournalNode getJNStartedTimeInMillis Method\nDESCRIPTION: The getJNStartedTimeInMillis() method in JournalNodeMXBean interface is missing the @since JavaDoc tag that documents the version when this method was introduced.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.4.1/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean.getJNStartedTimeInMillis()\n```\n\n----------------------------------------\n\nTITLE: Missing @since Tags in AMRMClientAsync waitFor Methods\nDESCRIPTION: Multiple overloads of the waitFor method in AMRMClientAsync class are missing @since tags. These methods take various combinations of Supplier and integer parameters.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r3.2.0/hadoop-yarn/hadoop-yarn-client/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.hadoop.yarn.client.api.async.AMRMClientAsync Method waitFor(java.util.function.Supplier, int, int)\n```\n\n----------------------------------------\n\nTITLE: Missing DAO Method Documentation in AppAttemptInfo Class\nDESCRIPTION: Missing documentation for timing-related methods in AppAttemptInfo class that track attempt start and finish times.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.10.1/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/jdiff/xml/missingSinces.txt#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\ngetFinishedTime()\ngetStartedTime()\n```\n\n----------------------------------------\n\nTITLE: Markdown Header Configuration for Hadoop Mailing Lists Page\nDESCRIPTION: YAML front matter configuration for the mailing lists documentation page, defining the title and menu structure.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/mailing_lists.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Hadoop Mailing Lists\nmenu:\n   main:\n      name: \"Mailing lists\"\n      parent: \"community\"\n---\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for the release announcement page, specifying the title, date, and linked status.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/3.3.2.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Release 3.3.2 available\ndate: 2022-03-03\nlinked: false\n---\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter for Release Announcement\nDESCRIPTION: YAML front matter defining the title and date for the release announcement page.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/0.16.1.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: release 0.16.1 available\ndate: 2008-03-13\n---\n```\n\n----------------------------------------\n\nTITLE: HTML License Comment Block\nDESCRIPTION: HTML comment block containing the Apache License 2.0 header that specifies usage terms and conditions for the content.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/src/release/0.15.1.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!---\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n```\n\n----------------------------------------\n\nTITLE: License for org.apache.hadoop.util.bloom.* Classes\nDESCRIPTION: This snippet contains the license text for the org.apache.hadoop.util.bloom.* classes, which are part of the Apache Hadoop project but have a separate license. It outlines the conditions for redistribution and use of the software.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.4.1/mapreduce/LICENSE.txt#2025-04-08_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n/**\n *\n * Copyright (c) 2005, European Commission project OneLab under contract\n * 034819 (http://www.one-lab.org)\n * All rights reserved.\n * Redistribution and use in source and binary forms, with or \n * without modification, are permitted provided that the following \n * conditions are met:\n *  - Redistributions of source code must retain the above copyright \n *    notice, this list of conditions and the following disclaimer.\n *  - Redistributions in binary form must reproduce the above copyright \n *    notice, this list of conditions and the following disclaimer in \n *    the documentation and/or other materials provided with the distribution.\n *  - Neither the name of the University Catholique de Louvain - UCL\n *    nor the names of its contributors may be used to endorse or \n *    promote products derived from this software without specific prior \n *    written permission.\n *    \n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT \n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS \n * FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE \n * COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, \n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, \n * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER \n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT \n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN \n * ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n * POSSIBILITY OF SUCH DAMAGE.\n */\n```\n\n----------------------------------------\n\nTITLE: No code snippets found\nDESCRIPTION: This file contains only changelog entries and release notes without any actual code snippets.\nSOURCE: https://github.com/apache/hadoop-site/blob/asf-site/content/docs/r2.7.1/mapreduce/CHANGES.txt#2025-04-08_snippet_0\n\n"
  }
]
[
  {
    "owner": "themanojdesai",
    "repo": "python-a2a",
    "content": "TITLE: Testing Function Calling Capability of an AI Agent in Python\nDESCRIPTION: Defines a function 'test_function_calling' that tests an AI agent's ability to invoke registered functions in response to a user prompt. It constructs a user message, sends it to the agent, inspects the response for function call intents, executes the called function locally if recognized (currently supports 'get_weather'), sends the function result back to the agent, and prints the intermediate and final responses. The function supports dynamic handling of function calls and message roles. Dependencies include the agent's correct implementation of 'handle_message' and supporting message/response classes.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef test_function_calling(agent, prompt, agent_name=\"Agent\"):\n    \"\"\"Test an agent's function calling capability with a prompt.\"\"\"\n    print(f\"🔍 Testing {agent_name} function calling...\")\n    \n    # Create a message with the prompt\n    message = Message(\n        content=TextContent(text=prompt),\n        role=MessageRole.USER\n    )\n    \n    # Get a response from the agent\n    response = agent.handle_message(message)\n    \n    # Check if the response is a function call\n    if response.content.type == \"function_call\":\n        print(f\"\\n🤖 {agent_name} wants to call a function:\\n\")\n        print(f\"Function: {response.content.name}\")\n        print(\"Parameters:\")\n        for param in response.content.parameters:\n            print(f\"  {param.name}: {param.value}\")\n        \n        # Execute the function\n        if response.content.name == \"get_weather\":\n            params = {p.name: p.value for p in response.content.parameters}\n            result = get_weather(**params)\n            \n            # Create a function response\n            function_response = Message(\n                content=FunctionResponseContent(\n                    name=\"get_weather\",\n                    response=result\n                ),\n                role=MessageRole.AGENT,\n                parent_message_id=response.message_id,\n                conversation_id=response.conversation_id\n            )\n            \n            print(f\"\\n🔧 Function result:\\n\")\n            print(result)\n            \n            # Get the final response from the agent\n            final_response = agent.handle_message(function_response)\n            \n            print(f\"\\n🤖 {agent_name} final response:\\n\")\n            print(final_response.content.text)\n            \n            return final_response\n    else:\n        print(f\"\\n🤖 {agent_name} response (no function call):\\n\")\n        print(response.content.text)\n        \n    print(\"\\n\" + \"-\"*80 + \"\\n\")\n    \n    return response\n```\n\n----------------------------------------\n\nTITLE: Setting Up API Keys for LLM Providers\nDESCRIPTION: Retrieves API keys and credentials from environment variables for OpenAI, Anthropic, and AWS Bedrock. Includes validation to check if the necessary credentials are available and provides appropriate feedback.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Get API keys from environment\nopenai_api_key = os.environ.get(\"OPENAI_API_KEY\")\nanthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n\n# AWS credentials from environment variables - use the same method\naws_access_key_id = os.environ.get(\"AWS_ACCESS_KEY_ID\")\naws_secret_access_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\naws_region = os.environ.get(\"AWS_DEFAULT_REGION\")\n\n# Check if API keys are available\nif not openai_api_key:\n    print(\"⚠️ OpenAI API key not found. OpenAI examples will not work.\")\nelse:\n    print(\"✅ OpenAI API key found.\")\n    \nif not anthropic_api_key:\n    print(\"⚠️ Anthropic API key not found. Claude examples will not work.\")\nelse:\n    print(\"✅ Anthropic API key found.\")\n\nif not aws_access_key_id or not aws_secret_access_key or not aws_region:\n    print(\"⚠️ AWS credentials not found. Bedrock examples will not work.\")\n```\n\n----------------------------------------\n\nTITLE: Building a Multi-Agent Customer Support System with Python A2A\nDESCRIPTION: A comprehensive example that demonstrates creating a customer support system using Python A2A. It includes specialized MCP servers for product database functions, custom support agents, an agent network with routing capabilities, and an interactive workflow for handling customer inquiries with streaming responses.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, AgentNetwork, AIAgentRouter, Flow, StreamingClient\nfrom python_a2a.mcp import FastMCP, A2AMCPAgent, text_response\nimport asyncio\n\n# Create specialized MCP servers for different functions\nproduct_db_mcp = FastMCP(name=\"Product Database\")\n\n@product_db_mcp.tool()\nasync def search_products(query: str) -> dict:\n    \"\"\"Search for products in the database.\"\"\"\n    # In a real implementation, this would query a database\n    return {\"products\": [{\"id\": 101, \"name\": \"Super Laptop\", \"price\": 999.99}]}\n\n@product_db_mcp.tool()\nasync def get_product_details(product_id: int) -> dict:\n    \"\"\"Get detailed information about a product.\"\"\"\n    return {\n        \"id\": product_id,\n        \"name\": \"Super Laptop\",\n        \"description\": \"Powerful laptop with 16GB RAM and 512GB SSD\",\n        \"price\": 999.99,\n        \"availability\": \"In Stock\"\n    }\n\n# Create specialized agents for different functions\n@agent(name=\"Support Agent\", description=\"Customer support specialist\")\nclass SupportAgent(A2AServer, A2AMCPAgent):\n    def __init__(self):\n        A2AServer.__init__(self)\n        A2AMCPAgent.__init__(\n            self,\n            name=\"Support Agent\",\n            description=\"Handles customer inquiries\",\n            mcp_servers={\"products\": product_db_mcp}\n        )\n    \n    async def handle_task_async(self, task):\n        # Process customer query\n        text = task.message.get(\"content\", {}).get(\"text\", \"\")\n        \n        if \"product\" in text.lower():\n            # Search for products\n            search_results = await self.call_mcp_tool(\"products\", \"search_products\", query=text)\n            \n            if search_results.get(\"products\"):\n                product = search_results[\"products\"][0]\n                details = await self.call_mcp_tool(\"products\", \"get_product_details\", product_id=product[\"id\"])\n                \n                task.artifacts = [{\n                    \"parts\": [{\"type\": \"text\", \"text\": f\"I found this product: {details['name']}\\n\\n\"\n                                                     f\"Price: ${details['price']}\\n\"\n                                                     f\"Description: {details['description']}\\n\"\n                                                     f\"Availability: {details['availability']}\"}]\n                }]\n            else:\n                task.artifacts = [{\n                    \"parts\": [{\"type\": \"text\", \"text\": \"I couldn't find any products matching your query.\"}]\n                }]\n        else:\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": \"How can I help you with our products today?\"}]\n            }]\n        \n        return task\n\n# Create a network of agents\nasync def setup_agent_network():\n    network = AgentNetwork(name=\"Customer Support Network\")\n    \n    # Add agents to the network\n    network.add(\"support\", \"http://localhost:5001\")\n    network.add(\"billing\", \"http://localhost:5002\")\n    network.add(\"technical\", \"http://localhost:5003\")\n    \n    # Create a router\n    router = AIAgentRouter(\n        llm_client=network.get_agent(\"support\"),\n        agent_network=network\n    )\n    \n    # Define a workflow for handling customer inquiries\n    flow = Flow(agent_network=network, router=router, name=\"Customer Support Workflow\")\n    \n    # Route the initial query\n    flow.auto_route(\"{customer_query}\")\n    \n    # If the query is about a technical issue, follow up with specific questions\n    flow.if_contains(\"technical\")\n    flow.ask(\"technical\", \"What operating system are you using? Please provide details about {latest_result}\")\n    flow.else_if_contains(\"billing\")\n    flow.ask(\"billing\", \"Can you provide your order number related to {latest_result}\")\n    flow.else_branch()\n    flow.ask(\"support\", \"Thank you for your query. Let me check on {latest_result}\")\n    flow.end_if()\n    \n    return network, flow\n\n# Main customer support application\nasync def main():\n    # Set up the agent network and workflow\n    network, workflow = await setup_agent_network()\n    \n    # Create a streaming client to get real-time responses\n    client = StreamingClient(\"http://localhost:5001\")\n    \n    # Simulate a customer query\n    print(\"Customer: I need information about your laptop products\")\n    \n    # Stream the response\n    print(\"\\nSupport Agent (streaming):\")\n    \n    def print_chunk(chunk):\n        print(chunk, end=\"\", flush=True)\n    \n    # Create and stream a task\n    task = await client.create_task(\"I need information about your laptop products\")\n    \n    async for chunk in client.stream_task(task, chunk_callback=print_chunk):\n        pass  # Chunks are handled by the callback\n    \n    print(\"\\n\\nNext Steps:\")\n    \n    # Run the workflow with the customer query\n    result = await workflow.run({\n        \"customer_query\": \"I'm having technical problems with my Super Laptop\"\n    })\n    \n    print(result)\n\n# Run the application\nif __name__ == \"__main__\":\n    # In a real application, you would start each agent in a separate process\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI-Powered Agent\nDESCRIPTION: Initializes an agent using the OpenAI LLM provider with GPT-4 model. Configures key parameters like temperature and system prompt to control agent behavior and responses.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Skip this cell if OpenAI API key is not available\nif openai_api_key:\n    # Create an OpenAI-powered agent\n    openai_agent = OpenAIA2AServer(\n        api_key=openai_api_key,\n        model=\"gpt-4\",  # You can use \"gpt-3.5-turbo\" for faster, cheaper responses\n        temperature=0.7,\n        system_prompt=\"You are a helpful assistant that provides clear, concise answers.\"\n    )\n    print(\"OpenAI agent created successfully!\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple A2A Agent with Python\nDESCRIPTION: Defines a custom A2A server agent named \"Weather Agent\" that responds to weather-related queries. The snippet uses decorators to register the agent and a skill which returns mocked weather information for a given location. The class includes a handle_task method that parses incoming task messages, determines if a weather query is present, and sets task status and response artifacts accordingly. Dependencies include the python_a2a library and its components (A2AServer, agent, skill, TaskStatus, TaskState). The agent runs a server listening on port 5000 when executed as a script.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/quickstart.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, skill, agent, run_server\nfrom python_a2a import TaskStatus, TaskState\n\n@agent(\n    name=\"Weather Agent\",\n    description=\"Provides weather information\",\n    version=\"1.0.0\"\n)\nclass WeatherAgent(A2AServer):\n    \n    @skill(\n        name=\"Get Weather\",\n        description=\"Get current weather for a location\",\n        tags=[\"weather\", \"forecast\"]\n    )\n    def get_weather(self, location):\n        \"\"\"Get weather for a location.\"\"\"\n        # Mock implementation\n        return f\"It's sunny and 75°F in {location}\"\n    \n    def handle_task(self, task):\n        # Extract location from message\n        message_data = task.message or {}\n        content = message_data.get(\"content\", {})\n        text = content.get(\"text\", \"\") if isinstance(content, dict) else \"\"\n        \n        if \"weather\" in text.lower() and \"in\" in text.lower():\n            location = text.split(\"in\", 1)[1].strip().rstrip(\"?.\")\n            \n            # Get weather and create response\n            weather_text = self.get_weather(location)\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": weather_text}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n        else:\n            task.status = TaskStatus(\n                state=TaskState.INPUT_REQUIRED,\n                message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \n                        \"text\": \"Please ask about weather in a specific location.\"}}\n            )\n        return task\n\n# Run the server\nif __name__ == \"__main__\":\n    agent = WeatherAgent()\n    run_server(agent, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Creating Specialized OpenAI Agents with Custom System Prompts\nDESCRIPTION: Creates two specialized OpenAI agents: a technical documentation agent with lower temperature for precise responses and a creative writing agent with higher temperature for more imaginative content. Each agent has a tailored system prompt for its specific purpose.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Create specialized OpenAI agents (if API key is available)\nif openai_api_key:\n    # Technical documentation agent\n    technical_agent = OpenAIA2AServer(\n        api_key=openai_api_key,\n        model=\"gpt-4\",\n        temperature=0.2,  # Lower temperature for more deterministic responses\n        system_prompt=(\n            \"You are a technical documentation specialist. \"\n            \"Provide detailed, accurate explanations of technical concepts with examples where appropriate. \"\n            \"Use markdown formatting for clarity and structure.\"\n        )\n    )\n    \n    # Creative writing agent\n    creative_agent = OpenAIA2AServer(\n        api_key=openai_api_key,\n        model=\"gpt-4\",\n        temperature=0.9,  # Higher temperature for more creative responses\n        system_prompt=(\n            \"You are a creative writer and storyteller. \"\n            \"Generate engaging, imaginative content with rich descriptions and compelling narratives. \"\n            \"Be original and think outside the box.\"\n        )\n    )\n    \n    print(\"Specialized OpenAI agents created successfully!\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Greeting Agent with Python A2A\nDESCRIPTION: This code creates a basic A2A agent that responds to greetings. It defines a `GreetingAgent` class that inherits from `A2AServer` and uses the `@skill` decorator to define a `greet` skill. The `handle_task` method processes incoming messages, checks for greetings, and responds with a personalized message if a name is provided, otherwise a generic greeting is returned. It uses `TaskStatus` and `TaskState` from the `python_a2a` library. Finally, it runs the agent using `run_server`.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/simple.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, skill, agent, run_server\nfrom python_a2a import TaskStatus, TaskState\n\n@agent(\n    name=\"Greeting Agent\",\n    description=\"A simple agent that responds to greetings\",\n    version=\"1.0.0\"\n)\nclass GreetingAgent(A2AServer):\n    \n    @skill(\n        name=\"Greet\",\n        description=\"Respond to a greeting\",\n        tags=[\"greeting\", \"hello\"]\n    )\n    def greet(self, name=None):\n        \"\"\"Respond to a greeting with a friendly message.\"\"\"\n        if name:\n            return f\"Hello, {name}! How can I help you today?\"\n        else:\n            return \"Hello there! How can I help you today?\"\n    \n    def handle_task(self, task):\n        # Extract message text\n        message_data = task.message or {}\n        content = message_data.get(\"content\", {})\n        text = content.get(\"text\", \"\") if isinstance(content, dict) else \"\"\n        \n        # Check if it's a greeting\n        greeting_words = [\"hello\", \"hi\", \"hey\", \"greetings\"]\n        is_greeting = any(word in text.lower() for word in greeting_words)\n        \n        if is_greeting:\n            # Extract name if present\n            name = None\n            if \"my name is\" in text.lower():\n                name = text.lower().split(\"my name is\")[1].strip()\n            \n            # Create greeting response\n            greeting = self.greet(name)\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": greeting}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n        else:\n            # Default response\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": \"I'm a greeting agent. Try saying hello!\"}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n        \n        return task\n\n# Run the server\nif __name__ == \"__main__\":\n    agent = GreetingAgent()\n    run_server(agent, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Building an Agent Network with Multiple Agents and Routing\nDESCRIPTION: This snippet demonstrates how to create an agent network and add multiple agents to it. It includes the setup of an AI agent router to decide the appropriate agent based on a query. The code shows routing a question to the best agent, retrieving a response, and listing available agents, facilitating multi-agent orchestration.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom python_a2a import AgentNetwork, A2AClient, AIAgentRouter\n\n# Create an agent network\nnetwork = AgentNetwork(name=\"Travel Assistant Network\")\n\n# Add agents to the network\nnetwork.add(\"weather\", \"http://localhost:5001\")\nnetwork.add(\"hotels\", \"http://localhost:5002\")\nnetwork.add(\"attractions\", \"http://localhost:5003\")\n\n# Create a router to intelligently direct queries to the best agent\nrouter = AIAgentRouter(\n    llm_client=A2AClient(\"http://localhost:5000/openai\"),  # LLM for making routing decisions\n    agent_network=network\n)\n\n# Route a query to the appropriate agent\nagent_name, confidence = router.route_query(\"What's the weather like in Paris?\")\nprint(f\"Routing to {agent_name} with {confidence:.2f} confidence\")\n\n# Get the selected agent and ask the question\nagent = network.get_agent(agent_name)\nresponse = agent.ask(\"What's the weather like in Paris?\")\nprint(f\"Response: {response}\")\n\n# List all available agents\nprint(\"\\nAvailable Agents:\")\nfor agent_info in network.list_agents():\n    print(f\"- {agent_info['name']}: {agent_info['description']}\")\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI-Powered LLM Agent with Python\nDESCRIPTION: Shows how to instantiate an OpenAI-based A2A agent using the OpenAIA2AServer class. The agent is configured with an API key from environment variables, the model name (\"gpt-4\"), and a system prompt to guide its behavior. This snippet runs an A2A server hosting the LLM-powered agent on host 0.0.0.0 and port 5000. Dependencies include the python_a2a library and a valid OpenAI API key set as an environment variable.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/quickstart.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom python_a2a import OpenAIA2AServer, run_server\n\n# Create an agent powered by OpenAI\nagent = OpenAIA2AServer(\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n    model=\"gpt-4\",\n    system_prompt=\"You are a helpful AI assistant specialized in explaining complex topics simply.\"\n)\n\n# Run the server\nif __name__ == \"__main__\":\n    run_server(agent, host=\"0.0.0.0\", port=5000)\n```\n\n----------------------------------------\n\nTITLE: Building a Conversation\nDESCRIPTION: This code demonstrates building a conversation using the `Conversation` class and the `add_message` method. It adds different message types (text, function call, function response) to a conversation. It includes an agent's text response as well, setting `parent_message_id` and `conversation_id` to establish the message chain.  `pretty_print_conversation` formats and displays the resulting conversation.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create a new conversation\nconversation = Conversation()\n\n# Add messages to the conversation\nconversation.add_message(text_message)\nconversation.add_message(function_call)\nconversation.add_message(function_response)\n\n# Add an agent's text response\nweather_response = Message(\n    content=TextContent(\n        text=\"The weather in New York is currently 22°C (71.6°F) and partly cloudy. \"\n             \"The humidity is 65% with wind speeds of 8 km/h.\"\n    ),\n    role=MessageRole.AGENT,\n    parent_message_id=function_response.message_id,\n    conversation_id=conversation.conversation_id\n)\nconversation.add_message(weather_response)\n\n# Display the conversation\npretty_print_conversation(conversation)\n```\n\n----------------------------------------\n\nTITLE: Install Python A2A with OpenAI Integration - Bash\nDESCRIPTION: Installs `python-a2a` and its optional dependencies needed for integrating with OpenAI services. This uses pip with the `[openai]` extra.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install \"python-a2a[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic A2A Server in Python\nDESCRIPTION: Illustrates creating a simple A2A server by inheriting from `A2AServer` and implementing the `handle_task` method. The example extracts text from the incoming task's message, crafts a simple response, adds it as an artifact, sets the task status to `COMPLETED`, and returns the task. Requires the `python_a2a` library.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/basics.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, TaskStatus, TaskState\n\nclass WeatherAgent(A2AServer):\n    def handle_task(self, task):\n        # Extract message text\n        message_data = task.message or {}\n        content = message_data.get(\"content\", {}) if isinstance(message_data, dict) else {}\n        text = content.get(\"text\", \"\") if isinstance(content, dict) else \"\"\n        \n        # Respond to the message\n        response_text = f\"It's sunny today!\"\n        \n        # Create artifact with response\n        task.artifacts = [{\n            \"parts\": [{\"type\": \"text\", \"text\": response_text}]\n        }]\n        \n        # Mark as completed\n        task.status = TaskStatus(state=TaskState.COMPLETED)\n        \n        return task\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses for Real-time Interaction with Python A2A\nDESCRIPTION: This snippet illustrates how to use the StreamingClient to stream responses from the A2A server in real-time. It demonstrates setting up message content, handling streamed chunks with callback functions, and managing task streaming. It emphasizes asynchronous programming for responsive updates in AI agent interactions.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport asyncio\nfrom python_a2a import StreamingClient, Message, TextContent, MessageRole\nfrom python_a2a import Task, TaskStatus, TaskState\n\nasync def main():\n    # Create a streaming client\n    client = StreamingClient(\"http://localhost:5000\")\n    \n    # Stream a simple message\n    message = Message(\n        content=TextContent(text=\"Write a short story about space exploration\"),\n        role=MessageRole.USER\n    )\n    \n    print(\"Streaming response:\")\n    print(\"-\" * 50)\n    \n    # Define a callback function to process chunks\n    def print_chunk(chunk):\n        print(chunk, end=\"\", flush=True)\n    \n    # Stream the response with the callback\n    async for chunk in client.stream_response(message, chunk_callback=print_chunk):\n        pass  # Chunks are handled by the callback\n    \n    print(\"\\n\" + \"-\" * 50)\n    \n    # Alternatively, create and stream a task\n    task = await client.create_task(\"Explain quantum computing in simple terms\")\n    \n    print(\"\\nStreaming task response:\")\n    print(\"-\" * 50)\n    \n    # Stream the task execution\n    async for chunk in client.stream_task(task, chunk_callback=lambda c: print(c.get(\"text\", \"\"), end=\"\", flush=True)):\n        pass\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Conditional Creation of Function-Enabled OpenAI Agent in Python\nDESCRIPTION: Instantiates a function-enabled OpenAI agent using the 'OpenAIA2AServer' class if an API key is available. The agent is configured with the GPT-4 model, temperature setting, a system prompt guiding agent behavior, and the weather function to enable function calling. The snippet checks for 'openai_api_key' and logs success upon creation. This setup prepares the agent for handling user prompts and invoking the weather function dynamically. Dependencies include valid OpenAI API credentials and the corresponding class implementations.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nif openai_api_key:\n    function_agent = OpenAIA2AServer(\n        api_key=openai_api_key,\n        model=\"gpt-4\",\n        temperature=0.7,\n        system_prompt=\"You are a helpful assistant that can provide weather information.\",\n        functions=[weather_function]\n    )\n    print(\"Function-enabled OpenAI agent created successfully!\")\n```\n\n----------------------------------------\n\nTITLE: Creating A2A Agent with Inline MCP Server\nDESCRIPTION: This code demonstrates creating an A2A agent with an embedded MCP server. It first defines an MCP server using `FastMCP` and adds an `add` tool.  Then, it initializes an `A2AMCPAgent` with this `calculator_mcp` server. This eliminates the need for a separate MCP server process, allowing the agent to handle calculations directly without external dependencies.  The rest of the agent implementation would be similar to the previous example, but the MCP server is initialized internally.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/mcp.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, A2AMCPAgent, AgentCard, run_server\nfrom python_a2a.mcp import FastMCP, text_response\n\n# Create MCP server\ncalculator_mcp = FastMCP(\n    name=\"Calculator MCP\",\n    description=\"Provides calculation functions\"\n)\n\n@calculator_mcp.tool()\ndef add(a: float, b: float) -> float:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\n# Create A2A agent with inline MCP server\nclass CalculatorAgent(A2AServer, A2AMCPAgent):\n    def __init__(self):\n        # Create the agent card\n        agent_card = AgentCard(\n            name=\"Calculator Agent\",\n            description=\"An agent that performs calculations\",\n            url=\"http://localhost:5000\",\n            version=\"1.0.0\"\n        )\n        \n        # Initialize A2AServer\n        A2AServer.__init__(self, agent_card=agent_card)\n        \n        # Initialize A2AMCPAgent with inline MCP server\n        A2AMCPAgent.__init__(\n            self, \n            name=\"Calculator Agent\",\n            description=\"An agent that performs calculations\",\n            mcp_servers={\"calc\": calculator_mcp}\n        )\n    \n    # ... rest of the implementation\n```\n\n----------------------------------------\n\nTITLE: Using Decorators to Define an A2A Calculator Agent in Python\nDESCRIPTION: Illustrates creating an A2A server agent named \"Calculator\" using decorators. The agent defines an \"Add\" skill that accepts two parameters and returns their sum as a float. The class stub includes a handle_task method placeholder for future task handling logic. Upon execution, the agent runs a server on port 5000. Requires the python_a2a library and its agent and skill decorators along with the base A2AServer class.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/quickstart.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import agent, skill, A2AServer, run_server\nfrom python_a2a import TaskStatus, TaskState\n\n@agent(\n    name=\"Calculator\",\n    description=\"Performs calculations\",\n    version=\"1.0.0\"\n)\nclass CalculatorAgent(A2AServer):\n    \n    @skill(\n        name=\"Add\",\n        description=\"Add two numbers\",\n        tags=[\"math\", \"addition\"]\n    )\n    def add(self, a, b):\n        \"\"\"\n        Add two numbers together.\n        \n        Examples:\n            \"What is 5 + 3?\"\n            \"Add 10 and 20\"\n        \"\"\"\n        return float(a) + float(b)\n    \n    def handle_task(self, task):\n        # Implementation details...\n        pass\n\n# Run the server\nif __name__ == \"__main__\":\n    calculator = CalculatorAgent()\n    run_server(calculator, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Calculator Agent with Python A2A\nDESCRIPTION: This code creates a simple calculator agent that can add, subtract, multiply, and divide two numbers. It defines a `CalculatorAgent` class that inherits from `A2AServer` and uses the `@skill` decorator to define the `add`, `subtract`, `multiply`, and `divide` skills. The `handle_task` method extracts numbers from the input text using regular expressions and performs the requested operation based on keywords found in the text. It uses `TaskStatus` and `TaskState` from the `python_a2a` library. The regular expression module `re` is required for extracting number from the input text.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/simple.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, skill, agent, run_server\nfrom python_a2a import TaskStatus, TaskState\nimport re\n\n@agent(\n    name=\"Calculator\",\n    description=\"A simple calculator agent\",\n    version=\"1.0.0\"\n)\nclass CalculatorAgent(A2AServer):\n    \n    @skill(\n        name=\"Add\",\n        description=\"Add two numbers\",\n        tags=[\"math\", \"addition\"]\n    )\n    def add(self, a, b):\n        \"\"\"Add two numbers together.\"\"\"\n        return float(a) + float(b)\n    \n    @skill(\n        name=\"Subtract\",\n        description=\"Subtract two numbers\",\n        tags=[\"math\", \"subtraction\"]\n    )\n    def subtract(self, a, b):\n        \"\"\"Subtract b from a.\"\"\"\n        return float(a) - float(b)\n    \n    @skill(\n        name=\"Multiply\",\n        description=\"Multiply two numbers\",\n        tags=[\"math\", \"multiplication\"]\n    )\n    def multiply(self, a, b):\n        \"\"\"Multiply two numbers together.\"\"\"\n        return float(a) * float(b)\n    \n    @skill(\n        name=\"Divide\",\n        description=\"Divide two numbers\",\n        tags=[\"math\", \"division\"]\n    )\n    def divide(self, a, b):\n        \"\"\"Divide a by b.\"\"\"\n        return float(a) / float(b)\n    \n    def handle_task(self, task):\n        # Extract message text\n        message_data = task.message or {}\n        content = message_data.get(\"content\", {})\n        text = content.get(\"text\", \"\") if isinstance(content, dict) else \"\"\n        \n        # Find numbers in the text\n        numbers = [float(n) for n in re.findall(r\"[-+]?\\d*\\.?\\d+\", text)]\n        \n        # Default response\n        response_text = \"I can add, subtract, multiply, and divide numbers. Try asking something like 'add 5 and 3' or '10 divided by 2'.\"\n        \n        # Check for operation keywords\n        if len(numbers) >= 2:\n            a, b = numbers[0], numbers[1]\n            \n            if any(word in text.lower() for word in [\"add\", \"plus\", \"sum\", \"+\"]):\n                result = self.add(a, b)\n                response_text = f\"{a} + {b} = {result}\"\n            elif any(word in text.lower() for word in [\"subtract\", \"minus\", \"difference\", \"-\"]):\n                result = self.subtract(a, b)\n                response_text = f\"{a} - {b} = {result}\"\n            elif any(word in text.lower() for word in [\"multiply\", \"times\", \"product\", \"*\", \"x\"]):\n                result = self.multiply(a, b)\n                response_text = f\"{a} × {b} = {result}\"\n            elif any(word in text.lower() for word in [\"divide\", \"quotient\", \"/\"]):\n                if b != 0:\n                    result = self.divide(a, b)\n                    response_text = f\"{a} ÷ {b} = {result}\"\n                else:\n                    response_text = \"Cannot divide by zero.\"\n        \n        # Create response artifact\n        task.artifacts = [{\n            \"parts\": [{\"type\": \"text\", \"text\": response_text}]\n        }]\n        task.status = TaskStatus(state=TaskState.COMPLETED)\n        \n        return task\n\n# Run the server\nif __name__ == \"__main__\":\n    agent = CalculatorAgent()\n    run_server(agent, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Testing Specialized OpenAI Agents with Same Prompt\nDESCRIPTION: Tests both the technical and creative specialized OpenAI agents with the same prompt about AI agents working together. Shows how different system prompts and temperature settings affect the responses to identical input.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Test specialized agents (if API key is available)\nif openai_api_key:\n    prompt = \"Write about artificial intelligence agents working together.\"\n    \n    technical_response = test_agent(technical_agent, prompt, \"Technical Documentation Agent\")\n    creative_response = test_agent(creative_agent, prompt, \"Creative Writing Agent\")\n```\n\n----------------------------------------\n\nTITLE: Creating MCP Server with FastMCP\nDESCRIPTION: This code creates an MCP server using the `FastMCP` class.  It defines two tools, `add` and `multiply`, which are exposed through the MCP.  The `FastMCP` server is initialized with a name and description. The `@calculator_mcp.tool()` decorator registers functions as callable tools. The server is then run, listening for connections on a specified host and port.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/mcp.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a.mcp import FastMCP, text_response\n\n# Create an MCP server\ncalculator_mcp = FastMCP(\n    name=\"Calculator MCP\",\n    description=\"Provides calculation functions\"\n)\n\n# Add a tool\n@calculator_mcp.tool()\ndef add(a: float, b: float) -> float:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\n# Add another tool\n@calculator_mcp.tool()\ndef multiply(a: float, b: float) -> float:\n    \"\"\"Multiply two numbers together.\"\"\"\n    return a * b\n\n# Run the server\nif __name__ == \"__main__\":\n    calculator_mcp.run(host=\"0.0.0.0\", port=5001)\n```\n\n----------------------------------------\n\nTITLE: Install Python A2A Core - Bash\nDESCRIPTION: Installs the base `python-a2a` package via pip, providing core functionality with the `requests` library as the primary dependency. This is the minimal installation required to use the library.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install python-a2a\n```\n\n----------------------------------------\n\nTITLE: Running the Python A2A SmartAssistant Server\nDESCRIPTION: This block serves as the entry point for the script when run directly. It checks for the presence of the `OPENAI_API_KEY` environment variable, which is required for initializing the `SmartAssistant`. If the key is found, it creates an instance of the agent and starts the A2A server using the `run_server` utility function, hosting the agent on port 5000.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n    \n    agent = SmartAssistant(api_key)\n    run_server(agent, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Testing Function\nDESCRIPTION: Defines a utility function to test LLM agents with a given prompt. The function creates a user message, gets a response from the agent, and displays the result with formatted output.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef test_agent(agent, prompt, agent_name=\"Agent\"):\n    \"\"\"Test an agent with a prompt and print the response.\"\"\"\n    print(f\"🔍 Testing {agent_name}...\")\n    \n    # Create a message with the prompt\n    message = Message(\n        content=TextContent(text=prompt),\n        role=MessageRole.USER\n    )\n    \n    # Get a response from the agent\n    response = agent.handle_message(message)\n    \n    print(f\"\\n🤖 {agent_name} response:\\n\")\n    print(response.content.text)\n    print(\"\\n\" + \"-\"*80 + \"\\n\")\n    \n    return response\n```\n\n----------------------------------------\n\nTITLE: Creating an Echo Agent\nDESCRIPTION: This snippet defines a simple `EchoAgent` class that inherits from `A2AServer`. The agent's `handle_message()` method processes incoming messages. It checks the message type, and returns a corresponding `Message` object with an echoed text content or a type description, including the original `message_id` and `conversation_id` to keep context. This is a basic example to demonstrate agent creation.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass EchoAgent(A2AServer):\n    \"\"\"A simple agent that echoes back messages.\"\"\"\n    \n    def handle_message(self, message):\n        \"\"\"Process incoming A2A messages.\"\"\"\n        if message.content.type == \"text\":\n            return Message(\n                content=TextContent(text=f\"Echo: {message.content.text}\"),\n                role=MessageRole.AGENT,\n                parent_message_id=message.message_id,\n                conversation_id=message.conversation_id\n            )\n        else:\n            return Message(\n                content=TextContent(text=f\"Received a {message.content.type} message type\"),\n                role=MessageRole.AGENT,\n                parent_message_id=message.message_id,\n                conversation_id=message.conversation_id\n            )\n\n# Create an instance of the echo agent\necho_agent = EchoAgent()\n```\n\n----------------------------------------\n\nTITLE: Using the A2A Client in Python\nDESCRIPTION: Demonstrates how to interact with an A2A server using the `A2AClient` from the `python_a2a` library. The example creates a client, sends a question to the specified server URL using the `ask` method, and prints the server's response. This assumes an A2A server is running at the specified URL.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/basics.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AClient\n\n# Create a client\nclient = A2AClient(\"http://localhost:5000\")\n\n# Send a message\nresponse = client.ask(\"What's the weather in Paris?\")\n\n# Print the response\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Defining Calculation Tools - Python\nDESCRIPTION: This code sets up calculation tools that will be used by the LLM agent via MCP. It defines various tools for common arithmetic operations like addition, subtraction, multiplication, and division. These tools are decorated with `@calculator_mcp.tool()`, which registers them with the MCP server.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    # llm_mcp_agent.py\n    import os\n    from python_a2a import A2AServer, A2AMCPAgent, run_server, AgentCard\n    from python_a2a import OpenAIA2AServer, TaskStatus, TaskState\n    from python_a2a.mcp import FastMCP, text_response\n    \n    # Create MCP server with calculation tools\n    calculator_mcp = FastMCP(\n        name=\"Calculator MCP\",\n        description=\"Provides calculation functions\"\n    )\n    \n    @calculator_mcp.tool()\n    def add(a: float, b: float) -> float:\n        \"\"\"Add two numbers together.\"\"\"\n        return a + b\n    \n    @calculator_mcp.tool()\n    def subtract(a: float, b: float) -> float:\n        \"\"\"Subtract b from a.\"\"\"\n        return a - b\n    \n    @calculator_mcp.tool()\n    def multiply(a: float, b: float) -> float:\n        \"\"\"Multiply two numbers together.\"\"\"\n        return a * b\n    \n    @calculator_mcp.tool()\n    def divide(a: float, b: float) -> float:\n        \"\"\"Divide a by b.\"\"\"\n        if b == 0:\n            return \"Cannot divide by zero\"\n        return a / b\n```\n\n----------------------------------------\n\nTITLE: Creating A2A Agent with MCP Capabilities\nDESCRIPTION: This code shows how to create an A2A agent (`CalculatorAgent`) that utilizes MCP tools. It initializes an `A2AMCPAgent` with a mapping to an MCP server. It overrides the `handle_task_async` method to process incoming tasks. Within the handler, it extracts numbers from the task's message content, calls the `add` tool via `call_mcp_tool`, and constructs a response. Error handling is included to catch and report any exceptions.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/mcp.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, A2AMCPAgent, AgentCard, run_server\nfrom python_a2a import TaskStatus, TaskState\n\n# Create an A2A agent with MCP capabilities\nclass CalculatorAgent(A2AServer, A2AMCPAgent):\n    def __init__(self):\n        # Create the agent card\n        agent_card = AgentCard(\n            name=\"Calculator Agent\",\n            description=\"An agent that performs calculations\",\n            url=\"http://localhost:5000\",\n            version=\"1.0.0\"\n        )\n        \n        # Initialize A2AServer\n        A2AServer.__init__(self, agent_card=agent_card)\n        \n        # Initialize A2AMCPAgent with MCP servers\n        A2AMCPAgent.__init__(\n            self, \n            name=\"Calculator Agent\",\n            description=\"An agent that performs calculations\",\n            mcp_servers={\"calc\": \"http://localhost:5001\"}\n        )\n    \n    async def handle_task_async(self, task):\n        try:\n            # Extract message text\n            text = task.message.get(\"content\", {}).get(\"text\", \"\")\n            \n            if \"add\" in text.lower():\n                # Extract numbers\n                import re\n                numbers = [float(n) for n in re.findall(r\"[-+]?\\d*\\.?\\d+\", text)]\n                \n                if len(numbers) >= 2:\n                    # Call MCP tool\n                    result = await self.call_mcp_tool(\"calc\", \"add\", a=numbers[0], b=numbers[1])\n                    \n                    # Create response\n                    task.artifacts = [{\n                        \"parts\": [{\"type\": \"text\", \"text\": f\"The sum is {result}\"}]\n                    }]\n                    task.status = TaskStatus(state=TaskState.COMPLETED)\n                    return task\n            \n            # Default response\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": \"I can help with calculations.\"}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n            return task\n            \n        except Exception as e:\n            # Handle errors\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": f\"Error: {str(e)}\"}]\n            }]\n            task.status = TaskStatus(state=TaskState.FAILED)\n            return task\n    \n    def handle_task(self, task):\n        # Convert sync to async\n        import asyncio\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(self.handle_task_async(task))\n\n# Run the agent\nif __name__ == \"__main__\":\n    agent = CalculatorAgent()\n    run_server(agent, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Creating a Function Call Message\nDESCRIPTION: This code demonstrates how to create a function call message using the `Message` class and `FunctionCallContent`. It defines the function name and includes a list of `FunctionParameter` objects, each with a name and value. The message role is set to `AGENT`. The `pretty_print_message()` function displays the generated message.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a function call message\nfunction_call = Message(\n    content=FunctionCallContent(\n        name=\"get_weather\",\n        parameters=[\n            FunctionParameter(name=\"location\", value=\"New York\"),\n            FunctionParameter(name=\"unit\", value=\"celsius\")\n        ]\n    ),\n    role=MessageRole.AGENT\n)\n\n# Display the function call\npretty_print_message(function_call)\n```\n\n----------------------------------------\n\nTITLE: Implementing the UI Agent - Python\nDESCRIPTION: This code defines the User Interface Agent (AssistantAgent) which acts as a central point of interaction. It uses other agents (weather and travel) to handle user requests. It routes user input (text) to the appropriate agent based on keywords.  It connects to the weather and travel agents on specified ports and creates a formatted response.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    # ui_agent.py\n    from python_a2a import A2AServer, skill, agent, run_server, A2AClient\n    from python_a2a import TaskStatus, TaskState\n    \n    @agent(\n        name=\"Travel Assistant\",\n        description=\"Your personal travel assistant\",\n        version=\"1.0.0\"\n    )\n    class AssistantAgent(A2AServer):\n        \n        def __init__(self):\n            super().__init__()\n            # Connect to other agents\n            self.weather_client = A2AClient(\"http://localhost:5001\")\n            self.travel_client = A2AClient(\"http://localhost:5002\")\n        \n        def handle_task(self, task):\n            # Extract message text\n            message_data = task.message or {}\n            content = message_data.get(\"content\", {})\n            text = content.get(\"text\", \"\") if isinstance(content, dict) else \"\"\n            \n            # Initialize response\n            response_text = \"I'm your travel assistant. I can help with weather information and travel recommendations.\"\n            \n            # Determine which agent to route to\n            if \"weather\" in text.lower() or \"forecast\" in text.lower() or \"temperature\" in text.lower():\n                # Route to weather agent\n                response_text = self.weather_client.ask(text)\n            elif \"recommend\" in text.lower() or \"suggest\" in text.lower() or \"destination\" in text.lower() or \"where should\" in text.lower():\n                # Route to travel agent\n                response_text = self.travel_client.ask(text)\n            elif text.lower() in [\"hi\", \"hello\", \"hey\"]:\n                # Greeting\n                response_text = \"Hello! I'm your travel assistant. I can help with weather information and travel recommendations. Try asking about the weather in a city or for recommendations for warm places with beaches.\"\n            elif \"help\" in text.lower() or \"what can you do\" in text.lower():\n                # Help message\n                response_text = \"\"\"I can help you with:\n                \n                1. Weather information: \\\"What's the weather in Paris?\\\" or \\\"Get me the forecast for Tokyo\\\"\n                2. Travel recommendations: \\\"Recommend warm destinations\\\" or \\\"Suggest cool places with museums\\\"\n                \n                Just let me know what you're interested in!\"\"\"\n            \n            # Create response artifact\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": response_text}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n            \n            return task\n    \n    # Run the server\n    if __name__ == \"__main__\":\n        agent = AssistantAgent()\n        run_server(agent, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Install Python A2A with Flask Support - Bash\nDESCRIPTION: Installs `python-a2a` along with the extra dependencies required for Flask-based server support. This method uses pip with the `[server]` extra.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"python-a2a[server]\"\n```\n\n----------------------------------------\n\nTITLE: Simulating Client-Server Interaction\nDESCRIPTION: This code simulates the interaction between a client and the EchoAgent server. It defines the `simulate_client_server` function, which takes an agent and a message as input. It prints the client's outgoing message, calls the agent's `handle_message` method, and then prints the response.  The function is used to demonstrate a client sending a message and the agent's response.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef simulate_client_server(agent, message):\n    \"\"\"Simulate a client sending a message to a server.\"\"\"\n    print(\"Client sending message:\")\n    pretty_print_message(message)\n    \n    print(\"\\nServer processing message...\\n\")\n    response = agent.handle_message(message)\n    \n    print(\"Client received response:\")\n    pretty_print_message(response)\n    \n    return response\n```\n\n----------------------------------------\n\nTITLE: Creating an LLM-Based Agent using OpenAI API with Python A2A\nDESCRIPTION: This code creates an LLM-based agent using OpenAI's API and the `OpenAIA2AServer` class from the `python_a2a` library. It retrieves the OpenAI API key from the environment variable `OPENAI_API_KEY`.  It then initializes an `OpenAIA2AServer` instance with the API key, model name (`gpt-4`), and a system prompt to define the agent's role. Finally, it runs the agent using `run_server`. Requires the `os` module to access environment variables.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/simple.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom python_a2a import OpenAIA2AServer, run_server\n\n# Get API key from environment variable\napi_key = os.environ.get(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n\n# Create an OpenAI-based A2A agent\nagent = OpenAIA2AServer(\n    api_key=api_key,\n    model=\"gpt-4\",\n    system_prompt=\"You are a helpful assistant that specializes in explaining complex concepts simply.\"\n)\n\n# Run the server\nif __name__ == \"__main__\":\n    print(\"Starting OpenAI-based A2A agent...\")\n    run_server(agent, host=\"0.0.0.0\", port=5000)\n```\n\n----------------------------------------\n\nTITLE: Defining a Mock Weather Function with Parameters in Python\nDESCRIPTION: Defines a Python function 'get_weather' that simulates retrieving weather data for a given city with support for temperature unit conversion between Celsius and Fahrenheit. It uses internal mock data and accepts 'location' and optional 'unit' parameters. The function returns weather details including temperature, conditions, humidity, wind speed, and location, with unit-adjusted temperature values. Dependencies include Python's built-in data structures and basic arithmetic operations.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef get_weather(location, unit=\"celsius\"):\n    \"\"\"Simulate getting weather data for a location.\"\"\"\n    # In a real application, this would call a weather API\n    # For this example, we'll use mock data\n    weather_data = {\n        \"New York\": {\"temperature\": 22, \"conditions\": \"Partly Cloudy\", \"humidity\": 65, \"wind_speed\": 8},\n        \"London\": {\"temperature\": 18, \"conditions\": \"Rainy\", \"humidity\": 80, \"wind_speed\": 12},\n        \"Tokyo\": {\"temperature\": 26, \"conditions\": \"Sunny\", \"humidity\": 70, \"wind_speed\": 5},\n        \"Sydney\": {\"temperature\": 28, \"conditions\": \"Clear\", \"humidity\": 55, \"wind_speed\": 10},\n        \"Paris\": {\"temperature\": 20, \"conditions\": \"Cloudy\", \"humidity\": 60, \"wind_speed\": 7}\n    }\n    \n    # Get the weather data for the location (default to New York if not found)\n    location_data = weather_data.get(location, weather_data[\"New York\"])\n    \n    # Convert temperature if necessary\n    if unit.lower() == \"fahrenheit\":\n        location_data[\"temperature\"] = location_data[\"temperature\"] * 9/5 + 32\n        location_data[\"unit\"] = \"fahrenheit\"\n    else:\n        location_data[\"unit\"] = \"celsius\"\n    \n    # Add the location to the data\n    location_data[\"location\"] = location\n    \n    return location_data\n```\n\n----------------------------------------\n\nTITLE: Install Python A2A with AWS Bedrock - Bash\nDESCRIPTION: Installs `python-a2a` including the optional dependencies for AWS Bedrock integration. This installation uses pip with the `[bedrock]` extra.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"python-a2a[bedrock]\"\n```\n\n----------------------------------------\n\nTITLE: Writing Unit Tests for Python A2A Agents\nDESCRIPTION: Demonstrates how to create comprehensive unit tests for A2A agents, including testing message handling and task processing. Covers setting up test fixtures, creating test messages, and validating agent responses for different interaction types.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\nfrom python_a2a import Message, TextContent, MessageRole\nfrom your_project import YourAgent\n\nclass TestYourAgent(unittest.TestCase):\n    def setUp(self):\n        # Create the agent\n        self.agent = YourAgent()\n    \n    def test_greeting(self):\n        # Create a greeting message\n        message = Message(\n            content=TextContent(text=\"Hello\"),\n            role=MessageRole.USER\n        )\n        \n        # Get the response\n        response = self.agent.handle_message(message)\n        \n        # Check the response\n        self.assertEqual(response.role, MessageRole.AGENT)\n        self.assertEqual(response.content.type, \"text\")\n        self.assertIn(\"hello\", response.content.text.lower())\n    \n    def test_task_handling(self):\n        # Create a task\n        from python_a2a import Task\n        \n        task = Task(\n            message={\n                \"content\": {\n                    \"type\": \"text\",\n                    \"text\": \"Hello\"\n                },\n                \"role\": \"user\"\n            }\n        )\n        \n        # Get the response\n        response = self.agent.handle_task(task)\n        \n        # Check the response\n        self.assertEqual(response.status.state, \"completed\")\n        self.assertTrue(response.artifacts)\n        self.assertEqual(response.artifacts[0][\"parts\"][0][\"type\"], \"text\")\n        self.assertIn(\"hello\", response.artifacts[0][\"parts\"][0][\"text\"].lower())\n\nif __name__ == \"__main__\":\n    unittest.main()\n```\n\n----------------------------------------\n\nTITLE: Handling Function Calls (Python)\nDESCRIPTION: This code shows how an agent processes function calls.  It retrieves the function name and parameters from the message content. Based on the function name, it simulates an action, creates a response with the function's result and sets the role.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, FunctionResponseContent, Message, MessageRole\n    \n    class WeatherAgent(A2AServer):\n        def handle_message(self, message):\n            # Check if this is a function call\n            if message.content.type == \"function_call\":\n                # Get function name\n                function_name = message.content.name\n                \n                # Get parameters\n                parameters = {p.name: p.value for p in message.content.parameters}\n                \n                # Handle based on function name\n                if function_name == \"get_weather\":\n                    location = parameters.get(\"location\", \"\")\n                    unit = parameters.get(\"unit\", \"celsius\")\n                    \n                    # Mock weather data\n                    weather_data = {\"temp\": 72, \"condition\": \"Sunny\"}\n                    \n                    # Return function response\n                    return Message(\n                        content=FunctionResponseContent(\n                            name=\"get_weather\",\n                            response=weather_data\n                        ),\n                        role=MessageRole.AGENT,\n                        parent_message_id=message.message_id,\n                        conversation_id=message.conversation_id\n                    )\n            \n            # Default response for non-function calls\n            return super().handle_message(message)\n```\n\n----------------------------------------\n\nTITLE: Creating Specialized AWS Bedrock Agents with Custom System Prompts\nDESCRIPTION: Creates two specialized AWS Bedrock agents: a technical documentation agent with lower temperature and a creative writing agent with higher temperature. Each agent has a custom system prompt tailored to its specific role and output style.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Create specialized Bedrock agents (if AWS credentials are available)\nif aws_access_key_id and aws_secret_access_key and aws_region:\n    # Technical documentation agent\n    technical_agent = BedrockA2AServer(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        aws_region=aws_region,\n        temperature=0.2,  # Lower temperature for more deterministic responses\n        system_prompt=(\n            \"You are a technical documentation specialist. \"\n            \"Provide detailed, accurate explanations of technical concepts with examples where appropriate. \"\n            \"Use markdown formatting for clarity and structure.\"\n        )\n    )\n    \n    # Creative writing agent\n    creative_agent = BedrockA2AServer(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        aws_region=aws_region,\n        temperature=0.9,  # Higher temperature for more creative responses\n        system_prompt=(\n            \"You are a creative writer and storyteller. \"\n            \"Generate engaging, imaginative content with rich descriptions and compelling narratives. \"\n            \"Be original and think outside the box.\"\n        )\n    )\n    \n    print(\"Specialized Bedrock agents created successfully!\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Function Response Message\nDESCRIPTION: This snippet shows how to create a function response message using `Message` and `FunctionResponseContent`. It includes the function name, the response data, the message role, and the `parent_message_id` to link it to the original function call. `pretty_print_message()` formats the message for display.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create a function response message\nfunction_response = Message(\n    content=FunctionResponseContent(\n        name=\"get_weather\",\n        response={\n            \"temperature\": 22,\n            \"conditions\": \"Partly Cloudy\",\n            \"humidity\": 65,\n            \"wind_speed\": 8\n        }\n    ),\n    role=MessageRole.AGENT,\n    parent_message_id=function_call.message_id\n)\n\n# Display the function response\npretty_print_message(function_response)\n```\n\n----------------------------------------\n\nTITLE: Conditional Creation of Function-Enabled AWS Bedrock Agent in Python\nDESCRIPTION: Creates a function-enabled agent instance for AWS Bedrock using 'BedrockA2AServer' if AWS credentials and region are available. This setup configures the agent with access keys, region, temperature, a system prompt, and the weather function enabling function calls. The agent can process prompts and perform function execution similarly to the OpenAI agent, enabling multi-provider interoperability. Dependencies include valid AWS credentials and Bedrock SDK integration.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nif aws_access_key_id and aws_secret_access_key and aws_region:\n    function_agent = BedrockA2AServer(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        aws_region=aws_region,\n        temperature=0.7,\n        system_prompt=\"You are a helpful assistant that can provide weather information.\",\n        functions=[weather_function]\n    )\n```\n\n----------------------------------------\n\nTITLE: Building Conversation with Helpers\nDESCRIPTION: This snippet shows an alternative way to build conversations using helper methods like `create_text_message` and `create_function_call` provided by the `Conversation` class. It demonstrates how to chain messages in the conversation, similar to the prior example, by setting `parent_message_id`. The conversation is displayed using `pretty_print_conversation`.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create a new conversation\nconversation2 = Conversation()\n\n# Add a user message\nmsg1 = conversation2.create_text_message(\n    text=\"What's the weather like in New York?\",\n    role=MessageRole.USER\n)\n\n# Add a function call\nmsg2 = conversation2.create_function_call(\n    name=\"get_weather\",\n    parameters=[\n        {\"name\": \"location\", \"value\": \"New York\"},\n        {\"name\": \"unit\", \"value\": \"celsius\"}\n    ],\n    role=MessageRole.AGENT,\n    parent_message_id=msg1.message_id\n)\n\n# Add a function response\nmsg3 = conversation2.create_function_response(\n    name=\"get_weather\",\n    response={\n        \"temperature\": 22,\n        \"conditions\": \"Partly Cloudy\",\n        \"humidity\": 65,\n        \"wind_speed\": 8\n    },\n    role=MessageRole.AGENT,\n    parent_message_id=msg2.message_id\n)\n\n# Add an agent's text response\nmsg4 = conversation2.create_text_message(\n    text=\"The weather in New York is currently 22°C (71.6°F) and partly cloudy. \"\n         \"The humidity is 65% with wind speeds of 8 km/h.\",\n    role=MessageRole.AGENT,\n    parent_message_id=msg3.message_id\n)\n\n# Display the conversation\npretty_print_conversation(conversation2)\n```\n\n----------------------------------------\n\nTITLE: Defining an Agent Card in Python\nDESCRIPTION: Shows how to create an `AgentCard` using the `python_a2a` library to describe an AI agent. The example specifies the agent's name, description, access URL, version, and defines its capabilities using `AgentSkill` objects.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/basics.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import AgentCard, AgentSkill\n\nagent_card = AgentCard(\n    name=\"Weather API\",\n    description=\"Get weather information for locations\",\n    url=\"http://localhost:5000\",\n    version=\"1.0.0\",\n    skills=[\n        AgentSkill(\n            name=\"Get Weather\",\n            description=\"Get current weather for a location\",\n            tags=[\"weather\", \"current\"],\n            examples=[\"What's the weather in New York?\"]\n        )\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Function Calling and Conversation Testing Across Multiple AI Providers in Python\nDESCRIPTION: Shows conditional test executions invoking function calling and conversation tests on agents instantiated for OpenAI, AWS Bedrock, and Anthropic Claude platforms depending on available API keys or credentials. Demonstrates prompts for weather queries triggering function calls, and multi-turn chatbot interactions exploring the A2A protocol. This setup emphasizes cross-provider interoperability and feature consistency. Dependencies include valid credentials for each service, agents constructed with matching APIs, and prior definitions of the test functions.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nif openai_api_key:\n    # Test with a prompt that should trigger function calling\n    function_response = test_function_calling(\n        function_agent,\n        \"What's the weather like in Tokyo right now?\",\n        \"Function-Enabled Agent\"\n    )\n    \n    # Test with another location\n    function_response2 = test_function_calling(\n        function_agent,\n        \"I'm planning a trip to London. What's the weather there?\",\n        \"Function-Enabled Agent\"\n    )\n\nif aws_access_key_id and aws_secret_access_key and aws_region:\n    # Test with a prompt that should trigger function calling\n    function_response = test_function_calling(\n        function_agent,\n        \"What's the weather like in Tokyo right now?\",\n        \"Function-Enabled Agent\"\n    )\n    \n    # Test with another location\n    function_response2 = test_function_calling(\n        function_agent,\n        \"I'm planning a trip to London. What's the weather there?\",\n        \"Function-Enabled Agent\"\n    )\n\n# Test conversation handling (if API key is available)\nif openai_api_key:\n    # Series of prompts for a conversation\n    prompts = [\n        \"What is the A2A protocol?\",\n        \"How does it relate to other agent frameworks?\",\n        \"Can you give an example of how it might be used in a real application?\"\n    ]\n    \n    conversation = test_conversation(openai_agent, prompts, \"OpenAI Agent\")\n\n# Test Claude agent conversation (if API key is available)\nif anthropic_api_key:\n    # Series of prompts for a conversation\n    prompts = [\n        \"What is the A2A protocol?\",\n        \"How does it relate to other agent frameworks?\",\n        \"Can you give an example of how it might be used in a real application?\"\n    ]\n    \n    claude_conversation = test_conversation(claude_agent, prompts, \"Claude Agent\")\n\n# Test Bedrock agent conversation (if creds are available)\nif aws_access_key_id and aws_secret_access_key and aws_region:\n    prompts = [\n        \"What is the A2A protocol?\",\n        \"How does it relate to other agent frameworks?\",\n        \"Can you give an example of how it might be used in a real application?\"\n    ]\n    bedrock_conversation = test_conversation(bedrock_agent, prompts,\"AWS Bedrock Agent\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Anthropic Claude-Powered Agent\nDESCRIPTION: Initializes an agent using Anthropic's Claude LLM. Configures parameters including model selection, temperature, token limits, and system prompt to define the agent's behavior.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Skip this cell if Anthropic API key is not available\nif anthropic_api_key:\n    # Create an Anthropic-powered agent\n    claude_agent = AnthropicA2AServer(\n        api_key=anthropic_api_key,\n        model=\"claude-3-opus-20240229\",  # You can use other Claude models too\n        temperature=0.7,\n        max_tokens=1000,\n        system_prompt=\"You are a helpful assistant that provides clear, concise answers.\"\n    )\n    print(\"Claude agent created successfully!\")\n```\n\n----------------------------------------\n\nTITLE: Importing Python A2A Components\nDESCRIPTION: This code imports the required classes and functions from the `python_a2a` package. These components are used throughout the notebook to create messages, build conversations, handle agent interactions, and display results. The imports include `Message`, `TextContent`, `FunctionCallContent`, `FunctionResponseContent`, `FunctionParameter`, `MessageRole`, `Conversation`, `A2AClient`, `A2AServer`, `pretty_print_message`, and `pretty_print_conversation`.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import (\n    Message, TextContent, FunctionCallContent, FunctionResponseContent, FunctionParameter,\n    MessageRole, Conversation,\n    A2AClient, A2AServer,\n    pretty_print_message, pretty_print_conversation\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Weather Agent with Python A2A\nDESCRIPTION: This snippet defines a Python A2A agent named \"Weather Agent\" with a skill to provide weather information. It illustrates the use of class decorators to specify agent and skill metadata, and a method to handle incoming tasks by extracting message content, generating a weather response, and managing task status. The code demonstrates how to run the server hosting the agent.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom python_a2a import A2AServer, skill, agent, run_server, TaskStatus, TaskState\n\n@agent(\n    name=\"Weather Agent\",\n    description=\"Provides weather information\",\n    version=\"1.0.0\"\n)\nclass WeatherAgent(A2AServer):\n    \n    @skill(\n        name=\"Get Weather\",\n        description=\"Get current weather for a location\",\n        tags=[\"weather\", \"forecast\"]\n    )\n    def get_weather(self, location):\n        \"\"\"Get weather for a location.\"\"\"\n        # Mock implementation\n        return f\"It's sunny and 75°F in {location}\"\n    \n    def handle_task(self, task):\n        # Extract location from message\n        message_data = task.message or {}\n        content = message_data.get(\"content\", {})\n        text = content.get(\"text\", \"\") if isinstance(content, dict) else \"\"\n        \n        if \"weather\" in text.lower() and \"in\" in text.lower():\n            location = text.split(\"in\", 1)[1].strip().rstrip(\"?.\")\n            \n            # Get weather and create response\n            weather_text = self.get_weather(location)\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": weather_text}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n        else:\n            task.status = TaskStatus(\n                state=TaskState.INPUT_REQUIRED,\n                message={\"role\": \"agent\", \"content\": {\"type\": \"text\", \n                         \"text\": \"Please ask about weather in a specific location.\"}}\n            )\n        return task\n\n# Run the server\nif __name__ == \"__main__\":\n    agent = WeatherAgent()\n    run_server(agent, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Deserializing Message from JSON\nDESCRIPTION: This code demonstrates deserializing a `Message` object from a JSON string using the `from_json()` method of the `Message` class. The deserialized message is then displayed using `pretty_print_message()`.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Deserialize a message from JSON\ndeserialized_message = Message.from_json(json_message)\npretty_print_message(deserialized_message)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAI-Compatible Function Schema in Python\nDESCRIPTION: Declares a Python dictionary 'weather_function' representing metadata for the 'get_weather' function compatible with OpenAI's function calling interface. It specifies the function name, description, required parameters with types and acceptable values, aiming to facilitate automated function invocation by an LLM. This schema enables integration with OpenAI agents expecting JSON-schema style function specifications. Dependencies include consistent parameter naming and compliance with OpenAI's function format requirements.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nweather_function = {\n    \"name\": \"get_weather\",\n    \"description\": \"Get the current weather for a location\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city name, e.g. New York\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\"],\n                \"description\": \"The temperature unit\"\n            }\n        },\n        \"required\": [\"location\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Perform Python A2A Development Install - Bash\nDESCRIPTION: Executes a sequence of commands to clone the repository, change directory, and install the library in editable mode with development dependencies using pip. This setup is specifically for contributors or those wishing to run tests against the source code.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/themanojdesai/python-a2a.git\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd python-a2a\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Authentication Implementation (Python)\nDESCRIPTION: This snippet demonstrates adding authentication to an A2A agent using API keys. It sets up an agent card with bearer authentication. A custom middleware checks for a valid `Authorization` header before processing tasks and rejects unauthorized requests.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, TaskStatus, TaskState\n    from python_a2a.models import AgentCard\n    from flask import request\n    \n    class AuthenticatedAgent(A2AServer):\n        def __init__(self):\n            # Create agent card with authentication\n            agent_card = AgentCard(\n                name=\"Authenticated Agent\",\n                description=\"Agent with authentication\",\n                url=\"http://localhost:5000\",\n                version=\"1.0.0\",\n                authentication=\"bearer\"\n            )\n            super().__init__(agent_card=agent_card)\n            \n            # API keys\n            self.api_keys = {\"MY_SECRET_KEY\": \"user1\"}\n        \n        def setup_routes(self, app):\n            # Add authentication middleware\n            @app.before_request\n            def authenticate():\n                # Skip authentication for agent card\n                if request.path in [\"/\", \"/a2a\", \"/agent.json\", \"/a2a/agent.json\"]:\n                    return None\n                \n                # Check for Authorization header\n                auth_header = request.headers.get(\"Authorization\")\n                if not auth_header or not auth_header.startswith(\"Bearer \"):\n                    return {\"error\": \"Unauthorized\"}, 401\n                \n                # Get token\n                token = auth_header.split(\"Bearer \")[1]\n                \n                # Check if token is valid\n                if token not in self.api_keys:\n                    return {\"error\": \"Invalid API key\"}, 401\n                \n                # Token is valid\n                return None\n            \n            # Call parent setup_routes\n            super().setup_routes(app)\n        \n        def handle_task(self, task):\n            # Get token from request\n            auth_header = request.headers.get(\"Authorization\")\n            token = auth_header.split(\"Bearer \")[1]\n            \n            # Get user from token\n            user = self.api_keys.get(token)\n            \n            # Create response\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": f\"Hello, {user}! This is a protected resource.\"}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n            \n            return task\n```\n\n----------------------------------------\n\nTITLE: Creating an AWS Bedrock-Powered Agent\nDESCRIPTION: Sets up an agent using AWS Bedrock's LLM services. Requires AWS credentials and configures parameters like temperature and system prompt for agent behavior.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Skip this cell if AWS credentials are not available\nif aws_access_key_id and aws_secret_access_key and aws_region:\n    # Create a Bedrock-powered agent\n    bedrock_agent = BedrockA2AServer(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        aws_region=aws_region,\n        temperature=0.7,\n        system_prompt=\"You are a helpful assistant that provides clear, concise answers.\"\n    )\n    print(\"Bedrock agent created successfully!\")\nelse:\n    print(\"⚠️ AWS credentials are missing. Bedrock agent cannot be created.\")\n```\n\n----------------------------------------\n\nTITLE: Handling Asynchronous Tasks in Python A2A Agent\nDESCRIPTION: Asynchronously processes incoming tasks by converting task data into a `python_a2a.Message` object. It then calls `handle_message_async` to process the message using the OpenAI/MCP logic. Based on the response content type (`text` or `function_response`), it populates the task's `artifacts` and sets the task `status` to `COMPLETED`, providing a text representation of the response or a fallback message.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nasync def handle_task_async(self, task):\n    \"\"\"Handle a task by converting to message and back\"\"\"\n    # Convert task to message\n    message_data = task.message or {}\n    \n    # Create a Message object\n    from python_a2a import Message, TextContent, MessageRole\n    message = Message(\n        content=TextContent(text=message_data.get(\"content\", {}).get(\"text\", \"\")),\n        role=MessageRole.USER,\n        conversation_id=task.id\n    )\n    \n    # Process with handle_message\n    response = await self.handle_message_async(message)\n    \n    # Convert response to task\n    if response.content.type == \"text\":\n        task.artifacts = [{\n            \"parts\": [{\"type\": \"text\", \"text\": response.content.text}]\n        }]\n        task.status = TaskStatus(state=TaskState.COMPLETED)\n    elif response.content.type == \"function_response\":\n        task.artifacts = [{\n            \"parts\": [{\"type\": \"text\", \"text\": str(response.content.response)}]\n        }]\n        task.status = TaskStatus(state=TaskState.COMPLETED)\n    else:\n        task.artifacts = [{\n            \"parts\": [{\"type\": \"text\", \"text\": f\"Response type: {response.content.type}\"}]\n        }]\n        task.status = TaskStatus(state=TaskState.COMPLETED)\n    \n    return task\n```\n\n----------------------------------------\n\nTITLE: Installing python-a2a Package\nDESCRIPTION: This snippet installs the necessary python-a2a package using pip. The package is required to run the examples in the notebook. The command `!pip install python-a2a` is used to install the package from the Python Package Index (PyPI).\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install python-a2a\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Python A2A Documentation (bash)\nDESCRIPTION: This snippet installs all required Python dependencies for building the Python A2A documentation. It includes an editable installation of all optional components and installs additional requirements from a separate requirements file located in the docs directory. These commands must be run in a shell environment before proceeding to build the documentation.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/DOCUMENTATION.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[all]\"\npip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Testing Conversation Handling with an AI Agent for Multiple Prompts in Python\nDESCRIPTION: Defines 'test_conversation', a Python function to test multi-turn conversational capability of an AI agent. It creates a new conversation, sends a series of user prompts in order, records agent responses, and prints both prompts and agent outputs. The function updates conversation context with every iteration to allow the agent to maintain state, simulating a real dialogue scenario. Dependencies include agent classes capable of handling conversation objects and roles, plus message history management.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef test_conversation(agent, prompts, agent_name=\"Agent\"):\n    \"\"\"Test an agent with a series of prompts in a conversation.\"\"\"\n    print(f\"🔍 Testing {agent_name} conversation...\")\n    \n    # Create a new conversation\n    conversation = Conversation()\n    \n    # Add each prompt to the conversation and get responses\n    for i, prompt in enumerate(prompts):\n        print(f\"\\n📝 User message {i+1}: {prompt}\")\n        \n        # Add the user message to the conversation\n        conversation.create_text_message(\n            text=prompt,\n            role=MessageRole.USER\n        )\n        \n        # Send the conversation to the agent\n        updated_conversation = agent.handle_conversation(conversation)\n        \n        # Update our conversation reference\n        conversation = updated_conversation\n        \n        # Print the agent's response\n        response = conversation.messages[-1]\n        print(f\"\\n🤖 {agent_name} response {i+1}:\\n\")\n        print(response.content.text)\n        print(\"\\n\" + \"-\"*50 + \"\\n\")\n    \n    return conversation\n```\n\n----------------------------------------\n\nTITLE: Testing AWS Bedrock Agent Response\nDESCRIPTION: Tests the AWS Bedrock agent with a prompt about agent interoperability. The test only runs if AWS credentials are available, using the previously defined test_agent function.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Test Bedrock agent\nif aws_access_key_id and aws_secret_access_key and aws_region:\n    bedrock_response = test_agent(\n        bedrock_agent, \n        \"Explain the concept of agent interoperability and why it's important for AI systems.\",\n        \"AWS Bedrock Agent\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting up a Virtual Environment and Installing Dependencies\nDESCRIPTION: Creates a virtual environment for the project, activates it, and installs the development dependencies specified in the project's setup.py. This ensures that the project's dependencies are isolated from the system-wide Python installation. Requires `python` and `pip` to be installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Dependencies for python-a2a\nDESCRIPTION: These bash snippets demonstrate installing optional dependencies (extras) for the python-a2a package using UV.  It uses the bracket syntax to specify which extras to install.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/uv-installation.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# For Flask-based server support\n    uv pip install \"python-a2a[server]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# For OpenAI integration\n    uv pip install \"python-a2a[openai]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# For Anthropic Claude integration\n    uv pip install \"python-a2a[anthropic]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# For AWS-Bedrock integration\n    uv pip install \"python-a2a[bedrock]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# For MCP support (Model Context Protocol)\n    uv pip install \"python-a2a[mcp]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# For all optional dependencies\n    uv pip install \"python-a2a[all]\"\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest\nDESCRIPTION: Executes the test suite for the Python A2A project using `pytest`. Ensures that code changes do not introduce regressions and maintain the project's functionality. Requires `pytest` to be installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Generating a Chart with Python A2A Agent\nDESCRIPTION: Creates a chart object with sample data and returns it as a structured agent message. Demonstrates how to initialize chart content with title, labels, data points, and chart type, then format it as a response message.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Generate a chart\nchart_content = ChartContent(\n    title=\"Sample Chart\",\n    labels=[\"A\", \"B\", \"C\", \"D\"],\n    data=[10, 20, 15, 25],\n    chart_type=\"bar\"\n)\n\n# Return the chart\nreturn Message(\n    content=chart_content,\n    role=MessageRole.AGENT,\n    parent_message_id=message.message_id,\n    conversation_id=message.conversation_id\n)\n```\n\n----------------------------------------\n\nTITLE: Building Package with UV and Python Build\nDESCRIPTION: This snippet demonstrates how to build the Python package into wheel and sdist distributions.  It first removes any existing distribution and build files to ensure a clean build.  Then, `python -m build` is used to build the package. The created distributions will be in the `dist/` directory.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf dist/ build/ *.egg-info/\nuv pip run python -m build\n```\n\n----------------------------------------\n\nTITLE: Installing Build Dependencies with UV\nDESCRIPTION: This snippet installs the necessary build tools, build and twine, using the uv package manager.  These tools are essential for creating and uploading Python packages to PyPI.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install build twine\n```\n\n----------------------------------------\n\nTITLE: Synchronous Task Handling Wrapper in Python A2A\nDESCRIPTION: Provides a synchronous wrapper around the asynchronous `handle_task_async` method. It retrieves the current asyncio event loop and executes the asynchronous task handler until completion. This makes the asynchronous task processing logic callable from synchronous code paths within the agent.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef handle_task(self, task):\n    \"\"\"Override to use our async handler\"\"\"\n    import asyncio\n    loop = asyncio.get_event_loop()\n    return loop.run_until_complete(self.handle_task_async(task))\n```\n\n----------------------------------------\n\nTITLE: Navigating to Documentation Directory for Local Build (bash)\nDESCRIPTION: This snippet changes the current shell working directory to the docs folder, which contains the source files and configuration needed by Sphinx to generate the documentation. It prepares the environment to invoke the make command to compile documentation.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/DOCUMENTATION.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\n```\n\n----------------------------------------\n\nTITLE: Creating an A2A Message in Python\nDESCRIPTION: Illustrates how to create a basic A2A text message using the `Message`, `TextContent`, and `MessageRole` classes from the `python_a2a` library. The example sets the message content to a question and assigns the sender role as USER.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/basics.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import Message, TextContent, MessageRole\n\nmessage = Message(\n    content=TextContent(text=\"What's the weather in Paris?\"),\n    role=MessageRole.USER\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: Installs the necessary Python packages for building the documentation using `pip` from the requirements file. Sets up the environment to generate the documentation. Requires `pip` to be installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Tests with UV and pytest\nDESCRIPTION: This command runs tests using `pytest` via the `uv pip run` command. It assumes that the project has tests defined and configured for pytest. The tests determine if the changes are properly implemented.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv pip run pytest\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic A2A Client with Python\nDESCRIPTION: This code creates a simple A2A client that connects to the greeting agent. It initializes an `A2AClient` instance with the address of the running agent. It then prints the agent's information, including its name, description, and available skills. Finally, it sends two greeting messages to the agent using the `ask` method and prints the responses. It requires the `python_a2a` library.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/simple.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AClient\n\n# Create a client\nclient = A2AClient(\"http://localhost:5000\")\n\n# Print agent information\nprint(f\"Connected to: {client.agent_card.name}\")\nprint(f\"Description: {client.agent_card.description}\")\nprint(f\"Skills: {[skill.name for skill in client.agent_card.skills]}\")\n\n# Send a greeting\nresponse = client.ask(\"Hello there! My name is Alice.\")\nprint(f\"Response: {response}\")\n\n# Send another message\nresponse = client.ask(\"What can you do?\")\nprint(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Connecting to an A2A Agent with Python\nDESCRIPTION: Demonstrates how to create a client instance that connects to a running A2A agent server. It uses the A2AClient class to initiate a connection to the agent hosted at the specified URL. The snippet retrieves and prints agent metadata such as name, description, and available skills. It then sends a query to the agent using the ask method and prints the response. Requires a running A2A server on http://localhost:5000 and the python_a2a library.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/quickstart.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AClient\n\n# Create a client connected to an A2A-compatible agent\nclient = A2AClient(\"http://localhost:5000\")\n\n# View agent information\nprint(f\"Connected to: {client.agent_card.name}\")\nprint(f\"Description: {client.agent_card.description}\")\nprint(f\"Skills: {[skill.name for skill in client.agent_card.skills]}\")\n\n# Ask a question\nresponse = client.ask(\"What's the weather in Paris?\")\nprint(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Virtual Environment and Installing Dependencies (Bash)\nDESCRIPTION: Commands to create a Python virtual environment named 'venv', activate it (with platform-specific instructions), and install the project's required packages, including development dependencies specified in 'setup.py' or 'pyproject.toml', using pip in editable mode.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with UV\nDESCRIPTION: This command installs the documentation dependencies listed in `requirements.txt` then builds the HTML documentation using `make html`. The `cd docs` command changes the current directory to the documentation folder before running this command.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nuv pip install -r requirements.txt\nmake html\n```\n\n----------------------------------------\n\nTITLE: Checking Package Integrity with Twine\nDESCRIPTION: This command uses `twine check` to verify the integrity of the built package distributions located in the `dist/` directory.  Twine will perform checks to confirm the package is valid and ready for upload to PyPI.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuv pip run twine check dist/*\n```\n\n----------------------------------------\n\nTITLE: Simulating a Client Interaction\nDESCRIPTION: This code uses the `simulate_client_server` function to simulate a client-server interaction with the `EchoAgent`. It creates a client message, calls `simulate_client_server` passing the agent and message. The agent processes the message and then displays the response. This example demonstrates a simple interaction between the client and agent.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Simulate a client interaction with our echo agent\nclient_message = Message(\n    content=TextContent(text=\"What is the meaning of life?\"),\n    role=MessageRole.USER\n)\n\nserver_response = simulate_client_server(echo_agent, client_message)\n```\n\n----------------------------------------\n\nTITLE: Creating the Smart Assistant Agent - Python\nDESCRIPTION: This code defines the SmartAssistant class, which is an OpenAI-based agent that integrates with the MCP tools for calculation and data lookup. It inherits from OpenAIA2AServer and A2AMCPAgent and initializes with an OpenAI API key. It includes an agent card containing agent metadata and  calls the system prompt to provide the context.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    # Create the OpenAI-based MCP-enabled agent\n    class SmartAssistant(OpenAIA2AServer, A2AMCPAgent):\n        def __init__(self, api_key):\n            # Create agent card\n            agent_card = AgentCard(\n                name=\"Smart Assistant\",\n                description=\"A smart assistant that can calculate and look up information\",\n                url=\"http://localhost:5000\",\n                version=\"1.0.0\"\n            )\n            \n            # Initialize OpenAI A2A server\n            OpenAIA2AServer.__init__(\n                self,\n                api_key=api_key,\n                model=\"gpt-4\",\n                system_prompt=\"\"\"You are a helpful assistant that can calculate and look up information.\n                \n                When a user asks for calculations, use the calculator tools to perform the calculation.\n                When a user asks for country information, use the data lookup tools.\n                \n                Make your responses concise and helpful.\"\"\"\n            )\n            \n            # Initialize MCP agent\n            A2AMCPAgent.__init__(\n                self,\n                name=\"Smart Assistant\",\n                description=\"A smart assistant that can calculate and look up information\",\n                mcp_servers={\n                    \"calc\": calculator_mcp,\n```\n\n----------------------------------------\n\nTITLE: Uploading Package to PyPI with Twine\nDESCRIPTION: This command uploads the built package distributions from the `dist/` directory to PyPI using the `twine upload` command.  This action makes the package publicly available.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nuv pip run twine upload dist/*\n```\n\n----------------------------------------\n\nTITLE: Connecting to an MCP Server with MCPClient\nDESCRIPTION: This snippet demonstrates connecting to a previously created MCP server using the `MCPClient`.  It initializes an `MCPClient` with the server's address. Then, it calls the `add` and `multiply` tools, providing input values for the parameters `a` and `b`. The results of the tool calls are then printed to the console. This assumes the MCP server is running and accessible at the specified address.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/mcp.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a.mcp import MCPClient\n\n# Create a client\nclient = MCPClient(\"http://localhost:5001\")\n\n# Call a tool\nresult = await client.call_tool(\"add\", a=5, b=3)\nprint(result)  # 8\n\n# Call another tool\nresult = await client.call_tool(\"multiply\", a=5, b=3)\nprint(result)  # 15\n```\n\n----------------------------------------\n\nTITLE: Defining Data Lookup Tools - Python\nDESCRIPTION: This code defines data lookup tools for the LLM agent via MCP. These tools are for looking up the capital and population of a given country. The `get_country_capital` and `get_country_population` functions use dictionaries to store the data and return the information when a valid country is requested.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    # Create MCP server with data lookup tools\n    data_mcp = FastMCP(\n        name=\"Data MCP\",\n        description=\"Provides data lookup functions\"\n    )\n    \n    @data_mcp.tool()\n    def get_country_capital(country: str) -> str:\n        \"\"\"\n        Get the capital city of a country.\n        \n        Args:\n            country: The country to look up\n            \n        Returns:\n            The capital city\n        \"\"\"\n        capitals = {\n            \"usa\": \"Washington, D.C.\",\n            \"uk\": \"London\",\n            \"france\": \"Paris\",\n            \"germany\": \"Berlin\",\n            \"japan\": \"Tokyo\",\n            \"china\": \"Beijing\",\n            \"india\": \"New Delhi\",\n            \"brazil\": \"Brasília\",\n            \"australia\": \"Canberra\",\n            \"canada\": \"Ottawa\"\n        }\n        \n        country = country.lower()\n        if country in capitals:\n            return capitals[country]\n        elif country == \"united states\" or country == \"united states of america\":\n            return capitals[\"usa\"]\n        elif country == \"united kingdom\":\n            return capitals[\"uk\"]\n        else:\n            return f\"I don't know the capital of {country}\"\n    \n    @data_mcp.tool()\n    def get_country_population(country: str) -> str:\n        \"\"\"\n        Get the population of a country.\n        \n        Args:\n            country: The country to look up\n            \n        Returns:\n            The population (approximate, as of 2023)\n        \"\"\"\n        populations = {\n            \"usa\": \"331 million\",\n            \"uk\": \"67 million\",\n            \"france\": \"65 million\",\n            \"germany\": \"83 million\",\n            \"japan\": \"126 million\",\n            \"china\": \"1.4 billion\",\n            \"india\": \"1.38 billion\",\n            \"brazil\": \"212 million\",\n            \"australia\": \"25 million\",\n            \"canada\": \"38 million\"\n        }\n        \n        country = country.lower()\n        if country in populations:\n            return populations[country]\n        elif country == \"united states\" or country == \"united states of america\":\n            return populations[\"usa\"]\n        elif country == \"united kingdom\":\n            return populations[\"uk\"]\n        else:\n            return f\"I don't know the population of {country}\"\n```\n\n----------------------------------------\n\nTITLE: Linting Code with Flake8\nDESCRIPTION: Checks the Python A2A codebase for linting errors and style issues using `flake8`. Helps to identify potential problems and enforce coding standards. Requires `flake8` to be installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nflake8 python_a2a\n```\n\n----------------------------------------\n\nTITLE: Running the UI Agent - Python\nDESCRIPTION: This snippet demonstrates how to run the UI agent. It initializes the AssistantAgent and then runs it using the `run_server` function at port 5000. This assumes the AssistantAgent class and the `run_server` function are defined elsewhere.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    # Run the server\n    if __name__ == \"__main__\":\n        agent = AssistantAgent()\n        run_server(agent, port=5000)\n```\n\n----------------------------------------\n\nTITLE: Extracting Preferences and Generating Recommendations - Python\nDESCRIPTION: This code snippet processes user input to extract preferences, such as weather and activities, and then generates travel recommendations based on those preferences. It uses a `recommend_destination` method, presumably defined elsewhere, to fetch destination data. If recommendations are found, it formats them for display.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n            # Extract preferences\n            weather_pref = None\n            activity_pref = None\n            \n            if \"warm\" in text.lower() or \"hot\" in text.lower():\n                weather_pref = \"warm\"\n            elif \"cool\" in text.lower() or \"cold\" in text.lower():\n                weather_pref = \"cool\"\n            elif \"moderate\" in text.lower() or \"mild\" in text.lower():\n                weather_pref = \"moderate\"\n            \n            # Check for activities\n            common_activities = [\"beach\", \"museum\", \"food\", \"shopping\", \"nature\", \"cruise\", \"show\"]\n            for activity in common_activities:\n                if activity in text.lower():\n                    activity_pref = activity\n                    break\n            \n            # Generate recommendations if preferences found\n            if weather_pref:\n                try:\n                    recommendations = self.recommend_destination(weather_pref, activity_pref)\n                    \n                    if isinstance(recommendations, str):\n                        response_text = recommendations\n                    else:\n                        response_text = f\"Here are some {weather_pref} destinations\"\n                        if activity_pref:\n                            response_text += f\" with {activity_pref} activities\"\n                        response_text += \":\\n\\n\"\n                        \n                        for dest in recommendations[:3]:  # Limit to top 3\n                            response_text += f\"- {dest['name'].title()}: {dest['weather']['temp']}°F, {dest['weather']['condition']}\\n\"\n                            response_text += f\"  Activities: {', '.join(dest['activities'])}\\n\\n\"\n                except Exception as e:\n                    response_text = f\"Sorry, I couldn't generate recommendations: {str(e)}\"\n            \n            # Create response artifact\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": response_text}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n            \n            return task\n```\n\n----------------------------------------\n\nTITLE: Linting Code with flake8 (Bash)\nDESCRIPTION: Runs the 'flake8' linter on the 'python_a2a' directory to check for code style violations, potential bugs, and complexity issues according to configured rules. This helps maintain code quality.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nflake8 python_a2a\n```\n\n----------------------------------------\n\nTITLE: Sending a Conversation to the Echo Agent\nDESCRIPTION: This snippet demonstrates sending a `Conversation` object to the `EchoAgent`. It creates a conversation with a user's text message. The `handle_conversation` method is used to process the conversation. The updated conversation (which in this case contains only the echoed response) is then displayed by `pretty_print_conversation()`.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Create a new conversation\ntest_conversation = Conversation()\ntest_conversation.create_text_message(\n    text=\"Hello, agent!\",\n    role=MessageRole.USER\n)\n\n# Send the conversation to the echo agent\nupdated_conversation = echo_agent.handle_conversation(test_conversation)\n\n# Display the updated conversation\npretty_print_conversation(updated_conversation)\n```\n\n----------------------------------------\n\nTITLE: Create Chart Agent (Python)\nDESCRIPTION: This snippet illustrates the creation of an A2A agent that utilizes the custom content type `ChartContent`. It shows how to define the handle_message method to parse and respond to a custom chart content. It includes dependencies on python_a2a, and the custom type declared previously.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, Message, MessageRole, BaseModel\n    from dataclasses import dataclass\n    from typing import Dict, Any, List\n    \n    # Define a custom content type\n    @dataclass\n    class ChartContent(BaseModel):\n        \"\"\"Chart content type\"\"\"\n        type: str = \"chart\"\n        title: str = \"\"\n        labels: List[str] = None\n        data: List[float] = None\n        chart_type: str = \"bar\"  # bar, line, pie, etc.\n        \n        def to_dict(self) -> Dict[str, Any]:\n            \"\"\"Convert to dictionary representation\"\"\"\n            return {\n                \"type\": self.type,\n                \"title\": self.title,\n                \"labels\": self.labels,\n                \"data\": self.data,\n                \"chart_type\": self.chart_type\n            }\n    \n    # Create an agent that uses the custom content type\n    class ChartAgent(A2AServer):\n        def handle_message(self, message):\n```\n\n----------------------------------------\n\nTITLE: Installing Python A2A Package with LLM Provider Dependencies\nDESCRIPTION: Installs the Python A2A package with optional dependencies for OpenAI, Anthropic, and AWS Bedrock integration. The command is commented out as it's meant to be run only if these packages aren't already installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# !pip install \"python-a2a[openai,anthropic,bedrock]\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Message\nDESCRIPTION: This code snippet demonstrates how to create a simple text message using the `Message` class.  It initializes a `TextContent` object with the message text and sets the message role to `USER`. The `pretty_print_message()` function is then used to display the created message in a readable format.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a text message from the user\ntext_message = Message(\n    content=TextContent(text=\"What's the weather like in New York?\"),\n    role=MessageRole.USER\n)\n\n# Display the message\npretty_print_message(text_message)\n```\n\n----------------------------------------\n\nTITLE: Creating a Development Branch (Bash)\nDESCRIPTION: This Git command creates a new branch with a descriptive name (e.g., 'name-of-your-bugfix-or-feature') and immediately switches the working directory to this new branch. This isolates development work from the main branch.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ngit checkout -b name-of-your-bugfix-or-feature\n```\n\n----------------------------------------\n\nTITLE: Sending a Message to the Echo Agent\nDESCRIPTION: This code demonstrates sending a `Message` object to the `EchoAgent` and receiving the response. It creates a test message with text content, sets its role, and calls `handle_message` to get the agent's response. The response is then displayed using `pretty_print_message()`.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Create a test message\ntest_message = Message(\n    content=TextContent(text=\"Hello, agent!\"),\n    role=MessageRole.USER\n)\n\n# Send the message to the echo agent\nresponse = echo_agent.handle_message(test_message)\n\n# Display the response\npretty_print_message(response)\n```\n\n----------------------------------------\n\nTITLE: Installing with Exact Versions from UVManifest.toml\nDESCRIPTION: This snippet installs python-a2a and its dependencies with exact versions specified in the UVManifest.toml file, ensuring reproducible builds.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/uv-installation.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Install with exact versions from UVManifest.toml\n    uv pip install --manifest UVManifest.toml\n```\n\n----------------------------------------\n\nTITLE: Creating a Git Branch for Code Changes\nDESCRIPTION: Creates a new branch in the local Git repository, allowing developers to isolate their changes. Use a descriptive name for bug fixes or features. Requires a local Git repository.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b name-of-your-bugfix-or-feature\n```\n\n----------------------------------------\n\nTITLE: Managing A2A Tasks in Python\nDESCRIPTION: Demonstrates creating an A2A `Task` from a message dictionary using the `python_a2a` library. It then shows how to add response data as artifacts and update the task's status to `COMPLETED` using `TaskStatus` and `TaskState`. This snippet assumes a `message` object (like the one from the previous example) exists.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/basics.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import Task, TaskStatus, TaskState\n\n# Create a task\ntask = Task(message=message.to_dict())\n\n# Complete the task with a response\ntask.artifacts = [{\n    \"parts\": [{\n        \"type\": \"text\",\n        \"text\": \"It's sunny and 72°F in Paris\"\n    }]\n}]\ntask.status = TaskStatus(state=TaskState.COMPLETED)\n```\n\n----------------------------------------\n\nTITLE: Creating Function Call Message (Python)\nDESCRIPTION: This snippet demonstrates how to create a function call message using the `python_a2a` library. It defines a message with function call content, specifying the function name and parameters. The example sets the `role` to agent.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import FunctionCallContent, Message, MessageRole, FunctionParameter\n    \n    # Create a function call message\n    message = Message(\n        content=FunctionCallContent(\n            name=\"get_weather\",\n            parameters=[\n                FunctionParameter(name=\"location\", value=\"New York\"),\n                FunctionParameter(name=\"unit\", value=\"fahrenheit\")\n            ]\n        ),\n        role=MessageRole.AGENT\n    )\n```\n\n----------------------------------------\n\nTITLE: Executing automated publishing script\nDESCRIPTION: This script is intended to automate the entire publishing process. It performs checks, cleans, builds, tests, uploads to TestPyPI (optional), uploads to PyPI, and creates Git tags, all in a single command.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/publish.sh\n```\n\n----------------------------------------\n\nTITLE: Robust Error Handling (Python)\nDESCRIPTION: This code shows an A2A agent with robust error handling. It encapsulates processing in a `try...except` block. It handles different exceptions (ValueError, ConnectionError, and a generic Exception). Appropriate error messages and task states are set to reflect error types.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, TaskStatus, TaskState\n    \n    class RobustAgent(A2AServer):\n        def handle_task(self, task):\n            try:\n                # Extract message text\n                message_data = task.message or {}\n                content = message_data.get(\"content\", {})\n                text = content.get(\"text\", \"\") if isinstance(content, dict) else \"\"\n                \n                # Process the message\n                # This might raise exceptions\n                result = self.process_message(text)\n                \n                # Create response artifact\n                task.artifacts = [{\n                    \"parts\": [{\"type\": \"text\", \"text\": result}]\n                }]\n                task.status = TaskStatus(state=TaskState.COMPLETED)\n                \n            except ValueError as e:\n                # Handle validation errors\n                task.artifacts = [{\n                    \"parts\": [{\"type\": \"text\", \"text\": f\"Validation error: {str(e)}\"}]\n                }]\n                task.status = TaskStatus(state=TaskState.INPUT_REQUIRED)\n                \n            except ConnectionError as e:\n                # Handle connection errors\n                task.artifacts = [{\n                    \"parts\": [{\"type\": \"text\", \"text\": f\"Service unavailable: {str(e)}\"}]\n                }]\n                task.status = TaskStatus(state=TaskState.FAILED)\n                \n            except Exception as e:\n                # Handle unexpected errors\n                import traceback\n                task.artifacts = [{\n                    \"parts\": [{\"type\": \"text\", \"text\": f\"An unexpected error occurred: {str(e)}\"}]\n                }]\n                task.status = TaskStatus(state=TaskState.FAILED)\n                \n                # Log the error\n                print(f\"Error: {str(e)}\")\n                print(traceback.format_exc())\n            \n            return task\n        \n        def process_message(self, text):\n            # This is a placeholder for your actual processing logic\n            if not text:\n                raise ValueError(\"Empty message\")\n                \n            if \"error\" in text.lower():\n                raise Exception(\"Simulated error\")\n                \n            return f\"Processed: {text}\"\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for A2A Project\nDESCRIPTION: A comprehensive requirements.txt file that specifies all the Python packages required for the A2A project. The dependencies are organized by purpose and include minimum version numbers to ensure compatibility across installations.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/requirements.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Core requirements\nrequests>=2.25.0\naiohttp>=3.8.0  # Added for streaming support\n\n# Server requirements\nflask>=2.0.0\n\n# LLM integration\nopenai>=1.0.0\nanthropic>=0.3.0\nboto3>=1.37.37\n\n# MCP requirements\nhttpx>=0.23.0\nfastapi>=0.95.0\nuvicorn>=0.21.0\npydantic>=1.10.7\n\n# Development requirements\npytest>=6.0.0\npytest-cov>=2.12.0\nblack>=21.5b2\nflake8>=3.9.2\nmypy>=0.812\nresponses>=0.13.3\n```\n\n----------------------------------------\n\nTITLE: Uploading Package to TestPyPI with Twine\nDESCRIPTION: This snippet uploads the built package to TestPyPI, allowing for testing before the actual release. The `--repository-url` flag specifies the URL of the TestPyPI repository. The upload command is executed via `uv pip run twine upload` .\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuv pip run twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n```\n\n----------------------------------------\n\nTITLE: Formatting Code with Black and isort (Bash)\nDESCRIPTION: These commands automatically format the Python code within the 'python_a2a' directory according to the project's standards. 'black' enforces code style, and 'isort' organizes import statements.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nblack python_a2a\nisort python_a2a\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Make\nDESCRIPTION: Builds the project documentation using `make` within the `docs` directory. Transforms the reStructuredText source files into HTML documentation. Requires `make` and Sphinx to be installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nmake html\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses (Python)\nDESCRIPTION: Demonstrates an agent capable of streaming responses for long-running tasks. It defines an agent card that enables streaming by setting capabilities.  The `handle_task` method simulates processing over time. Task artifacts are updated with progressive statuses.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, TaskStatus, TaskState\n    from python_a2a.models import AgentCard\n    import time\n    \n    class StreamingAgent(A2AServer):\n        def __init__(self):\n            # Create agent card with streaming capability\n            agent_card = AgentCard(\n                name=\"Streaming Agent\",\n                description=\"Agent with streaming capabilities\",\n                url=\"http://localhost:5000\",\n                version=\"1.0.0\",\n                capabilities={\"streaming\": True}\n            )\n            super().__init__(agent_card=agent_card)\n        \n        def handle_task(self, task):\n            # Set task to waiting state\n            task.status = TaskStatus(state=TaskState.WAITING)\n            \n            # Create initial artifact\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": \"Processing...\"}]\n            }]\n            \n            # In a real implementation, you would use server-sent events or websockets\n            # This is a simplified example\n            for i in range(5):\n                # In a real implementation, this would be sent as an update\n                task.artifacts = [{\n                    \"parts\": [{\"type\": \"text\", \"text\": f\"Processing... {(i+1)*20}%\"}]\n                }]\n                \n                # Simulate processing time\n                time.sleep(1)\n            \n            # Final response\n            task.artifacts = [{\n                \"parts\": [{\"type\": \"text\", \"text\": \"Processing complete!\"}]\n            }]\n            task.status = TaskStatus(state=TaskState.COMPLETED)\n            \n            return task\n```\n\n----------------------------------------\n\nTITLE: Formatting Code with Black\nDESCRIPTION: Formats the Python A2A codebase using the Black code formatter. Enforces a consistent coding style throughout the project. Requires `black` to be installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nblack python_a2a\n```\n\n----------------------------------------\n\nTITLE: Creating Test Environment with UV and Installing from TestPyPI\nDESCRIPTION: This snippet sets up a test environment using `uv venv create`, activates it, and then installs the package from TestPyPI using `uv pip install`. It creates a isolated environment for testing installation.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Create a test environment\nuv venv create --fresh .test-venv\nsource .test-venv/bin/activate\n\n# Install from TestPyPI\nuv pip install --index-url https://test.pypi.org/simple/ --no-deps python-a2a\n\n# Test import\npython -c \"import python_a2a; print(python_a2a.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Install Python A2A with MCP Support - Bash\nDESCRIPTION: Installs `python-a2a` and the required dependencies for Model Context Protocol (MCP) support. This installation is done via pip using the `[mcp]` extra.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install \"python-a2a[mcp]\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Documentation in Browser\nDESCRIPTION: Opens the generated HTML documentation in a web browser.  Platform specific commands are provided for macOS, Linux, and Windows. Requires a web browser to be installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# On macOS\nopen _build/html/index.html\n\n# On Linux\nxdg-open _build/html/index.html\n\n# On Windows\nstart _build/html/index.html\n```\n\n----------------------------------------\n\nTITLE: Testing OpenAI Agent Response\nDESCRIPTION: Tests the OpenAI agent with a prompt about agent interoperability. The test only runs if an OpenAI API key is available, using the previously defined test_agent function.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Test OpenAI agent\nif openai_api_key:\n    openai_response = test_agent(\n        openai_agent, \n        \"Explain the concept of agent interoperability and why it's important for AI systems.\",\n        \"OpenAI GPT-4\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Serializing a Conversation to JSON\nDESCRIPTION: This code serializes a `Conversation` object to a JSON string by calling the `to_json()` method on the `conversation` object. It then prints the first 500 characters of the JSON string to show the result, and adds '...' to indicate the rest of the string is truncated. This demonstrates the serialization process for conversations.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Serialize a conversation to JSON\njson_conversation = conversation.to_json()\nprint(json_conversation[:500] + \"...\")\n```\n\n----------------------------------------\n\nTITLE: Testing Anthropic Claude Agent Response\nDESCRIPTION: Tests the Anthropic Claude agent with a prompt about agent interoperability. The test only runs if an Anthropic API key is available, using the previously defined test_agent function.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Test Claude agent\nif anthropic_api_key:\n    claude_response = test_agent(\n        claude_agent, \n        \"Explain the concept of agent interoperability and why it's important for AI systems.\",\n        \"Anthropic Claude\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Testing Specialized AWS Bedrock Agents with Same Prompt\nDESCRIPTION: Tests both the technical and creative specialized AWS Bedrock agents with the same prompt about AI agents working together. Demonstrates how system prompts and temperature influence agent responses.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Test specialized agents (if API key is available)\nif aws_access_key_id and aws_secret_access_key and aws_region:\n    prompt = \"Write about artificial intelligence agents working together.\"\n    \n    technical_response = test_agent(technical_agent, prompt, \"Technical Documentation Agent\")\n    creative_response = test_agent(creative_agent, prompt, \"Creative Writing Agent\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Components from Python A2A\nDESCRIPTION: Imports necessary classes and functions from the Python A2A package for creating and managing LLM-powered agents, handling messages, and working with conversations.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/02_llm_powered_agents.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom python_a2a import (\n    Message, TextContent, FunctionCallContent, FunctionResponseContent, FunctionParameter,\n    MessageRole, Conversation,\n    OpenAIA2AServer, AnthropicA2AServer, BedrockA2AServer,\n    pretty_print_message, pretty_print_conversation\n)\n```\n\n----------------------------------------\n\nTITLE: Running Project Tests (Bash)\nDESCRIPTION: Executes the project's automated test suite using the 'pytest' command. This step is crucial to verify that code changes pass all existing tests before submitting a pull request.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Handling Asynchronous Messages with OpenAI and MCP in Python A2A\nDESCRIPTION: Overrides the base class's asynchronous message handler. It first processes the message using the standard OpenAIA2AServer handler. If the response type is a function call, it delegates handling to the parent class's asynchronous handler (which is expected to manage MCP execution). Otherwise, it returns the direct OpenAI response. Includes a catch-all exception handling that falls back to the parent class's asynchronous handler.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def handle_message_async(self, message):\n    \"\"\"Route all messages through OpenAI but add MCP capabilities\"\"\"\n    try:\n        # First try normal OpenAI processing\n        response = OpenAIA2AServer.handle_message(self, message)\n        \n        # Check if the response is a function call\n        if response.content.type == \"function_call\":\n            # Use our MCP handler to execute the function call\n            return await super().handle_message_async(message)\n        \n        return response\n    except Exception as e:\n        # Fall back to default handling\n        return await super().handle_message_async(message)\n```\n\n----------------------------------------\n\nTITLE: Tagging Release with Git\nDESCRIPTION: This command creates a Git tag associated with the release version. It uses the `git tag -a` command, with the version number, and a descriptive message, to create the tag. It then pushes the tag to the origin repository.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/PUBLICATION_GUIDE.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -a \"v0.3.3\" -m \"Release v0.3.3\"\ngit push origin \"v0.3.3\"\n```\n\n----------------------------------------\n\nTITLE: Sorting Imports with isort\nDESCRIPTION: Sorts the Python imports in the Python A2A codebase using `isort`. Maintains a consistent and readable import structure. Requires `isort` to be installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nisort python_a2a\n```\n\n----------------------------------------\n\nTITLE: Install Python A2A with All Dependencies - Bash\nDESCRIPTION: Installs the `python-a2a` package along with all available optional dependencies defined by the `[all]` extra. This command ensures you have all feature-specific requirements installed.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install \"python-a2a[all]\"\n```\n\n----------------------------------------\n\nTITLE: Building Documentation for Python A2A Locally with pip and Sphinx\nDESCRIPTION: Instructions for installing dependencies and building the Python A2A documentation locally using Sphinx. This process includes installing the package in development mode with all extras, installing documentation requirements, and using make commands to generate HTML documentation.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[all]\"\npip install -r docs/requirements.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\nLANGUAGE: bash\nCODE:\n```\n# On macOS\nopen _build/html/index.html\n\n# On Linux\nxdg-open _build/html/index.html\n\n# On Windows\nstart _build/html/index.html\n```\n\n----------------------------------------\n\nTITLE: Verify Python A2A Installation - Python\nDESCRIPTION: Executes Python code to import the `python_a2a` library and print its version attribute. This is a standard method to confirm successful installation and check the installed package version from the command line or within a script.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport python_a2a\nprint(python_a2a.__version__)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Content Type (Python)\nDESCRIPTION: This snippet shows how to create a custom content type,  `ChartContent`, using dataclasses. It defines its attributes (type, title, labels, data, and chart_type) and a `to_dict` method for conversion to a dictionary.  This allows the agent to handle custom data formats.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/guides/advanced.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom python_a2a import A2AServer, Message, MessageRole, BaseModel\n    from dataclasses import dataclass\n    from typing import Dict, Any, List\n    \n    # Define a custom content type\n    @dataclass\n    class ChartContent(BaseModel):\n        \"\"\"Chart content type\"\"\"\n        type: str = \"chart\"\n        title: str = \"\"\n        labels: List[str] = None\n        data: List[float] = None\n        chart_type: str = \"bar\"  # bar, line, pie, etc.\n        \n        def to_dict(self) -> Dict[str, Any]:\n            \"\"\"Convert to dictionary representation\"\"\"\n            return {\n                \"type\": self.type,\n                \"title\": self.title,\n                \"labels\": self.labels,\n                \"data\": self.data,\n                \"chart_type\": self.chart_type\n            }\n```\n\n----------------------------------------\n\nTITLE: Install Python A2A with Anthropic Integration - Bash\nDESCRIPTION: Installs `python-a2a` along with the dependencies required for integrating with Anthropic Claude. Use this command with pip and the `[anthropic]` extra.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/installation.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install \"python-a2a[anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Synchronous Message Handling Wrapper in Python A2A\nDESCRIPTION: Provides a synchronous wrapper around the asynchronous `handle_message_async` method. It retrieves the current asyncio event loop and runs the asynchronous handler until it completes, effectively blocking until the asynchronous operation is finished. This allows calling the asynchronous logic from synchronous contexts.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef handle_message(self, message):\n    \"\"\"Override to use our async handler\"\"\"\n    import asyncio\n    loop = asyncio.get_event_loop()\n    return loop.run_until_complete(self.handle_message_async(message))\n```\n\n----------------------------------------\n\nTITLE: Setting up python-a2a Development Environment\nDESCRIPTION: This snippet demonstrates setting up a development environment for python-a2a, using UV and a virtual environment. It clones the repository, creates and activates a virtual environment, installs the package in editable mode, and runs tests.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/uv-installation.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\n    git clone https://github.com/themanojdesai/python-a2a.git\n    cd python-a2a\n\n    # Create a virtual environment and install in development mode\n    uv venv create .venv\n    source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n    uv pip install -e \".[dev]\"\n\n    # Run tests\n    uv pip run pytest\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Python A2A with UV\nDESCRIPTION: This Dockerfile sets up a Docker image for a Python application using UV for package management.  It installs UV, sets environment variables, copies the application code, installs the application dependencies with all extras and defines the command to run the application.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/uv-installation.rst#_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM python:3.9-slim\n\n    # Install UV\n    RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n\n    # Set environment variables\n    ENV PATH=\"/root/.cargo/bin:${PATH}\"\n\n    # Install Python A2A\n    WORKDIR /app\n    COPY . .\n    RUN uv pip install \".[all]\"\n\n    # Run your application\n    CMD [\"python\", \"your_app.py\"]\n```\n\n----------------------------------------\n\nTITLE: Cloning the Python A2A Repository using Git\nDESCRIPTION: Clones the forked `python-a2a` repository from GitHub to a local machine, allowing developers to modify the codebase. The `cd` command then navigates into the cloned directory. Requires Git to be installed and configured.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/contributing.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:YOUR_USERNAME/python-a2a.git\ncd python-a2a\n```\n\n----------------------------------------\n\nTITLE: Installing UV on macOS/Linux via curl\nDESCRIPTION: This snippet installs UV (Ultraviolet) on macOS or Linux systems using a curl command. It fetches and executes an installation script from astral.sh.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/uv-installation.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Installing UV on Windows via PowerShell\nDESCRIPTION: This snippet installs UV (Ultraviolet) on Windows systems using a PowerShell command. It retrieves and executes an installation script from astral.sh.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/uv-installation.rst#_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nirm https://astral.sh/uv/install.ps1 | iex\n```\n\n----------------------------------------\n\nTITLE: Installing python-a2a with UV\nDESCRIPTION: This bash snippet installs the base python-a2a package using UV's pip command. It installs the package from PyPI or another configured source.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/uv-installation.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install python-a2a\n```\n\n----------------------------------------\n\nTITLE: Using Makefile Commands with UV\nDESCRIPTION: These bash snippets show examples of common tasks managed through a Makefile for the python-a2a project.  They use UV commands internally.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/uv-installation.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Set up development environment\n    make setup\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Run tests\n    make test\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Format code\n    make format\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Lint code\n    make lint\n```\n\n----------------------------------------\n\nTITLE: Cloning the Forked Repository (Bash)\nDESCRIPTION: These commands clone the user's forked 'python-a2a' repository from GitHub to their local machine and then change the current directory into the newly cloned project folder. This is the first step in setting up the local development environment after forking.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone git@github.com:YOUR_USERNAME/python-a2a.git\ncd python-a2a\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents using Sphinx toctree\nDESCRIPTION: This reStructuredText snippet utilizes the Sphinx `toctree` directive to generate a table of contents. It links to two other pages, 'simple' and 'advanced', presumably containing example code for the Python A2A library. The `maxdepth: 2` option controls the depth of the generated table of contents.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/index.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   \n   simple\n   advanced\n```\n\n----------------------------------------\n\nTITLE: Opening Generated Documentation in Browser (bash)\nDESCRIPTION: This snippet demonstrates how to open the generated HTML documentation file (_build/html/index.html) in a web browser depending on the operating system: macOS uses the open command, Linux uses xdg-open, and Windows uses start. This facilitates quick local review of the built docs.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/DOCUMENTATION.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# On macOS\nopen _build/html/index.html\n\n# On Linux\nxdg-open _build/html/index.html\n\n# On Windows\nstart _build/html/index.html\n```\n\n----------------------------------------\n\nTITLE: Running the Travel Agent - Python\nDESCRIPTION: This snippet demonstrates how to run the TravelAgent. It initializes the agent and then runs it using `run_server`. This assumes that the TravelAgent class and the `run_server` function are defined in another part of the code.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/docs/examples/advanced.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    # Run the server\n    if __name__ == \"__main__\":\n        agent = TravelAgent()\n        run_server(agent, port=5002)\n```\n\n----------------------------------------\n\nTITLE: Building HTML Documentation Using Make (bash)\nDESCRIPTION: This snippet uses the make utility in the docs directory to build HTML files for the Python A2A documentation. The makefile is provided by Sphinx and compiles reStructuredText sources into static HTML pages which can be viewed in a web browser.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/DOCUMENTATION.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Serializing a Message to JSON\nDESCRIPTION: This snippet serializes a `Message` object to a JSON string using the `to_json()` method. The resulting JSON string represents the message's structure and content. The `print()` function then displays the JSON output.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Serialize a message to JSON\njson_message = text_message.to_json()\nprint(json_message)\n```\n\n----------------------------------------\n\nTITLE: Deserializing Conversation from JSON\nDESCRIPTION: This snippet shows deserializing a `Conversation` object from a JSON string using the `from_json()` method. The resulting `Conversation` is displayed using `pretty_print_conversation()` to demonstrate successful deserialization.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/notebooks/01_basic_agent_conversation.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Deserialize a conversation from JSON\ndeserialized_conversation = Conversation.from_json(json_conversation)\npretty_print_conversation(deserialized_conversation)\n```\n\n----------------------------------------\n\nTITLE: No code snippets present\nDESCRIPTION: The provided content contains only descriptive text and no code snippets for analysis.\nSOURCE: https://github.com/themanojdesai/python-a2a/blob/main/examples/README.md#_snippet_0\n\n"
  }
]
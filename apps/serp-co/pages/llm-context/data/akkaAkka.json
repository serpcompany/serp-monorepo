[
  {
    "owner": "akka",
    "repo": "akka",
    "content": "TITLE: Implementing a Persistent Actor in Scala\nDESCRIPTION: Example of a Scala implementation of a PersistentActor that demonstrates event sourcing pattern. The actor persists events, recovers state from events, and handles commands by generating and persisting events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass ExamplePersistentActor extends PersistentActor {\n  \n  var state = ExampleState()\n  \n  override def persistenceId: String = \"sample-id-1\"\n  \n  override def receiveRecover: Receive = {\n    case evt: Evt => updateState(evt)\n    case SnapshotOffer(_, snapshot: ExampleState) => state = snapshot\n  }\n  \n  override def receiveCommand: Receive = {\n    case cmd: Cmd => persist(Evt(cmd.data)) { evt =>\n      updateState(evt)\n      context.system.eventStream.publish(evt)\n      if (lastSequenceNr % 1000 == 0)\n        saveSnapshot(state)\n    }\n    case \"snap\"  => saveSnapshot(state)\n    case SaveSnapshotSuccess(metadata) => // ...\n    case SaveSnapshotFailure(metadata, reason) => // ...\n  }\n  \n  def updateState(evt: Evt): Unit =\n    state = state.updated(evt)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in Build Tools\nDESCRIPTION: Defines the Akka library repository URL for build tools (SBT, Maven, Gradle). This configuration is necessary for the build tool to locate and download Akka artifacts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository and Dependency in Build Tools\nDESCRIPTION: Shows how to add the Akka repository URL and include the akka-actor-typed dependency in SBT, Maven, and Gradle build configurations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_0\n\nLANGUAGE: markup\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=\"com.typesafe.akka\"\n  artifact=\"akka-actor-typed_$scala.binary.version$\"\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Room Behavior with Akka Typed AbstractBehavior in Java\nDESCRIPTION: This snippet shows the implementation of a chat room actor using Akka Typed's AbstractBehavior in Java. The actor manages its state through class fields, processes RoomCommand messages, adds new session actors on GetSession, and disseminates messages to connected clients. Actor context is injected in the constructor and the behavior uses Java collections to track sessions. Akka Typed Java DSL is required, and the actor maintains protocol integrity by controlling access to internal commands.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\n// Akka Typed chat room actor implementation (Java)\npublic class ChatRoom extends AbstractBehavior<RoomCommand> {\n  private final List<ActorRef<SessionCommand>> sessions = new ArrayList<>();\n  public static Behavior<RoomCommand> create() {\n    return Behaviors.setup(ChatRoom::new);\n  }\n  private ChatRoom(ActorContext<RoomCommand> context) {\n    super(context);\n  }\n  @Override\n  public Behavior<RoomCommand> onMessage(RoomCommand msg) {\n    // Message handling and session management logic\n    return this;\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Starting Actor System in Scala\nDESCRIPTION: Initializes the Akka actor system and sends initial messages to start the interaction between actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nobject HelloWorldApp {\n  def main(args: Array[String]): Unit = {\n    val system: ActorSystem[HelloWorldMain.SayHello] =\n      ActorSystem(HelloWorldMain(), \"hello\")\n\n    system ! HelloWorldMain.SayHello(\"World\")\n    system ! HelloWorldMain.SayHello(\"Akka\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Transform Source in Scala\nDESCRIPTION: This Scala snippet details transforming a source stream using the scan operator to compute factorials and describes further stream processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [QuickStartDocSpec.scala](/akka-docs/src/test/scala/docs/stream/QuickStartDocSpec.scala) { #transform-source }\n```\n\n----------------------------------------\n\nTITLE: Testing Actors in Akka with ActorTestKit - Scala\nDESCRIPTION: This Scala snippet demonstrates the basic setup for testing actors in Akka using ActorTestKit. It includes creating an actor system, spawning actors, and shutting down the system. The example requires Akka dependencies and ScalaTest for easier test management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AsyncTestingExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/AsyncTestingExampleSpec.scala) { #under-test }\n```\n\n----------------------------------------\n\nTITLE: Adding Akka TestKit Dependencies\nDESCRIPTION: Maven/SBT/Gradle dependency configuration for adding Akka TestKit to a project. Uses the akka-actor-testkit-typed module with proper Scala binary version compatibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nbomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\nsymbol1=AkkaVersion\nvalue1=\"$akka.version$\"\ngroup=com.typesafe.akka\nartifact=akka-actor-testkit-typed_$scala.binary.version$\nversion=AkkaVersion\nscope=test\n```\n\n----------------------------------------\n\nTITLE: Importing Akka Stream Tools in Scala\nDESCRIPTION: The snippet shows how to import necessary Akka Stream tools in a Scala file to get started with stream creation and execution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [QuickStartDocSpec.scala](/akka-docs/src/test/scala/docs/stream/QuickStartDocSpec.scala) { #stream-imports }\n```\n\n----------------------------------------\n\nTITLE: Implementing Filter Transformation with GraphStage in Java\nDESCRIPTION: This snippet shows the Java implementation of a filter operator using GraphStage. It demonstrates how to create a many-to-one transformation that selectively passes elements downstream based on a predicate.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\npublic class Filter<A> extends GraphStage<FlowShape<A, A>> {\n  private final Inlet<A> in = Inlet.create(\"Filter.in\");\n  private final Outlet<A> out = Outlet.create(\"Filter.out\");\n  private final Predicate<A> p;\n\n  public Filter(Predicate<A> p) {\n    this.p = p;\n  }\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                A elem = grab(in);\n                if (p.test(elem)) push(out, elem);\n                else pull(in);\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() {\n                pull(in);\n              }\n            });\n      }\n    };\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor Lifecycle in Scala\nDESCRIPTION: Illustrates how to handle actor lifecycle events, specifically the PostStop signal, in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\nobject StartStopActor1 {\n  def apply(): Behavior[String] =\n    Behaviors.setup { context =>\n      println(\"first started\")\n      context.spawn(StartStopActor2(), \"second\")\n\n      Behaviors.receiveMessage { message =>\n        println(s\"first received: $message\")\n        Behaviors.stopped\n      }.receiveSignal {\n        case (_, PostStop) =>\n          println(\"first stopped\")\n          Behaviors.same\n      }\n    }\n}\n\nobject StartStopActor2 {\n  def apply(): Behavior[String] =\n    Behaviors.setup { context =>\n      println(\"second started\")\n\n      Behaviors.receiveSignal { case (_, PostStop) =>\n        println(\"second stopped\")\n        Behaviors.same\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Actor Lifecycle Example in Scala\nDESCRIPTION: Demonstrates how to create and stop actors to observe lifecycle events in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\nobject Main extends App {\n  val testSystem = ActorSystem(\n    Behaviors.setup[String] { context =>\n      val first = context.spawn(StartStopActor1(), \"first\")\n      first ! \"stop\"\n      Behaviors.empty\n    },\n    \"testSystem\"\n  )\n}\n```\n\n----------------------------------------\n\nTITLE: State Management with Latest Items List in Akka - Java\nDESCRIPTION: This snippet describes state management for a persistent actor, using a list to store the five latest items, as implemented in Akka's Java API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BasicPersistentBehaviorTest.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/BasicPersistentBehaviorTest.java) { #state }\n```\n\n----------------------------------------\n\nTITLE: Testing Query Timeout\nDESCRIPTION: Test case for handling scenarios where devices don't respond within the timeout period.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n#query-test-timeout\n```\n\nLANGUAGE: Java\nCODE:\n```\n#query-test-timeout\n```\n\n----------------------------------------\n\nTITLE: Complete device actor implementation with read and write protocols in Akka Typed\nDESCRIPTION: Full implementation of the device actor that handles both temperature queries (read) and temperature recording (write) with proper message correlation and acknowledgments.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n#full-device\n```\n\nLANGUAGE: Java\nCODE:\n```\n#full-device\n```\n\n----------------------------------------\n\nTITLE: Initializing Cluster Sharding for an Entity Type in Scala\nDESCRIPTION: Demonstrates how to initialize Akka Cluster Sharding for a specific entity type (`Counter`) in Scala. This setup registers the entity type and its behavior factory with the sharding extension.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.cluster.sharding.typed.scaladsl.Entity\nimport akka.cluster.sharding.typed.scaladsl.EntityTypeKey\n\nval TypeKey = EntityTypeKey[Counter.Command](\"Counter\")\n\n// #init\nval shardRegion: ActorRef[ShardingEnvelope[Counter.Command]] = sharding.init(Entity(TypeKey)(\n  createBehavior = entityContext =>\n    Counter(entityContext.entityId)\n))\n// #init\n```\n\n----------------------------------------\n\nTITLE: Configuring Recommended Strategy with Limit and Idle Timeout (HOCON)\nDESCRIPTION: This HOCON configuration combines the recommended default passivation strategy with an active entity limit of 100,000 and also enables idle entity passivation. Entities will be passivated if the active limit is reached OR if they remain idle for the specified `idle-timeout` (defaulting to 2 minutes if not explicitly set elsewhere).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_30\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.sharding {\n  passivation {\n    strategy = default-strategy\n    default-strategy {\n      # approximately 100k entities per node\n      limit = 100000\n      # also passivate entities that have been idle for 10 minutes\n      idle-entity {\n        timeout = 10 minutes\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Akka Supervision Example in Scala\nDESCRIPTION: This Scala snippet demonstrates how to run the actor supervision experiment. It creates an `ActorSystem` using the `SupervisingActor` behavior. It then sends a \"failChild\" message to the `SupervisingActor`, which in turn tells its child (`SupervisedActor`) to fail, thus triggering the restart supervision strategy.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nval system = ActorSystem(SupervisingActor(), \"ActorHierarchyExperiments\")\nsystem ! \"failChild\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Sharded Actor Behavior in Java\nDESCRIPTION: Illustrates the definition of a simple `Counter` actor behavior in Java, designed for use with Akka Cluster Sharding. It processes `Increment` and `GetValue` commands.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.typed.ActorRef;\nimport akka.actor.typed.Behavior;\nimport akka.actor.typed.javadsl.*;\nimport akka.cluster.sharding.typed.javadsl.EntityTypeKey;\n\n// #counter\npublic class Counter extends AbstractBehavior<Counter.Command> {\n\n  public interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    private final ActorRef<Integer> replyTo;\n\n    public GetValue(ActorRef<Integer> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final EntityTypeKey<Command> TypeKey =\n      EntityTypeKey.create(Command.class, \"Counter\");\n\n  private int value = 0;\n\n  private Counter(ActorContext<Command> context) {\n    super(context);\n  }\n\n  public static Behavior<Command> create(String entityId) {\n    return Behaviors.setup(context -> new Counter(context));\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Increment.class, msg -> onIncrement())\n        .onMessage(GetValue.class, this::onGetValue)\n        .build();\n  }\n\n  private Behavior<Command> onIncrement() {\n    value++;\n    return this;\n  }\n\n  private Behavior<Command> onGetValue(GetValue msg) {\n    msg.replyTo.tell(value);\n    return this;\n  }\n}\n// #counter\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chatroom Gabbler Actor in Java\nDESCRIPTION: This Java implementation of the Gabbler actor defines behavior that handles session events from a chat room. It joins the session, posts a message, and terminates after receiving its own message back. The actor uses a setup pattern to initialize its behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_21\n\nLANGUAGE: java\nCODE:\n```\npublic class Gabbler extends AbstractBehavior<ChatRoom.SessionEvent> {\n\n  public static Behavior<ChatRoom.SessionEvent> create() {\n    return Behaviors.setup(Gabbler::new);\n  }\n\n  private Gabbler(ActorContext<ChatRoom.SessionEvent> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<ChatRoom.SessionEvent> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(ChatRoom.SessionDenied.class, this::onSessionDenied)\n        .onMessage(ChatRoom.SessionGranted.class, this::onSessionGranted)\n        .onMessage(ChatRoom.MessagePosted.class, this::onMessagePosted)\n        .build();\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onSessionDenied(ChatRoom.SessionDenied message) {\n    getContext().getLog().info(\"cannot start chat session: {}\", message.reason);\n    return Behaviors.stopped();\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onSessionGranted(ChatRoom.SessionGranted message) {\n    message.handle.tell(new ChatRoom.PostMessage(\"Hello World!\"));\n    return this;\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onMessagePosted(ChatRoom.MessagePosted message) {\n    getContext()\n        .getLog()\n        .info(\"message has been posted by '{}': {}\", message.screenName, message.message);\n    if (message.message.equals(\"Hello World!\")) {\n      return Behaviors.stopped();\n    } else {\n      return this;\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Temperature Reading Protocol in Scala\nDESCRIPTION: Defines a simple request-respond message protocol for a device actor to handle temperature reading requests. The protocol includes a ReadTemperature message for requests and a RespondTemperature message for responses that may or may not contain a temperature value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\ncase class ReadTemperature(replyTo: ActorRef[RespondTemperature])\ncase class RespondTemperature(value: Option[Double])\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic EventSourcedBehavior Structure in Scala\nDESCRIPTION: Minimum required structure for creating an EventSourcedBehavior in Scala, including persistenceId, emptyState, commandHandler, and eventHandler components.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nobject BasicPersistentBehaviorCompileOnly {\n  trait Command\n  trait Event\n  trait State\n\n  val behavior: EventSourcedBehavior[Command, Event, State] =\n    EventSourcedBehavior.withEnforcedReplies[\n      Command,\n      Event,\n      State](persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n        emptyState = null.asInstanceOf[State],\n        commandHandler = (state, cmd) => Effect.none.thenNoReply(),\n        eventHandler = (state, evt) => null.asInstanceOf[State])\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Restart Supervision Strategy in Scala\nDESCRIPTION: This Scala snippet defines actors to demonstrate Akka Typed's supervision capabilities. A `SupervisingActor` creates a `SupervisedActor` child using `Behaviors.supervise` combined with `SupervisorStrategy.restart`. The `SupervisedActor` is designed to throw an exception upon receiving a \"fail\" message, which triggers the restart strategy defined by its parent.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.typed.ActorRef\nimport akka.actor.typed.ActorSystem\nimport akka.actor.typed.Behavior\nimport akka.actor.typed.SupervisorStrategy\nimport akka.actor.typed.Terminated\nimport akka.actor.typed.scaladsl.AbstractBehavior\nimport akka.actor.typed.scaladsl.ActorContext\nimport akka.actor.typed.scaladsl.Behaviors\n\nobject SupervisingActor {\n  def apply(): Behavior[String] = Behaviors.setup(context => new SupervisingActor(context))\n}\n\nclass SupervisingActor(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n  private val child = context.spawn(\n    Behaviors.supervise(SupervisedActor()).onFailure(SupervisorStrategy.restart),\n    name = \"supervised-actor\")\n\n  override def onMessage(msg: String): Behavior[String] = {\n    msg match {\n      case \"failChild\" =>\n        child ! \"fail\"\n        this\n    }\n  }\n}\n\nobject SupervisedActor {\n  def apply(): Behavior[String] = Behaviors.setup(context => new SupervisedActor(context))\n}\n\nclass SupervisedActor(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n  println(\"supervised actor started\")\n\n  override def onMessage(msg: String): Behavior[String] = {\n    msg match {\n      case \"fail\" =>\n        println(\"supervised actor fails now\")\n        throw new Exception(\"I failed!\")\n    }\n  }\n\n  override def onSignal: PartialFunction[akka.actor.typed.Signal, Behavior[String]] = {\n    case akka.actor.typed.PreRestart =>\n      println(\"supervised actor will be restarted\")\n      this\n    case akka.actor.typed.PostStop =>\n      println(\"supervised actor stopped\")\n      this\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Serializer Implementation\nDESCRIPTION: Example implementation of a custom serializer by extending Akka's Serializer class.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nclass MyOwnSerializer extends Serializer {\n  val identifier = 1234567\n\n  def includeManifest: Boolean = false\n\n  def toBinary(obj: AnyRef): Array[Byte] = {\n    // Put your code here\n    Array[Byte]()\n  }\n\n  def fromBinary(bytes: Array[Byte], manifest: Option[Class[_]]): AnyRef = {\n    // Put your code here\n    \"deserializedText\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Reply Handler Implementation in Java\nDESCRIPTION: Implementation of reply handling logic in a persistent actor using Java. Demonstrates how to send replies after handling commands.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_31\n\nLANGUAGE: Java\nCODE:\n```\n#reply\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency (sbt, Maven, Gradle)\nDESCRIPTION: This snippet shows how to declare the Akka Streams module as a dependency across sbt, Maven, and Gradle. The dependency declaration uses project or build tool placeholders for Akka version and Scala binary version. Ensure the variables (e.g., $akka.version$, $scala.binary.version$) are resolved correctly in your build environment.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt,Maven,Gradle\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=\"com.typesafe.akka\"\n  artifact=\"akka-stream_$scala.binary.version$\"\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Nesting Actor Behavior Setup in Scala\nDESCRIPTION: Demonstrates how to nest setup, withTimers, and withStash to access multiple dependencies in Akka Typed actors. This pattern provides access to all needed resources.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\nBehaviors.setup[Command] { context =>\n  // access to context\n  Behaviors.withTimers { timers =>\n    // access to timers\n    Behaviors.withStash(100) { stash =>\n      // access to stash\n      // behavior using context, timers, and stash\n      // ...\n      Behaviors.empty\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Dependency in sbt/Maven/Gradle (Scala/Java build tools)\nDESCRIPTION: Demonstrates how to set up the Akka Streams library repository and add the Akka Streams module dependency to Scala or Java projects using sbt, Maven, or Gradle. Required for initializing and running all Akka Streams recipes. Includes BOM and version symbol usage. Key parameters include group, artifact, version, and repository URL. Outputs standard dependency declarations for integration into project build files.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt/Maven/Gradle\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\\\"akka-repository\\\"\nname=\\\"Akka library repository\\\"\nurl=\\\"https://repo.akka.io/maven\\\"\n}\n```\n\nLANGUAGE: sbt/Maven/Gradle\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\\\"$akka.version$\\\"\n  group=\\\"com.typesafe.akka\\\"\n  artifact=\\\"akka-stream_$scala.binary.version$\\\"\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Improved write protocol with acknowledgment for device actor in Akka Typed\nDESCRIPTION: Enhanced write protocol that includes acknowledgment messages and correlation IDs. This design ensures that the sender can confirm when temperature records have been successfully processed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n#write-protocol\n```\n\nLANGUAGE: Java\nCODE:\n```\n#write-protocol\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Actor for Chatroom System in Java\nDESCRIPTION: This Java code creates the main actor that sets up the chat room system. It spawns the ChatRoom and Gabbler actors, initiates a session between them, and monitors the gabbler's lifecycle to shut down when it terminates. This demonstrates proper actor system organization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_23\n\nLANGUAGE: java\nCODE:\n```\npublic class Main extends AbstractBehavior<Void> {\n\n  public static Behavior<Void> create() {\n    return Behaviors.setup(Main::new);\n  }\n\n  private Main(ActorContext<Void> context) {\n    super(context);\n    ActorRef<ChatRoom.RoomCommand> chatRoom = context.spawn(ChatRoom.create(), \"chatRoom\");\n    ActorRef<ChatRoom.SessionEvent> gabbler = context.spawn(Gabbler.create(), \"gabbler\");\n    context.watch(gabbler);\n    chatRoom.tell(new ChatRoom.GetSession(\"ol' Gabbler\", gabbler));\n  }\n\n  @Override\n  public Receive<Void> createReceive() {\n    return newReceiveBuilder().onSignal(Terminated.class, sig -> Behaviors.stopped()).build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing read and write functionality for device actor in Akka Typed\nDESCRIPTION: Comprehensive test case that exercises both the temperature query and recording functionality of the device actor, verifying that recorded temperatures can be subsequently queried.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n#device-write-read-test\n```\n\nLANGUAGE: Java\nCODE:\n```\n#device-write-read-test\n```\n\n----------------------------------------\n\nTITLE: Printing Actor References in Scala\nDESCRIPTION: Demonstrates how to create actors and print their references in Scala, showing the hierarchical structure of actor paths.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nobject ActorHierarchyExperiments extends App {\n  val testSystem = ActorSystem(Behaviors.setup[String] { context =>\n    val firstRef = context.spawn(PrintMyActorRefActor(), \"first-actor\")\n    println(s\"First: $firstRef\")\n    firstRef ! \"printit\"\n    Behaviors.empty\n  }, \"testSystem\")\n}\n\nobject PrintMyActorRefActor {\n  def apply(): Behavior[String] =\n    Behaviors.setup { context =>\n      Behaviors.receiveMessage { message =>\n        val secondRef = context.spawn(Behaviors.empty[String], \"second-actor\")\n        println(s\"Second: $secondRef\")\n        Behaviors.same\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Materializing Akka Streams Step-by-Step in Java\nDESCRIPTION: Shows the construction of a RunnableGraph in Java by connecting a Source, Flow, and Sink. It explains how to materialize and run the graph, obtaining a MaterializedMap containing the materialized values, such as the CompletionStage from a `Sink.fold`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #materialization-in-steps }\n```\n\n----------------------------------------\n\nTITLE: Defining a Persistent Sharded Entity in Scala\nDESCRIPTION: Provides an example of a persistent entity (`HelloWorld`) using Akka Persistence Typed in Scala, suitable for cluster sharding. It defines commands, events, state, and the event-sourced behavior, using the entity ID as part of the persistence ID.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\n// #persistent-entity\nimport akka.actor.typed.{ ActorRef, Behavior }\nimport akka.cluster.sharding.typed.scaladsl.{ EntityContext, EntityTypeKey }\nimport akka.persistence.typed.PersistenceId\nimport akka.persistence.typed.scaladsl.{ Effect, EventSourcedBehavior }\n\nobject HelloWorldPersistentEntityExample {\n\n  object HelloWorld {\n    // Command\n    sealed trait Command extends CborSerializable\n    final case class Greet(name: String)(val replyTo: ActorRef[Greeting]) extends Command\n    // Response\n    final case class Greeting(message: String) extends CborSerializable\n\n    // Event\n    final case class Greeted(name: String) extends CborSerializable\n\n    // State\n    final case class KnownNames(names: Set[String]) extends CborSerializable {\n      def add(name: String): KnownNames = copy(names = names + name)\n      def contains(name: String): Boolean = names.contains(name)\n    }\n\n    val TypeKey = EntityTypeKey[Command](\"HelloWorld\")\n\n    def apply(entityContext: EntityContext[Command]): Behavior[Command] = {\n      EventSourcedBehavior[Command, Greeted, KnownNames](\n        persistenceId = PersistenceId(entityContext.entityTypeKey.name, entityContext.entityId),\n        emptyState = KnownNames(Set.empty),\n        commandHandler =\n          (state, command) => commandHandler(entityContext.entityId, state, command),\n        eventHandler = (state, event) => eventHandler(state, event))\n    }\n\n    private def commandHandler(\n        entityId: String,\n        state: KnownNames,\n        command: Command): Effect[Greeted, KnownNames] =\n      command match {\n        case Greet(name) =>\n          if (state.contains(name)) {\n            // Been greeted before, just reply\n            Effect.reply(command.replyTo)(Greeting(s\"Hi $name, already greeted you in $entityId\"))\n          } else {\n            // Haven't been greeted before, persist event and reply\n            Effect.persist(Greeted(name))\n              .thenReply(command.replyTo)(_ => Greeting(s\"Hello, $name, from $entityId!\"))\n          }\n      }\n\n    private def eventHandler(state: KnownNames, event: Greeted): KnownNames = event match {\n      case Greeted(name) => state.add(name)\n    }\n\n  }\n}\n// #persistent-entity\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Sink GraphStage (Scala)\nDESCRIPTION: This Scala snippet demonstrates the implementation of a custom Sink operator by subclassing GraphStage and providing an InHandler. The code illustrates how to process or consume incoming elements via the onPush callback and request data from upstream. It follows the Akka convention of initializing demand in preStart.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #custom-sink-example }\n```\n\n----------------------------------------\n\nTITLE: Using Custom Graph Components in Scala\nDESCRIPTION: Demonstrates using custom worker pool junction components in a stream graph.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n#graph-dsl-components-use\n```\n\n----------------------------------------\n\nTITLE: Handling Incoming TCP Connections with Akka Streams in Scala and Java\nDESCRIPTION: This snippet demonstrates handling each incoming TCP connection using a Flow that processes and transforms ByteString messages. It requires Akka Streams, with inputs being IncomingConnection elements and outputs processed ByteStrings. Constraints include the Flow being materialized only once per connection.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [StreamTcpDocSpec.scala](/akka-docs/src/test/scala/docs/stream/io/StreamTcpDocSpec.scala) { #echo-server-simple-handle }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [StreamTcpDocTest.java](/akka-docs/src/test/java/jdocs/stream/io/StreamTcpDocTest.java) { #echo-server-simple-handle }\n```\n\n----------------------------------------\n\nTITLE: Implementing Ask Pattern with Producer in Java\nDESCRIPTION: Java implementation example of using the ask pattern in an image converter work manager for message handling confirmation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/reliable-delivery.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nWorkPullingDocExample.java\n```\n\n----------------------------------------\n\nTITLE: Recovery Signal Handler in Scala\nDESCRIPTION: Example of handling recovery completion signals in Scala persistent actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_34\n\nLANGUAGE: Scala\nCODE:\n```\n#recovery\n```\n\n----------------------------------------\n\nTITLE: Building and Device Entity Definitions in Scala\nDESCRIPTION: Scala example showing entity definitions with custom shard allocation for buildings and devices\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\nobject Building {\\n  val TypeKey = EntityTypeKey[Command](\\\"Building\\\")\\n  \\n  sealed trait Command extends CborSerializable\\n  final case class BuildingEnvelope(buildingId: String) {\\n    val shardId: String = math.abs(buildingId.hashCode % 30).toString\\n  }\\n}\\n\\nobject Device {\\n  val TypeKey = EntityTypeKey[Command](\\\"Device\\\")\\n  \\n  sealed trait Command extends CborSerializable\\n  final case class DeviceEnvelope(buildingId: String, deviceId: String) {\\n    val shardId: String = math.abs(buildingId.hashCode % 30).toString\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Akka Streams Twitter Example in Java\nDESCRIPTION: Full example of processing a stream of tweets to extract authors tweeting about Akka in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_31\n\nLANGUAGE: Java\nCODE:\n```\nfinal ActorSystem system = ActorSystem.create(\"reactive-tweets\");\n\nSource<Tweet, NotUsed> tweets = Source.from(Collections.emptySet());\n\nSource<Author, NotUsed> authors =\n    tweets\n        .filter(tweet -> tweet.hashtags().contains(new Hashtag(\"#akka\")))\n        .map(Tweet::author);\n\nauthors.runWith(Sink.foreach(System.out::println), system);\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Akka Streams with GraphDSL in Java\nDESCRIPTION: Illustrates building a non-linear stream graph using `GraphDSL` in Java to broadcast elements from a single source (`tweets`) to multiple sinks (`writeAuthors` and `writeHashtags`). It explicitly uses the graph builder (`b`) and methods like `add()`, `from()`, `via()`, and `to()` to define the stream topology with a `Broadcast` junction.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_35\n\nLANGUAGE: java\nCODE:\n```\nRunnableGraph.fromGraph(GraphDSL.create(b -> {\n  final UniformFanOutShape<Tweet, Tweet> bcast = b.add(Broadcast.create(2));\n  b.from(tweets).toInlet(bcast.in());\n  b.from(bcast.out(0)).via(Flow.of(Tweet.class).map(t -> t.author)).to(writeAuthors);\n  b.from(bcast.out(1)).via(Flow.of(Tweet.class).mapConcat(t -> new ArrayList<>(t.getHashtags()))).to(writeHashtags);\n  return ClosedShape.getInstance();\n})).run(materializer);\n//#graph-dsl-broadcast\n```\n\n----------------------------------------\n\nTITLE: Defining Message Protocols in Akka\nDESCRIPTION: Examples demonstrate best practices for defining actor message protocols in Akka Typed. Messages are encapsulated with the defining actor's name for clarity, and the use of nested classes or companion objects is recommended for organization and encapsulation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\nStyleGuideDocExamples.scala { #messages }\n```\n\nLANGUAGE: Java\nCODE:\n```\nStyleGuideDocExamples.java { #messages }\n```\n\n----------------------------------------\n\nTITLE: Implementing Hello World Bot Actor in Scala\nDESCRIPTION: Creates a bot actor that interacts with the Greeter actor by sending multiple greetings and tracking responses. Shows state management through behavior changes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nobject HelloWorldBot {\n  def apply(max: Int): Behavior[HelloWorld.Greeted] = {\n    bot(0, max)\n  }\n\n  private def bot(greetingCounter: Int, max: Int): Behavior[HelloWorld.Greeted] =\n    Behaviors.receive { (context, message) =>\n      val n = greetingCounter + 1\n      context.log.info(\"Greeting {} for {}\", n, message.whom)\n      if (n == max) {\n        Behaviors.stopped\n      } else {\n        message.from ! HelloWorld.Greet(message.whom, context.self)\n        bot(n, max)\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Balancing Jobs to a Fixed Pool of Workers in Akka Streams\nDESCRIPTION: This pattern implements a worker pool that automatically balances incoming jobs to available workers. It uses a Balance operator to distribute work and a Merge operator to collect results, with workers running in parallel.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_36\n\nLANGUAGE: Scala\nCODE:\n```\ndef balancer[In, Out](worker: Flow[In, Out, Any], workerCount: Int): Flow[In, Out, NotUsed] =\n  Flow.fromGraph(GraphDSL.create() { implicit b =>\n    import GraphDSL.Implicits._\n\n    // This is the balancer which distributes elements to workers\n    val balancer = b.add(Balance[In](workerCount, waitForAllDownstreams = false))\n\n    // These are the workers, each will get its own stream of elements\n    val workers = for (i <- 1 to workerCount) yield b.add(worker.async)\n\n    // This is the merge which takes elements from workers\n    val merge = b.add(Merge[Out](workerCount))\n\n    // Connect the balancer to the workers, and the workers to the merge\n    for (i <- 0 until workerCount) {\n      balancer.out(i) ~> workers(i) ~> merge.in(i)\n    }\n\n    FlowShape(balancer.in, merge.out)\n  })\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Room Behavior with Akka Typed AbstractBehavior in Scala\nDESCRIPTION: This snippet presents a concrete implementation of a chat room actor using Akka Typed's AbstractBehavior in Scala. It maintains session state, handles protocol commands, and spawns session actors for new clients. The behavior is defined within an OO style, leveraging mutable fields for state, and demonstrates interaction patterns via message handling. It depends on akka-actor-typed, and key parameters include the actor context and session tracking collections. The actor consumes RoomCommand messages and relays messages to all connected clients.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\n// Akka Typed chat room actor implementation (Scala)\nobject ChatRoom {\n  def apply(): Behavior[RoomCommand] =\n    Behaviors.setup(context => new ChatRoom(context))\n  // ...\n}\n\nclass ChatRoom(context: ActorContext[RoomCommand]) extends AbstractBehavior[RoomCommand](context) {\n  private val sessions = mutable.ListBuffer.empty[ActorRef[SessionCommand]]\n  override def onMessage(msg: RoomCommand): Behavior[RoomCommand] = ...\n  // Message handling and session management logic\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Actor System in Scala\nDESCRIPTION: Main actor that sets up the actor system and initiates interaction between Greeter and Bot actors. Shows actor creation and message sending.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nobject HelloWorldMain {\n  final case class SayHello(name: String)\n\n  def apply(): Behavior[SayHello] =\n    Behaviors.setup { context =>\n      val greeter = context.spawn(HelloWorld(), \"greeter\")\n\n      Behaviors.receiveMessage { message =>\n        val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)\n        greeter ! HelloWorld.Greet(message.name, replyTo)\n        Behaviors.same\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Chat Room Protocol with Akka Typed in Scala\nDESCRIPTION: This snippet defines the message protocol for a chat room using Akka Typed in Scala. It establishes the types of commands actors can receive, such as GetSession and PublishSessionMessage, and sets up the expected message exchange flow necessary for clients to interact with the chat room. Actors use these message types to establish sessions, post messages, and notify other clients. It depends on akka-actor-typed for typed actors, and the protocol forms the interface for both clients and actors within the system.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\n/* Definitions of chat room protocol messages (Scala) */\nsealed trait RoomCommand\nfinal case class GetSession(screenName: String, replyTo: ActorRef[SessionGranted]) extends RoomCommand\nfinal case class SessionGranted(handle: ActorRef[PostMessage])\nfinal case class PostMessage(message: String) extends RoomCommand\nprivate final case class PublishSessionMessage(screenName: String, message: String) extends RoomCommand\nfinal case class MessagePosted(screenName: String, message: String)\n\n```\n\n----------------------------------------\n\nTITLE: Looking Up Distributed PubSub Topic with Akka Typed - Scala\nDESCRIPTION: Demonstrates how to look up or create a distributed pub/sub topic actor via the PubSub registry in Akka Typed using Scala. Relies on akka.actor.typed.pubsub.Topic and akka.actor.typed.pubsub.PubSub, and requires the akka-actor-typed and akka-cluster-typed modules. The key input is the topic name string, and if the topic with the specified type does not exist, it is created; otherwise, an existing ActorRef is returned. Using a different message type with the same name will throw an exception. Outputs the resolved topic ActorRef.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval topic: ActorRef[Message] = \n  PubSub(context.system).topic[Message](\"my-topic\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Room Behavior in Akka Typed - Java\nDESCRIPTION: Discusses the chat room's behavior in Java using Akka Typed, highlighting how actor sessions and message propagation are managed with `ActorRef` and specific commands.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/IntroTest.java) { #chatroom-behavior }\n```\n\n----------------------------------------\n\nTITLE: Custom Stash Buffer Configuration in Java\nDESCRIPTION: Example of configuring a custom stash buffer capacity for individual entities in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_47\n\nLANGUAGE: java\nCODE:\n```\nRefer to file: BasicPersistentBehaviorTest.java\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Actor Typed Dependencies in Build Tools\nDESCRIPTION: Declares the required dependencies for using Akka Typed Actors in SBT, Maven, or Gradle projects. It specifies the main `akka-actor-typed` artifact and the `akka-actor-testkit-typed` artifact for testing, leveraging the Akka Bill of Materials (BOM) for version management via the `AkkaVersion` symbol.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-actor-typed_$scala.binary.version$\n  version=AkkaVersion\n  group2=com.typesafe.akka\n  artifact2=akka-actor-testkit-typed_$scala.binary.version$\n  version2=AkkaVersion\n  scope2=test\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Hello World Actor in Scala\nDESCRIPTION: Defines a simple greeter actor that accepts Greet messages and responds with Greeted messages. Demonstrates basic actor message handling and replies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nobject HelloWorld {\n  final case class Greet(whom: String, replyTo: ActorRef[Greeted])\n  final case class Greeted(whom: String, from: ActorRef[Greet])\n\n  def apply(): Behavior[Greet] = Behaviors.receive { (context, message) =>\n    context.log.info(\"Hello {}!\", message.whom)\n    message.replyTo ! Greeted(message.whom, context.self)\n    Behaviors.same\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initialize Akka Actor System in Scala\nDESCRIPTION: This snippet demonstrates creating an object to start an Akka ActorSystem in Scala, making it implicit for stream operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [QuickStartDocSpec.scala](/akka-docs/src/test/scala/docs/stream/QuickStartDocSpec.scala) { #main-app }\n```\n\n----------------------------------------\n\nTITLE: Handling Last Writer Wins Event Updates in Akka Persistence - Scala\nDESCRIPTION: This Scala snippet demonstrates the event-handler logic for replicated event sourcing in Akka, specifically illustrating how to implement last writer wins (LWW) semantics. The handler likely uses the LwwTime utility to compare timestamps and update state based on which event was persisted most recently. Dependencies include Akka Persistence Typed (Scala) and custom event/state models. Inputs are event objects carrying both application data and timestamps; the output is an updated application state, ensuring the latest event by timestamp (and replica id tie-breaker) prevails. Limitations include clock skew handling and the requirement for consistent timestamp generation across all replicas.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n/* @@snip [blog](/akka-persistence-typed-tests/src/test/scala/docs/akka/persistence/typed/ReplicatedBlogExampleSpec.scala) { #event-handler } */\n```\n\n----------------------------------------\n\nTITLE: Utilizing BroadcastHub in Akka Streams - Java\nDESCRIPTION: Java code showcasing BroadcastHub usage in Akka Streams, facilitating dynamic consumer connections to a common producer. The setup adapts to the slowest consumer, emphasizing proper sequence for attaching producers and consumers. Prerequisite includes Akka Streams library.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [HubDocTest.java](/akka-docs/src/test/java/jdocs/stream/HubDocTest.java) { #broadcast-hub }\n```\n\n----------------------------------------\n\nTITLE: Connecting Akka Stream Components in Scala\nDESCRIPTION: Illustrates various methods for connecting Sources, Sinks, and Flows in Akka Streams using Scala. Examples likely include using `via()` to connect a Source to a Flow, `to()` to connect a Source or Flow to a Sink, and `toMat()` for combining materialized values.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #flow-connecting }\n```\n\n----------------------------------------\n\nTITLE: Defining Chat Room Protocol with Akka Typed in Java\nDESCRIPTION: This snippet defines a Java version of the chat room protocol using Akka Typed, including command interfaces and record messages for client interaction. It delineates the RoomCommand interface and specific message types, such as GetSession, SessionGranted, PostMessage, and the internal PublishSessionMessage (with private package access), facilitating type-safe actor communication. Java dependencies include Akka Typed and its transitive dependencies, and actors utilize these types to manage sessions and relay messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\n/* Definitions of chat room protocol messages (Java) */\ninterface RoomCommand {}\nfinal class GetSession implements RoomCommand {\n  public final String screenName;\n  public final ActorRef<SessionGranted> replyTo;\n  GetSession(String screenName, ActorRef<SessionGranted> replyTo) {...}\n}\nfinal class SessionGranted {\n  public final ActorRef<PostMessage> handle;\n  SessionGranted(ActorRef<PostMessage> handle) {...}\n}\nfinal class PostMessage implements RoomCommand {\n  public final String message;\n  PostMessage(String message) {...}\n}\nfinal class PublishSessionMessage implements RoomCommand {\n  private final String screenName;\n  private final String message;\n  PublishSessionMessage(String screenName, String message) {...}\n}\nfinal class MessagePosted {\n  public final String screenName;\n  public final String message;\n  MessagePosted(String screenName, String message) {...}\n}\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Sharding with Node Role Constraints in Java\nDESCRIPTION: Shows how to initialize Akka Cluster Sharding in Java, configuring it so that entities of this type are only allocated to nodes having the 'backend' role.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nimport akka.cluster.sharding.typed.javadsl.Entity;\nimport akka.cluster.sharding.typed.ClusterShardingSettings;\n\n// #roles\nsharding.init(\n    Entity.of(TypeKey, entityContext -> Counter.create(entityContext.getEntityId()))\n        .withSettings(ClusterShardingSettings.create(system).withRole(\"backend\")));\n// #roles\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chatroom Gabbler Actor in Scala\nDESCRIPTION: This code defines a 'Gabbler' actor that interacts with a chat room. It processes SessionEvent messages, joins the session, posts messages, and leaves after finishing its conversation. The implementation uses the functional style of Akka actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\nobject Gabbler {\n  def apply(): Behavior[SessionEvent] =\n    Behaviors.setup { context =>\n      Behaviors.receiveMessage {\n        case SessionDenied(reason) =>\n          context.log.info(\"cannot start chat session: {}\", reason)\n          Behaviors.stopped\n\n        case SessionGranted(handle) =>\n          handle ! PostMessage(\"Hello World!\")\n          Behaviors.same\n\n        case MessagePosted(screenName, message) =>\n          context.log.info(\"message has been posted by '{}': {}\", screenName, message)\n          if (message == \"Hello World!\") {\n            Behaviors.stopped\n          } else {\n            Behaviors.same\n          }\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Tagging in DurableStateBehavior for Persistence Querying (Java)\nDESCRIPTION: Shows how to implement tagging in Java DurableStatePersistentBehavior, supporting use of tags for separating subsets of persistent states in Akka Persistence Query. Prerequisites include Akka Persistence modules and a persistence query store that supports tagging. Inputs involve command/state/event objects with tags assigned; outputs are tag-based queryable persisted streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_30\n\nLANGUAGE: java\nCODE:\n```\n@@snip [DurableStatePersistentBehaviorTest.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/DurableStatePersistentBehaviorTest.java) { #tagging }\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency in Project\nDESCRIPTION: This snippet details how to add the Akka Streams module to a project using a build tool (sbt, Maven, Gradle) by specifying the group, artifact, and version.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt,Maven,Gradle\nCODE:\n```\nbomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\\nsymbol1=AkkaVersion\\nvalue1=\\\"$akka.version$\\\"\\ngroup=\\\"com.typesafe.akka\\\"\\nartifact=\\\"akka-stream_$scala.binary.version$\\\"\\nversion=AkkaVersion\n```\n\n----------------------------------------\n\nTITLE: Defining FSM Events in Scala\nDESCRIPTION: Defines the events that the Finite State Machine can receive as messages for the Actor. Includes SetTarget for initialization, Queue for adding to internal queue, and Flush for marking the end of a burst.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fsm.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsealed trait Event\nfinal case class SetTarget(ref: ActorRef[Batch]) extends Event\nfinal case class Queue(obj: Any) extends Event\ncase object Flush extends Event\n\nfinal case class Batch(obj: immutable.Seq[Any])\n```\n\n----------------------------------------\n\nTITLE: Configuring LRU Policy in Akka Cluster Sharding\nDESCRIPTION: Configures a passivation strategy using the least recently used (LRU) policy. This policy passivates entities with the least recent activity when the number of active entities exceeds the specified limit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_32\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = lru\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Chat Room Protocol in Akka Typed - Java\nDESCRIPTION: Defines the protocol for a chat room using Akka Typed in Java, managing client connections and message distribution. Usage of `ActorRef` for creating and maintaining sessions is key.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/IntroTest.java) { #chatroom-protocol }\n```\n\n----------------------------------------\n\nTITLE: Building a Simple Akka Streams Graph (Java)\nDESCRIPTION: Demonstrates constructing a basic `RunnableGraph` in Java by connecting a `Source`, two `Flow`s, and a `Sink` linearly using `via` and `to`. This example illustrates a flat, non-nested stream structure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n//##non-nested-flow\nfinal Source<Integer, NotUsed> source = Source.range(1, 10);\nfinal Sink<Integer, CompletionStage<Integer>> sink =\n    Sink.<Integer, Integer>fold(0, (agg, next) -> agg + next);\n\n// connect the Source to the Sink, implicitly creating a RunnableGraph\nfinal RunnableGraph<CompletionStage<Integer>> runnable = source.toMat(sink, Keep.right());\n\n// source via flow to sink\nfinal Flow<Integer, Integer, NotUsed> flow1 = Flow.of(Integer.class).map(elem -> elem * 2);\nfinal Flow<Integer, Integer, NotUsed> flow2 = Flow.of(Integer.class).map(elem -> elem + 1);\n\nfinal RunnableGraph<CompletionStage<Integer>> runnableWithFlow =\n    source.via(flow1).via(flow2).toMat(sink, Keep.right());\n//##non-nested-flow\n```\n\n----------------------------------------\n\nTITLE: Restarting Actor on Specific Exception with Akka Typed Supervision (Scala)\nDESCRIPTION: Demonstrates how to configure an Akka Typed actor in Scala to restart automatically when an `IllegalStateException` is thrown. It uses `Behaviors.supervise` to wrap the actor's behavior and applies `SupervisorStrategy.restart` as the recovery mechanism.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.typed.ActorRef\nimport akka.actor.typed.Behavior\nimport akka.actor.typed.SupervisorStrategy\nimport akka.actor.typed.scaladsl.AbstractBehavior\nimport akka.actor.typed.scaladsl.ActorContext\nimport akka.actor.typed.scaladsl.Behaviors\n\nval behavior: Behavior[String] = Behaviors.empty // dummy\n// #restart\nval supervisedBehavior: Behavior[String] =\n  Behaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.restart)\n// #restart\n\n```\n\n----------------------------------------\n\nTITLE: Parsing Lines from ByteString Streams (Akka Streams Java)\nDESCRIPTION: Demonstrates parsing lines from binary streams using the Framing helper in Akka Streams Java DSL. Suitable for processing structured textual or framed binary data received chunk-wise. Handles partial line buffering and configurable delimiters.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeParseLines.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeParseLines.java) { #parse-lines }\n```\n\n----------------------------------------\n\nTITLE: Defining a Persistent Sharded Entity in Java\nDESCRIPTION: Presents a Java implementation of a persistent entity (`HelloWorld`) using Akka Persistence Typed, designed for cluster sharding. It includes command, event, and state definitions, along with the event-sourced behavior logic, incorporating the entity ID into the persistence ID.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n// #persistent-entity-import\nimport akka.actor.typed.ActorRef;\nimport akka.actor.typed.Behavior;\nimport akka.cluster.sharding.typed.javadsl.EntityContext;\nimport akka.cluster.sharding.typed.javadsl.EntityTypeKey;\nimport akka.persistence.typed.PersistenceId;\nimport akka.persistence.typed.javadsl.*;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Set;\n// #persistent-entity-import\n\n// #persistent-entity\npublic class HelloWorldPersistentEntityExample {\n\n  public static class HelloWorld\n      extends EventSourcedBehavior<HelloWorld.Command, HelloWorld.Greeted, HelloWorld.KnownNames> {\n\n    // Command\n    interface Command extends CborSerializable {}\n\n    public static final class Greet implements Command {\n      public final String name;\n      public final ActorRef<Greeting> replyTo;\n\n      public Greet(String name, ActorRef<Greeting> replyTo) {\n        this.name = name;\n        this.replyTo = replyTo;\n      }\n    }\n\n    // Response\n    public static final class Greeting implements CborSerializable {\n      public final String message;\n\n      public Greeting(String message) {\n        this.message = message;\n      }\n    }\n\n    // Event\n    public static final class Greeted implements CborSerializable {\n      public final String name;\n\n      public Greeted(String name) {\n        this.name = name;\n      }\n    }\n\n    // State\n    public static final class KnownNames implements CborSerializable {\n      private Set<String> names = Collections.emptySet();\n\n      private KnownNames() {}\n\n      public KnownNames add(String name) {\n        Set<String> newNames = new HashSet<>(names);\n        newNames.add(name);\n        KnownNames newMe = new KnownNames();\n        newMe.names = newNames;\n        return newMe;\n      }\n\n      public boolean contains(String name) {\n        return names.contains(name);\n      }\n    }\n\n    public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =\n        EntityTypeKey.create(Command.class, \"HelloWorld\");\n\n    private final String entityId;\n\n    public static Behavior<Command> create(EntityContext<Command> entityContext) {\n      return new HelloWorld(entityContext);\n    }\n\n    private HelloWorld(EntityContext<Command> entityContext) {\n      super(PersistenceId.of(entityContext.getEntityTypeKey().name(), entityContext.getEntityId()));\n      this.entityId = entityContext.getEntityId();\n    }\n\n    @Override\n    public KnownNames emptyState() {\n      return new KnownNames();\n    }\n\n    @Override\n    public CommandHandler<Command, Greeted, KnownNames> commandHandler() {\n      return newCommandHandlerBuilder()\n          .forAnyState()\n          .onCommand(\n              Greet.class,\n              (state, command) -> {\n                if (state.contains(command.name)) {\n                  // Been greeted before, just reply\n                  command.replyTo.tell(\n                      new Greeting(\"Hi \" + command.name + \", already greeted you in \" + entityId));\n                  return Effect().none();\n                } else {\n                  // Haven't been greeted before, persist event and reply\n                  return Effect()\n                      .persist(new Greeted(command.name))\n                      .thenReply(\n                          command.replyTo,\n                          (KnownNames updatedState) ->\n                              new Greeting(\"Hello, \" + command.name + \", from \" + entityId + \"!\"));\n                }\n              })\n          .build();\n    }\n\n    @Override\n    public EventHandler<KnownNames, Greeted> eventHandler() {\n      return newEventHandlerBuilder()\n          .forAnyState()\n          .onEvent(Greeted.class, (state, event) -> state.add(event.name))\n          .build();\n    }\n  }\n}\n// #persistent-entity\n```\n\n----------------------------------------\n\nTITLE: Defining Commands for a Counter Actor (Scala)\nDESCRIPTION: Defines the command protocol for a simple counter actor using a Scala sealed trait `Command`. It includes commands for incrementing (`Increment`, `IncrementBy`) and retrieving the current value (`GetValue`, which includes an `ActorRef` for the reply).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.typed.ActorRef\n\nsealed trait Command\nfinal case object Increment extends Command\nfinal case class IncrementBy(value: Int) extends Command\nfinal case class GetValue(replyTo: ActorRef[State]) extends Command\n```\n\n----------------------------------------\n\nTITLE: Incrementing Distributed Counters in PNCounterMap CRDT (Akka Java)\nDESCRIPTION: Updates a distributed counter associated with a participant by incrementing the count in a PNCounterMap, with updates performed on the local replica for low latency. Changes are then asynchronously disseminated to other nodes. Requires Akka Distributed Data; input parameters involve node identity and participant info, output is a distributed, eventually consistent counter map.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-java/README.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nUpdate<PNCounterMap> update = new Update<>(countersKey, PNCounterMap.create(), Replicator.writeLocal(),\n        curr -> curr.increment(node, vote.participant, 1));\n rep licator.tell(update, self());\n```\n\n----------------------------------------\n\nTITLE: Materializing Akka Streams Step-by-Step in Scala\nDESCRIPTION: Demonstrates how to construct a RunnableGraph by connecting a Source, Flow, and Sink, and then materialize and run it. It uses `toMat` and `Keep.right` to retain the materialized value (a Future representing the fold result) from the Sink.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #materialization-in-steps }\n```\n\n----------------------------------------\n\nTITLE: Reply Command Handling with Akka DurableStateBehavior (Scala)\nDESCRIPTION: Illustrates implementing a command handler within a DurableStateBehavior that sends a reply using ActorRef. Requires the Akka Persistence Typed module and an understanding of persistent actor patterns in Scala. The snippet demonstrates how to accept a command, validate and persist events, and send appropriate replies (including rejections) back to the sender. Inputs include the command and sender reference; outputs are status replies. Limitations are related to serialization and correct reply handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [BlogPostEntityDurableState.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/BlogPostEntityDurableState.scala) { #reply-command }\n```\n\n----------------------------------------\n\nTITLE: Running Actor Lifecycle Example in Java\nDESCRIPTION: Shows how to create and stop actors to observe lifecycle events in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\npublic class Main {\n  public static void main(String[] args) {\n    ActorSystem<String> testSystem =\n        ActorSystem.create(\n            Behaviors.setup(\n                context -> {\n                  ActorRef<String> first = context.spawn(StartStopActor1.create(), \"first\");\n                  first.tell(\"stop\");\n                  return Behaviors.empty();\n                }),\n            \"testSystem\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Room Client Actor in Akka Typed - Scala\nDESCRIPTION: Demonstrates a sample client actor for a chat room using Akka Typed in Scala. The client interacts with the chat room to establish a session, post messages, and handle session events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/IntroSpec.scala) { #chatroom-gabbler }\n```\n\n----------------------------------------\n\nTITLE: Creating Main Application Entry Point in Scala\nDESCRIPTION: Defines the main entry point for the Scala IoT application. It creates the ActorSystem and starts the IotSupervisor actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_2.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nobject IotApp {\n  def main(args: Array[String]): Unit = {\n    // Create ActorSystem and top level supervisor\n    ActorSystem[Nothing](IotSupervisor(), \"iot-system\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Generic Response Wrapper in Akka Typed ask from Outside (Scala)\nDESCRIPTION: This example shows how to use the StatusReply generic response wrapper when asking an Akka Typed actor from outside the actor system in Scala. It demonstrates handling both successful responses and validation errors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [InteractionPatternsSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/InteractionPatternsSpec.scala) { #standalone-ask-with-status }\n```\n\n----------------------------------------\n\nTITLE: MapAsync Concurrent Processing in Scala\nDESCRIPTION: Example demonstrating concurrent processing of events using mapAsync with higher parallelism while still maintaining emission order.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapAsync.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(1 to 100)\n  .map(n => Event(n))\n  .mapAsync(3) { event =>\n    Future {\n      println(s\"Processing event number $event...\")\n      Thread.sleep(500) // processing takes time\n      println(s\"Completed processing ${event.number}\")\n      event\n    }\n  }\n  .map(e => {\n    println(\"`mapAsync` emitted event number: \" + e.number)\n    e\n  })\n```\n\n----------------------------------------\n\nTITLE: Initializing Akka Actor using message passing in Scala\nDESCRIPTION: This snippet demonstrates how to initialize an Akka actor using message passing in Scala. It uses the become method to transition between uninitialized and initialized states.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_46\n\nLANGUAGE: Scala\nCODE:\n```\ncase object Initialize\ncase class InitializeMsg(data: Any)\nclass InitializeViaMsgActor extends Actor {\n  override def receive: Receive = {\n    case Initialize =>\n      context.become(initialized(None))\n    case InitializeMsg(data) =>\n      context.become(initialized(Some(data)))\n  }\n  def initialized(state: Option[Any]): Receive = {\n    case \"get\" => sender() ! state\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Device Registration in Device Groups\nDESCRIPTION: Test case that verifies device registration functionality, ensuring different actors are created for different device IDs, and that the registered devices respond to temperature recording requests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_4.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [DeviceGroupSpec.scala](/akka-docs/src/test/scala/typed/tutorial_4/DeviceGroupSpec.scala) { #device-group-test-registration }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [DeviceGroupTest.java](/akka-docs/src/test/java/jdocs/typed/tutorial_4/DeviceGroupTest.java) { #device-group-test-registration }\n```\n\n----------------------------------------\n\nTITLE: Logging in Akka Stream GraphStages - Scala\nDESCRIPTION: This snippet demonstrates how to enable logging within an Akka Stream GraphStage using the StageLogging trait. It requires a stream.Materializer that can provide a LoggingAdapter. The purpose is to facilitate debugging and monitoring of the stream operations by safely accessing the 'log' field.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_19\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [GraphStageLoggingDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageLoggingDocSpec.scala) { #operator-with-logging }\n```\n\n----------------------------------------\n\nTITLE: Create Simple Source in Java\nDESCRIPTION: The code snippet demonstrates creating a simple Akka Stream source in Java, emitting integers from 1 to 100.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [QuickStartDocTest.java](/akka-docs/src/test/java/jdocs/stream/QuickStartDocTest.java) { #create-source }\n```\n\n----------------------------------------\n\nTITLE: Using unfoldResourceAsync with Database in Java\nDESCRIPTION: Implementation showing how to use unfoldResourceAsync to safely stream results from the async database API in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldResourceAsync.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSource.unfoldResourceAsync(\n  () -> database.runQuery(\"SELECT * FROM TWEETS\"),\n  queryResult -> database.hasMore(queryResult),\n  queryResult -> database.close(queryResult)\n)\n```\n\n----------------------------------------\n\nTITLE: Recommended Factory Methods for Props in Scala\nDESCRIPTION: Demonstrates the recommended approach for creating Props using factory methods in a companion object, which helps maintain actor encapsulation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nobject DemoActor {\n  /**\n   * Create Props for an actor of this type.\n   * @param magicNumber The magic number to be passed to this actor's constructor.\n   * @return a Props for creating this actor, which can then be further configured\n   *         (e.g. calling `.withDispatcher()` on it)\n   */\n  def props(magicNumber: Int): Props = Props(new DemoActor(magicNumber))\n}\n\nclass DemoActor(magicNumber: Int) extends Actor {\n  def receive = {\n    case x: Int => sender() ! (x + magicNumber)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Distributed PubSub Topic - Akka Typed - Scala\nDESCRIPTION: Shows how a local actor can subscribe to a distributed pub/sub topic by sending a Subscribe message in Scala. The example uses akka.actor.typed.pubsub.Topic.Subscribe and requires the topic ActorRef and the subscriber's ActorRef. Upon subscription, the actor will receive all published messages for that topic matching the type. No special outputs; subscription is managed internally by Akka.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\ntopic ! Topic.Subscribe(\n  subscriber, \n  subscriberMessageAdapter)\n```\n\n----------------------------------------\n\nTITLE: Complete Akka Streams Twitter Example in Scala\nDESCRIPTION: Full example of processing a stream of tweets to extract authors tweeting about Akka in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_30\n\nLANGUAGE: Scala\nCODE:\n```\nimplicit val system: ActorSystem = ActorSystem(\"reactive-tweets\")\n\nval tweets: Source[Tweet, NotUsed] = Source(???)\n\nval authors: Source[Author, NotUsed] =\n  tweets\n    .filter(_.hashtags.contains(Hashtag(\"#akka\")))\n    .map(_.author)\n\nauthors.runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using a Persistent Sharded Entity in Java\nDESCRIPTION: Demonstrates initializing cluster sharding for the persistent `HelloWorld` entity in Java and subsequently interacting with it using the `ask` pattern, sending a `Greet` command and awaiting the `Greeting` response.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\n// #persistent-entity-usage-import\nimport akka.actor.typed.ActorSystem;\nimport akka.cluster.sharding.typed.javadsl.ClusterSharding;\nimport akka.cluster.sharding.typed.javadsl.Entity;\nimport akka.cluster.sharding.typed.javadsl.EntityRef;\nimport akka.util.Timeout;\nimport java.time.Duration;\nimport java.util.concurrent.CompletionStage;\n// #persistent-entity-usage-import\n\n// #persistent-entity-usage\nActorSystem<?> system = null;\nClusterSharding sharding = ClusterSharding.get(system);\n\nsharding.init(\n    Entity.of(\n        HelloWorldPersistentEntityExample.HelloWorld.ENTITY_TYPE_KEY,\n        HelloWorldPersistentEntityExample.HelloWorld::create));\n\n// Ask pattern requires an implicit timeout\nTimeout timeout = Timeout.create(Duration.ofSeconds(5));\nString entityId = \"hello-1\";\nEntityRef<HelloWorldPersistentEntityExample.HelloWorld.Command> entityRef =\n    sharding.entityRefFor(HelloWorldPersistentEntityExample.HelloWorld.ENTITY_TYPE_KEY, entityId);\nCompletionStage<HelloWorldPersistentEntityExample.HelloWorld.Greeting> reply =\n    entityRef.ask(replyTo -> new HelloWorldPersistentEntityExample.HelloWorld.Greet(\"Alice\", replyTo), timeout);\n// #persistent-entity-usage\n```\n\n----------------------------------------\n\nTITLE: Request-Response Protocol in Scala and Java\nDESCRIPTION: Defines a protocol for request-response pattern with an actor, including request messages with a replyTo field that allows the receiving actor to send back responses.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n  sealed trait Request\n  final case class CookieOrder(cookie: String, replyTo: ActorRef[Response]) extends Request\n\n  sealed trait Response\n  final case class Confirmation(cookie: String) extends Response\n  final case class Invalid(reason: String) extends Response\n```\n\nLANGUAGE: java\nCODE:\n```\n  interface Request {}\n\n  public static class CookieOrder implements Request {\n    public final String cookie;\n    public final ActorRef<Response> replyTo;\n\n    public CookieOrder(String cookie, ActorRef<Response> replyTo) {\n      this.cookie = cookie;\n      this.replyTo = replyTo;\n    }\n  }\n\n  interface Response {}\n\n  public static class Confirmation implements Response {\n    public final String cookie;\n\n    public Confirmation(String cookie) {\n      this.cookie = cookie;\n    }\n  }\n\n  public static class Invalid implements Response {\n    public final String reason;\n\n    public Invalid(String reason) {\n      this.reason = reason;\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring Least Frequently Used with Dynamic Aging (LFUDA) Policy in Akka Cluster Sharding\nDESCRIPTION: Configures a passivation strategy using the least frequently used policy with dynamic aging. This variation adapts to shifts in entity popularity by increasing the frequency counts for recently accessed entities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_37\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = least-frequently-used\n  least-frequently-used {\n    dynamic-aging = true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Complex Runnable Graph with GraphDSL\nDESCRIPTION: Demonstrates creating a complex stream processing network with fan-in, fan-out operators and cycles using GraphDSL. The graph is closed (has no unwired ports) and therefore can be materialized.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\nval g = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder: GraphDSL.Builder[NotUsed] =>\n  import GraphDSL.Implicits._\n  val in = Source(1 to 10)\n  val out = Sink.foreach(println)\n\n  val bcast = builder.add(Broadcast[Int](2))\n  val merge = builder.add(Merge[Int](2))\n\n  val f1, f2, f3, f4 = Flow[Int].map(_ + 10)\n\n  in ~> f1 ~> bcast ~> f2 ~> merge ~> f3 ~> out\n              bcast ~> f4 ~> merge\n  ClosedShape\n})\n```\n\nLANGUAGE: Java\nCODE:\n```\nfinal RunnableGraph<NotUsed> g =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            builder -> {\n              final Source<Integer, NotUsed> in = Source.from(Arrays.asList(1, 2, 3, 4, 5));\n              final Sink<Integer, NotUsed> out = Sink.foreach(System.out::println);\n\n              final UniformFanOutShape<Integer, Integer> bcast = builder.add(Broadcast.create(2));\n              final UniformFanInShape<Integer, Integer> merge = builder.add(Merge.create(2));\n\n              final Flow<Integer, Integer, NotUsed> f1 = Flow.of(Integer.class).map(elem -> elem + 10);\n              final Flow<Integer, Integer, NotUsed> f2 = Flow.of(Integer.class).map(elem -> elem + 10);\n              final Flow<Integer, Integer, NotUsed> f3 = Flow.of(Integer.class).map(elem -> elem + 10);\n              final Flow<Integer, Integer, NotUsed> f4 = Flow.of(Integer.class).map(elem -> elem + 10);\n\n              builder\n                  .from(in)\n                  .via(builder.add(f1))\n                  .viaFanOut(bcast)\n                  .via(builder.add(f2))\n                  .viaFanIn(merge)\n                  .via(builder.add(f3))\n                  .to(out);\n              builder.from(bcast).via(builder.add(f4)).toFanIn(merge);\n              return ClosedShape.getInstance();\n            }));\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Cluster Dependencies in Build Tools\nDESCRIPTION: To integrate Akka Cluster into your Scala or Java project, you need to add specific dependencies from the Akka repository, which are configured for sbt, Maven, or Gradle build tools. This details the dependency configuration required for Akka Cluster setup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nid=\\\"akka-repository\\\"\nname=\\\"Akka library repository\\\"\nurl=\\\"https://repo.akka.io/maven\\\"\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nbomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\\\"$akka.version$\\\"\n  group=com.typesafe.akka\n  artifact=akka-cluster-typed_$scala.binary.version$\n  version=AkkaVersion\n```\n\n----------------------------------------\n\nTITLE: Scheduling Messages with Akka Timers\nDESCRIPTION: Shows how to use Akka's timer functionality for scheduling periodic or one-time messages. Includes timer management and lifecycle handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_36\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.Timers\n\nobject TimerBasedActor {\n  private case object TickKey\n  private case object FirstTick\n  private case object Tick\n}\n\nclass TimerBasedActor extends Actor with Timers {\n  import TimerBasedActor._\n  timers.startSingleTimer(TickKey, FirstTick, 500.millis)\n\n  def receive = {\n    case FirstTick =>\n      timers.startTimerWithFixedDelay(TickKey, Tick, 1.second)\n    case Tick =>\n      // do something\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Recovering from Stream Failure (Scala)\nDESCRIPTION: Illustrates using the `recover` operator in Scala to handle specific exceptions. If an `ArithmeticException` occurs, the stream emits a default value (0) and then completes gracefully. Other exceptions would still fail the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nSource(0 to 6)\n  .map(n =>\n    // assuming this could fail\n    if (n < 5) n.toString\n    else throw new RuntimeException(\"Boom!\"))\n  .recover {\n    case _: RuntimeException => \"stream truncated\"\n  }\n  .runForeach(println)\n```\n\n----------------------------------------\n\nTITLE: Combining Sinks in Scala/Java\nDESCRIPTION: Shows how to combine sinks for fan-out operations using simplified API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n#sink-combine\n```\n\nLANGUAGE: Java\nCODE:\n```\n#sink-combine\n```\n\n----------------------------------------\n\nTITLE: Creating EventSourcedBehavior for Blog Post Entity (Java)\nDESCRIPTION: Creates the EventSourcedBehavior for a blog post entity in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nEventSourcedBehavior.create(\n    PersistenceId.of(\"BlogPost\", entityId),\n    EmptyState.INSTANCE,\n    commandHandler(),\n    eventHandler);\n```\n\n----------------------------------------\n\nTITLE: Counting Tweets Using Fold in Akka Streams (Java)\nDESCRIPTION: Shows how to count the number of tweets in a stream using Sink.fold. The result is made available as a CompletionStage<Integer>.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_39\n\nLANGUAGE: Java\nCODE:\n```\nfinal Sink<Integer, CompletionStage<Integer>> sumSink =\n    Sink.fold(0, (acc, elem) -> acc + elem);\n\nfinal RunnableGraph<CompletionStage<Integer>> counterGraph =\n    tweets.map(t -> 1).toMat(sumSink, Keep.right());\n\nfinal CompletionStage<Integer> sum = counterGraph.run(mat);\n\nsum.thenAccept(c -> System.out.println(\"Total tweets processed: \" + c));\n```\n\n----------------------------------------\n\nTITLE: Implementing Counter Actor for Cluster Sharding\nDESCRIPTION: Example of an entity actor implementation using Event Sourcing and PersistentActor to store state. Shows how to define persistenceId using the entity identifier.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n#counter-actor\n```\n\nLANGUAGE: Java\nCODE:\n```\n#counter-actor\n```\n\n----------------------------------------\n\nTITLE: Filtering Elements in Akka Streams (Java)\nDESCRIPTION: Shows the usage of the filter operator in Akka Streams to select longer words from a source of words using Java. The predicate function checks if the word length is greater than 5.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/filter.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(words)\n  .filter(word -> word.length() > 5)\n  .runWith(Sink.foreach(System.out::println), system);\n```\n\n----------------------------------------\n\nTITLE: Implementing Duplicate Transformation with GraphStage in Scala\nDESCRIPTION: This snippet demonstrates a one-to-many operator implementation using GraphStage in Scala. It creates a duplicate operator that emits each upstream element twice downstream, showing state management within the operator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\nclass Duplicator[A] extends GraphStage[FlowShape[A, A]] {\n  val in = Inlet[A](\"Duplicator.in\")\n  val out = Outlet[A](\"Duplicator.out\")\n  override val shape = FlowShape(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      // Remember if we've seen the first element or not\n      private var seen = false\n\n      // If we've seen the first element, we don't need to pull again\n      private def maybePull(): Unit = if (!seen) pull(in)\n\n      setHandler(in, new InHandler {\n        override def onPush(): Unit = {\n          val elem = grab(in)\n          seen = true\n          push(out, elem)\n        }\n\n        override def onUpstreamFinish(): Unit = {\n          if (!seen) completeStage()\n          else {\n            // There is an element buffered, let it be sent downstream\n            // before completion\n          }\n        }\n      })\n      setHandler(out, new OutHandler {\n        override def onPull(): Unit = {\n          if (isClosed(in)) {\n            if (seen) {\n              // We're done\n              completeStage()\n            } // else onUpstreamFinish completed us\n          } else if (seen) {\n            seen = false\n            push(out, grab(in))\n          } else {\n            maybePull()\n          }\n        }\n      })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Functional Style Counter Actor in Akka Typed (Scala)\nDESCRIPTION: Demonstrates a counter actor implemented using functional programming paradigms in Akka Typed (Scala). This approach passes immutable state as parameters and returns new behaviors as state evolves. Dependencies include Akka Typed libraries, and the factory pattern is used to create behaviors. The behavior expects a message and state parameter, and typically uses pattern matching for message handling. Input is actor messages, and the output is the next behavior/state; requires no mutable state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n// @@snip [StyleGuideDocExamples.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/StyleGuideDocExamples.scala) { #fun-style }\n\n```\n\n----------------------------------------\n\nTITLE: Signature of flatMapPrefix for Source and Flow in Scala and Java\nDESCRIPTION: The flatMapPrefix operator is available for both Source and Flow in Akka Streams. It takes an integer n and a function f that processes the first n elements to create a new Flow for the rest of the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/flatMapPrefix.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.flatMapPrefix[Out2,Mat2](n:Int)(f:scala.collection.immutable.Seq[Out]=&gt;akka.stream.scaladsl.Flow[Out,Out2,Mat2]):FlowOps.this.Repr[Out2]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.flatMapPrefix(int,akka.japi.function.Function)\n```\n\nLANGUAGE: scala\nCODE:\n```\nFlow.flatMapPrefix[Out2,Mat2](n:Int)(f:scala.collection.immutable.Seq[Out]=&gt;akka.stream.scaladsl.Flow[Out,Out2,Mat2]):FlowOps.this.Repr[Out2]\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow.flatMapPrefix(int,akka.japi.function.Function)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Main Actor for Akka System in Scala\nDESCRIPTION: Sets up the `Main` actor for the Akka system in Scala, demonstrating how to initialize the chat room and client actors, and manage system termination based on actor lifecycle events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/IntroSpec.scala) { #chatroom-main }\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Akka Streams with GraphDSL in Scala\nDESCRIPTION: Illustrates building a non-linear stream graph using `GraphDSL` in Scala to broadcast elements from a single source (`tweets`) to multiple sinks (`writeAuthors` and `writeHashtags`). It uses an implicit builder (`b`) and the `~>` operator to define connections between the `Broadcast` junction and the processing flows.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_34\n\nLANGUAGE: scala\nCODE:\n```\nimport GraphDSL.Implicits._\n\nval g = RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  val bcast = b.add(Broadcast[Tweet](2))\n  tweets ~> bcast.in\n  bcast.out(0) ~> Flow[Tweet].map(_.author) ~> writeAuthors\n  bcast.out(1) ~> Flow[Tweet].mapConcat(_.hashtags.toList) ~> writeHashtags\n  ClosedShape\n})\n\ng.run()\n//-//#graph-dsl-broadcast\n```\n\n----------------------------------------\n\nTITLE: Fault Handling Sample Implementation in Scala\nDESCRIPTION: Implementation of fault tolerance patterns in Akka using Scala. Shows interaction between Worker, CounterService, Counter, and Storage actors with supervision and failure recovery mechanisms.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance-sample.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [FaultHandlingDocSample.scala](/akka-docs/src/test/scala/docs/actor/FaultHandlingDocSample.scala) { #all }\n```\n\n----------------------------------------\n\nTITLE: Configuring SSLEngine for TLS in Akka Streams (Scala)\nDESCRIPTION: Illustrates how to set up an `SSLEngine` in Scala for use with Akka Streams TLS features like `outgoingConnectionWithTls` and `bindWithTls`. This involves loading keystores and truststores, creating an `SSLContext`, and configuring the `SSLEngine` with desired parameters (e.g., client/server mode, protocols, cipher suites) for secure communication.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n// Code for [TcpSpec.scala](/akka-stream-tests/src/test/scala/akka/stream/io/TcpSpec.scala) { #setting-up-ssl-engine } not available in input\n```\n\n----------------------------------------\n\nTITLE: Terminate Source Run in Scala\nDESCRIPTION: The Scala snippet highlights how to run an Akka Stream and handle termination using a Future for completion signaling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [QuickStartDocSpec.scala](/akka-docs/src/test/scala/docs/stream/QuickStartDocSpec.scala) { #run-source-and-terminate }\n```\n\n----------------------------------------\n\nTITLE: Defining Commands for Blog Post Entity (Java)\nDESCRIPTION: Defines the commands for a blog post entity using interfaces and classes in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\ninterface Command {}\n\nclass AddPost implements Command {\n  final PostContent content;\n  public AddPost(PostContent content) {\n    this.content = content;\n  }\n}\n\nclass GetPost implements Command {\n  final ActorRef<PostContent> replyTo;\n  public GetPost(ActorRef<PostContent> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\nclass ChangeBody implements Command {\n  final String newBody;\n  public ChangeBody(String newBody) {\n    this.newBody = newBody;\n  }\n}\n\nenum Publish implements Command {\n  INSTANCE\n}\n\nclass AddComment implements Command {\n  final String user;\n  final String comment;\n  public AddComment(String user, String comment) {\n    this.user = user;\n    this.comment = comment;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Boundaries in Akka Streams - Scala\nDESCRIPTION: Demonstrates how to create asynchronous boundaries in Akka Streams using the async attribute to enable parallel processing across multiple actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\nSource.fromIterator(() => Iterator.from(1))\n    .map(x => x + 1)\n    .async\n    .map(x => x * 2)\n    .async\n    .runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Request-Response Message Sending in Scala and Java\nDESCRIPTION: Shows how to send a request message to an actor using the request-response pattern, where the sender includes its own ActorRef as the replyTo parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\n  val cookieShop: ActorRef[Request] = ???\n  cookieShop ! CookieOrder(\"Chocolate chip\", context.self)\n```\n\nLANGUAGE: java\nCODE:\n```\n  ActorRef<Request> cookieShop = ???;\n  cookieShop.tell(new CookieOrder(\"Chocolate chip\", context.getSelf()));\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Actor Imports in Scala\nDESCRIPTION: Required imports for creating Akka actors in Scala, including core actor system and behavior imports.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.typed.ActorRef\nimport akka.actor.typed.ActorSystem\nimport akka.actor.typed.Behavior\nimport akka.actor.typed.scaladsl.Behaviors\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Source as an Akka Streams Source (Scala)\nDESCRIPTION: This Scala snippet shows how to wrap a custom GraphStage as a Source using Source.fromGraph. This allows the custom operator to be used within normal Akka Streams DSL flows, enabling it to participate in further stream transformations and materializations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #simple-source-usage }\n```\n\n----------------------------------------\n\nTITLE: Implementing Supervisor Strategy in Scala\nDESCRIPTION: Creates a one-for-one supervision strategy that handles different exception types with specific directives. It allows up to 10 restarts per minute before stopping the child actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval strategy = OneForOneStrategy(maxNrOfRetries = 10, withinTimeRange = 1.minute) {\n  case _: ArithmeticException      => Resume\n  case _: NullPointerException     => Restart\n  case _: IllegalArgumentException => Stop\n  case _: Exception                => Escalate\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Request-Response with ask between Akka Typed Actors\nDESCRIPTION: This example shows how to use the 'ask' pattern for request-response interactions between two Akka Typed actors in Java. It demonstrates constructing the outgoing message, transforming the response, and handling timeouts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n@@snip [InteractionPatternsTest.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/InteractionPatternsTest.java) { #actor-ask }\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic EventSourcedBehavior Structure in Java\nDESCRIPTION: Minimum required structure for creating an EventSourcedBehavior in Java, showing the core components needed for event sourcing: persistenceId, emptyState, commandHandler, and eventHandler.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class BasicPersistentBehaviorTest {\n  public interface Command {}\n  public interface Event {}\n  public interface State {}\n\n  public EventSourcedBehavior<Command, Event, State> behavior() {\n    return EventSourcedBehavior.create(\n        PersistenceId.ofUniqueId(\"abc\"),\n        null,\n        (state, command) -> Effect().none(),\n        (state, event) -> null);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Failure Detector Implementation\nDESCRIPTION: Configuration snippet showing how to specify a custom failure detector implementation class for Akka Cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_9\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.cluster.implementation-class = \"com.example.CustomFailureDetector\"\n```\n\n----------------------------------------\n\nTITLE: Spawning Actors with Akka Typed in Scala\nDESCRIPTION: Demonstrates how to spawn actors using Akka Typed API in Scala. Requires Akka dependencies to be set up in the build system and uses ActorContext to manage actor lifecycles.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [HelloWorld.scala](/samples/akka-quickstart-scala/src/main/scala/com/example/HelloWorld.scala) { #hello-world-main }\n```\n\n----------------------------------------\n\nTITLE: Configuring Idle Entity Passivation Timeout (HOCON)\nDESCRIPTION: This HOCON configuration snippet sets the automatic passivation strategy based on entity idleness. It configures the `idle-timeout` to 1 minute, meaning sharded entities that haven't received messages via Cluster Sharding for 1 minute will be automatically passivated.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_27\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.sharding {\n  passivation {\n    strategy = idle\n    idle-entity {\n      timeout = 1 minute\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Email Address Lookup using Akka Streams in Scala\nDESCRIPTION: This Scala snippet demonstrates how to use an email lookup service within an Akka Stream to transform a stream of authors into a stream of email addresses. It handles potential failures during the lookup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_21\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntegrationDocSpec.scala](/akka-docs/src/test/scala/docs/stream/IntegrationDocSpec.scala) { #email-address-lookup2 }\n```\n\n----------------------------------------\n\nTITLE: Implementing Element Counter with statefulMapConcat in Scala\nDESCRIPTION: Example showing how to combine stream elements with unique IDs using a stateful counter, similar to zipWithIndex functionality.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMapConcat.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval result = Source(List(\"A\", \"B\", \"C\"))\n  .statefulMapConcat { () =>\n    var counter = 0L\n    element => {\n      val elementWithIndex = (element, counter)\n      counter += 1\n      elementWithIndex :: Nil\n    }\n  }\n  .runWith(Sink.seq)\n```\n\n----------------------------------------\n\nTITLE: Resuming Actor on Specific Exception with Akka Typed Supervision (Java)\nDESCRIPTION: Illustrates using `Behaviors.supervise` with `SupervisorStrategy.resume()` in Java to ignore an `IllegalStateException` and allow the actor to continue processing subsequent messages without restarting.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// #resume\nfinal Behavior<String> supervisedBehavior2 =\n    Behaviors.supervise(behavior).onFailure(IllegalStateException.class, SupervisorStrategy.resume());\n// #resume\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple TCP Chat Server with Flow.fromSinkAndSource in Java\nDESCRIPTION: This Java example shows how to create a simple TCP chat server using Flow.fromSinkAndSource. It combines MergeHub and BroadcastHub to dynamically merge incoming messages and broadcast them to all connected clients.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/fromSinkAndSource.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nPair<Sink<String>, Source<ByteString, NotUsed>> sinkAndSource =\n    MergeHub.of(String.class)\n        .map(msg -> ByteString.fromString(msg + \"\\n\"))\n        .toMat(BroadcastHub.of(ByteString.class), Keep.both())\n        .run(system);\n\nSink<String> sink = sinkAndSource.first();\nSource<ByteString, NotUsed> source = sinkAndSource.second();\n\nFlow<ByteString, ByteString, NotUsed> flow =\n    Flow.fromSinkAndSource(\n        Flow.of(ByteString.class)\n            .map(ByteString::utf8String)\n            .map(String::trim)\n            .to(sink),\n        source);\n```\n\n----------------------------------------\n\nTITLE: Chaining Custom Operators in Akka Streams (Java)\nDESCRIPTION: This snippet shows how to use custom GraphStage operators in a stream processing chain. It demonstrates the composition of map, filter, and duplicate operators using the 'via' method in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nCompletionStage<List<Integer>> result =\n    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n        .via(new Map<>((i) -> i + 1)) // add 1 to each number\n        .via(new Filter<>((i) -> i > 5)) // keep only numbers > 5\n        .via(new Duplicator<>()) // duplicate each number\n        .runWith(Sink.seq(), system);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using a Persistent Sharded Entity in Scala\nDESCRIPTION: Shows how to initialize cluster sharding for the persistent `HelloWorld` entity in Scala and then interact with it using the `ask` pattern to send a `Greet` command and receive a `Greeting` reply.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\n// #persistent-entity-usage\nimport akka.actor.typed.scaladsl.AskPattern._\nimport akka.util.Timeout\nimport scala.concurrent.duration._\nimport scala.concurrent.Future\n\nimplicit val system: ActorSystem[_] = ???\nval sharding = ClusterSharding(system)\n\nsharding.init(\n  Entity(HelloWorld.TypeKey)(\n    createBehavior = entityContext =>\n      HelloWorld(entityContext))\n)\n\n// Ask pattern requires an implicit timeout\nimplicit val timeout: Timeout = 5.seconds\nval entityId = \"hello-1\"\nval helloEntityRef = sharding.entityRefFor(HelloWorld.TypeKey, entityId)\nval reply: Future[HelloWorld.Greeting] = helloEntityRef.ask(HelloWorld.Greet(\"Alice\")(_))\n// #persistent-entity-usage\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Tagging in Akka Persistence\nDESCRIPTION: Demonstrates how to add tags to events without using an EventAdapter. Tags can be used for filtering events in read-side projections or for other event classification purposes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_38\n\nLANGUAGE: Scala\nCODE:\n```\nEventSourcedBehavior[Command, Event, State](\\n  persistenceId = persistenceId,\\n  emptyState = State.empty,\\n  commandHandler = commandHandler,\\n  eventHandler = eventHandler).withTagger(_ => Set(\"tag1\", \"tag2\"))\n```\n\nLANGUAGE: Java\nCODE:\n```\nEventSourcedBehavior<Command, Event, State> behavior =\\n    EventSourcedBehavior.create(\\n            persistenceId,\\n            State.empty(),\\n            commandHandler,\\n            eventHandler)\\n        .withTagger(event -> Collections.singleton(\"tag\"));\n```\n\n----------------------------------------\n\nTITLE: Sorting Elements to Multiple Groups with groupBy in Akka Streams\nDESCRIPTION: Java implementation of the multi-group pattern that maps messages to multiple topics and creates separate streams for each topic. It uses mapConcat to flatten the message-topic pairs and groupBy to create topic-specific streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\n// For every topic, a separate group of elements:\nSource.single(new Message())\n    .mapConcat(\n        (msg) -> {\n          // Get the topics a message belongs to\n          List<String> topics = topicMapper(msg);\n          // Create a group for each topic\n          return topics.stream()\n              .map((topic) -> new Pair<>(msg, topic))\n              .collect(Collectors.toList());\n        })\n    // now we group by the topic, which gives us a separate substream for each topic\n    .groupBy(maxTopics, pair -> pair.second())\n    // from each Message-Topic pair we only are interested in the Message\n    .map(pair -> pair.first())\n```\n\n----------------------------------------\n\nTITLE: Preventing Child Actor Stop on Parent Restart (Java)\nDESCRIPTION: Shows how to override the default behavior in Java to prevent child actors from being stopped when the parent restarts. This is achieved by placing `Behaviors.supervise` inside `Behaviors.setup` and using `SupervisorStrategy.restart().withStopChildren(false)`. Consequently, the `setup` block (and child creation) runs only once, not on restarts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\n// #restart-keep-children\nBehaviors.<String>setup(\n    context -> {\n      final ActorRef<String> child1 = context.spawn(behavior, \"child1\");\n      final ActorRef<String> child2 = context.spawn(behavior, \"child2\");\n\n      // supervision strategy inside setup to not recreate children on restart\n      return Behaviors.supervise(\n              Behaviors.<String>receiveMessage(\n                  message -> {\n                    // processing messages\n                    // ...\n                    // restart occurs, children will be kept\n                    return Behaviors.same();\n                  }))\n          .onFailure(\n              IllegalStateException.class,\n              SupervisorStrategy.restart().withStopChildren(false));\n    });\n// #restart-keep-children\n\n```\n\n----------------------------------------\n\nTITLE: Monotonically Increasing Timestamps using LwwTime Utility - Java\nDESCRIPTION: This Java snippet illustrates how to use the LwwTime utility's 'increase' method within a command-handler to ensure that each event's timestamp is monotonically greater than all previous updates for last writer wins semantics in Akka. The handler prepares a new LwwTime instance before persisting events, taking the previous timestamp and replica id into account. Dependencies include the Java Akka Persistence Typed API and the LwwTime utility. The input is the current/latest timestamp, and the output is an incremented LwwTime, used to enforce correct ordering of concurrent updates. This approach minimizes state divergence due to clock skew.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n/* @@snip [blog](/akka-persistence-typed-tests/src/test/java/jdocs/akka/persistence/typed/ReplicatedBlogExample.java) { #command-handler } */\n```\n\n----------------------------------------\n\nTITLE: Task Manager with Stashing in Scala\nDESCRIPTION: Example implementation of a task manager using Akka Persistence with command stashing. Handles StartTask, NextStep and EndTask commands while deferring commands for other tasks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_43\n\nLANGUAGE: scala\nCODE:\n```\nRefer to file: StashingExample.scala\n```\n\n----------------------------------------\n\nTITLE: Implementing a Persistent Actor in Java\nDESCRIPTION: Example of a Java implementation of an AbstractPersistentActor that demonstrates event sourcing pattern. The actor persists events, recovers state from events, and handles commands by generating and persisting events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass ExamplePersistentActor extends AbstractPersistentActor {\n  \n  private ExampleState state = new ExampleState();\n  \n  @Override\n  public String persistenceId() {\n    return \"sample-id-1\";\n  }\n  \n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder()\n      .match(Evt.class, this::updateState)\n      .match(\n          SnapshotOffer.class,\n          ss -> this.state = (ExampleState) ss.snapshot())\n      .build();\n  }\n  \n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n      .match(\n          Cmd.class,\n          cmd -> {\n            final Evt evt = new Evt(cmd.getData());\n            persist(\n                evt,\n                (Evt e) -> {\n                  updateState(e);\n                  getContext().getSystem().getEventStream().publish(e);\n                  if (lastSequenceNr() % 1000 == 0) saveSnapshot(state);\n                });\n          })\n      .matchEquals(\"snap\", s -> saveSnapshot(state))\n      .match(SaveSnapshotSuccess.class, s -> {})\n      .match(SaveSnapshotFailure.class, f -> {})\n      .build();\n  }\n  \n  private void updateState(Evt evt) {\n    state = state.updated(evt);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Command and Event Definition in Akka Persistence - Scala\nDESCRIPTION: This snippet defines how to set up commands and events used in a persistent actor using Akka's typed persistence in Scala. It is part of a base setup for persistent actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BasicPersistentBehaviorCompileOnly.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/BasicPersistentBehaviorCompileOnly.scala) { #command }\n```\n\n----------------------------------------\n\nTITLE: Implementing MergeHub in Akka Streams - Scala\nDESCRIPTION: This snippet in Scala illustrates the MergeHub feature within Akka Streams, which facilitates dynamic fan-in junction points. Producers emit data to a single consumer in a First-Comes-First-Served manner. Dependencies include Akka Streams library and proper Stream setup for dynamic data handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [HubsDocSpec.scala](/akka-docs/src/test/scala/docs/stream/HubsDocSpec.scala) { #merge-hub }\n```\n\n----------------------------------------\n\nTITLE: Simplified Duplicate Transformation with GraphStage in Java\nDESCRIPTION: This snippet shows a simplified Java implementation of the one-to-many duplicate operator using GraphStage. It uses the emitMultiple method to simplify the logic and reduce mutable state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\npublic class Duplicator<A> extends GraphStage<FlowShape<A, A>> {\n  private final Inlet<A> in = Inlet.create(\"Duplicator.in\");\n  private final Outlet<A> out = Outlet.create(\"Duplicator.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                A elem = grab(in);\n                emitMultiple(out, Arrays.asList(elem, elem));\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() {\n                pull(in);\n              }\n            });\n      }\n    };\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Sources with zipN in Scala\nDESCRIPTION: Example demonstrating how to zip three different sources (characters, numbers, and colors) into a single Source where each element is a Vector containing one element from each source. The stream completes when any of the sources reaches its end.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/zipN.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval characters = Source(\"a\"::\"b\"::\"c\"::Nil)\nval numbers = Source(1::2::3::Nil)\nval colors = Source(\"red\"::\"green\"::\"blue\"::Nil)\n\nval zipped: Source[Vector[Any], NotUsed] = \n  Source.zipN(List(characters, numbers, colors))\n\nzipped.runWith(Sink.foreach(println))\n// prints:\n// Vector(a, 1, red)\n// Vector(b, 2, green)\n// Vector(c, 3, blue)\n```\n\n----------------------------------------\n\nTITLE: Building a Simple Akka Streams Graph (Scala)\nDESCRIPTION: Demonstrates constructing a basic `RunnableGraph` in Scala by connecting a `Source`, two `Flow`s, and a `Sink` linearly using `via` and `to`. This example illustrates a flat, non-nested stream structure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n//##non-nested-flow\nimport akka.stream.scaladsl._\n\n// connect the Source to the Sink, implicitly creating a RunnableGraph\nval source = Source(1 to 10)\nval sink = Sink.fold[Int, Int](0)(_ + _)\n\nval runnable: RunnableGraph[Future[Int]] = source.toMat(sink)(Keep.right)\n\n// source.via(flow).to(sink)\nval flow1 = Flow[Int].map(_ * 2)\nval flow2 = Flow[Int].map(_ + 1)\nval runnableWithFlow: RunnableGraph[Future[Int]] = source.via(flow1).via(flow2).toMat(sink)(Keep.right)\n\n//##non-nested-flow\n```\n\n----------------------------------------\n\nTITLE: Implementing Command Handler for Blog Post Entity (Scala)\nDESCRIPTION: Implements the command handler for a blog post entity using pattern matching in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\nprivate def commandHandler(state: State, command: Command): Effect[Event, State] =\n  state match {\n    case EmptyState =>\n      command match {\n        case AddPost(content) =>\n          Effect.persist(PostAdded(content))\n        case _ =>\n          Effect.none\n      }\n\n    case DraftState(content) =>\n      command match {\n        case GetPost(replyTo) =>\n          replyTo ! content\n          Effect.none\n        case ChangeBody(newBody) =>\n          Effect.persist(BodyChanged(newBody))\n        case Publish =>\n          Effect.persist(PostPublished)\n        case AddComment(_, _) =>\n          Effect.unhandled\n        case _: AddPost =>\n          Effect.unhandled\n      }\n\n    case PublishedState(content) =>\n      command match {\n        case GetPost(replyTo) =>\n          replyTo ! content\n          Effect.none\n        case AddComment(user, comment) =>\n          Effect.persist(CommentAdded(user, comment))\n        case _: AddPost =>\n          Effect.unhandled\n        case _: ChangeBody =>\n          Effect.unhandled\n        case Publish =>\n          Effect.none\n      }\n  }\n```\n\n----------------------------------------\n\nTITLE: Handling Application-specific Stop Message in Singleton Actor - Akka Cluster Typed - Scala\nDESCRIPTION: Defines how to implement a stop message for the singleton actor in Scala, allowing pre-stop logic to run and resources to be released before termination. The behavior reacts to a defined Stop command, ensuring orderly shutdown. This is critical for applications needing to safely close resources on migration. Requires correct signal handling within the actor's receive block.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\ncase object Stop extends CounterCommand\n\nval counterBehaviorWithStop: Behavior[CounterCommand] =\n  Behaviors.receiveMessage {\n    case Stop =>\n      // Application-specific cleanup\n      Behaviors.stopped\n    // other cases...\n  }\n```\n\n----------------------------------------\n\nTITLE: Common Chained Effects in Akka Actors - Scala\nDESCRIPTION: Demonstrates how to define common chained effects for reuse in Akka's persistent actor commands using Scala, streamlining related command processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [PersistentActorCompileOnlyTest.scala](/akka-persistence-typed/src/test/scala/akka/persistence/typed/scaladsl/PersistentActorCompileOnlyTest.scala) { #commonChainedEffects }\n```\n\n----------------------------------------\n\nTITLE: Querying Events by Tag in Akka Persistence - Java\nDESCRIPTION: Java example of querying events by tag using Akka Persistence. The example uses stream operators available in Akka Streams on the resulting event stream and supports the offset parameter for resumable streams. This requires Akka Persistence and a compatible journal plugin.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [PersistenceQueryDocTest.java](/akka-docs/src/test/java/jdocs/persistence/PersistenceQueryDocTest.java) { #events-by-tag }\n```\n\n----------------------------------------\n\nTITLE: Using PartitionHub for Element Routing in Java\nDESCRIPTION: This snippet demonstrates how to use PartitionHub to route elements from a producer to multiple consumers based on a partitioning function in Java. It shows how to create the hub and attach consumers to it.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nSink<Integer, Source<Integer, NotUsed>> sink =\n  PartitionHub.of(Integer.class,\n    (size, elem) -> Math.abs(elem.hashCode()) % size,\n    2,\n    256);\n\nSource<Integer, NotUsed> producer =\n  Source.from(IntStream.rangeClosed(1, 1000).boxed().collect(Collectors.toList()))\n    .runWith(sink, mat);\n\n// wait until 2 consumers have been connected\n\nCompletionStage<Integer> count1 =\n  producer.runWith(Sink.fold(0, (acc, elem) -> acc + 1), mat);\nCompletionStage<Integer> count2 =\n  producer.runWith(Sink.fold(0, (acc, elem) -> acc + 1), mat);\n\nSystem.out.println(\"consumer1 got \" + count1.toCompletableFuture().get(3, TimeUnit.SECONDS));\nSystem.out.println(\"consumer2 got \" + count2.toCompletableFuture().get(3, TimeUnit.SECONDS));\n```\n\n----------------------------------------\n\nTITLE: Using Generic Response Wrapper in Akka Typed ask from Outside (Java)\nDESCRIPTION: This example shows how to use the StatusReply generic response wrapper when asking an Akka Typed actor from outside the actor system in Java. It demonstrates handling both successful responses and validation errors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\n@@snip [InteractionPatternsTest.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/InteractionPatternsAskWithStatusTest.java) { #standalone-ask-with-status }\n```\n\n----------------------------------------\n\nTITLE: Initializing Cluster Configuration in Scala and Java\nDESCRIPTION: This snippet establishes the minimum configuration required for Akka Cluster, setting the host, port, and actor provider to 'cluster'. This setup is crucial for enabling cluster capabilities in Akka ActorSystems.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nBasicClusterExampleSpec.scala { #config-seeds }\n```\n\n----------------------------------------\n\nTITLE: Creating a Parallel Unordered Java Collector Sink with Akka Streams - Java\nDESCRIPTION: This Java signature provides a method javaCollectorParallelUnordered that creates a Sink with the specified parallelism, accepting a Creator for the java.util.stream.Collector. It returns a sink that materializes into a CompletionStage completed with the reduced result. Dependencies include Akka Streams, Java 8+, and the Collector API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/javaCollectorParallelUnordered.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\njava=\"#javaCollectorParallelUnordered(int,akka.japi.function.Creator)\"\n```\n\n----------------------------------------\n\nTITLE: Composing Protocol Stack with Multiple BidiFlows in Akka Streams (Scala)\nDESCRIPTION: This Scala snippet illustrates composing multiple BidiFlow subgraphs and Flows into a protocol processing stack using Akka Streams, including reversal of flows to wire data in both directions. It demonstrates protocol layer ordering and how to directly connect flows for testing, without network involvement. Dependencies are akka.stream.scaladsl and the relevant BidiFlow/Flow operators. Inputs and outputs match the chained protocol types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\nval protocolStack = codec.atop(framing).reversed()\nprotocolStack.join(anotherFlow)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing REPL Client with Akka Streams in Scala and Java\nDESCRIPTION: This snippet implements a simple REPL Client over TCP using Akka Streams, initiating outgoing connections and processing server interactions. Dependencies include Akka Streams and ByteString handling. Inputs involve command line inputs, while the process outputs server interaction results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [StreamTcpDocSpec.scala](/akka-docs/src/test/scala/docs/stream/io/StreamTcpDocSpec.scala) { #repl-client }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [StreamTcpDocTest.java](/akka-docs/src/test/java/jdocs/stream/io/StreamTcpDocTest.java) { #repl-client }\n```\n\n----------------------------------------\n\nTITLE: Creating EventSourcedBehavior for Blog Post Entity (Scala)\nDESCRIPTION: Creates the EventSourcedBehavior for a blog post entity in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_26\n\nLANGUAGE: Scala\nCODE:\n```\nEventSourcedBehavior[\n    Command,\n    Event,\n    State\n](\n  persistenceId = PersistenceId.ofUniqueId(entityId),\n  emptyState = EmptyState,\n  commandHandler = commandHandler,\n  eventHandler = eventHandler\n)\n```\n\n----------------------------------------\n\nTITLE: Using mapWithResource with Database in Java\nDESCRIPTION: Implementation showing how to use mapWithResource to safely query a database using a shared connection in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapWithResource.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nBlockingDatabase blockingDb = null; // inject db\n\nSource.from(Arrays.asList(1, 2, 3))\n    .mapWithResource(\n        () -> blockingDb.newConnection(),                            // create\n        (connection, id) -> {                                        // extract\n          List<String> byId = connection.queryById(id);\n          List<String> byName = connection.queryByName(\"name\" + id);\n          List<String> all = new ArrayList<>(byId);\n          all.addAll(byName);\n          return all;\n        },\n        connection -> {                                              // close\n          connection.close();\n          return Optional.empty();\n        })\n    .mapConcat(elem -> elem)\n```\n\n----------------------------------------\n\nTITLE: Implementing Tagging in DurableStateBehavior for Persistence Querying (Scala)\nDESCRIPTION: Illustrates how to add tagging support to a Scala DurableStatePersistentBehavior, allowing persisted states to be tagged and later queried as filtered streams. Requires Akka Persistence Query and compatible DurableStateStoreQuery plugin. Tags are applied to states or events for aggregation and querying purposes. Includes the necessary configuration for practical use.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_29\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [DurableStatePersistentBehaviorCompileOnly.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/DurableStatePersistentBehaviorCompileOnly.scala) { #tagging }\n```\n\n----------------------------------------\n\nTITLE: Request-Response with Scala 3 Union Types\nDESCRIPTION: Shows how to implement request-response pattern in Scala 3 using union types to combine command protocol and response messages, narrowing the behavior to maintain the public protocol.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n  // Protocol for cookieShop\n  sealed trait Command\n  final case class CookieOrder(cookie: String, replyTo: ActorRef[Response]) extends Command\n  \n  // Protocol for responses\n  sealed trait Response\n  final case class Confirmation(cookie: String) extends Response\n  final case class Invalid(reason: String) extends Response\n  \n  // A registry actor that handles registration of cookie shops\n  sealed trait RegistryCommand\n  final case class Register(cookieShop: ActorRef[Command]) extends RegistryCommand\n\n  // The registry that also can receive responses from cookie shops but only exposes the registry API\n  val registry: Behavior[RegistryCommand] = registry(Map.empty).narrow[RegistryCommand]\n  \n  def registry(shops: Map[String, ActorRef[Command]]): Behavior[RegistryCommand | Response] =\n    Behaviors.receive { (context, message) =>\n      message match {\n        case Register(cookieShop) =>\n          // using self and receiving the response\n          cookieShop ! CookieOrder(\"Chocolate chip\", context.self)\n          registry(shops + (\"the cookie shop\" -> cookieShop))\n        \n        case Confirmation(cookie) =>\n          context.log.info(\"Yay! Our cookie {} was accepted\", cookie)\n          Behaviors.same\n  \n        case Invalid(reason) =>\n          context.log.warn(\"Bummer! {}\", reason)\n          Behaviors.same\n      }\n    }\n```\n\n----------------------------------------\n\nTITLE: Handling Multiple Exception Types with Nested Akka Typed Supervision (Scala)\nDESCRIPTION: Demonstrates nesting `Behaviors.supervise` calls in Scala to apply different supervisor strategies based on the exception type. `IllegalStateException` triggers a restart, while `IllegalArgumentException` triggers resuming.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n// #multiple\nval supervisedBehavior4: Behavior[String] = Behaviors\n  .supervise(\n    // Each exception needs its own supervisor, exceptions propagate upwards\n    Behaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.restart))\n  .onFailure[IllegalArgumentException](SupervisorStrategy.resume)\n// #multiple\n\n```\n\n----------------------------------------\n\nTITLE: Using the Database Connection Pool Extension in Java\nDESCRIPTION: Demonstrates how to access and use the DatabaseConnectionPool extension within an actor or other components of the ActorSystem in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nDatabaseConnectionPool connectionPool = DatabaseConnectionPool.Id.getInstance().get(system);\nconnectionPool.queryDatabase(\"id1\");\n```\n\n----------------------------------------\n\nTITLE: Changing Actor Behavior in Functional Style (Scala)\nDESCRIPTION: Illustrates a common functional programming pattern in Akka Typed (Scala) where an actor changes its behavior in response to messages (e.g., from `idle` to `workInProgress`). This pattern is often used with supervision, where the supervisor wraps the initial behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n// #wrap\ndef worker(inProgress: Int): Behavior[String] = {\n  Behaviors.receiveMessage {\n    case \"step\" =>\n      worker(inProgress + 1)\n    case \"reset\" =>\n      worker(0)\n    case _ => Behaviors.unhandled\n  }\n}\n\ndef workInProgress(count: Int): Behavior[String] = ???\ndef idle: Behavior[String] = ???\n\nval initial: Behavior[String] = Behaviors.receiveMessage[String] {\n  case \"start\" => idle\n  case _       => Behaviors.unhandled\n}\n// #wrap\n\n```\n\n----------------------------------------\n\nTITLE: Production Logback Configuration with AsyncAppender - Logback - XML\nDESCRIPTION: This configuration snippet (referenced as logback.xml) demonstrates how to set up Logback for production use with Akka, particularly focusing on asynchronous logging via AsyncAppender. By leveraging AsyncAppender, logging is offloaded to a background thread for higher throughput and non-blocking log emission. Key parameters include buffer capacity, log format, and output destinations; INFO and DEBUG messages may be dropped if overloaded. The configuration file must be placed in src/main/resources/logback.xml and assumes Logback is on the runtime classpath.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_13\n\nLANGUAGE: xml\nCODE:\n```\n@@snip [logback.xml](/akka-actor-typed-tests/src/test/resources/logback-doc-prod.xml)\n```\n\n----------------------------------------\n\nTITLE: Defining HelloWorld Actor in Akka Classic (Scala)\nDESCRIPTION: Example of a classic Akka actor extending the Actor class to create a simple HelloWorld actor in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/from-classic.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass HelloWorldClassic extends Actor {\n  override def receive: Receive = {\n    case \"hello\" =>\n      println(\"Hello World!\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Command Handler Logic (Scala)\nDESCRIPTION: Provides the Scala implementation of the `commandHandler` for the counter example. It pattern matches on the incoming `Command` and returns the appropriate `Effect`: `persist` for state changes (`Increment`, `IncrementBy`) and `none` for read-only operations (`GetValue`), potentially chaining a `thenReply` side effect.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.persistence.typed.scaladsl.Effect\n\nval commandHandler: (State, Command) => Effect[State] = {\n  (state, command) =>\n    command match {\n      case Increment =>\n        Effect.persist(State(state.value + 1))\n      case IncrementBy(value) =>\n        Effect.persist(State(state.value + value))\n      case GetValue(replyTo) =>\n        Effect.none.thenReply(replyTo)(s => s)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating a Digest of a ByteString Stream (Akka Streams Java, Part 2)\nDESCRIPTION: Continuation of the digest-calculation operator in Java, covering finalization, result emission, and custom handler management. Illustrates full pattern for Java users wanting to extend processing logic at the GraphStage level. Requires Akka Streams and Java security APIs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeDigest.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeDigest.java) { #calculating-digest2 }\n```\n\n----------------------------------------\n\nTITLE: Flattening a Stream of Sequences (Akka Streams Scala)\nDESCRIPTION: Demonstrates use of mapConcat(identity) to flatten incoming streams of Seq into individual element emissions. Useful for sources that emit collections which must be processed element-wise downstream. Requires elements to be convertible to Seq; outputs a stream of elements unwrapped from their parent collections.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeFlattenSeq.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeFlattenSeq.scala) { #flattening-seqs }\n```\n\n----------------------------------------\n\nTITLE: Defining Akka Stream Sources and Sinks in Java\nDESCRIPTION: Provides examples of creating basic Akka Stream Sources (e.g., from an Iterable, a single element, a CompletionStage) and Sinks (e.g., folding over elements, invoking a function per element, ignoring elements) using the static factory methods in the Source and Sink classes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #source-sink }\n```\n\n----------------------------------------\n\nTITLE: Defining State for Blog Post Entity (Java)\nDESCRIPTION: Defines the state for a blog post entity using interfaces and classes in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\ninterface State {}\n\nenum EmptyState implements State {\n  INSTANCE\n}\n\nclass DraftState implements State {\n  final PostContent content;\n  public DraftState(PostContent content) {\n    this.content = content;\n  }\n}\n\nclass PublishedState implements State {\n  final PostContent content;\n  public PublishedState(PostContent content) {\n    this.content = content;\n  }\n}\n\nclass PostContent {\n  final String title;\n  final String body;\n  public PostContent(String title, String body) {\n    this.title = title;\n    this.body = body;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing zipWithIndex using statefulMap in Scala\nDESCRIPTION: Example showing how to associate a unique incrementing index with each element in the stream starting from 0 using statefulMap.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMap.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval numbers = Source(List(\"a\", \"b\", \"c\"))\n  .statefulMap(() => 0L)(\n    (index, element) => (index + 1, (element, index)),\n    _ => None\n  )\n```\n\n----------------------------------------\n\nTITLE: Scheduling Messages to Self in Akka Actors\nDESCRIPTION: This code snippet shows how to use timers to schedule messages to an actor in Akka. The example implements a 'Buncher' actor that buffers incoming messages and delivers them as a batch after a timeout or when the number of batched messages exceeds a maximum size.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_21\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [InteractionPatternsSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/InteractionPatternsSpec.scala) { #timer }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [InteractionPatternsTest.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/InteractionPatternsTest.java) { #timer }\n```\n\n----------------------------------------\n\nTITLE: Implementing Replicated Shopping Cart in Java\nDESCRIPTION: Java implementation of a shopping cart using CRDT Counter to track product quantities. The cart maintains eventual consistency across replicas by using Counter CRDT for add/remove operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing-cart.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n@snip [ShoppingCartExample](/akka-persistence-typed-tests/src/test/java/jdocs/akka/persistence/typed/ReplicatedShoppingCartExample.java) { #shopping-cart }\n```\n\n----------------------------------------\n\nTITLE: Initializing Cluster Sharding for an Entity Type in Java\nDESCRIPTION: Shows how to initialize Akka Cluster Sharding for a specific entity type (`Counter`) in Java. This registers the entity type and provides a factory method for creating entity actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nimport akka.cluster.sharding.typed.javadsl.Entity;\nimport akka.cluster.sharding.typed.javadsl.EntityTypeKey;\n\nEntityTypeKey<Counter.Command> TypeKey = EntityTypeKey.create(Counter.Command.class, \"Counter\");\n\n// #init\nActorRef<ShardingEnvelope<Counter.Command>> shardRegion =\n    sharding.init(Entity.of(TypeKey, entityContext -> Counter.create(entityContext.getEntityId())));\n// #init\n```\n\n----------------------------------------\n\nTITLE: Merging Sources in Scala using Akka Streams\nDESCRIPTION: Example demonstrating how to merge multiple sources in Akka Streams using Scala. The merge operator combines elements from different sources randomly when elements are available.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/merge.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source1 = Source(List(1))\\nval source2 = Source(List(2))\\nval source3 = Source(List(3))\\n\\nval sources: Source[Int, NotUsed] =\\n  Source.combine(source1, source2, source3)(Merge(_))\n```\n\n----------------------------------------\n\nTITLE: Defining a Counter Singleton Actor - Akka Cluster Typed - Java\nDESCRIPTION: Defines a counter behavior to serve as a singleton actor in an Akka Cluster using Java’s typed actor API. This code outlines the typed message interface, stateful behavior for incrementing or reporting the counter, and is intended for use as the unique singleton instance cluster-wide. Requires Akka Cluster Typed libraries and Java Actor API. Handles message routing and state updates for the singleton.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic interface CounterCommand {}\npublic class Increment implements CounterCommand {}\npublic class GetValue implements CounterCommand {\n  public final ActorRef<Integer> replyTo;\n  public GetValue(ActorRef<Integer> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\nBehavior<CounterCommand> counterBehavior = Behaviors.setup(context -> {\n  class Counter {\n    int count = 0;\n    Behavior<CounterCommand> behavior() {\n      return Behaviors.receiveMessage(msg -> {\n        if (msg instanceof Increment) {\n          count++;\n          return behavior();\n        } else if (msg instanceof GetValue) {\n          ((GetValue) msg).replyTo.tell(count);\n          return Behaviors.same();\n        }\n        return Behaviors.unhandled();\n      });\n    }\n  }\n  return new Counter().behavior();\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka System with Fallback\nDESCRIPTION: Demonstrates how application config is merged with reference config using Config withFallback method.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/configuration.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nappConfig.withFallback(ConfigFactory.defaultReference(classLoader))\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Source with Emission Logic (Scala)\nDESCRIPTION: This Scala code snippet provides a complete implementation for a custom GraphStage Source that emits numbers from 1 upward. It demonstrates how to create mutable state within GraphStageLogic and emit elements by implementing an OutHandler. The onPull callback is leveraged to emit the next element, respecting Akka's backpressure model.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #custom-source-example }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Framing Protocol as a Bidirectional Flow using GraphStage (Java)\nDESCRIPTION: This snippet shows a Java implementation of a framing protocol using GraphStage in Akka Streams Java DSL. The stage manages delimited message extraction from a ByteString stream, supporting inbound/outbound framing of data. It depends on akka.stream.javadsl and akka.util.ByteString. Inputs and outputs are ByteString; operator ensures message boundaries are properly recognized and handled.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\nGraph<FlowShape<ByteString, ByteString>, NotUsed> framing = GraphDSL.create(b -> {\n  final Inlet<ByteString> in = Inlet.create(\"Framing.in\");\n  final Outlet<ByteString> out = Outlet.create(\"Framing.out\");\n  // Framing logic ...\n  return FlowShape.of(in, out);\n});\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Handler for Blog Post Entity (Scala)\nDESCRIPTION: Implements the event handler for a blog post entity using pattern matching in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\nprivate def eventHandler(state: State, event: Event): State =\n  state match {\n    case EmptyState =>\n      event match {\n        case PostAdded(content) =>\n          DraftState(content)\n        case _ =>\n          throw new IllegalStateException(s\"unexpected event [$event] in state [$state]\")\n      }\n\n    case d: DraftState =>\n      event match {\n        case BodyChanged(newBody) =>\n          d.copy(content = d.content.copy(body = newBody))\n        case PostPublished =>\n          PublishedState(d.content)\n        case _ =>\n          throw new IllegalStateException(s\"unexpected event [$event] in state [$state]\")\n      }\n\n    case p: PublishedState =>\n      event match {\n        case CommentAdded(_, _) =>\n          p // We could add the comment to the state if we like\n        case _ =>\n          throw new IllegalStateException(s\"unexpected event [$event] in state [$state]\")\n      }\n  }\n```\n\n----------------------------------------\n\nTITLE: Fire and Forget Protocol Definition in Scala and Java\nDESCRIPTION: Defines a protocol for fire and forget message interaction with an actor, including a CoffeeOrder message class and a Barista actor behavior that processes orders.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n  trait Command\n  final case class CoffeeOrder(coffee: String, replyTo: ActorRef[Confirmation]) extends Command\n\n  final case class Confirmation(coffee: String)\n\n  val coffeeShop: Behavior[Command] = Behaviors.receive { (context, message) =>\n    message match {\n      case CoffeeOrder(coffee, replyTo) =>\n        context.log.info(\"Preparing coffee {}...\", coffee)\n        // prepare coffee here...\n        context.log.info(\"Coffee {} ready\", coffee)\n        Behaviors.same\n    }\n  }\n```\n\nLANGUAGE: java\nCODE:\n```\n  interface Command {}\n\n  public static class CoffeeOrder implements Command {\n    public final String coffee;\n    public final ActorRef<Confirmation> replyTo;\n\n    public CoffeeOrder(String coffee, ActorRef<Confirmation> replyTo) {\n      this.coffee = coffee;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Confirmation {\n    public final String coffee;\n\n    public Confirmation(String coffee) {\n      this.coffee = coffee;\n    }\n  }\n\n  public static Behavior<Command> coffeeShop() {\n    return Behaviors.receive(Command.class)\n        .onMessage(CoffeeOrder.class, order -> {\n          System.out.println(\"Preparing coffee \" + order.coffee + \"...\");\n          // prepare coffee here...\n          System.out.println(\"Coffee \" + order.coffee + \" ready\");\n          return Behaviors.same();\n        })\n        .build();\n  }\n```\n\n----------------------------------------\n\nTITLE: Managing Child Actors in Typed Akka (Scala)\nDESCRIPTION: This snippet demonstrates how to manage child actors in Typed Akka using a Map for bookkeeping. It includes methods for creating, retrieving, and removing child actors, as well as handling actor termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/from-classic.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nfinal case class Command(name: String, replyTo: ActorRef[ActorRef[Child.Command]])\n\nclass Parent extends AbstractBehavior[Command] {\n  private var children = Map.empty[String, ActorRef[Child.Command]]\n\n  override def onMessage(msg: Command): Behavior[Command] = {\n    msg match {\n      case Command(name, replyTo) =>\n        children.get(name) match {\n          case Some(child) =>\n            replyTo ! child\n          case None =>\n            val child = context.spawn(Child(), name)\n            context.watchWith(child, ChildTerminated(name))\n            children += (name -> child)\n            replyTo ! child\n        }\n    }\n    this\n  }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case ChildTerminated(name) =>\n      children -= name\n      this\n  }\n\n  private case class ChildTerminated(name: String)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Sharded Actor Behavior in Scala\nDESCRIPTION: Illustrates the definition of a simple `Counter` actor behavior in Scala, intended to be managed by Akka Cluster Sharding. It handles `Increment` and `GetValue` messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.typed.ActorRef\nimport akka.actor.typed.Behavior\nimport akka.actor.typed.scaladsl.Behaviors\nimport akka.cluster.sharding.typed.scaladsl.EntityTypeKey\n\n// #counter\nobect Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Int]) extends Command\n\n  val TypeKey = EntityTypeKey[Command](\"Counter\")\n\n  def apply(entityId: String): Behavior[Command] = {\n    def updated(value: Int): Behavior[Command] =\n      Behaviors.receiveMessage[Command] {\n        case Increment =>\n          updated(value + 1)\n        case GetValue(replyTo) =>\n          replyTo ! value\n          Behaviors.same\n      }\n\n    updated(0)\n\n  }\n}\n// #counter\n```\n\n----------------------------------------\n\nTITLE: Streaming File Content using FileIO Source in Java\nDESCRIPTION: Shows the Java API for streaming file content using `akka.stream.javadsl.FileIO.fromPath`. It creates a `Source<ByteString, CompletionStage<IOResult>>` that reads the specified file in chunks, making it suitable for processing large files efficiently without high memory usage. The `chunkSize` parameter controls the size of the `ByteString` chunks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n// Code for [StreamFileDocTest.java](/akka-docs/src/test/java/jdocs/stream/io/StreamFileDocTest.java) { #file-source } not available in input\n```\n\n----------------------------------------\n\nTITLE: Combining Pipelining and Parallel Processing in Scala\nDESCRIPTION: This Scala snippet shows how to merge pipelining and parallel processing approaches using Akka Streams. The example employs multiple chefs to achieve efficient pancake cooking, illustrating the combined concurrency patterns. Dependencies include Akka Streams configuration. This pattern is beneficial for complex workflows with independent jobs requiring sequenced tasks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-parallelism.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowParallelismDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowParallelismDocSpec.scala) { #parallel-pipeline }\n```\n\n----------------------------------------\n\nTITLE: Implementing flatMapMerge in Java\nDESCRIPTION: Java implementation of the flatMapMerge operator to process customer IDs concurrently, demonstrating how to create and merge multiple Sources while maintaining per-customer ordering.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/flatMapMerge.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3))\n    .flatMapMerge(\n        2, // 2 means up to two customers can be interleaved\n        customerId ->\n            Source.fromIterator(\n                    () -> Stream.iterate(1, i -> i + 1).iterator())\n                .map(i -> new Pair<>(customerId, i))\n                .take(3))\n    // customerId -> count\n    // customer 1: (1,1), (1,2), (1,3)\n    // customer 2: (2,1), (2,2), (2,3)\n    // customer 3: (3,1), (3,2), (3,3)\n    // but the order between different customers is not deterministic\n```\n\n----------------------------------------\n\nTITLE: JUnit Integration for ActorTestKit - Java\nDESCRIPTION: Illustrates how to use JUnit's integration features to efficiently shutdown the ActorSystem after test execution with ActorTestKit, making test handling more streamlined in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AsyncTestingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/JunitIntegrationExampleTest.java) { #junit-integration }\n```\n\n----------------------------------------\n\nTITLE: Combining Materialized Values for a Composite Sink in Scala\nDESCRIPTION: This Scala snippet constructs a composite Akka Stream `Sink`. It connects the previously defined `nestedFlow` (materializing a `Future[OutgoingConnection]`) to a `Sink.fold` (materializing a `Future[String]`). The `Keep.both` combiner is used, resulting in the `nestedSink` having a materialized value of `Pair[Future[OutgoingConnection], Future[String]]`, preserving both values from the connected components.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\n//#mat-combine-3\nval sink = Sink.fold[String, ByteString](\"\")((acc, i) => acc + i.utf8String)\n\nval nestedSink: Sink[Int, (Future[OutgoingConnection], Future[String])] =\n  nestedFlow.toMat(sink)(Keep.both)\n//#mat-combine-3\n```\n\n----------------------------------------\n\nTITLE: Joining and Subscribing to Cluster Events in Java\nDESCRIPTION: Java implementation that combines joining a cluster and subscribing to cluster events. Shows a common pattern when initializing a cluster-aware actor in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nfinal Address selfAddress = Cluster.get(system).selfAddress();\nCluster.get(system).join(selfAddress);\nCluster.get(system).subscribe(getSelf(), ClusterEvent.MemberEvent.class);\n```\n\n----------------------------------------\n\nTITLE: Implementing Codec Functions for Bidirectional Flow (Scala)\nDESCRIPTION: This Scala code provides the actual implementations of codecs for transforming between strings and ByteString within a BidiFlow. The encode function serializes a String to ByteString, while decode reverses the process. Dependencies are akka.util.ByteString. The functions accept a string or ByteString and return the transformed representation for use in the data flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\ndef encode(str: String): ByteString = ByteString(str)\ndef decode(bs: ByteString): String = bs.utf8String\n\n```\n\n----------------------------------------\n\nTITLE: Sending Message to Remote Actor in Java\nDESCRIPTION: This Java snippet highlights how to send a message to a remote actor using ActorSelection's tell method, essential for distributed systems leveraging Akka's capabilities. The use of getSelf() ensures the sender information is attached to the message. The input is a message string, and the output is implicit, as the operation is asynchronous.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nselection.tell(\"Pretty awesome feature\", getSelf());\n```\n\n----------------------------------------\n\nTITLE: ByteString Chunker for Akka Streams (Java)\nDESCRIPTION: Java version of the custom GraphStage that chunks up a stream of ByteStrings into ByteStrings of a specified maximum size.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_50\n\nLANGUAGE: Java\nCODE:\n```\npublic class ByteStringChunker extends GraphStage<FlowShape<ByteString, ByteString>> {\n    private final int chunkSize;\n    private final Inlet<ByteString> in = Inlet.create(\"ByteStringChunker.in\");\n    private final Outlet<ByteString> out = Outlet.create(\"ByteStringChunker.out\");\n    private final FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);\n\n    public ByteStringChunker(int chunkSize) {\n        this.chunkSize = chunkSize;\n    }\n\n    @Override\n    public FlowShape<ByteString, ByteString> shape() {\n        return shape;\n    }\n\n    @Override\n    public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n        return new GraphStageLogic(shape) {\n            private ByteString buffer = ByteString.empty();\n\n            {\n                setHandler(\n                        out,\n                        new AbstractOutHandler() {\n                            @Override\n                            public void onPull() throws Exception {\n                                emitChunk();\n                            }\n                        });\n                setHandler(\n                        in,\n                        new AbstractInHandler() {\n                            @Override\n                            public void onPush() throws Exception {\n                                buffer = buffer.concat(grab(in));\n                                emitChunk();\n                            }\n\n                            @Override\n                            public void onUpstreamFinish() throws Exception {\n                                if (buffer.isEmpty()) completeStage();\n                                else emitChunk();\n                            }\n                        });\n            }\n\n            private void emitChunk() {\n                if (buffer.isEmpty()) {\n                    if (isClosed(in)) completeStage();\n                    else pull(in);\n                } else {\n                    Pair<ByteString, ByteString> split = buffer.splitAt(chunkSize);\n                    buffer = split.second();\n                    push(out, split.first());\n                }\n            }\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Emitting Initial Message in Back-Pressured Cycles with Akka Streams in Scala and Java\nDESCRIPTION: This snippet shows how to break back-pressure cycles by emitting an initial 'hello' message in a chat-like application, merging a Source with a single element. Dependencies are Akka Streams, with inputs being command data and outputs being processed messages. It demonstrates handling conversational flow starting.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [StreamTcpDocSpec.scala](/akka-docs/src/test/scala/docs/stream/io/StreamTcpDocSpec.scala) { #welcome-banner-chat-server }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [StreamTcpDocTest.java](/akka-docs/src/test/java/jdocs/stream/io/StreamTcpDocTest.java) { #welcome-banner-chat-server }\n```\n\n----------------------------------------\n\nTITLE: Defining a Counter Singleton Actor - Akka Cluster Typed - Scala\nDESCRIPTION: Demonstrates the creation of a basic counter behavior to be run as a singleton actor within an Akka Cluster using Scala's typed actor API. This actor’s behavior can be shared cluster-wide using the singleton manager mechanism. No external dependencies beyond Akka Cluster Typed are required. The snippet covers expected message handling and outputs updated state upon receiving increment requests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\ntrait CounterCommand\ncase object Increment extends CounterCommand\ncase class GetValue(replyTo: ActorRef[Int]) extends CounterCommand\n\nval counterBehavior: Behavior[CounterCommand] =\n  Behaviors.setup { context =>\n    def updated(count: Int): Behavior[CounterCommand] =\n      Behaviors.receiveMessage {\n        case Increment =>\n          updated(count + 1)\n        case GetValue(replyTo) =>\n          replyTo ! count\n          Behaviors.same\n      }\n    updated(0)\n  }\n```\n\n----------------------------------------\n\nTITLE: Using Timers in Akka Stream Operators - Scala\nDESCRIPTION: This snippet exemplifies the use of timers within Akka Stream GraphStages by subclassing TimerGraphStageLogic. Operators toggle states based on a timer's key, consuming upstream messages conditional upon the state. Timers cannot be scheduled in the constructor but within the preStart lifecycle method.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_21\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #timed }\n```\n\n----------------------------------------\n\nTITLE: Command Handler Persists Add Payload - Java\nDESCRIPTION: The Java code snippet illustrates a command handler that persists an 'Add' payload as an 'Added' event using Akka's typed persistence API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BasicPersistentBehaviorTest.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/BasicPersistentBehaviorTest.java) { #command-handler }\n```\n\n----------------------------------------\n\nTITLE: Implementing MergeHub in Akka Streams - Java\nDESCRIPTION: This Java code snippet demonstrates how to implement the MergeHub feature in Akka Streams. It implements a dynamic fan-in junction point, allowing multiple producers to send data to one consumer. Needs Akka Streams library, supporting dynamic producer-consumer connections.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [HubDocTest.java](/akka-docs/src/test/java/jdocs/stream/HubDocTest.java) { #merge-hub }\n```\n\n----------------------------------------\n\nTITLE: Handling PreRestart Signal in Akka Typed Actor (Scala)\nDESCRIPTION: This snippet demonstrates how to implement an Akka Typed actor in Scala that handles the PreRestart signal to perform cleanup before a supervised actor restarts. It registers a signal handler for PreRestart to ensure resource cleanup, with the returned behavior ignored after receiving this signal. Dependencies: Akka Typed (Scala), with the actor's behavior defined using message and signal handlers. No inputs beyond standard actor signals; output is side effects (cleanup) and continued actor lifecycle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\nBehaviors.receive[String] { (context, message) =>\n  // handle regular messages\n  Behaviors.same\n}.receiveSignal {\n  case (context, PreRestart) =>\n    // cleanup resources\n    Behaviors.same\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Command Handler Logic (Java)\nDESCRIPTION: Provides the Java implementation of the `commandHandler` method for the counter example. It uses `if-else` statements to check the command type and returns the appropriate `Effect`: `persist` for state changes (`Increment`, `IncrementBy`) and `none` for read-only operations (`GetValue`), potentially chaining a `thenReply` side effect.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic CommandHandler<Command, State> commandHandler() {\n\n  return newCommandHandlerBuilder()\n      .forAnyState()\n      .onCommand(\n          Increment.class,\n          (state, command) -> {\n            return Effect().persist(state.increment(1));\n          })\n      .onCommand(\n          IncrementBy.class,\n          (state, command) -> {\n            return Effect().persist(state.increment(command.value));\n          })\n      .onCommand(\n          GetValue.class,\n          (state, command) -> {\n            return Effect().none().thenReply(command.replyTo, s -> s);\n          })\n      .build();\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Stash in an Akka Typed Actor (Scala)\nDESCRIPTION: Demonstrates using `StashBuffer` within a Scala Akka Typed actor (`DataAccess`). It shows how to stash incoming messages (`Get`, `Save`) while loading initial data from a database or during a save operation, ensuring sequential processing. The example uses `Behaviors.withStash` to create a stash buffer with a defined capacity and `buffer.unstashAll` to process buffered messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/stash.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\npackage docs.akka.typed\n\nimport akka.actor.typed.ActorRef\nimport akka.actor.typed.Behavior\nimport akka.actor.typed.StashBuffer\nimport akka.actor.typed.scaladsl.Behaviors\n\nimport scala.concurrent.duration._\n\nobject StashDocSpec {\n\n  // #stashing\n  // imagine database can be unavailable for some time\n  trait DB {\n    def save(id: String, value: String): Unit\n    def load(id: String): String\n  }\n\n  sealed trait Command\n  final case class Save(value: String, replyTo: ActorRef[Unit]) extends Command\n  final case class Get(replyTo: ActorRef[String]) extends Command\n  private final case class InitialState(value: String) extends Command\n  private final case class SaveSuccess() extends Command\n  private case object DBError extends Command\n\n  object DataAccess {\n\n    def apply(id: String, db: DB): Behavior[Command] = {\n      Behaviors.setup[Command] { context =>\n        // create a stash buffer with capacity 100\n        Behaviors.withStash[Command](100) { buffer =>\n          // load initial state from db\n          // we can use context.pipeToSelf for this but this is an example\n          // we could also use a Twitter future\n          val initialValue: String = db.load(id)\n          context.log.info(s\"Loaded initial value [$initialValue] for id [$id]\")\n          context.self ! InitialState(initialValue)\n\n          // Initial behavior is waiting for InitialState\n          // All other messages are stashed\n          initializing(id, db, buffer)\n        }\n      }\n    }\n\n    private def initializing(id: String, db: DB, buffer: StashBuffer[Command]): Behavior[Command] =\n      Behaviors.receiveMessage {\n        case InitialState(value) =>\n          // Initial state received, unstash all messages & switch to active behavior\n          buffer.unstashAll(active(id, db, buffer, value))\n        case other =>\n          // stash all other messages for later processing\n          buffer.stash(other)\n          Behaviors.same\n      }\n\n    private def active(id: String, db: DB, buffer: StashBuffer[Command], state: String): Behavior[Command] =\n      Behaviors.receiveMessage[Command] {\n        case Get(replyTo) =>\n          replyTo ! state\n          Behaviors.same\n        case Save(value, replyTo) =>\n          db.save(id, value)\n          // might fail, requiring supervision\n          // switch to busy state\n          replyTo ! (())\n          busy(id, db, buffer, value)\n        case InitialState(_) | SaveSuccess() | DBError =>\n          // should not happen in active state\n          Behaviors.unhandled\n      }\n\n    private def busy(id: String, db: DB, buffer: StashBuffer[Command], state: String): Behavior[Command] =\n      Behaviors.receiveMessage {\n        case Get(replyTo) =>\n          // stash until saved\n          buffer.stash(Get(replyTo))\n          Behaviors.same\n        case Save(value, replyTo) =>\n          // stash until saved\n          buffer.stash(Save(value, replyTo))\n          Behaviors.same\n        case InitialState(_) =>\n          // should not happen in busy state\n          Behaviors.unhandled\n        case SaveSuccess() =>\n          // saved successfully, unstash all messages & switch to active behavior\n          buffer.unstashAll(active(id, db, buffer, state))\n        case DBError =>\n          // handle error, maybe retry\n          // for now just go back to active\n          buffer.unstashAll(active(id, db, buffer, state))\n\n      }\n\n  }\n  // #stashing\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Recovery Completion Event in Akka Persistence (Scala)\nDESCRIPTION: Illustrates how to execute custom logic after the recovery phase completes within a PersistentActor. This is done by handling the special `RecoveryCompleted` message in the `receiveRecover` block. This message is guaranteed to be received after all events/snapshots have been processed but before any regular commands.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\ndef receiveRecover: Receive = {\n  case evt: Evt                               => updateState(evt)\n  case SnapshotOffer(metadata, snapshot: State) => state = snapshot\n  case RecoveryCompleted =>\n    log.info(\n      \"MyPersistentActor [{}] recovery completed. Current state: [{}]\",\n      persistenceId,\n      state)\n}\n```\n\n----------------------------------------\n\nTITLE: Storing State in Scala FSM Implementation\nDESCRIPTION: Demonstrates how to store state in a Scala implementation of a Finite State Machine using Akka Actors. The state includes a queue of objects and a reference to the target actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fsm.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nprivate case object Timeout extends Event\n\nclass BatchFSM extends AbstractBehavior[Event] {\n  private var queue = Vector.empty[Any]\n  private var target: Option[ActorRef[Batch]] = None\n\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Time-Based Processing to Streams in Java\nDESCRIPTION: Demonstrates combining two streams (factorials and numbers) with throttling to produce time-based output in Java. Shows usage of zip, map, and throttle operators.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\nfactorials\n  .zipWith(\n    Source.range(0, 100),\n    (num, idx) -> String.format(\"%d! = %s\", idx, num)\n  )\n  .throttle(1, Duration.ofSeconds(1))\n  .runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Enforcing Replies with DurableStateBehaviorWithEnforcedReplies (Java)\nDESCRIPTION: Illustrates how to use DurableStateBehaviorWithEnforcedReplies in Java, ensuring all DurableStateBehavior command handlers return a reply effect at compile time. Relies on the Akka Persistence Typed Java DSL. Commands are required to include an ActorRef for reply purposes. Outputs are reply messages confirming acceptance or rejections. Limitations include stricter template requirements from the framework.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_24\n\nLANGUAGE: java\nCODE:\n```\n@@snip [AccountExampleWithNullDurableState.java](/akka-cluster-sharding-typed/src/test/java/jdocs/akka/cluster/sharding/typed/AccountExampleWithNullDurableState.java) { #withEnforcedReplies }\n```\n\n----------------------------------------\n\nTITLE: Configuring SSLEngine for TLS in Akka Streams (Java)\nDESCRIPTION: Provides the Java code for configuring an `SSLEngine` for secure TLS connections in Akka Streams. It covers loading security credentials (keystore, truststore) using Java's security APIs, initializing an `SSLContext`, and creating/configuring the `SSLEngine` instance required by Akka's TLS-enabled TCP methods.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n// Code for [TcpTest.java](/akka-stream-tests/src/test/java/akka/stream/javadsl/TcpTest.java) { #setting-up-ssl-engine } not available in input\n```\n\n----------------------------------------\n\nTITLE: Implementing Request-Response with ask from Outside an Akka Typed Actor System\nDESCRIPTION: This snippet demonstrates how to use the 'ask' pattern to interact with Akka Typed actors from outside the actor system. It returns a Future[Response] in Scala or CompletionStage<Response> in Java, which completes with a successful response or fails with a TimeoutException.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [InteractionPatternsSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/InteractionPatternsSpec.scala) { #standalone-ask }\n```\n\n----------------------------------------\n\nTITLE: Using Generic Response Wrapper in Akka Typed Actor-to-Actor ask (Java)\nDESCRIPTION: This snippet demonstrates the use of the StatusReply generic response wrapper in Akka Typed for actor-to-actor ask patterns in Java. It shows how to handle both successful responses and validation errors using askWithStatus.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\n@@snip [InteractionPatternsTest.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/InteractionPatternsAskWithStatusTest.java) { #actor-ask-with-status }\n```\n\n----------------------------------------\n\nTITLE: Entity Supervisor Strategy in Scala\nDESCRIPTION: Demonstrates how to implement a custom supervisor strategy for entity actors in Scala by creating an intermediate parent actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [ClusterShardingSpec.scala](/akka-cluster-sharding/src/multi-jvm/scala/akka/cluster/sharding/ClusterShardingSpec.scala) { #supervisor }\n```\n\n----------------------------------------\n\nTITLE: Implementing flexible read protocol for device messages in Akka Typed\nDESCRIPTION: Defines the read protocol for querying device temperature with a correlation ID to support distributed applications. This protocol allows for correlating requests and responses when implementing resends for timed out requests or querying multiple actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n#read-protocol-2\n```\n\nLANGUAGE: Java\nCODE:\n```\n#read-protocol-2\n```\n\n----------------------------------------\n\nTITLE: Binding Address for Echo Server with Akka Streams in Scala and Java\nDESCRIPTION: This snippet illustrates how to bind to a given address to create a simple Echo Server using Akka Streams. It provides IncomingConnection elements for each new connection. Dependencies include Akka Streams and Scala/Java Futures. Inputs are the address and port, while outputs include connection handling via Stream.Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [StreamTcpDocSpec.scala](/akka-docs/src/test/scala/docs/stream/io/StreamTcpDocSpec.scala) { #echo-server-simple-bind }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [StreamTcpDocTest.java](/akka-docs/src/test/java/jdocs/stream/io/StreamTcpDocTest.java) { #echo-server-simple-bind }\n```\n\n----------------------------------------\n\nTITLE: Handling Last Writer Wins Event Updates in Akka Persistence - Java\nDESCRIPTION: This Java snippet offers analogous functionality to the Scala event handler, implementing last writer wins (LWW) semantics for replicated event sourcing with Akka Persistence Typed (Java). It processes event objects, using timestamp comparison logic to determine if incoming or persisted events should take priority in state updates. The snippet depends on Akka Persistence Typed for Java and assumes custom event/state classes. The event handler receives events (likely containing LwwTime data) and produces updated state based on the latest timestamp. Constraints include requiring monotonic timestamp increases and careful design to avoid clock drift errors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n/* @@snip [blog](/akka-persistence-typed-tests/src/test/java/jdocs/akka/persistence/typed/ReplicatedBlogExample.java) { #event-handler } */\n```\n\n----------------------------------------\n\nTITLE: Testing Actors in Akka with ActorTestKit - Java\nDESCRIPTION: This Java snippet showcases the basic setup for testing actors in Akka using ActorTestKit, similar to its Scala counterpart. It includes creating an actor system, spawning actors, and handling system shutdown. Akka and JUnit dependencies are required for running tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AsyncTestingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/AsyncTestingExampleTest.java) { #under-test }\n```\n\n----------------------------------------\n\nTITLE: Adding Features to Publish-Subscribe Channel in Scala\nDESCRIPTION: This snippet enhances the publish-subscribe channel by attaching a Sink.ignore to keep it drained when there are no subscribers. It's part of building a more feature-rich publish-subscribe service in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\nval (sink, source) = MergeHub.source[String](perProducerBufferSize = 16)\n  .toMat(BroadcastHub.sink(bufferSize = 256))(Keep.both)\n  .run()\n\n// Ensure that the Broadcast output is dropped if there are no listeners\nsource.runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Joining and Leaving a Cluster Programmatically in Scala\nDESCRIPTION: Illustrates how to programmatically join and leave an Akka Cluster if configuration is not used to specify seed nodes. Utilizes the 'manager' in the typed Cluster API for these operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\nBasicClusterExampleSpec.scala { #cluster-join }\nBasicClusterExampleSpec.scala { #cluster-leave }\n```\n\n----------------------------------------\n\nTITLE: Initializing the Cluster Singleton - Akka Cluster Typed - Scala\nDESCRIPTION: Illustrates how to initialize and spawn the singleton using the ClusterSingleton extension API in Scala. This code must be executed on each participating node to start or access the unique singleton actor. Key dependency is the Akka Cluster Typed library, and the required parameters include the singleton name and desired actor behavior. Outputs an ActorRef that proxies communication to the singleton instance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nval singletonManager = ClusterSingleton(context.system)\nval proxy: ActorRef[CounterCommand] = singletonManager.init(\n  SingletonActor(counterBehavior, \"GlobalCounter\")\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Akka Streams Components (Scala)\nDESCRIPTION: Illustrates how to create composite, reusable `Source` and `Sink` components in Scala by grouping internal flows using `named()`. This technique hides internal complexity and promotes modularity in stream graph design.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n//##nested-flow\nimport akka.stream.scaladsl._\n\nval nestedSource = Source.single(0) // An atomic source\n  .map(_ + 1) // an atomic processing stage\n  .named(\"nestedSource\") // wrap it in a reusable composit module\n\nval nestedFlow = Flow[Int].filter(_ != 0) // an atomic processing stage\n  .map(_ - 2) // another atomic processing stage\n  .named(\"nestedFlow\") // wrap it in a reusable composit module\n\nval nestedSink = nestedFlow.toMat(Sink.head[Int])(Keep.right) // wire an atomic sink to the nestedFlow\n  .named(\"nestedSink\") // wrap it the nestedFlow and the atomic Sink in a composite module\n\n// Create a RunnableGraph from the composite modules\nval runnableGraph = nestedSource.toMat(nestedSink)(Keep.right)\n//##nested-flow\n```\n\n----------------------------------------\n\nTITLE: Dropping Broadcast in Akka Streams (Scala)\nDESCRIPTION: Implements a broadcast that drops elements for slow consumers instead of backpressuring. Uses buffer with OverflowStrategy.dropHead.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_41\n\nLANGUAGE: Scala\nCODE:\n```\nval droppyBroadcast = GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  val broadcast = b.add(Broadcast[Int](outputPorts = 2))\n  val droppyFlow1 = Flow[Int].buffer(10, OverflowStrategy.dropHead)\n  val droppyFlow2 = Flow[Int].buffer(10, OverflowStrategy.dropHead)\n\n  broadcast.out(0) ~> droppyFlow1\n  broadcast.out(1) ~> droppyFlow2\n\n  FlowShape(broadcast.in, UniformFanOutShape(broadcast.out(0), broadcast.out(1)))\n}\n```\n\n----------------------------------------\n\nTITLE: Using PNCounter CRDT in Akka Distributed Data (Scala)\nDESCRIPTION: Demonstrates how to use the PNCounter replicated data type in Akka's DistributedData extension using Scala. This includes incrementing and decrementing a distributed, eventually-consistent counter across a cluster. Requires the Akka Distributed Data module and an actor system configured for clustering. Input operations are typically increment/decrement commands, and the output is the updated counter state after merging across nodes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n/* Scala Example: PNCounter usage in Akka Distributed Data */\nval counterKey = PNCounterKey(\"exampleCounter\")\ndistributedData ! Update(counterKey, PNCounter(), WriteLocal)(_ + 1) // increment\n// To decrement\n// distributedData ! Update(counterKey, PNCounter(), WriteLocal)(_ - 1)\n\n```\n\n----------------------------------------\n\nTITLE: Applying Backoff Supervision Strategy to Singleton Actor - Akka Cluster Typed - Java\nDESCRIPTION: Demonstrates how to wrap the singleton actor with a backoff restart supervision strategy in Java, increasing reliability in the event of failures. This leverages the Behaviors.supervise and SupervisorStrategy.restartWithBackoff APIs from Akka Typed. Requires configuration of backoff parameters and depends on the Akka supervision library.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nBehavior<CounterCommand> supervisedCounterBehavior =\n    Behaviors.supervise(counterBehavior)\n        .onFailure(Exception.class, SupervisorStrategy.restartWithBackoff(\n            Duration.ofSeconds(1), Duration.ofSeconds(10), 0.2));\n```\n\n----------------------------------------\n\nTITLE: Balancing Jobs to a Fixed Pool of Workers in Akka Streams\nDESCRIPTION: Java implementation of the worker pool pattern that distributes jobs to a configurable number of parallel workers. It creates a balanced processing flow that handles work distribution and result merging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_37\n\nLANGUAGE: Java\nCODE:\n```\npublic static <In, Out> Flow<In, Out, NotUsed> balancer(\n    Flow<In, Out, NotUsed> worker, int workerCount) {\n  return Flow.fromGraph(\n      GraphDSL.create(\n          b -> {\n            final Outlet<In> balancerOut = b.add(Balance.<In>create(workerCount, false)).out(0);\n            final UniformFanInShape<Out, Out> mergeIn = b.add(Merge.<Out>create(workerCount));\n\n            for (int i = 0; i < workerCount; i++) {\n              final FlowShape<In, Out> workerFlow = b.add(worker.async());\n              b.from(balancerOut).viaFanOut(b.add(Balance.<In>create(workerCount, false))).toInlet(workerFlow.in());\n              b.from(workerFlow.out()).toInlet(mergeIn.in(i));\n            }\n\n            return new FlowShape<>(balancerOut, mergeIn.out());\n          }));\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting Akka Stream Components in Java\nDESCRIPTION: Shows different ways to wire Sources, Sinks, and Flows together in Akka Streams using Java. Examples likely cover methods like `via()` for attaching Flows, `to()` for terminating with a Sink, and potentially `toMat()` for managing materialized values during connection.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #flow-connecting }\n```\n\n----------------------------------------\n\nTITLE: Acquiring ActorRef from ActorSelection in Akka\nDESCRIPTION: Demonstrates how to acquire an ActorRef from an ActorSelection by sending an Identify message and handling the ActorIdentity response.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\ncase ActorIdentity(`actorName`, Some(actor)) =>\n  actor ! \"hello\"\n```\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.ActorIdentity;\nimport akka.actor.Identify;\n\n...\n\nActorSelection selection = getContext().actorSelection(\"/user/another\");\nselection.tell(new Identify(37), getSelf());\n\n...\n\n@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n    .match(ActorIdentity.class, id -> {\n      if (id.getActorRef().isPresent()) {\n        ActorRef actorRef = id.getActorRef().get();\n        actorRef.tell(\"hello\", getSelf());\n      }\n    })\n    .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Using PNCounter CRDT in Akka Distributed Data (Java)\nDESCRIPTION: Shows how to use the PNCounter replicated data type in Akka Distributed Data with Java. The snippet demonstrates incrementing and decrementing a counter in a distributed context. Prerequisite dependencies include Akka Distributed Data for Java and a clustered actor system. Inputs are update requests, outputs are the updated counter states distributed across the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n// Java Example: PNCounter usage in Akka Distributed Data\nPNCounterKey counterKey = PNCounterKey.create(\"exampleCounter\");\ndistributedData.tell(new Update<>(counterKey, PNCounter.create(), WriteLocal.instance(), curr -> curr.increment(node, 1)), self());\n// To decrement:\n// distributedData.tell(new Update<>(counterKey, PNCounter.create(), WriteLocal.instance(), curr -> curr.decrement(node, 1)), self());\n\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing with Akka Streams using Java\nDESCRIPTION: This Java snippet demonstrates the parallel processing strategy in Akka Streams documented through Patrik's pancake frying method. It effectively scales processing by executing independent operations concurrently. Required dependencies include Akka Streams' Java API for proper functionality.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-parallelism.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowParallelismDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowParallelismDocTest.java) { #parallelism }\n```\n\n----------------------------------------\n\nTITLE: Using PNCounterMap in Akka Distributed Data (Scala)\nDESCRIPTION: Illustrates management of multiple related distributed counters using PNCounterMap in Scala. Allows for grouping counters by keys with atomic replication and updates. Requires Akka Distributed Data and a proper cluster setup; inputs are update operations for named counters, and the outputs are consistent counter states mapped by keys.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n/* Scala Example: PNCounterMap usage */\nval mapKey = PNCounterMapKey(\"mapCounterSet\")\ndistributedData ! Update(mapKey, PNCounterMap(), WriteLocal)(_ + (\"k1\" -> 1))\n\n```\n\n----------------------------------------\n\nTITLE: Creating Publish-Subscribe Channel with MergeHub and BroadcastHub in Java\nDESCRIPTION: This snippet shows how to connect a MergeHub and a BroadcastHub to create a publish-subscribe channel in Java. It returns a pair of Source and Sink that define the publish and subscribe sides of the channel.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nPair<Sink<String>, Source<String, NotUsed>> sinkAndSource =\n  MergeHub.of(String.class, 16)\n    .toMat(BroadcastHub.of(String.class, 256), Keep.both())\n    .run(mat);\n```\n\n----------------------------------------\n\nTITLE: After Pattern Usage in Java\nDESCRIPTION: Example showing how to use the after() pattern in Java to complete a CompletionStage with a value or exception after a timeout.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/futures.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@@snip [FutureDocTest.java](/akka-docs/src/test/java/jdocs/future/FutureDocTest.java) { #imports #after }\n```\n\n----------------------------------------\n\nTITLE: Restarting Actor on Specific Exception with Akka Typed Supervision (Java)\nDESCRIPTION: Demonstrates how to configure an Akka Typed actor in Java to restart automatically when an `IllegalStateException` is thrown. It uses `Behaviors.supervise` to wrap the actor's behavior and applies `SupervisorStrategy.restart()` as the recovery mechanism.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.typed.ActorRef;\nimport akka.actor.typed.Behavior;\nimport akka.actor.typed.SupervisorStrategy;\nimport akka.actor.typed.javadsl.Behaviors;\nimport jdocs.akka.typed.supervision.SupervisionCompileOnlyTest.Child;\n\nBehavior<String> behavior = Behaviors.empty(); // dummy\n// #restart\nfinal Behavior<String> supervisedBehavior =\n    Behaviors.supervise(behavior)\n        .onFailure(IllegalStateException.class, SupervisorStrategy.restart());\n// #restart\n\n```\n\n----------------------------------------\n\nTITLE: Implementing an Object-Oriented Style Counter Actor in Akka Typed (Scala)\nDESCRIPTION: Provides an example of a counter actor written in the object-oriented style in Akka Typed (Scala), encapsulating mutable state within a class and exposing behavior through methods and fields. Dependent on Akka Typed libraries, it commonly uses `AbstractBehavior` and a factory method for instantiation. Receives messages as input, manipulates internal mutable state, and outputs updated actor behavior. Commonly preferred when mutable state or familiar OOP idioms are required.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n// @@snip [StyleGuideDocExamples.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/StyleGuideDocExamples.scala) { #oo-style }\n\n```\n\n----------------------------------------\n\nTITLE: Handling Optional Fields when Deserializing Protobuf Messages in Akka Persistence (Scala)\nDESCRIPTION: Shows how to read optional fields from a deserialized Protobuf event message in Scala, providing a default value or modeling the field as an Option if the field is missing. Relies on Protobuf auto-generated classes and Akka serialization. This technique is crucial during schema evolution (e.g., adding a field to an event class) and helps maintain backward compatibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\nval event = FlightAppModels.SeatReserved.parseFrom(bytes)\nval seatType = if (event.hasSeatType) Some(event.getSeatType) else None\n```\n\n----------------------------------------\n\nTITLE: Command Handler Persists Add Payload - Scala\nDESCRIPTION: The command handler uses Akka's typed persistence API to persist an 'Add' payload into an 'Added' event in Scala. This is crucial for ensuring changes are recorded persistently.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BasicPersistentBehaviorCompileOnly.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/BasicPersistentBehaviorCompileOnly.scala) { #command-handler }\n```\n\n----------------------------------------\n\nTITLE: Implementing Codec Functions for Bidirectional Flow (Java)\nDESCRIPTION: This Java snippet gives the encoding and decoding functions for a BidiFlow codec operator, converting between String and ByteString objects. The encode method turns a String into a ByteString, and decode does the reverse. These are key for protocol translation in Akka Streams and require the akka.util.ByteString dependency; inputs and outputs are String and ByteString.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nFunction<String, ByteString> encode = str -> ByteString.fromString(str);\nFunction<ByteString, String> decode = bytes -> bytes.utf8String();\n\n```\n\n----------------------------------------\n\nTITLE: Combining Materialized Values for a Composite Flow in Scala\nDESCRIPTION: This Scala snippet shows the creation of a composite Akka Stream `Flow`. It connects `flow1` and `flow2` using `viaMat`. The `flow2` component has a materialized value of type `Future[OutgoingConnection]`. By using `Keep.right` as the combiner, the resulting `nestedFlow` adopts the materialized value of `flow2`, allowing access to the `OutgoingConnection` future when the flow is materialized.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\n//#mat-combine-2\nval flow1 = Flow[Int].map(_ * 2)\nval flow2 = Flow[Int].mapAsyncUnordered(1)(i => ws.singleRequest(HttpRequest(uri = s\"http://localhost:6001/request/$i\")))\n\nval nestedFlow: Flow[Int, ByteString, Future[OutgoingConnection]] =\n  flow1.viaMat(flow2)(Keep.right).named(\"nestedFlow\")\n//#mat-combine-2\n```\n\n----------------------------------------\n\nTITLE: Verifying Logging Events in Scala\nDESCRIPTION: Demonstrates how to use LoggingTestKit to verify that specific logging events are emitted. This example checks for an INFO level event with a specific message.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [LoggingDocExamples.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/LoggingDocExamples.scala) { #test-logging }\n```\n\n----------------------------------------\n\nTITLE: Using Source.actorRef for Stream Integration - Scala\nDESCRIPTION: This Scala snippet shows how to materialize a Source using Source.actorRef, emitting messages to a stream as they are sent to the actor when there is demand. It demonstrates configuring buffer size and overload strategy, as well as handling stream completion and failure via actor messages. Dependencies include the Akka Streams and Akka Actors libraries. Inputs include messages sent to the materialized actor; outputs are elements emitted in the stream, subject to buffer limits.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/actor-interop.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nSource.actorRef[Int](completionMatcher, failureMatcher, bufferSize, OverflowStrategy.dropHead)\n  .to(Sink.foreach(println))\n  .run()\n```\n\n----------------------------------------\n\nTITLE: Handling Blocking Operations with mapAsync and Dedicated Dispatcher (Java)\nDESCRIPTION: Shows how to handle potentially blocking external service calls within `mapAsync`. The blocking call (`lookupEmailBlocking`) is wrapped in a `CompletableFuture.supplyAsync` and explicitly executed on a dedicated `blockingDispatcher` executor to avoid starving the default Akka dispatchers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\nExecutor blockingDispatcher = system.dispatchers().lookup(\"blocking-dispatcher\");\n\npublic static Optional<String> lookupEmailBlocking(String handle) {\n  // simulate blocking database lookup\n  System.out.println(\"Looking up email blocking for \" + handle);\n  try {\n    Thread.sleep(200);\n  } catch (InterruptedException e) {\n    Thread.currentThread().interrupt();\n  }\n  Optional<String> email;\n  switch (handle) {\n    case \"rolandkuhn\":\n      email = Optional.of(\"rk@example.com\");\n      break;\n    case \"patriknw\":\n      email = Optional.of(\"pn@example.com\");\n      break;\n    case \"konradmalawski\":\n      email = Optional.of(\"km@example.com\");\n      break;\n    default:\n      email = Optional.empty();\n  }\n  System.out.println(\"Email blocking for \" + handle + \": \" + email.orElse(\"None\"));\n  return email;\n}\n\nSource<String, NotUsed> emailAddressesBlocking =\n  authors\n    .mapAsync(parallelism, author -> CompletableFuture.supplyAsync(() -> lookupEmailBlocking(author.handle), blockingDispatcher))\n    .filter(Optional::isPresent)\n    .map(Optional::get);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Side-Channels in Akka Streams - Scala\nDESCRIPTION: This Scala snippet illustrates setting up side-channel communication in Akka Streams using AsyncCallback acquired through getAsyncCallback() in GraphStageLogic. It safely facilitates state changes triggered asynchronously, leveraging thread safety managed by the execution engine.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_23\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #async-side-channel }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Serializer for a Domain Event in Akka Persistence (Scala)\nDESCRIPTION: Implements a custom Akka serializer class for the 'Person' event in Scala, generally by subclassing 'Serializer' or an existing serialization interface. This serializer is responsible for converting between 'Person' instances and binary data for persistence. Dependencies include Akka core and the 'docs.persistence.Person' model; important fields include 'identifier' for correct serializer matching during de/serialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.serialization.Serializer\nimport docs.persistence.Person\nimport java.nio.charset.StandardCharsets\n\nclass PersonSerializer extends Serializer {\n  override def identifier: Int = 1234567\n  override def toBinary(o: AnyRef): Array[Byte] = o match {\n    case p: Person ⇒\n      s\"${p.name},${p.age}\".getBytes(StandardCharsets.UTF_8)\n  }\n  override def includeManifest: Boolean = false\n  override def fromBinary(bytes: Array[Byte], manifest: Option[Class[_]]): AnyRef = {\n    val str = new String(bytes, StandardCharsets.UTF_8)\n    val Array(name, age) = str.split(\",\")\n    Person(name, age.toInt)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Cache Actor with ConsistentHashing in Scala\nDESCRIPTION: Example of a cache actor implementation that defines consistent hashing behavior for message routing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_31\n\nLANGUAGE: scala\nCODE:\n```\nclass Cache extends Actor {\n  var cache = Map[String, String]()\n\n  def receive = {\n    case Entry(key, value) => cache += (key -> value)\n    case Get(key)        => sender() ! cache.get(key)\n    case Evict(key)      => cache -= key\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting Producer and Consumer Controllers in Scala\nDESCRIPTION: Example showing how to connect ProducerController and ConsumerController for establishing the reliable delivery channel.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/reliable-delivery.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nval producer = context.spawn(FibonacciProducer(), \"producer\")\nval producerController = context.spawn(\n  ProducerController[Int](\"producerId\"), \n  \"producerController\")\n\nval consumer = context.spawn(FibonacciConsumer(), \"consumer\")\nval consumerController = context.spawn(\n  ConsumerController[Int](), \n  \"consumerController\")\n\nproducerController ! ProducerController.Start(producer)\nconsumerController ! ConsumerController.Start(consumer)\n\n// connect consumer and producer via ConsumerController\nconsumerController ! ConsumerController.RegisterToProducerController(producerController)\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Akka Streams Components (Java)\nDESCRIPTION: Illustrates how to create composite, reusable `Source` and `Sink` components in Java by grouping internal flows using `named()`. This technique hides internal complexity and promotes modularity in stream graph design.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n//##nested-flow\nfinal Source<Integer, NotUsed> nestedSource =\n    Source.single(0) // An atomic source\n        .map(i -> i + 1) // an atomic processing stage\n        .named(\"nestedSource\"); // wrap it in a reusable composite module\n\nfinal Flow<Integer, Integer, NotUsed> nestedFlow =\n    Flow.of(Integer.class)\n        .filter(i -> i != 0) // an atomic processing stage\n        .map(i -> i - 2) // another atomic processing stage\n        .named(\"nestedFlow\"); // wrap it in a reusable composite module\n\nfinal Sink<Integer, CompletionStage<Integer>> nestedSink =\n    nestedFlow\n        .toMat(Sink.<Integer>head(), Keep.right()) // wire an atomic sink to the nestedFlow\n        .named(\"nestedSink\"); // wrap it the nestedFlow and the atomic Sink in a composite module\n\n// Create a RunnableGraph from the composite modules\nfinal RunnableGraph<CompletionStage<Integer>> runnableGraph =\n    nestedSource.toMat(nestedSink, Keep.right());\n//##nested-flow\n```\n\n----------------------------------------\n\nTITLE: Collecting Stream Elements to List in Akka Streams using Java\nDESCRIPTION: An example demonstrating how to use the Sink.collect operator to collect numbers from a stream into a list. The code creates a source of integers, processes them with a sink using Collectors.toList(), and then prints the collected results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/collect.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nCompletionStage<List<Integer>> result =\n    Source.range(1, 5)\n        .runWith(Sink.collect(Collectors.toList()), materializer);\n\nresult.thenAccept(list -> System.out.println(list));\n// prints: [1, 2, 3, 4, 5]\n```\n\n----------------------------------------\n\nTITLE: Create Transform Sink in Java\nDESCRIPTION: In Java, the snippet illustrates creating a reusable stream component using Flow to convert strings to ByteString before writing them to a file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [QuickStartDocTest.java](/akka-docs/src/test/java/jdocs/stream/QuickStartDocTest.java) { #transform-sink }\n```\n\n----------------------------------------\n\nTITLE: Utilizing BroadcastHub in Akka Streams - Scala\nDESCRIPTION: Scala example utilizing BroadcastHub in Akka Streams, where multiple consumers dynamically receive data from a common producer. The rate matches the slowest consumer, requiring Akka Streams dependency. The demonstration depicts initial producer attachment followed by consumer handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [HubsDocSpec.scala](/akka-docs/src/test/scala/docs/stream/HubsDocSpec.scala) { #broadcast-hub }\n```\n\n----------------------------------------\n\nTITLE: Defining Chat Room Protocol in Akka Typed - Scala\nDESCRIPTION: Defines the protocol for a chat room using Akka Typed in Scala. The protocol manages client sessions and message dissemination. Key types like `ActorRef` and `RoomCommand` are involved. No external dependencies are required.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/IntroSpec.scala) { #chatroom-protocol }\n```\n\n----------------------------------------\n\nTITLE: Implementing MyReadJournal in Scala\nDESCRIPTION: This code snippet shows how to implement a custom ReadJournal plugin in Scala that supports EventsByTagQuery. It demonstrates the implementation of both scaladsl and javadsl interfaces, with the javadsl implementation delegating to the scaladsl one.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\nclass MyReadJournalProvider(system: ExtendedActorSystem, config: Config) extends ReadJournalProvider {\n  override def getReadJournal(): scaladsl.ReadJournal with scaladsl.EventsByTagQuery = \n    new MyScaladslReadJournal(system, config)\n\n  override def getJavadslReadJournal(): javadsl.ReadJournal with javadsl.EventsByTagQuery =\n    new MyJavadslReadJournal(system, config)\n}\n\nclass MyScaladslReadJournal(system: ExtendedActorSystem, config: Config)\n  extends scaladsl.ReadJournal with scaladsl.EventsByTagQuery {\n  \n  private val refreshInterval: FiniteDuration =\n    config.getDuration(\"refresh-interval\", TimeUnit.MILLISECONDS).millis\n  \n  override def eventsByTag(tag: String, offset: Offset): Source[EventEnvelope, NotUsed] =\n    Source.fromGraph(new MyEventsByTagSource(tag, offset))\n}\n\nclass MyJavadslReadJournal(system: ExtendedActorSystem, config: Config)\n  extends javadsl.ReadJournal with javadsl.EventsByTagQuery {\n  \n  private val scalaReadJournal = new MyScaladslReadJournal(system, config)\n\n  override def eventsByTag(tag: String, offset: Offset): javadsl.Source[EventEnvelope, NotUsed] =\n    scalaReadJournal.eventsByTag(tag, offset).asJava\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Actor with Custom Mailbox via Code in Scala\nDESCRIPTION: Scala code showing how to create an actor with a specific mailbox set in code. This approach allows programmatically specifying the mailbox for an actor at creation time.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\n// this actor will have a custom mailbox applied programmatically\nval otherActor = context.actorOf(Props[MyActor]().withMailbox(\"prio-mailbox\"))\n```\n\n----------------------------------------\n\nTITLE: Interacting with an Actor using ask within mapAsync (Java)\nDESCRIPTION: Demonstrates using the `ask` pattern within `mapAsync` to send messages to an Actor (`databaseService`) and receive a response as a `CompletionStage`. This is useful for integrating Actors that encapsulate external services or stateful operations into a stream. A timeout is specified for the ask.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_20\n\nLANGUAGE: java\nCODE:\n```\nActorRef databaseService = system.actorOf(Props.create(DatabaseService.class), \"databaseService\");\nDuration askTimeout = Duration.ofSeconds(5);\n\nRunnableGraph<NotUsed> saveTweets = tweets\n    .filter(t -> t.hashtags().contains(akkaTag))\n    .mapAsync(parallelism, tweet -> \n        Patterns.ask(databaseService, tweet, askTimeout)\n            // we expect the response to be a String\n            .thenApply(response -> (String) response)\n    )\n    .to(Sink.foreach(System.out::println));\n\nsaveTweets.run(system);\n```\n\n----------------------------------------\n\nTITLE: Java Database Integration Example\nDESCRIPTION: Demonstrates creating a Source from a reactive database publisher and processing database rows in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromPublisher.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nimport akka.stream.javadsl.JavaFlowSupport;\nimport java.util.concurrent.Flow;\n\nFlow.Publisher<DatabaseRow> databasePublisher = getDatabasePublisher();\nSource<DatabaseRow, NotUsed> source = JavaFlowSupport.Source.fromPublisher(databasePublisher);\nSource<String, NotUsed> names = source.map(row -> row.name());\n```\n\n----------------------------------------\n\nTITLE: Reply Command Handling with Akka DurableStateBehavior (Java)\nDESCRIPTION: Shows implementation of a command handler with reply semantics in a Java Akka DurableStateBehavior. Relies on Akka Persistence Typed (Java DSL) and showcases event validation, persistence, and synchronous/asynchronous reply sending via ActorRef. Inputs are command objects and sender references, and outputs are Approval/Status replies. Constraints include proper message serialization and ensuring command handlers manage replies correctly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_20\n\nLANGUAGE: java\nCODE:\n```\n@@snip [BlogPostEntityDurableState.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/BlogPostEntityDurableState.java) { #reply-command }\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Handler for Blog Post Entity (Java)\nDESCRIPTION: Implements the event handler for a blog post entity using a switch statement in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\nprivate State eventHandler(State state, Event event) {\n  if (state instanceof EmptyState) {\n    if (event instanceof PostAdded) {\n      return new DraftState(((PostAdded) event).content);\n    } else {\n      throw new IllegalStateException(\"unexpected event [\" + event + \"] in state [\" + state + \"]\");\n    }\n  } else if (state instanceof DraftState) {\n    DraftState draft = (DraftState) state;\n    if (event instanceof BodyChanged) {\n      return new DraftState(\n          new PostContent(draft.content.title, ((BodyChanged) event).newBody));\n    } else if (event instanceof PostPublished) {\n      return new PublishedState(draft.content);\n    } else {\n      throw new IllegalStateException(\"unexpected event [\" + event + \"] in state [\" + state + \"]\");\n    }\n  } else if (state instanceof PublishedState) {\n    PublishedState published = (PublishedState) state;\n    if (event instanceof CommentAdded) {\n      return published; // We could add the comment to the state if we like\n    } else {\n      throw new IllegalStateException(\"unexpected event [\" + event + \"] in state [\" + state + \"]\");\n    }\n  } else {\n    throw new IllegalStateException(\"unexpected state [\" + state + \"]\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Akka Cluster Seed Nodes as Java System Properties\nDESCRIPTION: Specifies seed nodes for an Akka Cluster using Java system properties. This method allows for dynamic configuration of seed nodes at runtime.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\n-Dakka.cluster.seed-nodes.0=akka://ClusterSystem@host1:2552\n-Dakka.cluster.seed-nodes.1=akka://ClusterSystem@host2:2552\n```\n\n----------------------------------------\n\nTITLE: Bubbling Actor Failures Upwards with Terminated Signal (Scala)\nDESCRIPTION: This Scala code example shows how to bubble up child actor failures by handling the Terminated (or ChildFailed) signal. The parent actor watches the child, and upon receiving Terminated/ChildFailed, it may choose to handle the failure or rethrow the original exception to propagate the error further up the hierarchy. Dependencies: Akka Typed (Scala); actors must be manually set to watch their children; no direct inputs beyond signals; output is exception handling and possible propagation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\ncontext.watch(childRef)\nBehaviors.receiveSignal {\n  case (context, ChildFailed(child, cause)) =>\n    throw cause // rethrow to bubble up\n    Behaviors.same\n  case (context, Terminated(child)) =>\n    // handle normal termination\n    Behaviors.same\n}\n```\n\n----------------------------------------\n\nTITLE: Publishing to Distributed PubSub Topic - Akka Typed - Scala\nDESCRIPTION: Provides an example of publishing a message to a distributed pub/sub topic in Scala using the akka.actor.typed.pubsub.Topic.Publish message. Requires the topic ActorRef and the message to be published. Messages are deduplicated on the network and delivered only to nodes with active subscribers. Outputs no acknowledgment; delivery is at-most-once.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\ntopic ! Topic.Publish(\n  message)\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Spawn and Probe Verification - Scala\nDESCRIPTION: Demonstrates the process of spawning actors and verifying responses using TestProbe in a Scala testing environment with ActorTestKit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AsyncTestingExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/AsyncTestingExampleSpec.scala) { #test-spawn }\n```\n\n----------------------------------------\n\nTITLE: Non-Transitive Message Ordering Example in Akka\nDESCRIPTION: Demonstrates how message ordering is not transitive when messages pass through intermediate actors. Shows potential reordering of messages M1 and M2 when routed through actor B.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/message-delivery-reliability.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nActor `A` sends message `M1` to actor `C`\nActor `A` then sends message `M2` to actor `B`\nActor `B` forwards message `M2` to actor `C`\nActor `C` may receive `M1` and `M2` in any order\n```\n\n----------------------------------------\n\nTITLE: Logging from Actor Context in Java\nDESCRIPTION: Shows how to log messages using the ActorContext in Java. The logger is obtained through context.getLog(), which automatically includes the actor's path in the log output.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass MyActor extends AbstractBehavior<String> {\n  private final ActorContext<String> context;\n\n  public MyActor(ActorContext<String> context) {\n    this.context = context;\n  }\n\n  @Override\n  public Behavior<String> onMessage(String msg) {\n    context.getLog().debug(\"Received message {}\", msg);\n    return this;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Restarting Actor with Rate Limits using Akka Typed Supervision (Scala)\nDESCRIPTION: Shows how to configure a rate-limited restart strategy in Scala using `SupervisorStrategy.restartWithLimit`. This example restarts the actor at most 10 times within a 10-second window upon encountering an `IllegalStateException`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nimport scala.concurrent.duration._\n// #restart-limit\nval supervisedBehavior3: Behavior[String] = Behaviors\n  .supervise(behavior)\n  .onFailure[IllegalStateException](SupervisorStrategy.restartWithLimit(maxNrOfRetries = 10, withinTimeRange = 10.seconds))\n// #restart-limit\n\n```\n\n----------------------------------------\n\nTITLE: Observing Mocked Behavior in Actor Tests - Scala\nDESCRIPTION: Shows how to observe and verify interactions with mocked actors in Scala, allowing the testing of components dependent on other actors by using mock behaviors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AsyncTestingExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/AsyncTestingExampleSpec.scala) { #test-observe-mocked-behavior }\n```\n\n----------------------------------------\n\nTITLE: Sending Message to Remote Actor in Scala\nDESCRIPTION: This snippet demonstrates how to send a message to a previously obtained ActorSelection in Scala, showing a straightforward method of interacting with remote actors in a distributed system. It uses an implicit context to deliver the message asynchronously, with 'selection' being the targeted remote actor. The input is a message string, and there is no direct output from this operation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nselection ! \"Pretty awesome feature\"\n```\n\n----------------------------------------\n\nTITLE: Using foldAsync to Asynchronously Aggregate Values in Scala\nDESCRIPTION: Demonstrating how to use foldAsync to accumulate incoming values into a histogram map asynchronously in Scala. The example creates a map that counts occurrences of each value in the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/foldAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.stream.scaladsl._\nimport scala.concurrent.Future\n\nSource(1 to 100)\n  .map(_ => scala.util.Random.nextInt(10))\n  .foldAsync(Map.empty[Int, Int]) { (map, next) =>\n    Future.successful(map + (next -> (map.getOrElse(next, 0) + 1)))\n  }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Splitter and Aggregator in Akka Streams\nDESCRIPTION: Java implementation of the splitter-aggregator pattern that splits messages, processes them in parallel, and then aggregates results. It demonstrates converting strings to integers, doubling each value, and summing groups.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nSource<Integer, NotUsed> intsPerString =\n  sourceOfStrings\n      .map(s -> Arrays.asList(s.split(\" \")))\n      .map(l -> l.stream().map(Integer::valueOf).collect(Collectors.toList()))\n      .flatMapConcat(Source::from)\n      .async();\n\nSource<Integer, NotUsed> sumOfGroups =\n  intsPerString.map(i -> i * 2).grouped(5).map(group -> group.stream().reduce(0, (i, sum) -> i + sum)).async();\n```\n\n----------------------------------------\n\nTITLE: Implementing Durable Queue with EventSourcedProducerQueue in Scala\nDESCRIPTION: Example of implementing a durable producer queue using EventSourcedProducerQueue in an image converter work manager. This ensures messages can be redelivered after crashes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/reliable-delivery.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\nWorkPullingDocExample.scala\n```\n\n----------------------------------------\n\nTITLE: Defining FSM Events in Java\nDESCRIPTION: Java equivalent of the Scala code defining events for the Finite State Machine. Includes SetTarget, Queue, and Flush events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fsm.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ninterface Event {}\n\nclass SetTarget implements Event {\n  public final ActorRef<Batch> ref;\n  public SetTarget(ActorRef<Batch> ref) {\n    this.ref = ref;\n  }\n}\n\nclass Queue implements Event {\n  public final Object obj;\n  public Queue(Object obj) {\n    this.obj = obj;\n  }\n}\n\nenum Flush implements Event {\n  INSTANCE\n}\n\nclass Batch {\n  public final List<Object> obj;\n  public Batch(List<Object> obj) {\n    this.obj = obj;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Supervisor Strategy in Java\nDESCRIPTION: Java implementation of a one-for-one supervision strategy that defines how different exceptions should be handled with specific directives. Configures maximum retries and time window for restart attempts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nprivate static SupervisorStrategy strategy =\n    new OneForOneStrategy(\n        10,\n        Duration.create(1, TimeUnit.MINUTES),\n        DeciderBuilder\n            .match(ArithmeticException.class, e -> resume())\n            .match(NullPointerException.class, e -> restart())\n            .match(IllegalArgumentException.class, e -> stop())\n            .match(Exception.class, e -> escalate())\n            .build());\n```\n\n----------------------------------------\n\nTITLE: Implementing Sink.reduce in Scala\nDESCRIPTION: Example showing how to use Sink.reduce operator in Scala to perform reduction operations on stream elements. The reduction function processes two elements at a time, with results feeding into subsequent invocations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/reduce.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#reduce-operator-example\n```\n\n----------------------------------------\n\nTITLE: Implementing Effects in Akka Persistent Actors - Scala\nDESCRIPTION: Explains how to utilize various effects in Scala for Akka persistent actors, enabling operations like persisting events and handling command outcomes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BasicPersistentBehaviorCompileOnly.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/BasicPersistentBehaviorCompileOnly.scala) { #effects }\n```\n\n----------------------------------------\n\nTITLE: Using ORSet (Observed-Remove Set) in Akka Distributed Data (Java)\nDESCRIPTION: Shows how to use Akka's ORSet in Java for distributed sets with add and remove semantics, supporting concurrent modifications. Relies on Akka Distributed Data for Java and a running cluster with causal delivery. Input is typically add or remove requests for elements, and output is the observed-removed set state replicated throughout the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\n// Java Example: ORSet usage\nORSetKey<String> setKey = ORSetKey.create(\"exampleORSet\");\ndistributedData.tell(new Update<>(setKey, ORSet.create(), WriteLocal.instance(), curr -> curr.add(node, \"foo\")), self());\ndistributedData.tell(new Update<>(setKey, ORSet.create(), WriteLocal.instance(), curr -> curr.remove(node, \"foo\")), self());\n\n```\n\n----------------------------------------\n\nTITLE: Defining an Echo Manager Actor in Java\nDESCRIPTION: This Java snippet defines an `EchoManager` actor that listens for incoming TCP connections. Upon receiving a `Tcp.Connected` message, it creates and registers a `SimpleEchoHandler` actor for the connection, ensuring the `keepOpenOnPeerClosed` option is set to true. This allows the handler to finish sending data even if the client half-closes the connection.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic class EchoManager extends AbstractActor {\n\n  private final LoggingAdapter log = Logging.getLogger(getContext().system(), this);\n  private final ActorRef listener;\n\n  public EchoManager(ActorRef listener) {\n    this.listener = listener;\n\n    // sign death pact: this actor stops when the server stops\n    getContext().watch(listener);\n  }\n\n  public static Props props(ActorRef listener) {\n    return Props.create(EchoManager.class, listener);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Tcp.Connected.class,\n            conn -> {\n              log.info(\"Remote address {} connected\", conn.remoteAddress());\n              final ActorRef handler = getContext().actorOf(SimpleEchoHandler.props(getSender()));\n              getSender().tell(TcpMessage.register(handler, true), getSelf());\n            })\n        .matchEquals(\n            Stop.instance(),\n            stop -> {\n              log.info(\"Stopping echo manager\");\n              listener.tell(TcpMessage.unbind(), getSelf());\n              getContext().become(stopping());\n            })\n        .build();\n  }\n\n  private Receive stopping() {\n    return receiveBuilder()\n        .matchEquals(\n            TcpMessage.unbound(),\n            unbound -> {\n              log.info(\"Echo manager unbound\");\n              getContext().stop(getSelf());\n            })\n        .match(\n            CommandFailed.class,\n            failed -> {\n              // listener already unbound\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n\n  public static final class Stop { // this is the message that the main Actor sends to this actor\n    // to initiate the stopping sequence\n    public static final Stop instance() {\n      return INSTANCE;\n    }\n\n    private static final Stop INSTANCE = new Stop();\n\n    private Stop() {}\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Future-based Circuit Breaker Usage in Scala\nDESCRIPTION: Example of using circuit breaker with Future-based API in Scala, demonstrating async call protection pattern.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\ndef receive = {\n  case \"is-open\" =>\n    sender() ! breaker.isOpen()\n  case \"block-for-sec\" =>\n    breaker.withCircuitBreaker(Future {\n      Thread.sleep(2000)\n      \"We're done!\"\n    }) pipeTo sender()\n}\n```\n\n----------------------------------------\n\nTITLE: Binding Stream Lifecycle to ActorSystem in Scala\nDESCRIPTION: This Scala code snippet shows how a materializer can be passed into an actor to bind its lifecycle to the entire akka.actor.ActorSystem rather than a single actor, useful for long-running streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #materializer-from-system-in-actor }\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Actor in Scala\nDESCRIPTION: Example of implementing a basic Akka Actor in Scala by extending the Actor trait and implementing the receive method to handle different types of messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#imports1 #my-actor\nimport akka.actor.Actor\nimport akka.actor.Props\nimport akka.event.Logging\n\nclass MyActor extends Actor {\n  val log = Logging(context.system, this)\n\n  def receive = {\n    case \"test\" =>\n      log.info(\"received test\")\n    case _ =>\n      log.info(\"received unknown message\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing a Flow Component with Flow.fromSinkAndSource in Java\nDESCRIPTION: This Java snippet shows how to use Flow.fromSinkAndSource for testing a component that accepts a Flow. It uses TestPublisher and TestSubscriber from the Akka Streams TestKit to control and assert incoming and outgoing elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/fromSinkAndSource.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nTestPublisher.Probe<Integer> publisherProbe =\n    TestPublisher.probe(0, system);\nTestSubscriber.Probe<Integer> subscriberProbe =\n    TestSubscriber.probe(system);\n\nFlow<Integer, Integer, NotUsed> flow =\n    Flow.fromSinkAndSource(Sink.fromSubscriber(subscriberProbe), Source.fromPublisher(publisherProbe));\n\n// test code using flow and the probes\n```\n\n----------------------------------------\n\nTITLE: Handling Validation Errors with Generic Response Wrapper in Akka Typed (Scala)\nDESCRIPTION: This snippet demonstrates how to handle validation errors using the StatusReply generic response wrapper in Akka Typed when asking from outside the actor system in Scala. It shows how to construct and handle error responses.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [InteractionPatternsSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/InteractionPatternsSpec.scala) { #standalone-ask-with-status-fail-future }\n```\n\n----------------------------------------\n\nTITLE: Implementing Generic Reduce-by-Key in Akka Streams\nDESCRIPTION: A generalized version of the reduce-by-key pattern that can be applied to various aggregation tasks. It abstracts the group key function, mapping function, and reduction operation for flexibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_26\n\nLANGUAGE: Scala\nCODE:\n```\ndef reduceByKey[In, K, Out](\n  maximumGroupSize: Int,\n  groupKey: (In) => K,\n  map: (In) => Out,\n  reduce: (Out, Out) => Out\n): Flow[In, (K, Out), NotUsed] =\n  Flow[In]\n    .groupBy(maximumGroupSize, groupKey)\n    .map(element => (groupKey(element), map(element)))\n    .reduce((l, r) => (l._1, reduce(l._2, r._2)))\n    .mergeSubstreams\n```\n\n----------------------------------------\n\nTITLE: Defining a Codec Operator as a Bidirectional Flow in Akka Streams (Scala)\nDESCRIPTION: This snippet demonstrates how to construct a codec operator as a BidiFlow in Scala, enabling bidirectional conversion between domain objects and ByteString. The BidiFlow consists of two unidirectional Flows for encoding and decoding messages, and they are composed as a single subgraph. Dependencies include akka.stream and the ByteString class; key inputs are generic elements and ByteStrings, with outputs as ByteStrings and decoded elements respectively.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\nval codec = BidiFlow.fromFunctions[String, ByteString, ByteString, String](\n  str => ByteString(str),\n  bytes => bytes.utf8String\n)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Entity-based Message Processing with mapAsyncPartitioned in Java\nDESCRIPTION: Java implementation showing how to process messages for different entities concurrently while ensuring only one message per entity is processed at a time. The example demonstrates processing events with partitioning based on entity ID.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapAsyncPartitioned.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<String, NotUsed> source = messageSource\n    .mapAsyncPartitioned(\n        10,\n        1,\n        event -> event.entityId,\n        (event, entityId) -> {\n          System.out.println(\"Processing event \" + event + \" from partition \" + entityId);\n          return CompletableFuture.supplyAsync(\n              () -> {\n                try {\n                  // simulate some processing delay\n                  Thread.sleep(ThreadLocalRandom.current().nextInt(250));\n                  System.out.println(\n                      \"Completed processing \" + entityId + \"-\" + event.sequence);\n                  return entityId + \"-\" + event.sequence;\n                } catch (InterruptedException e) {\n                  throw new RuntimeException(e);\n                }\n              });\n        })\n    .map(\n        result -> {\n          System.out.println(\"`mapAsyncPartitioned` emitted \" + result);\n          return result;\n        });\n```\n\n----------------------------------------\n\nTITLE: Using PartialFunction for Message Handling in Scala\nDESCRIPTION: Using PartialFunction with receiveMessagePartial when exhaustive pattern matching is inconvenient. This is an alternative to the recommended sealed trait approach.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\nBehaviors.receiveMessagePartial {\n  case Increment(delta) =>\n    value += delta\n    Behaviors.same\n  case GetValue(replyTo) =>\n    replyTo ! value\n    Behaviors.same\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy with Async Boundary in Java\nDESCRIPTION: Shows groupBy implementation with async boundaries in Java for parallel processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/groupBy.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"))\n    .groupBy(3, word -> word.length())\n    .map(String::toUpperCase).async()\n    .mergeSubstreams()\n    .runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Partitioning Integers into Even and Odd Streams in Scala\nDESCRIPTION: This snippet demonstrates how to use the Partition operator to split a Source of integers into two Sinks: one for even numbers and another for odd numbers. It showcases the usage of the partition method and how to connect the resulting substreams to their respective sinks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Partition.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval source = Source(1 to 10)\nval numOutputs = 2\n\nval (even, odd) = source.partition(_ % 2 == 0, outputStreams = numOutputs)\n\nval sink1 = Sink.foreach[Int](x => println(s\"Even number: $x\"))\nval sink2 = Sink.foreach[Int](x => println(s\"Odd number: $x\"))\n\neven.runWith(sink1)\nodd.runWith(sink2)\n```\n\n----------------------------------------\n\nTITLE: Interacting with an Actor using ask within mapAsync (Scala)\nDESCRIPTION: Demonstrates using the `ask` pattern within `mapAsync` to send messages to an Actor (`databaseService`) and receive a response as a `Future`. This is useful for integrating Actors that encapsulate external services or stateful operations into a stream. A timeout is specified for the ask.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\nimplicit val askTimeout: Timeout = Timeout(5.seconds)\nval databaseService: ActorRef = system.actorOf(Props[DatabaseService](), \"databaseService\")\n\nval saveTweets: RunnableGraph[NotUsed] = tweets\n  .filter(_.hashtags.contains(akkaTag))\n  .mapAsync(4)(tweet => databaseService.ask(tweet).mapTo[String])\n  .to(Sink.foreach(println))\n\nsaveTweets.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Recovery in Akka Streams - Scala\nDESCRIPTION: Demonstrates how to use the recover operator to handle upstream failures gracefully in Scala. The example shows processing a stream that fails and recovering with a final element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recover.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(1 to 4).map(n => \n  if (n < 3) n.toString\n  else throw new RuntimeException(\"Boom!\"))\n\nval recoverFlow = source.recover {\n  case _: RuntimeException => \"fallback\"\n}\n\nrecoverFlow.runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Using flatMapConcat in Scala\nDESCRIPTION: Demonstrates the `flatMapConcat` operator in Akka Streams. It transforms each element of the input stream (integers 1 and 2) into a new Source (containing the integer repeated three times). These generated Sources are then concatenated sequentially into the output stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\n//#flatMapConcat\nSource(1 to 2)\n  .flatMapConcat(i => Source(List.fill(3)(i)))\n  .runForeach(println)\n//#flatMapConcat\n\n```\n\n----------------------------------------\n\nTITLE: Auction State Implementation in Scala\nDESCRIPTION: State class implementation representing the CRDT for the auction system, handling bid comparisons and state updates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing-auction.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nfinal case class AuctionState(\n  isOpen: Boolean,\n  highestBid: Bid,\n  highestCounterOffer: Int\n) {\n  def applyEvent(event: Event): AuctionState = {\n    event match {\n      case BidRegistered(offer, timestamp, fromReplica) =>\n        val newBid = Bid(offer, timestamp, fromReplica)\n        if (newBid.isHigherThan(highestBid)) {\n          copy(highestBid = newBid, highestCounterOffer = highestBid.offer)\n        } else {\n          copy(highestCounterOffer = math.max(highestCounterOffer, offer))\n        }\n      case AuctionFinished(_, _) =>\n        copy(isOpen = false)\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Source.actorRef for Stream Integration - Java\nDESCRIPTION: This Java snippet demonstrates how to materialize a Source using Source.actorRef, connecting an actor to a stream and emitting messages based on downstream demand. It covers buffer management and handling stream completion or failure through custom message matchers. Dependencies include Akka Streams and Akka Actors for Java. Inputs are messages sent to the materialized actor reference, and outputs are stream elements subject to buffer size and overflow strategy constraints.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/actor-interop.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nSource.<Integer>actorRef(\n    completionMatcher,\n    failureMatcher,\n    bufferSize,\n    OverflowStrategy.dropHead)\n  .to(Sink.foreach(System.out::println))\n  .run(materializer);\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.fromIterator in Scala\nDESCRIPTION: Demonstrates creating a Source from an Iterator in Scala that generates a stream of integers. The example shows how to create an infinite stream of integers using the fromIterator operator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromIterator.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#from-iterator\n```\n\n----------------------------------------\n\nTITLE: Dropping Elements with Conflate in Akka Streams (Scala)\nDESCRIPTION: Uses the conflate operator to drop elements when the upstream is faster than the downstream. It keeps only the latest element in case of backpressure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_39\n\nLANGUAGE: Scala\nCODE:\n```\nSource.cycle(() => List(1, 2, 3).iterator)\n  .throttle(3, 1.second)\n  .conflate((a, b) => b)\n  .throttle(1, 1.second)\n  .runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Implementing Echo Handler with NACK-Based Write Back-Pressure in Java\nDESCRIPTION: A Java implementation of an EchoHandler that uses NACK-based write back-pressure. It handles incoming TCP data and writes it back while managing potential command failures.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nclass EchoHandler extends AbstractActor {\n  private final ActorRef connection;\n  private final LoggingAdapter log = Logging.getLogger(getContext().system(), this);\n\n  public EchoHandler(ActorRef connection) {\n    this.connection = connection;\n    // sign death pact: this actor terminates when connection breaks\n    getContext().watch(connection);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(Tcp.Received.class, msg -> {\n      final ByteString data = msg.data();\n      connection.tell(TcpMessage.write(data, Ack.getInstance()), getSelf());\n      getContext().become(buffering(new ArrayList<ByteString>()));\n    }).match(Tcp.ConnectionClosed.class, msg -> {\n      getContext().stop(getSelf());\n    }).build();\n  }\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Akka Stream Attribute Inheritance in Java\nDESCRIPTION: This Java snippet demonstrates attribute inheritance in Akka Streams using `addAttributes`. It sets an `inputBuffer` attribute on `nestedSink`. The `nestedFlow` inherits this attribute, but the `map` operation within it explicitly overrides the inherited attribute with its own `inputBuffer` setting. The `nestedSource` uses the materializer's default attributes. This exemplifies how the attribute closest to an operator takes precedence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_21\n\nLANGUAGE: java\nCODE:\n```\n// #attributes-inheritance\nSource<Integer, NotUsed> nestedSource =\n    Source.range(1, 10).map(i -> i * 2).named(\"nestedSource\"); // Uses default attributes\n\nFlow<Integer, Integer, NotUsed> nestedFlow =\n    Flow.of(Integer.class)\n        .filter(i -> i % 2 == 0)\n        .map(i -> i / 2)\n        .addAttributes(Attributes.inputBuffer(1, 1)); // overrides attribute\nSink<Integer, NotUsed> nestedSink =\n    nestedFlow\n        .to(Sink.ignore())\n        .addAttributes(Attributes.inputBuffer(3, 3)); // sets attribute\n\nRunnableGraph<NotUsed> runnableGraph2 =\n    nestedSource.to(nestedSink); // nestedSink defines attributes\n// #attributes-inheritance\n```\n\n----------------------------------------\n\nTITLE: Implementing MyReadJournal in Java\nDESCRIPTION: This code snippet shows how to implement a custom ReadJournal plugin in Java that supports EventsByTagQuery. It implements both the Java and Scala APIs, with separate implementation classes for each.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\npublic class MyReadJournalProvider extends ReadJournalProvider {\n  private final ExtendedActorSystem system;\n  private final Config config;\n\n  public MyReadJournalProvider(ExtendedActorSystem system, Config config) {\n    this.system = system;\n    this.config = config;\n  }\n\n  @Override\n  public MyJavadslReadJournal getJavadslReadJournal() {\n    return new MyJavadslReadJournal(system, config);\n  }\n\n  @Override\n  public MyScaladslReadJournal getScaladslReadJournal() {\n    return new MyScaladslReadJournal(system, config);\n  }\n}\n\npublic class MyScaladslReadJournal extends AbstractReadJournal implements scaladsl.EventsByTagQuery {\n  private final Duration refreshInterval;\n\n  public MyScaladslReadJournal(ExtendedActorSystem system, Config config) {\n    refreshInterval = config.getDuration(\"refresh-interval\", TimeUnit.MILLISECONDS).millis();\n  }\n\n  @Override\n  public akka.stream.scaladsl.Source<EventEnvelope, NotUsed> eventsByTag(String tag, Offset offset) {\n    return akka.stream.scaladsl.Source.fromGraph(new MyEventsByTagSource(tag, offset));\n  }\n}\n\npublic class MyJavadslReadJournal extends AbstractReadJournal implements javadsl.EventsByTagQuery {\n  private final MyScaladslReadJournal scalaReadJournal;\n\n  public MyJavadslReadJournal(ExtendedActorSystem system, Config config) {\n    scalaReadJournal = new MyScaladslReadJournal(system, config);\n  }\n\n  @Override\n  public javadsl.Source<EventEnvelope, NotUsed> eventsByTag(String tag, Offset offset) {\n    return scalaReadJournal.eventsByTag(tag, offset).asJava();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipelining in Akka Streams with Scala\nDESCRIPTION: This code snippet demonstrates how to implement a pipelined approach in Akka Streams using Scala. It mimics Roland's method of using two frying pans asymmetrically, reflecting a sequence where one stream operator's output serves as the input to another. Dependencies include Akka Streams setup and configuration. The inputs represent stages of pancake cooking, and outputs are sequentially processed results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-parallelism.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowParallelismDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowParallelismDocSpec.scala) { #pipelining }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Stream Splitter (Akka Streams Scala)\nDESCRIPTION: Shows how to use Akka Stream operators to split composite string messages into sub-messages (e.g., splitting a '-' delimited string into numbers), following the Splitter integration pattern. Useful for message parsing, ETL, or workflow management scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeSplitter.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeSplitter.scala) { #Simple-Split }\n```\n\n----------------------------------------\n\nTITLE: Implementing Fibonacci Producer Actor in Scala\nDESCRIPTION: Example of a Fibonacci number generator producer actor that sends numbers through a ProducerController for reliable delivery.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/reliable-delivery.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.typed.ActorRef\nimport akka.actor.typed.Behavior\nimport akka.actor.typed.delivery.ProducerController\nimport akka.actor.typed.scaladsl.Behaviors\n\nobject FibonacciProducer {\n  sealed trait Command\n  private case class WrappedRequestNext(next: ProducerController.RequestNext[Int]) extends Command\n  private case class InitialState(sequence: List[Int])\n\n  def apply(): Behavior[Command] = {\n    Behaviors.setup { context =>\n      val requestNextAdapter: ActorRef[ProducerController.RequestNext[Int]] =\n        context.messageAdapter(WrappedRequestNext.apply)\n\n      Behaviors.receiveMessage {\n        case WrappedRequestNext(next) =>\n          context.log.info(\"Producer sending {}\", next.currentSeqNr)\n          val n1 = next.askNextTo.sequence.drop(1).headOption.getOrElse(1)\n          val n2 = next.askNextTo.sequence.headOption.getOrElse(0)\n          val n3 = n1 + n2\n          next.sendNextTo ! n3\n          apply(InitialState(n3 :: next.askNextTo.sequence))\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an ActorRef Source in Akka Streams Typed - Scala\nDESCRIPTION: This Scala example demonstrates how to use ActorSource.actorRef to materialize an ActorRef[T] that emits incoming messages into an Akka Stream. The example shows setting up the Play completion and failure matchers, buffer size, and overflow strategy. It assumes akka-stream-typed is on the classpath and a functioning ActorSystem is in scope. The input is an actor message; output is tuples or elements as emitted by the stream. Only messages of the correct type are accepted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorSource/actorRef.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n// ActorSourceSinkExample.scala\\n// #actor-source-ref\\nval source: Source[Message, ActorRef[Message]] =\\n  ActorSource.actorRef[Message](\\n    completionMatcher = {\\n      case Done =>\\n    },\\n    failureMatcher = PartialFunction.empty,\\n    bufferSize = 8,\\n    overflowStrategy = OverflowStrategy.dropHead\\n  )\\n// #actor-source-ref\n```\n\n----------------------------------------\n\nTITLE: Fire and Forget Implementation in Scala and Java\nDESCRIPTION: Demonstrates how to send a fire and forget message to an actor without waiting for a response. The code shows sending a CoffeeOrder to a barista actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n  val coffeeOrderActor: ActorRef[Command] = ???\n  coffeeOrderActor ! CoffeeOrder(\"Americano\", giveToCustomer)\n```\n\nLANGUAGE: java\nCODE:\n```\n  ActorRef<Command> coffeeOrderActor = ???;\n  coffeeOrderActor.tell(new CoffeeOrder(\"Americano\", giveToCustomer));\n```\n\n----------------------------------------\n\nTITLE: Using collectType to Filter and Transform Messages in Scala\nDESCRIPTION: Demonstrates filtering Ping messages with non-zero IDs from a stream of Messages and converting them to Pong objects.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collectType.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n#collectType\n```\n\n----------------------------------------\n\nTITLE: Implementing KillSwitch in Akka Streams - Scala\nDESCRIPTION: This Scala snippet demonstrates the implementation of KillSwitch using Akka Streams, a feature that allows external control of stream completion. Dependencies include Akka Streams library with a specific focus on the akka.stream package. The snippet includes commands for shutting down or aborting streams, highlighting control operations for stream lifecycle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [KillSwitch.scala](/akka-stream/src/main/scala/akka/stream/KillSwitch.scala) { #kill-switch }\n```\n\n----------------------------------------\n\nTITLE: Using Timers in Akka Stream Operators - Java\nDESCRIPTION: This Java snippet showcases scheduling and handling timers in Akka GraphStages using TimerGraphStageLogic as a base. Timer functionalities include scheduleOnce and scheduleAtFixedRate, with safety ensured by deferring timer operations to preStart.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [GraphStageDocTest.java](/akka-docs/src/test/java/jdocs/stream/GraphStageDocTest.java) { #timed }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Decoupled Buffer with GraphStage - Akka Streams - Java\nDESCRIPTION: Provides a Java variant of the decoupled buffer example using Akka Streams' GraphStage base. This demonstrates how to detach upstream and downstream rates using explicit buffer management. It requires Akka Streams for Java and utilizes callbacks such as onPush/onPull to manage internal buffer state. Inputs: arbitrary upstream elements; Outputs: elements when downstream is ready; works independently of demand from either end until the buffer's capacity is reached. Dependencies: Akka Streams Java API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_28\n\nLANGUAGE: Java\nCODE:\n```\n/*\n * Code demo for a buffer implemented with GraphStage in Akka Streams (Java API)\n * Pulls upstream on initialization, buffers up to capacity, decouples demand from downstream\n */\n\n// --- Code as referenced in docs ---\n// @@snip [GraphStageDocTest.java](/akka-docs/src/test/java/jdocs/stream/GraphStageDocTest.java) { #detached }\n\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Test Probes\nDESCRIPTION: Shows how to use multiple TestProbe actors to verify message flows between actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\n#imports-test-probe\\n#my-double-echo\\n#test-probe\n```\n\n----------------------------------------\n\nTITLE: Defining HelloWorld Actor in Akka Typed (Java)\nDESCRIPTION: Example of an Akka Typed actor extending the AbstractBehavior class to create a simple HelloWorld actor in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/from-classic.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nclass HelloWorld extends AbstractBehavior<String> {\n\n  public static Behavior<String> create() {\n    return Behaviors.setup(HelloWorld::new);\n  }\n\n  private HelloWorld(ActorContext<String> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder().onMessageEquals(\"hello\", () -> {\n      System.out.println(\"Hello World!\");\n      return this;\n    }).build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing device actor with read protocol in Akka Typed\nDESCRIPTION: Implementation of a device actor that handles temperature reading queries. The actor maintains state of the last temperature reading and responds to queries with the same ID parameter that was in the request.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n#device-with-read\n```\n\nLANGUAGE: Java\nCODE:\n```\n#device-with-read\n```\n\n----------------------------------------\n\nTITLE: Using Collect Operator in Scala\nDESCRIPTION: Demonstrates using the collect operator to filter Ping messages with non-zero IDs and transform them to Pong objects.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collect.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nmessageSource.collect {\n  case Ping(id) if id != 0 => Pong(id)\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency\nDESCRIPTION: This snippet demonstrates how to add Akka Streams as a dependency in sbt, Maven, or Gradle. By including the Akka Streams module in a project, users can leverage its stream processing capabilities. Dependencies are defined with group, artifact, and version placeholders to align with project requirements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt,Maven,Gradle\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=\"com.typesafe.akka\"\n  artifact=\"akka-stream_$scala.binary.version$\"\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Counting Characters per Line using splitAfter in Java\nDESCRIPTION: Provides a practical example of using `splitAfter` to process a stream of characters representing text lines. It splits the stream after each newline character, counts the characters in each resulting substream (line), and prints the count.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n//#wordCount\nfinal String text =\n    \"This is the first line.\\n\" + \"And this is the second line.\\n\" + \"Here is the third line.\";\n\nSource.from(Arrays.asList(text.split(\"\")))\n    .map(c -> c.charAt(0))\n    .splitAfter(c -> c == '\\n')\n    .filter(c -> c != '\\n')\n    .map(\n        c -> {\n          System.out.print(c);\n          return 1;\n        })\n    .reduce((i, acc) -> i + acc)\n    .map(\n        i -> {\n          System.out.println();\n          return i;\n        })\n    .concatSubstreams()\n    .runForeach(System.out::println, system);\n//#wordCount\n```\n\n----------------------------------------\n\nTITLE: Backend Worker Initialization in Akka\nDESCRIPTION: Implements the backend worker that processes transformation requests in an Akka cluster. The worker subscribes to cluster events to dynamically register with frontend nodes. Key dependencies include Akka actor and cluster libraries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [TransformationBackend.java](/akka-docs/src/test/java/jdocs/cluster/TransformationBackend.java) { #backend }\n```\n\n----------------------------------------\n\nTITLE: Backend Worker Initialization in Akka\nDESCRIPTION: Implements the backend worker that processes transformation requests in an Akka cluster. The worker subscribes to cluster events to dynamically register with frontend nodes. Key dependencies include Akka actor and cluster libraries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_17\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [TransformationBackend.scala](/akka-docs/src/test/scala/docs/cluster/TransformationBackend.scala) { #backend }\n```\n\n----------------------------------------\n\nTITLE: Concatenating Streams using Concat Operator in Scala\nDESCRIPTION: Demonstrates how to use the concat operator to combine two streams sequentially in Scala. After the first stream completes, elements from the second stream will be emitted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/concat.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFlowConcatSpec.scala\n```\n\n----------------------------------------\n\nTITLE: Implementing Reduce-by-Key for Word Count in Akka Streams\nDESCRIPTION: Java implementation of the word count example using reduce-by-key in Akka Streams. It groups words, counts occurrences, and merges results back into a single stream while limiting the maximum number of distinct words.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\nfinal Integer MAXIMUM_DISTINCT_WORDS = 1000;\n\nSource.from(Arrays.asList(\"abc\", \"def\", \"abc\", \"hij\", \"def\"))\n    .groupBy(MAXIMUM_DISTINCT_WORDS, i -> i)\n    .map(word -> new Pair<>(word, 1))\n    .reduce((pair1, pair2) -> new Pair<>(pair1.first(), pair1.second() + pair2.second()))\n    .mergeSubstreams()\n```\n\n----------------------------------------\n\nTITLE: ActorRef with Backpressure Sink in Scala\nDESCRIPTION: Implementation of a sink that sends stream elements to an actor with proper backpressure handling. Includes actor implementation that processes elements and sends acknowledgements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/actor-interop.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nclass Ack\n\nclass ExampleActor extends Actor {\n  override def receive = {\n    case \"init\" =>\n      sender() ! \"ack\"\n    case payload: String =>\n      sender() ! new Ack\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Prematerializing Akka Stream Source in Java\nDESCRIPTION: This Java code snippet illustrates the process of prematerializing an Akka Stream source to acquire its materialized value before hookup. It relies on akka.stream.Materializer passed into the method scope.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #source-prematerialization }\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Actor Typed Dependency - sbt, Maven, Gradle\nDESCRIPTION: This snippet defines how to add the Akka Typed Actor dependency to a project using sbt, Maven, or Gradle. It specifies the group, artifact, and version coordinates, which must be updated to match the desired Akka version. Dependencies vary with the Scala binary version and project build tool. The snippet is boilerplate required for using any Typed Actor features; ensure compatibility with your Scala version.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-discovery.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nbomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\\nsymbol1=AkkaVersion\\nvalue1=\\\"$akka.version$\\\"\\ngroup=com.typesafe.akka\\nartifact=akka-actor-typed_$scala.binary.version$\\nversion=AkkaVersion\n```\n\n----------------------------------------\n\nTITLE: Sending Replies After Persistence in Akka DurableStateBehavior (Java)\nDESCRIPTION: Shows the pattern of sending replies to an ActorRef after events have been persisted, using a side effect in Java DurableStateBehavior. Depends on Akka Persistence Typed (Java DSL). Inputs are events and reference to the reply receiver; output is a reply message indicating success or failure. It is especially useful for providing confirmation post-state change, following the resilient actor pattern.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_22\n\nLANGUAGE: java\nCODE:\n```\n@@snip [BlogPostEntityDurableState.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/BlogPostEntityDurableState.java) { #reply }\n```\n\n----------------------------------------\n\nTITLE: Saving Snapshots in Scala\nDESCRIPTION: Demonstrates how to save snapshots of actor state and handle success/failure responses\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_26\n\nLANGUAGE: scala\nCODE:\n```\nvar state: ExampleState = _\n\nvar numEvents = 0\nif ((numEvents % 100) == 0) {\n  saveSnapshot(state)\n}\n\noverride def receiveCommand: Receive = {\n  case \"print\" => // ...\n  case SaveSnapshotSuccess(metadata) => // ...\n  case SaveSnapshotFailure(metadata, reason) => // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Recovery Status within a Persistent Actor (Scala)\nDESCRIPTION: Demonstrates how a PersistentActor can determine if it is currently undergoing recovery by calling the `isRecoveryRunning` method. It also shows how to access the sequence number of the last replayed event using `recoveryRunningNum`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\ndef receiveRecover: Receive = {\n  case evt: Evt                               => updateState(evt)\n  case SnapshotOffer(metadata, snapshot: State) => state = snapshot\n  case RecoveryCompleted =>\n    log.info(\n      \"MyPersistentActor [{}] recovery completed. Current state: [{}]\",\n      persistenceId,\n      state)\n}\n\ndef receiveCommand: Receive = {\n  case GetState => sender() ! state\n  case \"snap\"   => saveSnapshot(state)\n  case \"print\" =>\n    log.info(\n      \"MyPersistentActor [{}] received [print] command. Current state: [{}]\",\n      persistenceId,\n      state)\n    if (recoveryRunning) {\n      log.info(\"RECOVERY RUNNING: \" + recoveryRunningNum)\n    } else {\n      log.info(\"RECOVERY COMPLETED\")\n    }\n\n  case SaveUser(user) =>\n    persist(UserSaved(user)) { evt =>\n      updateState(evt)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Flow.lazyFutureFlow in Scala\nDESCRIPTION: Creates a Flow that defers creation and materialization until the first element arrives. The actual Flow is created from a Future[Flow] when the first element comes from upstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/lazyFutureFlow.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFlow.lazyFutureFlow[I,O,M](create:()=>scala.concurrent.Future[akka.stream.scaladsl.Flow[I,O,M]]):akka.stream.scaladsl.Flow[I,O,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: Implementing Stash in an Akka Typed Actor (Java)\nDESCRIPTION: Demonstrates using `StashBuffer` within a Java Akka Typed actor (`DataAccess`). It shows how to stash incoming messages (`Get`, `Save`) while loading initial data from a database or during a save operation, ensuring sequential processing. The example uses `Behaviors.withStash` to create a stash buffer with a defined capacity and `buffer.unstashAll` to process buffered messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/stash.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// #import\nimport akka.actor.typed.ActorRef;\nimport akka.actor.typed.Behavior;\nimport akka.actor.typed.StashBuffer;\nimport akka.actor.typed.javadsl.Behaviors;\nimport akka.actor.typed.javadsl.ActorContext;\n// #import\n\n// #db\ninterface DB {\n  void save(String id, String value);\n\n  String load(String id);\n}\n// #db\n\n// #stashing\npublic abstract class DataAccess {\n\n  // commands\n  public interface Command {}\n\n  public static final class Save implements Command {\n    public final String value;\n    public final ActorRef<Void> replyTo;\n\n    public Save(String value, ActorRef<Void> replyTo) {\n      this.value = value;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class Get implements Command {\n    public final ActorRef<String> replyTo;\n\n    public Get(ActorRef<String> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  private static final class InitialState implements Command {\n    final String value;\n\n    private InitialState(String value) {\n      this.value = value;\n    }\n  }\n\n  private static final class SaveSuccess implements Command {\n    private SaveSuccess() {}\n\n    private static final SaveSuccess INSTANCE = new SaveSuccess();\n  }\n\n  private enum DBError implements Command {\n    INSTANCE\n  }\n\n  // query\n  private final ActorContext<Command> context;\n  private final StashBuffer<Command> buffer;\n  private final String id;\n  private final DB db;\n\n  private DataAccess(ActorContext<Command> context, StashBuffer<Command> buffer, String id, DB db) {\n    this.context = context;\n    this.buffer = buffer;\n    this.id = id;\n    this.db = db;\n  }\n\n  public static Behavior<Command> create(String id, DB db) {\n    return Behaviors.setup(\n        context -> {\n          // create a stash buffer with capacity 100\n          return Behaviors.withStash(\n              100,\n              buffer -> {\n                // load initial state from db\n                // we can use context.pipeToSelf for this but this is an example\n                String initialValue = db.load(id);\n                context.getLog().info(\"Loaded initial value [{}] for id [{}]\", initialValue, id);\n                context.getSelf().tell(new InitialState(initialValue));\n\n                // Initial behavior is waiting for InitialState\n                // All other messages are stashed\n                return new DataAccess(context, buffer, id, db).initializing();\n              });\n        });\n  }\n\n  private Behavior<Command> initializing() {\n    return Behaviors.receive(Command.class)\n        .onMessage(\n            InitialState.class,\n            message -> {\n              // initial state received, unstash all messages and switch to active behavior\n              context.getLog().info(\"Finished initializing, unstashing messages\");\n              return buffer.unstashAll(active(message.value));\n            })\n        .onAnyMessage(\n            message -> {\n              // stash all other messages for later processing\n              context.getLog().info(\"Stashing message [{}] while initializing\", message.getClass());\n              buffer.stash(message);\n              return Behaviors.same();\n            })\n        .build();\n  }\n\n  private Behavior<Command> active(String state) {\n    return Behaviors.receive(Command.class)\n        .onMessage(\n            Get.class,\n            message -> {\n              message.replyTo.tell(state);\n              return Behaviors.same();\n            })\n        .onMessage(\n            Save.class,\n            message -> {\n              db.save(id, message.value);\n              // might fail, requiring supervision\n              message.replyTo.tell(null);\n              // switch to busy state\n              return busy(message.value);\n            })\n        .build();\n  }\n\n  private Behavior<Command> busy(String state) {\n    return Behaviors.receive(Command.class)\n        .onMessage(\n            SaveSuccess.class,\n            message -> {\n              // saved successfully, unstash all messages and switch to active behavior\n              return buffer.unstashAll(active(state));\n            })\n        .onMessage(\n            DBError.class,\n            message -> {\n              // handle error, maybe retry\n              // for now just go back to active\n              return buffer.unstashAll(active(state));\n            })\n        .onAnyMessage(\n            message -> {\n              // stash all other messages until database operation completes\n              buffer.stash(message);\n              return Behaviors.same();\n            })\n        .build();\n  }\n}\n// #stashing\n```\n\n----------------------------------------\n\nTITLE: Creating a Live and Fair Cycle Using Dropping Buffer in Akka Streams (Java)\nDESCRIPTION: A Java implementation that introduces a dropping buffer on the feedback arc. This makes the cycle both live (not deadlocking) and fair by dropping elements when necessary, preventing indefinite backpressure on the source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\nfinal RunnableGraph<NotUsed> dropping =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final UniformFanInShape<Integer, Integer> merge = b.add(Merge.create(2));\n              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n\n              b.from(b.add(source))\n                  .viaFanIn(merge)\n                  .via(b.add(Flow.of(Integer.class).map(s -> { System.out.println(s); return s; })))\n                  .viaFanOut(bcast)\n                  .to(b.add(Sink.ignore()));\n              b.from(bcast)\n                  .via(b.add(Flow.of(Integer.class).buffer(10, OverflowStrategy.dropHead())))\n                  .toFanIn(merge);\n              return ClosedShape.getInstance();\n            }));\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Handlers in State Classes (Java)\nDESCRIPTION: Shows how to implement event handlers within state classes for a bank account entity using Akka Persistence in Java. The example demonstrates the structure for EmptyAccount, OpenedAccount, and ClosedAccount states.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-style.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AccountExampleWithEventHandlersInState.java](/akka-cluster-sharding-typed/src/test/java/jdocs/akka/cluster/sharding/typed/AccountExampleWithEventHandlersInState.java) { #account-entity }\n```\n\n----------------------------------------\n\nTITLE: Creating a Live and Fair Cycle Using Dropping Buffer in Akka Streams (Scala)\nDESCRIPTION: A cycle implementation that introduces a dropping buffer on the feedback arc. This makes the cycle both live (not deadlocking) and fair by dropping elements when necessary, preventing indefinite backpressure on the source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_25\n\nLANGUAGE: Scala\nCODE:\n```\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n  val merge = b.add(Merge[Int](2))\n  val bcast = b.add(Broadcast[Int](2))\n\n  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore\n           merge <~ Flow[Int].buffer(10, OverflowStrategy.dropHead) <~ bcast\n\n  ClosedShape\n})\n```\n\n----------------------------------------\n\nTITLE: Splitting a Stream using groupBy in Scala\nDESCRIPTION: Demonstrates splitting an Akka Stream Source of integers into substreams based on whether the number is even or odd using the `groupBy` operator. Each substream corresponds to a unique key (0 for even, 1 for odd). The `maxSubstreams` parameter limits the number of concurrent substreams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n//#groupBy1\nval source = Source(1 to 10)\nsource\n  .groupBy(maxSubstreams = 2, _ % 2 == 0)\n  .to(Sink.ignore)\n  .run()\n//#groupBy1\n```\n\n----------------------------------------\n\nTITLE: Creating Cluster Singleton Manager\nDESCRIPTION: Implementation of ClusterSingletonManager setup with configuration for worker role and termination handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-singleton.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nsystem.actorOf(\n  ClusterSingletonManager.props(\n    singletonProps = Props(classOf[Consumer], queue),\n    terminationMessage = End,\n    settings = ClusterSingletonManagerSettings(system).withRole(\"worker\")),\n  name = \"consumer\")\n```\n\nLANGUAGE: Java\nCODE:\n```\nsystem.actorOf(\n  ClusterSingletonManager.props(\n    Props.create(Consumer.class, queue),\n    TestSingletonMessages.end(),\n    ClusterSingletonManagerSettings.create(system).withRole(\"worker\")),\n  \"consumer\");\n```\n\n----------------------------------------\n\nTITLE: Flattening Akka Streams with mapConcat in Scala\nDESCRIPTION: Demonstrates using the `mapConcat` operator in Scala to transform each element (Tweet) in a stream into an iterable of elements (Hashtags) and flatten the results into a single output stream. This is similar to `flatMap` in Scala collections but designed for streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_32\n\nLANGUAGE: scala\nCODE:\n```\nval hashtags: Source[Hashtag, NotUsed] =\n  tweets\n    .mapConcat(t => t.hashtags.toList)\n//-//#hashtags-mapConcat\n```\n\n----------------------------------------\n\nTITLE: Email Address Lookup using Akka Streams in Java\nDESCRIPTION: This Java code snippet illustrates how to use an external email lookup service in an Akka Stream. It shows how to manage asynchronous email address retrievals from a stream of authors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntegrationDocTest.java](/akka-docs/src/test/java/jdocs/stream/IntegrationDocTest.java) { #email-address-lookup2 }\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Side-Channels in Akka Streams - Java\nDESCRIPTION: In this Java snippet, asynchronous event handling in Akka Streams is shown via AsyncCallback. getAsyncCallback() registers an event handler, allowing external triggers to invoke state-safe operations. This mechanism ensures thread-safe execution of callbacks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [GraphStageDocTest.java](/akka-docs/src/test/java/jdocs/stream/GraphStageDocTest.java) { #async-side-channel }\n```\n\n----------------------------------------\n\nTITLE: Using ZipWith for Programmatic Triggering in Akka Streams\nDESCRIPTION: Java implementation of the programmatic trigger pattern using ZipWith, which is more efficient than using Zip followed by map. It directly returns the message element when a trigger signal arrives.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_35\n\nLANGUAGE: Java\nCODE:\n```\n// This is the stream of triggers\nSource<Trigger, NotUsed> triggerSource = Source.fromPublisher(triggersForMessages);\n\n// This is the stream of messages\nSource<Message, NotUsed> messageSource = Source.fromPublisher(messages);\n\nSource<Message, NotUsed> controllableSource =\n    messageSource\n        // When we get a trigger, we pass the message that came with it\n        .zipWith(triggerSource, (msg, trigger) -> msg);\n```\n\n----------------------------------------\n\nTITLE: Using JSON Framing with Akka Streams in Java\nDESCRIPTION: Shows the Java equivalent of using `JsonFraming.objectScanner` to separate valid JSON objects from an incoming ByteString stream. It takes a stream of bytes representing potentially multiple or partial JSON objects and outputs each complete, valid JSON object as a ByteString. Dependencies include Akka Streams and `akka.util.ByteString`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n// Code for [JsonFramingTest.java](/akka-stream-tests/src/test/java/akka/stream/javadsl/JsonFramingTest.java) { #using-json-framing } not available in input\n```\n\n----------------------------------------\n\nTITLE: Using mapAsync with Slow Service in Scala\nDESCRIPTION: Demonstrates how to use mapAsync with the slow service, maintaining order of results despite varying processing times. Shows buffer configuration for controlling concurrent operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_22\n\nLANGUAGE: scala\nCODE:\n```\nSource(List(\"a\", \"B\", \"C\", \"D\", \"e\", \"F\", \"g\", \"H\", \"i\", \"J\"))\n  .map { s =>\n    println(s\"before: ${s}\")\n    s\n  }\n  .mapAsync(4)(service.convert)\n  .map { s =>\n    println(s\"after: ${s}\")\n    s\n  }\n  .withAttributes(Attributes.inputBuffer(initial = 4, max = 4))\n  .runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor preStart Lifecycle Hook in Akka (Scala)\nDESCRIPTION: Illustrates overriding the preStart lifecycle method in a Scala Akka actor to execute initialization logic right after the actor is created. This method is typically used for setting up resources or state when an actor starts, and is called before any messages are processed. Requires Akka library; no input parameters. The method's primary purpose is to allow custom initialization and preparation, with no direct output.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.Actor\n\nclass MyActor extends Actor {\n  override def preStart(): Unit = {\n    // custom initialization code\n  }\n  def receive = {\n    case _ =>\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Streams with zipWithN in Scala\nDESCRIPTION: Example showing how to zip three integer streams and calculate the maximum value for each zipped sequence. The operation stops when any of the source streams completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/zipWithN.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval sourceOne = Source(1 to 2)\nval sourceTwo = Source(10 to 20)\nval sourceThree = Source(100 to 200)\n\nval sources = List(sourceOne, sourceTwo, sourceThree)\nval zipped: Source[Int, NotUsed] = Source.zipWithN(seq => seq.max)(sources)\n```\n\n----------------------------------------\n\nTITLE: Bubbling Actor Failures Upwards with Terminated Signal (Java)\nDESCRIPTION: This Java snippet demonstrates how to bubble actor failures up through the actor hierarchy by responding to the Terminated signal. The parent actor watches its child, and when it receives a Terminated or ChildFailed signal (which extends Terminated), it can choose to throw the exception further up. This pattern supports hierarchical error propagation. Dependencies: Akka Typed (Java); actors are required to watch their children; main parameters are signal notifications; output can be exception propagation or actor cleanup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\ncontext.watch(childRef);\nreturn Behaviors.receive(Object.class)\n    .onSignal(ChildFailed.class, (ctx, signal) -> {\n        throw signal.cause(); // bubble up\n    })\n    .onSignal(Terminated.class, (ctx, signal) -> {\n        // handle normal termination\n        return Behaviors.same();\n    })\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Boundaries in Akka Streams - Java\nDESCRIPTION: Shows how to create asynchronous boundaries in Akka Streams using Java, enabling parallel processing across multiple actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\nSource.from(IntStream.iterate(1, i -> i + 1).boxed().collect(Collectors.toList()))\n    .map(x -> x + 1)\n    .async()\n    .map(x -> x * 2)\n    .async()\n    .runWith(Sink.ignore(), system)\n```\n\n----------------------------------------\n\nTITLE: Using scan operator in Akka Streams in Java\nDESCRIPTION: Java implementation of the scan operator in Akka Streams to process elements cumulatively, showing how it emits each intermediate result during stream processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/scan.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// #scan\nsource.scan(0, (acc, next) -> acc + next);\n// #scan\n```\n\n----------------------------------------\n\nTITLE: Implementing Scanning Classification Bus in Scala\nDESCRIPTION: A Scala implementation of an EventBus using Scanning Classification, which scans through all subscribers for each event. It allows for flexible custom matching between events and subscribers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\nclass ScanningBusImpl extends\n  EventBus[String, String, ActorRef] with ScanningClassification {\n  // will be invoked for each event for all subscribers\n  override protected def publish(event: String, subscriber: ActorRef): Unit = {\n    subscriber ! event\n  }\n\n  // is needed for determining matching classifiers and subscribers\n  override protected def matches(classifier: String, event: String): Boolean = {\n    // publish String messages to subscribers that handle events starting with\n    // the given classifier\n    event.startsWith(classifier)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Identity Event Adapter in Java\nDESCRIPTION: Shows how to implement a basic identity event adapter in Java that doesn't transform events. This adapter simply passes events through without modification during journaling and recovery.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_35\n\nLANGUAGE: Java\nCODE:\n```\nclass IdentityEventAdapter implements EventAdapter {\n  @Override\n  public String manifest(Object event) {\n    return \"\";\n  }\n\n  @Override\n  public Object toJournal(Object event) {\n    return event;\n  }\n\n  @Override\n  public EventSeq fromJournal(Object event, String manifest) {\n    return EventSeq.single(event);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Polymorphic Type Handling in Java\nDESCRIPTION: Example showing how to handle polymorphic types with Jackson annotations in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME)\n@JsonSubTypes({\n  @Type(value = Lion.class, name = \"lion\"),\n  @Type(value = Elephant.class, name = \"elephant\")})\npublic interface Animal {}\n\npublic class Lion implements Animal {\n  public final String name;\n  public Lion(String name) { this.name = name; }\n}\n\npublic class Elephant implements Animal {\n  public final String name;\n  public final int age;\n  public Elephant(String name, int age) {\n    this.name = name;\n    this.age = age;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Renaming Fields with JSON in Akka Persistence (Java)\nDESCRIPTION: Java implementation of renaming a field from 'code' to 'seatNr' using manual versioning with JSON in Akka Persistence. This approach is useful when the serialization format doesn't support automatic renames.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\npublic class SeatReserved {\n    public final String seatNr;\n    public SeatReserved(String seatNr) {\n        this.seatNr = seatNr;\n    }\n}\n\npublic class JsonSchemaEvolutionAdapter implements EventAdapter {\n    @Override\n    public Object fromJournal(Object event, String manifest) {\n        if (event instanceof JsObject) {\n            JsObject json = (JsObject) event;\n            int version = json.fields().containsKey(\"_version\") ?\n                    (int) json.fields().get(\"_version\").convertTo(Integer.class) : 1;\n            switch (version) {\n                case 1:\n                    String seatNr = json.fields().get(\"code\").convertTo(String.class);\n                    return new SeatReserved(seatNr);\n                case 2:\n                    return new SeatReserved(json.fields().get(\"seatNr\").convertTo(String.class));\n                default:\n                    throw new IllegalArgumentException(\"Unknown version \" + version);\n            }\n        } else {\n            return event;\n        }\n    }\n\n    @Override\n    public String manifest(Object event) {\n        return \"\";\n    }\n\n    @Override\n    public Object toJournal(Object event) {\n        if (event instanceof SeatReserved) {\n            SeatReserved seatReserved = (SeatReserved) event;\n            JsObject json = new JsObject();\n            json.fields().put(\"_version\", JsNumber.apply(2));\n            json.fields().put(\"seatNr\", JsString.apply(seatReserved.seatNr));\n            return json;\n        } else {\n            return event;\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Ask Pattern with ActorFlow.askWithContext in Akka Streams (Scala)\nDESCRIPTION: Illustrates the Scala API for ActorFlow.askWithContext, enabling each incoming element and context tuple to generate a message using the makeMessage function, which will be sent to the provided typed ActorRef. The operator expects an implicit akka.util.Timeout for ask pattern support, recombines the original context to the stream reply, and fails on timeouts or Actor termination. Dependencies include 'akka-stream-typed' for Scala, and necessary versions must be resolved as indicated in the dependency block. Inputs are (element, context); outputs are (reply, context) tuples.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithContext.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nActorFlow.askWithContext[I, Q, A, Ctx](ref: akka.actor.typed.ActorRef[Q])(makeMessage: (I, akka.actor.typed.ActorRef[A]) => Q)(implicit timeout: akka.util.Timeout): akka.stream.scaladsl.Flow[(I, Ctx), (A, Ctx), akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Testing the device actor's read protocol in Akka Typed\nDESCRIPTION: Unit test for the device actor's read functionality. The test verifies that the actor correctly responds to temperature queries with the appropriate response message and correlation ID.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n#device-read-test\n```\n\nLANGUAGE: Java\nCODE:\n```\n#device-read-test\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Asynchronous Pipelining in Akka Streams (Java)\nDESCRIPTION: This Java code demonstrates how using `.async()` creates asynchronous boundaries between stream operators (A, B, C). This enables pipelining, where operators can process the next element immediately after emitting the previous one, potentially improving throughput over synchronous execution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nSource.range(1, 3)\n    .map(\n        i -> {\n          System.out.println(\"A: \" + i);\n          return i;\n        })\n    .async()\n    .map(\n        i -> {\n          System.out.println(\"B: \" + i);\n          return i;\n        })\n    .async()\n    .map(\n        i -> {\n          System.out.println(\"C: \" + i);\n          return i;\n        })\n    .async()\n    .runWith(Sink.ignore(), system);\n```\n\n----------------------------------------\n\nTITLE: Creating Event Adapters for Akka Persistence\nDESCRIPTION: Shows how to define event adapters that can convert from your Event type to another type that is passed to the journal. This is useful for event evolution and compatibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_39\n\nLANGUAGE: Scala\nCODE:\n```\ncase class Wrapper(event: Event, tags: Set[String])\\n\\nobject EventWrapper extends EventAdapter[Event, Wrapper] {\\n  override def toJournal(e: Event): Wrapper = Wrapper(e, Set(\"tag\"))\\n  override def fromJournal(p: Wrapper, manifest: String): EventSeq[Event] = EventSeq.single(p.event)\\n  override def manifest(event: Event): String = \"\"\\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nclass Wrapper {\\n  public final Event event;\\n  public final Set<String> tags;\\n\\n  public Wrapper(Event event, Set<String> tags) {\\n    this.event = event;\\n    this.tags = tags;\\n  }\\n}\\n\\nclass EventWrapper implements EventAdapter<Event, Wrapper> {\\n  @Override\\n  public Wrapper toJournal(Event event) {\\n    return new Wrapper(event, Collections.singleton(\"tag\"));\\n  }\\n\\n  @Override\\n  public EventSeq<Event> fromJournal(Wrapper wrapper, String manifest) {\\n    return EventSeq.single(wrapper.event);\\n  }\\n\\n  @Override\\n  public String manifest(Event event) {\\n    return \"\";\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Chaining Custom Operators in Akka Streams (Scala)\nDESCRIPTION: This snippet demonstrates how to use custom GraphStage operators in a stream processing chain. It shows the composition of map, filter, and duplicate operators using the 'via' method in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_17\n\nLANGUAGE: Scala\nCODE:\n```\nval result: Future[Seq[Int]] =\n  Source(1 to 10)\n    .via(new Map(_ + 1)) // add 1 to each number\n    .via(new Filter(_ > 5)) // keep only numbers > 5\n    .via(new Duplicator()) // duplicate each number\n    .runWith(Sink.seq)\n```\n\n----------------------------------------\n\nTITLE: Initializing Sharded Daemon Process Example in Java\nDESCRIPTION: This snippet demonstrates the Java implementation for initializing a Sharded Daemon Process within Akka. It serves the same purpose as its Scala counterpart, providing a way to set up actors within a cluster using Akka's tools, crucial for distributed processing tasks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharded-daemon-process.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ShardedDaemonProcessExample.java](/akka-cluster-sharding-typed/src/test/java/akka/cluster/sharding/typed/javadsl/ShardedDaemonProcessCompileOnlyTest.java) { #tag-processing }\n```\n\n----------------------------------------\n\nTITLE: Async DNS Resolution in Java\nDESCRIPTION: Example of using the Async DNS provider directly through the actor API in Java. Uses DnsProtocol.Resolve and DnsProtocol.Resolved messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-dns.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n#actor-api-async\n```\n\n----------------------------------------\n\nTITLE: Implementing Latency Tail Chopping Pattern in Akka Actors\nDESCRIPTION: This code snippet demonstrates the implementation of the latency tail chopping pattern in Akka actors. It aims to reduce tail latencies by sending backup requests to multiple actors that can perform the same work.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [TailChopping.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/TailChopping.scala) { #behavior }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [TailChopping.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/TailChopping.java) { #behavior }\n```\n\n----------------------------------------\n\nTITLE: Initializing Sharded Daemon Process Example in Scala\nDESCRIPTION: This snippet initializes a Sharded Daemon Process in Scala, which is part of the Akka framework. It demonstrates how to run actors with Sharded Daemon Process using a basic setup, useful for maintaining actor processes in a distributed cluster environment. Dependencies include Akka's cluster sharding and typed modules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharded-daemon-process.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ShardedDaemonProcessExample.scala](/akka-cluster-sharding-typed/src/test/scala/akka/cluster/sharding/typed/scaladsl/ShardedDaemonProcessSpec.scala) { #tag-processing }\n```\n\n----------------------------------------\n\nTITLE: Actor Ask Pattern Implementation in Java\nDESCRIPTION: Java implementation of the ask pattern for integrating streams with actors, showing how to maintain backpressure and handle responses.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/actor-interop.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nstatic class Message {}\nstatic class Req extends Message {\n  private final int x;\n  public Req(int x) {\n    this.x = x;\n  }\n}\nstatic class Res extends Message {\n  private final int x;\n  public Res(int x) {\n    this.x = x;\n  }\n}\n\nstatic class ExampleActor extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Req.class,\n            req -> {\n              getSender().tell(new Res(req.x), getSelf());\n            })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Actor Lifecycle Monitoring API Methods (Java)\nDESCRIPTION: Java API methods for watching and unwatching actor lifecycle changes in Akka. These methods are used to monitor actor termination and receive Terminated messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/supervision.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nActorContext.watch(akka.actor.typed.ActorRef)\nActorContext.unwatch(akka.actor.typed.ActorRef)\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data with splitWhen in Scala\nDESCRIPTION: Demonstrates splitting a stream of time series data into substreams for each second using splitWhen. Uses statefulMapConcat to compare timestamps between elements and determine split points.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/splitWhen.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n@snip [Scan.scala](/akka-docs/src/test/scala/docs/stream/operators/sourceorflow/Split.scala) { #splitWhen }\n```\n\n----------------------------------------\n\nTITLE: Source Queue Implementation in Scala\nDESCRIPTION: Example of using Source.queue to emit elements to a stream with backpressure support and buffering capabilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/actor-interop.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval bufferSize = 10\nval queue = Source.queue[Int](bufferSize, OverflowStrategy.backpressure)\n    .throttle(5, 3.seconds)\n    .toMat(Sink.foreach(println))(Keep.left)\n    .run()\n\n// push element to queue\nval futureResult = queue.offer(1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Filter Transformation with GraphStage in Scala\nDESCRIPTION: This snippet demonstrates a many-to-one operator implementation using GraphStage in Scala. It shows how to create a filter operator that conditionally passes elements downstream based on a predicate.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\nclass Filter[A](p: A => Boolean) extends GraphStage[FlowShape[A, A]] {\n  val in = Inlet[A](\"Filter.in\")\n  val out = Outlet[A](\"Filter.out\")\n  val shape = FlowShape(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      setHandler(in, new InHandler {\n        override def onPush(): Unit = {\n          val elem = grab(in)\n          if (p(elem)) push(out, elem)\n          else pull(in)\n        }\n      })\n      setHandler(out, new OutHandler {\n        override def onPull(): Unit = {\n          pull(in)\n        }\n      })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Top-Level Supervision to Functional Actor Behavior (Scala)\nDESCRIPTION: Demonstrates applying `Behaviors.supervise` only once at the top level in Scala when using the functional style of changing behaviors (like the `worker` example). The supervisor automatically re-wraps subsequent behaviors returned by the actor, ensuring consistent supervision.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n// #top-level\nval supervisedWorker = Behaviors.supervise(worker(0)).onFailure[Exception](SupervisorStrategy.restart)\n// #top-level\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Deny List with statefulMapConcat in Scala\nDESCRIPTION: Example demonstrating how to implement a deny list that filters out words based on dynamic rules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMapConcat.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval result = Source(List(\"allow:hello\", \"deny:world\", \"hello\", \"world\"))\n  .statefulMapConcat { () =>\n    val deniedWords = mutable.Set[String]()\n    {\n      case el if el.startsWith(\"deny:\") =>\n        deniedWords.add(el.substring(5))\n        Nil\n      case el if deniedWords(el) => Nil\n      case el                    => el :: Nil\n    }\n  }\n  .runWith(Sink.seq)\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data with splitAfter in Scala\nDESCRIPTION: Example showing how to split a stream of time series data into substreams for each second using the splitAfter operator with sliding window comparison.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/splitAfter.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ncase class TimeSeriesEvent(timestamp: Long)\n\nSource(List(\n  TimeSeriesEvent(1000),\n  TimeSeriesEvent(2000), \n  TimeSeriesEvent(3000), \n  TimeSeriesEvent(4000)\n))\n  .sliding(2)\n  .splitAfter(SubstreamCancelStrategy.drain) { pair => \n    val current = pair.head.timestamp / 1000\n    val next = pair.last.timestamp / 1000\n    current != next\n  }\n```\n\n----------------------------------------\n\nTITLE: Constructing Replicator Get/Delete Messages using Curried Syntax in Scala\nDESCRIPTION: Illustrates an alternative, curried Scala syntax for creating `Replicator.Get` and `Replicator.Delete` messages. This approach separates the key, consistency level, request context (optional), and reply target into distinct parameter lists. The code is referenced from `ReplicatorCompileOnlyTest.scala`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ReplicatorSpec.scala](/akka-cluster-typed/src/test/scala/akka/cluster/ddata/typed/scaladsl/ReplicatorCompileOnlyTest.scala) { #curried-get }\n```\n\n----------------------------------------\n\nTITLE: Implementing zipWithIndex using statefulMap in Java\nDESCRIPTION: Java implementation showing how to associate a unique incrementing index with each element in the stream starting from 0 using statefulMap.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMap.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(Arrays.asList(\"a\", \"b\", \"c\"))\n    .statefulMap(\n        () -> 0L,\n        (Long index, String element) -> new Pair<>(index + 1, new Pair<>(element, index)),\n        state -> Optional.empty());\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Source GraphStage (Java)\nDESCRIPTION: This Java snippet demonstrates the boilerplate for a custom Source operator via subclassing GraphStage. It establishes the outlets and shapes that define the operator's interface. The next steps would be implementing createLogic to generate numbers or emit data, confined to each materialization instance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [GraphStageDocTest.java](/akka-docs/src/test/java/jdocs/stream/GraphStageDocTest.java) { #simple-source }\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency\nDESCRIPTION: This snippet provides instructions on adding Akka Streams as a dependency in your project using sbt, Maven, or Gradle. Users must configure the correct group and artifact IDs along with the version symbol, enabling them to leverage Akka Streams' capabilities in their applications. It also includes setting up the test kit for testing purposes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-introduction.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\nbomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\nsymbol1=AkkaVersion\nvalue1=\"$akka.version$\"\ngroup=\"com.typesafe.akka\"\nartifact=\"akka-stream_$scala.binary.version$\"\nversion=AkkaVersion\ngroup2=\"com.typesafe.akka\"\nartifact2=\"akka-stream-testkit_$scala.binary.version$\"\nversion2=AkkaVersion\nscope2=test\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Streams with zipWithN in Java\nDESCRIPTION: Java implementation demonstrating how to zip three integer streams and find the maximum value from each combined sequence. The stream terminates when any source completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/zipWithN.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer> sourceOne = Source.from(Arrays.asList(1, 2));\nSource<Integer> sourceTwo = Source.from(Arrays.asList(10, 20));\nSource<Integer> sourceThree = Source.from(Arrays.asList(100, 200));\n\nList<Source<Integer, NotUsed>> sources = Arrays.asList(sourceOne, sourceTwo, sourceThree);\nSource<Integer, NotUsed> zipped = Source.zipWithN(integers -> Collections.max(integers), sources);\n```\n\n----------------------------------------\n\nTITLE: Using ControlAwareMailbox in Java\nDESCRIPTION: Java code showing how to create an actor that uses a control-aware mailbox. This actor will process all control messages immediately, regardless of their position in the message queue.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\n// We create a new Actor that just prints out what it processes\nfinal ActorRef controlAwareActor =\n    system.actorOf(Props.create(MyActor.class).withMailbox(\"control-aware-mailbox\"));\n\n// We fill up the mailbox with messages, and the actor will prioritize MyControlMessage\nfor (int i = 0; i < 100; i++) {\n  controlAwareActor.tell(\"msg \" + i, ActorRef.noSender());\n  if (i == 49) controlAwareActor.tell(MyControlMessage.INSTANCE, ActorRef.noSender());\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing flatMapMerge in Scala\nDESCRIPTION: Example showing how to use flatMapMerge to process customer IDs concurrently by creating a Source for each customer. The operator maintains order within each customer's events while allowing interleaving between different customers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/flatMapMerge.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 3)\n  .flatMapMerge(2, customerId => { // 2 means up to two customers can be interleaved\n    Source\n      .fromIterator(() => Iterator.from(1))\n      .map(i => (customerId, i))\n      .take(3)\n  })\n  // customerId -> count\n  // customer 1: (1,1), (1,2), (1,3)\n  // customer 2: (2,1), (2,2), (2,3)\n  // customer 3: (3,1), (3,2), (3,3)\n  // but the order between different customers is not deterministic\n```\n\n----------------------------------------\n\nTITLE: Merge Prioritized Sources in Scala\nDESCRIPTION: Demonstrates merging multiple sources with different priorities using Akka Streams in Scala. The example shows how to assign priorities to different sources and merge them accordingly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/mergePrioritizedN.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nmergePrioritizedN(List(source1, source2, source3), List(1, 2, 3))\n```\n\n----------------------------------------\n\nTITLE: Sending Stream Elements to ActorRef using Akka Streams (Scala)\nDESCRIPTION: This snippet demonstrates how to use the ActorSink.actorRef in Akka Streams to send elements from a stream to a specified ActorRef. It handles stream completion by sending a predefined message to the actor and adapts errors using an onFailureMessage function. It requires Akka dependencies correctly set in your build tool (e.g., sbt, Maven, Gradle).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorSink/actorRef.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSink.actorRef[T](ref:akka.actor.typed.ActorRef[T], onCompleteMessage:T, onFailureMessage:Throwable => T):akka.stream.scaladsl.Sink[T,akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Implementing Backpressure Buffer in Akka Streams\nDESCRIPTION: Creates a buffer of 1000 jobs with backpressure to relieve external system load. When buffer is full, backpressure is applied to slow down upstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\nexternalService.runWith(\n  Flow[Job].buffer(1000, OverflowStrategy.backpressure)\n)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(externalService)\n  .buffer(1000, OverflowStrategy.backpressure())\n  .run(system)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom TwoPhaseSet in Scala\nDESCRIPTION: Example of implementing a custom TwoPhaseSet data type in Scala. It demonstrates how to create a CRDT that allows adding and removing elements, but never adding an element again after removal.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_26\n\nLANGUAGE: Scala\nCODE:\n```\ncase class TwoPhaseSet(\n    adds: GSet[String] = GSet.empty[String],\n    removes: GSet[String] = GSet.empty[String])\n  extends ReplicatedData with ReplicatedDataSerialization {\n\n  type T = TwoPhaseSet\n\n  def add(element: String): TwoPhaseSet =\n    copy(adds = adds.add(element))\n\n  def remove(element: String): TwoPhaseSet =\n    copy(removes = removes.add(element))\n\n  def contains(element: String): Boolean =\n    adds.contains(element) && !removes.contains(element)\n\n  override def merge(that: TwoPhaseSet): TwoPhaseSet =\n    TwoPhaseSet(adds.merge(that.adds), removes.merge(that.removes))\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Splitter and Aggregator in Akka Streams\nDESCRIPTION: This pattern demonstrates how to split a message and aggregate its sub-messages into a new message, useful for parallel processing. It involves splitting strings of numbers, converting each to integers, and then summing them.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\nval intsPerString = sourceOfStrings\n  .map(s => s.split(\" \").map(_.toInt).toIndexedSeq)\n  .flatMapConcat(ids => Source(ids))\n  .async\n\nval sumOfGroups = intsPerString\n  .map(_ * 2)\n  .grouped(5)\n  .map(_.sum)\n  .async\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using ORMultiMap in Scala\nDESCRIPTION: Example of creating and using an ORMultiMap (observed-remove multi-map) in Scala. It demonstrates adding key-value pairs and updating the map.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\nval multiMap = ORMultiMap.empty[String, Int]\nval updated = multiMap.addBinding(\"a\", 1).addBinding(\"b\", 2)\nval result = updated.get(\"a\") // returns Set(1)\nval result2 = updated.getOrElse(\"c\", Set.empty)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Functional Style Counter Actor in Akka Typed (Java)\nDESCRIPTION: Shows a counter actor in Akka Typed (Java) utilizing functional style where state is treated as immutable and new behaviors are created as state changes. Relies on the Akka Typed API and may use static factory methods for behavior instantiation. Input consists of messages and immutable state, and output is a new behavior. This pattern is less typical in Java but follows the Akka conventions for stateless, functional actor design.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// @@snip [StyleGuideDocExamples.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/StyleGuideDocExamples.java) { #fun-style }\n\n```\n\n----------------------------------------\n\nTITLE: Task Manager with Stashing in Java\nDESCRIPTION: Java implementation of a task manager using Akka Persistence with command stashing. Handles StartTask, NextStep and EndTask commands while deferring commands for other tasks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_44\n\nLANGUAGE: java\nCODE:\n```\nRefer to file: StashingExample.java\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Handlers in State Classes (Scala)\nDESCRIPTION: Demonstrates how to define event handlers within state classes for a bank account entity using Akka Persistence. The example shows the structure for EmptyAccount, OpenedAccount, and ClosedAccount states.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-style.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AccountExampleWithEventHandlersInState.scala](/akka-cluster-sharding-typed/src/test/scala/docs/akka/cluster/sharding/typed/AccountExampleWithEventHandlersInState.scala) { #account-entity }\n```\n\n----------------------------------------\n\nTITLE: Simple Projection using mapAsync in Akka - Scala\nDESCRIPTION: Demonstrates using mapAsync to implement state-less event conversion and projection to a datastore without a reactive streams interface. This Scala example highlights how to manage event transformation and subsequent storage using simple classes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [PersistenceQueryDocSpec.scala](/akka-docs/src/test/scala/docs/persistence/query/PersistenceQueryDocSpec.scala) { #projection-into-different-store-simple }\n```\n\n----------------------------------------\n\nTITLE: Converting Between Java IO Streams using Akka Streams - Scala\nDESCRIPTION: Example demonstrating how to read from InputStream, uppercase the content, and write to OutputStream using Akka Streams in Scala. Uses StreamConverters.fromInputStream and StreamConverters.fromOutputStream to handle stream conversion.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/fromOutputStream.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#tofromJavaIOStream\n```\n\n----------------------------------------\n\nTITLE: Implementing RestartSource with Backoff in Scala\nDESCRIPTION: Shows how to create a backoff supervisor using RestartSource that will retry failed Server Sent Events streams with exponential backoff intervals.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\nRestartSource.onFailuresWithBackoff(\n  minBackoff = 3.seconds,\n  maxBackoff = 30.seconds,\n  randomFactor = 0.2,\n  maxRestarts = -1\n) { () =>\n  Source.fromGraph(ServerSentEvent(\"http://example.com/events\"))\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Streams with Zip in Scala\nDESCRIPTION: Demonstrates using the zip operator to combine elements from two sources in Scala. The operator combines each element from source1 with a corresponding element from source2 into tuples.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zip.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 4).zip(Source(10 to 14))\n```\n\n----------------------------------------\n\nTITLE: Pipelined and Parallelized Operators with Scala\nDESCRIPTION: This snippet describes employing four chefs to efficiently cook pancakes using both pipelining and parallel processing in Akka Streams with Scala. It emphasizes sequential and parallel task distribution for optimal resource use. Dependencies include Akka Streams setup for execution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-parallelism.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowParallelismDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowParallelismDocSpec.scala) { #pipelined-parallel }\n```\n\n----------------------------------------\n\nTITLE: TCP Server Implementation - Scala\nDESCRIPTION: Implementation of a TCP server that accepts incoming connections and delegates handling to child actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nclass Server extends Actor {\n\n  import Tcp._\n  import context.system\n\n  IO(Tcp) ! Bind(self, new InetSocketAddress(\"localhost\", 0))\n\n  def receive = {\n    case b @ Bound(localAddress) =>\n      // do some logging or setup ...\n\n    case CommandFailed(_: Bind) =>\n      context.stop(self)\n\n    case c @ Connected(remote, local) =>\n      val handler = context.actorOf(Props[SimplisticHandler])\n      val connection = sender()\n      connection ! Register(handler)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Composing Protocol Stack with Multiple BidiFlows in Akka Streams (Java)\nDESCRIPTION: This Java snippet demonstrates composition of several BidiFlow and Flow subgraphs to build a protocol stack using Akka Streams Java DSL, including use of reversed() to flip flow direction as required. It enables modular protocol layering by composing encoding, framing, and business logic in a single pipeline. Dependencies include akka.stream.javadsl. Inputs and outputs depend on protocol definition.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nBidiFlow<String, ByteString, ByteString, String, NotUsed> protocolStack = codec.atop(framing).reversed();\nprotocolStack.join(anotherFlow);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Adhoc Source in Akka Streams\nDESCRIPTION: Java implementation of the adhoc source pattern that creates a source that starts on demand and shuts down when there's no demand. It handles backpressure timeouts and automatically recovers by creating a new source when needed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_31\n\nLANGUAGE: Java\nCODE:\n```\n// Imports\nimport akka.stream.javadsl.Source;\nimport akka.stream.javadsl.Flow;\nimport java.time.Duration;\nimport java.util.concurrent.TimeoutException;\n\n// We need some recursive type here because adhocSource uses adhocSource\nSource<TwitterDocument, NotUsed> adhocSource =\n    Source.repeat(1)\n        .map(n -> readTwitter())\n        // The lazy source, emits when there is demand\n        .via(Flow.lazyCompletionStageFlow(() -> createAdhocFlow()))\n        // If there was no demand for 5 seconds, complete\n        .backpressureTimeout(Duration.ofSeconds(5))\n        // If we got completion from upstream, just start over when there is demand again\n        .recoverWithRetries(\n            1,\n            new PFBuilder<Throwable, Source<TwitterDocument, NotUsed>>()\n                .match(\n                    TimeoutException.class,\n                    ex -> adhocSource) // Recursive reference here\n                .build());\n```\n\n----------------------------------------\n\nTITLE: Scheduling One-Off Message in Java\nDESCRIPTION: Schedule a single message 'foo' to be sent to testActor after 50ms delay using Akka Scheduler in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/scheduler.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nsystem.scheduler().scheduleOnce(\n  Duration.ofMillis(50),\n  testActor,\n  \"foo\",\n  system.dispatcher(),\n  null);\n```\n\n----------------------------------------\n\nTITLE: Using interleaveAll in Akka Streams (Java)\nDESCRIPTION: Shows how to use the interleaveAll operator in Java to combine elements from multiple sources. It creates three sources and interleaves them with a segment size of 2, demonstrating the usage in a Java context.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/interleaveAll.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource<Integer> sourceA = Source.from(Arrays.asList(1, 2, 3));\nSource<Integer> sourceB = Source.from(Arrays.asList(4, 5, 6));\nSource<Integer> sourceC = Source.from(Arrays.asList(7, 8, 9));\n\nList<Graph<SourceShape<Integer>, NotUsed>> sources = Arrays.asList(sourceB, sourceC);\n\nCompletionStage<List<Integer>> result =\n    sourceA\n        .interleaveAll(sources, 2, false)\n        .runWith(Sink.seq(), system);\n\n// result: [1, 2, 4, 5, 7, 8, 3, 6, 9]\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Actor in Java\nDESCRIPTION: Example of implementing a basic Akka Actor in Java by extending the AbstractActor class and defining message handling behavior using the createReceive method.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#imports #my-actor\nimport akka.actor.AbstractActor;\nimport akka.event.Logging;\nimport akka.event.LoggingAdapter;\n\npublic class MyActor extends AbstractActor {\n  private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"test\", s -> {\n          log.info(\"received test\");\n          getSender().tell(\"got it\", getSelf());\n        })\n        .matchAny(o -> log.info(\"received unknown message\"))\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Utilize Transformed Sink in Scala\nDESCRIPTION: The Scala snippet shows how to use a customized Sink to process and store factorial results as strings in a file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [QuickStartDocSpec.scala](/akka-docs/src/test/scala/docs/stream/QuickStartDocSpec.scala) { #use-transformed-sink }\n```\n\n----------------------------------------\n\nTITLE: Using RetryFlow.withBackoffAndContext in Scala\nDESCRIPTION: Example of implementing RetryFlow.withBackoffAndContext with a FlowWithContext handling Ints with SomeContext, retrying elements unless the result is 0 or negative, or maxRetries is reached.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RetryFlow/withBackoffAndContext.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval flow: FlowWithContext[Int, SomeContext, Int, SomeContext, NotUsed] =\n  // business logic\n  FlowWithContext[Int, SomeContext].map { i =>\n    i - 1\n  }\n\nval retryFlow: FlowWithContext[Int, SomeContext, Int, SomeContext, NotUsed] =\n  RetryFlow.withBackoffAndContext(minBackoff = 10.millis, maxBackoff = 5.seconds, randomFactor = 0.2, maxRetries = 3, flow =\n    flow) {\n    case ((in, ctx), (out, _)) =>\n      if (out <= 0) None // do not retry on 0 or negative\n      else if (in - 1 == out) None // do not retry on expected outcome\n      else Some((in, ctx)) // otherwise retry with the original value\n  }\n```\n\n----------------------------------------\n\nTITLE: Creating Publish-Subscribe Channel with MergeHub and BroadcastHub in Scala\nDESCRIPTION: This snippet demonstrates how to connect a MergeHub and a BroadcastHub to form a publish-subscribe channel in Scala. It returns a pair of Source and Sink that define the publish and subscribe sides of the channel.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\nval (sink, source) = MergeHub.source[String](perProducerBufferSize = 16)\n  .toMat(BroadcastHub.sink(bufferSize = 256))(Keep.both)\n  .run()\n```\n\n----------------------------------------\n\nTITLE: Defining Akka Stream Sources and Sinks in Scala\nDESCRIPTION: Provides examples of creating basic Akka Stream Sources (e.g., from an Iterable, a single element, a Future) and Sinks (e.g., folding over elements, printing elements, ignoring elements) using the factories provided by the Source and Sink companion objects.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #source-sink }\n```\n\n----------------------------------------\n\nTITLE: Distributed Publish Subscribe Documentation\nDESCRIPTION: Documentation describing the Distributed Pub/Sub messaging feature for topic-based communication between actors in the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/includes/cluster.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n### Distributed Publish Subscribe\n\nPublish-subscribe messaging between actors in the cluster based on a topic, \ni.e. the sender does not have to know on which node the destination actor is running.\n```\n\n----------------------------------------\n\nTITLE: Specifying a Custom Dispatcher for File IO in Java\nDESCRIPTION: Demonstrates the Java method for assigning a custom dispatcher to `FileIO` operations in Akka Streams. By applying `.withAttributes(ActorAttributes.dispatcher(\"custom-blocking-io-dispatcher\"))` to the `FileIO` source or sink, you ensure that the potentially blocking file I/O is executed on a designated thread pool, improving overall system responsiveness.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n// Code for [StreamFileDocTest.java](/akka-docs/src/test/java/jdocs/stream/io/StreamFileDocTest.java) { #custom-dispatcher-code } not available in input\n```\n\n----------------------------------------\n\nTITLE: Retrieving Data with Get Operation in Java\nDESCRIPTION: Java example showing how to send a Get message to the Replicator to retrieve distributed data. This shows how to specify the read consistency level.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nfinal Key<PNCounter> Counter1Key = PNCounterKey.create(\"counter1\");\nreplicator.tell(new Get<>(Counter1Key, ReadLocal.instance()), getSelf());\n```\n\n----------------------------------------\n\nTITLE: Configuring Pinned Dispatcher in Akka\nDESCRIPTION: Configuration for a PinnedDispatcher that dedicates a separate thread for each actor. Includes note about core pool timeout settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\n#my-pinned-dispatcher-config\n```\n\n----------------------------------------\n\nTITLE: Sample Mailbox Configuration in HOCON\nDESCRIPTION: Example configuration for a custom mailbox showing how to specify capacity and overflow strategy in a configuration file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/mailboxes.md#2025-04-22_snippet_2\n\nLANGUAGE: hocon\nCODE:\n```\nmy-app {\n  my-special-mailbox {\n    mailbox-type = \"akka.dispatch.SingleConsumerOnlyUnboundedMailbox\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using the Publish-Subscribe Channel in Scala\nDESCRIPTION: This snippet demonstrates how to use the created publish-subscribe channel. It shows how to connect a producer and a consumer to the channel, and how to use the KillSwitch to deregister a user in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\nval switchProducer1 = Source.repeat(\"Hello\")\n  .viaMat(publishSubscribeFlow)(Keep.right)\n  .to(Sink.ignore)\n  .run()\n\nval switchConsumer1 = Source.maybe[String]\n  .viaMat(publishSubscribeFlow)(Keep.right)\n  .to(Sink.foreach(elements => println(s\"consumer1 processedMsg $elements\")))\n  .run()\n\n// ....\n\n// Deregister a single consumer or producer\nswitchConsumer1.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Custom Materialized Values in GraphStages - Scala\nDESCRIPTION: This Scala snippet illustrates how to return custom materialized values by extending GraphStageWithMaterializedValue. This allows the logic to yield a custom materialized value, seen here as a future of the first passing element. Proper synchronization is imperative.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_25\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #materialized }\n```\n\n----------------------------------------\n\nTITLE: Implementing Movies Watch List with ORSet CRDT in Scala\nDESCRIPTION: Example implementation of a movies watch list using Observed Remove Set (ORSet) CRDT in Scala. The implementation shows how to handle concurrent add/remove operations with automatic conflict resolution where add wins over remove.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nfinal case class MovieWatchList(\n    userId: String,\n    movies: ORSet[String] = ORSet.empty) {\n\n  def add(movieId: String): MovieWatchList =\n    copy(movies = movies.add(ReplicaId.random, movieId))\n\n  def remove(movieId: String): MovieWatchList =\n    copy(movies = movies.remove(movieId))\n\n  def contains(movieId: String): Boolean =\n    movies.elements.contains(movieId)\n}\n```\n\n----------------------------------------\n\nTITLE: Using Sealed Traits for Message Types in Scala\nDESCRIPTION: Defining messages with sealed traits in Scala to leverage compiler warnings for non-exhaustive pattern matching. This helps catch missing message handlers at compile time.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\nsealed trait Command\nfinal case class Increment(delta: Int) extends Command\nfinal case class GetValue(replyTo: ActorRef[Int]) extends Command\n```\n\n----------------------------------------\n\nTITLE: Enabling Recommended Default Passivation Strategy (HOCON)\nDESCRIPTION: This HOCON configuration enables the recommended default passivation strategy in Akka Cluster Sharding, which is planned to become the standard default in future versions. This strategy uses active entity limits with a composite replacement policy combining recency and frequency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_28\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.sharding {\n  passivation {\n    # \"default-strategy\" is a pre-configured strategy that we recommend\n    strategy = default-strategy\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Counting Characters per Line using splitAfter in Scala\nDESCRIPTION: Provides a practical example of using `splitAfter` to process a stream of characters representing text lines. It splits the stream after each newline character, counts the characters in each resulting substream (line), and prints the count.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n//#wordCount\nval text = \"This is the first line.\\n\" +\n  \"And this is the second line.\\n\" +\n  \"Here is the third line.\"\n\nSource(text.toList)\n  .splitAfter(_ == '\\n')\n  .filter(_ != '\\n')\n  .map(c => { print(c); 1 })\n  .reduce(_ + _)\n  .map(i => { println(); i })\n  .concatSubstreams\n  .runForeach(println)\n//#wordCount\n```\n\n----------------------------------------\n\nTITLE: Flattening Akka Streams with mapConcat in Java\nDESCRIPTION: Demonstrates using the `mapConcat` operator in Java to transform each element (Tweet) in a stream into a collection of elements (Hashtags) and flatten the results into a single output stream. The provided function must return a strict collection (e.g., `java.util.List`).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_33\n\nLANGUAGE: java\nCODE:\n```\nfinal Source<Hashtag, NotUsed> hashtags =\n  tweets\n    .mapConcat(t -> new ArrayList<>(t.getHashtags()));\n//#hashtags-mapConcat\n```\n\n----------------------------------------\n\nTITLE: Testing Timer Actor Behavior in Scala\nDESCRIPTION: Demonstrates testing an actor that schedules a task by overriding scheduling behavior in tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n#timer\\n#timer-test\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Room Behavior in Akka Typed - Scala\nDESCRIPTION: Implements the behavior of a chat room using Akka Typed in Scala, focusing on session handling and message broadcasting. Utilizes `GetSession` and `PublishSessionMessage` to manage interactions between actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/IntroSpec.scala) { #chatroom-behavior }\n```\n\n----------------------------------------\n\nTITLE: Spawning Actors with Akka Typed in Java\nDESCRIPTION: Illustrates the process of spawning actors using Akka Typed API in Java. It requires Akka Typed dependencies and uses ActorContext to handle actor lifecycles.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [HelloWorldMain.java](/samples/akka-quickstart-java/src/main/java/com/example/HelloWorldMain.java) { #hello-world-main }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Decoupled Buffer with GraphStage - Akka Streams - Scala\nDESCRIPTION: Demonstrates how to implement a buffer operator as a custom GraphStage in Akka Streams, achieving decoupling of upstream and downstream rates. Requires Akka Streams, with the key class being 'GraphStage' and its associated life-cycle callbacks. The buffer allows upstream and downstream to progress independently (within the buffer's capacity), pulling upstream immediately to fill the buffer and only forwarding to downstream when possible; capacities and event-order are controlled by internal state management. Input: arbitrary upstream elements. Output: same elements, possibly buffered. Prerequisite: Akka Streams library (Scala), understanding of custom stages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_27\n\nLANGUAGE: Scala\nCODE:\n```\n/*\n * Code demo for a buffer implemented with GraphStage in Akka Streams\n * Pulls upstream on initialization, buffers up to capacity, decouples demand from downstream\n */\n\n// --- Code as referenced in docs ---\n// @@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #detached }\n\n```\n\n----------------------------------------\n\nTITLE: Nesting Actor Behavior Setup in Java\nDESCRIPTION: The Java equivalent of nesting setup, withTimers, and withStash to access multiple dependencies in Akka Typed actors. The nesting order doesn't matter as long as the actor logic is in the innermost function.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\nBehaviors.setup(context -> \n  Behaviors.withTimers(timers -> \n    Behaviors.withStash(100, stash -> {\n      // behavior using context, timers, and stash\n      // ...\n      return Behaviors.empty();\n    })\n  )\n)\n```\n\n----------------------------------------\n\nTITLE: Using flatMapMerge in Scala\nDESCRIPTION: Illustrates the `flatMapMerge` operator in Akka Streams. It transforms each input element (integers 1 and 2) into a new Source (repeating the integer three times). Unlike `flatMapConcat`, `flatMapMerge` runs and merges elements from up to `breadth` (here, 2) of these generated Sources concurrently.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_17\n\nLANGUAGE: Scala\nCODE:\n```\n//#flatMapMerge\nSource(1 to 2)\n  .flatMapMerge(breadth = 2, i => Source(List.fill(3)(i)))\n  .runForeach(println)\n//#flatMapMerge\n```\n\n----------------------------------------\n\nTITLE: Publishing to Distributed PubSub Topic - Akka Typed - Java\nDESCRIPTION: Describes how to publish a message to a distributed pub/sub topic in Java via the akka.actor.typed.pubsub.Topic.Publish message. Prerequisites include an ActorRef for the topic and the message instance. The message is delivered to active subscribers on all nodes, with network deduplication handled by Akka. Delivery is best-effort (at-most-once), with no explicit confirmation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\ntopic.tell(new Topic.Publish<>(\n  message));\n```\n\n----------------------------------------\n\nTITLE: Event Handler Updates State in Akka Persistence - Java\nDESCRIPTION: Describes how a Java-based Akka persistent actor modifies its state by adding items to a list when events are successfully persisted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BasicPersistentBehaviorTest.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/BasicPersistentBehaviorTest.java) { #event-handler }\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Cluster Events in Scala\nDESCRIPTION: Example of an actor that subscribes to cluster membership events and logs them. This shows the pattern for creating a listener that reacts to changes in the cluster topology.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.{Actor, ActorLogging, Props}\nimport akka.cluster.Cluster\nimport akka.cluster.ClusterEvent._\n\nclass SimpleClusterListener extends Actor with ActorLogging {\n\n  val cluster = Cluster(context.system)\n\n  // subscribe to cluster changes, re-subscribe when restart\n  override def preStart(): Unit = {\n    cluster.subscribe(self, initialStateMode = InitialStateAsEvents,\n      classOf[MemberEvent], classOf[UnreachableMember])\n  }\n  override def postStop(): Unit = cluster.unsubscribe(self)\n\n  def receive = {\n    case MemberUp(member) =>\n      log.info(\"Member is Up: {}\", member.address)\n    case UnreachableMember(member) =>\n      log.info(\"Member detected as unreachable: {}\", member)\n    case MemberRemoved(member, previousStatus) =>\n      log.info(\"Member is Removed: {} after {}\",\n        member.address, previousStatus)\n    case _: MemberEvent => // ignore\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Restarting Source on Failure with Backoff in Scala\nDESCRIPTION: This example shows how the inner source is restarted with increasing backoff when it fails. The source emits 1, 2, 3, and then throws an exception. The first time the exception is thrown, the source is restarted after 1s, then 2s, etc., until the maxBackoff of 10s.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartSource/onFailuresWithBackoff.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nRestartSource\n  .onFailuresWithBackoff(RestartSettings(minBackoff, maxBackoff, randomFactor)) { () =>\n    Source(1 to 4).map { n =>\n      if (n == 4) throw new RuntimeException(\"Boom!\")\n      else {\n        println(n)\n        n\n      }\n    }\n  }\n  .runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Splitting Streams using splitWhen and splitAfter in Scala\nDESCRIPTION: Compares the `splitWhen` and `splitAfter` operators in Akka Streams. `splitWhen` starts a new substream *with* the element satisfying the predicate, while `splitAfter` starts a new substream *after* the element satisfying the predicate. Both merge the substreams sequentially using `concatSubstreams`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n//#splitWhenAfter\nSource(1 to 10)\n  .splitWhen(i => i % 3 == 0)\n  .map(i => { println(s\"splitWhen: ${i}\"); i })\n  .concatSubstreams\n  .runWith(Sink.ignore)\n\nprintln(\"---------------------------\")\n\nSource(1 to 10)\n  .splitAfter(i => i % 3 == 0)\n  .map(i => { println(s\"splitAfter: ${i}\"); i })\n  .concatSubstreams\n  .runWith(Sink.ignore)\n//#splitWhenAfter\n```\n\n----------------------------------------\n\nTITLE: Enabling Rotating TLS in Akka via HOCON (HOCON)\nDESCRIPTION: Configures Akka cluster to use TLS transport with a specialized SSL engine provider that can handle automatic certificate rotation ('RotatingKeysSSLEngineProvider'). This snippet uses HOCON syntax and expects that Kubernetes secrets are made available at the correct mount point. The configuration must be included in the Akka application's configuration files for secure communication.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_6\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery {\\n  transport = tls-tcp\\n  ssl.ssl-engine-provider = \\\"akka.remote.artery.tcp.ssl.RotatingKeysSSLEngineProvider\\\"\\n}\n```\n\n----------------------------------------\n\nTITLE: Environment-Specific Configuration Include\nDESCRIPTION: Shows how to include base configuration file and override specific settings for different environments.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/configuration.md#2025-04-22_snippet_2\n\nLANGUAGE: hocon\nCODE:\n```\ninclude \"application\"\n\nakka {\n  loglevel = \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Backoff Supervision for Persistent Actor\nDESCRIPTION: Demonstrates how to implement backoff supervision strategy for handling persistence failures in Akka actors. Shows configuration for both Scala and Java implementations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_22\n\nLANGUAGE: scala\nCODE:\n```\n#backoff\n```\n\nLANGUAGE: java\nCODE:\n```\n#backoff\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Source as an Akka Streams Source (Java)\nDESCRIPTION: This Java snippet illustrates converting a custom Source GraphStage into a Akka Streams Source, making it usable within Java Akka Streams pipelines. The pattern involves wrapping the custom logic with Source.fromGraph, after which it can be materialized and run like built-in sources.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [GraphStageDocTest.java](/akka-docs/src/test/java/jdocs/stream/GraphStageDocTest.java) { #simple-source-usage }\n```\n\n----------------------------------------\n\nTITLE: Using ORSet (Observed-Remove Set) in Akka Distributed Data (Scala)\nDESCRIPTION: Demonstrates how to use the ORSet CRDT for distributed sets that support add and remove operations in Scala. Handles concurrent additions and removals with add-wins semantics, requiring causal delivery of update deltas. Akka Distributed Data module is required; input operations are adds/removes, output is the latest set. Useful for cases with high contention or concurrent modifications.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\n/* Scala Example: ORSet usage */\nval setKey = ORSetKey[String](\"exampleORSet\")\ndistributedData ! Update(setKey, ORSet.empty[String], WriteLocal)(_ + \"foo\")\ndistributedData ! Update(setKey, ORSet.empty[String], WriteLocal)(_ - \"foo\")\n\n```\n\n----------------------------------------\n\nTITLE: Adding Item to Cart with Consistency in Java\nDESCRIPTION: Java example of adding an item to a cart in distributed storage using WriteMajority consistency. It shows how to update the data with proper consistency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\nprivate CompletionStage<Done> addItem(String userId, String itemId, int quantity) {\\n  return askUpdate(\\n      new Update<>(\\n          DataKey.create(userId),\\n          Cart.empty(),\\n          writeMajority,\\n          curr -> curr.addItem(itemId, quantity)))\\n      .thenApply(updateDone -> Done.getInstance());\\n}\n```\n\n----------------------------------------\n\nTITLE: Converting InputStream to Uppercase with Akka Streams in Scala\nDESCRIPTION: Demonstrates reading from an InputStream, converting content to uppercase, and writing to an OutputStream using Akka Streams in Scala. Uses StreamConverters.fromInputStream and StreamConverters.fromOutputStream with ByteString manipulation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/fromInputStream.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval text = \"\"\"The quick brown fox...\n123\n456\n789\"\"\"\n\nval inputStream = new ByteArrayInputStream(text.getBytes(StandardCharsets.UTF_8))\nval outputStream = new ByteArrayOutputStream()\n\nval result = StreamConverters\n  .fromInputStream(() => inputStream)\n  .map(bytes => ByteString(bytes.utf8String.toUpperCase))\n  .runWith(StreamConverters.fromOutputStream(() => outputStream))\n\nresult.foreach { ioResult =>\n  println(s\"${outputStream.toString(StandardCharsets.UTF_8.name())}\")\n  println(s\"Read ${ioResult.count} bytes\")\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting Routing Strategy in Akka Scala\nDESCRIPTION: This snippet demonstrates different strategies for message routing in Akka using Scala: round-robin, random, and consistent hashing. Configure strategy prior to spawning routers and ensure hashing mappings are provided if using consistent hashing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n/* Routing strategy configuration in RouterSpec.scala */\n```\n\n----------------------------------------\n\nTITLE: Merging Partitioned Streams with MergeSequence in Java\nDESCRIPTION: This Java example shows how to use MergeSequence with Partition to merge a partitioned stream back into a single ordered stream. It processes even-numbered messages and merges them with unprocessed odd-numbered messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/MergeSequence.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource<Pair<String, Integer>> source =\n    Source.range(1, 10)\n        .map(n -> n % 2 == 0 ? \"even \" + n : \"odd \" + n)\n        .zipWith(Source.range(0, 100), Pair::new);\n\nPair<Source<Pair<String, Integer>>, Source<Pair<String, Integer>>> partition =\n    source.partition(pair -> pair.first().startsWith(\"even\"), 2);\n\nSource<Pair<String, Integer>> needsProcessing = partition.first();\nSource<Pair<String, Integer>> noProcessing = partition.second();\n\nSource<Pair<String, Integer>> processedMessages =\n    needsProcessing.map(\n        pair -> Pair.create(\"processed \" + pair.first(), pair.second()));\n\nSource<Pair<String, Integer>> mergedSource =\n    Source.fromGraph(\n        GraphDSL.create(\n            builder -> {\n              MergeSequence<Pair<String, Integer>> merge =\n                  builder.add(MergeSequence.create(2));\n              builder.from(processedMessages).toInlet(merge.in(0));\n              builder.from(noProcessing).toInlet(merge.in(1));\n              return SourceShape.of(merge.out());\n            }));\n\nmergedSource.runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Declaring Messages in Companion Object in Scala\nDESCRIPTION: Best practice for declaring message types that an actor can receive in the companion object of the Actor class for better code organization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nobject WatchActor {\n  // #messages-in-companion\n  // these are the messages our actor handles\n  final case class Transition(target: ActorRef)\n  final case class Watch(target: ActorRef)\n  // #messages-in-companion\n\n  def props(): Props = Props(new WatchActor)\n}\n\nclass WatchActor extends Actor {\n  import WatchActor._\n\n  var watchList = Map.empty[String, ActorRef]\n\n  def receive = {\n    case Watch(ref) =>\n      context.watch(ref)\n      watchList = watchList.updated(ref.path.name, ref)\n    case Transition(ref) =>\n      // ref might be the same, but path string is still different\n      context.watch(ref)\n      watchList = watchList.updated(ref.path.name, ref)\n    case Terminated(ref) =>\n      val name = ref.path.name\n      watchList.get(name).foreach { _ =>\n        watchList = watchList - name\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting InputStream to Uppercase with Akka Streams in Java\nDESCRIPTION: Demonstrates reading from an InputStream, converting content to uppercase, and writing to an OutputStream using Akka Streams in Java. Uses StreamConverters.fromInputStream and StreamConverters.fromOutputStream with ByteString manipulation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/fromInputStream.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nString text = \"The quick brown fox...\\n123\\n456\\n789\";\n\nInputStream inputStream = new ByteArrayInputStream(text.getBytes(StandardCharsets.UTF_8));\nByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n\nCompletionStage<IOResult> result =\n    StreamConverters.fromInputStream(() -> inputStream)\n        .map(bytes -> ByteString.fromString(bytes.utf8String().toUpperCase()))\n        .runWith(StreamConverters.fromOutputStream(() -> outputStream), system);\n\nresult.thenAccept(\n    ioResult -> {\n      System.out.println(outputStream.toString(StandardCharsets.UTF_8.name()));\n      System.out.println(\"Read \" + ioResult.getCount() + \" bytes\");\n    });\n```\n\n----------------------------------------\n\nTITLE: MapAsync Strict Order Processing in Scala\nDESCRIPTION: Example showing strict order processing of events using mapAsync with parallelism of 1. This ensures events are processed sequentially while maintaining their original order.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(1 to 100)\n  .map(n => Event(n))\n  .mapAsync(1) { event =>\n    Future {\n      println(s\"Processing event number $event...\")\n      Thread.sleep(500) // processing takes time\n      println(s\"Completed processing ${event.number}\")\n      event\n    }\n  }\n  .map(e => {\n    println(\"`mapAsync` emitted event number: \" + e.number)\n    e\n  })\n```\n\n----------------------------------------\n\nTITLE: Setting Per-Stream Input Buffer Size in Akka Streams (Scala)\nDESCRIPTION: This Scala snippet demonstrates how to override the default input buffer size for a specific section of a stream or the entire runnable graph. It defines a flow with custom attributes specifying an input buffer size of 1.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nval section = Flow[Int].map(_ * 2).withAttributes(Attributes.inputBuffer(initial = 1, max = 1))\n```\n\n----------------------------------------\n\nTITLE: Common Chained Effects in Akka Actors - Java\nDESCRIPTION: This Java snippet illustrates how common effects can be chained and reused within Akka persistent actors to efficiently manage command executions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [PersistentActorCompileOnlyTest.java](/akka-persistence-typed/src/test/java/akka/persistence/typed/javadsl/PersistentActorCompileOnlyTest.java) { #commonChainedEffects }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple TCP Chat Server with Flow.fromSinkAndSource in Scala\nDESCRIPTION: This Scala example demonstrates how to create a simple TCP chat server using Flow.fromSinkAndSource. It combines MergeHub and BroadcastHub to dynamically merge incoming messages and broadcast them to all connected clients.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/fromSinkAndSource.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nval (sink, source) =\n  MergeHub.source[String]\n    .map(msg => ByteString(s\"$msg\\n\"))\n    .toMat(BroadcastHub.sink[ByteString])(Keep.both)\n    .run()\n\nval flow: Flow[ByteString, ByteString, NotUsed] =\n  Flow.fromSinkAndSource(\n    Flow[ByteString]\n      .map(_.utf8String.trim)\n      .to(sink),\n    source\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating FileIO Sink for Writing ByteStrings to File in Scala\nDESCRIPTION: This snippet demonstrates how to create a sink using FileIO.toPath in Scala to write ByteStrings to a file. It shows the creation of a file path and the sink, which is then used in a stream to write data from a source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/FileIO/toPath.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.stream.scaladsl._\nimport akka.util.ByteString\nimport java.nio.file.Paths\n\nval file = Paths.get(\"example.txt\")\nval sink = FileIO.toPath(file)\n\nSource.single(ByteString(\"Hello Akka Streams!\"))\n  .runWith(sink)\n```\n\n----------------------------------------\n\nTITLE: Handling Blocking Operations with map and Dedicated Dispatcher (Java)\nDESCRIPTION: An alternative way to handle blocking calls using a simple `map` operation. The `withAttributes` and `ActorAttributes.dispatcher` are used to ensure the blocking `lookupEmailBlocking` method runs on the dedicated `blocking-dispatcher`. Note that this processes elements one by one, unlike the concurrent nature of `mapAsync`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\nSource<String, NotUsed> emailAddressesMapBlocking =\n  authors\n    .map(author -> lookupEmailBlocking(author.handle))\n    .withAttributes(ActorAttributes.dispatcher(\"blocking-dispatcher\"))\n    .filter(Optional::isPresent)\n    .map(Optional::get);\n```\n\n----------------------------------------\n\nTITLE: Accessing ActorContext in EventSourcedBehavior (Scala)\nDESCRIPTION: Demonstrates how to access the ActorContext within an EventSourcedBehavior by wrapping construction with Behaviors.setup in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\nBehaviors.setup[Command] { context =>\n  EventSourcedBehavior[Command, Event, State](\n    persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n    emptyState = State.empty,\n    commandHandler = (state, command) =>\n      if (command == Increment) Effect.persist(Incremented)\n      else Effect.none,\n    eventHandler = (state, event) => state.copy(value = state.value + 1)\n  )\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing mapAsyncUnordered in Scala\nDESCRIPTION: Example showing how to use mapAsyncUnordered to process events asynchronously with parallelism, prioritizing throughput over order.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapAsyncUnordered.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.range(1, 10)\n  .map(n => Event(n))\n  .mapAsyncUnordered(parallelism = 4) { event =>\n    Future {\n      println(s\"Processing event numner ${event}...\")\n      Thread.sleep(Random.nextInt(300))\n      println(s\"Completed processing ${event.id}\")\n      event.id\n    }\n  }\n  .runForeach(id => println(s\"`mapAsyncUnordered` emitted event number: $id\"))\n```\n\n----------------------------------------\n\nTITLE: Default Persistence Plugin Configuration\nDESCRIPTION: Shows the default configuration paths for journal and snapshot store plugins in Akka Persistence. These are the fallback paths used when no specific plugins are configured.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_40\n\nLANGUAGE: Scala\nCODE:\n```\nakka.persistence.journal.plugin = \"akka.persistence.journal.inmem\"\nakka.persistence.snapshot-store.plugin = \"akka.persistence.snapshot-store.local\"\n```\n\n----------------------------------------\n\nTITLE: Resume Section Supervision Example\nDESCRIPTION: Shows how to apply supervision strategy to a specific section of the flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_17\n\nLANGUAGE: Scala\nCODE:\n```\nval decider: Supervision.Decider = {\n  case _: ArithmeticException => Supervision.Resume\n  case _ => Supervision.Stop\n}\n\nFlow[Int].map(100 / _).withAttributes(ActorAttributes.supervisionStrategy(decider))\n```\n\n----------------------------------------\n\nTITLE: Implementing Request-Response with ask between Akka Typed Actors\nDESCRIPTION: This example shows how to use the 'ask' pattern for request-response interactions between two Akka Typed actors. It demonstrates constructing the outgoing message, transforming the response, and handling timeouts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [InteractionPatternsSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/InteractionPatternsSpec.scala) { #actor-ask }\n```\n\n----------------------------------------\n\nTITLE: Utilize Transformed Sink in Java\nDESCRIPTION: An example in Java demonstrating the application of a custom Sink to manage and output factorial computations to a file as strings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [QuickStartDocTest.java](/akka-docs/src/test/java/jdocs/stream/QuickStartDocTest.java) { #use-transformed-sink }\n```\n\n----------------------------------------\n\nTITLE: Implementing UnfoldAsync Stream in Scala\nDESCRIPTION: Creates a stream of ByteStrings using unfoldAsync by repeatedly requesting chunks from an actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldAsync.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\ndef createSource(actor: ActorRef): Source[ByteString, NotUsed] =\n  Source.unfoldAsync[Long, ByteString](0) { offset =>\n    val fut = (actor ? GetChunk(offset))(3.seconds).mapTo[Chunk]\n    fut.map { chunk =>\n      if (chunk.bytes.isEmpty) None\n      else Some((offset + chunk.bytes.length) -> chunk.bytes)\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Implementing Adhoc Source in Akka Streams\nDESCRIPTION: A pattern for creating sources that start only when there's demand and shut down when there's no more demand. Combines lazySource, backpressureTimeout, and recoverWithRetries to create on-demand processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_30\n\nLANGUAGE: Scala\nCODE:\n```\nval adhocSource = Source.repeat(1).map(_ => readTwitter())\n  // The lazy source, emits when there is demand\n  .via(Flow.lazyFlow(() => createAdhocSource()))\n  // If there was no demand for 5 seconds, complete\n  .backpressureTimeout(5.seconds)\n  // If we got completion from upstream, just start over when there is demand again\n  .recoverWithRetries(1, {\n    case _: TimeoutException => adhocSource\n  })\n```\n\n----------------------------------------\n\nTITLE: Creating EventSourcedBehavior in Akka - Java\nDESCRIPTION: Defines the process of creating an EventSourcedBehavior in Akka persistence using Java, necessary for ensuring actors can be persistent and resilient.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BasicPersistentBehaviorTest.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/BasicPersistentBehaviorTest.java) { #behavior }\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for Scala Echo Handler (ACK-Based Back-Pressure)\nDESCRIPTION: This Scala snippet contains helper functions used within the `SimpleEchoHandler`. The `buffer` function appends incoming data to a storage vector and sends the first chunk if the buffer was previously empty. The `acknowledge` function removes the acknowledged data chunk and sends the next one if available. It also handles closing the connection when all data is sent and the peer has closed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nprivate def buffer(data: ByteString): Unit = {\n    storage :+= data\n    stored += data.size\n\n    if (stored > maxStored) {\n      log.warning(s\"New data {}, total stored: {} exceeds max: {}\", data.size, stored, maxStored)\n      connection ! SuspendReading\n    }\n\n    if (stored == data.size) {\n      // storage was empty, start writing\n      connection ! Write(storage(0), Ack(0))\n    }\n  }\n\n  private def acknowledge(ack: Int): Unit = {\n    require(storage.nonEmpty, s\"storage was empty at ack $ack\")\n    require(ack == transferred, s\"expected ack $transferred but got $ack\")\n\n    val size = storage(0).size\n    stored -= size\n    transferred += 1\n\n    storage = storage.drop(1)\n\n    if (stored < maxStored) {\n      connection ! ResumeReading\n    }\n\n    if (storage.isEmpty) {\n      if (closed) {\n        log.info(\"Closing connection to {} after writing {} bytes\", remote, transferred)\n        context.stop(self)\n      }\n    } else {\n      connection ! Write(storage(0), Ack(transferred.toInt))\n    }\n  }\n\n  private def peerClosed(): Unit = {\n    if (storage.isEmpty) {\n      log.info(\"Closing connection to {}, peer closed\", remote)\n      context.stop(self)\n    } else {\n      closed = true\n      log.info(\"Peer closed, waiting for {} bytes to be written to {}\", stored, remote)\n    }\n  }\n\n  def closing: Receive = {\n    case Ack(ack) => acknowledge(ack)\n    case ErrorClosed(cause) =>\n      log.error(cause, \"Stopping, because connection dropped.\")\n      context.stop(self)\n    case ConnectionClosed =>\n      log.error(\"Stopping, because connection closed\")\n      context.stop(self)\n  }\n\n```\n\n----------------------------------------\n\nTITLE: Getting the ClusterSharding Extension in Java\nDESCRIPTION: Shows how to import necessary classes and obtain the ClusterSharding extension instance within a Java Akka Typed application via the ActorSystem.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// #import\nimport akka.actor.typed.ActorSystem;\nimport akka.cluster.sharding.typed.javadsl.ClusterSharding;\n\n// #import\n\nActorSystem<Void> system = null;\n\n// #sharding-extension\nClusterSharding sharding = ClusterSharding.get(system);\n// #sharding-extension\n```\n\n----------------------------------------\n\nTITLE: Logback Pattern for All MDC Properties - Logback - XML\nDESCRIPTION: This XML code sets up a Logback encoder pattern to output all available MDC properties inline for every log entry. By utilizing %mdc, every contextual property provided by Akka logging (such as akkaSource, sourceThread, etc.) is rendered in the log line. Suitable for detailed debugging where full context is required, this should be placed within the <encoder> section of logback.xml and expects Akka to provide MDC values on log events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_16\n\nLANGUAGE: xml\nCODE:\n```\n  <encoder>\n    <pattern>%date{ISO8601} %-5level %logger{36} - %msg MDC: {%mdc}%n</pattern>\n  </encoder>\n```\n\n----------------------------------------\n\nTITLE: Fabricating Parent Actors for Isolated Child Testing - Scala\nDESCRIPTION: Illustrates using a fabricated parent actor, created within the test scope, to encapsulate child actor interactions. This is useful for situations where neither the actual parent nor child constructors can be changed. Akka Actors and testing utilities are required.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ParentChildSpec.scala](/akka-docs/src/test/scala/docs/testkit/ParentChildSpec.scala) { #test-fabricated-parent }\n```\n\n----------------------------------------\n\nTITLE: Configuring BackoffSupervisor with Custom Fail Strategy\nDESCRIPTION: Shows configuration of a BackoffSupervisor with automatic reset and custom exception handling. The supervisor handles MyException specifically while escalating other exceptions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nBackoffSupervisor.props(\n  BackoffOpts.onFailure(\n    childProps = Props[FrontendActor],\n    childName = \"myActor\",\n    minBackoff = 3.seconds,\n    maxBackoff = 30.seconds,\n    randomFactor = 0.2\n  ).withAutoReset(10.seconds)\n    .withSupervisionStrategy(OneForOneStrategy() {\n      case _: MyException => SupervisorStrategy.Stop\n      case _             => SupervisorStrategy.Escalate\n    }))\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Pool Router in Scala\nDESCRIPTION: This snippet shows how to configure a pool router in Akka using Scala. The pool router forwards messages to a set of routees and handles routee supervision. Ensure Akka Typed is included in your project dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n/* Pool router configuration in RouterSpec.scala */\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Maven Repository (sbt/Maven/Gradle) - Configuration\nDESCRIPTION: This snippet shows how to register Akka's official library repository with sbt, Maven, or Gradle by providing the repository id, name, and URL. Registering this repository is a prerequisite for accessing any Akka module artifacts via a supported build tool. Required for installing Akka Stream TestKit and other Akka dependencies. No input/output: this is configuration. Constraints: URL must not change for proper resolution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-testkit.md#2025-04-22_snippet_0\n\nLANGUAGE: configuration\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\\\"akka-repository\\\"\nname=\\\"Akka library repository\\\"\nurl=\\\"https://repo.akka.io/maven\\\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Balanced and Live Cycle with Initial Element in Akka Streams (Java)\nDESCRIPTION: A Java implementation of a balanced cycle that injects an initial element using Concat. This solves the chicken-and-egg problem by providing an independent initial element that kicks off the cycle, allowing it to process elements continuously without deadlock.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_30\n\nLANGUAGE: Java\nCODE:\n```\nfinal RunnableGraph<NotUsed> zippingLive =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final FanInShape2<Integer, Integer, Integer> zip =\n                  b.add(ZipWith.create((Integer left, Integer right) -> right));\n              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n              final UniformFanInShape<Integer, Integer> concat = b.add(Concat.create(2));\n\n              b.from(b.add(source)).toInlet(zip.in0());\n              b.from(zip.out())\n                  .via(b.add(Flow.of(Integer.class).map(s -> { System.out.println(s); return s; })))\n                  .viaFanOut(bcast)\n                  .to(b.add(Sink.ignore()));\n\n              b.from(b.add(Source.single(0))).toFanIn(concat);\n              b.from(bcast).toFanIn(concat);\n              b.from(concat).toInlet(zip.in1());\n\n              return ClosedShape.getInstance();\n            }));\n```\n\n----------------------------------------\n\nTITLE: Using LWWRegister with Custom Clock in Scala\nDESCRIPTION: Example of using LWWRegister with a custom clock in Scala. It demonstrates creating a register with a custom timestamp based on a version number.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\ndef versionedLWWRegister(name: String, version: Long): LWWRegister[String] =\n  LWWRegister(name, version, LWWRegister.defaultClock[String])\n\nval r1 = versionedLWWRegister(\"value1\", 1L)\nval r2 = versionedLWWRegister(\"value2\", 2L)\n\nval merged = r1.merge(r2)\nmerged.value // \"value2\"\n```\n\n----------------------------------------\n\nTITLE: Restarting Source with Kill Switch in Scala\nDESCRIPTION: This example demonstrates how to stop the restarting of a source using a kill switch. The kill switch is inserted right after the restart source. The inner source emits 3 elements and then fails. A killswitch is used to be able to stop the source from being restarted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartSource/onFailuresWithBackoff.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nval (killSwitch, done) = RestartSource\n  .onFailuresWithBackoff(RestartSettings(minBackoff, maxBackoff, randomFactor)) { () =>\n    Source(1 to 4).map { n =>\n      if (n == 4) throw new RuntimeException(\"Boom!\")\n      else {\n        println(n)\n        n\n      }\n    }\n  }\n  .viaMat(KillSwitches.single)(Keep.right)\n  .toMat(Sink.ignore)(Keep.both)\n  .run()\n\n// Stop the restarts:\nkillSwitch.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Configuring Keep Majority Strategy in Akka Cluster (HOCON)\nDESCRIPTION: Sets the `active-strategy` property within `akka.cluster.split-brain-resolver` to `keep-majority` in the Akka configuration file (e.g., `application.conf`). This activates the strategy where the partition containing the majority of nodes (based on last known membership) survives a network split, downing the minority partition.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_2\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.split-brain-resolver.active-strategy=keep-majority\n```\n\n----------------------------------------\n\nTITLE: Configuring DistributedData Extension Settings in reference.conf\nDESCRIPTION: Reference configuration snippet from reference.conf showing available configuration properties for the DistributedData extension in Akka.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_43\n\nLANGUAGE: conf\nCODE:\n```\n#distributed-data\n```\n\n----------------------------------------\n\nTITLE: Combining Pipelining and Parallel Processing in Java\nDESCRIPTION: This Java code snippet merges pipelining and parallelism using Akka Streams, following the cooking analogy of utilizing chefs and frying pans efficiently. The approach allows multiple sequential operations to run in parallel, maximizing throughput. Important dependencies include Akka Streams' Java API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-parallelism.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowParallelismDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowParallelismDocTest.java) { #parallel-pipeline }\n```\n\n----------------------------------------\n\nTITLE: Creating a Pull Mode TCP Listener in Java\nDESCRIPTION: A Java example showing how to create a TCP server that uses pull mode for accepting connections. The pullMode parameter of the TcpMessage.bind() method is set to true to enable pull-based connection accepting.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_23\n\nLANGUAGE: java\nCODE:\n```\nfinal ActorRef tcp = Tcp.get(system).manager();\ntcp.tell(\n    TcpMessage.bind(\n        listener, new InetSocketAddress(\"localhost\", 0), 100, Option.apply(Inet.SO.TCP_NODELAY()), true),\n    getSelf());\n```\n\n----------------------------------------\n\nTITLE: Querying Actors with ActorFlow.ask in Akka Streams - Java\nDESCRIPTION: Illustrates usage of the ask pattern with ActorFlow.ask in a Java Akka Stream, sending messages to a typed actor for each stream element and processing the reply. Requires akka-stream-typed dependency, an ActorRef for the target actor, a BiFunction to create the message, and a timeout duration. Takes input elements from the stream and emits replies, failing the stream on timeout or actor termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/ask.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// Definitions for command and reply\npublic class Asking {\n  private final ActorRef<Reply> replyTo;\n  public Asking(ActorRef<Reply> replyTo) {\n    this.replyTo = replyTo;\n  }\n  public ActorRef<Reply> getReplyTo() { return replyTo; }\n}\n\npublic class Reply {\n  private final String message;\n  public Reply(String message) { this.message = message; }\n  public String getMessage() { return message; }\n}\n\n// Actor behavior omitted for brevity\n\n// Stream logic\nActorRef<Asking> actorRef = ...; // Assume created via ActorSystem\nFlow<Integer, Reply, NotUsed> flow = ActorFlow.ask(\n  actorRef,\n  Duration.ofSeconds(3),\n  (Integer elem, ActorRef<Reply> replyTo) -> new Asking(replyTo)\n);\n\n// Using the flow in a runnable graph\nSource.range(1, 10).via(flow).map(Reply::getMessage).runWith(Sink.foreach(System.out::println), system);\n\n```\n\n----------------------------------------\n\nTITLE: Monotonically Increasing Timestamps using LwwTime Utility - Scala\nDESCRIPTION: This Scala snippet demonstrates the use of LwwTime's 'increase' method to ensure a monotonically increasing timestamp for last writer wins conflict resolution in Akka Persistence. The code would typically be within a command-handler, producing a new LwwTime instance that is greater than any previous time associated with the state or event. Dependencies include LwwTime from Akka libraries, and it's critical that the handler has access to the prior timestamp. The main input is the previous timestamp; output is a new, strictly greater timestamp, addressing potential clock skew and concurrent modification issues. The strategy guarantees that new events can be reliably ordered across replicas.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n/* @@snip [blog](/akka-persistence-typed-tests/src/test/scala/docs/akka/persistence/typed/ReplicatedBlogExampleSpec.scala) { #command-handler } */\n```\n\n----------------------------------------\n\nTITLE: Implementing Logging in Scala Actors\nDESCRIPTION: Shows how to create a LoggingAdapter and use logging methods in a Scala actor. The example demonstrates logging at different severity levels and includes parameter substitution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass MyActor extends Actor {\n  val log = Logging(context.system, this)\n\n  override def preStart() = {\n    log.debug(\"Starting\")\n  }\n  \n  override def preRestart(reason: Throwable, message: Option[Any]) {\n    log.error(reason, \"Restarting due to [{}] when processing [{}]\",\n      reason.getMessage, message.getOrElse(\"(no message)\"))\n  }\n\n  def receive = {\n    case \"test\" => log.info(\"Received test\")\n    case x      => log.warning(\"Received unknown message: {}\", x)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Managing Child Actors in Typed Akka (Java)\nDESCRIPTION: This Java snippet shows how to manage child actors in Typed Akka using a Map for bookkeeping. It includes methods for creating, retrieving, and removing child actors, as well as handling actor termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/from-classic.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nclass Parent extends AbstractBehavior<Parent.Command> {\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(Parent::new);\n  }\n\n  private final Map<String, ActorRef<Child.Command>> children = new HashMap<>();\n\n  private Parent(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Command.class, this::onCommand)\n        .onSignal(ChildTerminated.class, this::onChildTerminated)\n        .build();\n  }\n\n  private Behavior<Command> onCommand(Command msg) {\n    ActorRef<Child.Command> child = children.get(msg.name);\n    if (child != null) {\n      msg.replyTo.tell(child);\n    } else {\n      child = getContext().spawn(Child.create(), msg.name);\n      getContext().watchWith(child, new ChildTerminated(msg.name));\n      children.put(msg.name, child);\n      msg.replyTo.tell(child);\n    }\n    return this;\n  }\n\n  private Behavior<Command> onChildTerminated(ChildTerminated terminated) {\n    children.remove(terminated.name);\n    return this;\n  }\n\n  public static class Command {\n    public final String name;\n    public final ActorRef<ActorRef<Child.Command>> replyTo;\n\n    public Command(String name, ActorRef<ActorRef<Child.Command>> replyTo) {\n      this.name = name;\n      this.replyTo = replyTo;\n    }\n  }\n\n  private static class ChildTerminated {\n    public final String name;\n\n    private ChildTerminated(String name) {\n      this.name = name;\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Akka Actor with Lombok Message Class\nDESCRIPTION: Shows how to implement an Akka actor with an immutable message class using Lombok's @Value annotation. The actor includes a message handler and a static inner Message class.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/immutable.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class MyActor extends AbstractActor {\n        private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n    \n        public Receive createReceive() {\n            return receiveBuilder()\n            .match(Message.class, message -> {\n                System.out.println(message.getMessage());\n            })\n            .matchAny(o -> log.info(\"Received unknown message\"))\n            .build();\n        }\n    \n        @Value\n        public static class Message {\n            private String message;\n    \n            public Message(String message) {\n                this.message = message;\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Using prependLazy in Java Akka Streams\nDESCRIPTION: Example of using the prependLazy operator in Java to prepend one source to another, consuming the prepended source first before the original source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/prependLazy.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// #prependLazy\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Extractors for Sharding\nDESCRIPTION: Demonstrates implementation of extractEntityId and extractShardId functions to handle message routing in cluster sharding. Shows different ways to define entity identifiers in messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n#counter-extractor\n```\n\nLANGUAGE: Java\nCODE:\n```\n#counter-extractor\n```\n\n----------------------------------------\n\nTITLE: Using Behaviors.withMdc for Custom MDC in Java\nDESCRIPTION: Shows how to use Behaviors.withMdc to add custom Mapped Diagnostic Context (MDC) attributes to log entries in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nBehavior<Protocol> behavior = Behaviors.setup(context -> \n  Behaviors.withMdc(\n    Protocol.class,\n    Collections.singletonMap(\"startTime\", Long.toString(System.currentTimeMillis())),\n    msg -> {\n      if (msg instanceof LoggedProtocol)\n        return Collections.singletonMap(\"messageId\", ((LoggedProtocol) msg).body.id);\n      else\n        return Collections.emptyMap();\n    },\n    Behaviors.receiveMessage(msg -> {\n      if (msg instanceof LoggedProtocol) {\n        context.getLog().info(\"Received message {}\", ((LoggedProtocol) msg).body.payload);\n      } else {\n        context.getLog().info(\"Received other message\");\n      }\n      return Behaviors.same();\n    })\n  )\n);\n```\n\n----------------------------------------\n\nTITLE: Context Propagation with Source.asSourceWithContext in Java\nDESCRIPTION: Shows how to extract correlation numbers as context from a Source of Pairs containing messages and correlation IDs in Java. Transforms a regular Source into a SourceWithContext to propagate correlation context alongside message elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/asSourceWithContext.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport akka.stream.javadsl.Source;\nimport akka.stream.javadsl.SourceWithContext;\nimport java.util.Arrays;\nimport org.apache.commons.lang3.tuple.Pair;\n\nSource<Pair<String, Integer>, NotUsed> source =\n    Source.from(Arrays.asList(\n        Pair.of(\"a\", 1),\n        Pair.of(\"b\", 2),\n        Pair.of(\"c\", 3)));\n\nSourceWithContext<String, Integer, NotUsed> contextualizedSource =\n    source.asSourceWithContext(Pair::getRight)  // extract context (correlation number)\n        .map(Pair::getLeft);  // keep only the message as element\n```\n\n----------------------------------------\n\nTITLE: Sending Stream Elements to ActorRef using Akka Streams (Java)\nDESCRIPTION: This snippet describes the use of ActorSink.actorRef in Akka Streams for Java to direct stream elements to an ActorRef. It sends a completion message upon successful stream termination and adapts failures using a designated function. Dependencies need to be included from the Akka library repository in respective build systems.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorSink/actorRef.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nactorRef(akka.actor.typed.ActorRef, java.lang.Object, akka.japi.function.Function)\n```\n\n----------------------------------------\n\nTITLE: Testing Timed Expectations with Akka TestKit - Scala\nDESCRIPTION: This snippet demonstrates the use of the `within` block and probe-based message expectations in Akka TestKit for Scala. It shows how each probe's timing deadline is isolated, meaning deadlines are local to that probe rather than shared. Required dependencies include Akka TestKit for Scala. Inputs are messages and probes; outputs are assertions based on message timings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [TestkitDocSpec.scala](/akka-docs/src/test/scala/docs/testkit/TestkitDocSpec.scala) { #test-within-probe }\n```\n\n----------------------------------------\n\nTITLE: Using SourceQueue in Java\nDESCRIPTION: Example showing how to use an unbounded SourceQueue with asynchronous feedback in Java. Demonstrates queue creation and element offering with CompletionStage results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/queue.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nfinal int bufferSize = 10;\nSourceQueue<Integer> queue =\n    Source.<Integer>queue(bufferSize, OverflowStrategy.dropHead())\n        .map(x -> x * x)\n        .to(Sink.foreach(x -> System.out.println(\"completed \" + x)))\n        .run(system);\n\nqueue\n    .offer(1)\n    .thenAccept(\n        result -> {\n          if (result.equals(QueueOfferResult.enqueued())) {\n            System.out.println(\"enqueued\");\n          } else if (result.equals(QueueOfferResult.dropped())) {\n            System.out.println(\"dropped\");\n          } else if (result instanceof QueueOfferResult.Failure) {\n            System.out.println(\n                \"Offer failed: \" + ((QueueOfferResult.Failure) result).cause().getMessage());\n          } else if (result.equals(QueueOfferResult.QueueClosed())) {\n            System.out.println(\"queue closed\");\n          }\n        });\n```\n\n----------------------------------------\n\nTITLE: Using Generic Response Wrapper in Akka Typed Actor-to-Actor ask (Scala)\nDESCRIPTION: This snippet demonstrates the use of the StatusReply generic response wrapper in Akka Typed for actor-to-actor ask patterns. It shows how to handle both successful responses and validation errors using askWithStatus.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [InteractionPatternsSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/InteractionPatternsSpec.scala) { #actor-ask-with-status }\n```\n\n----------------------------------------\n\nTITLE: Implementing Duplicate Transformation with GraphStage in Java\nDESCRIPTION: This snippet shows the Java implementation of a one-to-many operator using GraphStage. It creates a duplicate operator that emits each upstream element twice downstream, demonstrating state management within the operator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\npublic class Duplicator<A> extends GraphStage<FlowShape<A, A>> {\n  private final Inlet<A> in = Inlet.create(\"Duplicator.in\");\n  private final Outlet<A> out = Outlet.create(\"Duplicator.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      private boolean seen = false;\n\n      private void maybePull() {\n        if (!seen) pull(in);\n      }\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                A elem = grab(in);\n                seen = true;\n                push(out, elem);\n              }\n\n              @Override\n              public void onUpstreamFinish() {\n                if (!seen) completeStage();\n                else {\n                  // There is an element buffered, let it be sent downstream\n                  // before completion\n                }\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() {\n                if (isClosed(in)) {\n                  if (seen) {\n                    // We're done\n                    completeStage();\n                  } // else onUpstreamFinish completed us\n                } else if (seen) {\n                  seen = false;\n                  push(out, grab(in));\n                } else {\n                  maybePull();\n                }\n              }\n            });\n      }\n    };\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Resuming Actor on Specific Exception with Akka Typed Supervision (Scala)\nDESCRIPTION: Illustrates using `Behaviors.supervise` with `SupervisorStrategy.resume` in Scala to ignore an `IllegalStateException` and allow the actor to continue processing subsequent messages without restarting.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n// #resume\nval supervisedBehavior2: Behavior[String] =\n  Behaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.resume)\n// #resume\n```\n\n----------------------------------------\n\nTITLE: Implementing a DataBot with ORSet in Scala\nDESCRIPTION: An example actor that schedules tick messages to itself and for each tick adds or removes elements from an ORSet (observed-remove set). It also subscribes to changes of the data set.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass DataBot extends Actor with ActorLogging {\n  import DataBot._\n\n  val replicator = DistributedData(context.system).replicator\n  implicit val node: SelfUniqueAddress = DistributedData(context.system).selfUniqueAddress\n\n  import context.dispatcher\n  val tickTask = context.system.scheduler.scheduleWithFixedDelay(\n    5.seconds, 5.seconds, self, Tick)\n\n  val DataKey = ORSetKey[String](\"key\")\n\n  replicator ! Subscribe(DataKey, self)\n\n  def receive = {\n    case Tick =>\n      val s = ThreadLocalRandom.current().nextInt(97, 123).toChar.toString\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        // add\n        log.info(\"Adding: {}\", s)\n        replicator ! Update(DataKey, ORSet.empty[String], WriteLocal)(_ :+ s)\n      } else {\n        // remove\n        log.info(\"Removing: {}\", s)\n        replicator ! Update(DataKey, ORSet.empty[String], WriteLocal)(_.remove(s))\n      }\n\n    case c @ Changed(DataKey) =>\n      val data = c.get(DataKey)\n      log.info(\"Current elements: {}\", data.elements)\n  }\n\n  override def postStop(): Unit = tickTask.cancel()\n\n}\n\nobject DataBot {\n  def props: Props = Props(new DataBot)\n  private case object Tick\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Materialized Values for a Composite Source in Scala\nDESCRIPTION: This Scala snippet demonstrates creating a composite Akka Stream `Source`. It combines two sources, `source1` and `source2`, using `Source.combine`. The `Keep.left` combiner function ensures that the materialized value of the resulting composite source (`nestedSource`) is the materialized value of the first source (`source1`), which is a `Promise[[Option[Int]]]`. This allows external interaction (e.g., triggering the end of the stream) via the promise provided by the first source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\n//#mat-combine-1\n// Source requiring Promise materialize value\nval source1 = Source.maybe[Int]\n\n// Source requiring Future materialize value\nval source2 = Source.single(0)\n\n// Combine the sources, picking the Promise materialization\nval nestedSource: Source[Int, Promise[Option[Int]]] = Source.combine(source1, source2)(Merge(_)).named(\"nestedSource\")\n//#mat-combine-1\n```\n\n----------------------------------------\n\nTITLE: Actor Ask Pattern Implementation in Scala\nDESCRIPTION: Example showing how to use the ask pattern to delegate stream processing to an actor while maintaining backpressure. Includes both stream and actor implementation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/actor-interop.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsealed trait Message\ncase class Req(x: Int) extends Message\ncase class Res(x: Int) extends Message\n\nclass ExampleActor extends Actor {\n  override def receive = {\n    case Req(x) =>\n      sender() ! Res(x)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Echo Handler Actor in Java\nDESCRIPTION: This Java snippet defines a `SimpleEchoHandler` actor responsible for handling a single TCP connection. It implements an echo server logic with ACK-based back-pressure: it receives data (`Tcp.Received`), buffers it, writes it back using `TcpMessage.write` with a custom `Ack` event, and waits for the `Ack` before sending subsequent data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\npublic class SimpleEchoHandler extends AbstractActor {\n\n  private final LoggingAdapter log = Logging.getLogger(getContext().system(), this);\n\n  private final ActorRef connection;\n  private final Vector<ByteString> storage = new Vector<>();\n  private long stored = 0L;\n  private long transferred = 0L;\n  private static final long maxStored = 100000000L;\n  private boolean closed = false;\n  private int nextAck = 1;\n\n  public static Props props(ActorRef connection) {\n    return Props.create(SimpleEchoHandler.class, connection);\n  }\n\n  // this is the ACK object explained in the docs\n  public static final class Ack implements Tcp.Event {\n    private final int ack;\n\n    public Ack(int ack) {\n      this.ack = ack;\n    }\n\n    public static Ack create(int ack) {\n      return new Ack(ack);\n    }\n\n    @Override\n    public boolean equals(Object o) {\n      if (this == o) return true;\n      if (o == null || getClass() != o.getClass()) return false;\n      Ack ack1 = (Ack) o;\n      return ack == ack1.ack;\n    }\n\n    @Override\n    public int hashCode() {\n      return Objects.hash(ack);\n    }\n\n    @Override\n    public String toString() {\n      return \"Ack(\" + ack + ')';\n    }\n  }\n\n  public SimpleEchoHandler(ActorRef connection) {\n    this.connection = connection;\n\n    // sign death pact: this actor stops when the connection is closed\n    getContext().watch(connection);\n\n    connection.tell(TcpMessage.resumeReading(), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Tcp.Received.class,\n            data -> {\n              buffer(data.data());\n            })\n        .match(\n            Ack.class,\n            ack -> {\n              acknowledge(ack.ack);\n            })\n        .match(\n            Tcp.ConnectionClosed.class,\n            closed -> {\n              if (storage.isEmpty()) {\n                getContext().stop(getSelf());\n              } else {\n                getContext().become(closing());\n              }\n            })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Messages in Akka Pool Router using Scala\nDESCRIPTION: This snippet illustrates how to configure a pool router in Scala to broadcast messages to all routees using a broadcast predicate. Ensure that Akka Typed is correctly configured in your Scala project.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n/* Broadcasting configuration for pool router in RouterSpec.scala */\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradle Shadow Plugin for Akka Fat Jar\nDESCRIPTION: Gradle configuration using the Shadow plugin to create a fat jar that correctly merges reference.conf resources for Akka applications.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/packaging.md#2025-04-22_snippet_3\n\nLANGUAGE: groovy\nCODE:\n```\nimport com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer\n\nplugins {\n    id 'java'\n    id \"com.github.johnrengelman.shadow\" version \"7.0.0\"\n}\n\nshadowJar {\n    append 'reference.conf'\n    append 'version.conf'\n    with jar\n}\n```\n\n----------------------------------------\n\nTITLE: Recommended Factory Methods for Props in Java\nDESCRIPTION: Demonstrates the recommended approach for creating Props using static factory methods in Java, which helps maintain actor encapsulation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\npublic class DemoActor extends AbstractActor {\n\n  /**\n   * Create Props for an actor of this type.\n   * @param magicNumber The magic number to be passed to this actor's constructor.\n   * @return a Props for creating this actor, which can then be further configured\n   *         (e.g. calling `.withDispatcher()` on it)\n   */\n  public static Props props(final int magicNumber) {\n    return Props.create(DemoActor.class, () -> new DemoActor(magicNumber));\n  }\n\n  private final int magicNumber;\n\n  public DemoActor(int magicNumber) {\n    this.magicNumber = magicNumber;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(Integer.class, i -> {\n          getSender().tell(i + magicNumber, getSelf());\n        })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Master-Worker Pattern with Graceful Stopping in Akka Typed (Java)\nDESCRIPTION: Java version of the master-worker pattern implementation, showing how to manage worker actors and handle graceful stopping.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.typed.Behavior;\nimport akka.actor.typed.PostStop;\nimport akka.actor.typed.javadsl.Behaviors;\n\nclass Master {\n  interface Command {}\n  public static class StartJob implements Command {\n    public final int jobId;\n    public StartJob(int jobId) {\n      this.jobId = jobId;\n    }\n  }\n  public static class WorkerDone implements Command {\n    public final int result;\n    public WorkerDone(int result) {\n      this.result = result;\n    }\n  }\n  public enum Stop implements Command {\n    INSTANCE\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(\n        context -> {\n          return Behaviors.receive(Command.class)\n              .onMessage(\n                  StartJob.class,\n                  message -> {\n                    Behavior<Worker.Command> worker =\n                        context.spawn(Worker.create(), \"worker-\" + message.jobId);\n                    worker.tell(new Worker.DoWork(message.jobId));\n                    return Behaviors.same();\n                  })\n              .onMessage(\n                  WorkerDone.class,\n                  message -> {\n                    System.out.println(\"Worker is done. Result \" + message.result);\n                    return Behaviors.same();\n                  })\n              .onMessage(Stop.class, message -> Behaviors.stopped())\n              .onSignal(PostStop.class, signal -> {\n                System.out.println(\"Master stopped\");\n                return Behaviors.same();\n              })\n              .build();\n        });\n  }\n}\n\nclass Worker {\n  interface Command {}\n  public static class DoWork implements Command {\n    public final int jobId;\n    public DoWork(int jobId) {\n      this.jobId = jobId;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.receive(Command.class)\n        .onMessage(\n            DoWork.class,\n            message -> {\n              // do some work ...\n              System.out.println(\"Worker \" + message.jobId + \" is done\");\n              message.replyTo.tell(new Master.WorkerDone(message.jobId));\n              return Behaviors.stopped();\n            })\n        .onSignal(PostStop.class, signal -> {\n          System.out.println(\"Worker stopped\");\n          return Behaviors.same();\n        })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using expand for video frame handling in Akka Streams\nDESCRIPTION: This example demonstrates using the expand operator to maintain stream fluency in a video decoding scenario where network bandwidth might be unreliable. It watermarks decoded frames with a colleague's name and can create extra frames when the producer slows down.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/expand.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval videoDecoder = Flow[VideoChunk].expand { chunk =>\n  if (isBlank(chunk)) {\n    // network hickup, generate a placeholder frame\n    Iterator.single(watermark(generatePlaceholderFrame()))\n  } else {\n    // got a good chunk, decode all frames\n    decodeFrames(chunk).map(watermark)\n  }\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nFlow<VideoChunk, Frame, NotUsed> videoDecoder =\n    Flow.of(VideoChunk.class)\n        .expand(\n            chunk -> {\n              if (isBlank(chunk)) {\n                // network hickup, generate a placeholder frame\n                return Collections.singletonList(watermark(generatePlaceholderFrame())).iterator();\n              } else {\n                // got a good chunk, decode all frames\n                List<Frame> frames = decodeFrames(chunk);\n                return frames.stream().map(this::watermark).collect(Collectors.toList()).iterator();\n              }\n            });\n```\n\n----------------------------------------\n\nTITLE: Implementing EventsByTagSource in Scala\nDESCRIPTION: A GraphStage implementation in Scala that provides the eventsByTag functionality for a custom ReadJournal. This source can be used to query events by tag from a persistence store.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\nfinal class MyEventsByTagSource(tag: String, offset: Offset) extends GraphStage[SourceShape[EventEnvelope]] {\n  val out: Outlet[EventEnvelope] = Outlet(\"MyEventsByTagSource\")\n  override val shape: SourceShape[EventEnvelope] = SourceShape(out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {\n    // this query would be completed after processing the events that matched the tag that existed at the time the query was started.\n    // A Later query can include events that were stored after the original query was completed.\n\n    var completeAfterIdle: Option[Cancellable] = None\n\n    override def preStart(): Unit = {\n      completeAfterIdle = scheduleWithFixedDelay(1.second, 1.second, () => {\n        if (isAvailable(out))\n          getStoreEvents().foreach(push(out, _))\n      })\n    }\n\n    private def getStoreEvents(): List[EventEnvelope] = {\n      // retrieve events from the data store\n      // query events by tag, offset\n      List.empty\n    }\n\n    override def postStop(): Unit = {\n      completeAfterIdle.foreach(_.cancel())\n    }\n\n    setHandler(out, new OutHandler {\n      override def onPull(): Unit = {\n        getStoreEvents().foreach(push(out, _))\n      }\n    })\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Java alsoTo API Signature\nDESCRIPTION: Method signature for the alsoTo operator in Java, showing how to attach a Sink to a Flow or Source\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/alsoTo.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nalsoTo(akka.stream.Graph)\n```\n\n----------------------------------------\n\nTITLE: Using scanAsync to Process Streams Asynchronously in Java\nDESCRIPTION: Example showing scanAsync usage in Java to maintain state across asynchronous operations. The code demonstrates how intermediate values are emitted after each element is processed, unlike fold which only emits the final result.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/scanAsync.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource.range(1, 5)\n    .scanAsync(\n        0,\n        (sum, element) -> CompletableFuture.completedFuture(sum + element))\n    // Result: 0, 1, 3, 6, 10, 15\n    .runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Persistence Dependency\nDESCRIPTION: Dependency configuration for including Akka Persistence module in sbt, Maven, or Gradle projects.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_7\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-persistence-typed_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Decompressing Gzipped ByteString Streams (Akka Streams Java)\nDESCRIPTION: Demonstrates Java Akka Streams techniques to decompress gzipped ByteStrings using the Compression class. Typically employed in file IO or network processing where incoming data is compressed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeDecompress.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeDecompress.java) { #decompress-gzip }\n```\n\n----------------------------------------\n\nTITLE: Handling Back-pressure with Buffering in Akka Streams (Java)\nDESCRIPTION: Shows how to use the `buffer` operator in Java to manage back-pressure. It configures a buffer of size 10 with `OverflowStrategy.dropHead()` to discard the oldest element if the buffer fills up due to a slow downstream process, prioritizing recent data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_37\n\nLANGUAGE: java\nCODE:\n```\ntweets\n  .buffer(10, OverflowStrategy.dropHead())\n  .map(this::slowComputation)\n  .runWith(Sink.ignore(), materializer);\n//#tweets-slow-consumption-dropHead\n```\n\n----------------------------------------\n\nTITLE: Running the Akka Supervision Example in Java\nDESCRIPTION: This Java snippet demonstrates how to run the actor supervision experiment. It creates an `ActorSystem` using the `SupervisingActor` behavior. It then sends a \"failChild\" message to the `SupervisingActor`, which in turn tells its child (`SupervisedActor`) to fail, thus triggering the restart supervision strategy.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nfinal ActorSystem<String> system =\n    ActorSystem.create(SupervisingActor.create(), \"ActorHierarchyExperiments\");\nsystem.tell(\"failChild\");\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with askWithStatus Operator (Akka Streams, Scala)\nDESCRIPTION: This Scala snippet demonstrates usage of ActorFlow.askWithStatus to send stream elements to an Akka Typed actor using the ask pattern and extract StatusReply results. Requires Akka Stream Typed and Akka Typed dependencies from the Akka library repository. The example assumes an actor that handles messages expecting an actor reference for replies and emits either a String upon success or fails on timeout/termination. Key parameters include the actor ref, a message creation function, and an implicit Timeout. The input is a stream of messages; the output is a stream of unwrapped responses of type String. Limitations include stream failure if any ask times out or the actor terminates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithStatus.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.typed.ActorRef\nimport akka.pattern.StatusReply\nimport akka.stream.scaladsl.{Flow, Sink, Source}\nimport scala.concurrent.duration._\nimport akka.util.Timeout\n\n// Actor message definition\ndefinition AskingWithStatus(replyTo: ActorRef[StatusReply[String]])\n\n// Example actor (simplified)\nclass MyActor extends akka.actor.typed.scaladsl.Behaviors.Receive[AskingWithStatus] {\n  // handle AskingWithStatus: reply with StatusReply.Success(\"done\")\n}\n\nval actorRef: ActorRef[AskingWithStatus] = ???\nimplicit val timeout: Timeout = 3.seconds\nval parallelism = 5\n\nval flow: Flow[Any, String, akka.NotUsed] =\n  akka.stream.typed.scaladsl.ActorFlow.askWithStatus[Any, AskingWithStatus, String](parallelism)(actorRef) { (in, replyTo) =>\n    AskingWithStatus(replyTo)\n  }\n\n// Usage in a stream:\nSource.single(\"start-request\").via(flow).map(identity).runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Merging Substreams with Parallelism Limit in Scala\nDESCRIPTION: Demonstrates merging substreams created by `groupBy` while limiting the number of concurrently active substreams using `mergeSubstreamsWithParallelism`. This can control resource usage but might lead to deadlocks if not handled carefully.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n//#groupBy4\nsource\n  .groupBy(maxSubstreams = 3, _ % 3)\n  .mergeSubstreamsWithParallelism(parallelism = 2)\n  .runWith(Sink.fold(0)((acc, i) => acc + i))\n//#groupBy4\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to a Sharded Entity in Scala\nDESCRIPTION: Illustrates how to obtain an `EntityRef` for a specific entity ID and send a message (`Increment`) to it using the `tell` (`!`) operator in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.cluster.sharding.typed.scaladsl.EntityRef\n\n// #send\n// constructor takes entityId and TypeKey\nval counterOne: EntityRef[Counter.Command] = sharding.entityRefFor(TypeKey, \"counter-1\")\ncounterOne ! Counter.Increment\n// #send\n```\n\n----------------------------------------\n\nTITLE: Defining Routee Behavior in Akka Scala\nDESCRIPTION: This snippet demonstrates how to define a routee behavior in Scala for a pool router in Akka Typed. The routee is a child actor that receives messages from the pool router. Ensure Akka dependencies are included and configured correctly in your build tool.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n/* Scala routee definition in RouterSpec.scala */\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Room Client Actor in Akka Typed - Java\nDESCRIPTION: Illustrates how to set up a client actor for a chat room using Akka Typed in Java. It covers session management and message posting.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/IntroTest.java) { #chatroom-gabbler }\n```\n\n----------------------------------------\n\nTITLE: Implementing At-Least-Once Delivery in Scala\nDESCRIPTION: Example of using AtLeastOnceDelivery trait in a Scala PersistentActor to implement at-least-once message delivery. It demonstrates persisting events, handling confirmations, and managing delivery state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_32\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [PersistenceDocSpec.scala](/akka-docs/src/test/scala/docs/persistence/PersistenceDocSpec.scala) { #at-least-once-example }\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Pool Router in Java\nDESCRIPTION: This snippet shows how to configure a pool router in Akka using Java. The pool router forwards messages to a set of routees and handles routee supervision. Ensure Akka Typed is included in your project dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n/* Pool router configuration in RouterTest.java */\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipelining in Akka Streams with Java\nDESCRIPTION: This Java snippet implements pipelining in Akka Streams, similar to the Scala version. It visualizes Roland's cooking method with two frying pans, emphasizing sequential execution. Dependencies include Akka Streams' Java API. The primary focus is on demonstrating the sequential execution of streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-parallelism.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowParallelismDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowParallelismDocTest.java) { #pipelining }\n```\n\n----------------------------------------\n\nTITLE: Using SourceQueue in Scala\nDESCRIPTION: Example showing how to use an unbounded SourceQueue with asynchronous feedback in Scala. Demonstrates queue creation and element offering with Future results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/queue.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval bufferSize = 10\nval queue = Source\n  .queue[Int](bufferSize, OverflowStrategy.dropHead)\n  .map(x => x * x)\n  .toMat(Sink.foreach(x => println(s\"completed $x\")))(Keep.left)\n  .run()\n\nqueue.offer(1).map {\n  case QueueOfferResult.Enqueued    => println(\"enqueued\")\n  case QueueOfferResult.Dropped     => println(\"dropped\")\n  case QueueOfferResult.Failure(ex) => println(s\"Offer failed: $ex\")\n  case QueueOfferResult.QueueClosed => println(\"queue closed\")\n}\n```\n\n----------------------------------------\n\nTITLE: Using mergeSorted Operator in Java\nDESCRIPTION: Example of using the mergeSorted operator in Java to merge multiple sorted sources with a custom comparator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergeSorted.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer> sourceA = Source.from(Arrays.asList(1, 3, 5, 7));\nSource<Integer> sourceB = Source.from(Arrays.asList(2, 4, 6, 8));\n\nsourceA.mergeSorted(sourceB, Comparator.<Integer>naturalOrder())\n       .runForeach(System.out::println, materializer);\n// prints 1, 2, 3, 4, 5, 6, 7, 8\n```\n\n----------------------------------------\n\nTITLE: Configuring Retention Criteria for Snapshots in Akka\nDESCRIPTION: Sets up retention criteria for automatically saving snapshots every 100 events and keeping the last 2 snapshots.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-snapshot.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nRetentionCriteria.snapshotEvery(100, 2)\n```\n\n----------------------------------------\n\nTITLE: Adapted Response Pattern in Scala and Java\nDESCRIPTION: Demonstrates how to use message adapters to translate between different actor message protocols, allowing an actor to receive responses it normally wouldn't support in its protocol.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n  val bookstore: ActorRef[Bookstore.Command] = ???\n  val adapted: ActorRef[Bookstore.BookStored] =\n    context.messageAdapter(e => WrappedBookStoreResponse(e))\n  bookstore ! Bookstore.StoreBook(\"Akka in Action\", adapted)\n```\n\nLANGUAGE: java\nCODE:\n```\n  ActorRef<Bookstore.Command> bookstore = ???;\n  ActorRef<Bookstore.BookStored> adapted =\n      context.messageAdapter(Bookstore.BookStored.class, WrappedBookStoreResponse::new);\n  bookstore.tell(new Bookstore.StoreBook(\"Akka in Action\", adapted));\n```\n\n----------------------------------------\n\nTITLE: Implementing Effects in Akka Persistent Actors - Java\nDESCRIPTION: A Java example detailing the implementation of effects in Akka persistence that manage event persistence and command processing outcomes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BasicPersistentBehaviorTest.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/BasicPersistentBehaviorTest.java) { #effects }\n```\n\n----------------------------------------\n\nTITLE: Adding Features to Publish-Subscribe Channel in Java\nDESCRIPTION: This snippet enhances the publish-subscribe channel by attaching a Sink.ignore to keep it drained when there are no subscribers. It's part of building a more feature-rich publish-subscribe service in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nPair<Sink<String>, Source<String, NotUsed>> sinkAndSource =\n  MergeHub.of(String.class, 16)\n    .toMat(BroadcastHub.of(String.class, 256), Keep.both())\n    .run(mat);\n\n// Ensure that the Broadcast output is dropped if there are no listeners\nsinkAndSource.second().runWith(Sink.ignore(), mat);\n```\n\n----------------------------------------\n\nTITLE: Custom Mailbox Implementation in Scala\nDESCRIPTION: Complete example of a custom mailbox implementation with unbounded queue in Scala, showing how to define the mailbox type and message queue.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/mailboxes.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.ActorRef\nimport akka.actor.ActorSystem\nimport akka.dispatch.{Envelope, MailboxType, MessageQueue, ProducesMessageQueue}\nimport com.typesafe.config.Config\nimport java.util.concurrent.ConcurrentLinkedQueue\n\n// This is the MessageQueue implementation\nclass MyUnboundedMessageQueue extends MessageQueue\n    with MyUnboundedMessageQueueSemantics {\n\n  private final val queue = new ConcurrentLinkedQueue[Envelope]()\n\n  // these should be implemented; queue used as example\n  def enqueue(receiver: ActorRef, handle: Envelope): Unit = {\n    queue.offer(handle)\n  }\n  def dequeue(): Envelope = queue.poll()\n  def numberOfMessages: Int = queue.size\n  def hasMessages: Boolean = !queue.isEmpty\n  def cleanUp(owner: ActorRef, deadLetters: MessageQueue): Unit = {\n    while (hasMessages) {\n      deadLetters.enqueue(owner, dequeue())\n    }\n  }\n}\n\n// This is the Mailbox implementation\nclass MyUnboundedMailbox extends MailboxType\n  with ProducesMessageQueue[MyUnboundedMessageQueue] {\n\n  // This constructor signature must exist, it will be called by Akka\n  def this(settings: ActorSystem.Settings, config: Config) = {\n    // put your initialization code here\n    this()\n  }\n\n  // The create method is called to create the MessageQueue\n  final override def create(\n      owner: Option[ActorRef],\n      system: Option[ActorSystem]): MessageQueue = new MyUnboundedMessageQueue()\n}\n```\n\n----------------------------------------\n\nTITLE: Handling PreRestart Signal in Akka Typed Actor (Java)\nDESCRIPTION: This Java snippet showcases an Akka Typed actor that catches the PreRestart signal, enabling application-specific resource cleanup before the actor restarts under supervision. It implements a signal handler using ReceiveBuilder, where PreRestart triggers cleanup logic. Dependencies: Akka Typed (Java); expected parameters are standard signal notifications; key output is cleanup and actor's continued operation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nreturn Behaviors\n    .receive(String.class)\n    .onSignal(PreRestart.class, signal -> {\n        // cleanup resources\n        return Behaviors.same();\n    })\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Source GraphStage (Scala)\nDESCRIPTION: This Scala snippet outlines the boilerplate for defining a custom Source operator by subclassing GraphStage. The code shows how to declare output ports and shapes, establishing the contract for data emission. Key aspects include class immutability and preparing for implementation of the createLogic method, which later handles element emission.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #boilerplate-example }\n```\n\n----------------------------------------\n\nTITLE: Using UniqueKillSwitch and SharedKillSwitch in Akka - Java\nDESCRIPTION: Java snippets illustrating the use of UniqueKillSwitch and SharedKillSwitch in Akka Streams. UniqueKillSwitch is for a single materialized graph control, whereas SharedKillSwitch handles multiple FlowShape operators. Demonstrations include shutdown and abort techniques. Key prerequisites are the Akka Streams library.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [KillSwitchDocTest.java](/akka-docs/src/test/java/jdocs/stream/KillSwitchDocTest.java) { #unique-shutdown }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [KillSwitchDocTest.java](/akka-docs/src/test/java/jdocs/stream/KillSwitchDocTest.java) { #unique-abort }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [KillSwitchDocTest.java](/akka-docs/src/test/java/jdocs/stream/KillSwitchDocTest.java) { #shared-shutdown }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [KillSwitchDocTest.java](/akka-docs/src/test/java/jdocs/stream/KillSwitchDocTest.java) { #shared-abort }\n```\n\n----------------------------------------\n\nTITLE: Logging Messages with Behaviors.logMessages in Scala\nDESCRIPTION: Demonstrates how to use Behaviors.logMessages to enable detailed logging of messages and signals for a specific actor behavior in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nval behavior: Behavior[String] =\n  Behaviors.setup { context =>\n    Behaviors.logMessages(\n      LogOptions().withLevel(LogLevel.INFO).withSeparateLogger,\n      Behaviors.receiveMessage[String] { message =>\n        // ... handle message\n        Behaviors.same\n      }\n    )\n  }\n```\n\n----------------------------------------\n\nTITLE: Cluster Sharding Documentation\nDESCRIPTION: Documentation describing the Cluster Sharding feature for distributing actors across cluster nodes with logical identifier access.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/includes/cluster.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n### Cluster Sharding\n\nDistributes actors across several nodes in the cluster and supports interaction\nwith the actors using their logical identifier, but without having to care about\ntheir physical location in the cluster.\n```\n\n----------------------------------------\n\nTITLE: Recovering with Retries (Scala)\nDESCRIPTION: Demonstrates using `recoverWithRetries` in Scala. When an `IllegalArgumentException` occurs, it attempts to recover by replacing the failed source with a new source (`Source.fromIterator(() => List(5, 6).iterator)`) up to one time. The stream then continues with elements from the new source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nval planB = Source(List(\"five\", \"six\"))\n\nSource(0 to 6)\n  .map(n =>\n    if (n < 5) n.toString\n    else throw new IllegalArgumentException(\"Boom!\"))\n  .recoverWithRetries(\n    attempts = 1,\n    {\n      case _: IllegalArgumentException => planB\n    })\n  .runForeach(println)\n```\n\n----------------------------------------\n\nTITLE: Creating a Balanced but Deadlocked Cycle Using ZipWith in Akka Streams (Java)\nDESCRIPTION: A Java implementation attempting to create a balanced cycle by replacing Merge with ZipWith. While this should maintain the balance of elements, it creates a chicken-and-egg problem where no processing can start because each junction needs an element from the other.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_28\n\nLANGUAGE: Java\nCODE:\n```\nfinal RunnableGraph<NotUsed> zipping =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final FanInShape2<Integer, Integer, Integer> zip =\n                  b.add(ZipWith.create((Integer left, Integer right) -> right));\n              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n\n              b.from(b.add(source)).toInlet(zip.in0());\n              b.from(zip.out())\n                  .via(b.add(Flow.of(Integer.class).map(s -> { System.out.println(s); return s; })))\n                  .viaFanOut(bcast)\n                  .to(b.add(Sink.ignore()));\n              b.from(bcast).toInlet(zip.in1());\n              return ClosedShape.getInstance();\n            }));\n```\n\n----------------------------------------\n\nTITLE: Defining an Akka Extension in Scala\nDESCRIPTION: Creates a simple Counter extension that tracks the number of occurrences. The Extension trait defines the functionality while the CountExtension class extends ExtensionId to provide access to the extension instance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nclass Counter extends Extension {\n  //Since this Extension is a shared instance\n  // per ActorSystem we need to be threadsafe\n  private val counter = new AtomicLong(0)\n\n  //This is the operation this Extension provides\n  def increment() = counter.incrementAndGet()\n}\n```\n\n----------------------------------------\n\nTITLE: Terminate Source Run in Java\nDESCRIPTION: In Java, this snippet demonstrates running an Akka Stream and utilizing a CompletionStage for stream completion and finalization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [QuickStartDocTest.java](/akka-docs/src/test/java/jdocs/stream/QuickStartDocTest.java) { #run-source-and-terminate }\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.maybe in Scala\nDESCRIPTION: Example showing how to use Source.maybe operator in Scala to create a source that emits a single value when a Promise[Option[T]] is completed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/maybe.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source.maybe[Int]\nval promise = source.run()\n// complete with value\npromise.success(Some(42))\n// or complete with no value\npromise.success(None)\n// or fail the promise\npromise.failure(new RuntimeException())\n```\n\n----------------------------------------\n\nTITLE: Prepending a Source to a Flow in Scala\nDESCRIPTION: Example of using the prepend operator to combine two sources in Scala. It demonstrates prepending a source of integers to another source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/prepend.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval sourceToPrepend = Source(List(\"A\", \"B\", \"C\"))\nval sourceToAppendTo = Source(List(1, 2, 3))\nval resultSource = sourceToAppendTo.prepend(sourceToPrepend)\nresultSource.runWith(Sink.seq).futureValue should be(Seq(\"A\", \"B\", \"C\", 1, 2, 3))\n```\n\n----------------------------------------\n\nTITLE: Safe Shutdown Implementation for Persistent Actors\nDESCRIPTION: Shows the implementation of safe shutdown mechanisms for persistent actors, demonstrating proper message handling and stashing behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_23\n\nLANGUAGE: scala\nCODE:\n```\n#safe-shutdown\n```\n\nLANGUAGE: java\nCODE:\n```\n#safe-shutdown\n```\n\n----------------------------------------\n\nTITLE: Querying Current Persistence IDs Snapshot in Scala\nDESCRIPTION: Demonstrates how to get a non-live snapshot of all current persistence IDs using the currentPersistenceIds query. This stream completes when all current IDs have been delivered.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval persistenceIds = readJournal.currentPersistenceIds()\n\npersistenceIds.runForeach { id =>\n  println(s\"We have persistence id: $id\")\n}\n```\n\n----------------------------------------\n\nTITLE: Merging Price and Quantity Streams with MergeLatest in Java\nDESCRIPTION: Java implementation demonstrating how to use mergeLatest to combine price and quantity streams, emitting updates when either value changes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergeLatest.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#mergeLatest\n```\n\n----------------------------------------\n\nTITLE: DNS Service Lookup in Java\nDESCRIPTION: Example of using DNS-based service discovery in Java. This code looks up a service using the configured DNS resolver and returns the resolved targets.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nServiceDiscovery discovery = Discovery.get(system).discovery();\nCompletionStage<Resolved> result = discovery.lookup(\n    Lookup.create(\"service-name\").withPortName(\"http\").withProtocol(\"tcp\"),\n    Duration.ofSeconds(1));\n```\n\n----------------------------------------\n\nTITLE: Messaging Patterns for Akka Cluster\nDESCRIPTION: Defines messages used for communication between frontend and backend nodes in an Akka cluster. The messages facilitate the service provided by backend nodes and must be consistent across all nodes. Dependencies include Akka libraries for cluster management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [TransformationMessages.scala](/akka-docs/src/test/scala/docs/cluster/TransformationMessages.scala) { #messages }\n```\n\n----------------------------------------\n\nTITLE: Joining Seed Nodes Programmatically in Scala\nDESCRIPTION: Shows how to programmatically join an Akka cluster by specifying seed nodes. This approach is useful when dynamically discovering other nodes at startup using external tools or APIs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval seedNodes = List(\n  \"akka://ClusterSystem@host1:2552\",\n  \"akka://ClusterSystem@host2:2552\")\n  .map(s => Address(s))\n\nCluster(system).joinSeedNodes(seedNodes)\n```\n\n----------------------------------------\n\nTITLE: Converting Akka Stream to Java Stream in Scala\nDESCRIPTION: Example showing how to create a Sink that materializes into a Java 8 Stream using StreamConverters.asJavaStream in Scala. The stream blocks the current thread while waiting for next element and closes when the input stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/asJavaStream.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n#import #asJavaStream\n```\n\n----------------------------------------\n\nTITLE: Sending Emails Asynchronously and Consuming the Stream (Scala)\nDESCRIPTION: Constructs an email for each address in the `emailAddresses` stream and uses `mapAsync` to send them via the `EmailServer`. The `parallelism` factor (set to 4) controls concurrent sends. Finally, `Sink.ignore` consumes the stream, triggering its execution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nval sendEmails: RunnableGraph[NotUsed] =\n  emailAddresses\n    .mapAsync(4)(address => {\n      EmailServer.send(Email(to = address, title = \"Akka\", body = \"I like your tweet\"))\n    })\n    .to(Sink.ignore)\n\nsendEmails.run()\n```\n\n----------------------------------------\n\nTITLE: Adding Time-Based Processing to Streams in Scala\nDESCRIPTION: Demonstrates combining two streams (factorials and numbers) with throttling to produce time-based output in Scala. Shows usage of zip, map, and throttle operators.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\nfactorials\n  .zipWith(Source(0 to 100))((num, idx) => s\"$idx! = $num\")\n  .throttle(1, 1.second)\n  .runForeach(println)\n```\n\n----------------------------------------\n\nTITLE: Calculating a Digest of a ByteString Stream (Akka Streams Scala)\nDESCRIPTION: Implements a custom GraphStage in Scala that computes a cryptographic digest (e.g., MD5, SHA-1) across a stream of ByteStrings using a mutable MessageDigest. Carefully manages demands and final calculation, emitting the digest only at stream completion without missing any disposition. Depends on java.security.MessageDigest.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeDigest.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeDigest.scala) { #calculating-digest }\n```\n\n----------------------------------------\n\nTITLE: Proper Usage of lazySource for Per-Materialization State in Scala\nDESCRIPTION: Example showing the correct usage of lazySource to create separate stateful instances for each stream materialization, avoiding shared mutable state issues.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazySource.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n#one-per-materialization\n```\n\n----------------------------------------\n\nTITLE: Basic Throttling in Akka Streams\nDESCRIPTION: Demonstrates how to throttle a video stream to 24 frames per second using Akka Streams throttle operator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/throttle.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.fromIterator(() => Iterator.from(1))\n    .throttle(elements = 24, per = 1.second)\n    .runWith(Sink.foreach(println))\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.fromIterator(() -> Stream.iterate(1, i -> i + 1).iterator())\n    .throttle(24, Duration.ofSeconds(1))\n    .runWith(Sink.foreach(System.out::println), system);\n```\n\n----------------------------------------\n\nTITLE: Setting Dispatcher for Akka Pool Router in Java\nDESCRIPTION: This snippet demonstrates how to configure dispatchers for pool routers in Akka using Java. It addresses configuring the Props used by the PoolRouter for its routees. Make sure to have Akka and relevant dispatcher settings in your configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n/* Dispatcher configuration for pool router in RouterTest.java */\n```\n\n----------------------------------------\n\nTITLE: Simple write protocol design for device actor in Akka Typed\nDESCRIPTION: Initial simple design for the write protocol that updates the device's temperature. This approach is incomplete as it doesn't account for message delivery guarantees in a distributed system.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\n#write-protocol-1\n```\n\nLANGUAGE: Java\nCODE:\n```\n#write-protocol-1\n```\n\n----------------------------------------\n\nTITLE: Limiting ByteString Data Size Using limitWeighted in Scala\nDESCRIPTION: Example demonstrating how to use limitWeighted to limit the total size of ByteString data from an untrusted source. This snippet uses the number of bytes in each ByteString element as the weight and accepts at most 10,000 bytes total.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/limitWeighted.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval untrustedSource: Source[ByteString, NotUsed] = Source.repeat(ByteString(\"x\" * 1000))\n\nval limited: Source[ByteString, NotUsed] = \n  untrustedSource\n    .limitWeighted(10000)(_.length) // max 10000 bytes\n\nval fut: Future[ByteString] = limited.runWith(Sink.fold(ByteString.empty)(_ ++ _))\nfut.failed.foreach(e => println(s\"Failed with $e\"))\n```\n\n----------------------------------------\n\nTITLE: Creating File Writing Sink with FileIO.toFile in Akka Streams (Scala)\nDESCRIPTION: Creates a sink that writes incoming ByteString elements to a specified file. It materializes a Future of IOResult containing the file size and any potential exceptions. This method is deprecated in favor of toPath.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/FileIO/toFile.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFileIO.toFile(f: java.io.File, options: Set[java.nio.file.OpenOption]): akka.stream.scaladsl.Sink[akka.util.ByteString, scala.concurrent.Future[akka.stream.IOResult]]\n```\n\n----------------------------------------\n\nTITLE: Grouping Elements by Weight in Akka Streams (Java)\nDESCRIPTION: This snippet shows how to use the groupedWeighted operator in Java to accumulate elements based on their weight and perform operations on the grouped results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/groupedWeighted.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource.range(1, 10)\n  .groupedWeighted(10, i -> i)\n  .map(group -> group.stream().mapToInt(i -> i).sum())\n  .runWith(Sink.foreach(System.out::println), system);\n\n// prints:\n// 10\n// 26\n// 19\n```\n\n----------------------------------------\n\nTITLE: Flow Creation for Gzip Decompression in Akka Streams - Scala\nDESCRIPTION: Creates a flow in Scala that decompresses a stream of gzip-encoded ByteStrings using Akka Streams. This snippet defines a method with a single argument, `maxBytesPerChunk`, which specifies the maximum length of the emitted ByteString. Dependencies include Akka Streams, and the expected input is a compressed stream of ByteStrings. The output is a decompressed stream of ByteStrings or an exception if the input is invalid.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Compression/gunzip.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n\"scala=\\\"#gunzip(maxBytesPerChunk:Int):akka.stream.scaladsl.Flow[akka.util.ByteString,akka.util.ByteString,akka.NotUsed]\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in sbt\nDESCRIPTION: Specifies the Akka library repository configuration for sbt build tool. This allows sbt to resolve Akka dependencies from the official repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\n----------------------------------------\n\nTITLE: Using Partial Graph in Linear DSL\nDESCRIPTION: Demonstrates how to use a partial graph component in a linear stream processing chain, showing the interoperability between complex and linear DSLs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\nSource(1 to 10).via(partial).runWith(Sink.foreach(println))\n```\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3, 4, 5)).via(partial).runWith(Sink.foreach(System.out::println), system)\n```\n\n----------------------------------------\n\nTITLE: Initiating Manual Passivation in an Akka Entity (Scala)\nDESCRIPTION: This Scala code demonstrates how an entity actor can initiate its own passivation process. It sends a `ClusterSharding.Passivate` message to its parent shard, specifying a `GoodByeCounter` message to be sent back to itself as a signal to stop. This allows for graceful shutdown and prevents message loss.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_23\n\nLANGUAGE: scala\nCODE:\n```\ncase class GoodByeCounter(id: String) extends CounterCommand\n\nval counterActor = Behaviors.receiveMessage[CounterCommand] { cmd =>\n  cmd match {\n    case Increment =>\n      println(s\"Counter ${context.self.path.name} incremented\")\n      Behaviors.same\n    case GetValue(replyTo) =>\n      replyTo ! 42\n      Behaviors.same\n    case PassivateCounter =>\n      context.log.info(\"Passivating\")\n      shard ! ClusterSharding.Passivate(context.self)\n      Behaviors.same\n    case GoodByeCounter(_) =>\n      // can do cleanup here, then stop\n      Behaviors.stopped\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Delay to Source in Akka Streams (Scala)\nDESCRIPTION: Applies a delay to elements in a Source. The delay duration and overflow strategy are specified as parameters. This operator is part of the timer-driven operators in Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/delay.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.delay(of: scala.concurrent.duration.FiniteDuration, strategy: akka.stream.DelayOverflowStrategy): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Projecting into Different Store using Reactive Streams in Akka - Scala\nDESCRIPTION: Scala implementation shows how to project events into a Reactive Streams compatible datastore using Akka persistence. This involves querying events from the read-journal and feeding them into the database driver's interface. The use of Akka Streams ensures scalability.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [PersistenceQueryDocSpec.scala](/akka-docs/src/test/scala/docs/persistence/query/PersistenceQueryDocSpec.scala) { #projection-into-different-store-rs }\n```\n\n----------------------------------------\n\nTITLE: Implementing Master-Worker Pattern with Graceful Stopping in Akka Typed (Scala)\nDESCRIPTION: Demonstrates how to implement a master actor that manages worker actors, including stopping workers and handling PostStop signals for cleanup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.typed.Behavior\nimport akka.actor.typed.PostStop\nimport akka.actor.typed.scaladsl.Behaviors\n\nobject Master {\n  sealed trait Command\n  case class StartJob(jobId: Int) extends Command\n  case class WorkerDone(result: Int) extends Command\n  case object Stop extends Command\n\n  def apply(): Behavior[Command] =\n    Behaviors.setup { context =>\n      Behaviors.receiveMessage[Command] {\n        case StartJob(jobId) =>\n          val worker = context.spawn(Worker(), s\"worker-$jobId\")\n          worker ! Worker.DoWork(jobId)\n          Behaviors.same\n        case WorkerDone(result) =>\n          println(s\"Worker is done. Result $result\")\n          Behaviors.same\n        case Stop =>\n          Behaviors.stopped\n      }.receiveSignal {\n        case (context, PostStop) =>\n          println(\"Master stopped\")\n          Behaviors.same\n      }\n    }\n}\n\nobject Worker {\n  sealed trait Command\n  case class DoWork(jobId: Int) extends Command\n\n  def apply(): Behavior[Command] =\n    Behaviors.receive { (context, message) =>\n      message match {\n        case DoWork(jobId) =>\n          // do some work ...\n          println(s\"Worker $jobId is done\")\n          context.parent ! Master.WorkerDone(jobId)\n          Behaviors.stopped\n      }\n    }.receiveSignal {\n      case (context, PostStop) =>\n        println(\"Worker stopped\")\n        Behaviors.same\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Materialized Values in GraphStages - Java\nDESCRIPTION: This Java snippet shows how to implement custom materialized values by extending AbstractGraphStageWithMaterializedValue. By overriding createLogicAndMaterializedValue, streams can yield a specific materialized outcome, like a future, necessitating non-blocking synchronization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [GraphStageDocTest.java](/akka-docs/src/test/java/jdocs/stream/GraphStageDocTest.java) { #materialized }\n```\n\n----------------------------------------\n\nTITLE: Obtaining SelfUniqueAddress for Akka Distributed Data in Scala\nDESCRIPTION: Shows the Scala code required to obtain the implicit `SelfUniqueAddress` from the `DistributedData` extension. This address uniquely identifies the node and is needed as an implicit parameter when interacting with the `Replicator`. The code is referenced from `ReplicatorDocSpec.scala`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ReplicatorSpec.scala](/akka-cluster-typed/src/test/scala/docs/akka/cluster/ddata/typed/scaladsl/ReplicatorDocSpec.scala) { #selfUniqueAddress }\n```\n\n----------------------------------------\n\nTITLE: Implementing Command Handler in Scala\nDESCRIPTION: Command handler logic for processing auction commands and emitting corresponding events. Handles bid offers, highest bid queries, and auction completion.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing-auction.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\ndef commandHandler(): (State, Command) => Effect[Event, State] = { (state, command) =>\n  command match {\n    case OfferBid(offer, replyTo) =>\n      if (state.isOpen) {\n        val bid = Bid(offer, timestamp(), context.system.name)\n        Effect.persist(BidRegistered(bid.offer, bid.timestamp, bid.fromReplica))\n          .thenRun(_ => replyTo ! bid)\n      } else {\n        Effect.none.thenRun(_ => replyTo ! state.highestBid.copy(offer = state.highestCounterOffer))\n      }\n    case GetHighestBid(replyTo) =>\n      Effect.none.thenRun(_ => replyTo ! state.highestBid.copy(offer = state.highestCounterOffer))\n    case AuctionFinished =>\n      if (state.isOpen)\n        Effect.persist(AuctionFinished(state.highestBid, state.highestCounterOffer))\n      else\n        Effect.none\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Remote Deployment Allow List Configuration\nDESCRIPTION: Configuration example showing how to enable and configure the remote deployment allow list feature for security.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_22\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.deployment.enable-allow-list = on\n```\n\n----------------------------------------\n\nTITLE: Creating Props for Actors in Java\nDESCRIPTION: Demonstrates various ways to create Props objects in Java, including using static creation methods and passing constructor arguments to actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n#import-props #creating-props\nimport akka.actor.Props;\n\nProps props1 = Props.create(MyActor.class);\nProps props2 = Props.create(MyActorWithArgs.class, \"arg1\", \"arg2\");\nProps props3 = Props.create(MyActorWithArgs.class, () -> new MyActorWithArgs(\"arg1\", \"arg2\"));\n```\n\n----------------------------------------\n\nTITLE: Managing Pull-Based Connection Accepting in Java\nDESCRIPTION: A Java example showing how to handle connection acceptance in pull mode. After processing a new connection, the actor resumes accepting more connections by sending a TcpMessage.resumeAccepting() message.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_25\n\nLANGUAGE: java\nCODE:\n```\nprivate final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\nprivate final ActorRef connectionsCounter;\nprivate final Class<?> handlerClass;\n\n@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n      .match(\n          Tcp.Bound.class,\n          msg -> {\n            getSender().tell(TcpMessage.resumeAccepting(1), getSelf());\n            log.info(\"Listening on {}\", msg.localAddress());\n            getContext().become(listening(getSender()));\n          })\n      .build();\n}\n\nprivate Receive listening(final ActorRef connectionTacker) {\n  return receiveBuilder()\n      .match(\n          Tcp.Connected.class,\n          conn -> {\n            final ActorRef handler =\n                getContext()\n                    .actorOf(\n                        Props.create(handlerClass, getSender(), conn.remoteAddress(), connectionsCounter));\n\n            getSender().tell(TcpMessage.register(handler, true, true), getSelf());\n\n            // after handling one connection we need to resume accepting again\n            connectionTacker.tell(TcpMessage.resumeAccepting(1), getSelf());\n          })\n      .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Logging with log() Operation (Akka Streams Scala)\nDESCRIPTION: Uses the log() operation in Akka Streams to gain more control over logging level and event granularity, including completion and failure signals. This approach is more suited for production as it integrates with Akka logging infrastructure. Outputs diagnostic info at specified log levels and requires Akka logging to be properly configured.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeLoggingElements.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeLoggingElements.scala) { #log-custom }\n```\n\n----------------------------------------\n\nTITLE: Using BoundedSourceQueue in Java\nDESCRIPTION: Example demonstrating how to use a BoundedSourceQueue with synchronous feedback in Java. Shows queue creation, offering elements, and handling the queue offer results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/queue.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nfinal int queueSize = 10;\nBoundedSourceQueue<Integer> queue =\n    Source.<Integer>queue(queueSize)\n        .map(x -> x * x)\n        .to(Sink.foreach(x -> System.out.println(\"completed \" + x)))\n        .run(system);\n\nQueueOfferResult result = queue.offer(1);\nif (result == QueueOfferResult.enqueued()) {\n  System.out.println(\"enqueued\");\n} else if (result == QueueOfferResult.dropped()) {\n  System.out.println(\"dropped\");\n} else if (result instanceof QueueOfferResult.Failure) {\n  System.out.println(\"Offer failed: \" + ((QueueOfferResult.Failure) result).cause());\n} else if (result == QueueOfferResult.QueueClosed()) {\n  System.out.println(\"queue closed\");\n}\n```\n\n----------------------------------------\n\nTITLE: Retry Pattern Usage in Java\nDESCRIPTION: Example demonstrating the retry() pattern in Java to retry a CompletionStage operation multiple times with delay between attempts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/futures.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n@@snip [FutureDocTest.java](/akka-docs/src/test/java/jdocs/future/FutureDocTest.java) { #imports #retry }\n```\n\n----------------------------------------\n\nTITLE: Context Propagation with Source.asSourceWithContext in Scala\nDESCRIPTION: Demonstrates extracting correlation numbers as context from a Source of tuples containing messages and correlation IDs. The example shows how to transform a regular Source into a SourceWithContext to propagate correlation context alongside message elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/asSourceWithContext.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(List((\"a\", 1), (\"b\", 2), (\"c\", 3)))\n\nval contextualizedSource = source\n  .asSourceWithContext[Int](_._2)  // extract context (correlation number)\n  .map(_._1)  // keep only the message as element\n```\n\n----------------------------------------\n\nTITLE: Configuring Recovery from Only Last Event in Akka Persistence\nDESCRIPTION: Shows how to optimize recovery by only replaying the last event instead of all events. This is an optimization technique supported by the R2DBC plugin that doesn't load snapshots.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_37\n\nLANGUAGE: Scala\nCODE:\n```\nval behavior = EventSourcedBehavior[Command, Event, State](\\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\\n  emptyState = State.empty,\\n  commandHandler = (state, cmd) => Effect.persist(???),\\n  eventHandler = (state, evt) => ???)\\n.withRecovery(Recovery.fromLastEvent())\n```\n\nLANGUAGE: Java\nCODE:\n```\nEventSourcedBehavior<Command, Event, State> behavior =\\n    EventSourcedBehavior.create(\\n            PersistenceId.ofUniqueId(\"abc\"),\\n            State.empty(),\\n            (state, command) -> Effect().persist(commandToEvent(command)),\\n            (state, event) -> handleEvent(state, event))\\n        .withRecovery(Recovery.fromLastEvent());\n```\n\n----------------------------------------\n\nTITLE: Restarting Actor with Rate Limits using Akka Typed Supervision (Java)\nDESCRIPTION: Shows how to configure a rate-limited restart strategy in Java using `SupervisorStrategy.restartWithLimit()`. This example restarts the actor at most 10 times within a 10-second window upon encountering an `IllegalStateException`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nimport java.time.Duration;\n// #restart-limit\nfinal Behavior<String> supervisedBehavior3 =\n    Behaviors.supervise(behavior)\n        .onFailure(\n            IllegalStateException.class,\n            SupervisorStrategy.restartWithLimit(10, Duration.ofSeconds(10)));\n// #restart-limit\n\n```\n\n----------------------------------------\n\nTITLE: Handling Blocking Operations with mapAsync and Dedicated Dispatcher (Scala)\nDESCRIPTION: Shows how to handle potentially blocking external service calls within `mapAsync`. The blocking call (`lookupEmailBlocking`) is wrapped in a `Future` and explicitly executed on a dedicated `blockingDispatcher` to avoid starving the default Akka dispatchers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\nimplicit val blockingDispatcher = system.dispatchers.lookup(\"blocking-dispatcher\")\n\ndef lookupEmailBlocking(handle: String): Option[String] = {\n  // simulate blocking database lookup\n  println(s\"Looking up email blocking for $handle\")\n  Thread.sleep(200)\n  val email = handle match {\n    case \"rolandkuhn\" => Some(\"rk@example.com\")\n    case \"patriknw\"   => Some(\"pn@example.com\")\n    case \"konradmalawski\" => Some(\"km@example.com\")\n    case _            => None\n  }\n  println(s\"Email blocking for $handle: ${email.getOrElse(\"None\")}\")\n  email\n}\n\nval emailAddressesBlocking: Source[String, NotUsed] =\n  authors\n    .mapAsync(4)(author => Future(lookupEmailBlocking(author.handle)))\n    .collect { case Some(emailAddress) => emailAddress }\n```\n\n----------------------------------------\n\nTITLE: Registering Actor DeathWatch Monitoring in Akka (Scala)\nDESCRIPTION: Demonstrates registering an actor to watch another actor for termination notifications using DeathWatch in Scala Akka. Requires Akka libraries and a running actor system; imports akka.actor.Terminated. The code shows how to use the 'watch' method within an actor to receive Terminated messages, which notify the actor of the watched actor's lifecycle end. Input parameters typically include the actor reference to watch; output is the reception of Terminated messages upon target actor's termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.{ Actor, ActorRef, Props, Terminated }\n\nclass WatchActor(target: ActorRef) extends Actor {\n  context.watch(target) // start watching\n  def receive = {\n    case Terminated(t) if t == target => context.stop(self)\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Disabling Recovery in Akka Persistence EventSourcedBehavior\nDESCRIPTION: Demonstrates how to completely disable the recovery of events and snapshots in an EventSourcedBehavior. This configuration still recovers the highest sequence number to prevent corrupting the event log.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_36\n\nLANGUAGE: Scala\nCODE:\n```\nval behavior = EventSourcedBehavior[Command, Event, State](\\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\\n  emptyState = State.empty,\\n  commandHandler = (state, cmd) => Effect.persist(???),\\n  eventHandler = (state, evt) => ???)\\n.withRecovery(Recovery.disabled)\n```\n\nLANGUAGE: Java\nCODE:\n```\nEventSourcedBehavior<Command, Event, State> behavior =\\n    EventSourcedBehavior.create(\\n            PersistenceId.ofUniqueId(\"abc\"),\\n            State.empty(),\\n            (state, command) -> Effect().persist(commandToEvent(command)),\\n            (state, event) -> handleEvent(state, event))\\n        .withRecovery(Recovery.disabled());\n```\n\n----------------------------------------\n\nTITLE: Combining Materialized Values for a Composite Sink in Java\nDESCRIPTION: This Java snippet builds a composite Akka Stream `Sink`. It attaches the `nestedFlow` (materializing `CompletionStage<OutgoingConnection>`) to a `Sink.fold` (materializing `CompletionStage<String>`). By using `Keep.both()`, the resulting `nestedSink` materializes a `Pair` containing both the `CompletionStage<OutgoingConnection>` from the flow and the `CompletionStage<String>` from the sink.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\n// #mat-combine-3\nSink<HttpResponse, CompletionStage<String>> sink =\n    Sink.fold(\"\", (acc, i) -> acc + i.entity().toString());\n\nSink<Integer, Pair<CompletionStage<OutgoingConnection>, CompletionStage<String>>> nestedSink =\n    nestedFlow.toMat(sink, Keep.both());\n// #mat-combine-3\n```\n\n----------------------------------------\n\nTITLE: Simple Projection using mapAsync in Akka - Java\nDESCRIPTION: Illustration of using Java to project events using Akka with mapAsync for stateless conversions between data types before writing to a datastore. This is feasible when the target database lacks a Reactive Streams interface.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [PersistenceQueryDocTest.java](/akka-docs/src/test/java/jdocs/persistence/PersistenceQueryDocTest.java) { #projection-into-different-store-simple }\n```\n\n----------------------------------------\n\nTITLE: Using flatMapConcat to Process Customer Events Sequentially in Scala\nDESCRIPTION: This Scala example demonstrates using flatMapConcat to process customer events one customer at a time. For each customerId, it creates a Source by calling lookupCustomerEvents, which could represent a database query or calculation. All events for a customer are processed completely before moving to the next customer.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/flatMapConcat.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval customerIds = Source(1 to 3)\n\nval customerEvents = customerIds.flatMapConcat(customerId => lookupCustomerEvents(customerId))\n\n// Where lookupCustomerEvents could be implemented as:\ndef lookupCustomerEvents(customerId: Int): Source[CustomerEvent, NotUsed] =\n  Source(\n    List(\n      CustomerEvent(customerId, \"signed-up\"),\n      CustomerEvent(customerId, \"activated\"),\n      CustomerEvent(customerId, \"upgraded\")\n    )\n  )\n```\n\n----------------------------------------\n\nTITLE: Using LWWRegister in Java\nDESCRIPTION: Example of using LWWRegister (last writer wins register) in Java. It shows how to create a register, update its value, and retrieve the value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nLWWRegister<String> r1 = LWWRegister.create(\"value1\");\nLWWRegister<String> r2 = r1.withValue(\"value2\");\nString value1 = r1.getValue();\nString value2 = r2.getValue();\n```\n\n----------------------------------------\n\nTITLE: DNS Service Lookup in Scala\nDESCRIPTION: Example of using DNS-based service discovery in Scala. This code looks up a service using the configured DNS resolver and returns the resolved targets.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nval discovery = Discovery(system).discovery\nval result: Future[Resolved] = discovery.lookup(Lookup(\"service-name\").withPortName(\"http\").withProtocol(\"tcp\"), 1.second)\n```\n\n----------------------------------------\n\nTITLE: Using Sink.last Operator in Scala\nDESCRIPTION: Example demonstrating how to use the Sink.last operator to obtain the last element of a stream in Scala. It materializes into a Future which will complete with the last value when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/last.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [LastSinkSpec.scala](/akka-stream-tests/src/test/scala/akka/stream/scaladsl/LastSinkSpec.scala) { #last-operator-example }\n```\n\n----------------------------------------\n\nTITLE: Configuring BalancingPool Router via HOCON\nDESCRIPTION: Defines an Akka BalancingPool router named 'router1' in HOCON. This router attempts to redistribute work from busy routees to idle ones by having all routees share a mailbox. It creates 5 routee instances.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_21\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-balancing-pool\nakka.actor.deployment {\n  /parent/router1 {\n    router = balancing-pool\n    nr-of-instances = 5\n  }\n}\n//#config-balancing-pool\n```\n\n----------------------------------------\n\nTITLE: Initializing Sharding with Custom Stop Message (Scala)\nDESCRIPTION: This Scala snippet shows how to initialize Akka Cluster Sharding for a specific entity type (`TypeKey`). It registers the entity creator and specifies `GoodByeCounter` as the `stopMessage`. This message will be sent to the entity when passivation is triggered either manually or automatically.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_25\n\nLANGUAGE: scala\nCODE:\n```\nval TypeKey = EntityTypeKey[CounterCommand](\"Counter\")\n\nval shardRegion: ActorRef[ShardingEnvelope[CounterCommand]] = ClusterSharding(system).init(\n  Entity(TypeKey)(\n    createBehavior = entityContext =>\n      counter(entityContext.shard, entityContext.entityId)\n  ).withStopMessage(GoodByeCounter(???))) // ??? can be filled with the entityId in createBehavior\n```\n\n----------------------------------------\n\nTITLE: Converting Akka Stream to Java Stream in Java\nDESCRIPTION: Example showing how to create a Sink that materializes into a Java 8 Stream using StreamConverters.asJavaStream in Java. The stream blocks the current thread while waiting for next element and closes when the input stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/asJavaStream.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n#import #asJavaStream\n```\n\n----------------------------------------\n\nTITLE: Waiting for Cluster Size Before Startup\nDESCRIPTION: Demonstrates how to configure an Akka cluster to wait for a minimum number of members before transitioning nodes from 'Joining' to 'Up' status. The configuration is set in the Akka configuration file and supports role-specific conditions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_21\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FactorialFrontend.scala](/akka-docs/src/test/scala/docs/cluster/FactorialFrontend.scala) { #registerOnUp }\n```\n\n----------------------------------------\n\nTITLE: Initializing Akka Actor using preStart and postRestart in Java\nDESCRIPTION: This snippet shows how to override preStart and postRestart methods in Java to control actor initialization. It demonstrates how to prevent child actor recreation during restarts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_45\n\nLANGUAGE: Java\nCODE:\n```\n@Override\npublic void preStart() {\n    // Initialize children here\n}\n\n@Override\npublic void postRestart(Throwable reason) {\n    // do not call preStart()\n    // initialization code should happen in postRestart\n}\n\n@Override\npublic void preRestart(Throwable reason, Optional<Object> message) {\n    // keep the call to postStop(), but no stopping of children\n    postStop();\n}\n```\n\n----------------------------------------\n\nTITLE: MapAsync Concurrent Processing in Java\nDESCRIPTION: Java implementation showing concurrent event processing using mapAsync with higher parallelism while maintaining emission order.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapAsync.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSource.range(1, 100)\n    .map(Event::new)\n    .mapAsync(3, event -> CompletableFuture.supplyAsync(() -> {\n        System.out.println(\"Processing event number \" + event + \"...\");\n        try {\n            Thread.sleep(500); // processing takes time\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n        System.out.println(\"Completed processing \" + event.getNumber());\n        return event;\n    }))\n    .map(event -> {\n        System.out.println(\"`mapAsync` emitted event number: \" + event.getNumber());\n        return event;\n    })\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.fromIterator in Java\nDESCRIPTION: Shows how to create a Source from an Iterator in Java that generates a stream of integers. The example illustrates using the fromIterator operator with Java's Creator interface.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromIterator.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#from-iterator\n```\n\n----------------------------------------\n\nTITLE: Accessing Graph Materialized Value using builder.materializedValue in Akka Streams (Java)\nDESCRIPTION: This Java snippet shows how to access the materialized value inside a custom stream graph using builder.materializedValue, yielding an Outlet for the value. This lets stream logic use the materialized value as part of the flow. Dependency is akka.stream.javadsl. Returned values are typically CompletionStage or NotUsed, depending on stream logic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nfinal Outlet<CompletionStage<Integer>> matValueSource = builder.materializedValue();\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Reduce-by-Key for Word Count in Akka Streams\nDESCRIPTION: This pattern demonstrates the classic word count example using reduce-by-key operations in Akka Streams. It groups identical words together and counts their occurrences, with a limit on the maximum number of distinct words.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\nval MaximumDistinctWords = 1000\n\nSource(List(\"abc\", \"def\", \"abc\", \"hij\", \"def\"))\n  .groupBy(MaximumDistinctWords, identity)\n  .map(_ -> 1)\n  .reduce((l, r) => (l._1, l._2 + r._2))\n  .mergeSubstreams\n```\n\n----------------------------------------\n\nTITLE: Effects and Side Effects Implementation in Java\nDESCRIPTION: Shows the Java implementation of effects and side effects in DurableStateBehavior, handling command persistence and replies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_14\n\nLANGUAGE: java\nCODE:\n```\n#effects\n```\n\n----------------------------------------\n\nTITLE: Defining Device Registration Messages in Akka\nDESCRIPTION: The message protocol used for device registration, including DeviceRegistration request message and DeviceRegistered response message with the device actor reference.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_4.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [DeviceManager.scala](/akka-docs/src/test/scala/typed/tutorial_4/DeviceManager.scala) { #device-registration-msgs }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [DeviceManager.java](/akka-docs/src/test/java/jdocs/typed/tutorial_4/DeviceManager.java) { #device-registration-msgs }\n```\n\n----------------------------------------\n\nTITLE: Creating Worker Pool Graph Component in Scala\nDESCRIPTION: Creates a reusable worker pool component using custom Shape and Graph DSL.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n#graph-dsl-components-create\n```\n\n----------------------------------------\n\nTITLE: Streaming File Content using FileIO Source in Scala\nDESCRIPTION: Demonstrates creating an Akka Streams `Source` from a file path using `FileIO.fromPath` in Scala. This source reads the file and emits its content as a stream of `ByteString` chunks. The optional `chunkSize` parameter determines the maximum size of each emitted chunk.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n// Code for [StreamFileDocSpec.scala](/akka-docs/src/test/scala/docs/stream/io/StreamFileDocSpec.scala) { #file-source } not available in input\n```\n\n----------------------------------------\n\nTITLE: Implementing Request-Response with ask from Outside an Akka Typed Actor System\nDESCRIPTION: This snippet demonstrates how to use the 'ask' pattern to interact with Akka Typed actors from outside the actor system in Java. It returns a CompletionStage<Response>, which completes with a successful response or fails with a TimeoutException.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n@@snip [InteractionPatternsTest.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/InteractionPatternsTest.java) { #standalone-ask }\n```\n\n----------------------------------------\n\nTITLE: Implementing Map Transformation with GraphStage in Scala\nDESCRIPTION: This snippet demonstrates how to create a custom GraphStage that implements the map transformation in Akka Streams. It shows the usage of push and pull operations to handle upstream and downstream events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\nclass Map[A, B](f: A => B) extends GraphStage[FlowShape[A, B]] {\n  val in = Inlet[A](\"Map.in\")\n  val out = Outlet[B](\"Map.out\")\n  override val shape = FlowShape(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      setHandler(in, new InHandler {\n        override def onPush(): Unit = {\n          push(out, f(grab(in)))\n        }\n      })\n      setHandler(out, new OutHandler {\n        override def onPull(): Unit = {\n          pull(in)\n        }\n      })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Conflate Operator in Scala\nDESCRIPTION: Demonstrates using the conflate operator to aggregate elements during backpressure in Scala. The example shows how elements are summed together when downstream processing is slower than upstream production.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/conflate.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#conflate\n```\n\n----------------------------------------\n\nTITLE: Combining Streams with Zip in Java\nDESCRIPTION: Shows how to use the zip operator to combine elements from two sources in Java. The operator pairs elements from both sources into Pair objects.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zip.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer, NotUsed> source1 = Source.from(Arrays.asList(1, 2, 3));\nSource<Integer, NotUsed> source2 = Source.from(Arrays.asList(4, 5, 6));\nsource1.zip(source2)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Stream Termination in Scala\nDESCRIPTION: Demonstrates how to use watchTermination operator in Scala to monitor stream completion and handle termination states. The operator provides a Future that completes with Done on successful completion or fails with an error.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/watchTermination.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#watchTermination\n```\n\n----------------------------------------\n\nTITLE: Creating a Balanced and Live Cycle with Initial Element in Akka Streams (Scala)\nDESCRIPTION: A balanced cycle implementation that injects an initial element using Concat. This solves the chicken-and-egg problem by providing an independent initial element that kicks off the cycle, allowing it to process elements continuously without deadlock.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_29\n\nLANGUAGE: Scala\nCODE:\n```\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n  val zip = b.add(ZipWith[Int, Int, Int]((left, right) => right))\n  val bcast = b.add(Broadcast[Int](2))\n  val concat = b.add(Concat[Int](2))\n\n  source ~> zip.in0\n  zip.out.map { s => println(s); s } ~> bcast ~> Sink.ignore\n  Source.single(0) ~> concat.in(0)\n                      bcast ~> concat.in(1)\n                               concat ~> zip.in1\n\n  ClosedShape\n})\n```\n\n----------------------------------------\n\nTITLE: Message Forwarding in Akka Actors - Java\nDESCRIPTION: Shows how to forward messages between actors while preserving the original sender reference in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_31\n\nLANGUAGE: java\nCODE:\n```\n#forward\n```\n\n----------------------------------------\n\nTITLE: LevelDB Native Configuration for Akka Persistence\nDESCRIPTION: Configuration snippet for using the native LevelDB implementation with Akka Persistence. This sets the storage location and ensures native library is used.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_38\n\nLANGUAGE: Scala\nCODE:\n```\nakka.persistence.journal.leveldb {\n  native = off // switch to using the Java port of leveldb\n  dir = \"target/test-journals/native-java\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using flatMapConcat in Java\nDESCRIPTION: Demonstrates the `flatMapConcat` operator in Akka Streams. It transforms each element of the input stream (integers 1 and 2) into a new Source (containing the integer repeated three times). These generated Sources are then concatenated sequentially into the output stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\n//#flatMapConcat\nSource.range(1, 2)\n    .flatMapConcat(i -> Source.repeat(i).take(3))\n    .runForeach(System.out::println, system);\n//#flatMapConcat\n\n```\n\n----------------------------------------\n\nTITLE: Implementing mapAsyncUnordered in Java\nDESCRIPTION: Java implementation demonstrating mapAsyncUnordered usage for parallel event processing without maintaining order.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapAsyncUnordered.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.range(1, 10)\n  .map(Event::new)\n  .mapAsyncUnordered(\n      4,\n      event -> {\n        return CompletableFuture.supplyAsync(\n            () -> {\n              System.out.println(\"Processing event number \" + event);\n              try {\n                Thread.sleep(new Random().nextInt(300));\n              } catch (InterruptedException e) {\n                e.printStackTrace();\n              }\n              System.out.println(\"Completed processing \" + event.id);\n              return event.id;\n            });\n      })\n  .runForeach(\n      id -> System.out.println(\"`mapAsyncUnordered` emitted event number: \" + id),\n      system);\n```\n\n----------------------------------------\n\nTITLE: Querying Actors with ActorFlow.ask in Akka Streams - Scala\nDESCRIPTION: Demonstrates how to use the ask pattern within a Scala Akka Stream, sending a message to an actor for each element, awaiting the reply and emitting it downstream. Requires akka-stream-typed library, a typed ActorRef, a makeMessage function, and an implicit timeout. Inputs are stream elements and replies are emitted as outputs, with failure occurring on ask timeout or actor termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/ask.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n// Imports required for actor and stream setup\nimport akka.actor.typed.ActorRef\nimport akka.actor.typed.Behavior\nimport akka.actor.typed.scaladsl.Behaviors\nimport akka.stream.scaladsl.{ Flow, Sink, Source }\nimport akka.stream.typed.scaladsl.ActorFlow\nimport scala.concurrent.duration._\n\n// Define protocol\nsealed trait Command\nfinal case class Asking(replyTo: ActorRef[Reply]) extends Command\nfinal case class Reply(message: String)\n\n// Actor definition\nval behavior: Behavior[Command] = Behaviors.receiveMessage {\n  case Asking(replyTo) =>\n    replyTo ! Reply(\"success\")\n    Behaviors.same\n}\n\n// Akka Stream setup using ActorFlow.ask\nval actorRef: ActorRef[Command] = ??? // Assume obtained via ActorSystem\nimplicit val timeout = akka.util.Timeout(3.seconds)\n\nval flow: Flow[Int, Reply, _] = ActorFlow.ask(actorRef) { (elem, replyTo: ActorRef[Reply]) =>\n  Asking(replyTo)\n}\n\n// Using the flow in a runnable graph\nSource(1 to 10).via(flow).map(_.message).runWith(Sink.foreach(println))\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Graph Materialized Value using builder.materializedValue in Akka Streams (Scala)\nDESCRIPTION: This Scala snippet demonstrates how to access the materialized value of a stream graph inside the graph logic using builder.materializedValue. The generated Outlet makes the value available downstream. It works with all graph types, such as Source, Flow, Sink, or BidiFlow. Dependencies: akka.stream.scaladsl. The materialized value can be any type; outputs are connected as graph elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_17\n\nLANGUAGE: Scala\nCODE:\n```\nval matValueSource: Outlet[Future[Int]] = builder.materializedValue[Future[Int]]\n\n```\n\n----------------------------------------\n\nTITLE: Recovering with Retries (Java)\nDESCRIPTION: Demonstrates using `recoverWithRetries` in Java. When an `IllegalArgumentException` occurs, it attempts to recover by replacing the failed source with a new source (`Source.from(Arrays.asList(\"five\", \"six\"))`) up to one time. The stream then continues with elements from the new source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nfinal Source<String, NotUsed> planB = Source.from(Arrays.asList(\"five\", \"six\"));\n\nSource.from(Arrays.asList(0, 1, 2, 3, 4, 5, 6))\n    .map(\n        n -> {\n          if (n < 5) return n.toString();\n          else throw new IllegalArgumentException(\"Boom!\");\n        })\n    .recoverWithRetries(\n        1, // max attempts\n        IllegalArgumentException.class,\n        () -> {\n          return planB;\n        })\n    .runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Prematerializing Akka Stream Source in Scala\nDESCRIPTION: This Scala code demonstrates how to prematerialize an Akka Stream source to obtain its materialized value before the source is connected to the rest of the graph. Dependencies include implicit access to akka.stream.Materializer.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #source-prematerialization }\n```\n\n----------------------------------------\n\nTITLE: Configuring Journal Failure Supervision in Akka Persistence\nDESCRIPTION: Demonstrates how to configure supervision strategy for journal failures in an EventSourcedBehavior. This allows the actor to restart with backoff when journal operations fail instead of just stopping.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_42\n\nLANGUAGE: Scala\nCODE:\n```\nval behavior: Behavior[Command] =\\n  EventSourcedBehavior[Command, Event, State](\\n    persistenceId = persistenceId,\\n    emptyState = State.empty,\\n    commandHandler = (state, cmd) => Effect.persist(???),\\n    eventHandler = (state, evt) => ???)\\n    .onPersistFailure(\\n      SupervisorStrategy.restartWithBackoff(\\n        minBackoff = 10.seconds, maxBackoff = 60.seconds, randomFactor = 0.1))\n```\n\nLANGUAGE: Java\nCODE:\n```\nBehavior<Command> behavior =\\n    EventSourcedBehavior.create(\\n            PersistenceId.ofUniqueId(\"abc\"),\\n            State.empty(),\\n            (state, command) -> Effect().persist(commandToEvent(command)),\\n            (state, event) -> handleEvent(state, event))\\n        .onPersistFailure(\\n            SupervisorStrategy.restartWithBackoff(\\n                Duration.ofSeconds(10), Duration.ofSeconds(30), 0.2));\n```\n\n----------------------------------------\n\nTITLE: Implementing Countdown with Source.unfold in Scala\nDESCRIPTION: Creates a source that counts down from a given number to zero using Source.unfold. The state represents the current number, and each iteration decrements it until reaching zero.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfold.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval countdownSource = Source.unfold(10) { n =>\n  if (n == 0) None\n  else Some((n - 1, n))\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Swapper in Scala\nDESCRIPTION: Demonstrates actor behavior stack manipulation using become/unbecome for message handling alternation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_42\n\nLANGUAGE: scala\nCODE:\n```\nclass Swapper extends Actor {\n  override def receive = {\n    case Swap =>\n      context.become({\n        case Swap =>\n          context.unbecome()\n      }, discardOld = false)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Echo Handler with NACK-Based Write Back-Pressure in Scala\nDESCRIPTION: A Scala implementation of an EchoHandler that uses NACK-based write back-pressure. It processes incoming TCP data and writes it back, handling command failures appropriately.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nclass EchoHandler(connection: ActorRef) extends Actor {  \n  import Tcp._\n  implicit val byteOrder: ByteOrder = ByteOrder.LITTLE_ENDIAN\n\n  // sign death pact: this actor terminates when connection breaks\n  context.watch(connection)\n\n  case object Ack extends Event\n\n  def receive: Receive = writing\n\n  def writing: Receive = {\n    case Received(data) =>\n      connection ! Write(data, Ack)\n      context.become(buffering(Vector.empty[ByteString]))\n\n    case _: ConnectionClosed =>\n      context.stop(self)\n  }\n```\n\n----------------------------------------\n\nTITLE: Updating Flag in Low Latency Voting Service with Akka in Scala\nDESCRIPTION: This code snippet demonstrates updating a Flag in Akka's low latency voting service. The Opened state is replicated to all cluster nodes using the 'WriteAll' consistency level, ensuring all nodes are aware of the voting status. Dependencies include the Akka library for distributed data, and key parameters involve the Update message and OpenedKey.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-scala/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nreplicator ! Update(OpenedKey, Flag(), WriteAll(5.seconds))(_.switchOn)\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.actorRef in Java\nDESCRIPTION: Example showing how to use Source.actorRef operator in Java to create an ActorRef that emits messages to a stream. The example includes necessary imports and demonstrates message handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/actorRef.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n#actor-ref-imports #actor-ref\n```\n\n----------------------------------------\n\nTITLE: Joining and Subscribing to Cluster Events in Scala\nDESCRIPTION: Example that combines joining a cluster and subscribing to cluster events. This shows a common pattern when initializing a cluster-aware actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nval selfAddress = Cluster(system).selfAddress\nCluster(system).join(selfAddress)\nCluster(system).subscribe(self, classOf[ClusterEvent.MemberEvent])\n```\n\n----------------------------------------\n\nTITLE: Running Akka Streams with runWith in Scala\nDESCRIPTION: Illustrates the use of the `runWith` convenience method on a Source to attach a Sink and materialize the stream in a single step. This implicitly uses `Keep.right` to return the Sink's materialized value (e.g., a Future from `Sink.fold`).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #materialization-runWith }\n```\n\n----------------------------------------\n\nTITLE: Implementing FSM State Behavior in Scala\nDESCRIPTION: Shows how to implement a specific state behavior (idle state) in a Scala Finite State Machine. Demonstrates handling different events and transitioning to other states.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fsm.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nprivate def idle(): Behavior[Event] =\n  Behaviors.receive { (context, message) =>\n    message match {\n      case SetTarget(ref) =>\n        target = Some(ref)\n        Behaviors.same\n      case Queue(obj) =>\n        queue :+= obj\n        active()\n      case Flush =>\n        Behaviors.unhandled\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Circuit Breaker Usage in Java\nDESCRIPTION: Example of implementing circuit breaker protection in Java using CompletionStage for async operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nprivate static final CompletionStage<String> opBlockingWithCircuitBreaker(\n    CircuitBreaker breaker) {\n  return breaker.callWithCircuitBreakerCS(\n      () -> CompletableFuture.supplyAsync(\n          () -> {\n            try {\n              Thread.sleep(2000);\n            } catch (InterruptedException e) {\n              throw new RuntimeException(e);\n            }\n            return \"We're done!\";\n          }));\n}\n```\n\n----------------------------------------\n\nTITLE: Example Message Ordering Between Actors in Akka\nDESCRIPTION: Illustrates the message ordering guarantee between sender-receiver pairs in Akka. Shows how messages M1-M3 from Actor A1 and M4-M6 from Actor A3 are delivered to Actor A2.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/message-delivery-reliability.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nActor `A1` sends messages `M1`, `M2`, `M3` to `A2`\nActor `A3` sends messages `M4`, `M5`, `M6` to `A2`\n```\n\n----------------------------------------\n\nTITLE: Creating EventSourcedBehavior in Akka - Scala\nDESCRIPTION: This snippet demonstrates how to instantiate an EventSourcedBehavior in Scala using Akka's typed persistence API, critical for building durable actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BasicPersistentBehaviorCompileOnly.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/BasicPersistentBehaviorCompileOnly.scala) { #behavior }\n```\n\n----------------------------------------\n\nTITLE: State Management with Latest Items List in Akka - Scala\nDESCRIPTION: Defines how to manage actor state as a list of the five latest items using Akka typed persistence API in Scala. Helps in tracking recency within persistent actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BasicPersistentBehaviorCompileOnly.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/BasicPersistentBehaviorCompileOnly.scala) { #state }\n```\n\n----------------------------------------\n\nTITLE: Looking Up Distributed PubSub Topic with Akka Typed - Java\nDESCRIPTION: Illustrates how to retrieve or create a distributed pub/sub topic actor using the PubSub registry in Akka Typed (Java API). This operation depends on akka.actor.typed.pubsub.Topic and akka.actor.typed.pubsub.PubSub and assumes akka-actor-typed and akka-cluster-typed dependencies. It requires a topic name and message class; if a topic already exists with a different message class, an exception is thrown. The output is an ActorRef instance for interacting with the topic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nActorRef<Message> topic = \n  PubSub.get(system).topic(\"my-topic\", Message.class);\n```\n\n----------------------------------------\n\nTITLE: DNS Resolution Using Extension in Scala\nDESCRIPTION: Example of performing DNS resolution using the DNS extension in Scala. Shows how to resolve domain names using the Akka DNS API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-dns.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n#resolve\n```\n\n----------------------------------------\n\nTITLE: Implementing Ask Pattern in Akka Actors - Scala\nDESCRIPTION: Demonstrates the ask pattern with pipeTo for asynchronous request-response handling between actors using Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_26\n\nLANGUAGE: scala\nCODE:\n```\n#ask-pipeTo\n```\n\n----------------------------------------\n\nTITLE: Merging Partitioned Streams with MergeSequence in Scala\nDESCRIPTION: This example demonstrates how to use MergeSequence in combination with Partition to merge a partitioned stream back into a single stream while maintaining the original element order. It processes messages selectively and then merges them for acknowledgment.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/MergeSequence.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval source = Source(1 to 10)\n  .map(n => if (n % 2 == 0) s\"even $n\" else s\"odd $n\")\n  .zipWithIndex\n  .map { case (elem, idx) => (elem, idx.toInt) }\n\nval (needsProcessing, noProcessing) = source.partition(_._1.startsWith(\"even\"), outputArity = 2)\n\nval processedMessages = needsProcessing\n  .map { case (msg, idx) => (s\"processed $msg\", idx) }\n\nSource.fromGraph(\n  GraphDSL.create() { implicit b =>\n    import GraphDSL.Implicits._\n    val merge = b.add(MergeSequence[(String, Int)](outputArity = 2))\n    processedMessages ~> merge.in(0)\n    noProcessing ~> merge.in(1)\n    SourceShape(merge.out)\n  }\n).runForeach(println)\n```\n\n----------------------------------------\n\nTITLE: Messaging Patterns for Akka Cluster\nDESCRIPTION: Defines messages used for communication between frontend and backend nodes in an Akka cluster. The messages facilitate the service provided by backend nodes and must be consistent across all nodes. Dependencies include Akka libraries for cluster management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [TransformationMessages.java](/akka-docs/src/test/java/jdocs/cluster/TransformationMessages.java) { #messages }\n```\n\n----------------------------------------\n\nTITLE: Using SpawnProtocol to Spawn Actors with Akka System in Scala\nDESCRIPTION: Demonstrates spawning additional actors using SpawnProtocol within an Akka ActorSystem in Scala. Includes utilizing ask pattern to asynchronously create actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/SpawnProtocolDocSpec.scala) { #imports2 #system-spawn }\n```\n\n----------------------------------------\n\nTITLE: Common Akka Logger Name Examples for Troubleshooting\nDESCRIPTION: A list of common Akka logger names that can be used in logging configuration to enable specific debug logging for various Akka modules including Cluster, Persistence, Remote, and more.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\nakka.cluster\nakka.cluster.Cluster\nakka.cluster.ClusterHeartbeat\nakka.cluster.ClusterGossip\nakka.cluster.ddata\nakka.cluster.pubsub\nakka.cluster.singleton\nakka.cluster.sharding\nakka.coordination.lease\nakka.discovery\nakka.persistence\nakka.remote\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Media Driver Configuration\nDESCRIPTION: Kubernetes deployment YAML for configuring memory-based media driver mount point.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  containers:\n  - name: artery-udp-cluster\n    // rest of container spec...\n    volumeMounts:\n    - mountPath: /dev/shm\n      name: media-driver\n  volumes:\n  - name: media-driver\n    emptyDir:\n      medium: Memory\n      name: media-driver\n```\n\n----------------------------------------\n\nTITLE: Wrapping DurableStateBehavior with ActorContext Setup for Logging (Java)\nDESCRIPTION: Demonstrates wrapping a Java DurableStateBehavior in an ActorContext setup to enable logging or context-based logic within command handlers. This pattern uses the Akka Java DSL and is crucial for actors that need to interact with the system context or log internal activity at the onset of behavior definition.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_32\n\nLANGUAGE: java\nCODE:\n```\n@@snip [DurableStatePersistentBehaviorTest.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/DurableStatePersistentBehaviorTest.java) { #wrapPersistentBehavior }\n```\n\n----------------------------------------\n\nTITLE: Using ask with Placeholder Syntax in Scala\nDESCRIPTION: A more concise way to use the ask pattern with placeholder syntax. This alternative still maintains readability while being more terse.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_21\n\nLANGUAGE: Scala\nCODE:\n```\nask(actorRef, GetValue(_))\n```\n\n----------------------------------------\n\nTITLE: Java API - AggregateWithBoundary Signature\nDESCRIPTION: Java method signature for aggregateWithBoundary operator that uses Java functional interfaces for supplier, aggregation function, and harvest function along with timer pair.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/aggregateWithBoundary.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\naggregateWithBoundary(java.util.function.Supplier,akka.japi.function.Function2,akka.japi.function.Function,akka.japi.Pair)\n```\n\n----------------------------------------\n\nTITLE: Collecting Elements with End Signal in Scala\nDESCRIPTION: Example showing how to collect elements starting with 'b' and emit them at stream end using a special end element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMapConcat.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval result = Source(List(\"abc\", \"bcd\", \"bef\", \"cde\", \"end\"))\n  .statefulMapConcat { () =>\n    val bs = mutable.Buffer[String]()\n    {\n      case \"end\" => bs.toList\n      case el if el.startsWith(\"b\") =>\n        bs.append(el)\n        Nil\n      case _ => Nil\n    }\n  }\n  .runWith(Sink.seq)\n```\n\n----------------------------------------\n\nTITLE: Creating ScalaTest Multi-JVM Tests\nDESCRIPTION: Scala example showing how to create ScalaTest-based multi-JVM tests using ScalaTest suites instead of objects with main methods.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\npackage sample\n\nimport org.scalatest.wordspec.AnyWordSpec\nimport org.scalatest.matchers.must.Matchers\n\nclass SpecMultiJvmNode1 extends AnyWordSpec with Matchers {\n  \"A node\" should {\n    \"be able to say hello\" in {\n      val message = \"Hello from node 1\"\n      message must be(\"Hello from node 1\")\n    }\n  }\n}\n\nclass SpecMultiJvmNode2 extends AnyWordSpec with Matchers {\n  \"A node\" should {\n    \"be able to say hello\" in {\n      val message = \"Hello from node 2\"\n      message must be(\"Hello from node 2\")\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cycling Iterator Elements in Scala\nDESCRIPTION: Example demonstrating how to use Source.cycle to create an infinite stream from an iterator in Scala. The operator continuously cycles through the iterator elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/cycle.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#cycle\n```\n\n----------------------------------------\n\nTITLE: Using UniqueKillSwitch and SharedKillSwitch in Akka - Scala\nDESCRIPTION: These Scala snippets showcase the usage of UniqueKillSwitch and SharedKillSwitch within Akka Streams. UniqueKillSwitch manages the completion of one materialized graph, while SharedKillSwitch controls multiple operators of FlowShape. Both examples demonstrate shutdown and abort capabilities. Dependencies include Akka Streams library.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [KillSwitchDocSpec.scala](/akka-docs/src/test/scala/docs/stream/KillSwitchDocSpec.scala) { #unique-shutdown }\n```\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [KillSwitchDocSpec.scala](/akka-docs/src/test/scala/docs/stream/KillSwitchDocSpec.scala) { #unique-abort }\n```\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [KillSwitchDocSpec.scala](/akka-docs/src/test/scala/docs/stream/KillSwitchDocSpec.scala) { #shared-shutdown }\n```\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [KillSwitchDocSpec.scala](/akka-docs/src/test/scala/docs/stream/KillSwitchDocSpec.scala) { #shared-abort }\n```\n\n----------------------------------------\n\nTITLE: Looking Up a Dispatcher Interface Implementation in Java\nDESCRIPTION: Demonstrates how to look up a Dispatcher implementing the Executor interface in Java. Dependencies: akka-actor-typed library. Uses CompletableFuture invocations with dispatchers. Outputs the dispatcher retrieved for executing actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [DispatcherDocTest.java](/akka-docs/src/test/java/jdocs/actor/typed/DispatcherDocTest.java) { #lookup }\n```\n\n----------------------------------------\n\nTITLE: Alternative Balancing Jobs Implementation in Akka Streams\nDESCRIPTION: A second Java implementation of the worker pool pattern that provides a more straightforward approach to distributing work among parallel workers. It creates a complete graph with balance and merge operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_38\n\nLANGUAGE: Java\nCODE:\n```\npublic static <In, Out> Flow<In, Out, NotUsed> balancer2(\n    Flow<In, Out, NotUsed> worker, int workerCount) {\n  return Flow.fromGraph(\n      GraphDSL.create(\n          b -> {\n            final UniformFanOutShape<In, In> balance = b.add(Balance.create(workerCount, false));\n            final UniformFanInShape<Out, Out> merge = b.add(Merge.create(workerCount));\n\n            for (int i = 0; i < workerCount; i++) {\n              b.from(balance.out(i)).via(b.add(worker.async())).toInlet(merge.in(i));\n            }\n\n            return new FlowShape<In, Out>(balance.in(), merge.out());\n          }));\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Email Lookup Failures with mapAsync in Scala\nDESCRIPTION: This Scala snippet shows how to use mapAsync with supervision strategies to handle email lookup failures in Akka Streams. It uses resuming decider to discard unknown emails safely without terminating the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_23\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntegrationDocSpec.scala](/akka-docs/src/test/scala/docs/stream/IntegrationDocSpec.scala) { #email-addresses-mapAsync-supervision }\n```\n\n----------------------------------------\n\nTITLE: Implementing Sharded Response Pattern in Scala\nDESCRIPTION: This code snippet demonstrates how to implement a sharded response pattern in Scala using Akka Cluster Sharding. It shows the structure of messages, the sharded actor behavior, and how to send and receive messages through sharding.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\nobject ShardedResponseExample {\n  // Protocol\n  sealed trait Message\n  final case class Request(entityId: String, msg: String, replyTo: ActorRef[Response]) extends Message\n  final case class Response(msg: String) extends Message\n\n  val TypeKey: EntityTypeKey[Message] =\n    EntityTypeKey[Message](\"ResponseExample\")\n\n  def apply(): Behavior[Message] =\n    Behaviors.setup { context =>\n      Behaviors.receiveMessage {\n        case Request(id, msg, replyTo) =>\n          // Remember who to reply to\n          context.log.info(\"Got request for: {}\", id)\n          replyTo ! Response(msg.toUpperCase)\n          Behaviors.same\n      }\n    }\n\n  // Request from some other actor\n  object OtherActor {\n    def apply(region: ActorRef[ShardingEnvelope[Message]]): Behavior[Response] =\n      Behaviors.setup { context =>\n        region ! ShardingEnvelope(\"cat\", Request(\"cat\", \"meow\", context.self))\n        Behaviors.receiveMessage { response =>\n          context.log.info(\"Got response: {}\", response.msg)\n          Behaviors.same\n        }\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Shared Resource Class for Database Connection Pool in Scala\nDESCRIPTION: Creates a SharedResource class representing an expensive database connection pool. This class is used to demonstrate the purpose of the Akka extension.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass SharedResource {\n  private val connection = {\n    // expensive database connection setup\n    \"DB-Connection\"\n  }\n\n  def query(id: String): Unit =\n    println(s\"Querying $id using $connection\")\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Echo Handler Actor in Scala\nDESCRIPTION: This Scala snippet defines a `SimpleEchoHandler` actor that manages an individual TCP connection. It receives data (`Tcp.Received`), buffers it, and sends it back (`Tcp.Write`) using an ACK-based back-pressure mechanism. It waits for an `Ack` message before sending the next chunk, buffering incoming data in the meantime.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nobject SimpleEchoHandler {\n  final case class Ack(offset: Int) extends Tcp.Event\n}\n\nclass SimpleEchoHandler(connection: ActorRef, remote: InetSocketAddress) extends Actor with ActorLogging {\n  import SimpleEchoHandler._\n  import Tcp._\n\n  // sign death pact: this actor stops when the connection is closed\n  context.watch(connection)\n\n  private var storage = Vector.empty[ByteString]\n  private var stored = 0L\n  private var transferred = 0L\n  private val maxStored = 100000000L\n  private var closed = false\n\n  override def preStart(): Unit = {\n    connection ! ResumeReading\n  }\n\n  def receive = {\n    case Received(data) =>\n      buffer(data)\n    case Ack(ack) =>\n      acknowledge(ack)\n    case PeerClosed =>\n      peerClosed()\n    case ErrorClosed(cause) =>\n      log.error(cause, \"Stopping, because connection dropped.\")\n      context.stop(self)\n    case ConnectionClosed =>\n      if (storage.isEmpty) {\n        log.info(\"Stopping, because connection closed\")\n        context.stop(self)\n      } else {\n        context.become(closing)\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using zipAll Operator in Scala\nDESCRIPTION: Demonstrates how to use the zipAll operator to combine elements from two sources in Scala, handling early completion with default values.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipAll.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n// #zipAll-simple\nSource(List(1, 2, 3))\n  .zipAll(Source(List(\"A\", \"B\")), 0, \"MISSING\")\n  .runWith(Sink.foreach(println))\n// prints:\n// (1,A)\n// (2,B)\n// (3,MISSING)\n// #zipAll-simple\n```\n\n----------------------------------------\n\nTITLE: Implementing Generic Reduce-by-Key in Akka Streams\nDESCRIPTION: Java implementation of the generalized reduce-by-key pattern that provides a reusable solution for aggregating elements by key. It separates the concerns of grouping, mapping, and reducing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\npublic static <In, K, Out> Flow<In, Pair<K, Out>, NotUsed> reduceByKey(\n    int maximumGroupSize,\n    Function<In, K> groupKey,\n    Function<In, Out> map,\n    BiFunction<Out, Out, Out> reduce) {\n  return Flow.<In>create()\n      .groupBy(maximumGroupSize, groupKey::apply)\n      .map(el -> new Pair<>(groupKey.apply(el), map.apply(el)))\n      .reduce(\n          (left, right) -> new Pair<>(left.first(), reduce.apply(left.second(), right.second())))\n      .mergeSubstreams();\n}\n```\n\n----------------------------------------\n\nTITLE: Reading File Contents with FileIO.fromPath in Java\nDESCRIPTION: This snippet shows how to use FileIO.fromPath to read the contents of a file in Java. It creates a source from a file path and connects it to a sink that prints each chunk of data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/FileIO/fromPath.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nfinal Path path = Paths.get(\"example.txt\");\nFileIO.fromPath(path)\n    .to(Sink.foreach(chunk -> System.out.println(chunk.utf8String())))\n    .run(mat);\n```\n\n----------------------------------------\n\nTITLE: Implementing Lazy Sink in Scala\nDESCRIPTION: Demonstrates using Sink.lazySink in Scala to create a sink that is only materialized when the first element arrives. The example shows side effects to illustrate the order of operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/lazySink.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsource.map { element =>\n  println(s\"map - $element\")\n  element\n}.to(Sink.lazySink(() => {\n  println(\"Creating sink\")\n  Sink.foreach[String](elem => println(s\"foreach - $elem\"))\n})).run()\n```\n\n----------------------------------------\n\nTITLE: Using orElse Operator in Akka Streams (Java)\nDESCRIPTION: Example of using the orElse operator in Java to provide a fallback source when the primary source completes without emitting elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/orElse.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource<String, NotUsed> source1 = Source.empty();\nSource<String, NotUsed> source2 = Source.single(\"yes\");\nSource<String, NotUsed> source3 = Source.from(Arrays.asList(\"no1\", \"no2\", \"no3\"));\n\nSource<String, NotUsed> s1 = source1.orElse(source2);\n// s1 will emit \"yes\"\n\nSource<String, NotUsed> s2 = source3.orElse(source2);\n// s2 will emit \"no1\", \"no2\", \"no3\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Group Router with Consistent Hashing for Akka Cluster\nDESCRIPTION: Configuration for a cluster-aware group router using consistent hashing strategy. The router sends messages to actor paths on cluster nodes with the \"compute\" role, including local routees.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nakka.actor.deployment {\n  /statsService/workerRouter {\n      router = consistent-hashing-group\n      routees.paths = [\"/user/statsWorker\"]\n      cluster {\n        enabled = on\n        allow-local-routees = on\n        use-roles = [\"compute\"]\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Akka Stream groupedWithin API Signatures\nDESCRIPTION: API signatures for the groupedWithin operator in both Scala and Java. This operator chunks a stream into groups of elements received within a time window or limited by element count.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/groupedWithin.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.groupedWithin(n: Int, d: scala.concurrent.duration.FiniteDuration): FlowOps.this.Repr[scala.collection.immutable.Seq[Out]]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.groupedWithin(int, java.time.Duration)\n```\n\nLANGUAGE: scala\nCODE:\n```\nFlow.groupedWithin(n: Int, d: scala.concurrent.duration.FiniteDuration): FlowOps.this.Repr[scala.collection.immutable.Seq[Out]]\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow.groupedWithin(int, java.time.Duration)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Publisher Actor in Java\nDESCRIPTION: Java implementation of a publisher actor that sends messages to a specific topic via the DistributedPubSubMediator. Messages sent to this actor will be published to the 'content' topic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nstatic class Publisher extends AbstractActor {\n  // activate the extension\n  ActorRef mediator = DistributedPubSub.get(getContext().getSystem()).mediator();\n  \n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(String.class, in -> mediator.tell(new Publish(\"content\", in), getSelf()))\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing zipWith in Scala\nDESCRIPTION: Example showing how to use the zipWith operator in Scala to combine elements from multiple sources.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipWith.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFlowZipWithSpec.scala\n```\n\n----------------------------------------\n\nTITLE: Sending Elements to Typed Actor With Backpressure Using Akka Streams (Java)\nDESCRIPTION: This Java example illustrates pushing elements from an Akka Stream to a typed ActorRef using actorRefWithBackpressure, where each element triggers an acknowledgement mechanism to implement backpressure. Prerequisites include akka-actor-typed, akka-stream-typed, and a properly initialized ActorSystem. It demonstrates supplying the actor reference, lambda functions for message adaptation, initialization, completion and failure message handling. The Sink created will only pull new elements when the actor acknowledges readiness.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorSink/actorRefWithBackpressure.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.typed.ActorRef;\nimport akka.stream.javadsl.Sink;\nimport akka.stream.typed.javadsl.ActorSink;\nimport java.util.Arrays;\nimport java.util.concurrent.CompletionStage;\n\ninterface Command {}\nclass Element implements Command {\n  public final String value;\n  public final ActorRef<Ack> replyTo;\n  public Element(String value, ActorRef<Ack> replyTo) { this.value = value; this.replyTo = replyTo; }\n}\nclass Init implements Command {}\nclass Complete implements Command {}\nclass Fail implements Command {\n  public final Throwable ex;\n  public Fail(Throwable ex) { this.ex = ex; }\n}\nclass Ack {}\n\nActorRef<Command> actor = ...; // Acquired from system\n\nSink<String, akka.NotUsed> sink = ActorSink.actorRefWithBackpressure(\n  actor,\n  (replyTo, elem) -> new Element(elem, replyTo),\n  replyTo -> new Init(),\n  new Ack(),\n  new Complete(),\n  ex -> new Fail(ex)\n);\n\na.List.of(\"a\", \"b\", \"c\").stream().runWith(sink, system)\n```\n\n----------------------------------------\n\nTITLE: Creating Akka Group Router in Scala\nDESCRIPTION: This snippet illustrates how to create a group router in Akka using Scala. It includes using the receptionist to manage group membership dynamically. Dependencies must include Akka Typed and appropriate cluster modules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n/* Group router configuration in RouterSpec.scala */\n```\n\n----------------------------------------\n\nTITLE: Filtering and Mapping Tweet Stream in Java\nDESCRIPTION: Demonstrates filtering tweets containing '#akka' and mapping to author handles in Java Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nSource<Author, NotUsed> authors =\n    tweets\n        .filter(tweet -> tweet.hashtags().contains(new Hashtag(\"#akka\")))\n        .map(Tweet::author);\n```\n\n----------------------------------------\n\nTITLE: Nesting Supervise with Other Behaviors in Scala\nDESCRIPTION: Showing how supervise differs from other behavior wrappers as it only restarts the behavior it wraps, not outer behaviors. This is important for understanding restart semantics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_26\n\nLANGUAGE: Scala\nCODE:\n```\nBehaviors.setup[Command] { context =>\n  // this is not restarted\n  Behaviors.supervise(\n    Behaviors.withTimers { timers =>\n      // this is restarted, including the timer setup\n      // ...\n      Behaviors.empty\n    }\n  ).onFailure(SupervisorStrategy.restart)\n}\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Sources Using Source.combine in Java\nDESCRIPTION: Java implementation showing how to merge three sources of integers using Source.combine with Merge strategy. The sources will emit elements concurrently with non-deterministic ordering between sources.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/combine.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#imports #source-combine-merge\n```\n\n----------------------------------------\n\nTITLE: Deferred Subscription until MemberUp in Java\nDESCRIPTION: Java implementation for deferring cluster event subscription until the MemberUp event for the own node is received. Avoids handling an initial empty CurrentClusterState.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nfinal Address selfAddress = Cluster.get(system).selfAddress();\nCluster.get(system).join(selfAddress);\n\nCluster.get(system).registerOnMemberUp(() -> {\n  System.out.println(\"Member is up!\");\n  Cluster.get(system).subscribe(getSelf(), ClusterEvent.MemberEvent.class);\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom TwoPhaseSet in Java\nDESCRIPTION: Example of implementing a custom TwoPhaseSet data type in Java. It shows how to create a CRDT that allows adding and removing elements, but never adding an element again after removal.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\npublic class TwoPhaseSet extends AbstractReplicatedData<TwoPhaseSet> {\n  private final GSet<String> adds;\n  private final GSet<String> removes;\n\n  public TwoPhaseSet() {\n    this(GSet.create(), GSet.create());\n  }\n\n  private TwoPhaseSet(GSet<String> adds, GSet<String> removes) {\n    this.adds = adds;\n    this.removes = removes;\n  }\n\n  public TwoPhaseSet add(String element) {\n    return new TwoPhaseSet(adds.add(element), removes);\n  }\n\n  public TwoPhaseSet remove(String element) {\n    return new TwoPhaseSet(adds, removes.add(element));\n  }\n\n  public boolean contains(String element) {\n    return adds.contains(element) && !removes.contains(element);\n  }\n\n  @Override\n  public TwoPhaseSet mergeData(TwoPhaseSet other) {\n    return new TwoPhaseSet(adds.merge(other.adds), removes.merge(other.removes));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Limiting ByteString Data Size Using limitWeighted in Java\nDESCRIPTION: Example demonstrating how to use limitWeighted to limit the total size of ByteString data from an untrusted source in Java. This snippet uses the number of bytes in each ByteString element as the weight and accepts at most 10,000 bytes total.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/limitWeighted.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<ByteString, NotUsed> untrustedSource = Source.repeat(ByteString.fromString(\"x\".repeat(1000)));\n\nSource<ByteString, NotUsed> limited =\n    untrustedSource\n        .limitWeighted(10000, ByteString::length); // max 10000 bytes\n\nCompletionStage<ByteString> completionStage =\n    limited.runWith(\n        Sink.fold(\n            ByteString.emptyByteString(),\n            ByteString::concat),\n        system);\n\ncompletionStage.whenComplete(\n    (result, exception) -> {\n      if (exception != null)\n        System.out.println(\"Failed with \" + exception);\n      else\n        System.out.println(\"Got result\" + result);\n    });\n```\n\n----------------------------------------\n\nTITLE: Using Sink.head Operator in Java\nDESCRIPTION: Shows how to use the Sink.head operator in Java to get the first element from a stream. The operation returns a CompletionStage that completes with the first value and then cancels the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/head.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n//head-operator-example\n```\n\n----------------------------------------\n\nTITLE: Implementing a Replicated GCounter Actor with Akka Distributed Data in Scala\nDESCRIPTION: Presents a Scala actor example demonstrating the use of `Replicator.Update` and `Replicator.Get` messages with a `GCounter` CRDT via a `ReplicatorMessageAdapter`. This actor implements a distributed counter, handling `Increment` and `GetValue` commands. Requires `SelfUniqueAddress`. The code is referenced from `ReplicatorDocSpec.scala`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ReplicatorSpec.scala](/akka-cluster-typed/src/test/scala/docs/akka/cluster/ddata/typed/scaladsl/ReplicatorDocSpec.scala) { #sample }\n```\n\n----------------------------------------\n\nTITLE: Defining a Restart Supervision Strategy in Java\nDESCRIPTION: This Java snippet defines actors to demonstrate Akka Typed's supervision capabilities. A `SupervisingActor` creates a `SupervisedActor` child using `Behaviors.supervise` combined with `SupervisorStrategy.restart`. The `SupervisedActor` is designed to throw an exception upon receiving a \"fail\" command, which triggers the restart strategy defined by its parent.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.typed.ActorRef;\nimport akka.actor.typed.ActorSystem;\nimport akka.actor.typed.Behavior;\nimport akka.actor.typed.PostStop;\nimport akka.actor.typed.PreRestart;\nimport akka.actor.typed.SupervisorStrategy;\nimport akka.actor.typed.javadsl.AbstractBehavior;\nimport akka.actor.typed.javadsl.ActorContext;\nimport akka.actor.typed.javadsl.Behaviors;\nimport akka.actor.typed.javadsl.Receive;\n\nclass SupervisingActor extends AbstractBehavior<String> {\n\n  public static Behavior<String> create() {\n    return Behaviors.setup(SupervisingActor::new);\n  }\n\n  private final ActorRef<String> child;\n\n  private SupervisingActor(ActorContext<String> context) {\n    super(context);\n    child =\n        context.spawn(\n            Behaviors.supervise(SupervisedActor.create()).onFailure(SupervisorStrategy.restart()),\n            \"supervised-actor\");\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder().onMessageEquals(\"failChild\", this::onFailChild).build();\n  }\n\n  private Behavior<String> onFailChild() {\n    child.tell(\"fail\");\n    return this;\n  }\n}\n\nclass SupervisedActor extends AbstractBehavior<String> {\n\n  public static Behavior<String> create() {\n    return Behaviors.setup(SupervisedActor::new);\n  }\n\n  private SupervisedActor(ActorContext<String> context) {\n    super(context);\n    System.out.println(\"supervised actor started\");\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder()\n        .onMessageEquals(\"fail\", this::fail)\n        .onSignal(PreRestart.class, signal -> preRestart())\n        .onSignal(PostStop.class, signal -> postStop())\n        .build();\n  }\n\n  private Behavior<String> fail() {\n    System.out.println(\"supervised actor fails now\");\n    throw new RuntimeException(\"I failed!\");\n  }\n\n  private Behavior<String> preRestart() {\n    System.out.println(\"supervised actor will be restarted\");\n    return this;\n  }\n\n  private Behavior<String> postStop() {\n    System.out.println(\"supervised actor stopped\");\n    return this;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Publishing Messages to a Topic in Scala\nDESCRIPTION: Example of how to publish a message to a topic from anywhere in the Akka cluster. This snippet demonstrates sending a message to the 'content' topic via a publisher actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nval publisher = system.actorOf(Props[Publisher], \"publisher\")\n// after a while the message is published to the subscribers\npublisher ! \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Persistence Plugin Initialization in Java\nDESCRIPTION: Example demonstrating how to initialize persistence plugins using PersistenceInit in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n\"#imports #init\"\n```\n\n----------------------------------------\n\nTITLE: Implementing EventBus API in Java\nDESCRIPTION: The Java implementation of the EventBus interface, providing publish-subscribe functionality with typed events, classifiers, and subscribers. It defines methods for subscription management and event publishing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n/**\n * Base class for typed event buses specialized per classifier of events and subscriber.\n */\npublic abstract class EventBus<E, C, S> {\n  /**\n   * Attempts to register the subscriber to the specified Classifier\n   *\n   * @return true if successful and false if not (because it was already subscribed to\n   *     that Classifier, or otherwise)\n   */\n  public abstract boolean subscribe(S subscriber, C to);\n\n  /**\n   * Attempts to deregister the subscriber from the specified Classifier\n   *\n   * @return true if successful and false if not (because it wasn't subscribed to that\n   *     Classifier, or otherwise)\n   */\n  public abstract boolean unsubscribe(S subscriber, C from);\n\n  /**\n   * Attempts to deregister the subscriber from all Classifiers it may be subscribed to\n   */\n  public abstract void unsubscribe(S subscriber);\n\n  /**\n   * Publishes the specified Event to this bus\n   */\n  public abstract void publish(E event);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Room Behavior with Akka Typed AbstractOnMessageBehavior in Java (Java 21)\nDESCRIPTION: This snippet uses Akka Typed's AbstractOnMessageBehavior with Java 21's pattern matching for a direct, switch-based implementation of chat room logic. Message handling is performed directly in the onMessage method, enabling ergonomic type dispatch without the need for the builder pattern. This approach is suitable for users preferring an alternative to the AbstractBehavior/builder API and requires Akka Typed, Java 21 or later, and access to the pattern matching features. The actor receives RoomCommand messages, controls session logic, and dispatches chat messages to session actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\n// Akka Typed chat room actor implementation using switch pattern matching (Java 21)\npublic class ChatRoom extends AbstractOnMessageBehavior<RoomCommand> {\n  private final List<ActorRef<SessionCommand>> sessions = new ArrayList<>();\n  public static Behavior<RoomCommand> create() {\n    return Behaviors.setup(ChatRoom::new);\n  }\n  private ChatRoom(ActorContext<RoomCommand> context) {\n    super(context);\n  }\n  @Override\n  public Behavior<RoomCommand> onMessage(RoomCommand msg) {\n    return switch (msg) {\n      case GetSession gs -> { /* Handle get session */ yield this; }\n      case PublishSessionMessage psm -> { /* Handle message publish */ yield this; }\n      default -> this;\n    };\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Akka Stream Attribute Inheritance in Scala\nDESCRIPTION: This Scala snippet illustrates how attributes, specifically `inputBuffer`, are inherited in nested Akka Stream components. Attributes are set using `addAttributes`. The `nestedSink` defines an initial buffer size. The `nestedFlow` inherits this, but the inner `map` operator overrides it with a different buffer size. `nestedSource` uses the default attributes from the materializer. This shows that attributes are inherited unless explicitly overridden at a deeper nesting level.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\n//#attributes-inheritance\nval nestedSource = Source(1 to 10).map(_ * 2).named(\"nestedSource\") // Uses default attributes\n\nval nestedFlow =\n  Flow[Int].filter(_ % 2 == 0).map(_ / 2).addAttributes(Attributes.inputBuffer(1, 1)) // overrides\nval nestedSink = nestedFlow.to(Sink.ignore).addAttributes(Attributes.inputBuffer(3, 3)) // sets attributes\n\nval runnableGraph2 = nestedSource.to(nestedSink) // nestedSink defines attributes\n//#attributes-inheritance\n```\n\n----------------------------------------\n\nTITLE: State Transition Monitoring\nDESCRIPTION: Example of monitoring state transitions using onTransition handler.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nonTransition(handler)\n```\n\n----------------------------------------\n\nTITLE: Implementing ActorRef with Backpressure in Scala\nDESCRIPTION: Example showing how to create and use an ActorRef source with backpressure control in Scala. The source materializes an ActorRef that emits messages onto the stream after receiving acknowledgments.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/actorRefWithBackpressure.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n#actor-ref-imports #actorRefWithBackpressure\n```\n\n----------------------------------------\n\nTITLE: Stateful Routing with PartitionHub in Scala\nDESCRIPTION: This snippet shows how to implement stateful routing using PartitionHub in Scala. It demonstrates a round-robin partitioning strategy that maintains state across invocations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_17\n\nLANGUAGE: Scala\nCODE:\n```\nval statefulSink = PartitionHub.statefulSink(\n  () => {\n    var i = -1\n    (info, elem) => {\n      i += 1\n      val idx = info.consumerIds(i % info.size)\n      idx\n    }\n  },\n  startAfterNrOfConsumers = 2,\n  bufferSize = 256)\n```\n\n----------------------------------------\n\nTITLE: Defining Basic DurableStateBehavior Structure (Scala)\nDESCRIPTION: Outlines the fundamental structure for defining a `DurableStateBehavior` in Scala. It shows the creation using `DurableStateBehavior.apply` which requires a `PersistenceId`, an `emptyState` definition, and a `commandHandler` function.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.typed.Behavior\nimport akka.persistence.typed.PersistenceId\nimport akka.persistence.typed.scaladsl.DurableStateBehavior\nimport akka.persistence.typed.scaladsl.Effect\n\nobject Counter {\n  // Command\n  sealed trait Command\n  final case object Increment extends Command\n\n  // State\n  final case class State(value: Int)\n\n  def apply(persistenceId: PersistenceId): Behavior[Command] =\n    DurableStateBehavior[Command, State](\n      persistenceId = persistenceId,\n      emptyState = State(0),\n      commandHandler = (state, command) =>\n        command match {\n          case Increment => Effect.persist(state.copy(value = state.value + 1))\n        })\n}\n\n```\n\n----------------------------------------\n\nTITLE: Testing Normal Temperature Query Response\nDESCRIPTION: Test case verifying the scenario where two devices successfully report their temperatures to the query actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n#query-test-normal\n```\n\nLANGUAGE: Java\nCODE:\n```\n#query-test-normal\n```\n\n----------------------------------------\n\nTITLE: Enabling Local Snapshot Store Plugin (HOCON)\nDESCRIPTION: Provides the configuration required to enable the local snapshot store plugin (`akka.persistence.snapshot-store.local`) by setting the `akka.persistence.snapshot-store.plugin` property. This plugin stores snapshots in the local filesystem and is not suitable for cluster use.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_12\n\nLANGUAGE: hocon\nCODE:\n```\n# Assuming the snippet sets the local snapshot store plugin\nakka.persistence.snapshot-store.plugin = \"akka.persistence.snapshot-store.local\"\n```\n\n----------------------------------------\n\nTITLE: Throttling with Initial Burst in Akka Streams\nDESCRIPTION: Shows how to implement throttling with an initial burst of 30 seconds worth of data followed by constant rate throttling at 24 frames per second.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/throttle.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nSource.fromIterator(() => Iterator.from(1))\n    .throttle(\n      elements = 24,\n      per = 1.second,\n      maximumBurst = 24 * 30,\n      mode = ThrottleMode.Shaping)\n    .runWith(Sink.foreach(println))\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.fromIterator(() -> Stream.iterate(1, i -> i + 1).iterator())\n    .throttle(24, Duration.ofSeconds(1), 24 * 30, ThrottleMode.shaping())\n    .runWith(Sink.foreach(System.out::println), system);\n```\n\n----------------------------------------\n\nTITLE: Running Multi-JVM Tests using SBT (Shell)\nDESCRIPTION: Executes the multi-JVM tests defined in the project using the Scala Build Tool (sbt). These tests typically involve starting multiple JVMs to simulate a real Akka cluster environment and verify cluster-specific behavior. This requires sbt to be installed and configured.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nsbt multi-jvm:test\n```\n\n----------------------------------------\n\nTITLE: Consuming Tweet Stream with Sink in Java\nDESCRIPTION: Shows how to attach a Sink to the stream to print author handles in Java Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\nauthors.runWith(Sink.foreach(System.out::println), system);\n```\n\n----------------------------------------\n\nTITLE: Converting Akka Stream to OutputStream in Java\nDESCRIPTION: Creates a Source that materializes to an OutputStream and connects it to a ByteString-concatenating Sink. Shows how to write bytes to the stream and handle completion in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/asOutputStream.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nfinal OutputStream outputStream =\n    StreamConverters.asOutputStream()\n        .toMat(Sink.foreach(bytes -> System.out.println(bytes.utf8String())), Keep.left())\n        .run(mat);\n\noutputStream.write(\"Hello World\\n\".getBytes(StandardCharsets.UTF_8));\noutputStream.close();\n```\n\n----------------------------------------\n\nTITLE: Consistent Hashing Shard Allocation Initialization in Java\nDESCRIPTION: Java example showing how to initialize entities with consistent hashing shard allocation\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_22\n\nLANGUAGE: java\nCODE:\n```\nShardedDaemonProcess.get(system)\\n    .init(\\n        ShardedDaemonProcessSettings.create(system),\\n        \\\"BuildingDevices\\\",\\n        30,\\n        instanceId -> {\\n          ShardRegion<Building.Command> region =\\n              ClusterSharding.get(system)\\n                  .init(\\n                      Entity.of(\\n                              Building.TypeKey,\\n                              entityContext -> Building.create(entityContext.getEntityId()))\\n                          .withAllocationStrategy(\\n                              new ConsistentHashingShardAllocationStrategy(30, 5))\\n                          .withMessageExtractor(\\n                              new ShardingMessageExtractor<\\n                                  Building.Command,\\n                                  Building.Command,\\n                                  Building.BuildingEnvelope>(30) {\\n                                @Override\\n                                public Building.BuildingEnvelope unwrapMessage(\\n                                    Building.Command message) {\\n                                  return new Building.BuildingEnvelope(message.buildingId);\\n                                }\\n                              }));\\n\\n          ShardRegion<Device.Command> deviceRegion =\\n              ClusterSharding.get(system)\\n                  .init(\\n                      Entity.of(\\n                              Device.TypeKey,\\n                              entityContext -> Device.create(entityContext.getEntityId()))\\n                          .withAllocationStrategy(\\n                              new ConsistentHashingShardAllocationStrategy(30, 5))\\n                          .withMessageExtractor(\\n                              new ShardingMessageExtractor<\\n                                  Device.Command, Device.Command, Device.DeviceEnvelope>(30) {\\n                                @Override\\n                                public Device.DeviceEnvelope unwrapMessage(Device.Command message) {\\n                                  return new Device.DeviceEnvelope(\\n                                      message.buildingId, message.deviceId);\\n                                }\\n                              }));\\n\\n          // ... behavior using the regions\\n          throw new UnsupportedOperationException(\\\"Not implemented\\\");\\n        });\n```\n\n----------------------------------------\n\nTITLE: Enforced Replies Example in Scala\nDESCRIPTION: Example showing how to enforce replies at compile time using EventSourcedBehavior in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_32\n\nLANGUAGE: Scala\nCODE:\n```\n#withEnforcedReplies\n```\n\n----------------------------------------\n\nTITLE: Defining Dispatcher in Code in Scala\nDESCRIPTION: Demonstrates how to programmatically specify a custom dispatcher for an actor in Scala using the withDispatcher method on Props.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nval myActor = context.actorOf(Props[MyActor].withDispatcher(\"my-dispatcher\"), \"myactor\")\n```\n\n----------------------------------------\n\nTITLE: Debugging FSM with Event Tracing in Akka\nDESCRIPTION: Describes enabling event tracing for finite state machines (FSM) in Akka using the setting akka.actor.debug.fsm. This setup logs all processed events at DEBUG level, includes state transitions, and supports FSM debugging and development troubleshooting. When enabled, it heavily utilizes LoggingFSM instances.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FSMDocSpec.scala](/akka-docs/src/test/scala/docs/actor/FSMDocSpec.scala) { #logging-fsm }\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic FSM Structure in Scala\nDESCRIPTION: Core FSM implementation showing state transitions and initialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\n#simple-fsm\n```\n\n----------------------------------------\n\nTITLE: Distinct Until Changed Pattern in Scala\nDESCRIPTION: Implementation that only emits elements that are different from the previous element, dropping duplicates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMap.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nval numbers = Source(List(1, 1, 1, 2, 2, 3, 3, 3))\n  .statefulMap(() => Option.empty[Int])(\n    (state, element) => {\n      if (state.contains(element)) (state, Vector.empty)\n      else (Some(element), Vector(element))\n    },\n    _ => None\n  )\n```\n\n----------------------------------------\n\nTITLE: Reusing Akka Stream Definitions in Scala\nDESCRIPTION: Shows that a defined RunnableGraph (representing the stream blueprint) can be materialized multiple times. Each materialization executes independently and yields a new, distinct materialized value (e.g., a separate Future), even if the same Sink definition is used.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #stream-reuse }\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Distributed PubSub Topic - Akka Typed - Java\nDESCRIPTION: Demonstrates subscribing an actor to a distributed pub/sub topic in Akka Typed using Java. Involves sending a Topic.Subscribe message to the topic's ActorRef. Requires the topic and subscriber ActorRefs. The subscriber will start receiving messages published to this topic. No output; registration is managed by Akka.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\ntopic.tell(new Topic.Subscribe<>(\n  subscriber, \n  messageAdapter));\n```\n\n----------------------------------------\n\nTITLE: Implementing the Akka Extension for Database Connection Pool in Java\nDESCRIPTION: Defines the DatabaseConnectionPool class that implements akka.actor.typed.Extension in Java. This class manages the SharedResource instance and provides a method to access it.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic class DatabaseConnectionPool implements Extension {\n  private final SharedResource sharedResource = new SharedResource();\n\n  public DatabaseConnectionPool(ActorSystem<?> system) {}\n\n  public void queryDatabase(String id) {\n    sharedResource.query(id);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping Akka Actors\nDESCRIPTION: Shows proper patterns for stopping actors including handling of the postStop hook. Demonstrates both self-termination and stopping child actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_37\n\nLANGUAGE: Scala\nCODE:\n```\nclass StoppingActor extends Actor {\n  def receive = {\n    case \"stop\" =>\n      context.stop(self)\n  }\n\n  override def postStop(): Unit = {\n    // clean up resources here ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cart Data with Consistency in Scala\nDESCRIPTION: Scala example of retrieving cart data from distributed storage using ReadMajority consistency. It demonstrates handling different response types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\nprivate def getCart(userId: String): Future[Cart] = {\\n  implicit val timeout = Timeout(5.seconds)\\n  (replicator ? Get(DataKey(userId), ReadMajority(timeout))).map {\\n    case g @ GetSuccess(DataKey(_), _) => g.get(Cart.empty)\\n    case NotFound(_, _) => Cart.empty\\n    case GetFailure(_, _) =>\\n      throw new IllegalStateException(\\\"Get failed\\\")\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Simplified Duplicate Transformation with GraphStage in Scala\nDESCRIPTION: This snippet demonstrates a simplified implementation of the one-to-many duplicate operator using GraphStage in Scala. It uses the emitMultiple method to simplify the logic and reduce mutable state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\nclass Duplicator[A] extends GraphStage[FlowShape[A, A]] {\n  val in = Inlet[A](\"Duplicator.in\")\n  val out = Outlet[A](\"Duplicator.out\")\n  override val shape = FlowShape(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      setHandler(in, new InHandler {\n        override def onPush(): Unit = {\n          val elem = grab(in)\n          emitMultiple(out, List(elem, elem))\n        }\n      })\n      setHandler(out, new OutHandler {\n        override def onPull(): Unit = {\n          pull(in)\n        }\n      })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository for Dependency Management\nDESCRIPTION: This snippet details configuring the Akka repository in your build tool of choice to access Akka dependencies. By setting the repository URL, users can ensure that their project can pull the necessary Akka libraries for functionality. The code snippet is applicable for sbt, Maven, and Gradle users.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-introduction.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Device Group Registration in Akka\nDESCRIPTION: DeviceGroup actor implementation that handles registration requests by either finding an existing device actor or creating a new one, and storing actor references in a map.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_4.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [DeviceGroup.scala](/akka-docs/src/test/scala/typed/tutorial_4/DeviceGroup.scala) { #device-group-register }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [DeviceGroup.java](/akka-docs/src/test/java/jdocs/typed/tutorial_4/DeviceGroup.java) { #device-group-register }\n```\n\n----------------------------------------\n\nTITLE: Global Rate Limiting Actor for Akka Streams (Scala)\nDESCRIPTION: An actor-based global rate limiter that can be used to limit the aggregate throughput of multiple independent streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_47\n\nLANGUAGE: Scala\nCODE:\n```\nobject GlobalRateLimit {\n  case object WantToPass\n  case object MayPass\n  case object ReplenishTokens\n\n  class Limiter(rate: Float, per: FiniteDuration, maximumBurst: Int) extends Actor {\n    private var tokens = maximumBurst\n    private var lastReplenish = Instant.now()\n    private val queue = mutable.Queue.empty[ActorRef]\n\n    context.system.scheduler.scheduleAtFixedRate(\n      per,\n      per,\n      self,\n      ReplenishTokens\n    )(context.dispatcher)\n\n    override def receive: Receive = open\n\n    def open: Receive = {\n      case WantToPass =>\n        tokens -= 1\n        sender() ! MayPass\n        if (tokens == 0) context.become(closed)\n\n      case ReplenishTokens => replenish()\n    }\n\n    def closed: Receive = {\n      case WantToPass =>\n        queue.enqueue(sender())\n\n      case ReplenishTokens =>\n        replenish()\n        if (tokens > 0) {\n          while (tokens > 0 && queue.nonEmpty) {\n            queue.dequeue() ! MayPass\n            tokens -= 1\n          }\n          if (tokens > 0) context.become(open)\n        }\n    }\n\n    private def replenish(): Unit = {\n      val now = Instant.now()\n      val sinceLastReplenish = now.toEpochMilli - lastReplenish.toEpochMilli\n      val toAdd = ((sinceLastReplenish.millis.toFloat / per.toMillis) * rate).toInt\n      tokens = math.min(tokens + toAdd, maximumBurst)\n      lastReplenish = now\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing a Flow Component with Flow.fromSinkAndSource in Scala\nDESCRIPTION: This Scala snippet demonstrates how to use Flow.fromSinkAndSource for testing a component that accepts a Flow. It uses TestProbe from the Akka Streams TestKit to control and assert incoming and outgoing elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/fromSinkAndSource.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nval sinkProbe = TestProbe()\nval sourceProbe = TestProbe()\nval flow = Flow.fromSinkAndSource(\n  Sink.actorRef(sinkProbe.ref, \"done\", _ => \"failed\"),\n  Source.fromPublisher(sourceProbe.ref)\n)\n\n// test code using flow and the probes\n```\n\n----------------------------------------\n\nTITLE: Setting Up Main Actor for Akka System in Java\nDESCRIPTION: Demonstrates the setup of the `Main` actor in Java for initiating chat room and client actors within an Akka system, including handling of system shutdown events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/IntroTest.java) { #chatroom-main }\n```\n\n----------------------------------------\n\nTITLE: Implementing DurableStateUpdateWithChangeEventStore Interface in Java\nDESCRIPTION: Implementation of the extended DurableStateUpdateWithChangeEventStore interface in Java, which adds functionality for storing additional change events alongside the state updates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n#state-store-plugin-api\n```\n\n----------------------------------------\n\nTITLE: Configuring Admission Window Proportion in Akka Cluster Sharding\nDESCRIPTION: Configures the proportion of the active entity limit to be used for the admission window. The default is 1% but can be adjusted based on workload characteristics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_39\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = least-recently-used\n  admission-window {\n    strategy = least-recently-used\n    proportion = 0.05 # 5%\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Fibonacci Sequence with Source.unfold in Java\nDESCRIPTION: Implements an infinite Fibonacci sequence generator using Source.unfold in Java. Uses a Pair to maintain state of the last two numbers in the sequence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfold.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSource.unfold(\n    new Pair<>(0, 1),\n    (Pair<Integer, Integer> pair) -> {\n      Integer n1 = pair.first();\n      Integer n2 = pair.second();\n      return Optional.of(new Pair<>(new Pair<>(n2, n1 + n2), n1));\n    });\n```\n\n----------------------------------------\n\nTITLE: Initializing initialTimeout Operator in Akka Streams (Java)\nDESCRIPTION: Applies an initial timeout to a Source or Flow in Akka Streams. The method takes a java.time.Duration parameter and returns a new representation of the stream with the timeout applied.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/initialTimeout.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ninitialTimeout(java.time.Duration)\n```\n\n----------------------------------------\n\nTITLE: Initializing the Cluster Singleton - Akka Cluster Typed - Java\nDESCRIPTION: Shows initialization of a singleton actor via ClusterSingleton extension in an Akka Cluster node using Java. The code samples demonstrate accessing the singleton by name and behavior. Dependency is Akka Cluster Typed, and it outputs an ActorRef for message delivery to the singleton. Suitable for Java projects coordinating singleton actors cluster-wide.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// imports for Akka and ClusterSingleton\nClusterSingleton singletonManager = ClusterSingleton.get(system);\nActorRef<CounterCommand> proxy = singletonManager.init(\n    SingletonActor.of(counterBehavior, \"GlobalCounter\")\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing ActorRef with Backpressure in Java\nDESCRIPTION: Example demonstrating the creation and usage of an ActorRef source with backpressure control in Java. Shows how to materialize an ActorRef that emits messages onto the stream with proper flow control.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/actorRefWithBackpressure.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n#actor-ref-imports #actorRefWithBackpressure\n```\n\n----------------------------------------\n\nTITLE: Performing Update Operation on Distributed Data in Scala\nDESCRIPTION: Example of sending an Update message to the Replicator to modify and replicate a data value. The current data value is passed to the modify function which returns the new value to be replicated.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval Counter1Key = PNCounterKey(\"counter1\")\nval AddCounter1 = Update(Counter1Key, PNCounter.empty, WriteLocal)(_ :+ 1)\n```\n\n----------------------------------------\n\nTITLE: Programmatic Serialization in Scala\nDESCRIPTION: Example showing how to programmatically serialize and deserialize objects using Akka's SerializationExtension.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval system = ActorSystem(\"example\")\nval serialization = SerializationExtension(system)\n\nval original = \"hello\"\nval bytes = serialization.serialize(original).get\nval back = serialization.deserialize(bytes, serialization.findSerializerFor(original).identifier, classOf[String]).get\n```\n\n----------------------------------------\n\nTITLE: One-line Tweet Counting in Akka Streams (Java)\nDESCRIPTION: Shows a concise way to count tweets using runWith in Java, which is equivalent to the multi-line version.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_43\n\nLANGUAGE: Java\nCODE:\n```\nCompletionStage<Integer> sum = tweets.map(t -> 1).runWith(Sink.fold(0, (a, b) -> a + b), mat);\n```\n\n----------------------------------------\n\nTITLE: Defining an Akka Extension in Java\nDESCRIPTION: Java implementation of a Counter extension that tracks occurrences. The Extension interface defines the functionality while ensuring thread safety with an AtomicLong.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.AbstractExtensionId;\nimport akka.actor.ExtendedActorSystem;\nimport akka.actor.Extension;\nimport akka.actor.ExtensionIdProvider;\n\nimport java.util.concurrent.atomic.AtomicLong;\n\npublic class CountExtension implements Extension {\n  // Since this Extension is a shared instance\n  // per ActorSystem we need to be threadsafe\n  private final AtomicLong counter = new AtomicLong(0);\n\n  // This is the operation this Extension provides\n  public long increment() {\n    return counter.incrementAndGet();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Conceptual Durable State Processing Function Signature\nDESCRIPTION: Illustrates the core concept of durable state processing as a function that takes the current state and an incoming command, and returns the next state to be persisted. This represents the fundamental logic within a DurableStateBehavior's command handler.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n(State, Command) => State\n```\n\n----------------------------------------\n\nTITLE: Using JSON Framing with Akka Streams in Scala\nDESCRIPTION: Demonstrates how to use `JsonFraming.objectScanner` in Scala to parse a stream of ByteStrings into distinct JSON objects. This flow processes incoming bytes, identifies boundaries between valid JSON objects, and emits each complete object as a separate ByteString element. It handles concatenated or fragmented JSON objects within the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n// Code for [JsonFramingSpec.scala](/akka-stream-tests/src/test/scala/akka/stream/scaladsl/JsonFramingSpec.scala) { #using-json-framing } not available in input\n```\n\n----------------------------------------\n\nTITLE: Router PoisonPill Handling Example\nDESCRIPTION: Demonstrates sending PoisonPill messages to routers and routees for graceful shutdown.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_34\n\nLANGUAGE: scala\nCODE:\n```\nrouter ! PoisonPill\n```\n\n----------------------------------------\n\nTITLE: Transforming Stream Elements with Map in Akka Streams (Scala)\nDESCRIPTION: This example demonstrates how to use the 'map' operator in Akka Streams to transform each element in a stream. It shows the imports required and a simple usage of 'map' to convert integers to strings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/map.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.stream.scaladsl._\n\nSource(1 to 5).map(_.toString)\n```\n\n----------------------------------------\n\nTITLE: Combining Materialized Values in Akka Streams - Java\nDESCRIPTION: Example showing how to combine materialized values from different stream operators in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer> source = Source.from(Arrays.asList(1, 2, 3));\nFlow<Integer, Integer, NotUsed> flow = Flow.of(Integer.class).map(i -> i * 2);\nSink<Integer, CompletionStage<Integer>> sink = Sink.fold(0, (aggr, next) -> aggr + next);\n\n// connect the Source to the Sink, obtaining a RunnableGraph\nRunnableGraph<CompletionStage<Integer>> runnable =\n  source.viaMat(flow, Keep.none())\n        .toMat(sink, Keep.right());\n\n// materialize the flow\nCompletionStage<Integer> sum = runnable.run(system);\n```\n\n----------------------------------------\n\nTITLE: Using collectType to Filter and Transform Messages in Java\nDESCRIPTION: Java implementation showing how to filter Ping messages and convert them to Pong objects using collectType operator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collectType.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n#collectType\n```\n\n----------------------------------------\n\nTITLE: Installing Event Adapters in Akka Persistence\nDESCRIPTION: Demonstrates how to install a custom event adapter on an EventSourcedBehavior. This connects the adapter to the persistence system for event transformation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_40\n\nLANGUAGE: Scala\nCODE:\n```\nEventSourcedBehavior[Command, Event, State](\\n  persistenceId = persistenceId,\\n  emptyState = State.empty,\\n  commandHandler = commandHandler,\\n  eventHandler = eventHandler).eventAdapter(EventWrapper)\n```\n\nLANGUAGE: Java\nCODE:\n```\nEventSourcedBehavior<Command, Event, State> persistentBehavior =\\n    EventSourcedBehavior.create(\\n            PersistenceId.ofUniqueId(\"myPersistenceId\"),\\n            emptyState,\\n            commandHandler,\\n            eventHandler)\\n        .eventAdapter(new EventWrapper());\n```\n\n----------------------------------------\n\nTITLE: Initiating Manual Passivation in an Akka Entity (Java)\nDESCRIPTION: This Java code demonstrates how an entity actor can initiate its own passivation. It sends a `ClusterSharding.Passivate` message containing a reference to itself to the shard actor. A custom stop message, `GoodByeCounter`, is defined and expected to be received before the actor stops itself, allowing for cleanup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_24\n\nLANGUAGE: java\nCODE:\n```\nstatic class GoodByeCounter implements CounterCommand {\n  private final String id;\n\n  GoodByeCounter(String id) {\n    this.id = id;\n  }\n}\n\nBehavior<CounterCommand> counter(ActorRef<ClusterSharding.ShardCommand> shard, String entityId) {\n  return Behaviors.receive(CounterCommand.class)\n      .onMessage(\n          Increment.class,\n          (context, cmd) -> {\n            context.getLog().info(\"Counter {} incremented\", entityId);\n            return Behaviors.same();\n          })\n      .onMessage(\n          GetValue.class,\n          (context, cmd) -> {\n            cmd.replyTo.tell(42);\n            return Behaviors.same();\n          })\n      .onMessage(\n          PassivateCounter.class,\n          (context, cmd) -> {\n            context.getLog().info(\"Passivating counter {}\", entityId);\n            shard.tell(new ClusterSharding.Passivate<>(context.getSelf()));\n            return Behaviors.same();\n          })\n      .onMessage(\n          GoodByeCounter.class,\n          (context, cmd) -> {\n            // can do cleanup here\n            context.getLog().info(\"Counter {} stopped\", entityId);\n            return Behaviors.stopped();\n          })\n      .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Drop Elements Using dropWhile in Scala\nDESCRIPTION: Example showing how to use dropWhile operator to filter out negative numbers from a source stream until the first non-negative number is encountered. After that point, all remaining elements are emitted regardless of the predicate.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/dropWhile.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource(List(-3, -2, -1, 0, 1, -1, 2))\n  .dropWhile(_ < 0)\n  .runWith(Sink.foreach(println))\n// prints: 0, 1, -1, 2\n```\n\n----------------------------------------\n\nTITLE: GroupBy with Async Boundary in Scala\nDESCRIPTION: Illustrates using groupBy with async boundaries for concurrent processing of substreams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/groupBy.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval words = Source(List(\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"))\nval groups = words\n  .groupBy(3, word => word.length)\n  .map(_.toUpperCase).async\n  .mergeSubstreams\n\ngroups.runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Configuring Child Creation in Parent Actors - Scala\nDESCRIPTION: This snippet demonstrates how to externalize child construction from the parent actor by passing in Props or a factory function in Scala. Enhances flexibility and testability for actor hierarchies. Requires Akka Actors and corresponding test harness.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ParentChildSpec.scala](/akka-docs/src/test/scala/docs/testkit/ParentChildSpec.scala) { #test-dependentparent }\n```\n\n----------------------------------------\n\nTITLE: Configuring Child Creation in Parent Actors - Java\nDESCRIPTION: Shows how to let Java Akka parent actors accept either a Props object or a child creation function, supporting generic test setups and production code flexibility. Depends on Akka Actors (Java) and test framework support.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #test-dependentparent }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #test-dependentparent-generic }\n```\n\n----------------------------------------\n\nTITLE: Skipping Snapshot Loading During Recovery in Akka Persistence (Scala)\nDESCRIPTION: Customizes the recovery process within a PersistentActor to skip loading from snapshots and replay all journaled events. This is achieved by overriding the `recovery` method and returning a `Recovery` object configured with `SnapshotSelectionCriteria.None`. This approach is useful if snapshot serialization formats have changed in an incompatible way but should generally be avoided if events have been deleted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n@Override\npublic Recovery recovery() {\n  return Recovery.create(SnapshotSelectionCriteria.none());\n}\n```\n\n----------------------------------------\n\nTITLE: EventsByPersistenceId Query Implementation in Scala\nDESCRIPTION: Demonstrates querying events by persistenceId to retrieve events for a specific PersistentActor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query-leveldb.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval events = readJournal.eventsByPersistenceId(\"user-1337\", 0L, Long.MaxValue)\n```\n\n----------------------------------------\n\nTITLE: Wiring Components and Customizing Materialized Value Combination in Java\nDESCRIPTION: This Java snippet constructs a `RunnableGraph` by connecting `nestedSource` and `nestedSink`. It employs a custom combiner function with `toMat` that takes the materialized `CompletableFuture` from the source and the `Pair` of `CompletionStage`s from the sink. The function then selectively creates an instance of the `MyClass` helper class, packaging the desired materialized values (`p` and `conn.first()`) together.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\n// #mat-combine-4b\nRunnableGraph<MyClass> runnableGraph =\n    nestedSource.toMat(\n        nestedSink,\n        (p, conn) -> {\n          // Ignoring the CompletionStage<String> result\n          return new MyClass(p, conn.first());\n        });\n// #mat-combine-4b\n```\n\n----------------------------------------\n\nTITLE: Declaring prefixAndTail Operator for Akka Source in Scala\nDESCRIPTION: Defines the prefixAndTail operator for Akka Source in Scala. It takes a number n and returns a tuple containing a sequence of up to n elements and a Source for the remaining elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/prefixAndTail.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.prefixAndTail[U>:Out](n:Int):FlowOps.this.Repr[(scala.collection.immutable.Seq[Out],akka.stream.scaladsl.Source[U,akka.NotUsed])]\n```\n\n----------------------------------------\n\nTITLE: Using mapAsyncUnordered with Slow Service in Scala\nDESCRIPTION: Shows implementation of mapAsyncUnordered which processes elements as they complete, potentially out of order. Includes buffer configuration for controlling concurrent operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_23\n\nLANGUAGE: scala\nCODE:\n```\nSource(List(\"a\", \"B\", \"C\", \"D\", \"e\", \"F\", \"g\", \"H\", \"i\", \"J\"))\n  .map { s =>\n    println(s\"before: ${s}\")\n    s\n  }\n  .mapAsyncUnordered(4)(service.convert)\n  .map { s =>\n    println(s\"after: ${s}\")\n    s\n  }\n  .withAttributes(Attributes.inputBuffer(initial = 4, max = 4))\n  .runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Multi-Node Test Specification in Scala\nDESCRIPTION: This code demonstrates a complete multi-node test that initializes two nodes, uses a barrier for synchronization, and tests actor message passing between nodes. It includes environment setup, node identification, and remote messaging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-node-testing.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\npackage akka.remote.sample\n\n// #spec\nimport akka.actor._\nimport akka.remote.testkit.{ MultiNodeSpec, MultiNodeConfig }\nimport com.typesafe.config.ConfigFactory\n\nclass MultiNodeSampleSpecMultiJvmNode1 extends MultiNodeSample\nclass MultiNodeSampleSpecMultiJvmNode2 extends MultiNodeSample\n\nclass MultiNodeSample extends MultiNodeSpec(new MultiNodeSampleConfig) with STMultiNodeSpec {\n  import MultiNodeSampleConfig._\n\n  def initialParticipants = roles.size\n\n  \"A MultiNodeSample\" must {\n\n    \"wait for all nodes to enter a barrier\" in {\n      enterBarrier(\"startup\")\n    }\n\n    \"send to and receive from a remote node\" in {\n      runOn(node1) {\n        enterBarrier(\"deployed\")\n        val ponger = system.actorSelection(node(node2) / \"user\" / \"ponger\")\n        ponger ! Ping\n        expectMsg(Pong)\n      }\n\n      runOn(node2) {\n        system.actorOf(Props[Ponger], \"ponger\")\n        enterBarrier(\"deployed\")\n      }\n\n      enterBarrier(\"finished\")\n    }\n  }\n}\n\ncase object Ping\ncase object Pong\n\nclass Ponger extends Actor {\n  def receive = {\n    case Ping => sender() ! Pong\n  }\n}\n// #spec\n```\n\n----------------------------------------\n\nTITLE: Sending Elements to Typed Actor With Backpressure Using Akka Streams (Scala)\nDESCRIPTION: This Scala snippet demonstrates forwarding elements from an Akka Stream to a typed actor using the actorRefWithBackpressure sink operator, enabling explicit backpressure handling through acknowledgement messages. Dependencies include akka-actor-typed and akka-stream-typed; example requires a configured ActorSystem. Key parameters are the target ActorRef, message adapter for stream-to-actor translation, functions to set up handshake/init, message to signal completion, and how to handle errors. Output is a Sink for stream materialization with demand controlled by the actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorSink/actorRefWithBackpressure.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.typed.ActorRef\nimport akka.stream.scaladsl.{Sink, Source}\nimport akka.stream.typed.scaladsl.ActorSink\nimport akka.util.Timeout\nimport scala.concurrent.duration._\n\n// Message protocol for Ack\nsealed trait Command\ncase class Element(value: String, replyTo: ActorRef[Ack]) extends Command\ncase object Init extends Command\ncase object Complete extends Command\ncase class Fail(ex: Throwable) extends Command\ncase object Ack\n\nval actor: ActorRef[Command] = ??? // Initialized elsewhere\n\nval sink: Sink[String, akka.NotUsed] = ActorSink.actorRefWithBackpressure[\n  String, Command, Ack\n](\n  actor,\n  onMessage = (replyTo, elem) => Element(elem, replyTo),\n  onInitMessage = replyTo => Init,\n  ackMessage = Ack,\n  onCompleteMessage = Complete,\n  onFailureMessage = ex => Fail(ex)\n)\n\nSource(List(\"a\", \"b\", \"c\")).runWith(sink)\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency (sbt, Maven, Gradle)\nDESCRIPTION: Declares the necessary dependency for using Akka Streams in a project. It uses the Akka Bill of Materials (BOM) to manage versions consistently. Examples are shown for sbt, Maven, and Gradle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\nval AkkaVersion = \"$akka.version$\"\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-stream\" % AkkaVersion\n)\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<properties>\n  <scala.binary.version>$scala.binary.version$</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_${scala.binary.version}</artifactId>\n      <version>$akka.version$</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\ndef versions = [\n  ScalaBinary: \"$scala.binary.version$\"\n]\ndef akkaVersion = \"$akka.version$\"\n\ndependencies {\n  implementation platform(\"com.typesafe.akka:akka-bom_${versions.ScalaBinary}:\" + akkaVersion)\n\n  implementation \"com.typesafe.akka:akka-stream_${versions.ScalaBinary}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency\nDESCRIPTION: Dependency configuration for including Akka Streams Typed module for stream processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_10\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-stream-typed_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Merging Substreams after groupBy in Scala\nDESCRIPTION: Shows how to merge the elements from all substreams created by `groupBy` back into a single stream using `mergeSubstreams`. The order of elements in the merged stream depends on the emission order of the substreams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n//#groupBy3\nsource\n  .groupBy(maxSubstreams = 2, _ % 2 == 0)\n  .mergeSubstreams\n  .runWith(Sink.fold(0)((acc, i) => acc + i))\n//#groupBy3\n```\n\n----------------------------------------\n\nTITLE: Creating an ExtensionId for Database Connection Pool in Java\nDESCRIPTION: Defines the ExtensionId for the DatabaseConnectionPool extension in Java. This class is used to identify and access the extension within the ActorSystem.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\npublic static class Id extends ExtensionId<DatabaseConnectionPool> {\n  private static final Id instance = new Id();\n\n  private Id() {}\n\n  public static Id getInstance() {\n    return instance;\n  }\n\n  @Override\n  public DatabaseConnectionPool createExtension(ActorSystem<?> system) {\n    return new DatabaseConnectionPool(system);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Dependency Injection for Actors in Scala\nDESCRIPTION: Demonstrates factory method pattern for actor creation when using dependency injection frameworks. Shows indirect actor instantiation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\n#creating-indirectly\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Round Robin Pool Router in Akka\nDESCRIPTION: This configuration snippet demonstrates how to set up a pool of remote deployed routees using a round-robin router in Akka. It specifies the number of instances and target nodes for deployment.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_10\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.deployment {\n  /parent/remotePool {\n    router = round-robin-pool\n    nr-of-instances = 10\n    target.nodes = [\"akka://app@10.0.0.2:25520\", \"akka://app@10.0.0.3:25520\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Connection Closing Handler for NACK-Based Write Back-Pressure in Java\nDESCRIPTION: The closing state of the Java EchoHandler that manages graceful TCP connection shutdown while ensuring all buffered data is sent. It handles write command failures during the closing process.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\nprivate Receive closing(final Tcp.ConnectionClosed reason, final List<ByteString> buffer) {\n  return receiveBuilder()\n      .match(\n          Tcp.CommandFailed.class,\n          msg -> {\n            connection.tell(TcpMessage.resumeWriting(), getSelf());\n            return waitingForAckAndClosing(reason, buffer);\n          })\n      .match(\n          Ack.class,\n          msg -> {\n            if (buffer.isEmpty()) {\n              getContext().stop(getSelf());\n              return Stay();\n            } else {\n              connection.tell(\n                  TcpMessage.write(buffer.remove(0), Ack.getInstance()), getSelf());\n              return closing(reason, buffer);\n            }\n          })\n      .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Per-Stream Input Buffer Size in Akka Streams (Java)\nDESCRIPTION: This Java snippet shows how to configure a custom input buffer size for a specific flow or graph segment, overriding the default settings. It creates a flow with attributes defining an input buffer of size 1.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nfinal Flow<Integer, Integer, NotUsed> section =\n    Flow.of(Integer.class)\n        .map(i -> i * 2)\n        .withAttributes(Attributes.inputBuffer(1, 1));\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor Classification Bus in Scala\nDESCRIPTION: A Scala implementation of an EventBus using Actor Classification, where both events and subscribers are ActorRefs. This implementation is used for DeathWatch functionality in Akka.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\nclass ActorBusImpl extends\n  EventBus[ActorEvent, ActorRef, ActorRef] with ActorClassification {\n  // will be invoked for each event for all subscribers\n  override protected def publish(event: ActorEvent, subscriber: ActorRef): Unit = {\n    // in this case event is ActorRef and subscriber is ActorRef\n    if (event.sender == subscriber) subscriber ! event.payload\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Inet-Address DNS Resolution in Scala\nDESCRIPTION: Example of using the Inet-Address DNS provider directly through the actor API in Scala. Uses Dns.Resolve and Dns.Resolved messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-dns.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n#actor-api-inet-address\n```\n\n----------------------------------------\n\nTITLE: Implementing a Framing Protocol as a Bidirectional Flow using GraphStage (Scala)\nDESCRIPTION: This Scala snippet defines a GraphStage that implements a framing protocol for Akka Streams, enabling correct grouping and extraction of message frames from an incoming byte stream. It handles potentially partial or multiple messages per chunk, making it suitable for deserializing network frames. Dependencies include akka.stream, akka.util.ByteString, and GraphStage-related classes. Inputs are ByteStrings; outputs are grouped, extracted message ByteStrings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\nval framing = GraphDSL.create() { implicit b =>\n  val in = Inlet[ByteString](\"Framing.in\")\n  val out = Outlet[ByteString](\"Framing.out\")\n  val shape = FlowShape(in, out)\n  // framing logic...\n  FlowShape(in, out)\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing ByteBufferSerializer for Akka Remote Serialization in Scala\nDESCRIPTION: This code snippet demonstrates the ByteBufferSerializer interface in Scala for Akka remote serialization. It allows direct writing into a shared ByteBuffer for improved performance in high-throughput messaging scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\ntrait ByteBufferSerializer {\n  def toBinary(o: AnyRef, buf: ByteBuffer): Unit\n  def fromBinary(buf: ByteBuffer, manifest: String): AnyRef\n}\n```\n\n----------------------------------------\n\nTITLE: Using mapWithResource with Database in Scala\nDESCRIPTION: Implementation showing how to use mapWithResource to safely query a database using a shared connection in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapWithResource.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval blockingDb: BlockingDatabase = ???\n\nSource(List(1, 2, 3))\n  .mapWithResource(\n    () => blockingDb.newConnection())(  // create\n    (connection, id) => {               // extract\n      val byId = connection.queryById(id)\n      val byName = connection.queryByName(s\"name$id\")\n      byId ++ byName\n    },\n    connection => {                     // close\n      connection.close()\n      None\n    }\n  )\n  .mapConcat(identity)\n```\n\n----------------------------------------\n\nTITLE: Using PriorityDispatcher in Scala\nDESCRIPTION: Scala code demonstrating how to create an actor that uses a priority dispatcher. The actor will process messages according to the priority rules defined in the referenced mailbox implementation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n// We create a new Actor that just prints out what it processes\nval a = context.actorOf(\n  Props(new Actor {\n    val log: LoggingAdapter = Logging(context.system, this)\n    def receive = {\n      case x => log.info(x.toString)\n    }\n  }).withDispatcher(\"prio-dispatcher\")\n)\n\n// These are the messages we will send\na ! 'lowpriority\na ! 'lowpriority\na ! 'highpriority\na ! 'pigdog\na ! 'pigdog2\na ! 'pigdog3\na ! 'highpriority\na ! PoisonPill\n```\n\n----------------------------------------\n\nTITLE: Using Mutable State in Akka Persistence (Java)\nDESCRIPTION: Illustrates how to implement an event-sourced entity using mutable state in Java with Akka Persistence. This approach requires careful handling to ensure message immutability.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-style.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AccountExampleWithNullState.java](/akka-cluster-sharding-typed/src/test/java/jdocs/akka/cluster/sharding/typed/AccountExampleWithMutableState.java) { #account-entity }\n```\n\n----------------------------------------\n\nTITLE: Adding Item to Cart with Consistency in Scala\nDESCRIPTION: Scala example of adding an item to a cart in distributed storage using WriteMajority consistency. It demonstrates updating the data with proper consistency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\nprivate def addItem(userId: String, itemId: String, quantity: Int): Future[Done] = {\\n  implicit val timeout = Timeout(5.seconds)\\n  (replicator ? Update(DataKey(userId), Cart.empty, writeMajority, None) { cart =>\\n    cart.addItem(itemId, quantity)\\n  }).map(_ => Done)\\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Ask Pattern in Akka Actors - Java\nDESCRIPTION: Shows the ask pattern implementation with pipeTo for asynchronous request-response handling between actors using Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_27\n\nLANGUAGE: java\nCODE:\n```\n#import-ask #ask-pipe\n```\n\n----------------------------------------\n\nTITLE: Observing Mocked Behavior in Actor Tests - Java\nDESCRIPTION: Java snippet demonstrating the verification of interactions with mocked actors, easing the testing process of components that rely on other actors by employing mock responses.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AsyncTestingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/AsyncTestingExampleTest.java) { #test-observe-mocked-behavior }\n```\n\n----------------------------------------\n\nTITLE: Creating a Half-Closed TCP Server with Flow.fromSinkAndSource in Java\nDESCRIPTION: This Java snippet shows how to create a TCP server that cancels the incoming stream but continues to send periodic output to the client. It uses Flow.fromSinkAndSource to combine a Sink that cancels after processing one element and a Source that generates timestamps every second.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/fromSinkAndSource.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nFlow<ByteString, ByteString, NotUsed> handler =\n    Flow.fromSinkAndSource(\n        Sink.foreach(\n            data -> System.out.println(\"Server received: \" + data.utf8String())),\n        Source.tick(Duration.ZERO, Duration.ofSeconds(1), NotUsed.getInstance())\n            .map(\n                tick ->\n                    ByteString.fromString(\n                        \"Server time is \" + LocalTime.now().toString() + \"\\n\")));\n```\n\n----------------------------------------\n\nTITLE: Deflate-Decompressing Streams with Akka Streams Compression.inflate (Scala)\nDESCRIPTION: Provides the method signature for the Compression.inflate operator in Akka Streams when using Scala. This flow takes a stream of ByteStrings and decompresses them using the deflate algorithm, configurable via maxBytesPerChunk and nowrap parameters. Required dependency: Akka Streams library. Inputs and outputs are ByteString, and the maximum length of each emitted chunk is specified by maxBytesPerChunk. The nowrap flag determines the zlib header behavior. The Flow does not modify materialized values (returns akka.NotUsed).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Compression/inflate.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ndef inflate(maxBytesPerChunk: Int, nowrap: Boolean): akka.stream.scaladsl.Flow[akka.util.ByteString, akka.util.ByteString, akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Create Transform Sink in Scala\nDESCRIPTION: This Scala snippet demonstrates creating a reusable piece using a Flow to transform strings into ByteString before saving to a file with Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [QuickStartDocSpec.scala](/akka-docs/src/test/scala/docs/stream/QuickStartDocSpec.scala) { #transform-sink }\n```\n\n----------------------------------------\n\nTITLE: Enforcing Replies with DurableStateBehavior.withEnforcedReplies (Scala)\nDESCRIPTION: Shows the use of DurableStateBehavior.withEnforcedReplies in Scala to enforce that all command handlers provide a reply effect, guarding against missing replies at compile time. Requires Akka Typed and persistence packages. Inputs are command definitions with an ActorRef field; outputs must always include a reply or explicit noReply. This encourages strict contract adherence in the API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_23\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [AccountExampleWithCommandHandlersInDurableState.scala](/akka-cluster-sharding-typed/src/test/scala/docs/akka/cluster/sharding/typed/AccountExampleWithCommandHandlersInDurableState.scala) { #withEnforcedReplies }\n```\n\n----------------------------------------\n\nTITLE: Using ActorLogging Trait in Scala\nDESCRIPTION: Shows how to use the ActorLogging trait in Scala to add logging capabilities to an actor without explicitly creating a LoggingAdapter instance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nclass MyActor extends Actor with akka.actor.ActorLogging {\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Helper Class for Combined Materialized Values in Java\nDESCRIPTION: This Java snippet defines a simple helper class `MyClass`. This class is used to hold the combined materialized values from different parts of an Akka Stream graph, specifically a `CompletableFuture<Optional<Integer>>` and a `CompletionStage<OutgoingConnection>`, as demonstrated in the subsequent snippet.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\n// #mat-combine-4a\npublic static class MyClass {\n  private final CompletableFuture<Optional<Integer>> p;\n  private final CompletionStage<OutgoingConnection> conn;\n\n  public MyClass(CompletableFuture<Optional<Integer>> p, CompletionStage<OutgoingConnection> conn) {\n    this.p = p;\n    this.conn = conn;\n  }\n\n  public CompletableFuture<Optional<Integer>> getP() {\n    return p;\n  }\n\n  public CompletionStage<OutgoingConnection> getConn() {\n    return conn;\n  }\n}\n// #mat-combine-4a\n```\n\n----------------------------------------\n\nTITLE: Running Akka Streams with runWith in Java\nDESCRIPTION: Demonstrates the `runWith` convenience method in Java for Sources, Sinks, or Flows. This example likely shows attaching a Sink to a Source using `runWith` to materialize the stream and obtain the Sink's materialized value (e.g., a CompletionStage).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #materialization-runWith }\n```\n\n----------------------------------------\n\nTITLE: Combining Sinks with Fan-out Junction in Java\nDESCRIPTION: This example shows how to combine multiple sinks using a Fan-out Junction in Akka Streams with Java. It demonstrates the usage of Sink.combine with a custom fan-out strategy.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/combine.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSink<String> sink1 = Sink.foreach(System.out::println);\nSink<String> sink2 = Sink.foreach(System.out::println);\nSink<String> sink3 = Sink.foreach(System.out::println);\n\nSink<String> fanOutSink =\n    Sink.combine(\n        sink1,\n        sink2,\n        Arrays.asList(sink3),\n        (Integer fanOutAmount) -> Broadcast.create(String.class, fanOutAmount));\n\nSource.from(Arrays.asList(\"a\", \"b\", \"c\")).runWith(fanOutSink, system);\n```\n\n----------------------------------------\n\nTITLE: Extracting Context from Flow Elements in Scala\nDESCRIPTION: This snippet demonstrates how to use asFlowWithContext to extract correlation numbers as context from flow elements, focusing on the text message content. It shows the conversion of a regular flow to a FlowWithContext and back.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/asFlowWithContext.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval flow = Flow[(String, Int)]\n  .map(t => (t._1.toUpperCase, t._2))\n  .asFlowWithContext[String, Int, Int](\n    (text, correlationNumber) => (text, correlationNumber)\n  )(\n    _.length\n  )\n  .map(_.toLowerCase)\n\nval result = Source(List((\"abc\", 1), (\"def\", 2), (\"ghi\", 3)))\n  .via(flow)\n  .runWith(Sink.seq)\n\nresult.map(println)\n```\n\n----------------------------------------\n\nTITLE: ActorRef with Backpressure Sink in Java\nDESCRIPTION: Java implementation of an actor-based sink with backpressure support, showing message handling and acknowledgement patterns.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/actor-interop.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nstatic class Ack {}\n\nstatic class ExampleActor extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            \"init\",\n            s -> {\n              getSender().tell(\"ack\", getSelf());\n            })\n        .match(\n            String.class,\n            payload -> {\n              getSender().tell(new Ack(), getSelf());\n            })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Stateful Routing with PartitionHub in Java\nDESCRIPTION: This snippet shows how to implement stateful routing using PartitionHub in Java. It demonstrates a round-robin partitioning strategy that maintains state across invocations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nSink<String, Source<String, NotUsed>> statefulSink =\n  PartitionHub.ofStateful(\n    String.class,\n    () -> new RoundRobin(),\n    2,\n    256);\n```\n\n----------------------------------------\n\nTITLE: Metrics Listener Implementation in Java\nDESCRIPTION: Example Java implementation of an actor that listens to metrics events directly, allowing custom functionality based on cluster metrics data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n@@snip [MetricsListener.java](/akka-docs/src/test/java/jdocs/cluster/MetricsListener.java) { #metrics-listener }\n```\n\n----------------------------------------\n\nTITLE: Routing to Fastest Consumer with PartitionHub in Scala\nDESCRIPTION: This snippet demonstrates how to route elements to the fastest consumer using PartitionHub in Scala. It uses the queueSize information to determine the consumer with the least buffered elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\nval fastestSink = PartitionHub.statefulSink(\n  () => {\n    (info, elem) => {\n      val idx = info.consumerIds.minBy(id => info.queueSize(id))\n      idx\n    }\n  },\n  startAfterNrOfConsumers = 2,\n  bufferSize = 256)\n```\n\n----------------------------------------\n\nTITLE: Configuring a ThreadPoolExecutor-based Dispatcher in HOCON\nDESCRIPTION: HOCON configuration for a custom dispatcher using thread-pool-executor. Sets a fixed pool size and uses PinnedDispatcher type for dedicated thread allocation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_3\n\nLANGUAGE: hocon\nCODE:\n```\nblocking-io-dispatcher {\n  type = Dispatcher\n  executor = \"thread-pool-executor\"\n  thread-pool-executor {\n    fixed-pool-size = 32\n  }\n  throughput = 1\n}\n```\n\n----------------------------------------\n\nTITLE: Creating TestActorRef for Synchronous Actor Testing in Java\nDESCRIPTION: Shows how to create a TestActorRef for synchronous testing of an actor in Java, providing access to the underlying actor instance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_37\n\nLANGUAGE: java\nCODE:\n```\nfinal Props props = Props.create(MyActor.class);\nfinal TestActorRef<MyActor> ref = TestActorRef.create(system, props, \"testA\");\nfinal MyActor actor = ref.underlyingActor();\n```\n\n----------------------------------------\n\nTITLE: Cluster Subscriptions for Membership Events\nDESCRIPTION: Shows how to subscribe to cluster state change events using the 'subscriptions' provided by the Cluster API. This enables handling MemberEvent updates when nodes join or leave the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nBasicClusterExampleSpec.scala { #cluster-subscribe }\nBasicClusterExampleSpec.scala { #cluster-leave-example }\n```\n\n----------------------------------------\n\nTITLE: SRV Record Resolution in Scala\nDESCRIPTION: Example of resolving DNS SRV records using the Async DNS provider in Scala. Shows how to specifically request SRV record types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-dns.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n#srv\n```\n\n----------------------------------------\n\nTITLE: Development Logback Configuration with Multiple Outputs - Logback - XML\nDESCRIPTION: This snippet provides an example logback.xml tailored for development, showing how to simultaneously log to standard output and persist all debug-level logs to a file. This allows developers to monitor console output while retaining granular information for diagnosis. To use, update src/main/resources/logback.xml or src/test/resources/logback-test.xml as needed. Output encoders and appenders can be customized; dependencies mirror those of standard Logback usage.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_14\n\nLANGUAGE: xml\nCODE:\n```\n@@snip [logback.xml](/akka-actor-typed-tests/src/test/resources/logback-doc-dev.xml)\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive Streams Publishers with Sink.asPublisher in Java\nDESCRIPTION: Shows how to create a Publisher from a Sink in Java using both fanout=true to allow multiple subscribers and fanout=false to restrict to a single subscriber.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/asPublisher.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nfinal Source<Integer, NotUsed> source = Source.range(1, 5);\n\n// When fanout is set to true, the Sink can support multiple subscribers\nfinal Pair<NotUsed, Publisher<Integer>> publisherPairWithFanout =\n    source.toMat(Sink.asPublisher(AsPublisher.WITH_FANOUT), Keep.both()).run(system);\n\npublisherPairWithFanout.second().subscribe(createSubscriber(\"WithFanout-A\"));\npublisherPairWithFanout.second().subscribe(createSubscriber(\"WithFanout-B\"));\npublisherPairWithFanout.second().subscribe(createSubscriber(\"WithFanout-C\"));\n\n// expect 5 elements for each of the three subscribers\n\n// When fanout is set to false, the first subscriber will receive the elements\n// and the rest will be rejected\nfinal Pair<NotUsed, Publisher<Integer>> publisherPairWithoutFanout =\n    source.toMat(Sink.asPublisher(AsPublisher.WITHOUT_FANOUT), Keep.both()).run(system);\n\npublisherPairWithoutFanout.second().subscribe(createSubscriber(\"WithoutFanout-A\"));\npublisherPairWithoutFanout.second().subscribe(createSubscriber(\"WithoutFanout-B\")); // will receive onError\npublisherPairWithoutFanout.second().subscribe(createSubscriber(\"WithoutFanout-C\")); // will receive onError\n\n// expect 5 elements for subscriber \"WithoutFanout-A\", but error for \"WithoutFanout-B/C\"\n```\n\n----------------------------------------\n\nTITLE: Fault Handling Sample Implementation in Java\nDESCRIPTION: Implementation of fault tolerance patterns in Akka using Java. Shows interaction between Worker, CounterService, Counter, and Storage actors with supervision and failure recovery mechanisms.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance-sample.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n@@snip [FaultHandlingDocSample.java](/akka-docs/src/test/java/jdocs/actor/FaultHandlingDocSample.java) { #all }\n```\n\n----------------------------------------\n\nTITLE: Fail Buffer Strategy in Akka Streams\nDESCRIPTION: Implements a buffer that fails the stream when full. Useful for enforcing client quotas and preventing flooding.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\nexternalService.runWith(\n  Flow[Job].buffer(1000, OverflowStrategy.fail)\n)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(externalService)\n  .buffer(1000, OverflowStrategy.fail())\n  .run(system)\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with askWithStatus Operator (Akka Streams, Java)\nDESCRIPTION: This Java snippet illustrates how to use the ActorFlow.askWithStatus method in Akka Streams to send elements in a Java stream to a typed actor and handle typed StatusReply responses. Dependency on Akka Stream Typed and Typed libraries is required, along with configuration of the Akka library repository for Maven or Gradle. The code involves supplying the target actor ref, timeout, parallelism, and a BiFunction to create the message expecting a reply. Inputs are stream elements; outputs are unwrapped statuses, e.g., Strings. The stream may fail on ask timeout or actor termination. Inputs and outputs are strongly typed, and generic parameters are used for maximum flexibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithStatus.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.typed.ActorRef;\nimport akka.pattern.StatusReply;\nimport akka.stream.javadsl.Flow;\nimport akka.stream.javadsl.Sink;\nimport akka.stream.javadsl.Source;\nimport java.time.Duration;\n\n// Actor message definition\nclass AskingWithStatus {\n  public final ActorRef<StatusReply<String>> replyTo;\n  public AskingWithStatus(ActorRef<StatusReply<String>> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\nActorRef<AskingWithStatus> actorRef = ...;\nDuration timeout = Duration.ofSeconds(3);\nint parallelism = 5;\n\nFlow<Object, String, akka.NotUsed> flow =\n  akka.stream.typed.javadsl.ActorFlow.askWithStatus(\n      parallelism,\n      actorRef,\n      timeout,\n      (in, replyTo) -> new AskingWithStatus(replyTo)\n  );\n\n// Usage in a stream:\nSource.single(\"start-request\")\n      .via(flow)\n      .runWith(Sink.foreach(System.out::println), actorSystem);\n```\n\n----------------------------------------\n\nTITLE: Implementing Subchannel Classification Bus in Java\nDESCRIPTION: A Java implementation of an EventBus using Subchannel Classification. It demonstrates hierarchical event classification and publishing based on topic hierarchies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\npublic class SubchannelBusImpl extends EventBus<MsgEnvelope, String, ActorRef>\n    implements SubchannelClassification {\n  // Subclassification means that subscribers won't get messages with\n  // a classifier that is more specific than what they have subscribed to\n  @Override\n  public String classify(MsgEnvelope event) {\n    return event.topic;\n  }\n\n  // Topic can be freely defined by the publisher\n  // Subscribers receive messages if the topic is equal or a parent of\n  // what they have subscribed to\n  @Override\n  public Subclassification<String> subclassification() {\n    return new Subclassification<String>() {\n      @Override\n      public boolean isEqual(String x, String y) {\n        return x.equals(y);\n      }\n\n      @Override\n      public boolean isSubclass(String x, String y) {\n        return x.startsWith(y);\n      }\n    };\n  }\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for the event's classifier\n  @Override\n  public void publish(MsgEnvelope event, ActorRef subscriber) {\n    subscriber.tell(event.payload, ActorRef.noSender());\n  }\n\n  // may define the subscriber order which is used throughout the lifetime of the EventBus\n  @Override\n  public int compareSubscribers(ActorRef a, ActorRef b) {\n    return a.compareTo(b);\n  }\n\n  // determines the initial size of the index data structure\n  // used internally (i.e. the expected number of different classifiers)\n  @Override\n  public int mapSize() {\n    return 128;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Overriding Akka Stream Reference Configuration - HOCON\nDESCRIPTION: This HOCON snippet illustrates how to override global Akka StreamRefs settings in the application.conf file. By setting values under the akka.stream.materializer.stream-ref keyspace, administrators can globally adjust parameters like subscription timeouts and other stream reference behaviors for their application. Prerequisites: an Akka application with access to the application.conf; changes apply across all stream references unless specifically overridden via attributes. Key parameters include subscription-timeout, buffer-size, etc.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-refs.md#2025-04-22_snippet_7\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.stream.materializer.stream-ref {\n  subscription-timeout = 30s\n  # ... other stream reference settings\n}\n\n```\n\n----------------------------------------\n\nTITLE: Restarting Source on Failure with Backoff in Java\nDESCRIPTION: This example shows how the inner source is restarted with increasing backoff when it fails. The source emits 1, 2, 3, and then throws an exception. The first time the exception is thrown, the source is restarted after 1s, then 2s, etc., until the maxBackoff of 10s.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartSource/onFailuresWithBackoff.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nRestartSource.onFailuresWithBackoff(\n    Duration.ofSeconds(1),\n    Duration.ofSeconds(10),\n    0.2,\n    () ->\n        Source.from(Arrays.asList(1, 2, 3, 4))\n            .map(\n                n -> {\n                  if (n == 4) {\n                    throw new RuntimeException(\"Boom!\");\n                  } else {\n                    System.out.println(n);\n                    return n;\n                  }\n                }))\n    .run(system);\n```\n\n----------------------------------------\n\nTITLE: Handling Back-pressure with Buffering in Akka Streams (Scala)\nDESCRIPTION: Shows how to use the `buffer` operator in Scala to manage back-pressure when a downstream consumer is slow. It specifies a buffer size (10) and an `OverflowStrategy.dropHead`, which discards the oldest element when the buffer is full, ensuring the stream processes only the most recent elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_36\n\nLANGUAGE: scala\nCODE:\n```\ntweets\n  .buffer(10, OverflowStrategy.dropHead)\n  .map(slowComputation)\n  .runWith(Sink.ignore)\n//-//#tweets-slow-consumption-dropHead\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Failure Counting in Akka CircuitBreaker (Scala)\nDESCRIPTION: This Scala example demonstrates how to define custom failure criteria for a circuit breaker by treating even numbers as failures, regardless of whether the operation succeeds or fails.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n// We define a function that will determine whether to count the result as a failure.\n// In this example, we treat even numbers as failure, regardless whether\n// the operation succeeded or not\ndef defineEvenNumberAsFailure[T]: Try[T] => Boolean = {\n  case Success(i: Int) => i % 2 == 0\n  case Success(_)      => false\n  case Failure(_)      => true\n}\n\nval circuitBreaker = CircuitBreaker.create(\n  system.scheduler,\n  maxFailures = 1,\n  callTimeout = 1.second,\n  resetTimeout = 1.second)\n\n// Wrapping a future, defining even numbers as failure\nval oddAndEvenNumbers = Vector(1, 2, 3, 4, 5)\n\nval futures = oddAndEvenNumbers.map(n => {\n  circuitBreaker.withCircuitBreaker(\n    Future {\n      Thread.sleep(10)\n      n\n    },\n    defineEvenNumberAsFailure\n  )\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing ExtensionId in Java\nDESCRIPTION: Java implementation of ExtensionId for the Counter extension. It provides methods to create and access the extension instance within an ActorSystem.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic static class CountExtensionId extends AbstractExtensionId<CountExtension>\n    implements ExtensionIdProvider {\n  public static final CountExtensionId CountExtensionProvider = new CountExtensionId();\n\n  private CountExtensionId() {}\n\n  // This method will be called by Akka to instantiate our Extension\n  public CountExtension createExtension(ExtendedActorSystem system) {\n    return new CountExtension();\n  }\n\n  // The lookup method is required by ExtensionIdProvider,\n  // so we return ourselves here, this allows us\n  // to configure our extension to be loaded when the ActorSystem starts up\n  public ExtensionId<CountExtension> lookup() {\n    return CountExtensionProvider;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Java Stream to Akka Source in Java\nDESCRIPTION: Example showing how to create an Akka Source from a Java Stream in Java. The operator uses a Creator to generate a new stream for each materialization and handles backpressure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromJavaStream.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#from-javaStream\n```\n\n----------------------------------------\n\nTITLE: Collecting Missed Ticks in Akka Streams (Scala)\nDESCRIPTION: Uses conflateWithSeed to count missed ticks when the downstream is slower than the upstream tick source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_43\n\nLANGUAGE: Scala\nCODE:\n```\nval ticks = Source.tick(0.seconds, 1.second, \"\")\n\nval collectTicks = Flow[String].conflateWithSeed(seed = () => 0)((count, _) => count + 1)\n\nval flow = ticks\n  .via(collectTicks)\n  .take(10)\n\nflow.runForeach(println)\n```\n\n----------------------------------------\n\nTITLE: Using Sink.head Operator in Scala\nDESCRIPTION: Demonstrates using the Sink.head operator in Scala to obtain the first element from a stream. The operation returns a Future that completes with the first value and then cancels the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/head.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n//head-operator-example\n```\n\n----------------------------------------\n\nTITLE: Configuring Singleton Lease in Java\nDESCRIPTION: Example of loading lease configuration for an Akka Cluster Singleton from application configuration in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nConfig leaseConfig = system.settings().config().getConfig(\"pekko.cluster.singleton.singleton-lease\");\nClusterSingletonSettings singletonSettings = ClusterSingletonSettings.create(system)\n    .withLeaseSettings(Optional.of(LeaseUsageSettings.create(leaseConfig)));\n```\n\n----------------------------------------\n\nTITLE: Creating Akka Group Router in Java\nDESCRIPTION: This snippet illustrates how to create a group router in Akka using Java. It includes using the receptionist to manage group membership dynamically. Dependencies must include Akka Typed and appropriate cluster modules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n/* Group router configuration in RouterTest.java */\n```\n\n----------------------------------------\n\nTITLE: Using Extensions from Inside an Actor in Java\nDESCRIPTION: Shows how to access and use the Counter extension from within a Java actor. The extension is retrieved using the actor's context system.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\npublic class MyActor extends AbstractActor {\n  final CountExtension counter = CountExtensionId.CountExtensionProvider.get(getContext().getSystem());\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchAny(\n            message -> {\n              final long counterValue = counter.increment();\n              getSender().tell(counterValue, getSelf());\n            })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Application-specific Stop Message in Singleton Actor - Akka Cluster Typed - Java\nDESCRIPTION: Explains how to implement a stop message in a Java-based Akka singleton actor. On receiving the Stop command, the behavior performs cleanup (such as closing resources) and then stops. This mechanism enables graceful handover of singleton responsibility across cluster nodes. Prerequisite is Akka Typed and appropriate message definition.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\npublic class Stop implements CounterCommand {}\n\nBehavior<CounterCommand> counterBehaviorWithStop = Behaviors.receive((context, message) -> {\n  if (message instanceof Stop) {\n    // Application-specific cleanup\n    return Behaviors.stopped();\n  }\n  // handle other messages\n  return Behaviors.same();\n});\n```\n\n----------------------------------------\n\nTITLE: Using foldAsync to Asynchronously Aggregate Values in Java\nDESCRIPTION: Example of using foldAsync in Java to asynchronously accumulate stream values. This operator processes elements one by one, applying an async function to combine the current accumulator with each new element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/foldAsync.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.range(1, 100)\n    .map(i -> ThreadLocalRandom.current().nextInt(10))\n    .foldAsync(\n        new HashMap<Integer, Integer>(),\n        (map, next) -> {\n          // In case of an async computation, we need to create a CompletableFuture\n          CompletableFuture<HashMap<Integer, Integer>> stage = CompletableFuture.supplyAsync(() -> {\n            map.put(next, map.getOrDefault(next, 0) + 1);\n            return map;\n          });\n          return stage;\n        })\n```\n\n----------------------------------------\n\nTITLE: Initializing Sharding with Node Role Constraints in Scala\nDESCRIPTION: Demonstrates initializing Akka Cluster Sharding in Scala while specifying that entities of this type should only run on nodes with the role 'backend'.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.cluster.sharding.typed.scaladsl.Entity\nimport akka.cluster.sharding.typed.ClusterShardingSettings\n\n// #roles\nsharding.init(\n  Entity(TypeKey)(\n    createBehavior = entityContext =>\n      Counter(entityContext.entityId)\n  ).withSettings(ClusterShardingSettings(system).withRole(\"backend\")))\n// #roles\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Subscriber Actor in Java\nDESCRIPTION: Java implementation of a subscriber actor that registers for a specific topic with the DistributedPubSubMediator. The actor joins the 'content' topic and handles received messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nstatic class Subscriber extends AbstractActor {\n  String id = self().path().name();\n  ActorRef mediator = DistributedPubSub.get(getContext().getSystem()).mediator();\n  \n  @Override\n  public void preStart() {\n    // subscribe to the topic named \"content\"\n    mediator.tell(new Subscribe(\"content\", self()), self());\n  }\n  \n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            SubscribeAck.class,\n            msg -> msg.subscribe().topic().equals(\"content\") && msg.subscribe().ref().equals(self()),\n            s -> getContext().become(ready()))\n        .build();\n  }\n  \n  private Receive ready() {\n    return receiveBuilder().matchAny(msg -> System.out.println(\"Got: \" + msg)).build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Coordinated Shutdown Phases Configuration in HOCON\nDESCRIPTION: The reference configuration showing the default phases of the coordinated shutdown process in HOCON format. Defines the execution order of phases and their dependencies, timeouts, and recovery settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_10\n\nLANGUAGE: hocon\nCODE:\n```\ncoordinated-shutdown.phases {\n            # The first pre-defined phase during shutdown.\n            before-service-unbind {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 5s\n            }\n  \n            # The second pre-defined phase during shutdown.\n            service-unbind {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 10s\n              # If this phase fails the shutdown continues with the next phase.\n              # It is typically not the desired behavior to recover from failed\n              # service unbinding because clients might experience problems.\n              # recover = on\n              depends-on = [before-service-unbind]\n            }\n  \n            service-requests-done {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 35s\n              # If this phase fails the shutdown continues with the next phase.\n              # It is typically not desired to recover from this phase since that\n              # would mean that service requests are not completed.\n              # recover = on\n              depends-on = [service-unbind]\n            }\n    \n            # Phase for custom application tasks that are to be run\n            # after service shutdown and before cluster shutdown.\n            before-cluster-shutdown {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 10s\n              depends-on = [service-requests-done]\n            }\n    \n            # Shutdown cluster singletons\n            cluster-sharding-shutdown-region {\n              timeout = 10s\n              depends-on = [before-cluster-shutdown]\n            }\n  \n            # Emit the leave command for the node that is shutting down.\n            cluster-leave {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 10s\n              depends-on = [cluster-sharding-shutdown-region]\n            }\n    \n            # Shutdown cluster singleton manager\n            cluster-exiting {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 10s\n              depends-on = [cluster-leave]\n            }\n    \n            # When member status is removed the node has been removed from the cluster membership.\n            cluster-exiting-done {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 30s\n              depends-on = [cluster-exiting]\n            }\n    \n            # Shutdown remote daemon, which is mostly subsystems using remoting.\n            cluster-shutdown {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 10s\n              depends-on = [cluster-exiting-done]\n            }\n    \n            # Phase for custom application tasks that are to be run\n            # after cluster shutdown and before ActorSystem termination.\n            before-actor-system-terminate {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 5s\n              depends-on = [cluster-shutdown]\n            }\n    \n            # Last phase. See terminate-actor-system and exit-jvm above.\n            # Don't add phases that depends on this phase because the\n            # dispatcher and scheduler of the ActorSystem have been shutdown.\n            actor-system-terminate {\n              # Timeout in this phase will occur after the specified duration from the\n              # time the phase was started.\n              timeout = 10s\n              depends-on = [before-actor-system-terminate]\n            }\n          }\n```\n\n----------------------------------------\n\nTITLE: Implementing Movies Watch List with ORSet CRDT in Java\nDESCRIPTION: Example implementation of a movies watch list using Observed Remove Set (ORSet) CRDT in Java. The implementation demonstrates handling concurrent add/remove operations with automatic conflict resolution where add wins over remove.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class MovieWatchList {\n  private final String userId;\n  private final ORSet<String> movies;\n\n  public MovieWatchList(String userId, ORSet<String> movies) {\n    this.userId = userId;\n    this.movies = movies;\n  }\n\n  public MovieWatchList(String userId) {\n    this(userId, ORSet.empty());\n  }\n\n  public MovieWatchList add(String movieId) {\n    return new MovieWatchList(userId, movies.add(ReplicaId.random(), movieId));\n  }\n\n  public MovieWatchList remove(String movieId) {\n    return new MovieWatchList(userId, movies.remove(movieId));\n  }\n\n  public boolean contains(String movieId) {\n    return movies.getElements().contains(movieId);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Extending Source Operators with Custom Extension Methods - Akka Streams - Scala\nDESCRIPTION: Presents an approach for enriching Akka Streams' 'Source' type with custom extension methods via Scala implicits, allowing the addition of new methods in a concise way. This requires a working knowledge of Scala implicits, type parameters, and the Akka Streams API. Parameters typically include the 'Source' type to extend, with possible additional implicit class parameters. Input: a Source instance; Output: augmented functionality (e.g., '.myExtension()' methods). Limitations: applies only to Source or Flow, not SubFlow; subject to Scala type system constraints.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_29\n\nLANGUAGE: Scala\nCODE:\n```\n// @@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #extending-source }\n\n```\n\n----------------------------------------\n\nTITLE: Implementing PriorityMailbox in Java\nDESCRIPTION: Java implementation of a priority mailbox that sorts messages by type. This allows for prioritized message processing where certain message types like highpriority are handled before other types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.ActorSystem;\nimport akka.dispatch.PriorityGenerator;\nimport akka.dispatch.UnboundedPriorityMailbox;\nimport com.typesafe.config.Config;\n\npublic class MyPrioMailbox extends UnboundedPriorityMailbox {\n  // needed for reflective instantiation\n  public MyPrioMailbox(ActorSystem.Settings settings, Config config) {\n    // Create a new PriorityGenerator, lower prio means more important\n    super(new PriorityGenerator() {\n      @Override\n      public int gen(Object message) {\n        if (message.equals(\"highpriority\"))\n          return 0;\n        else if (message.equals(\"lowpriority\"))\n          return 2;\n        else if (message.equals(PoisonPill.getInstance()))\n          return 3;\n        else\n          return 1;\n      }\n    });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Lease Implementation Example in Java\nDESCRIPTION: Template for implementing a custom lease system in Java implementing the Lease interface.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordination.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n#lease-example\n```\n\n----------------------------------------\n\nTITLE: Implementing a Stream Splitter (Akka Streams Java)\nDESCRIPTION: Provides the Java equivalent for splitting strings in a stream into multiple sub-messages using Akka Streams, suitable for extracting structured data from composite payloads. Applies to a wide range of text and protocol processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeSplitter.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeSplitter.java) { #Simple-Split }\n```\n\n----------------------------------------\n\nTITLE: Dropping Elements with Conflate in Akka Streams (Java)\nDESCRIPTION: Java version of the conflate operator usage to drop elements when the upstream is faster than the downstream, keeping only the latest element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_40\n\nLANGUAGE: Java\nCODE:\n```\nSource.cycle(() -> Arrays.asList(1, 2, 3).iterator())\n  .throttle(3, Duration.ofSeconds(1))\n  .conflate((a, b) -> b)\n  .throttle(1, Duration.ofSeconds(1))\n  .runWith(Sink.foreach(System.out::println), mat);\n```\n\n----------------------------------------\n\nTITLE: Offering Streaming Data: SourceRef Example in Java\nDESCRIPTION: Example of how to offer a `SourceRef` within an Akka Cluster in Java, allowing a remote actor system to consume a locally prepared source of data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-refs.md#2025-04-22_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nsnip [FlowStreamRefsDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowStreamRefsDocTest.java) { #offer-source }\n```\n\nLANGUAGE: Java\nCODE:\n```\nsnip [FlowStreamRefsDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowStreamRefsDocTest.java) { #offer-source-use }\n```\n\n----------------------------------------\n\nTITLE: Deflate-Decompressing Streams with Akka Streams Compression.inflate (Java)\nDESCRIPTION: Shows the Java method signature for the Akka Streams Compression.inflate operator. This method allows decompression of a stream of ByteStrings using the deflate algorithm, with parameters for chunk size and zlib header usage (nowrap). Required dependency: Akka Streams Java API. Both input and output types are ByteString, supporting backpressure and emitting decompressed data as available. The function returns a Flow for seamless integration into Akka reactive stream pipelines.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Compression/inflate.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nFlow<ByteString, ByteString, NotUsed> inflate(int maxBytesPerChunk, boolean nowrap)\n```\n\n----------------------------------------\n\nTITLE: Sending Emails Asynchronously and Consuming the Stream (Java)\nDESCRIPTION: Constructs an email for each address in the `emailAddresses` stream and uses `mapAsync` to send them via the `EmailServer`. The `parallelism` factor controls concurrent sends. Finally, `Sink.ignore` consumes the stream, triggering its execution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nEmailServer emailServer = new EmailServer();\nRunnableGraph<NotUsed> sendEmails = emailAddresses\n    .mapAsync(parallelism, address -> {\n      return emailServer.send(new Email(address, \"Akka\", \"I like your tweet\"));\n    })\n    .to(Sink.ignore());\n\nsendEmails.run(system);\n```\n\n----------------------------------------\n\nTITLE: Creating Reply Effects in Akka DurableStateBehavior (Scala)\nDESCRIPTION: Provides an example of using Effect.reply and other reply-related methods in Scala to return reply effects from command handlers. The snippet demonstrates how to structure effectful replies within the actor state transitions. Prerequisites include Akka Persistence Typed and Scala syntax. All reply scenarios are represented, including explicit reply and noReply patterns.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_27\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [AccountExampleWithCommandHandlersInDurableState.scala](/akka-cluster-sharding-typed/src/test/scala/docs/akka/cluster/sharding/typed/AccountExampleWithCommandHandlersInDurableState.scala) { #reply }\n```\n\n----------------------------------------\n\nTITLE: Implementing FSM State Behavior in Java\nDESCRIPTION: Java equivalent of implementing a specific state behavior (idle state) in a Finite State Machine. Shows event handling and state transitions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fsm.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nprivate Behavior<Event> idle() {\n  return newReceiveBuilder()\n      .onMessage(SetTarget.class, this::onSetTarget)\n      .onMessage(Queue.class, this::onQueue)\n      .onMessage(Flush.class, message -> Behaviors.unhandled())\n      .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Interleaving Stream Elements in Java\nDESCRIPTION: Shows how to use the interleave operator in Java to alternate between elements from two sources. The example demonstrates combining elements with a specific segment size and eager close behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/interleave.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer> src1 = Source.from(Arrays.asList(1, 2, 3, 4));\\nSource<Integer> src2 = Source.from(Arrays.asList(10, 11, 12, 13));\\nSource<Integer> interleaved = src1.interleave(src2, 2, false);\n```\n\n----------------------------------------\n\nTITLE: Renaming Fields with JSON in Akka Persistence (Scala)\nDESCRIPTION: Scala implementation of renaming a field from 'code' to 'seatNr' using manual versioning with JSON in Akka Persistence. This approach is useful when the serialization format doesn't support automatic renames.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\ncase class SeatReserved(code: String)\n\nclass JsonSchemaEvolutionAdapter extends EventAdapter {\n  override def fromJournal(event: Any, manifest: String): EventSeq = event match {\n    case json: JsObject =>\n      val version = json.fields.get(\"_version\").map(_.convertTo[Int]).getOrElse(1)\n      version match {\n        case 1 =>\n          val seatNr = json.fields(\"code\").convertTo[String]\n          EventSeq.single(SeatReserved(seatNr))\n        case 2 =>\n          EventSeq(SeatReserved(json.fields(\"seatNr\").convertTo[String]))\n        case _ => throw new IllegalArgumentException(s\"Unknown version $version\")\n      }\n    case _ => EventSeq(event)\n  }\n\n  override def manifest(event: Any): String = \"\"\n\n  override def toJournal(event: Any): Any = event match {\n    case SeatReserved(seatNr) =>\n      JsObject(\n        \"_version\" -> JsNumber(2),\n        \"seatNr\" -> JsString(seatNr)\n      )\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using unfoldResourceAsync with Database in Scala\nDESCRIPTION: Implementation showing how to use unfoldResourceAsync to safely stream results from the async database API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldResourceAsync.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nSource.unfoldResourceAsync[Data, QueryResult](\n  () => database.runQuery(\"SELECT * FROM TWEETS\"),\n  queryResult => database.hasMore(queryResult),\n  queryResult => database.close(queryResult)\n)\n```\n\n----------------------------------------\n\nTITLE: UDP Simple Sender Implementation\nDESCRIPTION: Demonstrates the simplest form of UDP usage to send datagrams without requiring replies. Uses the SimpleSender message to create a sender actor that can send UTF-8 encoded strings to a predefined remote address.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-udp.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass SimpleSender extends Actor {\n  IO(Udp) ! SimpleSender\n  def receive = {\n    case SimpleSenderReady =>\n      context.become(ready(sender()))\n  }\n  def ready(send: ActorRef): Receive = {\n    case msg: String =>\n      send ! Send(ByteString(msg), remote)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Projecting into Different Store using Reactive Streams in Akka - Java\nDESCRIPTION: Java example of projecting events into a Reactive Streams compatible datastore using Akka. The code uses Akka Streams API to perform the projection efficiently. Dependencies include Akka Persistence and a Reactive Streams supporting database.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [PersistenceQueryDocTest.java](/akka-docs/src/test/java/jdocs/persistence/PersistenceQueryDocTest.java) { #projection-into-different-store-rs }\n```\n\n----------------------------------------\n\nTITLE: Routing to Fastest Consumer with PartitionHub in Java\nDESCRIPTION: This snippet demonstrates how to route elements to the fastest consumer using PartitionHub in Java. It uses the queueSize information to determine the consumer with the least buffered elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nSink<Integer, Source<Integer, NotUsed>> fastestSink =\n  PartitionHub.ofStateful(\n    Integer.class,\n    () ->\n      (info, elem) -> {\n        long fastestId =\n          info.consumerIds().stream()\n            .reduce((a, b) -> info.queueSize(a) < info.queueSize(b) ? a : b)\n            .get();\n        return fastestId;\n      },\n    2,\n    256);\n```\n\n----------------------------------------\n\nTITLE: Collecting Elements with End Signal in Java\nDESCRIPTION: Java implementation of collecting elements starting with 'b' and emitting them at stream end.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMapConcat.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(\"abc\", \"bcd\", \"bef\", \"cde\", \"end\"))\n    .statefulMapConcat(() -> {\n        final List<String> bs = new ArrayList<>();\n        return el -> {\n            if (el.equals(\"end\")) {\n                return new ArrayList<>(bs);\n            } else if (el.startsWith(\"b\")) {\n                bs.add(el);\n                return Collections.emptyList();\n            } else {\n                return Collections.emptyList();\n            }\n        };\n    })\n    .runWith(Sink.seq(), system);\n```\n\n----------------------------------------\n\nTITLE: Finalizing Publish-Subscribe Channel with Additional Features in Java\nDESCRIPTION: This snippet wraps the Sink and Source in a Flow, adds a KillSwitch, and implements a backpressure timeout. It creates a more robust publish-subscribe channel with additional control features in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nFlow<String, String, UniqueKillSwitch> publishSubscribeFlow =\n  Flow.fromSinkAndSource(sinkAndSource.first(), sinkAndSource.second())\n    .joinMat(KillSwitches.singleBidi(), Keep.right())\n    .backpressureTimeout(Duration.ofSeconds(3));\n```\n\n----------------------------------------\n\nTITLE: Combining Materialized Values for a Composite Flow in Java\nDESCRIPTION: This Java snippet demonstrates creating a composite Akka Stream `Flow`. It chains `flow1` and `flow2` together using `viaMat`. The `flow2` materializes a `CompletionStage<OutgoingConnection>`. The `Keep.right()` combiner function ensures that the composite `nestedFlow` retains this `CompletionStage` as its materialized value, propagating the connection information.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_14\n\nLANGUAGE: java\nCODE:\n```\n// #mat-combine-2\nFlow<Integer, Integer, NotUsed> flow1 = Flow.of(Integer.class).map(i -> i * 2);\nFlow<Integer, HttpResponse, CompletionStage<OutgoingConnection>> flow2 =\n    http.outgoingConnection(\"localhost\", 6001);\n\nFlow<Integer, HttpResponse, CompletionStage<OutgoingConnection>> nestedFlow =\n    flow1.viaMat(flow2, Keep.right()).named(\"nestedFlow\");\n// #mat-combine-2\n```\n\n----------------------------------------\n\nTITLE: Reactive Streams Semantics for lazyCompletionStageSink\nDESCRIPTION: Describes the cancellation and backpressure behavior of the lazyCompletionStageSink operator. It specifies that cancellation occurs if the future fails or if the created sink cancels, and backpressure is applied during initialization and when the created sink backpressures.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/lazyCompletionStageSink.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n**cancels** if the future fails or if the created sink cancels \n\n**backpressures** when initialized and when created sink backpressures\n```\n\n----------------------------------------\n\nTITLE: Adding Logback Dependency - sbt - Scala\nDESCRIPTION: This snippet specifies the build configuration for adding the 'logback-classic' dependency in an sbt, Maven, or Gradle project. It sets the group, artifact, and version required for Logback, which acts as the SLF4J backend. This step is prerequisite for enabling structured and performant logging via Logback in Akka projects. Inputs should be adapted for the user's build tool, and the version string should be resolved to a specific release.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_12\n\nLANGUAGE: sbt\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  group=\"ch.qos.logback\"\n  artifact=\"logback-classic\"\n  version=\"$logback_version$\"\n}\n```\n\n----------------------------------------\n\nTITLE: Reusing RunnableGraph in Akka Streams (Scala)\nDESCRIPTION: Illustrates how a RunnableGraph can be reused and materialized multiple times, producing different results for each materialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_40\n\nLANGUAGE: Scala\nCODE:\n```\nval sumSink = Sink.fold[Int, Int](0)(_ + _)\nval counterRunnableGraph: RunnableGraph[Future[Int]] =\n  tweetsInMinuteFromNow\n    .filter(_.hashtags.contains(akkaTag))\n    .map(t => 1)\n    .toMat(sumSink)(Keep.right)\n\n// materialize the stream once in the morning\nval morningTweetsCount: Future[Int] = counterRunnableGraph.run()\nmorningTweetsCount.foreach(c => println(s\"\"\"Tweets counted in the morning: $c\"\"\"))\n\n// and once in the evening, reusing the flow\nval eveningTweetsCount: Future[Int] = counterRunnableGraph.run()\neveningTweetsCount.foreach(c => println(s\"\"\"Tweets counted in the evening: $c\"\"\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Control Messages in Java\nDESCRIPTION: Java implementation of control messages that will be prioritized by a control-aware mailbox. This shows how to create control messages that will be processed with higher priority.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\nimport akka.dispatch.ControlMessage;\n\npublic enum MyControlMessage implements ControlMessage {\n  INSTANCE\n}\n```\n\n----------------------------------------\n\nTITLE: Transforming Author Handles to Email Addresses with mapAsync (Java)\nDESCRIPTION: Uses `mapAsync` on the `authors` stream to asynchronously look up the email address for each author's handle using the `lookupEmail` method. It allows up to 4 lookups to run concurrently while preserving the original order. Results are filtered to keep only found email addresses.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nint parallelism = 4;\nSource<String, NotUsed> emailAddresses = authors\n    .mapAsync(parallelism, author -> lookupEmail(author.handle))\n    .filter(Optional::isPresent)\n    .map(Optional::get);\n```\n\n----------------------------------------\n\nTITLE: Parameter Management in Akka with Scala\nDESCRIPTION: This snippet demonstrates how to manage multiple parameters in Akka Typed using Scala by encapsulating them in a class, reducing boilerplate and complexity. It begins with adding parameters directly, then refactors them into a single class to simplify handling. The functional style is maintained while enhancing manageability.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nStyleGuideDocExamples.scala { #fun-style-setup-params1 }\n```\n\nLANGUAGE: Scala\nCODE:\n```\nStyleGuideDocExamples.scala { #fun-style-setup-params2 }\n```\n\nLANGUAGE: Scala\nCODE:\n```\nStyleGuideDocExamples.scala { #fun-style-setup-params3 }\n```\n\nLANGUAGE: Scala\nCODE:\n```\nStyleGuideDocExamples.scala { #fun-style-setup-params4 }\n```\n\n----------------------------------------\n\nTITLE: Splitting a Stream using groupBy in Java\nDESCRIPTION: Demonstrates splitting an Akka Stream Source of integers into substreams based on whether the number is even or odd using the `groupBy` operator. Each substream corresponds to a unique key (0 for even, 1 for odd). The `maxSubstreams` parameter limits the number of concurrent substreams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n//#groupBy1\nfinal Source<Integer, NotUsed> source = Source.range(1, 10);\nsource\n    .groupBy(2, element -> element % 2 == 0)\n    // add sink here, as discussed below\n    .to(Sink.ignore())\n    .run(system);\n//#groupBy1\n```\n\n----------------------------------------\n\nTITLE: Reliable Delivery Documentation\nDESCRIPTION: Documentation describing the reliable message delivery and flow control features between cluster actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/includes/cluster.md#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n### Reliable Delivery\n\nReliable delivery and flow control of messages between actors in the Cluster.\n```\n\n----------------------------------------\n\nTITLE: Using Dedicated Dispatcher for Blocking Operations (Java)\nDESCRIPTION: This snippet shows how to use a dedicated dispatcher for blocking operations in Java, which helps prevent thread starvation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [SeparateDispatcherCompletionStageActor.java](/akka-docs/src/test/java/jdocs/actor/typed/SeparateDispatcherCompletionStageActor.java) { #separate-dispatcher }\n```\n\n----------------------------------------\n\nTITLE: Selecting a Dispatcher for Actor System in Java\nDESCRIPTION: Illustrates how to select a custom dispatcher for spawning an actor in Java using DispatcherSelector. Requires akka-actor-typed library. Employs methods like DispatcherSelector.defaultDispatcher for dispatcher selection. Used to create Props instances for configuring dispatchers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [DispatcherDocTest.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/DispatchersDocTest.java) { #spawn-dispatcher }\n```\n\n----------------------------------------\n\nTITLE: Retrieving Data with Get Operation in Scala\nDESCRIPTION: Example of sending a Get message to the Replicator to retrieve the current value of a data. A consistency level is supplied to determine how many replicas must respond before the request is considered successful.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nval Counter1Key = PNCounterKey(\"counter1\")\nreplicator ! Get(Counter1Key, ReadLocal)\n```\n\n----------------------------------------\n\nTITLE: Manual Scheduling for Timing-Sensitive Tests in Scala\nDESCRIPTION: Demonstrates how to use manual scheduling in Akka tests to control timing and make tests more reliable. This example shows how to advance the clock explicitly in a test scenario.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ManualTimerExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/ManualTimerExampleSpec.scala) { #manual-scheduling-simple }\n```\n\n----------------------------------------\n\nTITLE: Renaming Classes in Serialization - Akka with Jackson\nDESCRIPTION: Shows how to rename serialized classes and manage this change through configuration and migration code in Akka using Jackson. The transformation of the class name is handled by overriding the transformClassName method.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [OrderPlacedMigration.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v2a/OrderPlacedMigration.scala) { #rename-class }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [OrderPlacedMigration.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v2a/OrderPlacedMigration.java) { #rename-class }\n```\n\n----------------------------------------\n\nTITLE: Event Handler Updates State in Akka Persistence - Scala\nDESCRIPTION: An event handler in Akka that updates the actor's state by appending an item and managing list size limits, using Scala. Triggered post-event persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BasicPersistentBehaviorCompileOnly.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/BasicPersistentBehaviorCompileOnly.scala) { #event-handler }\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.actorRef in Scala\nDESCRIPTION: Example showing how to use Source.actorRef operator in Scala to create an ActorRef that emits messages to a stream. The example demonstrates message handling and stream completion.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/actorRef.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n#actorRef\n```\n\n----------------------------------------\n\nTITLE: Renaming Fields with Protobuf in Akka Persistence\nDESCRIPTION: Example of renaming a field from 'code' to 'seatNr' using Protobuf IDL in Akka Persistence. This demonstrates how IDL-based serializers handle renames efficiently.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_10\n\nLANGUAGE: Protobuf\nCODE:\n```\nmessage SeatReserved {\n  string seatNr = 1; // formerly 'code'\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Service Lookup in Scala\nDESCRIPTION: Basic example of looking up a service by name in Scala. This performs a simple lookup with just the service name and returns a Future with the resolved targets.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval lookup: Future[Resolved] =\n  discovery.lookup(Lookup(\"service-name\"), resolveTimeout)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Sources Using concatAllLazy in Scala\nDESCRIPTION: Demonstrates sequential concatenation of multiple sources using concatAllLazy operator in Scala. The example shows how sources are concatenated after the original upstream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/concatAllLazy.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(List(1, 2, 3))\nval source2 = Source(List(4, 5, 6))\nval source3 = Source(List(7, 8, 9))\n\nval concatLazy = source.concatAllLazy(source2, source3)\n```\n\n----------------------------------------\n\nTITLE: Configuring PriorityDispatcher\nDESCRIPTION: HOCON configuration for a custom dispatcher that uses a priority mailbox. This associates the dispatcher with a specific mailbox implementation, enabling prioritized message handling for all actors using this dispatcher.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_5\n\nLANGUAGE: hocon\nCODE:\n```\nprio-dispatcher {\n  mailbox-type = \"docs.dispatcher.DispatcherDocSpec$MyPrioMailbox\"\n}\n```\n\n----------------------------------------\n\nTITLE: Actor Termination Task in Java\nDESCRIPTION: Demonstrates how to add a task that sends a message to an actor and waits for its termination in Java. This is a convenience method for shutdown tasks that involve stopping actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nCoordinatedShutdown.get(system).addActorTerminationTask(\n    CoordinatedShutdown.PhaseBeforeServiceUnbind(),\n    \"someTaskName\",\n    actor,\n    new Shutdown());\n```\n\n----------------------------------------\n\nTITLE: Implementing Scanning Classification in Akka EventBus\nDESCRIPTION: Demonstrates implementation of a scanning classifier for EventBus that handles overlapping classifiers across the event space. This approach is useful when classifiers don't form a strict hierarchy and need to cover various parts of the event space independently.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/event-stream.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [EventBusDocSpec.scala](/akka-docs/src/test/scala/docs/event/EventBusDocSpec.scala) { #scanning-bus }\n```\n\nLANGUAGE: java\nCODE:\n```\n@@snip [EventBusDocTest.java](/akka-docs/src/test/java/jdocs/event/EventBusDocTest.java) { #scanning-bus }\n```\n\n----------------------------------------\n\nTITLE: Using Flag Data Type in Scala\nDESCRIPTION: Example of using the Flag data type in Scala. It demonstrates creating a Flag, switching it to true, and checking its value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\nval flag = Flag.empty\nval f2 = flag.switchOn\nflag.enabled // false\nf2.enabled // true\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.repeat in Java\nDESCRIPTION: Example demonstrating the usage of Source.repeat to emit the first 4 elements of a repeated value stream in Java. The source continuously emits the same value and is limited using a take operator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/repeat.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#repeat\n```\n\n----------------------------------------\n\nTITLE: Custom Mailbox Marker Interface in Scala\nDESCRIPTION: A marker interface implementation that indicates a custom mailbox queue semantics in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/mailboxes.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\n// Marker trait used for mailbox requirements mapping\ntrait MyUnboundedMessageQueueSemantics\n```\n\n----------------------------------------\n\nTITLE: Using Request Context with Get in Scala\nDESCRIPTION: Example of passing a request context with a Get message, which is included in the reply messages. This allows for the original sender to be replied to after receiving and transforming GetSuccess.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\nval Counter1Key = PNCounterKey(\"counter1\")\nreplicator ! Get(Counter1Key, ReadLocal, request = Some(sender()))\n\n// reply will be sent to the original sender\nreceive {\n  case g @ GetSuccess(Counter1Key, Some(replyTo: ActorRef)) =>\n    val value = g.get(Counter1Key).value\n    replyTo ! value\n  case NotFound(Counter1Key, Some(replyTo: ActorRef)) =>\n    replyTo ! 0\n  case GetFailure(Counter1Key, Some(replyTo: ActorRef)) =>\n    replyTo ! -1\n}\n```\n\n----------------------------------------\n\nTITLE: Nested Persist Calls in Akka Actors\nDESCRIPTION: Demonstrates how to safely nest persist and persistAsync calls within callbacks while maintaining proper thread safety and stashing guarantees.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_21\n\nLANGUAGE: Scala\nCODE:\n```\nclass NestedPersists extends PersistentActor {\n  override def persistenceId = \"nested-persists-actor-1\"\n\n  override def receiveCommand: Receive = {\n    case c: String =>\n      persist(s\"$c-outer-1\") { outer1 =>\n        sender() ! outer1\n        persist(s\"$c-inner-1\") { inner1 =>\n          sender() ! inner1\n        }\n      }\n\n      persist(s\"$c-outer-2\") { outer2 =>\n        sender() ! outer2\n        persist(s\"$c-inner-2\") { inner2 =>\n          sender() ! inner2\n        }\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Expiry Settings in Akka Cluster Distributed Data\nDESCRIPTION: Configures expiry settings for distributed data entries in an Akka cluster. Expiry settings determine how long a key can remain inactive before it is removed from the system. Prefix matching with '*' is supported, allowing bulk configuration of multiple keys with a shared prefix.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_8\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.cluster.distributed-data.expire-keys-after-inactivity {\n \"key-1\" = 10 minutes\n \"cache-*\" = 2 minutes\n}\n\n```\n\n----------------------------------------\n\nTITLE: Buffer Until Changed Pattern in Java\nDESCRIPTION: Java implementation that buffers elements until the incoming element changes, then emits the buffered elements downstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMap.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(Arrays.asList(1, 1, 1, 2, 2, 3, 3, 3))\n    .statefulMap(\n        () -> new Pair<>(new ArrayList<Integer>(), Optional.empty()),\n        (Pair<ArrayList<Integer>, Optional<Integer>> state, Integer element) -> {\n          ArrayList<Integer> buffer = state.first();\n          Optional<Integer> prev = state.second();\n          if (prev.isPresent() && prev.get().equals(element)) {\n            buffer.add(element);\n            return new Pair<>(\n                new Pair<>(buffer, prev),\n                new ArrayList<Integer>());\n          } else {\n            ArrayList<Integer> emitBuffer = buffer;\n            ArrayList<Integer> newBuffer = new ArrayList<>();\n            newBuffer.add(element);\n            return new Pair<>(\n                new Pair<>(newBuffer, Optional.of(element)),\n                emitBuffer);\n          }\n        },\n        state -> Optional.of(state.first()));\n```\n\n----------------------------------------\n\nTITLE: Logging Stream Errors (Scala)\nDESCRIPTION: Demonstrates logging errors in an Akka Stream using the `log()` operator in Scala. The stream processes integers, intentionally causing an `ArithmeticException` when dividing by zero, which is then logged.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimplicit val system: ActorSystem = ActorSystem()\n\nSource(0 to 5)\n  .map(100 / _)\n  .log(\"error logging\")\n  .runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Using LWWRegister in Scala\nDESCRIPTION: Example of using LWWRegister (last writer wins register) in Scala. It demonstrates creating a register, updating its value, and retrieving the value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\nval r1 = LWWRegister(\"value1\")\nval r2 = r1.withValue(\"value2\")\nr1.value // \"value1\"\nr2.value // \"value2\"\n```\n\n----------------------------------------\n\nTITLE: Not Recommended Usage of lazySource with Queue Sink in Scala\nDESCRIPTION: Example demonstrating how using lazySource with Sink.queue may not achieve expected lazy initialization due to queue's immediate buffering behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazySource.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#not-a-good-example\n```\n\n----------------------------------------\n\nTITLE: Limiting ByteString Stream Size in Scala\nDESCRIPTION: Implements a GraphStage to fail the stream if more than a given maximum of bytes has been consumed. It updates a counter in onPush() and signals failure if the limit is exceeded.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_51\n\nLANGUAGE: Scala\nCODE:\n```\nclass ByteLimiter(maximumBytes: Long) extends GraphStage[FlowShape[ByteString, ByteString]] {\n  val in = Inlet[ByteString](\"ByteLimiter.in\")\n  val out = Outlet[ByteString](\"ByteLimiter.out\")\n  override val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) with InHandler with OutHandler {\n      private var count = 0L\n      setHandlers(in, out, this)\n\n      override def onPush(): Unit = {\n        val chunk = grab(in)\n        count += chunk.size\n        if (count > maximumBytes) failStage(new IllegalStateException(\"Too many bytes!\"))\n        else push(out, chunk)\n      }\n\n      override def onPull(): Unit = pull(in)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Actor Protocol for UnfoldAsync in Java\nDESCRIPTION: Defines the protocol classes for communicating with an actor that provides chunks of bytes from an offset.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldAsync.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic interface Protocol {}\n\nfinal class GetChunk implements Protocol {\n    private final long offset;\n\n    public GetChunk(long offset) {\n        this.offset = offset;\n    }\n\n    public long getOffset() {\n        return offset;\n    }\n}\n\nfinal class Chunk implements Protocol {\n    private final ByteString bytes;\n\n    public Chunk(ByteString bytes) {\n        this.bytes = bytes;\n    }\n\n    public ByteString getBytes() {\n        return bytes;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing with Akka Streams using Scala\nDESCRIPTION: This snippet illustrates Patrik's method of using two frying pans for parallel pancake cooking using Akka Streams in Scala. It showcases how to distribute tasks evenly across multiple actors for simultaneous processing. The setup requires Akka Streams configuration. Inputs are processed concurrently, enhancing scalability and throughput.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-parallelism.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowParallelismDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowParallelismDocSpec.scala) { #parallelism }\n```\n\n----------------------------------------\n\nTITLE: Configuring Number of Shards in Akka\nDESCRIPTION: HOCON configuration snippet showing how to set the number of shards for cluster sharding\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_16\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.sharding.least-shard-allocation-strategy.rebalance-absolute-limit = 20\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in Maven\nDESCRIPTION: Specifies the Akka library repository configuration within a Maven `pom.xml`. This allows Maven to resolve Akka dependencies from the official repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\n----------------------------------------\n\nTITLE: Overriding Persistence Plugins in Java Actor\nDESCRIPTION: Example of a Java persistent actor that overrides both the journal and snapshot store plugin IDs. This allows using different persistence plugins for different actors in the same system.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_44\n\nLANGUAGE: Java\nCODE:\n```\nclass OverridePluginsActor extends AbstractPersistentActor {\n  private final String id;\n\n  public OverridePluginsActor(String id) {\n    this.id = id;\n  }\n\n  @Override\n  public String persistenceId() {\n    return id;\n  }\n\n  @Override\n  public String journalPluginId() {\n    return \"akka.persistence.journal.leveldb\";\n  }\n\n  @Override\n  public String snapshotPluginId() {\n    return \"akka.persistence.snapshot-store.local\";\n  }\n\n  // ... rest of the actor implementation ...\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Serializers\nDESCRIPTION: Configuration example showing how to bind serializer implementations and wire them to specific classes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nakka {\n  actor {\n    serializers {\n      java = \"akka.serialization.JavaSerializer\"\n      myown = \"docs.serialization.MyOwnSerializer\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Source from Partial Graph in Scala/Java\nDESCRIPTION: Demonstrates creating a Source by zipping together two numbers using GraphDSL.create and SourceShape.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\n#source-from-partial-graph-dsl\n```\n\nLANGUAGE: Java\nCODE:\n```\n#source-from-partial-graph-dsl\n```\n\n----------------------------------------\n\nTITLE: Using zipLatest Operator in Scala\nDESCRIPTION: Example demonstrating how to use the zipLatest operator to combine elements from multiple sources in Scala. Showcases the Source.zipLatest and Flow.zipLatest functionality.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipLatest.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#zipLatest-example\n```\n\n----------------------------------------\n\nTITLE: Creating an ActorRef Source in Akka Streams Typed - Java\nDESCRIPTION: This Java example illustrates using ActorSource.actorRef to create an ActorRef<T> for use in Akka Streams, accepting only messages matching the stream's type. The sample sets up predicates and functions for completion and failure, as well as buffer size and overflow strategy. Requires akka-stream-typed dependency, ActorSystem, and proper message types. Input is Java messages sent to the actor ref, output is emission into the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorSource/actorRef.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// ActorSourceExample.java\\n// #actor-source-ref\\nSource\\u003cMessage, ActorRef\\u003cMessage\\u003e\\u003e source =\\n  ActorSource.actorRef(\\n    msg => msg == Done.INSTANCE,\\n    msg => null,\\n    8,\\n    OverflowStrategy.dropHead\\n  );\\n// #actor-source-ref\n```\n\n----------------------------------------\n\nTITLE: Combining Materialized Values in Akka Streams - Scala\nDESCRIPTION: Example showing how to combine materialized values from different stream operators in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(1 to 3)\nval flow = Flow[Int].map(_ * 2)\nval sink = Sink.fold(0)(_ + _)\n\n// connect the Source to the Sink, obtaining a RunnableGraph\nval runnable: RunnableGraph[Future[Int]] =\n  source.viaMat(flow)(Keep.none).toMat(sink)(Keep.right)\n\n// materialize the flow\nval sum: Future[Int] = runnable.run()\n```\n\n----------------------------------------\n\nTITLE: Using Broadcast to Aggregate Stream Values in Java\nDESCRIPTION: This Java snippet illustrates using `builder.broadcast(2)` within a `GraphDSL` to split a stream of integers. One output path calculates the sum using `reduce`, while the other calculates the count using `reduce`. The results are then zipped together and printed. It requires importing necessary Akka Streams classes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Broadcast.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport akka.NotUsed;\nimport akka.stream.*;\nimport akka.stream.javadsl.*;\nimport jdocs.AbstractJavaTest;\n```\n\nLANGUAGE: java\nCODE:\n```\nfinal Source<Integer, NotUsed> source = Source.range(1, 10);\n\nfinal Sink<BigInteger, CompletionStage<BigInteger>> sumSink =\n    Sink.<BigInteger, Integer>fold(BigInteger.ZERO, (acc, el) -> acc.add(BigInteger.valueOf(el)));\nfinal Sink<Integer, CompletionStage<Integer>> countSink =\n    Sink.<Integer, Integer>fold(0, (acc, el) -> acc + 1);\n\nfinal RunnableGraph<Pair<CompletionStage<BigInteger>, CompletionStage<Integer>>> graph =\n    RunnableGraph.<Pair<CompletionStage<BigInteger>, CompletionStage<Integer>>>fromGraph(\n        GraphDSL.create(\n            sumSink, // S1\n            countSink, // S2\n            Keep.both(),\n            (builder, s1, s2) -> {\n              final UniformFanOutShape<Integer, Integer> broadcast =\n                  builder.add(Broadcast.create(2));\n\n              builder.from(builder.add(source)).toFanOut(broadcast);\n\n              builder.from(broadcast.out(0)).via(builder.add(Flow.of(Integer.class).map(BigInteger::valueOf))).to(s1);\n              builder.from(broadcast.out(1)).to(s2);\n              return ClosedShape.getInstance();\n            }));\n\nfinal Pair<CompletionStage<BigInteger>, CompletionStage<Integer>> result = graph.run(system);\n\nresult\n    .first()\n    .thenAcceptAsync(s -> System.out.println(\"Sum: \" + s), system.dispatcher());\nresult\n    .second()\n    .thenAcceptAsync(c -> System.out.println(\"Count: \" + c), system.dispatcher());\n```\n\n----------------------------------------\n\nTITLE: Actor Lifecycle Callbacks in Java\nDESCRIPTION: Demonstrates the default lifecycle callback implementations in AbstractActor class for Java, including preStart, postStop, preRestart, and postRestart methods.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n#lifecycle-callbacks\n```\n\n----------------------------------------\n\nTITLE: After Pattern Usage in Scala\nDESCRIPTION: Example showing how to use the after() pattern in Scala to complete a Future with a value or exception after a timeout.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/futures.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [FutureDocSpec.scala](/akka-docs/src/test/scala/docs/future/FutureDocSpec.scala) { #after }\n```\n\n----------------------------------------\n\nTITLE: Terminating an ActorSystem in Scala and Java\nDESCRIPTION: Shows how to properly terminate an ActorSystem in both Scala and Java by calling the terminate() method. This initiates the CoordinatedShutdown process which stops all running actors in the system.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/actor-systems.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nterminate()\n```\n\nLANGUAGE: java\nCODE:\n```\nterminate()\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to a Sharded Entity in Java\nDESCRIPTION: Shows how to get an `EntityRef` for a specific entity ID using its `TypeKey` and then send a message (`Increment.INSTANCE`) to that entity in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nimport akka.cluster.sharding.typed.javadsl.EntityRef;\n\n// #send\n// constructor takes TypeKey and entityId\nEntityRef<Counter.Command> counterOne =\n    sharding.entityRefFor(Counter.TypeKey, \"counter-1\");\ncounterOne.tell(Counter.Increment.INSTANCE);\n// #send\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Classification Implementation\nDESCRIPTION: Test cases for the actor classification implementation demonstrating proper handling of actor-based subscriptions and event routing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/event-stream.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [EventBusDocSpec.scala](/akka-docs/src/test/scala/docs/event/EventBusDocSpec.scala) { #actor-bus-test }\n```\n\nLANGUAGE: java\nCODE:\n```\n@@snip [EventBusDocTest.java](/akka-docs/src/test/java/jdocs/event/EventBusDocTest.java) { #actor-bus-test }\n```\n\n----------------------------------------\n\nTITLE: Defining Akka Repository with sbt, Maven, and Gradle - Configuration Snippet\nDESCRIPTION: This configuration snippet shows how to add the Akka library repository to your build tool (sbt, Maven, or Gradle) to fetch Akka dependencies. The repository URL should be added to your build tool's configuration as shown, enabling resolution of all Akka library artifacts. This dependency is required for using ActorSource and related operators.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorSource/actorRef.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nid=\\\"akka-repository\\\"\\nname=\\\"Akka library repository\\\"\\nurl=\\\"https://repo.akka.io/maven\\\"\n```\n\n----------------------------------------\n\nTITLE: Using Custom Router Group Programmatically in Akka\nDESCRIPTION: Demonstrates how to use the custom RedundancyGroup router by creating it programmatically and sending messages to it. The router will replicate each message to multiple routees.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_45\n\nLANGUAGE: Scala\nCODE:\n```\nval paths = List(\"/user/a1\", \"/user/a2\", \"/user/a3\")\nval router = system.actorOf(RedundancyGroup(paths, nbrCopies = 2).props(), \"redundancy1\")\n```\n\nLANGUAGE: Java\nCODE:\n```\nList<String> paths = Arrays.asList(\"/user/a1\", \"/user/a2\", \"/user/a3\");\nActorRef router1 = system.actorOf(new RedundancyGroup(paths, 2).props(), \"redundancy1\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Entity-based Message Processing with mapAsyncPartitioned in Scala\nDESCRIPTION: Example showing how to process messages for different entities concurrently while ensuring only one message per entity is processed at a time. The snippet demonstrates processing events with partitioning based on entity ID.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapAsyncPartitioned.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = messageSource\n  .mapAsyncPartitioned(10, 1)(\n    event => event.entityId\n  )(\n    (event, entityId) => {\n      println(s\"Processing event $event from partition $entityId\")\n      Future {\n        // simulate some processing delay\n        Thread.sleep(ThreadLocalRandom.current().nextInt(250))\n        println(s\"Completed processing $entityId-${event.sequence}\")\n        s\"$entityId-${event.sequence}\"\n      }\n    }\n  )\n  .map { result =>\n    println(s\"`mapAsyncPartitioned` emitted $result\")\n    result\n  }\n```\n\n----------------------------------------\n\nTITLE: Blocking Dispatcher Sample (Scala)\nDESCRIPTION: This snippet demonstrates how to set up an application with blocking and non-blocking actors in Scala to showcase the problem of thread starvation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BlockingDispatcherSample.scala](/akka-docs/src/test/scala/docs/actor/typed/BlockingDispatcherSample.scala) { #blocking-main }\n```\n\n----------------------------------------\n\nTITLE: Manually Starting and Managing Topic Actor - Akka Typed - Scala\nDESCRIPTION: Illustrates manual creation and management of a distributed topic actor in Scala using akka.actor.typed.pubsub.Topic.create. This bypasses the singleton registry, allowing multiple actors for a topic name on a single node. Input parameters are the topic name and message type. Each manual topic actor manages subscriptions and message flow independently.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nval manualTopic: Behavior[Message] = \n  Topic.create[Message](\"manual-topic\")\n```\n\n----------------------------------------\n\nTITLE: Using Broadcast to Aggregate Stream Values in Scala\nDESCRIPTION: This Scala snippet demonstrates the basic usage of `Broadcast(2)` to split a stream of integers. One output flow calculates the sum using `fold`, and the other calculates the count using `fold`. The results from both flows are then combined using `Zip` and printed to the console.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Broadcast.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(1 to 10)\n\nval graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n  import GraphDSL.Implicits._\n\n  val broadcast = builder.add(Broadcast[Int](2))\n  val merge = builder.add(Zip[BigInt, Int]())\n\n  val sumSink = Sink.fold[BigInt, Int](0)(_ + _)\n  val countSink = Sink.fold[Int, Int](0)((c, _) => c + 1)\n\nsource ~> broadcast\n\n  broadcast.out(0) ~> Flow[Int].map(BigInt(_)) ~> merge.in0\n  broadcast.out(1) ~> merge.in1\n\n  merge.out ~> Sink.foreach(println)\n\n  ClosedShape\n})\n\ngraph.run()\n```\n\n----------------------------------------\n\nTITLE: Fabricating Parent Actors for Isolated Child Testing - Java\nDESCRIPTION: This Java snippet shows how to fabricate a parent actor and connect it to a child for test isolation, allowing message interception without modifying production constructors. Libraries include Akka (Java) and testing frameworks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #test-fabricated-parent-creator }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #test-fabricated-parent }\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Suppressed DeadLetters in Scala\nDESCRIPTION: Scala code demonstrating how to subscribe to suppressed DeadLetters that are normally hidden from the default logging. This is useful for low-level debugging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_23\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.SuppressedDeadLetter\n// subscribe to all dead letters\nactorSystem.eventStream.subscribe(actor, classOf[SuppressedDeadLetter])\n```\n\n----------------------------------------\n\nTITLE: Using PNCounterMap in Akka Distributed Data (Java)\nDESCRIPTION: Demonstrates usage of PNCounterMap for managing multiple named counters in Java within Akka Distributed Data. This approach is useful when related counters must be replicated together. Dependencies are Akka Distributed Data for Java and a running actor cluster; input is an update specifying the map key and counter delta, output is the updated map.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n// Java Example: PNCounterMap usage\nPNCounterMapKey mapKey = PNCounterMapKey.create(\"mapCounterSet\");\ndistributedData.tell(new Update<>(mapKey, PNCounterMap.create(), WriteLocal.instance(), curr -> curr.increment(node, \"k1\", 1)), self());\n\n```\n\n----------------------------------------\n\nTITLE: Using Extensions from Inside an Actor in Scala\nDESCRIPTION: Demonstrates how to access the Counter extension from within an Akka actor. The extension is accessed through its ExtensionId using the actor's context.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nclass MyActor extends Actor {\n  val counter = Counter(context.system)\n  \n  def receive = {\n    case someMessage =>\n      val counterValue = counter.increment()\n      sender() ! counterValue\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Persistence in Akka Actors\nDESCRIPTION: Example showing how to use persistAsync for high-throughput scenarios where events can be persisted asynchronously while continuing to process new commands. Demonstrates event ordering guarantees.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_19\n\nLANGUAGE: Scala\nCODE:\n```\nclass MyPersistentActor extends PersistentActor {\n  override def persistenceId = \"my-persistent-actor-2\"\n\n  override def receiveRecover: Receive = {\n    case evt: String => // handle recovery here\n  }\n\n  override def receiveCommand: Receive = {\n    case c: String => persistAsync(s\"$c-1\") { e =>\n      sender() ! e\n      persistAsync(s\"$c-2\") { e =>\n        sender() ! e\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Transforming Arithmetic Exceptions with mapError in Akka Streams (Scala)\nDESCRIPTION: This snippet demonstrates how to use the mapError operator in Scala to transform an ArithmeticException into a UnsupportedOperationException when the element 0 goes through a map operator that divides by the element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapError.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.ActorSystem\nimport akka.stream.scaladsl._\n\nimplicit val system: ActorSystem = ActorSystem(\"mapError-system\")\n\nval source = Source(0 to 2)\n// #map-error\nval stream = source\n  .map(n => 1 / n) // will throw ArithmeticException for n = 0\n  .mapError {\n    case _: ArithmeticException => new UnsupportedOperationException(\"Operation not supported\")\n  }\n// #map-error\n```\n\n----------------------------------------\n\nTITLE: Drop Buffer Strategy in Akka Streams\nDESCRIPTION: Implements an aggressive buffer strategy that drops all queued jobs when the buffer becomes full. Used when dropping jobs is preferred over delays.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_17\n\nLANGUAGE: Scala\nCODE:\n```\nexternalService.runWith(\n  Flow[Job].buffer(1000, OverflowStrategy.dropBuffer)\n)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(externalService)\n  .buffer(1000, OverflowStrategy.dropBuffer())\n  .run(system)\n```\n\n----------------------------------------\n\nTITLE: Advanced Journal Query Types in Akka Persistence - Scala\nDESCRIPTION: Implementation of advanced journal query types using Materialized Values in Scala to provide additional information related to the query. This demonstrates defining and utilizing the character of materialized streams such as ordering and the finite nature of streams. Requires Akka Persistence library.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [PersistenceQueryDocSpec.scala](/akka-docs/src/test/scala/docs/persistence/query/PersistenceQueryDocSpec.scala) { #advanced-journal-query-types }\n```\n\n----------------------------------------\n\nTITLE: Starting Subscriber Actors in Scala\nDESCRIPTION: Example of how to start multiple subscriber actors across different nodes in an Akka cluster. All subscribers will receive messages published to the 'content' topic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n// start a subscriber on each node\nsystem.actorOf(Props[Subscriber], \"subscriber1\")\nsystem2.actorOf(Props[Subscriber], \"subscriber2\")\nsystem3.actorOf(Props[Subscriber], \"subscriber3\")\n```\n\n----------------------------------------\n\nTITLE: Using Method References with ReceiveBuilder in Java\nDESCRIPTION: Using method references instead of lambdas to improve code clarity and potentially better type inference in Akka Typed message handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nBehaviors.receive(Command.class)\n    .onMessage(GetValue.class, this::onGetValue)\n    .onMessage(Increment.class, this::onIncrement)\n    .build()\n```\n\n----------------------------------------\n\nTITLE: Using the Publish-Subscribe Channel in Java\nDESCRIPTION: This snippet demonstrates how to use the created publish-subscribe channel. It shows how to connect a producer and a consumer to the channel, and how to use the KillSwitch to deregister a user in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\nUniqueKillSwitch switchProducer1 =\n  Source.repeat(\"Hello\")\n    .viaMat(publishSubscribeFlow, Keep.right())\n    .to(Sink.ignore())\n    .run(mat);\n\nUniqueKillSwitch switchConsumer1 =\n  Source.<String>maybe()\n    .viaMat(publishSubscribeFlow, Keep.right())\n    .to(Sink.foreach(elem -> System.out.println(\"consumer1 processed: \" + elem)))\n    .run(mat);\n\n// ....\n\n// Deregister a single consumer or producer\nswitchConsumer1.shutdown();\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Element Source - Scala\nDESCRIPTION: Example showing how to create a Source that emits a single element once using Akka Streams in Scala. The source completes immediately after emitting the single value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/single.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.stream.scaladsl.Source\n\nSource.single(42)\n```\n\n----------------------------------------\n\nTITLE: Configuring Least Frequently Used (LFU) Policy in Akka Cluster Sharding\nDESCRIPTION: Configures a passivation strategy using the least frequently used policy. This policy passivates entities with the least frequent activity when the number of active entities exceeds the specified limit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_36\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = least-frequently-used\n}\n```\n\n----------------------------------------\n\nTITLE: Basic GroupBy Implementation in Scala\nDESCRIPTION: Demonstrates basic usage of groupBy operator to partition a stream based on word length, with merging of substreams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/groupBy.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval words = Source(List(\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"))\nval groups = words\n  .groupBy(3, word => word.length)\n  .reduce(_ + _)\n  .mergeSubstreams\n\ngroups.runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Defining Basic DurableStateBehavior Structure (Java)\nDESCRIPTION: Outlines the fundamental structure for defining a `DurableStateBehavior` in Java by extending the `DurableStateBehavior` class. It requires implementing methods for `emptyState` and `commandHandler`, and passing the `PersistenceId` to the super constructor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.typed.ActorRef;\nimport akka.actor.typed.Behavior;\nimport akka.persistence.typed.PersistenceId;\nimport akka.persistence.typed.javadsl.CommandHandler;\nimport akka.persistence.typed.javadsl.DurableStateBehavior;\nimport akka.persistence.typed.javadsl.Effect;\n\npublic class Counter extends DurableStateBehavior<Counter.Command, Counter.State> {\n\n  // Command\n  interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  // State\n  public static class State {\n    public final int value;\n\n    public State(int value) {\n      this.value = value;\n    }\n\n    State increment() {\n      return new State(value + 1);\n    }\n  }\n\n  private Counter(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return new Counter(persistenceId);\n  }\n\n  @Override\n  public State emptyState() {\n    return new State(0);\n  }\n\n  @Override\n  public CommandHandler<Command, State> commandHandler() {\n    return (state, command) -> {\n      if (command == Increment.INSTANCE) {\n        return Effect().persist(state.increment());\n      } else {\n        return Effect().none();\n      }\n    };\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Starting Cluster and Creating Actors in Scala Multi-Node Test\nDESCRIPTION: Demonstrates how to start up a cluster, let members join, and create actors in a multi-node test scenario.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_26\n\nLANGUAGE: scala\nCODE:\n```\nCluster(system).subscribe(testActor, classOf[MemberUp])\nexpectMsgClass(classOf[CurrentClusterState])\n\nval addressesF = Future.sequence(Seq(\n  system.actorSelection(node(first) / \"user\" / \"statsService\").resolveOne(),\n  system.actorSelection(node(second) / \"user\" / \"statsService\").resolveOne(),\n  system.actorSelection(RootActorPath(node(third).address) / \"user\" / \"statsService\").resolveOne()))\n  .map(_.map(_.path.address))\n\naddressesF.onComplete {\n  case Success(addresses) =>\n    println(s\"Addresses: ${addresses.mkString(\", \")}\")\n    addresses.foreach { address =>\n      Cluster(system).join(address)\n    }\n  case Failure(t) =>\n    println(s\"Failed to resolve actor references: ${t.getMessage}\")\n}\n\nreceiveN(3).collect { case MemberUp(m) => m.address }.toSet should be(\n  Set(node(first).address, node(second).address, node(third).address))\n\nCluster(system).unsubscribe(testActor)\n\nsystem.actorOf(Props[StatsWorker], name = \"statsWorker\")\nSystem.actorOf(Props[StatsService], name = \"statsService\")\n```\n\n----------------------------------------\n\nTITLE: Triggering Stream Elements Programmatically in Akka Streams\nDESCRIPTION: Java implementation of the programmatic trigger pattern that controls when elements flow through a stream. It zips a message stream with trigger signals and selects the message part of each pair when a trigger arrives.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_33\n\nLANGUAGE: Java\nCODE:\n```\n// This is the stream of triggers\nSource<Trigger, NotUsed> triggerSource = Source.fromPublisher(triggersForMessages);\n\n// This is the stream of messages\nSource<Message, NotUsed> messageSource = Source.fromPublisher(messages);\n\nSource<Message, NotUsed> controllableSource =\n    messageSource\n        // We pair each message with a trigger\n        .zip(triggerSource)\n        // When we get a trigger, we pass the message that came with it\n        .map((pair) -> pair.first());\n```\n\n----------------------------------------\n\nTITLE: Reusing Akka Stream Definitions in Java\nDESCRIPTION: Demonstrates that a RunnableGraph definition in Java can be materialized multiple times using `run()`. Each call produces a separate MaterializedMap containing distinct materialized values (e.g., different CompletionStages), illustrating that materialization creates a new execution instance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #stream-reuse }\n```\n\n----------------------------------------\n\nTITLE: Implementing foreachAsync Stream Processing in Scala\nDESCRIPTION: Demonstrates asynchronous processing of stream elements using Sink.foreachAsync in Scala. Shows how to process elements with a configurable parallelism level while maintaining backpressure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/foreachAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval sink = Sink.foreachAsync[String](parallelism = 2) { element =>\n  Future {\n    println(element)\n    Thread.sleep(1000) // do some heavy computation\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing EventsByTagSource in Java\nDESCRIPTION: A GraphStage implementation in Java that provides the eventsByTag functionality for a custom ReadJournal. This source queries events by tag from a persistence store with a scheduled refresh to support continuous streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\nfinal class MyEventsByTagSource extends GraphStage<SourceShape<EventEnvelope>> {\n  private final String tag;\n  private final Offset offset;\n  private final Outlet<EventEnvelope> out = Outlet.create(\"MyEventsByTagSource\");\n  private final SourceShape<EventEnvelope> shape = SourceShape.of(out);\n\n  public MyEventsByTagSource(String tag, Offset offset) {\n    this.tag = tag;\n    this.offset = offset;\n  }\n\n  @Override\n  public SourceShape<EventEnvelope> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      private Optional<Cancellable> completeAfterIdle = Optional.empty();\n\n      @Override\n      public void preStart() {\n        completeAfterIdle =\n            Optional.of(\n                scheduleWithFixedDelay(\n                    Duration.ofSeconds(1),\n                    Duration.ofSeconds(1),\n                    new Runnable() {\n                      @Override\n                      public void run() {\n                        if (isAvailable(out())) {\n                          for (EventEnvelope envelope : getStoreEvents()) {\n                            push(out(), envelope);\n                          }\n                        }\n                      }\n                    }));\n      }\n\n      private List<EventEnvelope> getStoreEvents() {\n        // retrieve events from the data store\n        // query events by tag, offset\n        return Collections.emptyList();\n      }\n\n      @Override\n      public void postStop() {\n        completeAfterIdle.ifPresent(Cancellable::cancel);\n      }\n\n      {\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() {\n                for (EventEnvelope envelope : getStoreEvents()) {\n                  push(out, envelope);\n                }\n              }\n            });\n      }\n    };\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Reply Handler Implementation in Scala\nDESCRIPTION: Implementation of reply handling logic in a persistent actor using Scala. Demonstrates how to send replies after handling commands.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_30\n\nLANGUAGE: Scala\nCODE:\n```\n#reply\n```\n\n----------------------------------------\n\nTITLE: Overriding Persistence Plugins in Scala Actor\nDESCRIPTION: Example of a persistent actor that overrides both the journal and snapshot store plugin IDs. This allows using different persistence plugins for different actors in the same system.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_43\n\nLANGUAGE: Scala\nCODE:\n```\nclass OverridePluginsActor(id: String) extends PersistentActor {\n  override def persistenceId: String = id\n\n  // Overrides to use a specific journal plugin\n  override def journalPluginId: String = \"akka.persistence.journal.leveldb\"\n\n  // Overrides to use a specific snapshot store plugin\n  override def snapshotPluginId: String = \"akka.persistence.snapshot-store.local\"\n\n  // ... rest of the actor implementation ...\n\n  override def receiveCommand: Receive = { case _ => }\n\n  override def receiveRecover: Receive = { case _ => }\n}\n```\n\n----------------------------------------\n\nTITLE: Moving Average Calculation Using Sliding\nDESCRIPTION: Implements a moving average calculation using sliding windows with size 5, showing how to transform windowed data into meaningful metrics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/sliding.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval numbers = Source(1 to 10)\nnumbers\n  .sliding(5)\n  .map(window => window.sum.toDouble / window.size)\n  .runWith(Sink.foreach(println))\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n    .sliding(5, 1)\n    .map(window -> window.stream()\n        .mapToInt(x -> (Integer) x)\n        .average()\n        .getAsDouble())\n    .runForeach(avg -> System.out.println(avg), system);\n```\n\n----------------------------------------\n\nTITLE: Implementing TwoPhaseSet Serializer in Scala\nDESCRIPTION: Scala implementation of a serializer for TwoPhaseSet that converts between the CRDT and its protobuf representation, ensuring deterministic serialization for proper SHA-1 digest generation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_29\n\nLANGUAGE: scala\nCODE:\n```\nclass TwoPhaseSetSerializer extends SerializerWithStringManifest with SerializationSupport {\n  private val TwoPhaseSetManifest = \"A\"\n\n  def manifest(o: AnyRef): String = o match {\n    case _: TwoPhaseSet => TwoPhaseSetManifest\n    case _ =>  throw new IllegalArgumentException(s\"Can't serialize object of type ${o.getClass}\")\n  }\n\n  def toBinary(o: AnyRef): Array[Byte] = o match {\n    case m: TwoPhaseSet => twoPhaseSetToProto(m).toByteArray\n    case _ => throw new IllegalArgumentException(s\"Can't serialize object of type ${o.getClass}\")\n  }\n\n  def fromBinary(bytes: Array[Byte], manifest: String): AnyRef = manifest match {\n    case TwoPhaseSetManifest => twoPhaseSetFromBinary(bytes)\n    case _ => throw new IllegalArgumentException(\n      s\"Unknown manifest [${manifest}], known manifests [${TwoPhaseSetManifest}]\")\n  }\n\n  def twoPhaseSetToProto(twoPhaseSet: TwoPhaseSet): TwoPhaseSetMessages.TwoPhaseSet = {\n    val b = TwoPhaseSetMessages.TwoPhaseSet.newBuilder()\n    import scala.collection.JavaConverters._\n    // sort elements to get same bytes for same content\n    val sortedAdds = twoPhaseSet.adds.elements.toVector.sorted\n    b.addAllAdds(sortedAdds.asJava)\n    val sortedRemoves = twoPhaseSet.removals.elements.toVector.sorted\n    b.addAllRemovals(sortedRemoves.asJava)\n    b.build()\n  }\n\n  def twoPhaseSetFromBinary(bytes: Array[Byte]): TwoPhaseSet = {\n    val msg = TwoPhaseSetMessages.TwoPhaseSet.parseFrom(bytes)\n    import scala.collection.JavaConverters._\n    val adds = GSet(msg.getAddsList.asScala.toSet)\n    val removals = GSet(msg.getRemovalsList.asScala.toSet)\n    TwoPhaseSet(adds, removals)\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: ByteString Chunker for Akka Streams (Scala)\nDESCRIPTION: Custom GraphStage that chunks up a stream of ByteStrings into ByteStrings of a specified maximum size.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_49\n\nLANGUAGE: Scala\nCODE:\n```\nclass ByteStringChunker(val chunkSize: Int) extends GraphStage[FlowShape[ByteString, ByteString]] {\n  requireArg(chunkSize > 0, \"chunkSize must be greater than 0\")\n  val in = Inlet[ByteString](\"ByteStringChunker.in\")\n  val out = Outlet[ByteString](\"ByteStringChunker.out\")\n  override val shape = FlowShape(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      private var buffer = ByteString.empty\n\n      setHandler(out, new OutHandler {\n        override def onPull(): Unit = emitChunk()\n      })\n      setHandler(in, new InHandler {\n        override def onPush(): Unit = {\n          buffer ++= grab(in)\n          emitChunk()\n        }\n\n        override def onUpstreamFinish(): Unit = {\n          if (buffer.isEmpty) completeStage()\n          else {\n            emitChunk()\n          }\n        }\n      })\n\n      private def emitChunk(): Unit = {\n        if (buffer.isEmpty) {\n          if (isClosed(in)) completeStage()\n          else pull(in)\n        } else {\n          val (chunk, nextBuffer) = buffer.splitAt(chunkSize)\n          buffer = nextBuffer\n          push(out, chunk)\n        }\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Akka Stream to InputStream in Java\nDESCRIPTION: Java implementation that creates a sink which reads content from a source, converts it to uppercase, and materializes into a Java InputStream. Shows how to use StreamConverters.asInputStream() in Java with transformation operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/asInputStream.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<String> source = Source.single(\"chunk\");\nInputStream inputStream =\n    source\n        .map(s -> ByteString.fromString(s.toUpperCase()))\n        .runWith(StreamConverters.asInputStream(), system);\n```\n\n----------------------------------------\n\nTITLE: Implementing Command Handlers in State Classes (Scala)\nDESCRIPTION: Illustrates how to define both event handlers and command handlers within state classes for a bank account entity using Akka Persistence in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-style.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AccountExampleWithCommandHandlersInState.scala](/akka-cluster-sharding-typed/src/test/scala/docs/akka/cluster/sharding/typed/AccountExampleWithCommandHandlersInState.scala) { #account-entity }\n```\n\n----------------------------------------\n\nTITLE: Global Rate Limiting Actor for Akka Streams (Java)\nDESCRIPTION: Java version of the actor-based global rate limiter for limiting the aggregate throughput of multiple independent streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_48\n\nLANGUAGE: Java\nCODE:\n```\npublic class GlobalRateLimit {\n    public static final Object WantToPass = new Object();\n    public static final Object MayPass = new Object();\n    public static final Object ReplenishTokens = new Object();\n\n    public static class Limiter extends AbstractActor {\n        private final float rate;\n        private final Duration per;\n        private final int maximumBurst;\n        private int tokens;\n        private Instant lastReplenish;\n        private final Queue<ActorRef> queue;\n\n        public Limiter(float rate, Duration per, int maximumBurst) {\n            this.rate = rate;\n            this.per = per;\n            this.maximumBurst = maximumBurst;\n            this.tokens = maximumBurst;\n            this.lastReplenish = Instant.now();\n            this.queue = new LinkedList<>();\n\n            getContext()\n                    .getSystem()\n                    .scheduler()\n                    .scheduleAtFixedRate(\n                            per,\n                            per,\n                            getSelf(),\n                            ReplenishTokens,\n                            getContext().getDispatcher(),\n                            getSelf());\n        }\n\n        @Override\n        public Receive createReceive() {\n            return receiveBuilder().match(Object.class, this::onMessage).build();\n        }\n\n        private void onMessage(Object message) {\n            if (message == WantToPass) {\n                if (tokens > 0) {\n                    tokens--;\n                    getSender().tell(MayPass, getSelf());\n                } else {\n                    queue.offer(getSender());\n                }\n            } else if (message == ReplenishTokens) {\n                replenish();\n                while (tokens > 0 && !queue.isEmpty()) {\n                    queue.poll().tell(MayPass, getSelf());\n                    tokens--;\n                }\n            }\n        }\n\n        private void replenish() {\n            Instant now = Instant.now();\n            long sinceLastReplenish = Duration.between(lastReplenish, now).toMillis();\n            int toAdd = (int) ((sinceLastReplenish / (float) per.toMillis()) * rate);\n            tokens = Math.min(tokens + toAdd, maximumBurst);\n            lastReplenish = now;\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Applying a Sink to Substreams after groupBy in Scala\nDESCRIPTION: Illustrates how to apply a processing stage (a `Sink.fold` in this case, summing elements) independently to each substream created by the `groupBy` operator. The result of the fold for each substream is then ignored.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n//#groupBy2\nsource\n  .groupBy(maxSubstreams = 2, _ % 2 == 0)\n  .to(Sink.fold(0)((acc, i) => acc + i))\n  // From Sink.fold(0)(_ + _), even substream will emit 30 and odd 25\n  // It means this will be applied independently for each substream\n  .run()\n//#groupBy2\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Round Robin Group Router in Akka\nDESCRIPTION: This configuration snippet shows how to set up a group of remote actors using a round-robin router in Akka. It specifies the paths of the remote actors to which messages will be sent.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_11\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.deployment {\n  /parent/remoteGroup {\n    router = round-robin-group\n    routees.paths = [\n      \"akka://app@10.0.0.2:25520/user/workers/w1\",\n      \"akka://app@10.0.0.2:25520/user/workers/w2\",\n      \"akka://app@10.0.0.3:25520/user/workers/w3\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Passivation Strategy (HOCON)\nDESCRIPTION: This HOCON snippet demonstrates defining a custom passivation strategy named `my-strategy`. It selects this custom strategy using `passivation.strategy = my-strategy`. The custom strategy uses the Least Recently Used (`lru`) replacement policy, sets an active entity limit of 50,000, and configures an idle timeout of 5 minutes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_31\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.sharding {\n  passivation {\n    # select the custom strategy\n    strategy = my-strategy\n\n    # define the custom strategy\n    my-strategy {\n      # active entity limit for the shard region\n      limit = 50000\n      # replacement policy for the active entity limit\n      replacement {\n        policy = lru\n      }\n      # also passivate entities that have been idle\n      idle-entity {\n        timeout = 5 minutes\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Source from a Function (Akka Streams Java)\nDESCRIPTION: Presents the Java equivalent for a source evaluated on-demand through a provided function, leveraging Source.repeat then mapping to the function output. Intended for functions with safe side-effects or pure computation. Prefer more advanced techniques for mutable state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeSourceFromFunction.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeSourceFromFunction.java) { #source-from-function }\n```\n\n----------------------------------------\n\nTITLE: Offering to Receive Data: SinkRef Example in Scala\nDESCRIPTION: Demonstrates how to set up a `SinkRef` in Scala to receive data streams from a remote system, using flow-control mechanisms.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-refs.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\nsnip [FlowStreamRefsDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowStreamRefsDocSpec.scala) { #offer-sink }\n```\n\nLANGUAGE: Scala\nCODE:\n```\nsnip [FlowStreamRefsDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowStreamRefsDocSpec.scala) { #offer-sink-use }\n```\n\n----------------------------------------\n\nTITLE: UDP Bind and Listen Implementation\nDESCRIPTION: Shows how to implement a UDP server that listens for incoming datagrams on a socket. Uses the Bind message to create a socket actor that can send and receive datagrams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-udp.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nclass Listener extends Actor {\n  IO(Udp) ! Bind(self, new InetSocketAddress(\"localhost\", 0))\n  def receive = {\n    case Bound(local) =>\n      context.become(ready(sender()))\n  }\n  def ready(socket: ActorRef): Receive = {\n    case Received(data, remote) =>\n      socket ! Send(data, remote)\n    case Unbind  =>\n      socket ! Unbind\n    case Unbound =>\n      context.stop(self)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Collecting Missed Ticks in Akka Streams (Java)\nDESCRIPTION: Java version of using conflateWithSeed to count missed ticks when the downstream is slower than the upstream tick source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_44\n\nLANGUAGE: Java\nCODE:\n```\nfinal Source<String, Cancellable> ticks = Source.tick(Duration.ZERO, Duration.ofSeconds(1), \"\");\n\nfinal Flow<String, Integer, NotUsed> collectTicks =\n        Flow.<String>create()\n                .conflateWithSeed(() -> 0, (count, tick) -> count + 1);\n\nfinal RunnableGraph<NotUsed> flow =\n        ticks.via(collectTicks)\n                .take(10)\n                .to(Sink.foreach(System.out::println));\n\nflow.run(mat);\n```\n\n----------------------------------------\n\nTITLE: Command and Event Definition in Akka Persistence - Java\nDESCRIPTION: This snippet shows how commands and events are defined for a persistent actor using Akka's typed persistence API in Java. It's foundational for setting up persistent behaviors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BasicPersistentBehaviorTest.java](/akka-persistence-typed/src/test/java/jdocs/akka/persistence/typed/BasicPersistentBehaviorTest.java) { #command }\n```\n\n----------------------------------------\n\nTITLE: Defining an Echo Manager Actor in Scala\nDESCRIPTION: This Scala snippet defines an `EchoManager` actor responsible for handling incoming TCP connections. When a connection is received (`Tcp.Connected`), it registers a new `SimpleEchoHandler` actor to manage that specific connection, enabling the `keepOpenOnPeerClosed` option to allow sending remaining data after the client closes its write end.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nclass EchoManager(listener: ActorRef) extends Actor with ActorLogging {\n\n  // sign death pact: this actor stops when the server stops\n  context.watch(listener)\n\n  def receive = {\n    case Tcp.Connected(remote, local) =>\n      log.info(\"Remote address {} connected\", remote)\n      val handler = context.actorOf(Props(classOf[SimpleEchoHandler], sender(), remote))\n      sender() ! Tcp.Register(handler, keepOpenOnPeerClosed = true)\n    case Stop =>\n      log.info(\"Stopping echo manager\")\n      listener ! Tcp.Unbind\n      context.become(stopping)\n  }\n\n  def stopping: Receive = {\n    case Tcp.Unbound =>\n      log.info(\"Echo manager unbound\")\n      context.stop(self)\n    case _: Tcp.CommandFailed =>\n      //listener already unbound\n      context.stop(self)\n\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Disabling Recovery Completely in Akka Persistence (Java)\nDESCRIPTION: Disables the entire recovery process for an AbstractPersistentActor. This is done by overriding the `recovery` method and returning `Recovery.none()`. Consequently, the actor will start with an empty state, disregarding any journaled events or snapshots.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic Recovery recovery() {\n  return Recovery.none();\n}\n```\n\n----------------------------------------\n\nTITLE: Write-Behind Configuration for Performance\nDESCRIPTION: Configuration for enabling write-behind mode to improve performance with durable storage by batching writes, with a trade-off of potential data loss in case of JVM crashes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_42\n\nLANGUAGE: text\nCODE:\n```\nakka.cluster.distributed-data.durable.lmdb.write-behind-interval = 200 ms\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Cluster Typed Dependency for sbt/Maven/Gradle\nDESCRIPTION: Shows how to add the `akka-cluster-typed` dependency to a project using sbt, Maven, or Gradle. This dependency is required to use Akka Distributed Data with typed actors. It utilizes the Akka BOM (Bill of Materials) for consistent version management across Akka modules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-cluster-typed_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Lazy Sink in Java\nDESCRIPTION: Shows the Java implementation of Sink.lazySink where sink creation is deferred until the first element arrives. The example uses side effects to demonstrate the execution order.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/lazySink.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nsource.map(element -> {\n  System.out.println(\"map - \" + element);\n  return element;\n}).to(Sink.lazySink(() -> {\n  System.out.println(\"Creating sink\");\n  return Sink.foreach(elem -> System.out.println(\"foreach - \" + elem));\n})).run(mat);\n```\n\n----------------------------------------\n\nTITLE: Configuring SmallestMailboxPool Router\nDESCRIPTION: Configuration example for SmallestMailboxPool router that routes messages to actors with the smallest mailbox size.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_27\n\nLANGUAGE: scala\nCODE:\n```\nakka.actor.deployment {\n  /parent/router1 {\n    router = smallest-mailbox-pool\n    nr-of-instances = 5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Parent-Child Actor Relationships - Scala\nDESCRIPTION: This code illustrates the general test structure for parent-child actors in Akka using Scala. It covers various patterns for wiring child actors, allowing better testability and flexibility in actor creation. Dependencies include Akka Actors and ScalaTest or equivalent test frameworks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ParentChildSpec.scala](/akka-docs/src/test/scala/docs/testkit/ParentChildSpec.scala) { #test-example }\n```\n\n----------------------------------------\n\nTITLE: Implementing Sharded Response Pattern in Java\nDESCRIPTION: This code snippet shows how to implement a sharded response pattern in Java using Akka Cluster Sharding. It defines the message protocol, sharded actor behavior, and demonstrates how to send and receive messages through sharding.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\npublic class ShardedResponseExample {\n\n  public interface Message {}\n\n  public static class Request implements Message {\n    public final String entityId;\n    public final String msg;\n    public final ActorRef<Response> replyTo;\n\n    public Request(String entityId, String msg, ActorRef<Response> replyTo) {\n      this.entityId = entityId;\n      this.msg = msg;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Response implements Message {\n    public final String msg;\n\n    public Response(String msg) {\n      this.msg = msg;\n    }\n\n    @Override\n    public String toString() {\n      return \"Response{\" + \"msg='\" + msg + '\\'' + '}';\n    }\n  }\n\n  public static final EntityTypeKey<Message> TypeKey =\n      EntityTypeKey.create(Message.class, \"ResponseExample\");\n\n  public static Behavior<Message> create() {\n    return Behaviors.setup(\n        context -> {\n          return Behaviors.receiveMessage(\n              message -> {\n                if (message instanceof Request) {\n                  Request request = (Request) message;\n                  // Remember who to reply to\n                  context.getLog().info(\"Got request for: {}\", request.entityId);\n                  request.replyTo.tell(new Response(request.msg.toUpperCase()));\n                  return Behaviors.same();\n                } else {\n                  return Behaviors.unhandled();\n                }\n              });\n        });\n  }\n\n  // Request from some other actor\n  public static class OtherActor extends AbstractBehavior<Response> {\n\n    public static Behavior<Response> create(ActorRef<ShardingEnvelope<Message>> region) {\n      return Behaviors.setup(context -> new OtherActor(context, region));\n    }\n\n    private final ActorRef<ShardingEnvelope<Message>> region;\n\n    private OtherActor(\n        ActorContext<Response> context, ActorRef<ShardingEnvelope<Message>> region) {\n      super(context);\n      this.region = region;\n      region.tell(\n          new ShardingEnvelope<>(\"cat\", new Request(\"cat\", \"meow\", context.getSelf())));\n    }\n\n    @Override\n    public Receive<Response> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(\n              Response.class,\n              response -> {\n                getContext().getLog().info(\"Got response: {}\", response);\n                return this;\n              })\n          .build();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining State for Blog Post Entity (Scala)\nDESCRIPTION: Defines the state for a blog post entity using sealed traits and case classes in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\nsealed trait State\n\ncase object EmptyState extends State\n\nfinal case class DraftState(content: PostContent) extends State\n\nfinal case class PublishedState(content: PostContent) extends State\n\nfinal case class PostContent(title: String, body: String)\n```\n\n----------------------------------------\n\nTITLE: Scala Sink Publisher Code Reference\nDESCRIPTION: Scala code reference showing how to obtain a Publisher using Sink.asPublisher with fanout parameter\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/stream/stream-design.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSink.asPublisher[T](fanout:Boolean):akka.stream.scaladsl.Sink[T,org.reactivestreams.Publisher[T]]\n```\n\n----------------------------------------\n\nTITLE: Scala Reactive Database Integration Example\nDESCRIPTION: Example showing how to use Source.asSubscriber to integrate with a reactive database client, demonstrating backpressure-aware streaming of database rows.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/asSubscriber.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.stream.javadsl.JavaFlowSupport\nimport java.util.concurrent.Flow\n\nval db: ReactiveDB = ???\n\nval rowSource = JavaFlowSupport.Source.asSubscriber[Row].map { row =>\n  row.getString(\"name\")\n}\n\n// Each materialization will query the database\nval subscriber: Flow.Subscriber[Row] = rowSource.run()\ndb.query(\"SELECT * FROM TABLES\", subscriber)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Sink GraphStage (Java)\nDESCRIPTION: This Java snippet shows the pattern for implementing a custom Sink in Akka Streams, using GraphStage's AbstractInHandler for handling incoming data. It is structured to handle element receipt and use the preStart callback to request upstream data, ensuring proper backpressured flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [GraphStageDocTest.java](/akka-docs/src/test/java/jdocs/stream/GraphStageDocTest.java) { #simple-sink }\n```\n\n----------------------------------------\n\nTITLE: Using mapAsyncUnordered for Unordered Processing (Scala)\nDESCRIPTION: Demonstrates using `mapAsyncUnordered` instead of `mapAsync` when the order of elements downstream does not need to match the upstream order. This can improve performance as results are emitted as soon as they become available.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\nval otherTweets: Source[Tweet, NotUsed] = tweets // imagine ordering is not important\n\nval authors: Source[Author, NotUsed] =\n  otherTweets.filter(_.hashtags.contains(akkaTag)).map(_.author)\n\nval emailAddresses: Source[String, NotUsed] =\n  authors.mapAsyncUnordered(4)(author => lookupEmail(author.handle)).collect { case Some(emailAddress) => emailAddress }\n```\n\n----------------------------------------\n\nTITLE: Using zipLatest Operator in Java\nDESCRIPTION: Example demonstrating how to use the zipLatest operator to combine elements from multiple sources in Java. Shows implementation of Source.zipLatest and Flow.zipLatest functionality.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipLatest.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#zipLatest-example\n```\n\n----------------------------------------\n\nTITLE: Interleaving Stream Elements in Scala\nDESCRIPTION: Demonstrates how to use the interleave operator in Scala to combine elements from two sources with a specified segment size. The operator emits a defined number of elements from the original source before switching to the secondary source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/interleave.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval first = Source(List(1, 2, 3, 4))\\nval second = Source(List(10, 11, 12, 13))\\nval resultSeq = first.interleave(second, 2, eagerClose = false).runWith(Sink.seq)\n```\n\n----------------------------------------\n\nTITLE: Implementing a DataBot with ORSet in Java\nDESCRIPTION: A Java implementation of an actor that periodically adds or removes elements from an ORSet and subscribes to changes of the data set.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class DataBot extends AbstractActor {\n\n  public static Props props() {\n    return Props.create(DataBot.class);\n  }\n\n  private static final class Tick {\n    static final Tick INSTANCE = new Tick();\n\n    private Tick() {}\n  }\n\n  private final ActorRef replicator =\n      DistributedData.get(getContext().getSystem()).replicator();\n\n  private final SelfUniqueAddress node =\n      DistributedData.get(getContext().getSystem()).selfUniqueAddress();\n\n  private final Key<ORSet<String>> dataKey = ORSetKey.create(\"key\");\n\n  private final Cancellable tickTask =\n      getContext()\n          .getSystem()\n          .scheduler()\n          .scheduleWithFixedDelay(\n              Duration.ofSeconds(5),\n              Duration.ofSeconds(5),\n              getSelf(),\n              Tick.INSTANCE,\n              getContext().getDispatcher(),\n              getSelf());\n\n  @Override\n  public void preStart() {\n    replicator.tell(new Subscribe<>(dataKey, getSelf()), ActorRef.noSender());\n  }\n\n  @Override\n  public void postStop() {\n    tickTask.cancel();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Tick.class,\n            t -> {\n              String s =\n                  String.valueOf((char) ThreadLocalRandom.current().nextInt(97, 123));\n              if (ThreadLocalRandom.current().nextBoolean()) {\n                // add\n                log().info(\"Adding: {}\", s);\n                replicator.tell(\n                    new Update<>(\n                        dataKey, ORSet.create(), WriteLocal.instance(), curr -> curr.add(node, s)),\n                    getSelf());\n              } else {\n                // remove\n                log().info(\"Removing: {}\", s);\n                replicator.tell(\n                    new Update<>(\n                        dataKey, ORSet.create(), WriteLocal.instance(), curr -> curr.remove(node, s)),\n                    getSelf());\n              }\n            })\n        .match(\n            Changed.class,\n            c -> {\n              if (c.key().equals(dataKey)) {\n                @SuppressWarnings(\"unchecked\")\n                ORSet<String> data = (ORSet<String>) c.dataValue();\n                log().info(\"Current elements: {}\", data.getElements());\n              }\n            })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Distributed Data Mode for Remembering Entities in Akka Cluster Sharding\nDESCRIPTION: Configuration snippet for enabling distributed data mode for the remember entities store in Akka Cluster Sharding. This is the default mode and persists entity state to disk using distributed data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_44\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.remember-entities-store = ddata\n```\n\n----------------------------------------\n\nTITLE: Creating DurableStateBehavior Instance (Scala)\nDESCRIPTION: Demonstrates the complete creation of a `DurableStateBehavior` instance in Scala for the counter actor. It combines the `PersistenceId`, the `emptyState`, and the previously defined `commandHandler` logic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.typed.Behavior\nimport akka.persistence.typed.PersistenceId\nimport akka.persistence.typed.scaladsl.DurableStateBehavior\nimport akka.persistence.typed.scaladsl.Effect\n\nobject CounterBehavior {\n\n  // Command, State, commandHandler defined above\n\n  def apply(persistenceId: PersistenceId): Behavior[Command] =\n    DurableStateBehavior[Command, State](\n      persistenceId = persistenceId,\n      emptyState = State(0),\n      commandHandler = commandHandler)\n}\n\n```\n\n----------------------------------------\n\nTITLE: Using Sink.foreach in Java\nDESCRIPTION: Example demonstrating how to use Sink.foreach to print each element from a stream to standard output in Java. The sink materializes into a CompletionStage<Done> which completes when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/foreach.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3)).map(String::valueOf).runWith(Sink.foreach(System.out::println), mat);\n```\n\n----------------------------------------\n\nTITLE: Triggering Stream Elements Programmatically in Akka Streams\nDESCRIPTION: This pattern allows controlling the emission of elements according to a trigger signal, even when the stream is not backpressured. It zips the stream of data elements with trigger signals to control flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_32\n\nLANGUAGE: Scala\nCODE:\n```\n// This is the stream of triggers\nval triggerSource: Source[Trigger, NotUsed] = Source.fromPublisher(triggersForMessages)\n\n// This is the stream of messages\nval messageSource: Source[Message, NotUsed] = Source.fromPublisher(messages)\n\nval controllableSource: Source[Message, NotUsed] =\n  messageSource\n    // We pair each message with a trigger\n    .zip(triggerSource)\n    // When we get a trigger, we pass the message that came with it\n    .map(value => value._1)\n```\n\n----------------------------------------\n\nTITLE: Implementing mergePreferred in Java\nDESCRIPTION: Example of using the mergePreferred operator in Java to merge streams with preference for one source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergePreferred.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// prefer the right source (source2) if both sources has elements ready\nSource<Integer> source1 = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<Integer> source2 = Source.from(Arrays.asList(10, 20, 30, 40));\n\nSource<Integer> mergedWithPreferenceForSource2 = source1.mergePreferred(source2, true, false);\n\n// prefer the left source (source1) if both sources has elements ready\nSource<Integer> mergedWithPreferenceForSource1 = source1.mergePreferred(source2, false, false);\n```\n\n----------------------------------------\n\nTITLE: Implementing zipWith in Java\nDESCRIPTION: Example demonstrating the usage of zipWith operator in Java for combining elements from multiple sources.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipWith.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSourceOrFlow.java\n```\n\n----------------------------------------\n\nTITLE: Updated ItemAdded Event Class with Renamed Field in Java\nDESCRIPTION: Java code snippet showing the final version of the ItemAdded event class after the field renaming.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ItemAdded.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v2c/ItemAdded.java) { #rename }\n```\n\n----------------------------------------\n\nTITLE: Configuring Segmented Least Recently Used (SLRU) Policy in Akka Cluster Sharding\nDESCRIPTION: Configures a two-level segmented least recently used policy with a protected segment limited to 80% of the total limit. This policy divides entities into segments based on access frequency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_33\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = least-recently-used\n  least-recently-used {\n    segmented = true\n    segments = 2\n    segment-proportion = [0.2, 0.8]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Stash Overflow Strategy in Akka Persistence (HOCON)\nDESCRIPTION: Defines the default strategy for handling internal stash overflows in persistent actors using the `akka.persistence.internal-stash-overflow-strategy` configuration key. It requires the fully qualified class name of a `StashOverflowStrategyConfigurator`, such as `akka.persistence.ThrowExceptionConfigurator` (default) or `akka.persistence.DiscardConfigurator`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_16\n\nLANGUAGE: hocon\nCODE:\n```\nakka.persistence.internal-stash-overflow-strategy=\n  \"akka.persistence.ThrowExceptionConfigurator\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Map Transformation with GraphStage in Java\nDESCRIPTION: This snippet shows the Java implementation of a custom GraphStage that performs the map transformation in Akka Streams. It demonstrates how to handle push and pull events in the GraphStageLogic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\npublic class Map<A, B> extends GraphStage<FlowShape<A, B>> {\n  private final Inlet<A> in = Inlet.create(\"Map.in\");\n  private final Outlet<B> out = Outlet.create(\"Map.out\");\n  private final Function<A, B> f;\n\n  public Map(Function<A, B> f) {\n    this.f = f;\n  }\n\n  private final FlowShape<A, B> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, B> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                push(out, f.apply(grab(in)));\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() {\n                pull(in);\n              }\n            });\n      }\n    };\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Distributed Data Changes in Java\nDESCRIPTION: Java example of subscribing to changes in distributed data using Akka's Replicator. It shows how to register a subscriber and handle change notifications.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\nActorRef subscriber = system.actorOf(Props.create(Subscriber.class));\\n\\nreplicator.tell(new Subscribe<>(dataKey, subscriber), ActorRef.noSender());\n```\n\n----------------------------------------\n\nTITLE: Wiring Components and Customizing Materialized Value Combination in Scala\nDESCRIPTION: This Scala snippet demonstrates creating a complete `RunnableGraph` by connecting `nestedSource` and `nestedSink`. It uses a custom combiner function within `toMat`. This function takes the materialized values from the source (`Promise[[Option[Int]]]`) and the sink (`Pair[Future[OutgoingConnection], Future[String]]`) and combines them into a custom `MyClass` instance, effectively selecting and transforming the relevant materialized values for the final graph.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\n//#mat-combine-4\nfinal case class MyClass(p: Promise[Option[Int]], conn: Future[OutgoingConnection])\n\n// Custom combiner function\ndef customCombiner(p: Promise[Option[Int]],\n                   pair: (Future[OutgoingConnection], Future[String])): MyClass = {\n  // Ignore the Future[String] value\n  MyClass(p, pair._1)\n}\n\nval runnableGraph: RunnableGraph[MyClass] =\nnestedSource.toMat(nestedSink)(customCombiner)\n//#mat-combine-4\n```\n\n----------------------------------------\n\nTITLE: ActorRef Serialization Implementation - Java\nDESCRIPTION: Implementation of ActorRef serialization and deserialization logic in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-classic.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n#actorref-serializer\n```\n\n----------------------------------------\n\nTITLE: Full Service Lookup with Port and Protocol in Java\nDESCRIPTION: Advanced example of service lookup in Java that includes service name, port name, and protocol. This is used for more specific service discovery queries, particularly useful for SRV DNS record lookups.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nCompletionStage<Resolved> lookup =\n  discovery.lookup(\n      Lookup.create(\"service-name\").withPortName(\"http\").withProtocol(\"tcp\"),\n      resolveTimeout);\n```\n\n----------------------------------------\n\nTITLE: Configuring Retention Criteria with Event Deletion in Java\nDESCRIPTION: Shows how to configure retention criteria with event deletion enabled in Java. This setup enables automatic deletion of events after successful snapshot creation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-snapshot.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nRetentionCriteria.snapshotEvery(100, 2).withDeleteEventsOnSnapshot()\n```\n\n----------------------------------------\n\nTITLE: External Address Configuration - Scala\nDESCRIPTION: Configuration for external address handling in Scala for actor serialization\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-classic.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n#external-address-default\n```\n\n----------------------------------------\n\nTITLE: Defining State for a Counter Actor (Scala)\nDESCRIPTION: Defines the immutable state representation for the counter actor in Scala using a case class `State`. It holds the current integer `value` of the counter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nfinal case class State(value: Int)\n```\n\n----------------------------------------\n\nTITLE: Merging Price and Quantity Streams with MergeLatest in Scala\nDESCRIPTION: Example showing how to use mergeLatest to combine a stream of prices with a stream of quantities, producing updates whenever either value changes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergeLatest.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#mergeLatest\n```\n\n----------------------------------------\n\nTITLE: Handling Invalid Requests in Akka Typed ask Pattern (Scala)\nDESCRIPTION: This example shows how to handle invalid requests when using the 'ask' pattern in Akka Typed. It demonstrates mapping an InvalidRequest reply to a failed Future, allowing for explicit error handling on the requestor side.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [InteractionPatternsSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/InteractionPatternsSpec.scala) { #standalone-ask-fail-future }\n```\n\n----------------------------------------\n\nTITLE: Adding Elements to GSet using Akka Distributed Data (Scala)\nDESCRIPTION: Exemplifies usage of the GSet (grow-only set) CRDT in Scala. The snippet focuses on how to add elements to the set, guaranteeing that merges are handled as unions. Requires Akka Distributed Data and input operations to add serialized elements. Outputs are the current set contents distributed across the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n/* Scala Example: Add to GSet */\nval setKey = GSetKey[String](\"exampleSet\")\ndistributedData ! Update(setKey, GSet.empty[String], WriteLocal)(_ + \"foo\")\n\n```\n\n----------------------------------------\n\nTITLE: Lease Usage Implementation in Scala\nDESCRIPTION: Demonstrates how to acquire and use a lease in Scala, including error handling and lease checking.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordination.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n#lease-usage\n```\n\n----------------------------------------\n\nTITLE: Using flatMapConcat to Process Customer Events Sequentially in Java\nDESCRIPTION: This Java example demonstrates using flatMapConcat to process customer events one customer at a time. For each customerId, it creates a Source by calling lookupCustomerEvents, which could represent a database query or calculation. All events for a customer are processed completely before moving to the next customer.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/flatMapConcat.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource<Integer, NotUsed> customerIds = Source.range(1, 3);\n\nSource<CustomerEvent, NotUsed> customerEvents =\n    customerIds.flatMapConcat(customerId -> lookupCustomerEvents(customerId));\n\n// Where lookupCustomerEvents could be implemented as:\nprivate Source<CustomerEvent, NotUsed> lookupCustomerEvents(int customerId) {\n  return Source.from(\n      Arrays.asList(\n          new CustomerEvent(customerId, \"signed-up\"),\n          new CustomerEvent(customerId, \"activated\"),\n          new CustomerEvent(customerId, \"upgraded\")));\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Snapshot Offers in Scala\nDESCRIPTION: Shows how to handle snapshot restoration during actor recovery\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_28\n\nLANGUAGE: scala\nCODE:\n```\noverride def receiveRecover: Receive = {\n  case SnapshotOffer(metadata, offeredSnapshot: ExampleState) =>\n    state = offeredSnapshot\n  case event: Event => // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Pull-Based Reading Echo Server in Java\nDESCRIPTION: A Java implementation of an Echo server using pull-based reading mode. This approach eliminates the need for a buffer by resuming reading only after the previous data has been written successfully.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\npublic class PullEcho extends AbstractActor {\n\n  private final ActorSystem system = getContext().getSystem();\n  private final ActorRef listener = getContext().actorOf(Props.create(Listener.class, getSelf()));\n\n  @Override\n  public void preStart() {\n    final ActorRef manager = Tcp.get(system).manager();\n    manager.tell(\n        TcpMessage.bind(\n            listener, new InetSocketAddress(\"localhost\", 0), 100, Option.apply(Inet.SO.TCP_NODELAY()), true),\n        getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Tcp.Bound.class,\n            bound -> {\n              final InetSocketAddress localAddress = bound.localAddress();\n              System.out.println(\"Listening on port: \" + localAddress.getPort());\n            })\n        .build();\n  }\n\n  static class Listener extends AbstractActor {\n    private final ActorRef boss;\n\n    public Listener(ActorRef boss) {\n      this.boss = boss;\n    }\n\n    @Override\n    public Receive createReceive() {\n      return receiveBuilder()\n          .match(\n              Tcp.Bound.class,\n              msg -> {\n                boss.tell(msg, getSelf());\n                getContext()\n                    .become(\n                        receiveBuilder()\n                            .match(\n                                Tcp.Connected.class,\n                                conn -> {\n                                  final ActorRef handler =\n                                      getContext().actorOf(Props.create(PullEchoHandler.class));\n                                  getSender()\n                                      .tell(\n                                          TcpMessage.register(\n                                              handler, true, true),\n                                          getSelf());\n                                  getSender().tell(TcpMessage.resumeAccepting(1), getSelf());\n                                })\n                            .build());\n              })\n          .build();\n    }\n  }\n\n  static class PullEchoHandler extends AbstractActor {\n\n    private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n    @Override\n    public void preStart() {\n      getSender().tell(TcpMessage.resumeReading(), getSelf());\n    }\n\n    @Override\n    public Receive createReceive() {\n      return receiveBuilder()\n          .match(\n              Tcp.Received.class,\n              msg -> {\n                final ByteString data = msg.data();\n                final ActorRef tcp = getSender();\n                tcp.tell(TcpMessage.write(data, TcpMessage.ack()), getSelf());\n\n                getContext()\n                    .become(\n                        receiveBuilder()\n                            .match(\n                                Tcp.WriteCommand.Ack.class,\n                                ack -> {\n                                  tcp.tell(TcpMessage.resumeReading(), getSelf());\n                                  getContext().unbecome();\n                                })\n                            .build());\n              })\n          .build();\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Async Database API in Scala\nDESCRIPTION: Example showing an asynchronous database API interface with methods for querying, checking more results, and closing connections.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldResourceAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ntrait AsyncDatabaseApi {\n  // Initial async query\n  def runQuery(query: String): Future[QueryResult]\n  // Check if has more results\n  def hasMore(result: QueryResult): Future[Option[Data]]\n  // Close the query/connection\n  def close(result: QueryResult): Future[Done]\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring prefixAndTail Operator for Akka Flow in Scala\nDESCRIPTION: Defines the prefixAndTail operator for Akka Flow in Scala. It takes a number n and returns a tuple containing a sequence of up to n elements and a Source for the remaining elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/prefixAndTail.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nFlow.prefixAndTail[U>:Out](n:Int):FlowOps.this.Repr[(scala.collection.immutable.Seq[Out],akka.stream.scaladsl.Source[U,akka.NotUsed])]\n```\n\n----------------------------------------\n\nTITLE: Injecting Keep-Alive Messages into ByteString Stream in Java\nDESCRIPTION: Uses a built-in operation to inject keep-alive messages into a stream of ByteStrings, but only if it doesn't interfere with normal traffic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_56\n\nLANGUAGE: Java\nCODE:\n```\nimport java.time.Duration;\n\nByteString keepAliveMessage = ByteString.fromString(\"\\u0000\");\n\nFlow<ByteString, ByteString, NotUsed> injectKeepAlive =\n    Flow.of(ByteString.class)\n        .keepAlive(Duration.ofSeconds(1), () -> keepAliveMessage);\n```\n\n----------------------------------------\n\nTITLE: Testing Cluster Stats Service in Scala Multi-Node Test\nDESCRIPTION: Demonstrates how to test a cluster stats service by running specific code on certain roles and verifying expected behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_28\n\nLANGUAGE: scala\nCODE:\n```\nrunOn(first) {\n  val service = system.actorSelection(node(first) / \"user\" / \"statsService\")\n  service ! StatsJob(\"this is the text to analyze\")\n  expectMsgType[StatsResult]\n}\n\nrunOn(second) {\n  val service = system.actorSelection(node(second) / \"user\" / \"statsService\")\n  service ! StatsJob(\"this is the text to analyze\")\n  expectMsgType[StatsResult]\n}\n\nrunOn(third) {\n  val service = system.actorSelection(node(third) / \"user\" / \"statsService\")\n  service ! StatsJob(\"this is the text to analyze\")\n  expectMsgType[StatsResult]\n}\n```\n\n----------------------------------------\n\nTITLE: Anonymous Actor Spawning - Java\nDESCRIPTION: Java code demonstrating how to spawn actors anonymously during tests using ActorTestKit, allowing for dynamic and flexible actor management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AsyncTestingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/AsyncTestingExampleTest.java) { #test-spawn-anonymous }\n```\n\n----------------------------------------\n\nTITLE: Configuring Admission Window Optimizer in Akka Cluster Sharding\nDESCRIPTION: Configures a hill-climbing optimizer for dynamically adapting the admission window proportion. This allows the passivation strategy to adjust between recency-biased and frequency-biased workloads.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_40\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = least-recently-used\n  admission-window {\n    strategy = least-recently-used\n    optimizer = hill-climbing\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logback Encoder Pattern with Markers and MDC in Akka\nDESCRIPTION: A Logback encoder configuration that includes date, level, logger, marker, thread, message and MDC properties. This pattern is useful for capturing Akka's security events and other marker-based logs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_26\n\nLANGUAGE: xml\nCODE:\n```\n  <encoder>\n    <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n  </encoder>\n```\n\n----------------------------------------\n\nTITLE: Offering Streaming Data: SourceRef Example in Scala\nDESCRIPTION: Example of how to offer a `SourceRef` within an Akka Cluster in Scala, allowing a remote actor system to consume a locally prepared source of data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-refs.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nsnip [FlowStreamRefsDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowStreamRefsDocSpec.scala) { #offer-source }\n```\n\nLANGUAGE: Scala\nCODE:\n```\nsnip [FlowStreamRefsDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowStreamRefsDocSpec.scala) { #offer-source-use }\n```\n\n----------------------------------------\n\nTITLE: Adding Tags to MDC in Java\nDESCRIPTION: Shows how to add tags to the Mapped Diagnostic Context (MDC) for an actor in Java, which can be used for categorizing log entries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nBehavior<String> behavior = Behaviors.setup(context -> {\n  context.setTags(ActorTags.create(\"tag1\", \"tag2\"));\n  // ...\n  return Behaviors.empty();\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Props for Actors in Scala\nDESCRIPTION: Demonstrates various ways to create Props objects in Scala, which are configuration classes for actor creation containing deployment information.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval props1 = Props[MyActor]()\nval props2 = Props(new MyActor)\nval props3 = Props(classOf[MyActor])\nval props4 = Props(new MyActorC(\"arg\"))\nval props5 = Props(classOf[MyActorC], \"arg\")\n```\n\n----------------------------------------\n\nTITLE: Creating Gzip Compression Flow in Akka - Scala\nDESCRIPTION: This snippet demonstrates how to create a flow that gzip-compresses a stream of ByteStrings using Akka Streams in Scala. It ensures that each ByteString can be decompressed independently by flushing after each element. Use the overload method to adjust compression levels as needed. This flow emits when the compression yields output, backpressures when backpressure is encountered from the downstream, and completes when upstream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Compression/gzip.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nscala=\\\"#gzip:akka.stream.scaladsl.Flow[akka.util.ByteString,akka.util.ByteString,akka.NotUsed]\\\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Role-based Minimum Members\nDESCRIPTION: Configuration for specifying the minimum number of members per role required in the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_11\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.cluster.role {\n  frontend.min-nr-of-members = 1\n  backend.min-nr-of-members = 2\n}\n```\n\n----------------------------------------\n\nTITLE: Using Cluster Sharding\nDESCRIPTION: Example of how to send messages to entities through the ShardRegion. Shows practical usage of the cluster sharding functionality.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n#counter-usage\n```\n\nLANGUAGE: Java\nCODE:\n```\n#counter-usage\n```\n\n----------------------------------------\n\nTITLE: Configuring BackoffSupervisor with Custom Stop Strategy\nDESCRIPTION: Demonstrates setting up a BackoffSupervisor that requires manual reset from the child actor and uses a default stopping strategy. The supervisor will stop the child actor on any exception.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nBackoffSupervisor.props(\n  BackoffOpts.onStop(\n    childProps = Props[FrontendActor],\n    childName = \"myActor\",\n    minBackoff = 3.seconds,\n    maxBackoff = 30.seconds,\n    randomFactor = 0.2\n  ).withManualReset\n    .withDefaultStoppingStrategy)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Streams using Concat Operator in Java\nDESCRIPTION: Shows the usage of concat operator for combining two streams sequentially in Java. The second stream's elements are emitted after the first stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/concat.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSourceOrFlow.java\n```\n\n----------------------------------------\n\nTITLE: Extracting ShardId from StartEntity in Java\nDESCRIPTION: Shows the Java implementation for extracting a ShardId from an EntityId when handling Shard.StartEntity messages for remembered entities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n@@snip [ClusterShardingTest.java](/akka-docs/src/test/java/jdocs/sharding/ClusterShardingTest.java) { #extractShardId-StartEntity }\n```\n\n----------------------------------------\n\nTITLE: Handling Update Response Pattern 1 in Scala\nDESCRIPTION: Example of handling the response from an Update operation by pattern matching on different response types like UpdateSuccess and UpdateTimeout.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nreplicator ! Update(Counter1Key, PNCounter.empty, WriteLocal)(_ :+ 1)\n\nreceive {\n  case UpdateSuccess(Counter1Key, _) =>\n    // ok\n  case UpdateTimeout(Counter1Key, _) =>\n    // will eventually be replicated, but read from other nodes might not\n    // see the update yet\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating Persistence Plugin Proxy Extension in Akka\nDESCRIPTION: Methods to instantiate the persistence plugin proxy extension in Akka, ensuring the target persistence plugin is initialized.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nPersistencePluginProxyExtension\nPersistencePluginProxy.start\n```\n\n----------------------------------------\n\nTITLE: Implementing Remote Router Deployment in Scala\nDESCRIPTION: Scala code for creating a cluster-aware router that deploys routee actors on remote nodes with the \"compute\" role, with a maximum of 3 instances per node.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\nval workerRouter = context.actorOf(\n  ClusterRouterPool(ConsistentHashingPool(0), ClusterRouterPoolSettings(\n    totalInstances = 100, maxInstancesPerNode = 3,\n    allowLocalRoutees = false, useRoles = Set(\"compute\"))).\n    props(Props[StatsWorker]),\n  name = \"workerRouter2\")\n\n```\n\n----------------------------------------\n\nTITLE: Provisioning a Rotating TLS Certificate for an Akka Service (YAML)\nDESCRIPTION: Creates a Certificate resource for an Akka service ('my-service') signed by the custom CA issuer. The certificate secret ('my-service-akka-tls-certificate') is rotated every 16 hours and lasts for 24 hours, using DNS naming conventions for clarity. This secret is later consumed by Akka pods for mTLS authentication. Dependencies: prior Issuer and CA certificate setup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1\\nkind: Certificate\\nmetadata:\\n  name: my-service-akka-tls-certificate\\n  namespace: default\\nspec:\\n  issuerRef:\\n    name: akka-tls-ca-issuer\\n  secretName: my-service-akka-tls-certificate\\n  dnsNames:\\n  - my-service.default.akka.cluster.local\\n  duration: 24h\\n  renewBefore: 16h\\n  privateKey:\\n    rotationPolicy: Always\n```\n\n----------------------------------------\n\nTITLE: LMDB Directory Configuration in Scala\nDESCRIPTION: Configuration for specifying the directory location for LMDB files when using durable storage in Scala, with options for relative or absolute paths.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_40\n\nLANGUAGE: text\nCODE:\n```\n# Directory of LMDB file. There are two options:\n# 1. A relative or absolute path to a directory that ends with 'ddata'\n#    the full name of the directory will contain name of the ActorSystem\n#    and its remote port.\n# 2. Otherwise the path is used as is, as a relative or absolute path to\n#    a directory.\nakka.cluster.distributed-data.durable.lmdb.dir = \"ddata\"\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents in Akka Documentation with Markdown\nDESCRIPTION: This snippet demonstrates how to create a table of contents structure in Akka documentation using Paradox markdown directives, including toc and index blocks that link to various deployment-related documentation pages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/deploy.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Package, Deploy and Run\n \n@@toc { depth=2 }\n\n@@@ index\n \n* [Packaging](packaging.md)\n* [Operating, Managing, Observability](operations.md)\n* [Deploying](deploying.md)\n* [Rolling Updates](rolling-updates.md)\n* [Native Image](native-image.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Configuring 4-Level Segmented Least Recently Used (S4LRU) Policy in Akka Cluster Sharding\nDESCRIPTION: Configures a 4-level segmented least recently used policy with evenly divided levels. This creates a more granular segmentation of entities based on access frequency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_34\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = least-recently-used\n  least-recently-used {\n    segmented = true\n    segments = 4\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Admission Window Policy in Akka Cluster Sharding\nDESCRIPTION: Configures an admission window policy for newly activated entities. This policy tracks new entities and determines if they should enter the main entity tracking area based on an admission filter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_38\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = least-recently-used\n  admission-window {\n    strategy = least-recently-used\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Self-Signed ClusterIssuer with cert-manager (YAML)\nDESCRIPTION: Defines a cluster-wide ClusterIssuer resource in Kubernetes using cert-manager with a self-signed strategy. No external dependencies are required except for cert-manager being installed in the cluster. The issuer is named 'self-signed-issuer', and it is intended to sign the CA certificate used later for Akka. This is a foundational step and typically only one is needed per cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1\\nkind: ClusterIssuer\\nmetadata:\\n  name: self-signed-issuer\\nspec:\\n  selfSigned: {}\n```\n\n----------------------------------------\n\nTITLE: Connection Closing Handler for NACK-Based Write Back-Pressure in Scala\nDESCRIPTION: The closing state of the Scala EchoHandler that manages the graceful shutdown of a TCP connection while ensuring all data is sent. It handles command failures during the closing process.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\ndef closing(reason: ConnectionClosed, buffer: Vector[ByteString]): Receive = {\n  case CommandFailed(Write(_, Ack)) =>\n    connection ! ResumeWriting\n    context.become(waitingForAckAndClosing(reason, buffer))\n\n  case Ack =>\n    if (buffer.isEmpty) {\n      context.stop(self)\n    } else {\n      connection ! Write(buffer.head, Ack)\n      context.become(closing(reason, buffer.tail))\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Avoiding Deadlock Using MergePreferred in Akka Streams (Java)\nDESCRIPTION: A Java implementation using MergePreferred to prioritize the feedback loop. This avoids deadlock by always consuming from the preferred port if elements are available, ensuring that elements in the cycle can always flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\nfinal RunnableGraph<NotUsed> unfair =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final MergePreferredShape<Integer> merge = b.add(MergePreferred.create(1));\n              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n\n              b.from(b.add(source))\n                  .viaFanIn(merge)\n                  .via(b.add(Flow.of(Integer.class).map(s -> { System.out.println(s); return s; })))\n                  .viaFanOut(bcast)\n                  .to(b.add(Sink.ignore()));\n              b.from(bcast).toInlet(merge.preferred());\n              return ClosedShape.getInstance();\n            }));\n```\n\n----------------------------------------\n\nTITLE: Accessing ActorContext in EventSourcedBehavior (Java)\nDESCRIPTION: Shows how to access the ActorContext within an EventSourcedBehavior by wrapping construction with Behaviors.setup in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nBehaviors.setup(\n    (ActorContext<Command> context) -> EventSourcedBehavior.create(\n        PersistenceId.ofUniqueId(\"abc\"),\n        State.empty(),\n        (state, command) -> {\n          if (command == Increment.INSTANCE) {\n            return Effect().persist(Incremented.INSTANCE);\n          } else {\n            return Effect().none();\n          }\n        },\n        (state, event) -> state.withValue(state.value + 1)));\n```\n\n----------------------------------------\n\nTITLE: Querying Shard Region State in Java\nDESCRIPTION: Code snippet demonstrating how to request the current state of a shard region using GetShardRegionState in Java. This returns information about shards and entities in the region.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_52\n\nLANGUAGE: java\nCODE:\n```\nCompletionStage<ShardRegion.CurrentShardRegionState> shardRegionState =\n    sharding.shardState(\"user\");\nshardRegionState.thenAccept(state -> {\n  state.getShards().forEach((shardId, entityIds) -> {\n    System.out.println(\"Shard: \" + shardId + \" Entity IDs: \" + entityIds);\n  });\n});\n```\n\n----------------------------------------\n\nTITLE: Checking Recovery Status within a Persistent Actor (Java)\nDESCRIPTION: Shows how an AbstractPersistentActor can check if the recovery process is currently active using the `isRecoveryRunning()` method. Additionally, it demonstrates retrieving the sequence number of the most recently replayed event via `recoveryRunningNum()`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic Receive createReceiveRecover() {\n  return receiveBuilder()\n      .match(Evt.class, this::updateState)\n      .match(SnapshotOffer.class, ss -> state = (State) ss.snapshot())\n      .match(RecoveryCompleted.class, this::onRecoveryComplete)\n      .build();\n}\n\nprivate void onRecoveryComplete(RecoveryCompleted completed) {\n  log().info(\n      \"MyPersistentActor [{}] recovery completed. Current state: [{}]\", persistenceId(), state);\n}\n\n@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n      .matchEquals(GetState.class, s -> sender().tell(state, self()))\n      .matchEquals(\"snap\", s -> saveSnapshot(state))\n      .matchEquals(\n          \"print\",\n          s -> {\n            log().info(\n                \"MyPersistentActor [{}] received [print] command. Current state: [{}]\",\n                persistenceId(),\n                state);\n            if (isRecoveryRunning()) {\n              log().info(\"RECOVERY RUNNING: \" + recoveryRunningNum());\n            } else {\n              log().info(\"RECOVERY COMPLETED\");\n            }\n          })\n      .match(\n          SaveUser.class,\n          save ->\n              persist(\n                  new UserSaved(save.getUser()),\n                  (UserSaved evt) -> {\n                    updateState(evt);\n                  }))\n      .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Basic GroupBy Implementation in Java\nDESCRIPTION: Shows how to use groupBy in Java to partition words by length and process substreams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/groupBy.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"))\n    .groupBy(3, word -> word.length())\n    .reduce((word1, word2) -> word1 + \"-\" + word2)\n    .mergeSubstreams()\n    .runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Handling Get Response Pattern 1 in Scala\nDESCRIPTION: Example of handling the response from a Get operation by pattern matching on different response types like GetSuccess and GetFailure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\nreplicator ! Get(Counter1Key, ReadLocal)\n\nreceive {\n  case g @ GetSuccess(Counter1Key, _) =>\n    val value = g.get(Counter1Key).value\n    ...\n  case NotFound(Counter1Key, _) =>\n    // key counter1 does not exist\n  case GetFailure(Counter1Key, _) =>\n    // read failed, try again later?\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Most Recently Used (MRU) Policy in Akka Cluster Sharding\nDESCRIPTION: Configures a passivation strategy using the most recently used policy. This policy passivates entities with the most recent activity when the number of active entities exceeds the specified limit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_35\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.passivation {\n  strategy = most-recently-used\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Half-Closed TCP Server with Flow.fromSinkAndSource in Scala\nDESCRIPTION: This snippet demonstrates how to create a TCP server that cancels the incoming stream but continues to send periodic output to the client. It uses Flow.fromSinkAndSource to combine a Sink that cancels after processing one element and a Source that generates timestamps every second.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/fromSinkAndSource.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval handler: Flow[ByteString, ByteString, NotUsed] =\n  Flow.fromSinkAndSource(\n    Sink.foreach[ByteString](data => println(s\"Server received: $data\")),\n    Source\n      .tick(Duration.Zero, 1.second, NotUsed)\n      .map(_ => ByteString(s\"Server time is ${LocalTime.now()}\\n\"))\n  )\n```\n\n----------------------------------------\n\nTITLE: Actor Polling with Tick in Scala\nDESCRIPTION: Example of using Source.tick to periodically query an actor using ask pattern and emit responses downstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/tick.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\ndef queryActor(actor: ActorRef[Query]): Source[Response, NotUsed] =\n  Source\n    .tick(0.seconds, 1.second, \"tick\")\n    .mapAsync(1)(_ => actor.ask(Query(_)))\n```\n\n----------------------------------------\n\nTITLE: Akka Flow Ask Pattern Signature\nDESCRIPTION: API signatures for the ask operator on Flow, which sends request-reply messages to an ActorRef. Available for both Scala and Java with different type parameter handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/ask.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nFlow.ask[S](ref:akka.actor.ActorRef)(implicittimeout:akka.util.Timeout,implicittag:scala.reflect.ClassTag[S]):FlowOps.this.Repr[S]\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow.ask(akka.actor.ActorRef,java.lang.Class,akka.util.Timeout)\n```\n\n----------------------------------------\n\nTITLE: Using the actorRefWithBackpressure Sink in Java\nDESCRIPTION: Java code demonstrating how to use the actorRefWithBackpressure sink to send stream elements to an actor. It shows configuration of initialization, acknowledgement, and completion messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/actorRefWithBackpressure.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nfinal ActorRef ref = system.actorOf(Props.create(Ref.class));\n\nSource.range(1, 10)\n  .map(i -> i.toString())\n  .runWith(\n    Sink.actorRefWithBackpressure(\n      ref,\n      new StreamInit(),\n      Continue.instance,\n      new StreamComplete(),\n      StreamFailure::new),\n    mat);\n```\n\n----------------------------------------\n\nTITLE: Defining sbt Version for Akka Packaging\nDESCRIPTION: Specifies the sbt version to use in the project/build.properties file when packaging an Akka application.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/packaging.md#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nsbt.version=1.3.12\n```\n\n----------------------------------------\n\nTITLE: Creating Top-Level Actors with Props in Scala\nDESCRIPTION: Demonstrates creating top-level actors using ActorSystem.actorOf() with Props configuration. Shows basic actor instantiation pattern.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n#system-actorOf\n```\n\n----------------------------------------\n\nTITLE: Using Sink.takeLast in Scala\nDESCRIPTION: Example demonstrating how to use Sink.takeLast operator in Scala to collect the last n elements from a stream into an immutable.Seq. The materialized Future will complete when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/takeLast.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#takeLast-operator-example\n```\n\n----------------------------------------\n\nTITLE: Enforced Replies Example in Java\nDESCRIPTION: Example showing how to enforce replies at compile time using EventSourcedBehavior in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_33\n\nLANGUAGE: Java\nCODE:\n```\n#withEnforcedReplies\n```\n\n----------------------------------------\n\nTITLE: Preventing Child Actor Stop on Parent Restart (Scala)\nDESCRIPTION: Shows how to override the default behavior in Scala to prevent child actors from being stopped when the parent restarts. This is achieved by placing `Behaviors.supervise` inside `Behaviors.setup` and using `SupervisorStrategy.restart.withStopChildren(false)`. Consequently, the `setup` block (and child creation) runs only once, not on restarts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\n// #restart-keep-children\nBehaviors.setup[String] { context =>\n  val child1 = context.spawn(child, \"child1\")\n  val child2 = context.spawn(child, \"child2\")\n\n  // supervision strategy inside setup to not recreate children on restart\n  Behaviors\n    .supervise( Behaviors.receiveMessage[String] { msg =>\n      // processing messages\n      // ...\n      // restart occurs, children will be kept\n      Behaviors.same\n    })\n    .onFailure(SupervisorStrategy.restart.withStopChildren(false))\n\n}\n// #restart-keep-children\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Large Message Logging in Akka\nDESCRIPTION: HOCON configuration for enabling logging of messages exceeding a specified size threshold in bytes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_13\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery {\n  log-frame-size-exceeding = 10000b\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Shared Resource Class for Database Connection Pool in Java\nDESCRIPTION: Creates a SharedResource class representing an expensive database connection pool in Java. This class is used to demonstrate the purpose of the Akka extension.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nclass SharedResource {\n  private final String connection;\n\n  public SharedResource() {\n    connection = \"DB-Connection\";\n  }\n\n  public void query(String id) {\n    System.out.println(\"Querying \" + id + \" using \" + connection);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing conflateWithSeed in Java\nDESCRIPTION: Example demonstrating the usage of conflateWithSeed operator in Java with type transformation and element aggregation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/conflateWithSeed.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#conflateWithSeed-type #conflateWithSeed\n```\n\n----------------------------------------\n\nTITLE: Handling Buffering for NACK-Based Write Back-Pressure in Scala\nDESCRIPTION: The buffering state of the Scala EchoHandler that manages write failures. It buffers data while writing is suspended and resumes when TCP connection is ready to accept more writes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\ndef buffering(buffer: Vector[ByteString]): Receive = {\n  case Received(data) =>\n    connection ! Write(data, Ack)\n    context.become(buffering(buffer :+ data))\n\n  case CommandFailed(Write(_, Ack)) =>\n    connection ! ResumeWriting\n    context.become(waitingForAck(buffer))\n\n  case Ack =>\n    if (buffer.isEmpty) {\n      context.become(writing)\n    } else {\n      connection ! Write(buffer.head, Ack)\n      context.become(buffering(buffer.tail))\n    }\n\n  case _: ConnectionClosed =>\n    context.stop(self)\n}\n```\n\n----------------------------------------\n\nTITLE: Using the actorRefWithBackpressure Sink in Scala\nDESCRIPTION: Scala code demonstrating how to use the actorRefWithBackpressure sink to send stream elements to an actor. It shows configuration of initialization, acknowledgement, and completion messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/actorRefWithBackpressure.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nval ref = system.actorOf(Props[Ref]())\n\nSource(1 to 10)\n  .map(i => i.toString)\n  .runWith(Sink.actorRefWithBackpressure(\n    ref,\n    StreamInit,\n    Continue,\n    StreamComplete,\n    e => StreamFailure(e)))\n```\n\n----------------------------------------\n\nTITLE: Creating File Writing Sink with FileIO.toFile in Akka Streams (Java)\nDESCRIPTION: Creates a sink that writes incoming ByteString elements to a specified file. It materializes a CompletionStage of IOResult containing the file size and any potential exceptions. This method is deprecated in favor of toPath.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/FileIO/toFile.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nFileIO.toFile(java.io.File, java.util.Set)\n```\n\n----------------------------------------\n\nTITLE: Request-Response Handling in Scala and Java\nDESCRIPTION: Demonstrates how a recipient actor handles a request and sends back a response using the replyTo ActorRef provided in the request message.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n  val cookieShop: Behavior[Request] =\n    Behaviors.receiveMessage {\n      case CookieOrder(cookie, replyTo) =>\n        if (cookie == \"Chocolate chip\") {\n          replyTo ! Confirmation(cookie)\n        } else {\n          replyTo ! Invalid(\"We only sell chocolate chip cookies\")\n        }\n        Behaviors.same\n    }\n```\n\nLANGUAGE: java\nCODE:\n```\n  public static Behavior<Request> cookieShop() {\n    return Behaviors.receive(Request.class)\n        .onMessage(CookieOrder.class, order -> {\n          if (order.cookie.equals(\"Chocolate chip\")) {\n            order.replyTo.tell(new Confirmation(order.cookie));\n          } else {\n            order.replyTo.tell(new Invalid(\"We only sell chocolate chip cookies\"));\n          }\n          return Behaviors.same();\n        })\n        .build();\n  }\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Behavior with TestActorRef in Java\nDESCRIPTION: Shows how to test actor behavior in Java using TestActorRef. The example demonstrates synchronous message processing and state verification using the TestActorRef API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_40\n\nLANGUAGE: java\nCODE:\n```\nclass MyActor extends AbstractActor {\n    private String state = \"\";\n    public String getState() { return state; }\n    @Override\n    public Receive createReceive() {\n        return receiveBuilder()\n            .matchEquals(\"blah\", s -> state = \"blah received\")\n            .build();\n    }\n}\n\nstatic ActorSystem system = ActorSystem.create();\nTestActorRef<MyActor> actorRef = TestActorRef.create(system, Props.create(MyActor.class));\nactorRef.tell(\"blah\", ActorRef.noSender());\nassert(actorRef.underlyingActor().getState().equals(\"blah received\"));\n```\n\n----------------------------------------\n\nTITLE: Implementing TakeWhile Operator in Scala\nDESCRIPTION: Example demonstrating how to use the takeWhile operator in Scala to filter stream elements based on a predicate condition. The operator passes elements downstream until the predicate returns false.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/takeWhile.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#take-while\n```\n\n----------------------------------------\n\nTITLE: Using PartitionHub for Element Routing in Scala\nDESCRIPTION: This snippet demonstrates how to use PartitionHub to route elements from a producer to multiple consumers based on a partitioning function in Scala. It shows how to create the hub and attach consumers to it.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\nval sink = PartitionHub.sink(\n  (size, elem) => math.abs(elem.hashCode) % size,\n  startAfterNrOfConsumers = 2,\n  bufferSize = 256)\n\nval producer = Source(1 to 1000).runWith(sink)\n\n// wait until 2 consumers have been connected\n\nval consumer1 = producer.runWith(Sink.fold(0)((c, _) => c + 1))\nval consumer2 = producer.runWith(Sink.fold(0)((c, _) => c + 1))\nval count1 = Await.result(consumer1, 3.seconds)\nval count2 = Await.result(consumer2, 3.seconds)\nprintln(s\"consumer1 got $count1 elements\")\nprintln(s\"consumer2 got $count2 elements\")\n```\n\n----------------------------------------\n\nTITLE: Using LoggerFactory in Java\nDESCRIPTION: Shows how to use LoggerFactory to obtain a logger outside of an actor context in Java. This is useful for logging in non-actor classes or in CompletionStage callbacks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nclass MyClass {\n  private static final Logger log = LoggerFactory.getLogger(MyClass.class);\n\n  public void doSomething(String msg) {\n    log.debug(\"Doing something with {}\", msg);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in Build Tools (sbt, Maven, Gradle)\nDESCRIPTION: This snippet demonstrates how to configure the Akka library repository in common build tools (sbt, Maven, and Gradle). Adding this repository allows the project to resolve Akka modules. Replace the sample values with those appropriate for the build environment if needed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt,Maven,Gradle\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Video Frame Handling with extrapolate in Java\nDESCRIPTION: Example showing how to use the extrapolate operator in Java to maintain video display by repeating the last frame when network issues cause frame drops. This ensures the UI always shows the most recent successfully decoded frame.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/extrapolate.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource<VideoFrame, NotUsed> videoStream = networkVideo\n  .extrapolate(frame -> Collections.singletonList(frame).iterator())\n  .map(VideoFrame::display);\n```\n\n----------------------------------------\n\nTITLE: Configuring Coordinated Shutdown Exit in Akka\nDESCRIPTION: Configuration to enable JVM exit during coordinated shutdown when a node is removed from the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_1\n\nLANGUAGE: hocon\nCODE:\n```\nakka.coordinated-shutdown.exit-jvm = on\n```\n\n----------------------------------------\n\nTITLE: Pull-Based Reading Echo Server in Scala\nDESCRIPTION: A Scala implementation of an Echo server using pull-based reading mode. This approach eliminates the need for a buffer by only resuming reading when the previous write operation has been acknowledged.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\nclass PullEcho extends Actor {\n\n  implicit val system = context.system\n  val listener = context.actorOf(Props(classOf[Listener], self))\n\n  override def preStart(): Unit = {\n    IOManager(system) ! IOManager.Listen(listener, new InetSocketAddress(\"localhost\", 0), pullMode = true)\n  }\n\n  def receive = {\n    case Bound(address) =>\n      println(s\"Listening on port ${address.getPort}\")\n\n  }\n\n}\n\nclass Listener(boss: ActorRef) extends Actor {\n\n  def receive = {\n    case Bound(address) =>\n      boss ! Bound(address)\n      context.become(bound)\n  }\n\n  def bound: Receive = {\n    case Connected(remote, local) =>\n      val handler = context.actorOf(Props[PullEchoHandler])\n      sender() ! Register(handler, keepOpenOnPeerClosed = true)\n      context.watch(handler)\n\n  }\n}\n\nclass PullEchoHandler extends Actor {\n  import Tcp._\n  \n  def receive = {\n    case Received(data) =>\n      sender() ! Write(data, Ack)\n      context.become(waiting(sender()))\n  }\n\n  def waiting(connection: ActorRef): Receive = {\n    case Ack =>\n      connection ! ResumeReading\n      context.unbecome()\n  }\n\n  override def preStart(): Unit = {\n    super.preStart()\n    // start out in pull mode\n    sender() ! ResumeReading\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Using Dedicated Dispatcher for Blocking Operations (Scala)\nDESCRIPTION: This snippet demonstrates how to use a dedicated dispatcher for blocking operations in Scala, which helps prevent thread starvation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BlockingDispatcherSample.scala](/akka-docs/src/test/scala/docs/actor/typed/BlockingDispatcherSample.scala) { #separate-dispatcher }\n```\n\n----------------------------------------\n\nTITLE: Repeating Last Element Stream Processor in Akka Streams (Java)\nDESCRIPTION: Java version of the custom GraphStage that repeats the last seen element if the upstream is slower than the downstream. Includes an initial value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_46\n\nLANGUAGE: Java\nCODE:\n```\npublic class HoldWithInitial<T> extends GraphStage<FlowShape<T, T>> {\n    private final Inlet<T> in = Inlet.create(\"HoldWithInitial.in\");\n    private final Outlet<T> out = Outlet.create(\"HoldWithInitial.out\");\n    private final FlowShape<T, T> shape = FlowShape.of(in, out);\n    private final T initial;\n\n    public HoldWithInitial(T initial) {\n        this.initial = initial;\n    }\n\n    @Override\n    public FlowShape<T, T> shape() {\n        return shape;\n    }\n\n    @Override\n    public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n        return new GraphStageLogic(shape()) {\n            private T currentValue = initial;\n\n            {\n                setHandler(\n                        in,\n                        new AbstractInHandler() {\n                            @Override\n                            public void onPush() throws Exception {\n                                currentValue = grab(in);\n                                pull(in);\n                            }\n                        });\n\n                setHandler(\n                        out,\n                        new AbstractOutHandler() {\n                            @Override\n                            public void onPull() throws Exception {\n                                push(out, currentValue);\n                            }\n                        });\n            }\n\n            @Override\n            public void preStart() {\n                pull(in);\n            }\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Partial Functions for Message Handling in Scala\nDESCRIPTION: Creating reusable partial functions to handle specific message types, which can be composed into different behaviors. This pattern enables code reuse across actor states.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\nval getHandler: PartialFunction[Command, Behavior[Command]] = {\n  case GetValue(replyTo) =>\n    replyTo ! value\n    Behaviors.same\n}\n```\n\nLANGUAGE: Scala\nCODE:\n```\nval setHandlerNonZero: PartialFunction[Command, Behavior[Command]] = {\n  case SetValue(newValue) if newValue != 0 =>\n    nonZeroBehavior(newValue)\n}\n```\n\nLANGUAGE: Scala\nCODE:\n```\nval setHandlerZero: PartialFunction[Command, Behavior[Command]] = {\n  case SetValue(0) =>\n    zeroBehavior(0)\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Messages with Behaviors.logMessages in Java\nDESCRIPTION: Shows how to use Behaviors.logMessages to enable detailed logging of messages and signals for a specific actor behavior in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nBehavior<String> behavior = Behaviors.setup(context -> \n  Behaviors.logMessages(\n    LogOptions.create().withLevel(LogLevel.INFO).withSeparateLogger(),\n    Behaviors.receiveMessage(message -> {\n      // ... handle message\n      return Behaviors.same();\n    })\n  )\n);\n```\n\n----------------------------------------\n\nTITLE: Registering Actor DeathWatch Monitoring in Akka (Java)\nDESCRIPTION: Shows how to register a Java Akka actor to watch another actor for Terminated messages via DeathWatch. Requires Akka libraries, actor references, and appropriate imports. The snippet includes both relevant import of Terminated and usage of context().watch(target), allowing actors to observe and respond to the lifecycle end of peer actors. Input is the ActorRef to be watched; output is handling of Terminated messages when the target actor is stopped or terminated.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\n// Relevant import:\nimport akka.actor.Terminated;\n\npublic class WatchActor extends AbstractActor {\n  private final ActorRef target;\n\n  public WatchActor(ActorRef target) {\n    this.target = target;\n    getContext().watch(target);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n      .match(Terminated.class, t -> t.actor().equals(target), t -> getContext().stop(getSelf()))\n      .build();\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Basic Actor Message Receiving - Scala\nDESCRIPTION: Basic implementation of an actor's receive method to handle incoming messages using pattern matching in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_32\n\nLANGUAGE: scala\nCODE:\n```\n#imports1 #my-actor\n```\n\n----------------------------------------\n\nTITLE: Logging in Akka Stream GraphStages - Java\nDESCRIPTION: This Java snippet illustrates how to use GraphStageLogicWithLogging or TimerGraphStageLogicWithLogging to obtain logging capabilities in Akka Stream GraphStages. A stream.Materializer capable of providing a logger is required, allowing for debugging and tracking within stream operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [GraphStageLoggingDocTest.java](/akka-docs/src/test/java/jdocs/stream/GraphStageLoggingDocTest.java) { #operator-with-logging }\n```\n\n----------------------------------------\n\nTITLE: Obtaining SelfUniqueAddress for Akka Distributed Data in Java\nDESCRIPTION: Shows the Java code required to obtain the `SelfUniqueAddress` from the `DistributedData` extension. This address uniquely identifies the node and is needed when interacting with the `Replicator`. The code is referenced from `ReplicatorDocSample.java`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ReplicatorTest.java](/akka-cluster-typed/src/test/java/jdocs/akka/cluster/ddata/typed/javadsl/ReplicatorDocSample.java) { #selfUniqueAddress }\n```\n\n----------------------------------------\n\nTITLE: FSM State Function Definition in Scala\nDESCRIPTION: Example of defining state handling logic using pattern matching in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nstartWith(state, data[, timeout])\n```\n\n----------------------------------------\n\nTITLE: Implementing a Subscriber Actor in Scala\nDESCRIPTION: Example of creating a subscriber actor that registers for a specific topic with the DistributedPubSubMediator. The actor receives messages published to the 'content' topic and prints them.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nclass Subscriber extends Actor {\n  import DistributedPubSubMediator.{ Subscribe, SubscribeAck }\n  val mediator = DistributedPubSub(context.system).mediator\n  // subscribe to the topic named \"content\"\n  mediator ! Subscribe(\"content\", self)\n\n  def receive = {\n    case SubscribeAck(Subscribe(\"content\", None, `self`)) =>\n      context.become(ready)\n  }\n\n  def ready: Receive = {\n    case s: String =>\n      println(s\"Got: $s\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Managing Pull-Based Connection Accepting in Scala\nDESCRIPTION: A Scala example showing how to handle connection acceptance in pull mode. After receiving a Bound event, the actor manually resumes accepting connections using the ResumeAccepting message.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_24\n\nLANGUAGE: scala\nCODE:\n```\ndef receive = {\n  case b @ Bound(address) =>\n    // start accepting connections\n    sender() ! ResumeAccepting(batchSize = 1)\n    context.parent ! b\n    context.become(listening(sender()))\n}\n\ndef listening(connectionTacker: ActorRef): Receive = {\n  case Connected(remote, local) =>\n    val handler = context.actorOf(Props(handlerClass, sender(), remote))\n    sender() ! Register(handler, keepOpenOnPeerClosed = true)\n    connectionTacker ! ResumeAccepting(batchSize = 1)\n}\n```\n\n----------------------------------------\n\nTITLE: Finalizing Publish-Subscribe Channel with Additional Features in Scala\nDESCRIPTION: This snippet wraps the Sink and Source in a Flow, adds a KillSwitch, and implements a backpressure timeout. It creates a more robust publish-subscribe channel with additional control features in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\nval publishSubscribeFlow: Flow[String, String, UniqueKillSwitch] =\n  Flow.fromSinkAndSourceCoupled(sink, source)\n    .joinMat(KillSwitches.singleBidi[String, String])(Keep.right)\n    .backpressureTimeout(3.seconds)\n```\n\n----------------------------------------\n\nTITLE: Watching Actor Termination with Custom Message in Akka Typed (Scala)\nDESCRIPTION: Demonstrates how to use watchWith to monitor actor termination and receive a custom message upon termination, allowing for additional context to be included.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\nobject Master {\n  sealed trait Command\n  case class StartJob(jobId: Int, replyToWhenDone: ActorRef[JobResult]) extends Command\n  private case class JobTerminated(jobId: Int, replyToWhenDone: ActorRef[JobResult])\n      extends Command\n\n  def apply(): Behavior[Command] =\n    Behaviors.setup { context =>\n      Behaviors.receiveMessage {\n        case StartJob(jobId, replyToWhenDone) =>\n          val worker = context.spawn(Worker(), s\"worker-$jobId\")\n          context.watchWith(worker, JobTerminated(jobId, replyToWhenDone))\n          worker ! Worker.DoWork(jobId)\n          Behaviors.same\n        case JobTerminated(jobId, replyToWhenDone) =>\n          println(s\"Worker for job $jobId finished\")\n          replyToWhenDone ! JobResult(jobId)\n          Behaviors.same\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Spawn and Probe Verification - Java\nDESCRIPTION: Illustrates the Java approach to spawning actors and using a TestProbe for response verification in tests using ActorTestKit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AsyncTestingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/AsyncTestingExampleTest.java) { #test-spawn }\n```\n\n----------------------------------------\n\nTITLE: Log Output Showing Partition Assignment on Second Node (Text)\nDESCRIPTION: This log line shows that when the second processor node starts and joins the Kafka consumer group, Kafka rebalances the partitions. This specific line indicates that partition 29 has been assigned to this new node ('current node'). The external shard allocation strategy will use this information to potentially move the corresponding shard (also ID 29 in this setup) to this node.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nPartition [29] assigned to current node. Updating shard allocation\n```\n\n----------------------------------------\n\nTITLE: Setting Event Storage Policy in Scala\nDESCRIPTION: Example demonstrating how to set a custom event storage policy in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n\"#set-event-storage-policy\"\n```\n\n----------------------------------------\n\nTITLE: Querying Cluster Sharding Stats in Java\nDESCRIPTION: Code snippet showing how to retrieve cluster-wide sharding statistics using GetClusterShardingStats in Java. This provides information about shards and entity counts across all regions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_54\n\nLANGUAGE: java\nCODE:\n```\nCompletionStage<ShardRegion.ClusterShardingStats> shardingStats =\n    sharding.shardingStats(\"user\");\nshardingStats.thenAccept(stats -> {\n  stats.getRegions().forEach((region, regionStats) -> {\n    System.out.println(\"Region: \" + region);\n    regionStats.getStats().forEach((shardId, entityCount) -> {\n      System.out.println(\"  Shard: \" + shardId + \" Count: \" + entityCount);\n    });\n  });\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Persistence Typed Dependency (sbt/Maven/Gradle)\nDESCRIPTION: Shows how to add the `akka-persistence-typed` library and the `akka-persistence-testkit` (for testing) to a project using sbt, Maven, or Gradle. It leverages the Akka Bill of Materials (BOM) for consistent version management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\nval AkkaVersion = \"$akka.version$\"\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-persistence-typed\" % AkkaVersion,\n  \"com.typesafe.akka\" %% \"akka-persistence-testkit\" % AkkaVersion % Test\n)\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<properties>\n  <scala.binary.version>$scala.binary.version$</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_${scala.binary.version}</artifactId>\n      <version>$akka.version$</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-persistence-typed_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-persistence-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\ndef versions = [\n  ScalaBinary: \"$scala.binary.version$\"\n]\ndef akkaVersion = \"$akka.version$\"\n\n\n\nimplementation platform(\"com.typesafe.akka:akka-bom_${versions.ScalaBinary}:${akkaVersion}\")\n\nimplementation \"com.typesafe.akka:akka-persistence-typed_${versions.ScalaBinary}\"\ntestImplementation \"com.typesafe.akka:akka-persistence-testkit_${versions.ScalaBinary}\"\n```\n\n----------------------------------------\n\nTITLE: Selecting a Dispatcher for Actor System in Scala\nDESCRIPTION: Illustrates how to select a custom dispatcher for spawning an actor in Scala using DispatcherSelector. Requires akka-actor-typed library. Employs methods like DispatcherSelector.default to manage dispatcher selection. Intended for creating Props instances for precise actor dispatcher control.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [DispatcherDocSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/DispatchersDocSpec.scala) { #spawn-dispatcher }\n```\n\n----------------------------------------\n\nTITLE: Defining Immutable Messages in Akka\nDESCRIPTION: Demonstrates how to define immutable messages in Akka using case classes in Scala and final classes with immutable fields in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\ncase class ImmutableMessage(data: String)\n\nval message = ImmutableMessage(\"Hello\")\n```\n\nLANGUAGE: Java\nCODE:\n```\npublic final class ImmutableMessage {\n  private final String data;\n\n  public ImmutableMessage(String data) {\n    this.data = data;\n  }\n\n  public String getData() {\n    return data;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Tasks to Coordinated Shutdown in Scala\nDESCRIPTION: Demonstrates how to add a custom task to the 'before-service-unbind' phase of the coordinated shutdown process in Scala. The task sends a message to an actor and completes when the actor replies with a 'Done' message.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nCoordinatedShutdown(system).addTask(CoordinatedShutdown.PhaseBeforeServiceUnbind, \"someTaskName\") { () =>\n  val promise = Promise[Done]()\n  actorRef ! DoSomething(promise)\n  promise.future\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Worker Actors Externally (Scala/Java)\nDESCRIPTION: Illustrates how to create the routee actors (Workers) separately before creating the group router. These actors are registered with the actor system under specific paths ('/user/workers/w1', etc.), which are then used by the group router configuration or programmatic setup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\n//#create-worker-actors\nsystem.actorOf(Props[Worker](), \"w1\")\nsystem.actorOf(Props[Worker](), \"w2\")\nsystem.actorOf(Props[Worker](), \"w3\")\n//#create-worker-actors\n```\n\nLANGUAGE: java\nCODE:\n```\n//#create-worker-actors\nsystem.actorOf(Props.create(Worker.class), \"w1\");\nsystem.actorOf(Props.create(Worker.class), \"w2\");\nsystem.actorOf(Props.create(Worker.class), \"w3\");\n//#create-worker-actors\n```\n\n----------------------------------------\n\nTITLE: Custom Logback Pattern with akkaSource in MDC - Logback - XML\nDESCRIPTION: This XML fragment configures a Logback encoder to include the MDC property 'akkaSource' in each log line. The pattern specifies ISO8601 date, log level, logger name, actor path, and message, enabling context-rich output through %X{akkaSource}. Proper MDC setup in Akka/SLF4J is required for the actor context to appear. This should be embedded inside a layout section of logback.xml, and assumes Akka is configured to provide MDC values.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_15\n\nLANGUAGE: xml\nCODE:\n```\n  <encoder>\n    <pattern>%date{ISO8601} %-5level %logger{36} %X{akkaSource} - %msg%n</pattern>\n  </encoder>\n```\n\n----------------------------------------\n\nTITLE: Configuring Local LevelDB Journal Directory (HOCON)\nDESCRIPTION: Illustrates configuring the file system path for the local LevelDB journal using the `akka.persistence.journal.leveldb.dir` setting. The specified path can be relative or absolute; if not set, it defaults to a 'journal' directory in the current working directory.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_4\n\nLANGUAGE: hocon\nCODE:\n```\n# Assuming the snippet configures the leveldb journal directory\nakka.persistence.journal.leveldb {\n  # Path to the journal directory\n  dir = \"target/journal\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Router with OptimalSizeExploringResizer in Scala\nDESCRIPTION: Creates a router pool with an OptimalSizeExploringResizer programmatically in Scala. This resizer attempts to find the optimal pool size between 1 and 10 routees.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_40\n\nLANGUAGE: scala\nCODE:\n```\nval resizer = OptimalSizeExploringResizer()\nval router3 = context.actorOf(\n  RoundRobinPool(5, Some(resizer)).props(Props[Worker]),\n  \"router3\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cluster Membership State\nDESCRIPTION: Demonstrates retrieving the full cluster membership state via the Cluster API without subscribing to events. This snapshot of state may not reflect the latest changes immediately published as events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\nCluster(system).state\n```\n\n----------------------------------------\n\nTITLE: Creating a DurableStateStoreProvider in Java\nDESCRIPTION: Implementation of the DurableStateStoreProvider in Java, which is responsible for creating instances of the custom plugin. This provider handles the plugin instantiation and initialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n#plugin-provider\n```\n\n----------------------------------------\n\nTITLE: Simple Router Implementation in Scala\nDESCRIPTION: Example showing how to use a Router and manage routees from within an actor using RoundRobinRoutingLogic\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RouterDocSpec.scala](/akka-docs/src/test/scala/docs/routing/RouterDocSpec.scala) { #router-in-actor }\n```\n\n----------------------------------------\n\nTITLE: Implementing zipWithIndex in Scala\nDESCRIPTION: Example showing how to use zipWithIndex operator in Scala to pair stream elements with their indices.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipWithIndex.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFlowZipWithIndexSpec.scala\n```\n\n----------------------------------------\n\nTITLE: Implementing foreachAsync Stream Processing in Java\nDESCRIPTION: Shows how to use Sink.foreachAsync for asynchronous stream processing in Java. Demonstrates element processing with controlled parallelism and CompletionStage integration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/foreachAsync.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSink<String> sink = Sink.foreachAsync(\n    2, // parallelism\n    element -> CompletableFuture.runAsync(() -> {\n        try {\n            System.out.println(element);\n            Thread.sleep(1000); // do some heavy computation\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    })\n);\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Cluster Events with CurrentClusterState in Java\nDESCRIPTION: Java implementation for subscribing to cluster events that receives CurrentClusterState as first message. Demonstrates handling current state and incremental updates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nCluster.get(system).subscribe(getSelf(), ClusterEvent.MemberEvent.class);\n```\n\n----------------------------------------\n\nTITLE: Project YAML Configuration\nDESCRIPTION: YAML front matter defining the project description for the migration guide document.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guide-2.6.x-2.7.x.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nproject.description: Migrating to Akka 2.7.\n```\n\n----------------------------------------\n\nTITLE: Defining CA Certificate for Akka TLS with cert-manager (YAML)\nDESCRIPTION: Provisions a Certificate resource in the 'default' namespace, instructing cert-manager to create a secret ('akka-tls-ca-certificate') containing a self-signed CA certificate via the previously defined ClusterIssuer. This certificate is marked as a CA (isCA: true), uses a long duration (100 years), and is set to always rotate its private key. Required for issuing service certificates, with the naming convention assisting in management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1\\nkind: Certificate\\nmetadata:\\n  name: akka-tls-ca-certificate\\n  namespace: default\\nspec:\\n  issuerRef:\\n    name: self-signed-issuer\\n    kind: ClusterIssuer\\n  secretName: akka-tls-ca-certificate\\n  commonName: default.akka.cluster.local\\n  # 100 years\\n  duration: 876000h\\n  # 99 years\\n  renewBefore: 867240h\\n  isCA: true\\n  privateKey:\\n    rotationPolicy: Always\n```\n\n----------------------------------------\n\nTITLE: Defining Routee Behavior in Akka Java\nDESCRIPTION: This snippet demonstrates how to define a routee behavior in Java for a pool router in Akka Typed. The routee is a child actor that receives messages from the pool router. Ensure Akka dependencies are included and configured correctly in your build tool.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n/* Java routee definition in RouterTest.java */\n```\n\n----------------------------------------\n\nTITLE: Creating Reply Effects in Akka DurableStateBehavior (Java)\nDESCRIPTION: Demonstrates the creation of ReplyEffect instances in Java Akka DurableStateBehavior command handlers using methods like Effect().reply and Effect().noReply. This ensures that replies or explicit omission thereof are always considered in persisted command processing. Dependencies include Akka Persistence Typed Java APIs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_28\n\nLANGUAGE: java\nCODE:\n```\n@@snip [AccountExampleWithNullDurableState.java](/akka-cluster-sharding-typed/src/test/java/jdocs/akka/cluster/sharding/typed/AccountExampleWithNullDurableState.java) { #reply }\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository for sbt/Maven/Gradle\nDESCRIPTION: Specifies the repository URL (`https://repo.akka.io/maven`) required to fetch Akka dependencies. Provides configuration snippets for sbt, Maven, and Gradle build tools to add Akka's library repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Element Counter with statefulMapConcat in Java\nDESCRIPTION: Java implementation of combining stream elements with unique IDs using a stateful counter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMapConcat.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(\"A\", \"B\", \"C\"))\n    .statefulMapConcat(() -> {\n        final AtomicLong counter = new AtomicLong();\n        return el -> Collections.singletonList(Pair.create(el, counter.getAndIncrement()));\n    })\n    .runWith(Sink.seq(), system);\n```\n\n----------------------------------------\n\nTITLE: Drop New Buffer Strategy in Akka Streams\nDESCRIPTION: Implements a buffer that drops new elements when full without enqueueing them. Used when prioritizing existing elements over new ones.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\nexternalService.runWith(\n  Flow[Job].buffer(1000, OverflowStrategy.dropNew)\n)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(externalService)\n  .buffer(1000, OverflowStrategy.dropNew())\n  .run(system)\n```\n\n----------------------------------------\n\nTITLE: Deferred Stream Creation Based on Initial Element in Java\nDESCRIPTION: This example demonstrates how to create a deferred stream based on the initial element by combining completionStageFlow with prefixAndTail in Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/completionStageFlow.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3))\n    .prefixAndTail(1)\n    .flatMapConcat(\n        (pair) -> {\n          Integer firstElement = pair.first().get(0);\n          Source<Integer> tail = pair.second();\n\n          CompletionStage<Flow<Integer, Integer, NotUsed>> futureFlow =\n              CompletableFuture.supplyAsync(\n                  () -> {\n                    // create a different flow based on the first element\n                    if (firstElement % 2 == 0) {\n                      return Flow.of(Integer.class).map(i -> i * 2);\n                    } else {\n                      return Flow.of(Integer.class).map(i -> i + 1);\n                    }\n                  });\n\n          return tail.via(Flow.completionStageFlow(futureFlow));\n        })\n    .runWith(Sink.foreach(System.out::println), system);\n```\n\n----------------------------------------\n\nTITLE: Importing and Creating Integer Range Source in Akka Streams\nDESCRIPTION: Shows how to import necessary classes and create a Source that emits integers within a specified range. The range is defined with a start value, end value, and step size.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/range.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport akka.stream.javadsl.Source;\n\nSource<Integer> source = Source.range(1, 100, 2);\n```\n\n----------------------------------------\n\nTITLE: Journal TCK Test Implementation - Java\nDESCRIPTION: Example of implementing the Journal Technology Compatibility Kit tests in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-journals.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nclass MyJournalTest extends JavaJournalSpec {\n\n  public MyJournalTest() {\n    super(ConfigFactory.parseString(\n      \"\"\"{\n        akka.persistence.journal.plugin = \"my-journal\"\n        my-journal {\n          class = \"docs.persistence.MyJournal\"\n          plugin-dispatcher = \"akka.actor.default-dispatcher\"\n        }\n      }\"\"\"));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository (SBT, Maven, Gradle)\nDESCRIPTION: Defines the repository details required to fetch Akka dependencies. This configuration specifies the ID, name, and URL for the Akka library repository, compatible with SBT, Maven, and Gradle build tools.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\nLANGUAGE: maven\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\nLANGUAGE: gradle\nCODE:\n```\nrepositories {\n  maven {\n    url = \"https://repo.akka.io/maven\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Settings Extension in an Actor in Java\nDESCRIPTION: Shows how to access application settings from within a Java actor using the Settings extension. This provides a clean way to access configuration in actor code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\npublic class MyActor extends AbstractActor {\n  final SettingsImpl settings = Settings.get(getContext().getSystem());\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchAny(\n            message -> {\n              final String greeting = settings.greeting;\n              final int threads = settings.threads;\n              getSender().tell(greeting + \": using \" + threads + \" threads\", getSelf());\n            })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Cluster Dependency\nDESCRIPTION: Dependency configuration for adding the Akka cluster library to your project using SBT, Maven, or Gradle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_3\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-cluster-typed_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Replicated Shopping Cart in Scala\nDESCRIPTION: Scala implementation of a shopping cart using CRDT Counter to track product quantities. The cart maintains eventual consistency across replicas by using Counter CRDT for add/remove operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing-cart.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n@snip [ShoppingCartExample](/akka-persistence-typed-tests/src/test/scala/docs/akka/persistence/typed/ReplicatedShoppingCartExampleSpec.scala) { #shopping-cart }\n```\n\n----------------------------------------\n\nTITLE: Defining Actor Protocol for UnfoldAsync in Scala\nDESCRIPTION: Defines the protocol messages for communicating with an actor that provides chunks of bytes from an offset.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nsealed trait Protocol\nfinal case class GetChunk(offset: Long) extends Protocol\nfinal case class Chunk(bytes: ByteString) extends Protocol\n```\n\n----------------------------------------\n\nTITLE: Using TestProbe as Parent in Akka Actor Tests - Scala\nDESCRIPTION: This code shows how to create a child actor with a TestProbe as its parent in Scala Akka tests. This allows the probe to intercept messages sent to context.parent for verification. Akka TestKit is required for this pattern.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ParentChildSpec.scala](/akka-docs/src/test/scala/docs/testkit/ParentChildSpec.scala) { #test-TestProbe-parent }\n```\n\n----------------------------------------\n\nTITLE: Using Default Persistence Plugins in Java Actor\nDESCRIPTION: Example of a Java persistent actor that uses the default journal and snapshot plugins by only overriding the persistenceId method. This actor will use the plugins configured in reference.conf.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_42\n\nLANGUAGE: Java\nCODE:\n```\nclass DefaultPluginsActor extends AbstractPersistentActor {\n  private final String id;\n\n  public DefaultPluginsActor(String id) {\n    this.id = id;\n  }\n\n  @Override\n  public String persistenceId() {\n    return id;\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data with splitWhen in Java\nDESCRIPTION: Shows how to split a stream of time series data into substreams for each second using splitWhen in Java. Implements timestamp comparison logic to determine stream splitting points.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/splitWhen.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n@snip [SourceOrFlow.java](/akka-docs/src/test/java/jdocs/stream/operators/sourceorflow/Split.java) { #splitWhen }\n```\n\n----------------------------------------\n\nTITLE: Retry Pattern Usage in Scala\nDESCRIPTION: Example demonstrating the retry() pattern in Scala to retry a Future operation multiple times with delay between attempts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/futures.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [FutureDocSpec.scala](/akka-docs/src/test/scala/docs/future/FutureDocSpec.scala) { #retry }\n```\n\n----------------------------------------\n\nTITLE: Integration Testing Device Group Query\nDESCRIPTION: Integration test verifying the complete query functionality within the device group context.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n#group-query-integration-test\n```\n\nLANGUAGE: Java\nCODE:\n```\n#group-query-integration-test\n```\n\n----------------------------------------\n\nTITLE: Implementing Remote Router Deployment in Java\nDESCRIPTION: Java code for creating a cluster-aware router that deploys routee actors on remote nodes with the \"compute\" role, with a maximum of 3 instances per node.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nActorRef workerRouter = getContext().actorOf(\n  new ClusterRouterPool(new ConsistentHashingPool(0),\n    new ClusterRouterPoolSettings(100, 3, false, \"compute\"))\n      .props(Props.create(StatsWorker.class)), \"workerRouter2\");\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor Classification in Akka EventBus\nDESCRIPTION: Shows implementation of actor-based classification for EventBus, specifically designed for DeathWatch functionality. Requires ActorSystem for managing actor lifecycle and automatic unsubscription of terminated actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/event-stream.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [EventBusDocSpec.scala](/akka-docs/src/test/scala/docs/event/EventBusDocSpec.scala) { #actor-bus }\n```\n\nLANGUAGE: java\nCODE:\n```\n@@snip [EventBusDocTest.java](/akka-docs/src/test/java/jdocs/event/EventBusDocTest.java) { #actor-bus }\n```\n\n----------------------------------------\n\nTITLE: Transforming Author Handles to Email Addresses with mapAsync (Scala)\nDESCRIPTION: Uses `mapAsync` on the `authors` stream to asynchronously look up the email address for each author's handle using the `lookupEmail` function. It allows up to 4 lookups to run concurrently while preserving the original order. Results are filtered to keep only found email addresses.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nval emailAddresses: Source[String, NotUsed] =\n  authors.mapAsync(4)(author => lookupEmail(author.handle)).collect { case Some(emailAddress) => emailAddress }\n```\n\n----------------------------------------\n\nTITLE: Logging Reachable Node in Akka Cluster\nDESCRIPTION: This snippet shows the log message generated when a previously unreachable node becomes reachable again in the Akka Cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/failure-detector.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n```\nMarking node(s) as REACHABLE\n```\n```\n\n----------------------------------------\n\nTITLE: Cluster Metrics Extension Reference Configuration\nDESCRIPTION: The reference configuration for Cluster Metrics Extension showing all available configuration properties and their default values.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_12\n\nLANGUAGE: hocon\nCODE:\n```\n@@snip [reference.conf](/akka-cluster-metrics/src/main/resources/reference.conf)\n```\n\n----------------------------------------\n\nTITLE: Creating Index Links in Markdown\nDESCRIPTION: This snippet uses custom Markdown syntax to create an index of links to various documentation pages related to Akka Persistence and durable state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/index-persistence-durable-state.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n@@@ index\n\n* [persistence-durable-state](durable-state/persistence.md)\n* [persistence-style](durable-state/persistence-style.md)\n* [cqrs](durable-state/cqrs.md)\n* [persistence-query](../durable-state/persistence-query.md)\n* [state-store-plugin](../durable-state/state-store-plugin.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Running Seed Nodes in Separate JVMs\nDESCRIPTION: These commands illustrate how to start Akka Cluster nodes in separate JVM processes. Each process listens on a different port and joins as a seed node in the cluster, with ports dynamically or manually assigned.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-java/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nmvn -pl killrweather exec:java -Dexec.args=\"2553\"\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmvn -pl killrweather exec:java -Dexec.args=\"2554\"\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmvn -pl killrweather exec:java -Dexec.args=\"0\"\n```\n\n----------------------------------------\n\nTITLE: Using Configuration-Defined Custom Router in Akka\nDESCRIPTION: Demonstrates how to create an instance of the custom router defined in configuration. This approach uses the FromConfig factory method to instantiate the router.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_47\n\nLANGUAGE: Scala\nCODE:\n```\nval router = system.actorOf(FromConfig.props(), \"redundancy2\")\n```\n\nLANGUAGE: Java\nCODE:\n```\nActorRef router2 = system.actorOf(FromConfig.getInstance().props(), \"redundancy2\");\n```\n\n----------------------------------------\n\nTITLE: Setting Persistence ID in Java\nDESCRIPTION: Example of implementing the persistenceId method in a Java AbstractPersistentActor. The persistenceId must be unique to a given entity in the journal and remain consistent across actor incarnations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic String persistenceId() {\n  return \"abc\";\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring BalancingPool Dispatcher via HOCON\nDESCRIPTION: Shows how to configure the underlying dispatcher used by a BalancingPool router via the 'pool-dispatcher' section within the router's deployment configuration in HOCON. This allows fine-tuning of the dispatcher behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_24\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-balancing-pool2\nakka.actor.deployment {\n  /parent/router1 {\n    router = balancing-pool\n    nr-of-instances = 5\n    pool-dispatcher {\n      # dispatcher configuration ...\n    }\n  }\n}\n//#config-balancing-pool2\n```\n\n----------------------------------------\n\nTITLE: Defining Reply-Enabled Commands for Akka DurableStateBehavior (Java)\nDESCRIPTION: Shows how to define commands in Java that carry an ActorRef for replying in Akka DurableStateBehavior. Requires Akka Typed Java classes, and appropriate message interfaces and serialization. The pattern enables sending a reply back to the sender of the command, often used in distributed and persistent actor systems for reliability.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_26\n\nLANGUAGE: java\nCODE:\n```\n@@snip [AccountExampleWithNullDurableState.java](/akka-cluster-sharding-typed/src/test/java/jdocs/akka/cluster/sharding/typed/AccountExampleWithNullDurableState.java) { #reply-command }\n```\n\n----------------------------------------\n\nTITLE: Metrics Listener Implementation in Scala\nDESCRIPTION: Example Scala implementation of an actor that listens to metrics events directly, allowing custom functionality based on cluster metrics data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [MetricsListener.scala](/akka-docs/src/test/scala/docs/cluster/MetricsListener.scala) { #metrics-listener }\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Sources with zipN in Java\nDESCRIPTION: Java implementation showing how to combine three sources (characters, numbers, and colors) into a single Source where each element is a List containing one element from each source. The stream completes when any of the sources reaches its end.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/zipN.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<String> characters = Source.from(Arrays.asList(\"a\", \"b\", \"c\"));\nSource<Integer> numbers = Source.from(Arrays.asList(1, 2, 3));\nSource<String> colors = Source.from(Arrays.asList(\"red\", \"green\", \"blue\"));\n\nList<Source<Object, NotUsed>> sources = Arrays.asList(\n    characters.map(x -> (Object) x),\n    numbers.map(x -> (Object) x),\n    colors.map(x -> (Object) x)\n);\n\nSource<List<Object>, NotUsed> zipped = Source.zipN(sources);\n\nzipped.runWith(Sink.foreach(System.out::println), system);\n// prints:\n// [a, 1, red]\n// [b, 2, green]\n// [c, 3, blue]\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Distributed Data Changes in Scala\nDESCRIPTION: Example of subscribing to changes in distributed data using Akka's Replicator. It demonstrates how to register interest in change notifications and handle updates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\nval subscriber = system.actorOf(Props[Subscriber])\\n\\nreplicator ! Subscribe(dataKey, subscriber)\n```\n\n----------------------------------------\n\nTITLE: Implementing Command Handler for Blog Post Entity (Java)\nDESCRIPTION: Implements the command handler for a blog post entity using a CommandHandlerBuilder in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nprivate CommandHandlerBuilder<Command, Event, State> commandHandler() {\n  return newCommandHandlerBuilder()\n      .forStateType(EmptyState.class)\n      .onCommand(AddPost.class, this::onAddPost)\n      .forStateType(DraftState.class)\n      .onCommand(GetPost.class, this::onGetPost)\n      .onCommand(ChangeBody.class, this::onChangeBody)\n      .onCommand(Publish.class, this::onPublish)\n      .forStateType(PublishedState.class)\n      .onCommand(GetPost.class, this::onGetPost)\n      .onCommand(AddComment.class, this::onAddComment)\n      .orElse(newCommandHandlerBuilder().onAnyCommand(() -> Effect().unhandled()));\n}\n\nprivate Effect<Event, State> onAddPost(EmptyState state, AddPost command) {\n  return Effect().persist(new PostAdded(command.content));\n}\n\nprivate Effect<Event, State> onGetPost(DraftState state, GetPost command) {\n  command.replyTo.tell(state.content);\n  return Effect().none();\n}\n\nprivate Effect<Event, State> onChangeBody(DraftState state, ChangeBody command) {\n  return Effect().persist(new BodyChanged(command.newBody));\n}\n\nprivate Effect<Event, State> onPublish(DraftState state, Publish command) {\n  return Effect().persist(PostPublished.INSTANCE);\n}\n\nprivate Effect<Event, State> onGetPost(PublishedState state, GetPost command) {\n  command.replyTo.tell(state.content);\n  return Effect().none();\n}\n\nprivate Effect<Event, State> onAddComment(PublishedState state, AddComment command) {\n  return Effect().persist(new CommentAdded(command.user, command.comment));\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Asynchronous Pipelining in Akka Streams (Scala)\nDESCRIPTION: This Scala code demonstrates how using `.async` creates asynchronous boundaries between stream operators (A, B, C). This allows subsequent operators to start processing the next element as soon as the previous one is handed off, enabling pipelining and potentially increasing throughput compared to fully synchronous execution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 3)\n  .map { i => println(s\"A: $i\"); i }.async\n  .map { i => println(s\"B: $i\"); i }.async\n  .map { i => println(s\"C: $i\"); i }.async\n  .runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Creating DurableStateBehavior Instance (Java)\nDESCRIPTION: Shows the complete class definition for the counter actor in Java, extending `DurableStateBehavior`. It includes the `Command` and `State` definitions, the constructor taking `PersistenceId`, and the implementations for `emptyState()` and `commandHandler()`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.typed.ActorRef;\nimport akka.actor.typed.Behavior;\nimport akka.persistence.typed.PersistenceId;\nimport akka.persistence.typed.javadsl.CommandHandler;\nimport akka.persistence.typed.javadsl.DurableStateBehavior;\nimport akka.persistence.typed.javadsl.Effect;\n\npublic class CounterBehavior extends DurableStateBehavior<CounterBehavior.Command, CounterBehavior.State> {\n\n  // Command interface and classes (Increment, IncrementBy, GetValue)\n  interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class IncrementBy implements Command {\n    public final int value;\n\n    public IncrementBy(int value) {\n      this.value = value;\n    }\n  }\n\n  public static class GetValue implements Command {\n    public final ActorRef<State> replyTo;\n\n    public GetValue(ActorRef<State> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  // State class\n  public static class State {\n    public final int value;\n\n    public State(int value) {\n      this.value = value;\n    }\n\n    State increment(int amount) {\n      return new State(value + amount);\n    }\n  }\n\n  // Factory and Constructor\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return new CounterBehavior(persistenceId);\n  }\n\n  private CounterBehavior(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  // Initial state\n  @Override\n  public State emptyState() {\n    return new State(0);\n  }\n\n  // Command handler\n  @Override\n  public CommandHandler<Command, State> commandHandler() {\n\n    return newCommandHandlerBuilder()\n        .forAnyState()\n        .onCommand(\n            Increment.class,\n            (state, command) -> {\n              return Effect().persist(state.increment(1));\n            })\n        .onCommand(\n            IncrementBy.class,\n            (state, command) -> {\n              return Effect().persist(state.increment(command.value));\n            })\n        .onCommand(\n            GetValue.class,\n            (state, command) -> {\n              // Reply with current state\n              return Effect().none().thenReply(command.replyTo, s -> s);\n            })\n        .build();\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining IotSupervisor Actor in Java\nDESCRIPTION: Creates the IotSupervisor actor in Java, representing the top-level component of the IoT system. It uses Akka's built-in logging facility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_2.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class IotSupervisor {\n  public static Behavior<Void> create() {\n    return Behaviors.setup(context -> {\n      context.getLog().info(\"IoT Application started\");\n      return Behaviors.empty();\n    });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating a Cycle with Materialized Value Feedback in Akka Streams (Java)\nDESCRIPTION: This Java snippet illustrates how feeding the materialized value, such as a CompletionStage from a Sink.fold, back into its own computation in a GraphDsl is an unsafe pattern, leading to a cycle. The snippet serves as an example of what to avoid in Akka Streams graph composition. Dependencies: akka.stream.javadsl.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nfinal Outlet<CompletionStage<Integer>> matValue = builder.materializedValue();\n// Use matValue as input, creating an unsafe feedback cycle\n\n```\n\n----------------------------------------\n\nTITLE: Watching ActorRef Termination with Source/Flow in Akka Streams (Java)\nDESCRIPTION: This example displays how to apply the 'watch' operator to a Source or Flow in Akka Streams using Java, allowing monitoring of an ActorRef's termination. If the actor ends, the stream fails with a WatchedActorTerminatedException. Dependencies are Akka Streams and a specific ActorRef. Main parameter: the ActorRef instance. Input/output is as per the source or flow, but will terminate with exception when the watched actor stops.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/watch.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n\"\"\"@@snip [SourceOrFlow.java](/akka-docs/src/test/java/jdocs/stream/operators/SourceOrFlow.java) { #watch }\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Global Expiry for All Entries in Akka Distributed Data\nDESCRIPTION: Sets a global expiry time for all distributed data entries within an Akka cluster. The configuration specifies the time limit after which any inactive entry, regardless of its specific key, will be removed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_9\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.cluster.distributed-data.expire-keys-after-inactivity {\n  \"*\" = 10 minutes\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing PriorityMailbox in Scala\nDESCRIPTION: Scala implementation of a priority mailbox that processes messages based on their priority. Messages with higher priority (lower numeric value) are processed first, enabling time-critical message handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.dispatch.{PriorityGenerator, UnboundedPriorityMailbox}\nimport com.typesafe.config.Config\n\n// We inherit, in this case, from UnboundedPriorityMailbox\n// and seed it with the priority generator\nclass MyPrioMailbox(settings: ActorSystem.Settings, config: Config)\n    extends UnboundedPriorityMailbox(\n      // Create a new PriorityGenerator, lower prio means more important\n      PriorityGenerator {\n        // 'highpriority messages should be treated first if possible\n        case 'highpriority => 0\n\n        // 'lowpriority messages should be treated last if possible\n        case 'lowpriority => 2\n\n        // PoisonPill when no other left\n        case PoisonPill    => 3\n\n        // We default to 1, which is in between high and low\n        case otherwise     => 1\n      })\n```\n\n----------------------------------------\n\nTITLE: Declaring akka-stream-typed Dependency for sbt, Maven, and Gradle - Build Definition\nDESCRIPTION: This snippet demonstrates how to declare the dependency for akka-stream-typed using Akka's BOM in sbt, Maven, and Gradle. It outlines the necessary group ID, artifact ID, and version, which should be replaced with your project's Scala/VERSION variables as required. Including this dependency enables access to the ActorSource APIs in your code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorSource/actorRef.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nbomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\\nsymbol1=AkkaVersion\\nvalue1=\\\"$akka.version$\\\"\\ngroup=\\\"com.typesafe.akka\\\"\\nartifact=\\\"akka-stream-typed_$scala.binary.version$\\\"\\nversion=AkkaVersion\n```\n\n----------------------------------------\n\nTITLE: Hot-swapping Actor Behavior in Scala\nDESCRIPTION: Shows how to dynamically change actor behavior using context.become(), implementing a state machine pattern.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_41\n\nLANGUAGE: scala\nCODE:\n```\nclass HotSwapActor extends Actor {\n  override def receive = {\n    case \"hello\" => context.become({\n      case \"world\" => // handle world\n      case \"foo\" => context.unbecome()\n    }, discardOld = true)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Singleton Lease in Scala\nDESCRIPTION: Example of loading lease configuration for an Akka Cluster Singleton from application configuration in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nval leaseConfig = system.settings.config.getConfig(\"pekko.cluster.singleton.singleton-lease\")\nval singletonSettings = ClusterSingletonSettings(system)\n  .withLeaseSettings(Option(LeaseUsageSettings(leaseConfig)))\n```\n\n----------------------------------------\n\nTITLE: Manual Construction of CircuitBreaker (Scala)\nDESCRIPTION: This snippet demonstrates how to manually create a CircuitBreaker instance in Scala with custom parameters instead of looking it up by name from configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nval circuitBreaker = new CircuitBreaker(\n  system.scheduler,\n  maxFailures = 5,\n  callTimeout = 10.seconds,\n  resetTimeout = 1.minute)\n```\n\n----------------------------------------\n\nTITLE: Defining HelloWorld Actor in Akka Typed (Scala)\nDESCRIPTION: Example of an Akka Typed actor extending the AbstractBehavior class to create a simple HelloWorld actor in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/from-classic.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nobject HelloWorld {\n  def apply(): Behavior[String] =\n    Behaviors.setup(context => new HelloWorld(context))\n}\n\nclass HelloWorld(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n\n  override def onMessage(msg: String): Behavior[String] =\n    msg match {\n      case \"hello\" =>\n        println(\"Hello World!\")\n        this\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Avoiding Shared Mutable State in Java Actors\nDESCRIPTION: A Java example showing incorrect patterns for handling mutable state in Akka actors. This demonstrates practices to avoid when working with actor state that could be accessed by multiple threads.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/jmm.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#mutable-state\n```\n\n----------------------------------------\n\nTITLE: Effects and Side Effects Implementation in Scala\nDESCRIPTION: Demonstrates how to implement effects and side effects in a DurableStateBehavior using Scala. Shows handling of IncrementWithConfirmation command with persistence and reply effects.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\n#effects\n```\n\n----------------------------------------\n\nTITLE: Using TestFSMRef for Testing Finite State Machines in Scala\nDESCRIPTION: Illustrates how to use TestFSMRef to test Finite State Machine actors, allowing access to internal state and transitions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_38\n\nLANGUAGE: scala\nCODE:\n```\nval fsm = TestFSMRef(new MyFSM)\nfsmActor.stateName // current state name\nfsmActor.stateData // current state data\nfsm.setState(stateName, stateData) // set state\nfsmActor.setTimer(name, msg, timeout, repeat) // set timer\nfsmActor.cancelTimer(name) // cancel timer\nfsmActor.isTimerActive(name) // check if timer is active\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Actor Dependencies\nDESCRIPTION: Dependency configuration block for including Akka Actor and TestKit modules in your project. Uses version placeholders and supports multiple build tools.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-actors.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=\"com.typesafe.akka\"\n  artifact=\"akka-actor_$scala.binary.version$\"\n  version=AkkaVersion\n  group2=\"com.typesafe.akka\"\n  artifact2=\"akka-testkit_$scala.binary.version$\"\n  scope2=test\n  version2=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Operator Immutability in Akka Streams (Scala)\nDESCRIPTION: Highlights the immutability of Akka Streams operators. Connecting an operator (like adding a `map` transformation to a `Source`) returns a *new* Source instance, leaving the original unchanged. It emphasizes the need to assign the result of the connection to a variable or use it directly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #source-immutable }\n```\n\n----------------------------------------\n\nTITLE: Using mergeSorted Operator in Scala\nDESCRIPTION: Example of using the mergeSorted operator in Scala to merge multiple sorted sources and emit elements in order.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergeSorted.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval sourceA = Source(List(1, 3, 5, 7))\nval sourceB = Source(List(2, 4, 6, 8))\n\nval merged = sourceA.mergeSorted(sourceB)\n\nmerged.runWith(Sink.foreach(println))\n// prints 1, 2, 3, 4, 5, 6, 7, 8\n```\n\n----------------------------------------\n\nTITLE: Creating Deflate Compression Flow - Akka Streams - Java\nDESCRIPTION: This snippet uses the Java API to create an Akka Streams Flow that performs deflate compression on a stream of akka.util.ByteString instances. Each output chunk is flushed after processing, ensuring independent decompression capability. Requires Akka Streams and ByteString support. The flow accepts and emits ByteStrings and is well-suited for reactive streams pipeline tasks where chunked compression is needed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Compression/deflate.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nCompression.deflate()\n```\n\n----------------------------------------\n\nTITLE: Event Adapter for Skipping Deleted Events in Akka Persistence (Java)\nDESCRIPTION: Java implementation of an EventAdapter that handles EventDeserializationSkipped objects, emitting an empty EventSeq for removed event types in Akka Persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\npublic class RemovedEventsAwareAdapter implements EventAdapter {\n    @Override\n    public EventSeq fromJournal(Object event, String manifest) {\n        if (event instanceof EventDeserializationSkipped) {\n            return EventSeq.empty();\n        } else {\n            return EventSeq.single(event);\n        }\n    }\n\n    @Override\n    public String manifest(Object event) {\n        return \"\";\n    }\n\n    @Override\n    public Object toJournal(Object event) {\n        return event;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Heartbeat Interval Warning in Akka Cluster\nDESCRIPTION: This snippet displays the warning log message when the heartbeat arrival interval exceeds 2/3 of the acceptable-heartbeat-pause in the Akka Cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/failure-detector.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n```\nheartbeat interval is growing too large\n```\n```\n\n----------------------------------------\n\nTITLE: Custom Persistence Plugin Configuration\nDESCRIPTION: Configuration example for creating custom persistence plugin entries. This shows how to configure LevelDB journal and local snapshot store with custom settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_45\n\nLANGUAGE: Scala\nCODE:\n```\nakka.persistence.journal.leveldb {\n  dir = \"/some/other/dir\"\n  native = on\n}\n\nakka.persistence.snapshot-store.local {\n  dir = \"/some/other/dir\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining IotSupervisor Actor in Scala\nDESCRIPTION: Creates the IotSupervisor actor which represents the top-level component of the IoT system. It uses Akka's built-in logging facility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_2.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nobject IotSupervisor {\n  def apply(): Behavior[Nothing] =\n    Behaviors.setup[Nothing] { context =>\n      context.log.info(\"IoT Application started\")\n      Behaviors.empty\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Java divertTo API Signature\nDESCRIPTION: API signature for divertTo operator in Java for Source and Flow types. Takes a sink graph and predicate function to determine where elements should be diverted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/divertTo.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource#divertTo(akka.stream.Graph,akka.japi.function.Predicate)\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow#divertTo(akka.stream.Graph,akka.japi.function.Predicate)\n```\n\n----------------------------------------\n\nTITLE: Loading Extensions from Configuration\nDESCRIPTION: Configuration example showing how to auto-load extensions when the ActorSystem starts. Extensions are specified by their fully qualified class names in the akka.extensions section.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\nakka {\n  extensions = [\"docs.extension.Counter\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Replicated GCounter Actor with Akka Distributed Data in Java\nDESCRIPTION: Presents a Java actor example demonstrating the use of `Replicator.Update` and `Replicator.Get` messages with a `GCounter` CRDT via a `ReplicatorMessageAdapter`. This actor implements a distributed counter, handling `Increment` and `GetValue` commands. Requires `SelfUniqueAddress`. The code is referenced from `ReplicatorDocSample.java`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ReplicatorTest.java](/akka-cluster-typed/src/test/java/jdocs/akka/cluster/ddata/typed/javadsl/ReplicatorDocSample.java) { #sample }\n```\n\n----------------------------------------\n\nTITLE: Getting the ClusterSharding Extension in Scala\nDESCRIPTION: Demonstrates how to access the ClusterSharding extension within a Scala Akka Typed application using the ActorSystem.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.typed.ActorSystem\nimport akka.cluster.sharding.typed.scaladsl.ClusterSharding\n\nval system: ActorSystem[_] = ???\n// #sharding-extension\nval sharding = ClusterSharding(system)\n// #sharding-extension\n```\n\n----------------------------------------\n\nTITLE: Defining Stats Service Messages in Java\nDESCRIPTION: Java classes defining the messages used in the statistics service example, including requests, worker results, and response messages with ConsistentHashable implementation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\ninterface StatsMessages {\n\n  public static final class StatsJob implements ConsistentHashable, Serializable {\n    private final String text;\n\n    public StatsJob(String text) {\n      this.text = text;\n    }\n\n    public String getText() {\n      return text;\n    }\n\n    @Override\n    public Object consistentHashKey() {\n      return text;\n    }\n  }\n\n  public static final class StatsResult implements Serializable {\n    private final double meanWordLength;\n\n    public StatsResult(double meanWordLength) {\n      this.meanWordLength = meanWordLength;\n    }\n\n    public double getMeanWordLength() {\n      return meanWordLength;\n    }\n\n    @Override\n    public String toString() {\n      return String.format(\"Mean word length: %.2f\", meanWordLength);\n    }\n  }\n\n  public static final class JobFailed implements Serializable {\n    private final String reason;\n\n    public JobFailed(String reason) {\n      this.reason = reason;\n    }\n\n    public String getReason() {\n      return reason;\n    }\n\n    @Override\n    public String toString() {\n      return String.format(\"Job failed: %s\", reason);\n    }\n  }\n\n  public static final class ProcessText implements Serializable {\n    private final String text;\n\n    public ProcessText(String text) {\n      this.text = text;\n    }\n\n    public String getText() {\n      return text;\n    }\n  }\n\n  public static final class ProcessWord implements Serializable {\n    private final String word;\n\n    public ProcessWord(String word) {\n      this.word = word;\n    }\n\n    public String getWord() {\n      return word;\n    }\n  }\n\n  public static final class WordCount implements Serializable {\n    private final String word;\n    private final int count;\n\n    public WordCount(String word, int count) {\n      this.word = word;\n      this.count = count;\n    }\n\n    public String getWord() {\n      return word;\n    }\n\n    public int getCount() {\n      return count;\n    }\n  }\n\n  public static final Object Start = new Object() {\n    @Override\n    public String toString() {\n      return \"Start\";\n    }\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Stopped Device Actors\nDESCRIPTION: Test case handling scenarios where device actors stop before providing a response.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n#query-test-stopped\n```\n\nLANGUAGE: Java\nCODE:\n```\n#query-test-stopped\n```\n\n----------------------------------------\n\nTITLE: Using Settings Extension in an Actor in Scala\nDESCRIPTION: Demonstrates how to access application settings from within an actor using the Settings extension. This provides a clean way to access configuration in actor code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\nclass MyActor extends Actor {\n  val settings = Settings(context.system)\n  \n  def receive = {\n    case someMessage =>\n      val greeting = settings.Greeting\n      val threads = settings.Threads\n      sender() ! s\"$greeting: using $threads threads\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Histogram Aggregation with Fold in Scala\nDESCRIPTION: This snippet demonstrates how to use the fold operator in Scala to create a histogram from incoming stream values. It initializes an empty Map and updates it with each incoming value, incrementing the count for each unique value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/fold.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.stream.scaladsl._\n\nSource(1 to 10)\n  .fold(Map.empty[Int, Int]) { (map, next) =>\n    map + (next -> (map.getOrElse(next, 0) + 1))\n  }\n  .runWith(Sink.head)\n```\n\n----------------------------------------\n\nTITLE: Prepending a Source to a Flow in Java\nDESCRIPTION: Example of using the prepend operator to combine two sources in Java. It demonstrates prepending a source of strings to another source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/prepend.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource<String> sourceToPrepend = Source.from(Arrays.asList(\"A\", \"B\", \"C\"));\nSource<String> sourceToAppendTo = Source.from(Arrays.asList(\"1\", \"2\", \"3\"));\nSource<String> resultSource = sourceToAppendTo.prepend(sourceToPrepend);\nCompletionStage<List<String>> result = resultSource.runWith(Sink.seq(), system);\n// result will be completed with the list [\"A\", \"B\", \"C\", \"1\", \"2\", \"3\"]\n```\n\n----------------------------------------\n\nTITLE: Testing Timing Assertions with TestKit\nDESCRIPTION: Shows how to test timing-sensitive behavior using the within block to specify time constraints.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\n#test-within\n```\n\n----------------------------------------\n\nTITLE: TCP Client Connection - Scala\nDESCRIPTION: Implementation of a TCP client that connects to a remote address and handles the connection lifecycle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nclass Client(remote: InetSocketAddress) extends Actor {\n\n  import Tcp._\n  import context.system\n\n  IO(Tcp) ! Connect(remote)\n\n  def receive = {\n    case CommandFailed(_: Connect) =>\n      listener ! \"connect failed\"\n      context.stop(self)\n\n    case c @ Connected(remote, local) =>\n      listener ! c\n      val connection = sender()\n      connection ! Register(self)\n      context.become {\n        case data: ByteString =>\n          connection ! Write(data)\n        case CommandFailed(w: Write) =>\n          // O/S buffer was full\n          listener ! \"write failed\"\n        case Received(data) =>\n          listener ! data\n        case \"close\" =>\n          connection ! Close\n        case _: ConnectionClosed =>\n          context.stop(self)\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Configuration Compatibility Checker\nDESCRIPTION: Configuration for adding custom configuration compatibility checkers to verify cluster node configurations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_13\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.cluster.configuration-compatibility-check.checkers {\n  my-custom-config = \"com.company.MyCustomJoinConfigCompatChecker\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Service Lookup in Java\nDESCRIPTION: Basic example of looking up a service by name in Java. This performs a simple lookup with just the service name and returns a CompletionStage with the resolved targets.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nCompletionStage<Resolved> lookup =\n  discovery.lookup(Lookup.create(\"service-name\"), resolveTimeout);\n```\n\n----------------------------------------\n\nTITLE: Using logWithMarker in Scala Akka Streams\nDESCRIPTION: Example showing how to use logWithMarker operator in Scala to log stream elements with custom markers. The operator logs elements, completion, and error signals with configurable log levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/logWithMarker.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n#logWithMarker\n```\n\n----------------------------------------\n\nTITLE: Manually Starting and Managing Topic Actor - Akka Typed - Java\nDESCRIPTION: Shows how to manually start and supervise a distributed pub/sub topic actor in Akka Typed (Java API) using Topic.create. Inputs are the topic name and message class. Allows for multiple topic actors with the same name on the same node, each with its own local subscribers. The behavior for the topic actor is returned and needs to be started as an actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nBehavior<Message> manualTopic = \n  Topic.create(\"manual-topic\", Message.class);\n```\n\n----------------------------------------\n\nTITLE: Interactive SBT Session Example\nDESCRIPTION: Example showing interactive sbt shell usage for building and testing Akka.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n% sbt\n[info] Set current project to default (in build file:/.../akka/project/plugins/)\n[info] Set current project to akka (in build file:/.../akka/)\n> compile\n...\n> test\n...\n```\n\n----------------------------------------\n\nTITLE: Defining a Parent Actor for Routers (Scala/Java)\nDESCRIPTION: Defines a parent actor used as a context for creating router actors in the examples. Router deployment paths in configuration often start with '/parent/' reflecting this structure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\n//#create-parent\nclass Parent extends Actor {\n  // definitions depend on the example\n  override def supervisorStrategy = SupervisorStrategy.stoppingStrategy\n\n  def receive = {\n    case \"stop\" => context.stop(self)\n    case _      =>\n  }\n}\n//#create-parent\n```\n\nLANGUAGE: java\nCODE:\n```\n//#create-parent\nstatic public class Parent extends AbstractActor {\n  // definitions depend on the example\n\n  private static SupervisorStrategy strategy =\n          new OneForOneStrategy(DeciderBuilder\n                  // last param is unused when apply() is overridden\n                  .matchAny(o -> SupervisorStrategy.stop())\n                  .build());\n\n  @Override\n  public SupervisorStrategy supervisorStrategy() {\n      return strategy;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n            .matchEquals(\"stop\", s -> getContext().stop(getSelf()))\n            .build();\n  }\n\n}\n//#create-parent\n```\n\n----------------------------------------\n\nTITLE: Implementing Compression for TwoPhaseSet Serialization in Java\nDESCRIPTION: Java example showing how to add Gzip compression to a serializer using the AbstractSerializationSupport interface, which can reduce data size for network transmission.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_34\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic byte[] toBinary(Object o) {\n  if (o instanceof TwoPhaseSet) {\n    TwoPhaseSetMessages.TwoPhaseSet msg = twoPhaseSetToProto((TwoPhaseSet) o);\n    return compress(msg.toByteArray());\n  } else\n    throw new IllegalArgumentException(\"Can't serialize object of type \" + o.getClass());\n}\n\n@Override\npublic Object fromBinary(byte[] bytes, String manifest) {\n  if (TWOPHASESET_MANIFEST.equals(manifest))\n    return twoPhaseSetFromBinary(uncompress(bytes));\n  else\n    throw new IllegalArgumentException(\"Unknown manifest: \" + manifest);\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Adaptive Load Balancing Router in Scala Code\nDESCRIPTION: Scala code that programmatically defines the same type of router that was shown in the configuration example. This demonstrates how to create router lookup and deployment in code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [FactorialFrontend.scala](/akka-docs/src/test/scala/docs/cluster/FactorialFrontend.scala) { #router-lookup-in-code #router-deploy-in-code }\n```\n\n----------------------------------------\n\nTITLE: Date/Time Format Configuration\nDESCRIPTION: Configuration snippet demonstrating how to change date/time serialization format for improved performance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_28\n\nLANGUAGE: HOCON\nCODE:\n```\n@@snip [config](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/SerializationDocSpec.scala) { #date-time }\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Cluster Size Configuration\nDESCRIPTION: Configuration for specifying the minimum number of cluster members required before joining members are promoted to Up status.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_10\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.cluster.min-nr-of-members = 3\n```\n\n----------------------------------------\n\nTITLE: Using Log Operator in Scala Streams\nDESCRIPTION: Demonstrates how to use the log operator in Scala to monitor elements flowing through an Akka stream. The operator logs elements, completion, and error events with configurable log levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/log.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#log\n```\n\n----------------------------------------\n\nTITLE: Configuring Trusted Selection Paths in Akka Remote (HOCON)\nDESCRIPTION: This configuration allows specific actors to receive messages sent via actor selection in untrusted mode. It defines a list of trusted actor paths that are permitted to receive such messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_11\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery.trusted-selection-paths = [\"/user/receptionist\", \"/user/namingService\"]\n```\n\n----------------------------------------\n\nTITLE: Runtime Persistence Plugin Configuration in Java\nDESCRIPTION: Example of a Java persistent actor that provides plugin configurations at runtime. This approach allows dynamic configuration of persistence plugins instead of using static configuration files.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_47\n\nLANGUAGE: Java\nCODE:\n```\nclass RuntimePluginActor extends AbstractPersistentActor {\n  private final String id;\n\n  public RuntimePluginActor(String id) {\n    this.id = id;\n  }\n\n  @Override\n  public String persistenceId() {\n    return id;\n  }\n\n  @Override\n  public Config journalPluginConfig() {\n    return ConfigFactory.parseString(\n        \"dir = \\\"target/journal-runtime-conf\\\"\"\n            + \"\\nnative = off\")\n        .withFallback(context().system().settings().config().getConfig(journalPluginId()));\n  }\n\n  @Override\n  public Config snapshotPluginConfig() {\n    return ConfigFactory.parseString(\n        \"dir = \\\"target/snapshot-runtime-conf\\\"\")\n        .withFallback(context().system().settings().config().getConfig(snapshotPluginId()));\n  }\n\n  // ... rest of the actor implementation ...\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Saving Snapshots in Java\nDESCRIPTION: Shows snapshot saving implementation in Java with response handling\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_27\n\nLANGUAGE: java\nCODE:\n```\nprivate ExampleState state;\nprivate int numEvents;\n\n@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n      .matchEquals(\"print\", s -> /* ... */)\n      .match(SaveSnapshotSuccess.class, ss -> /* ... */)\n      .match(SaveSnapshotFailure.class, sf -> /* ... */)\n      .build();\n}\n\nprivate void checkSnapshot() {\n  if (numEvents % 100 == 0)\n    saveSnapshot(state);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring a ForkJoinPool-based Dispatcher in HOCON\nDESCRIPTION: HOCON configuration for a custom dispatcher using the fork-join-executor. Sets thread pool parameters including parallelism levels, thread name, and daemon status.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_2\n\nLANGUAGE: hocon\nCODE:\n```\nmy-dispatcher {\n  # Dispatcher is the name of the event-based dispatcher\n  type = Dispatcher\n  # What kind of ExecutionService to use\n  executor = \"fork-join-executor\"\n  # Configuration for the fork join pool\n  fork-join-executor {\n    # Min number of threads to cap factor-based parallelism number to\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads to cap factor-based parallelism number to\n    parallelism-max = 10\n  }\n  # Throughput defines the maximum number of messages to be\n  # processed per actor before the thread jumps to the next actor.\n  # Set to 1 for as fair as possible.\n  throughput = 100\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Adaptive Load Balancing Router in Java Code\nDESCRIPTION: Java code that programmatically defines the same type of router that was shown in the configuration example. This demonstrates how to create router lookup and deployment in code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n@@snip [FactorialFrontend.java](/akka-docs/src/test/java/jdocs/cluster/FactorialFrontend.java) { #router-lookup-in-code #router-deploy-in-code }\n```\n\n----------------------------------------\n\nTITLE: Run Source in Scala\nDESCRIPTION: This snippet in Scala demonstrates how to run a source by attaching a consumer function that processes streamed data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [QuickStartDocSpec.scala](/akka-docs/src/test/scala/docs/stream/QuickStartDocSpec.scala) { #run-source }\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Cluster Join Timeout and Coordinated Shutdown\nDESCRIPTION: Sets a timeout for unsuccessful join attempts to seed nodes and configures the system to exit the JVM after coordinated shutdown. This is useful when seed nodes are dynamically discovered.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_8\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.shutdown-after-unsuccessful-join-seed-nodes = 20s\nakka.coordinated-shutdown.exit-jvm = on\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Tell in Akka\nDESCRIPTION: Shows how to send messages using the tell (!) method in Akka, which is a fire-and-forget approach that doesn't block or wait for a response.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_25\n\nLANGUAGE: Scala\nCODE:\n```\nactor ! \"hello\"\nactor.!( \"hello\")(sender)\n```\n\nLANGUAGE: Java\nCODE:\n```\nactor.tell(\"hello\", ActorRef.noSender());\n```\n\n----------------------------------------\n\nTITLE: Declaring Akka Stream Typed Dependency with sbt, Maven, or Gradle - Dependency Management\nDESCRIPTION: Declares the required Akka Streams Typed module as a project dependency using variables for the Scala version and Akka version. The snippet covers dependency notation for sbt, Maven, and Gradle environments. Prerequisites include configuring the repository as shown in the previous snippet. The group, artifact, and version must match the intended release.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithStatusAndContext.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\\\"$akka.version$\\\"\n  group=\\\"com.typesafe.akka\\\"\n  artifact=\\\"akka-stream-typed_$scala.binary.version$\\\"\n  version=AkkaVersion\n}\n```\n\nLANGUAGE: Maven\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\\\"$akka.version$\\\"\n  group=\\\"com.typesafe.akka\\\"\n  artifact=\\\"akka-stream-typed_$scala.binary.version$\\\"\n  version=AkkaVersion\n}\n```\n\nLANGUAGE: Gradle\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\\\"$akka.version$\\\"\n  group=\\\"com.typesafe.akka\\\"\n  artifact=\\\"akka-stream-typed_$scala.binary.version$\\\"\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partial Graph with Multiple Inputs in Scala\nDESCRIPTION: Creates a partial graph with 3 inputs that picks the greatest integer value from each zipped triple using UniformFanInShape.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n#simple-partial-graph-dsl\n```\n\n----------------------------------------\n\nTITLE: Protobuf Schema Definition with Optional Field for FlightAppModels (protobuf)\nDESCRIPTION: Defines a Protobuf schema including an optional 'seatType' field for the 'SeatReserved' event model. This schema illustrates how to add a new field without breaking backward compatibility, as required for safe schema evolution in Akka Persistence environments. The numeric field identifiers must remain unchanged when evolving schema, and new fields use 'optional' or singular fields with appropriate types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_9\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage SeatReserved {\n  required string letter = 1;\n  required int32 row = 2;\n  optional string seatType = 3;\n}\n```\n\n----------------------------------------\n\nTITLE: Deferred Actions in Akka Persistence\nDESCRIPTION: Shows how to use defer and deferAsync to execute actions after persist handlers complete, without actually persisting new events. Useful for read operations and non-event actions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\ndef updated(user: User): Unit = {\n  persist(UserUpdated(user)) { _ =>\n    updateCounter()\n\n    defer(\"user-updated\") { _ =>\n      context.system.eventStream.publish(user)\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.repeat in Scala\nDESCRIPTION: Example showing how to use Source.repeat to emit the first 4 elements of a repeated value stream in Scala. The source continuously emits the same value and is limited using a take operator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/repeat.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#repeat\n```\n\n----------------------------------------\n\nTITLE: Common Chained Effects in Java\nDESCRIPTION: Demonstrates reusable side effects implementation in Java using thenRun for multiple commands.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\n#commonChainedEffects\n```\n\n----------------------------------------\n\nTITLE: Verifying Logging Events in Java\nDESCRIPTION: Java version of the LoggingTestKit example, showing how to verify specific logging events in Akka tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [LoggingDocExamples.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/LoggingDocExamples.java) { #test-logging }\n```\n\n----------------------------------------\n\nTITLE: Applying Delay to Flow in Akka Streams (Scala)\nDESCRIPTION: Applies a delay to elements in a Flow. The delay duration and overflow strategy are specified as parameters. This operator is part of the timer-driven operators in Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/delay.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nFlow.delay(of: scala.concurrent.duration.FiniteDuration, strategy: akka.stream.DelayOverflowStrategy): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Logging from Actor Context in Scala\nDESCRIPTION: Demonstrates how to log messages using the ActorContext in Scala. The logger is accessed via the context.log method, which automatically includes the actor's path in the log output.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass MyActor extends AbstractBehavior[String](context) {\n  override def onMessage(msg: String): Behavior[String] = {\n    context.log.debug(\"Received message {}\", msg)\n    this\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Messages in Static Classes in Java\nDESCRIPTION: Best practice for declaring message types that an actor can receive as static classes inside the Actor class for better code organization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\npublic class WatchActor extends AbstractActor {\n  // #messages-in-companion\n  // these are the messages our actor handles\n  static public class Watch {\n    public final ActorRef target;\n\n    public Watch(ActorRef target) {\n      this.target = target;\n    }\n  }\n\n  static public class Transition {\n    public final ActorRef target;\n\n    public Transition(ActorRef target) {\n      this.target = target;\n    }\n  }\n  // #messages-in-companion\n\n  public static Props props() {\n    return Props.create(WatchActor.class, WatchActor::new);\n  }\n\n  private final Map<String, ActorRef> watchList = new HashMap<>();\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Watch.class,\n            w -> {\n              ActorRef ref = w.target;\n              getContext().watch(ref);\n              watchList.put(ref.path().name(), ref);\n            })\n        .match(\n            Transition.class,\n            t -> {\n              ActorRef ref = t.target;\n              getContext().watch(ref);\n              watchList.put(ref.path().name(), ref);\n            })\n        .match(\n            Terminated.class,\n            t -> {\n              ActorRef ref = t.actor();\n              String name = ref.path().name();\n              if (watchList.get(name) != null) {\n                watchList.remove(name);\n              }\n            })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing with InitialStateAsEvents in Scala\nDESCRIPTION: Example of subscribing to cluster events using InitialStateAsEvents mode, which provides initial events corresponding to current state instead of a single CurrentClusterState object.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\ncluster.subscribe(self, initialStateMode = InitialStateAsEvents,\n  classOf[MemberEvent], classOf[UnreachableMember])\n```\n\n----------------------------------------\n\nTITLE: Creating Shared LevelDB Store Actor (Java - Deprecated)\nDESCRIPTION: Demonstrates the Java code needed to create an instance of the `SharedLeveldbStore` actor using `system.actorOf`. This actor serves as the central, shared LevelDB instance for the deprecated shared LevelDB journal plugin.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n// Assuming the snippet creates the SharedLeveldbStore actor\nimport akka.actor.ActorRef;\nimport akka.actor.ActorSystem;\nimport akka.actor.Props;\nimport akka.persistence.shared.SharedLeveldbStore;\n\nActorSystem system = ActorSystem.create();\nfinal ActorRef store = system.actorOf(Props.create(SharedLeveldbStore.class), \"store\");\n```\n\n----------------------------------------\n\nTITLE: Using Flag Data Type in Java\nDESCRIPTION: Example of using the Flag data type in Java. It shows how to create a Flag, switch it on, and check its state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nFlag flag = Flag.create();\nFlag f2 = flag.switchOn();\nboolean enabled = flag.enabled();\nboolean enabled2 = f2.enabled();\n```\n\n----------------------------------------\n\nTITLE: Case Object Serialization in Scala\nDESCRIPTION: Example of serializing case objects with custom serializer in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME)\n@JsonSubTypes(Array(\n  new JsonSubTypes.Type(value = classOf[Lion], name = \"lion\"),\n  new JsonSubTypes.Type(value = classOf[Elephant], name = \"elephant\"),\n  new JsonSubTypes.Type(value = classOf[UnicornAnimal], name = \"unicorn\")))\nsealed trait Animal extends JsonSerializable\n\ntrait UnicornAnimal extends Animal {\n  @JsonDeserialize(using = classOf[UnicornDeserializer])\n  def marker: Boolean\n}\n\n@JsonTypeName(\"unicorn\")\ncase object Unicorn extends UnicornAnimal {\n  override def marker = true\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Tick with ZipLatest in Java\nDESCRIPTION: Demonstrates combining Source.tick with zipLatest in Java to periodically update values in a stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/tick.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nSource<Pair<T, Response>, NotUsed> streamWithUpdatingValue(ActorRef<Query> actor) {\n  Source<Response, NotUsed> responseStream =\n      Source.tick(Duration.ZERO, Duration.ofSeconds(1), \"tick\")\n          .mapAsync(\n              1,\n              tick ->\n                  Patterns.ask(\n                          actor, Query::new, Duration.ofSeconds(3), system.scheduler())\n                      .thenApply(Response.class::cast));\n\n  return elements.zipLatest(responseStream);\n}\n```\n\n----------------------------------------\n\nTITLE: Binding Stream Lifecycle to ActorSystem in Java\nDESCRIPTION: In this Java example, a materializer is provided to an actor, linking the stream's lifecycle to the akka.actor.ActorSystem, which is advantageous for managing streams requiring prolonged execution beyond the actor's life.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #materializer-from-system-in-actor }\n```\n\n----------------------------------------\n\nTITLE: Merge Prioritized Sources in Java\nDESCRIPTION: Shows how to merge multiple sources with assigned priorities using Akka Streams in Java. The priorities determine the probability of each source being selected when multiple sources have available elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/mergePrioritizedN.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer> source = Source.merge(\n    Arrays.asList(source1, source2, source3),\n    Arrays.asList(1, 2, 3)\n);\n```\n\n----------------------------------------\n\nTITLE: Stopping Actors in Tests - Scala\nDESCRIPTION: Illustrates how to stop actors during tests in Scala with ActorTestKit, ensuring that actors are properly terminated and applying timeouts to avoid hanging tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AsyncTestingExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/AsyncTestingExampleSpec.scala) { #test-stop-actors }\n```\n\n----------------------------------------\n\nTITLE: Mutable State Handling with Lazy Flow in Java\nDESCRIPTION: Java implementation demonstrating safe mutable state handling using lazyFlow with an ArrayList.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/lazyFlow.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3))\n    .via(\n        Flow.lazyFlow(\n            () -> {\n              // safe to create a new mutable object here\n              ArrayList<Integer> list = new ArrayList<>();\n              return Flow.of(Integer.class)\n                  .fold(\n                      list,\n                      (l, element) -> {\n                        l.add(element);\n                        return l;\n                      });\n            }))\n    .run(system);\n```\n\n----------------------------------------\n\nTITLE: Parsing Lines from ByteString Streams (Akka Streams Scala)\nDESCRIPTION: Utilizes Akka Streams' Framing utility to parse lines from a stream of ByteStrings, handling common delimiters like newline. Appropriate for line-oriented parsing or splitting binary streams. Ensures that partial frames are buffered until completion and supports setting maximum line lengths.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeParseLines.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeParseLines.scala) { #parse-lines }\n```\n\n----------------------------------------\n\nTITLE: Using scan operator in Akka Streams in Scala\nDESCRIPTION: Example of using the scan operator in Akka Streams to accumulate values from a stream, similar to fold but emitting intermediate results. The example demonstrates calculating a running sum of numbers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/scan.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n// #scan\nSource(1 to 5)\n  .scan(0)((acc, next) => acc + next)\n  .runForeach(println)\n// 0  <-- scan emits the zero value first\n// 1  <-- 0 + 1\n// 3  <-- 1 + 2\n// 6  <-- 3 + 3\n// 10 <-- 6 + 4\n// 15 <-- 10 + 5\n// #scan\n```\n\n----------------------------------------\n\nTITLE: Akka Cluster Configuration\nDESCRIPTION: Configuration example for setting up an Akka cluster with seed nodes. This shows the minimum required configuration with host/port for remoting and the cluster provider.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\nakka {\n  actor {\n    provider = \"cluster\"\n  }\n  remote {\n    artery {\n      canonical {\n        hostname = \"127.0.0.1\"\n        port = 2551\n      }\n    }\n  }\n  cluster {\n    seed-nodes = [\n      \"akka://ClusterSystem@127.0.0.1:2551\",\n      \"akka://ClusterSystem@127.0.0.1:2552\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Aeron Media Driver Properties Configuration\nDESCRIPTION: Example Aeron properties configuration file showing various performance and threading settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_15\n\nLANGUAGE: properties\nCODE:\n```\naeron.mtu.length=16384\naeron.socket.so_sndbuf=2097152\naeron.socket.so_rcvbuf=2097152\naeron.rcv.buffer.length=16384\naeron.rcv.initial.window.length=2097152\nagrona.disable.bounds.checks=true\n\naeron.threading.mode=SHARED_NETWORK\n\n# low latency settings\n#aeron.threading.mode=DEDICATED\n#aeron.sender.idle.strategy=org.agrona.concurrent.BusySpinIdleStrategy\n#aeron.receiver.idle.strategy=org.agrona.concurrent.BusySpinIdleStrategy\n\naeron.dir=/dev/shm/aeron\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Lookup in Scala\nDESCRIPTION: Scala code for creating a cluster-aware router that uses ActorSelection to look up routees by path on cluster nodes with the \"compute\" role.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval workerRouter = context.actorOf(\n  ClusterRouterGroup.props(\n    ConsistentHashingGroup(Nil),\n    ClusterRouterGroupSettings(\n      totalInstances = 100, routeesPaths = List(\"/user/statsWorker\"),\n      allowLocalRoutees = true, useRoles = Set(\"compute\")))\n    .withDispatcher(\"akka.actor.load-balancing-dispatcher\"),\n  name = \"workerRouter3\")\n\n```\n\n----------------------------------------\n\nTITLE: Source Prematerialization API - Scala\nDESCRIPTION: Scala API signature for prematerializing a Source, returning a tuple of materialized value and new Source with NotUsed type parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/preMaterialize.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\npreMaterialize()(implicit materializer: akka.stream.Materializer): (Mat, akka.stream.scaladsl.Source[Out,akka.NotUsed])\n```\n\n----------------------------------------\n\nTITLE: Factorial Backend Implementation in Java\nDESCRIPTION: Example Java implementation of a backend worker that performs factorial calculations. This actor is used as a routee for the adaptive load balancing example.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n@@snip [FactorialBackend.java](/akka-docs/src/test/java/jdocs/cluster/FactorialBackend.java) { #backend }\n```\n\n----------------------------------------\n\nTITLE: Avoiding Deadlock Using MergePreferred in Akka Streams (Scala)\nDESCRIPTION: An improved cycle implementation using MergePreferred to prioritize the feedback loop. This avoids deadlock by always consuming from the preferred port if elements are available, ensuring that elements in the cycle can always flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_23\n\nLANGUAGE: Scala\nCODE:\n```\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n  val merge = b.add(MergePreferred[Int](1))\n  val bcast = b.add(Broadcast[Int](2))\n\n  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore\n           merge.preferred <~ bcast\n\n  ClosedShape\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository for SBT, Maven, and Gradle\nDESCRIPTION: Configuration snippet for adding the Akka library repository to your build tool. This is required to access Akka dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_0\n\nLANGUAGE: markup\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Default Akka Persistence Plugin Configuration (HOCON)\nDESCRIPTION: Displays the default empty configuration keys (`akka.persistence.journal.plugin`, `akka.persistence.snapshot-store.plugin`, `akka.persistence.state.plugin`) in `reference.conf`. These must be overridden in the user's `application.conf` to select specific persistence plugins for journal, snapshot store, and durable state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka.persistence.journal.plugin = \"\"\nakka.persistence.snapshot-store.plugin = \"\"\nakka.persistence.state.plugin = \"\"\n```\n\n----------------------------------------\n\nTITLE: ScalaTest Integration for ActorTestKit\nDESCRIPTION: This example illustrates integrating ScalaTest with ActorTestKit to automatically manage system shutdown post-test, improving test suite efficiency and management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AsyncTestingExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/ScalaTestIntegrationExampleSpec.scala) { #scalatest-integration }\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Cluster Sharding Typed Dependency\nDESCRIPTION: Declares the necessary dependency for using Akka Cluster Sharding Typed in sbt, Maven, or Gradle projects. It utilizes the Akka BOM (Bill of Materials) for consistent version management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-cluster-sharding-typed\" % AkkaVersion\n)\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_$scala.binary.version$</artifactId>\n      <version>$akka.version$</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-cluster-sharding-typed_$scala.binary.version$</artifactId>\n  </dependency>\n</dependencies>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\ndependencies {\n  implementation platform(\"com.typesafe.akka:akka-bom_$scala.binary.version$:$akka.version$\")\n\n  implementation \"com.typesafe.akka:akka-cluster-sharding-typed_$scala.binary.version$\"\n}\n```\n\n----------------------------------------\n\nTITLE: Multiple Jackson Binding Configurations\nDESCRIPTION: Configuration snippet showing how to define multiple Jackson bindings with different settings for various use cases.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_25\n\nLANGUAGE: HOCON\nCODE:\n```\n@@snip [config](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/SerializationDocSpec.scala) { #several-config }\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Cluster Metrics Events in Scala\nDESCRIPTION: Code for subscribing a metrics listener actor to cluster metrics events. This enables custom handling of metrics data for node lifecycle management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nClusterMetricsExtension(system).subscribe(metricsListenerActor)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Buffer Abstraction Leak in Akka Streams (Scala)\nDESCRIPTION: This Scala example highlights how internal buffers can cause unexpected behavior. The `conflateWithSeed` operator is intended to aggregate elements based on the downstream demand rate set by `ZipWith` and `TickSource`. However, the default internal buffer pre-fetches elements, leading to the conflation logic receiving elements faster than expected, resulting in an output of '1' instead of the anticipated '3'. Setting the buffer size to 1 mitigates this.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\nval flow1 = Flow[Int].map { i => println(s\"A: $i\"); i }.async\nval flow2 = flow1.via(Flow[Int].map { i => println(s\"B: $i\"); i }).async\nval flow3 = flow2.via(Flow[Int].map { i => println(s\"C: $i\"); i }).async\n\nval runnableGraph = Source.tick(FiniteDuration(1, \"second\"), FiniteDuration(1, \"second\"), \"tick\")\n  .zipWith(Source(1 to 10))((tick, i) => i)\n  .via(flow1)\n  .conflateWithSeed(seed = i => { println(s\"first: $i\"); i })((agg, i) => {\n    println(s\"agg: $agg -> $i\");\n    agg + i\n  })\n  .map(i => {\n    println(s\"last: $i\");\n    i\n  })\n  .zipWith(Source.tick(FiniteDuration(3, \"second\"), FiniteDuration(3, \"second\"), \"tick\"))(\n    (i, tick) => i\n  )\n  .map(i => println(s\"Got: $i\"))\n\nrunnableGraph.withAttributes(Attributes.inputBuffer(initial = 1, max = 1)).runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Using flatMapMerge in Java\nDESCRIPTION: Illustrates the `flatMapMerge` operator in Akka Streams. It transforms each input element (integers 1 and 2) into a new Source (repeating the integer three times). Unlike `flatMapConcat`, `flatMapMerge` runs and merges elements from up to `breadth` (here, 2) of these generated Sources concurrently.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\n//#flatMapMerge\nSource.range(1, 2)\n    .flatMapMerge(2, i -> Source.repeat(i).take(3))\n    .runForeach(System.out::println, system);\n//#flatMapMerge\n```\n\n----------------------------------------\n\nTITLE: Draining a Stream to a Seq with Boundedness (Akka Streams Scala, SAFE)\nDESCRIPTION: Illustrates the correct way to collect a bounded stream into a Seq using limit or take before Sink.seq, protecting against out-of-memory errors. Input stream is truncated to a specified maximum size. Outputs Future[Seq[T]] containing up to max elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeSeq.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeSeq.scala) { #draining-to-seq-safe }\n```\n\n----------------------------------------\n\nTITLE: Handling Multiple Exception Types with Nested Akka Typed Supervision (Java)\nDESCRIPTION: Demonstrates nesting `Behaviors.supervise` calls in Java to apply different supervisor strategies based on the exception type. `IllegalStateException` triggers a restart, while `IllegalArgumentException` triggers resuming.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n// #multiple\nfinal Behavior<String> supervisedBehavior4 =\n    Behaviors.supervise(\n            // Each exception needs its own supervisor, exceptions propagate upwards\n            Behaviors.supervise(behavior)\n                .onFailure(IllegalStateException.class, SupervisorStrategy.restart()))\n        .onFailure(IllegalArgumentException.class, SupervisorStrategy.resume());\n// #multiple\n\n```\n\n----------------------------------------\n\nTITLE: Concatenating Sources Using concatAllLazy in Java\nDESCRIPTION: Shows how to use concatAllLazy operator to concatenate multiple sources sequentially in Java. The sources are materialized together but pulled only after original upstream completion.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/concatAllLazy.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer> source = Source.from(Arrays.asList(1, 2, 3));\nSource<Integer> source2 = Source.from(Arrays.asList(4, 5, 6));\nSource<Integer> source3 = Source.from(Arrays.asList(7, 8, 9));\n\nSource<Integer> concatLazy = source.concatAllLazy(source2, source3);\n```\n\n----------------------------------------\n\nTITLE: Sending Replies After Persistence in Akka DurableStateBehavior (Scala)\nDESCRIPTION: Demonstrates how to use a thenRun side effect to send reply messages after successfully persisting an event in a Scala DurableStateBehavior. Requires the Akka Typed DSL and makes use of ActorRef for message passing. Inputs include event data and a command context; the output is a reply message sent to another actor. Intended for scenarios where acknowledgement/confirmation needs to be sent only after persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_21\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [BlogPostEntityDurableState.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/BlogPostEntityDurableState.scala) { #reply }\n```\n\n----------------------------------------\n\nTITLE: Configuring Pool Router Dispatcher in Akka\nDESCRIPTION: Shows how to define a dedicated dispatcher for a router pool in configuration. This allows router pools to use specific dispatchers for their routees.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_48\n\nLANGUAGE: Scala\nCODE:\n```\nakka.actor.deployment {\n  /poolWithDispatcher {\n    router = random-pool\n    nr-of-instances = 5\n    pool-dispatcher {\n      fork-join-executor.parallelism-min = 5\n      fork-join-executor.parallelism-max = 5\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Default Behavior: Stopping Child Actors on Parent Restart (Java)\nDESCRIPTION: Illustrates the default Akka Typed supervision behavior in Java. When a parent actor supervised with `SupervisorStrategy.restart()` restarts, any child actors it created within its `Behaviors.setup` block are stopped automatically.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n// #restart-stop-children\nBehaviors.supervise(\n        Behaviors.<String>setup(\n            context -> {\n              final ActorRef<String> child1 =\n                  context.spawn(behavior, \"child1\");\n              final ActorRef<String> child2 =\n                  context.spawn(behavior, \"child2\");\n\n              return Behaviors.receiveMessage(\n                  message -> {\n                    // processing messages\n                    // ...\n                    // restart occurs, children will be stopped\n                    return Behaviors.same();\n                  });\n            }))\n    .onFailure(IllegalStateException.class, SupervisorStrategy.restart());\n// #restart-stop-children\n\n```\n\n----------------------------------------\n\nTITLE: Using Take Operator in Java\nDESCRIPTION: Shows how to use the take operator to limit stream elements in Java. Takes only the first n elements from a source before completing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/take.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#take\n```\n\n----------------------------------------\n\nTITLE: Using Sink.fold to accumulate elements in Akka Streams (Java)\nDESCRIPTION: This snippet demonstrates using Sink.fold to read numbers from a source, perform calculations in the flow, and then fold over the results by adding the incoming elements together in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/fold.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// #fold\nSource<Integer, NotUsed> ints = Source.range(1, 100);\n\nCompletionStage<Integer> sum =\n    ints.runWith(Sink.fold(0, (aggr, next) -> aggr + next), materializer);\n\nsum.thenAccept(System.out::println);\n// 5050\n// #fold\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Actor Typed Dependency (sbt, Maven, Gradle)\nDESCRIPTION: Defines the necessary dependency for using Akka Actor Typed in a project. It utilizes the Akka BOM (Bill of Materials) for consistent version management across Akka modules. Replace `$scala.binary.version$` and `$akka.version$` with appropriate values.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/stash.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\nval AkkaVersion = \"$akka.version$\"\nlazy val akkaHttpVersion = \"10.5.0\"\nlazy val akkaVersion = \"2.8.0\"\nlazy val scalaTestVersion = \"3.2.15\"\n\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-actor-typed\" % AkkaVersion,\n)\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<properties>\n  <scala.binary.version>$scala.binary.version$</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_$scala.binary.version$</artifactId>\n      <version>$akka.version$</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-actor-typed_$scala.binary.version$</artifactId>\n  </dependency>\n</dependencies>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\ndependencies {\n  implementation platform(\"com.typesafe.akka:akka-bom_$scala.binary.version$:$akka.version$\")\n\n  implementation \"com.typesafe.akka:akka-actor-typed_$scala.binary.version$\"\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Akka Streams Typed Dependency for SBT, Maven, and Gradle (Scala/Java)\nDESCRIPTION: Specifies how to declare the 'akka-stream-typed' dependency in build tools (SBT, Maven, Gradle), using the Akka BOM and resolving the proper Scala binary version and Akka version variables. This block is required for compilation and use of ActorFlow.askWithContext in either Scala or Java. Ensure variables like 'scala.binary.version' and 'akka.version' match your project and Akka BOM usage.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithContext.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\ngroup=\"com.typesafe.akka\"\nartifact=\"akka-stream-typed_$scala.binary.version$\"\nversion=AkkaVersion\n```\n\n----------------------------------------\n\nTITLE: Defining Dispatcher in Actor Configuration in Java\nDESCRIPTION: Shows how to create an actor and define its dispatcher through configuration in Java. The dispatcher is specified in the deployment configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nActorRef myActor = context.actorOf(Props.create(MyActor.class), \"myactor\");\n```\n\n----------------------------------------\n\nTITLE: Creating an ExtensionId for Database Connection Pool in Scala\nDESCRIPTION: Defines the ExtensionId for the DatabaseConnectionPool extension. This object is used to identify and access the extension within the ActorSystem.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nobject DatabaseConnectionPool extends ExtensionId[DatabaseConnectionPool] {\n  def createExtension(system: ActorSystem[_]): DatabaseConnectionPool =\n    new DatabaseConnectionPool(system)\n  def get(system: ActorSystem[_]): DatabaseConnectionPool = apply(system)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster Info Logging\nDESCRIPTION: Configuration options for controlling cluster event logging levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_12\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.cluster.log-info = off\n```\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.cluster.log-info-verbose = on\n```\n\n----------------------------------------\n\nTITLE: Restarting Source on Completion in Scala\nDESCRIPTION: This example demonstrates that a Source is not restarted if it completes, only if it fails. The 'tick' is only printed three times as the take(3) means the inner source completes successfully after emitting the first 3 elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartSource/onFailuresWithBackoff.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nRestartSource\n  .onFailuresWithBackoff(RestartSettings(minBackoff, maxBackoff, randomFactor)) { () =>\n    Source\n      .tick(1.second, 1.second, \"tick\")\n      .take(3)\n      .map { s =>\n        println(s)\n        s\n      }\n  }\n  .runWith(Sink.ignore)\n```\n\n----------------------------------------\n\nTITLE: Factorial Frontend Implementation in Scala\nDESCRIPTION: Example Scala implementation of a frontend actor that receives user jobs and delegates to the backends via the metrics-based router.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [FactorialFrontend.scala](/akka-docs/src/test/scala/docs/cluster/FactorialFrontend.scala) { #frontend }\n```\n\n----------------------------------------\n\nTITLE: Querying Events by Persistence ID in Java\nDESCRIPTION: Shows how to subscribe to a stream of events for a specific persistent actor using the eventsByPersistenceId query in Java. This query is similar to replaying events for an actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nreadJournal.eventsByPersistenceId(\"some-persistence-id\", 0L, Long.MAX_VALUE).runForeach(event -> {\n  System.out.println(\"Got event: \" + event);\n}, system);\n```\n\n----------------------------------------\n\nTITLE: Creating a Source of Tweet Authors (Scala)\nDESCRIPTION: Defines an Akka Streams `Source` that emits a sequence of `Author` objects, representing a stream of tweet authors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval authors: Source[Author, NotUsed] =\n  tweets.filter(_.hashtags.contains(akkaTag)).map(_.author)\n```\n\n----------------------------------------\n\nTITLE: Handling Invalid Requests in Akka Typed ask Pattern (Java)\nDESCRIPTION: This example shows how to handle invalid requests when using the 'ask' pattern in Akka Typed with Java. It demonstrates mapping an InvalidRequest reply to a failed CompletionStage, allowing for explicit error handling on the requestor side.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n@@snip [InteractionPatternsTest.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/InteractionPatternsTest.java) { #standalone-ask-fail-future }\n```\n\n----------------------------------------\n\nTITLE: Guardian Actor Initialization with SpawnProtocol in Java\nDESCRIPTION: Details using SpawnProtocol for initializing tasks and creating initial actors from the guardian actor in Java. Relevant for applications that require spawning of actors from outside the guardian actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/SpawnProtocolDocTest.java) { #imports1 #main }\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Graph Shape in Scala\nDESCRIPTION: Defines a custom Shape for a reusable worker pool junction with multiple input/output ports.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\n#graph-dsl-components-shape\n```\n\n----------------------------------------\n\nTITLE: Using prependLazy in Scala Akka Streams\nDESCRIPTION: Example of using the prependLazy operator in Scala to prepend one source to another, consuming the prepended source first before the original source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/prependLazy.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n// #prependLazy\n```\n\n----------------------------------------\n\nTITLE: Create Simple Source in Scala\nDESCRIPTION: The code creates a simple Akka Stream source emitting integers from 1 to 100 in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [QuickStartDocSpec.scala](/akka-docs/src/test/scala/docs/stream/QuickStartDocSpec.scala) { #create-source }\n```\n\n----------------------------------------\n\nTITLE: Configuring JacksonCborSerializer in Akka\nDESCRIPTION: Configuration for enabling CBOR serialization format in Akka 2.6.5 and later. This configuration allows early adoption of CBOR format before 2.6.6 release after performing an initial rolling update to 2.6.5.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/rolling-update.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor {\n  serializers {\n    jackson-cbor = \"akka.serialization.jackson.JacksonCborSerializer\"\n  }\n  serialization-identifiers {\n    jackson-cbor = 33\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up ActorSystem for Akka Streams in Scala\nDESCRIPTION: Initializes the ActorSystem required for running Akka Streams in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\nimplicit val system: ActorSystem = ActorSystem(\"reactive-tweets\")\n```\n\n----------------------------------------\n\nTITLE: Akka Core Dependency Configuration\nDESCRIPTION: Dependency configuration block for including Akka core module which contains Future helpers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/futures.md#2025-04-22_snippet_1\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=\"com.typesafe.akka\"\n  artifact=\"akka-actor_$scala.binary.version$\"\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Video Frame Handling with extrapolate in Scala\nDESCRIPTION: Example showing how to use the extrapolate operator in Scala to maintain video display by repeating the last frame when network issues cause frame drops. This ensures the UI always shows the most recent successfully decoded frame.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/extrapolate.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval videoStream = networkVideo\n  .extrapolate(frame => Iterator.continually(frame))\n  .map(_.display)\n```\n\n----------------------------------------\n\nTITLE: Defining Bidirectional Shape with Akka Streams in Scala\nDESCRIPTION: This snippet defines the BidiShape class, which represents a bidirectional shape with two inlets and two outlets, serving as the foundational component for building bidirectional flows in Akka Streams. The BidiShape is instrumental when creating custom bidirectional subgraphs allowing data to flow in both directions. The main parameters are the two input and two output ports used for connecting upstream and downstream modules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\nfinal class BidiShape[-In1, +Out1, -In2, +Out2](_in1: Inlet[In1@uncheckedVariance], _out1: Outlet[Out1@uncheckedVariance], _in2: Inlet[In2@uncheckedVariance], _out2: Outlet[Out2@uncheckedVariance]) extends Shape {\n  val in1: Inlet[In1 @uncheckedVariance] = _in1\n  val out1: Outlet[Out1 @uncheckedVariance] = _out1\n  val in2: Inlet[In2 @uncheckedVariance] = _in2\n  val out2: Outlet[Out2 @uncheckedVariance] = _out2\n\n  // ... other members ...\n}\n\n```\n\n----------------------------------------\n\nTITLE: Splitting Large Events During Recovery\nDESCRIPTION: EventAdapter implementation that converts a coarse-grained event into multiple fine-grained events during recovery. Shows how to handle schema evolution by splitting events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\nclass UserEventAdapter extends EventAdapter {\n  override def fromJournal(event: Any, manifest: String): EventSeq = event match {\n    case UserDetailsChanged(name, address) =>\n      val events = Vector.newBuilder[Any]\n      if (name != null) events += UserNameChanged(name)\n      if (address != null) events += UserAddressChanged(address) \n      EventSeq(events.result())\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Error Log Output\nDESCRIPTION: Shows the typical output logged when an `ArithmeticException` occurs within a stream processed by the `log()` operator as demonstrated in the Scala and Java examples.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n[error logging] Upstream failed.\njava.lang.ArithmeticException: / by zero\n```\n\n----------------------------------------\n\nTITLE: Implementing mergePreferred in Scala\nDESCRIPTION: Example of using the mergePreferred operator in Scala to merge streams with preference for one source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergePreferred.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval sourceA = Source(List(1, 2, 3, 4))\nval sourceB = Source(List(10, 20, 30, 40))\n\n// prefer the right (sourceB) if both sources has elements ready\nval mergedWithPreferenceForB = sourceA.mergePreferred(sourceB, true)\nmergedWithPreferenceForB.runWith(Sink.seq).futureValue should contain inOrderElementsOf List(10, 20, 30, 40, 1, 2, 3, 4)\n\n// prefer the left (sourceA) if both sources has elements ready\nval mergedWithPreferenceForA = sourceA.mergePreferred(sourceB, false)\nmergedWithPreferenceForA.runWith(Sink.seq).futureValue should contain inOrderElementsOf List(1, 2, 3, 4, 10, 20, 30, 40)\n```\n\n----------------------------------------\n\nTITLE: Using Default Persistence Plugins in Scala Actor\nDESCRIPTION: Example of a persistent actor that uses the default journal and snapshot plugins by only overriding the persistenceId method. This actor will use the plugins configured in reference.conf.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_41\n\nLANGUAGE: Scala\nCODE:\n```\nclass DefaultPluginsActor(id: String) extends PersistentActor {\n  override def persistenceId: String = id\n\n  // ... rest of the actor implementation ...\n\n  override def receiveCommand: Receive = { case _ => }\n\n  override def receiveRecover: Receive = { case _ => }\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping Actors in Tests - Java\nDESCRIPTION: Java example of stopping actors in tests using ActorTestKit, complete with timeout settings to ensure prompt termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AsyncTestingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/AsyncTestingExampleTest.java) { #test-stop-actors }\n```\n\n----------------------------------------\n\nTITLE: EventStream Superclass Subscription in Java\nDESCRIPTION: Java code showing how to subscribe to a group of events by subscribing to their common superclass on the event stream. This allows receiving multiple related event types with a single subscription.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_28\n\nLANGUAGE: Java\nCODE:\n```\n// common superclass or trait\npublic static class AllKindsOfMusic {\n  public final String artist;\n\n  public AllKindsOfMusic(String artist) {\n    this.artist = artist;\n  }\n}\n\npublic static class Jazz extends AllKindsOfMusic {\n  public Jazz(String artist) {\n    super(artist);\n  }\n}\n\npublic static class Electronic extends AllKindsOfMusic {\n  public Electronic(String artist) {\n    super(artist);\n  }\n}\n\nActorRef musicListener =\n    system.actorOf(\n        Props.create(\n            AbstractActor.class,\n            () ->\n                new AbstractActor() {\n                  @Override\n                  public Receive createReceive() {\n                    return receiveBuilder()\n                        .match(\n                            Jazz.class,\n                            msg -> {\n                              System.out.println(\n                                  getSelf().path().name()\n                                      + \" is listening to: \"\n                                      + msg.artist);\n                            })\n                        .match(\n                            Electronic.class,\n                            msg -> {\n                              System.out.println(\n                                  getSelf().path().name()\n                                      + \" is listening to: \"\n                                      + msg.artist);\n                            })\n                        .match(\n                            AllKindsOfMusic.class,\n                            msg -> {\n                              System.out.println(\n                                  getSelf().path().name()\n                                      + \" heard some music but didn't recognize what kind it is\");\n                            })\n                        .build();\n                  }\n                }));\n\nsystem.getEventStream().subscribe(musicListener, AllKindsOfMusic.class);\n\nsystem.eventEventStream().publish(new Electronic(\"Parov Stelar\"));\n```\n\n----------------------------------------\n\nTITLE: Using Akka Extensions in Java\nDESCRIPTION: Shows how to access and use the Counter extension in Java code. The extension is retrieved from the ActorSystem using the ExtensionId.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n// typically you would use static imports here\nCountExtension counter1 = CountExtensionId.CountExtensionProvider.get(system);\nCountExtension counter2 = CountExtensionId.CountExtensionProvider.get(system);\nassert counter1 == counter2;\ncounter1.increment();\nsystem.log().info(\"counter incremented: {}\", counter1.increment());\n```\n\n----------------------------------------\n\nTITLE: Reading File Contents with FileIO.fromPath in Scala\nDESCRIPTION: This snippet demonstrates how to use FileIO.fromPath to read the contents of a file in Scala. It creates a source from a file path and connects it to a sink that prints each chunk of data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/FileIO/fromPath.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval path = Paths.get(\"example.txt\")\nFileIO.fromPath(path)\n  .to(Sink.foreach(chunk => println(chunk.utf8String)))\n  .run()\n```\n\n----------------------------------------\n\nTITLE: Actor Testing with Test Headers - Scala\nDESCRIPTION: This snippet provides an example of setting up test headers for actor testing in Scala using ActorTestKit. It demonstrates the creation of test probes and actor spawning, aiding in testing actor behavior under test conditions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AsyncTestingExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/AsyncTestingExampleSpec.scala) { #test-header }\n```\n\n----------------------------------------\n\nTITLE: Java Sink Publisher Code Reference\nDESCRIPTION: Java code reference showing how to obtain a Publisher using Sink.asPublisher with AsPublisher parameter\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/stream/stream-design.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSink.asPublisher(akka.stream.javadsl.AsPublisher)\n```\n\n----------------------------------------\n\nTITLE: Implementing TwoPhaseSet Serializer in Java\nDESCRIPTION: Java implementation of a serializer for TwoPhaseSet that converts between the CRDT and its protobuf representation, ensuring deterministic serialization for proper SHA-1 digest generation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_30\n\nLANGUAGE: java\nCODE:\n```\npublic class TwoPhaseSetSerializer extends AbstractSerializationSupport\n    implements SerializerWithStringManifest {\n\n  private static final String TWOPHASESET_MANIFEST = \"A\";\n\n  @Override\n  public String manifest(Object o) {\n    if (o instanceof TwoPhaseSet)\n      return TWOPHASESET_MANIFEST;\n    else\n      throw new IllegalArgumentException(\"Can't serialize object of type \" + o.getClass());\n  }\n\n  @Override\n  public byte[] toBinary(Object o) {\n    if (o instanceof TwoPhaseSet)\n      return twoPhaseSetToProto((TwoPhaseSet) o).toByteArray();\n    else\n      throw new IllegalArgumentException(\"Can't serialize object of type \" + o.getClass());\n  }\n\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) {\n    if (TWOPHASESET_MANIFEST.equals(manifest))\n      return twoPhaseSetFromBinary(bytes);\n    else\n      throw new IllegalArgumentException(\"Unknown manifest: \" + manifest);\n  }\n\n  private TwoPhaseSetMessages.TwoPhaseSet twoPhaseSetToProto(TwoPhaseSet twoPhaseSet) {\n    TwoPhaseSetMessages.TwoPhaseSet.Builder b = TwoPhaseSetMessages.TwoPhaseSet.newBuilder();\n    // sort elements to get same bytes for same content\n    Set<String> adds = twoPhaseSet.getAdds().getElements();\n    List<String> sortedAdds = new ArrayList<>(adds);\n    Collections.sort(sortedAdds);\n    b.addAllAdds(sortedAdds);\n    Set<String> removals = twoPhaseSet.getRemovals().getElements();\n    List<String> sortedRemovals = new ArrayList<>(removals);\n    Collections.sort(sortedRemovals);\n    b.addAllRemovals(sortedRemovals);\n    return b.build();\n  }\n\n  private TwoPhaseSet twoPhaseSetFromBinary(byte[] bytes) {\n    try {\n      TwoPhaseSetMessages.TwoPhaseSet msg = TwoPhaseSetMessages.TwoPhaseSet.parseFrom(bytes);\n      Set<String> adds = new HashSet<>(msg.getAddsList());\n      Set<String> removals = new HashSet<>(msg.getRemovalsList());\n      return new TwoPhaseSet(new GSet<>(adds), new GSet<>(removals));\n    } catch (InvalidProtocolBufferException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Materializer from ActorContext in Scala\nDESCRIPTION: In this Scala example, a materializer is instantiated using akka.actor.ActorContext, thus linking its lifecycle to the surrounding actor. This demonstrates lifecycle-bound stream management within actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FlowDocSpec.scala](/akka-docs/src/test/scala/docs/stream/FlowDocSpec.scala) { #materializer-from-actor-context }\n```\n\n----------------------------------------\n\nTITLE: Killing Akka Actors with Kill Message\nDESCRIPTION: Shows how to forcefully terminate actors using the Kill message, which triggers an ActorKilledException and supervisor handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_39\n\nLANGUAGE: Scala\nCODE:\n```\nvictim ! Kill\n```\n\n----------------------------------------\n\nTITLE: Getting DurableStateStoreQuery in Java\nDESCRIPTION: Example showing how to retrieve DurableStateStoreQuery from the DurableStateStoreRegistry extension in Java. This is used for implementing tag-based searches in Akka Projections.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/persistence-query.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nDurableStateStoreQuery<String> durableStateStoreQuery = DurableStateStoreRegistry.get(system).getDurableStateStoreFor(DurableStateStoreQuery.class, pluginId);\n```\n\n----------------------------------------\n\nTITLE: Flow Setup Method Signature in Scala\nDESCRIPTION: Scala signature for the Flow.setup method that takes a factory function accepting ActorMaterializer and Attributes, returning a Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/setup.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nsetup[T,U,M](factory:(akka.stream.ActorMaterializer,akka.stream.Attributes)=>akka.stream.scaladsl.Flow[T,U,M]):akka.stream.scaladsl.Flow[T,U,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: Initializing Akka Actor using preStart and postRestart in Scala\nDESCRIPTION: This snippet demonstrates how to override preStart and postRestart methods in Scala to control actor initialization. It shows how to prevent child actor recreation during restarts.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_44\n\nLANGUAGE: Scala\nCODE:\n```\noverride def preStart(): Unit = {\n  // Initialize children here\n}\n\noverride def postRestart(reason: Throwable): Unit = {\n  // do not call preStart()\n  // initialization code should happen in postRestart\n}\n\noverride def preRestart(reason: Throwable, message: Option[Any]): Unit = {\n  // keep the call to postStop(), but no stopping of children\n  postStop()\n}\n```\n\n----------------------------------------\n\nTITLE: Concatenating Streams Lazily in Scala\nDESCRIPTION: Example showing how to use concatLazy operator to concatenate two streams in Scala, where the second stream is only pulled after the first one completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/concatLazy.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nflowA.concatLazy(flowB)\n```\n\n----------------------------------------\n\nTITLE: Applying Backoff Supervision Strategy to Singleton Actor - Akka Cluster Typed - Scala\nDESCRIPTION: Presents a strategy to supervise the singleton actor using backoff restarting in Scala. In case of exceptions, the actor is restarted after a backoff period, increasing system resilience but potentially causing temporary unavailability. Makes use of Behaviors.supervise and a backoff supervisor, needing Akka Typed’s supervision framework. Accepts initial and maximum backoff durations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nval supervisedCounterBehavior =\n  Behaviors.supervise(counterBehavior)\n    .onFailure[Exception](SupervisorStrategy.restartWithBackoff(1.second, 10.seconds, 0.2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Sink.reduce in Java\nDESCRIPTION: Example demonstrating the usage of Sink.reduce operator in Java for stream element reduction. The operator applies a reduction function that combines elements sequentially, materializing into a CompletionStage.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/reduce.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#reduce-operator-example\n```\n\n----------------------------------------\n\nTITLE: Configuring Event Sourced Mode for Remembering Entities in Akka Cluster Sharding\nDESCRIPTION: Configuration snippet for enabling event sourced mode for the remember entities store in Akka Cluster Sharding. This mode uses Event Sourcing to store active shards and entities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_46\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.remember-entities-store = eventsourced\n```\n\n----------------------------------------\n\nTITLE: Injecting Keep-Alive Messages into ByteString Stream in Scala\nDESCRIPTION: Uses a built-in operation to inject keep-alive messages into a stream of ByteStrings, but only if it doesn't interfere with normal traffic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_55\n\nLANGUAGE: Scala\nCODE:\n```\nimport scala.concurrent.duration._\n\nval keepAliveMessage = ByteString(\"\\u0000\")\n\nval injectKeepAlive: Flow[ByteString, ByteString, NotUsed] =\n  Flow[ByteString].keepAlive(1.second, () => keepAliveMessage)\n```\n\n----------------------------------------\n\nTITLE: Implementing Receive Timeout in Akka Actors\nDESCRIPTION: Demonstrates how to set up and handle receive timeouts in actors. Shows timeout configuration and message handling for both Scala and Java implementations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_35\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.ReceiveTimeout\nimport scala.concurrent.duration._\n\ndef receive = {\n  case \"Hello\" =>\n    context.setReceiveTimeout(3.seconds)\n  case ReceiveTimeout =>\n    // To turn it off\n    context.setReceiveTimeout(Duration.Undefined)\n    throw new RuntimeException(\"Receive timeout\")\n}\n```\n\n----------------------------------------\n\nTITLE: DNS Resolution Using Extension in Java\nDESCRIPTION: Example of performing DNS resolution using the DNS extension in Java. Shows how to resolve domain names using the Akka DNS API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-dns.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n#resolve\n```\n\n----------------------------------------\n\nTITLE: Handling Get Response Pattern 2 in Java\nDESCRIPTION: Alternative approach to handling Get responses in Java by checking for different response types and then verifying if the response key matches the expected key.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\nreplicator.tell(new Get<>(Counter1Key, ReadLocal.instance()), getSelf());\n\n// handle response\nreceiveBuilder()\n    .matchEquals(\n        new GetSuccess<>(Counter1Key, Optional.empty()),\n        a -> {\n          long value = ((PNCounter) a.dataValue()).getValue().intValue();\n        })\n    .matchEquals(\n        new NotFound<>(Counter1Key, Optional.empty()),\n        a -> {\n          // key counter1 does not exist\n        })\n    .matchEquals(\n        new GetFailure<>(Counter1Key, Optional.empty()),\n        a -> {\n          // read failed, try again later?\n        })\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Dependency Configuration for SBT, Maven, and Gradle\nDESCRIPTION: Shows how to include the necessary dependencies for using PubSub.source in your build configuration. Requires adding the Akka library repository and the akka-stream-typed module.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/PubSub/source.md#2025-04-22_snippet_1\n\nLANGUAGE: markup\nCODE:\n```\n// Repository configuration\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n\n// Dependency\nbomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\nsymbol1=AkkaVersion\nvalue1=\"$akka.version$\"\ngroup=\"com.typesafe.akka\"\nartifact=\"akka-stream-typed_$scala.binary.version$\"\nversion=AkkaVersion\n```\n\n----------------------------------------\n\nTITLE: Using Collect Operator in Java with PFBuilder\nDESCRIPTION: Shows how to use the collect operator with PFBuilder in Java to filter and transform messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collect.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nmessageSource.collect(\n    new PFBuilder<Message, Pong>()\n        .match(Ping.class, \n            ping -> ping.id() != 0,\n            ping -> new Pong(ping.id()))\n        .build());\n```\n\n----------------------------------------\n\nTITLE: Forward-Compatible ItemAdded Migration in Scala\nDESCRIPTION: Scala code snippet showing the migration class for ItemAdded during the first deployment, capable of reading the new schema version.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ItemAddedMigration.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v1withv2/ItemAddedMigration.scala) { #forward-one-rename }\n```\n\n----------------------------------------\n\nTITLE: Setting Dispatcher for Router Actors in Akka\nDESCRIPTION: Demonstrates how to set dispatchers for router actors programmatically. This example shows setting both the router dispatcher and the pool dispatcher for routees.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_49\n\nLANGUAGE: Scala\nCODE:\n```\nval router = system.actorOf(\n  // \"head\" router actor will run on \"router-dispatcher\" dispatcher\n  // Worker routees will run on \"pool-dispatcher\" dispatcher\n  RandomPool(5, routerDispatcher = \"router-dispatcher\").props(\n    Props[Worker].withDispatcher(\"pool-dispatcher\")), \"poolWithDispatcher\")\n```\n\nLANGUAGE: Java\nCODE:\n```\nActorRef router = system.actorOf(\n  // \"head\" router actor will run on \"router-dispatcher\" dispatcher\n  // Worker routees will run on \"pool-dispatcher\" dispatcher\n  new RandomPool(5).withRouteeProps(\n    Props.create(Worker.class).withDispatcher(\"pool-dispatcher\"))\n    .withDispatcher(\"router-dispatcher\"),\n  \"poolWithDispatcher\");\n```\n\n----------------------------------------\n\nTITLE: Using scanAsync to Process Streams Asynchronously in Scala\nDESCRIPTION: Example demonstrating scanAsync to accumulate values asynchronously in a stream. The example simulates async operations with Future.successful, summing incoming integers with the current state and emitting each intermediate result.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/scanAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.stream.scaladsl.Source\nimport scala.concurrent.Future\n\nval source = Source(1 to 5)\n\nsource\n  .scanAsync(0) { (sum, element) =>\n    Future.successful(sum + element)\n  }\n  // Result: 0, 1, 3, 6, 10, 15\n  .runForeach(println)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stats Aggregator in Java\nDESCRIPTION: Java implementation of the aggregator actor that collects and processes character count results from worker routees to calculate the mean word length.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\npublic class StatsAggregator extends AbstractActor {\n  final int expectedResults;\n  final ActorRef replyTo;\n  final List<Integer> results = new ArrayList<Integer>();\n\n  public StatsAggregator(int expectedResults, ActorRef replyTo) {\n    this.expectedResults = expectedResults;\n    this.replyTo = replyTo;\n    receive(ReceiveTimeout.getInstance(),\n      Duration.ofSeconds(3));\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n      .match(StatsMessages.WordCount.class, wc -> {\n        results.add(wc.getCount());\n        if (results.size() == expectedResults) {\n          int sum = 0;\n          for (int c : results)\n            sum += c;\n          double meanWordLength = ((double) sum) / results.size();\n          replyTo.tell(new StatsMessages.StatsResult(meanWordLength),\n            getSelf());\n          getContext().stop(getSelf());\n        }\n      })\n      .match(ReceiveTimeout.class, x -> {\n        replyTo.tell(\n          new StatsMessages.JobFailed(\n            \"Service unavailable, try again later\"),\n          getSelf());\n        getContext().stop(getSelf());\n      })\n      .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor with Stash in Scala\nDESCRIPTION: Shows how to use the Stash trait to temporarily store and later process messages that can't be handled in current state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_43\n\nLANGUAGE: scala\nCODE:\n```\nclass ActorWithStash extends Actor with Stash {\n  def receive = {\n    case \"open\" =>\n      unstashAll()\n      context.become({\n        case \"write\" => // handle write\n        case \"close\" => context.unbecome()\n        case msg => stash()\n      })\n    case msg => stash()\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up ActorSystem for Akka Streams in Java\nDESCRIPTION: Initializes the ActorSystem required for running Akka Streams in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nfinal ActorSystem system = ActorSystem.create(\"reactive-tweets\");\n```\n\n----------------------------------------\n\nTITLE: Handling Validation Errors with Generic Response Wrapper in Akka Typed (Java)\nDESCRIPTION: This snippet demonstrates how to handle validation errors using the StatusReply generic response wrapper in Akka Typed when asking from outside the actor system in Java. It shows how to construct and handle error responses.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\n@@snip [InteractionPatternsTest.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/InteractionPatternsAskWithStatusTest.java) { #standalone-ask-with-status-fail-future }\n```\n\n----------------------------------------\n\nTITLE: Accessing Typed Cluster Extension\nDESCRIPTION: Demonstrates how to access the typed Cluster extension on each node, which provides management tasks and exposes cluster membership events via APIs. This is key for interacting with cluster state and membership.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nBasicClusterExampleSpec.scala { #cluster-create }\n```\n\n----------------------------------------\n\nTITLE: Updating a Flag CRDT with Akka Distributed Data Replicator in Java\nDESCRIPTION: Performs a distributed flag update to indicate the opening of voting by creating an Update operation on a Flag key. Utilizes Akka's Replicator and the 'writeAll' consistency level for immediate replication across nodes. Requires Akka Distributed Data dependencies; inputs are a key and a local replica, output is an updated flag state replicated cluster-wide.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-java/README.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nUpdate<Flag> update = new Update<>(openedKey, Flag.create(), writeAll, curr -> curr.switchOn());\n```\n\n----------------------------------------\n\nTITLE: Basic Scala Publisher Integration\nDESCRIPTION: Shows the basic signature for creating a Source from a Flow.Publisher in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromPublisher.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ndef fromPublisher[T](publisher: java.util.concurrent.Flow.Publisher[T]): Source[T, NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Implementing EventBus API in Scala\nDESCRIPTION: The core EventBus interface in Scala that defines the publish-subscribe model for event distribution in Akka. It specifies methods for subscribing, unsubscribing, and publishing events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n/**\n * Base class for typed event buses specialized per classifier of events and subscriber.\n */\nabstract class EventBus[E, C, S] {\n  /**\n   * Attempts to register the subscriber to the specified Classifier\n   * @return true if successful and false if not (because it was already\n   *   subscribed to that Classifier, or otherwise)\n   */\n  def subscribe(subscriber: S, to: C): Boolean\n\n  /**\n   * Attempts to deregister the subscriber from the specified Classifier\n   * @return true if successful and false if not (because it wasn't subscribed\n   *   to that Classifier, or otherwise)\n   */\n  def unsubscribe(subscriber: S, from: C): Boolean\n\n  /**\n   * Attempts to deregister the subscriber from all Classifiers it may be subscribed to\n   */\n  def unsubscribe(subscriber: S): Unit\n\n  /**\n   * Publishes the specified Event to this bus\n   */\n  def publish(event: E): Unit\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Akka Stream Reference Configuration\nDESCRIPTION: Snippet that loads the reference.conf file containing default configuration settings for Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/stream/stream-configuration.md#2025-04-22_snippet_0\n\nLANGUAGE: conf\nCODE:\n```\n@@snip [reference.conf](/akka-stream/src/main/resources/reference.conf)\n```\n\n----------------------------------------\n\nTITLE: Implementing Deny List with statefulMapConcat in Java\nDESCRIPTION: Java implementation of a deny list filter using statefulMapConcat.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMapConcat.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(\"allow:hello\", \"deny:world\", \"hello\", \"world\"))\n    .statefulMapConcat(() -> {\n        final Set<String> deniedWords = new HashSet<>();\n        return el -> {\n            if (el.startsWith(\"deny:\")) {\n                deniedWords.add(el.substring(5));\n                return Collections.emptyList();\n            } else if (deniedWords.contains(el)) {\n                return Collections.emptyList();\n            } else {\n                return Collections.singletonList(el);\n            }\n        };\n    })\n    .runWith(Sink.seq(), system);\n```\n\n----------------------------------------\n\nTITLE: Handling Unmatched Messages in Scala with Behaviors.unhandled\nDESCRIPTION: Using Behaviors.unhandled to explicitly mark messages that an actor shouldn't process, while still maintaining exhaustive pattern matching.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\nBehaviors.receiveMessage {\n  case Increment(delta) =>\n    value += delta\n    Behaviors.same\n  case GetValue(replyTo) =>\n    replyTo ! value\n    Behaviors.same\n  case GiveMeYourState(_) =>\n    // don't expose internal state\n    Behaviors.unhandled\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Behavior with TestActorRef in Scala\nDESCRIPTION: Demonstrates how to test an actor's behavior synchronously using TestActorRef with CallingThreadDispatcher. The test creates an actor reference and directly accesses its underlying actor instance for verification.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_39\n\nLANGUAGE: scala\nCODE:\n```\nfinal class MyActor extends Actor {\n  var state = \"\"\n  def receive = {\n    case \"blah\" => state = \"blah received\"\n  }\n}\n\nval actor = TestActorRef(new MyActor)\nactor ! \"blah\"\nassert(actor.underlyingActor.state == \"blah received\")\n```\n\n----------------------------------------\n\nTITLE: Using Sink.actorRef API in Scala and Java\nDESCRIPTION: API signature for Sink.actorRef which creates a sink that sends elements to an ActorRef. In Scala, it takes parameters for completion and failure messages, while the Java version only shows the completion message parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/actorRef.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSink.actorRef[T](ref:akka.actor.ActorRef,onCompleteMessage:Any,onFailureMessage:Throwable=>Any):akka.stream.scaladsl.Sink[T,akka.NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nSink.actorRef(akka.actor.ActorRef,java.lang.Object)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pull Mode TCP Listener in Scala\nDESCRIPTION: A Scala example showing how to create a TCP server that uses pull mode for accepting connections. The pullMode parameter of the Bind command is set to true to enable pull-based accepting of connections.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_22\n\nLANGUAGE: scala\nCODE:\n```\nIOManager(system) ! Bind(listener, new InetSocketAddress(\"localhost\", 0), pullMode = true)\n```\n\n----------------------------------------\n\nTITLE: Configuring DistributedPubSub Extension\nDESCRIPTION: HOCON configuration for the DistributedPubSub extension showing how to activate it at system startup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_12\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.extensions = [\"akka.cluster.pubsub.DistributedPubSub\"]\n```\n\n----------------------------------------\n\nTITLE: Querying Default Internal Stash Overflow Strategy (Java)\nDESCRIPTION: Shows how to programmatically retrieve the configured default internal stash overflow strategy from within an actor. It involves getting the `Persistence` extension using `Persistence.get(getContext().getSystem())` and then calling its `defaultInternalStashOverflowStrategy()` method.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\nPersistence.get(getContext().getSystem()).defaultInternalStashOverflowStrategy();\n```\n\n----------------------------------------\n\nTITLE: Configuring Lease-Majority Strategy for Akka Split Brain Resolver\nDESCRIPTION: Configuration example for the lease-majority strategy in Akka's Split Brain Resolver. The lease implementation is set to use Kubernetes, requiring additional dependencies as noted in the Kubernetes Lease documentation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_8\n\nLANGUAGE: conf\nCODE:\n```\nakka {\n  cluster {\n    downing-provider-class = \"akka.cluster.sbr.SplitBrainResolverProvider\"\n    split-brain-resolver {\n      active-strategy = \"lease-majority\"\n      lease-majority {\n        lease-implementation = \"akka.coordination.lease.kubernetes\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Logging Criteria in Scala\nDESCRIPTION: Shows how to build more advanced criteria for logging event verification using LoggingTestKit in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [LoggingDocExamples.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/LoggingDocExamples.scala) { #test-logging-criteria }\n```\n\n----------------------------------------\n\nTITLE: Forward-Compatible ItemAdded Migration in Java\nDESCRIPTION: Java code snippet showing the migration class for ItemAdded during the first deployment, capable of reading the new schema version.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ItemAddedMigration.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v1withv2/ItemAddedMigration.java) { #forward-one-rename }\n```\n\n----------------------------------------\n\nTITLE: Illustrating Buffer Abstraction Leak in Akka Streams (Java)\nDESCRIPTION: This Java example demonstrates a potential issue where internal buffering affects rate-based operations. The `conflateWithSeed` operator aggregates elements based on downstream demand timed by `ZipWith` and a `TickSource`. Due to the default internal buffer prefetching elements into `ZipWith`, the conflation step receives elements prematurely, leading to an output of '1' instead of the expected '3'. Reducing the buffer size to 1 helps align behavior with expectations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nfinal Flow<Integer, Integer, NotUsed> flow = Flow.of(Integer.class).map(i -> i);\n\nfinal Attributes bufferAttr = Attributes.inputBuffer(1, 1);\n\nfinal RunnableGraph<?> runnableGraph = Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), \"tick\")\n    .zipWith(Source.range(1, 10), (tick, i) -> i)\n    .via(flow.async())\n    .conflateWithSeed(\n        i -> {\n          System.out.println(\"first: \" + i);\n          return i;\n        },\n        (agg, i) -> {\n          System.out.println(\"agg: \" + agg + \" -> \" + i);\n          return agg + i;\n        }\n    )\n    .map(i -> { System.out.println(\"last: \" + i); return i; })\n    .zipWith(Source.tick(Duration.ofSeconds(3), Duration.ofSeconds(3), \"tick\"), (i, tick) -> i)\n    // demonstrate the need for controlling buffer sizes\n    .withAttributes(bufferAttr)\n    .map(i -> { System.out.println(\"Got: \" + i); return i; });\n\nrunnableGraph.run(system);\n```\n\n----------------------------------------\n\nTITLE: Defining askWithStatusAndContext Signature for ActorFlow - Akka Streams API Declaration (Java)\nDESCRIPTION: Shows the Java API method signature for ActorFlow.askWithStatusAndContext, specifying the parameter types and expected flow for sending each element as an ask to a typed actor. Key parameters are parallelism, reference to the actor, timeout, and a message-creation function. Inputs are tuples of input and context, outputs are tuples of actor reply and context. Akka Streams and Akka Typed Actor dependencies are required.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithStatusAndContext.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nActorFlow.askWithStatusAndContext[I,Q,A,Ctx](parallelism:Int, ref:akka.actor.typed.ActorRef[Q], timeout:java.time.Duration, makeMessage:java.util.function.BiFunction[I, akka.actor.typed.ActorRef[akka.pattern.StatusReply[A]], Q])\n```\n\n----------------------------------------\n\nTITLE: Creating 3-Node Multi-JVM Application Test in Scala\nDESCRIPTION: Scala example showing how to create a 3-node multi-JVM test application with three separate objects named according to the naming convention.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\npackage sample\n\nobject SampleMultiJvmNode1 {\n  def main(args: Array[String]) {\n    println(\"Hello from node 1\")\n  }\n}\n\nobject SampleMultiJvmNode2 {\n  def main(args: Array[String]) {\n    println(\"Hello from node 2\")\n  }\n}\n\nobject SampleMultiJvmNode3 {\n  def main(args: Array[String]) {\n    println(\"Hello from node 3\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using PoisonPill for Actor Termination\nDESCRIPTION: Demonstrates how to terminate actors using PoisonPill message, which stops the actor after processing current mailbox messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_38\n\nLANGUAGE: Scala\nCODE:\n```\nvictim ! PoisonPill\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistence Plugins for Event Sourced Remember Entities Store\nDESCRIPTION: Configuration snippet for specifying the journal and snapshot plugins to use with the event sourced remember entities store in Akka Cluster Sharding.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_47\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.journal-plugin-id = <plugin>\nakka.cluster.sharding.snapshot-plugin-id = <plugin>\n```\n\n----------------------------------------\n\nTITLE: Running Akka Benchmarks Using Single SBT Command\nDESCRIPTION: Single-line command for running Akka microbenchmarks without entering the SBT shell. Uses the same parameters as the interactive version.\nSOURCE: https://github.com/akka/akka/blob/main/akka-bench-jmh/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsbt akka-bench-jmh/jmh:run -i 3 -wi 3 -f 1 .*ActorCreationBenchmark\n```\n\n----------------------------------------\n\nTITLE: Concatenating Streams Lazily in Java\nDESCRIPTION: Example showing how to use concatLazy operator to concatenate two streams in Java, where the second stream is only pulled after the first one completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/concatLazy.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nsourceA.concatLazy(sourceB)\n```\n\n----------------------------------------\n\nTITLE: Declaring prefixAndTail Operator for Akka Flow in Java\nDESCRIPTION: Defines the prefixAndTail operator for Akka Flow in Java. It takes an integer parameter specifying the number of elements to take as prefix.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/prefixAndTail.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nFlow.prefixAndTail(int)\n```\n\n----------------------------------------\n\nTITLE: Implementing Durable Queue with EventSourcedProducerQueue in Java\nDESCRIPTION: Java implementation example of a durable producer queue using EventSourcedProducerQueue in an image converter work manager for reliable message delivery.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/reliable-delivery.md#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nWorkPullingDocExample.java\n```\n\n----------------------------------------\n\nTITLE: Obtaining a Read Journal Instance in Scala\nDESCRIPTION: Demonstrates how to get a reference to a read journal implementation using PersistenceQuery in Scala. This is the first step to access persistence query capabilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval readJournal =\n  PersistenceQuery(system).readJournalFor[MyScaladslReadJournal](\"akka.persistence.query.my-read-journal\")\n```\n\n----------------------------------------\n\nTITLE: Handling Get Response Pattern 1 in Java\nDESCRIPTION: Java example showing how to handle the response from a Get operation by checking for different response types like GetSuccess, NotFound, and GetFailure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nreplicator.tell(new Get<>(Counter1Key, ReadLocal.instance()), getSelf());\n\n// handle response\nreceiveBuilder()\n    .match(\n        GetSuccess.class,\n        g -> {\n          if (g.key().equals(Counter1Key)) {\n            long value = ((PNCounter) g.dataValue()).getValue().intValue();\n          }\n        })\n    .match(\n        NotFound.class,\n        n -> {\n          if (n.key().equals(Counter1Key)) {\n            // key counter1 does not exist\n          }\n        })\n    .match(\n        GetFailure.class,\n        f -> {\n          if (f.key().equals(Counter1Key)) {\n            // read failed, try again later?\n          }\n        })\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Implementing At-Least-Once Delivery in Java\nDESCRIPTION: Example of using AbstractPersistentActorWithAtLeastOnceDelivery class in a Java PersistentActor to implement at-least-once message delivery. It shows how to persist events, handle confirmations, and manage delivery state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_33\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [LambdaPersistenceDocTest.java](/akka-docs/src/test/java/jdocs/persistence/LambdaPersistenceDocTest.java) { #at-least-once-example }\n```\n\n----------------------------------------\n\nTITLE: Monitoring Stream Elements with FlowMonitor in Java\nDESCRIPTION: Example showing how to use monitorMat operator to observe stream elements and completion state in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/monitor.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#monitor\n```\n\n----------------------------------------\n\nTITLE: Using PriorityDispatcher in Java\nDESCRIPTION: Java code showing how to create an actor that uses a priority dispatcher. Messages sent to this actor will be processed according to their priority as defined in the mailbox implementation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n// We create a new Actor that just prints out what it processes\nfinal ActorRef a =\n    system.actorOf(\n        Props.create(\n                Actor.class,\n                () ->\n                    new Actor() {\n                      private final LoggingAdapter log = Logging.getLogger(context().system(), this);\n\n                      @Override\n                      public Receive createReceive() {\n                        return receiveBuilder()\n                            .matchAny(\n                                x -> {\n                                  log.info(x.toString());\n                                })\n                            .build();\n                      }\n                    })\n            .withDispatcher(\"prio-dispatcher\"));\n\n// These are the messages we will send\na.tell(\"lowpriority\", ActorRef.noSender());\na.tell(\"lowpriority\", ActorRef.noSender());\na.tell(\"highpriority\", ActorRef.noSender());\na.tell(\"pigdog\", ActorRef.noSender());\na.tell(\"pigdog2\", ActorRef.noSender());\na.tell(\"pigdog3\", ActorRef.noSender());\na.tell(\"highpriority\", ActorRef.noSender());\na.tell(PoisonPill.getInstance(), ActorRef.noSender());\n```\n\n----------------------------------------\n\nTITLE: Limiting Elements from Untrusted Source in Akka Streams (Scala)\nDESCRIPTION: This code demonstrates how to use the limit operator to protect against potential memory issues when processing elements from an untrusted source. It collects at most 10,000 elements into a sequence, failing the stream if more elements are received.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/limit.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nSource.from(untrustedSource)\n  .limit(10000) // at most 10000 elements will be allowed through\n  .toMat(Sink.seq)(Keep.right) // collect the elements into a sequence\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor Classification Bus in Java\nDESCRIPTION: A Java implementation of an EventBus using Actor Classification, where both events and subscribers are ActorRefs. This pattern is used for implementing DeathWatch in Akka.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\npublic class ActorBusImpl extends EventBus<ActorEvent, ActorRef, ActorRef>\n    implements ActorClassification {\n  // will be invoked for each event for all subscribers\n  @Override\n  public void publish(ActorEvent event, ActorRef subscriber) {\n    // in this case event is ActorEvent and subscriber is ActorRef\n    if (event.sender.equals(subscriber)) subscriber.tell(event.payload, ActorRef.noSender());\n  }\n\n  // may define the subscriber order which is used throughout the lifetime of the EventBus\n  @Override\n  public int compareSubscribers(ActorRef a, ActorRef b) {\n    return a.compareTo(b);\n  }\n\n  // determines the initial size of the index data structure\n  // used internally (i.e. the expected number of different classifiers)\n  @Override\n  public int mapSize() {\n    return 128;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Actor Termination Task in Scala\nDESCRIPTION: Shows how to add a task that sends a shutdown message to an actor and waits for its termination in Scala. This is a convenience method that monitors actor termination instead of waiting for a Done reply.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nCoordinatedShutdown(system).addActorTerminationTask(\n  CoordinatedShutdown.PhaseBeforeServiceUnbind,\n  \"someTaskName\",\n  actor,\n  Shutdown)\n```\n\n----------------------------------------\n\nTITLE: Publishing Messages to a Topic in Java\nDESCRIPTION: Java example showing how to publish a message to a topic in an Akka cluster. The message is sent to a publisher actor which distributes it to all subscribers of the 'content' topic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nActorRef publisher = system.actorOf(Props.create(Publisher.class), \"publisher\");\n// after a while the message is published to the subscribers\npublisher.tell(\"hello\", ActorRef.noSender());\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository Access (sbt/Maven/Gradle)\nDESCRIPTION: Specifies the repository URL required to fetch Akka dependencies. Provides configuration snippets for sbt, Maven, and Gradle build tools.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\nrepositories {\n  mavenCentral()\n  maven {\n    url = uri(\"https://repo.akka.io/maven\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-Node Test Configuration in Scala\nDESCRIPTION: This code defines a MultiNodeConfig class that configures a two-node test environment. It sets up roles for \"node1\" and \"node2\" and configures the test transport system for failure injection and throttling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-node-testing.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\npackage akka.remote.sample\n\nobject MultiNodeSample {\n  // #config\n  import akka.remote.testkit.MultiNodeConfig\n\n  class MultiNodeSampleConfig extends MultiNodeConfig {\n    val node1 = role(\"node1\")\n    val node2 = role(\"node2\")\n\n    testTransport(on = true)\n  }\n  // #config\n```\n\n----------------------------------------\n\nTITLE: Disabling Durable Storage for Remembered Entities in Akka Cluster Sharding\nDESCRIPTION: Configuration snippet for disabling durable storage of remembered entities in distributed data mode. This improves performance but entities won't be remembered after a full cluster restart.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_45\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.distributed-data.durable.keys = []\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Sources with MergeAll in Scala\nDESCRIPTION: Demonstrates using mergeAll operator to combine multiple source streams in Scala. The operator randomly picks elements from available sources when multiple sources have elements ready.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergeAll.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFlowMergeSpec.scala\n```\n\n----------------------------------------\n\nTITLE: Basic Actor Message Receiving - Java\nDESCRIPTION: Basic implementation of an actor's createReceive method to handle incoming messages using receiveBuilder in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_33\n\nLANGUAGE: java\nCODE:\n```\n#imports #my-actor\n```\n\n----------------------------------------\n\nTITLE: Unit Testing Akka Custom Routing Logic\nDESCRIPTION: Tests the RedundancyRoutingLogic to verify it correctly selects multiple routees for each message. The test creates ActorRef routees, applies the routing logic, and confirms the correct number of routees are selected.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_43\n\nLANGUAGE: Scala\nCODE:\n```\nval logic = new RedundancyRoutingLogic(nbrCopies = 3)\nval routees = Vector(\n  ActorRefRoutee(system.actorOf(Props(new Actor {\n    def receive = {\n      case msg => testActor ! msg\n    }\n  }))),\n  ActorRefRoutee(system.actorOf(Props(new Actor {\n    def receive = {\n      case msg => testActor ! msg\n    }\n  }))),\n  ActorRefRoutee(system.actorOf(Props(new Actor {\n    def receive = {\n      case msg => testActor ! msg\n    }\n  }))))\n\nlogic.select(\"msg\", routees).send(\"msg\", testActor)\nexpectMsg(\"msg\")\nexpectMsg(\"msg\")\nexpectMsg(\"msg\")\n```\n\nLANGUAGE: Java\nCODE:\n```\nfinal int nbrCopies = 3;\nRedundancyRoutingLogic logic = new RedundancyRoutingLogic(nbrCopies);\nTestProbe probe = new TestProbe(system);\nfinal IndexedSeq<Routee> routees = JavaTestKit.immutableIndexedSeq(Arrays.asList(\n  new ActorRefRoutee(system.actorOf(Props.create(SimpleActor.class, probe.ref()))),\n  new ActorRefRoutee(system.actorOf(Props.create(SimpleActor.class, probe.ref()))),\n  new ActorRefRoutee(system.actorOf(Props.create(SimpleActor.class, probe.ref())))));\n\nlogic.select(\"msg\", routees).send(\"msg\", probe.ref());\nprobe.expectMsgAllOf(Duration.Zero(), \"msg\", \"msg\", \"msg\");\n```\n\n----------------------------------------\n\nTITLE: Command Class Example in Java\nDESCRIPTION: Example of a command class containing an ActorRef for replies in Java. Shows how to structure commands with reply channels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\n#reply-command\n```\n\n----------------------------------------\n\nTITLE: Draining a Stream to a List with Boundedness (Akka Streams Java, SAFE)\nDESCRIPTION: Presents safe practice for collecting elements from a stream to a List in Java via limit or take before the collecting sink, thus bounding collection size. Suitable for memory-constrained or untrusted input scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeSeq.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeSeq.java) { #draining-to-list-safe }\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Actor Outline in Scala\nDESCRIPTION: Provides the basic structure of the DeviceGroupQuery actor, including initialization and message handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nobject DeviceGroupQuery {\n  def apply(\n      deviceIdToActor: Map[String, ActorRef[Device.Command]],\n      requestId: Long,\n      requester: ActorRef[RespondAllTemperatures],\n      timeout: FiniteDuration): Behavior[Command] = {\n    Behaviors.setup { context =>\n      Behaviors.withTimers { timers =>\n        new DeviceGroupQuery(deviceIdToActor, requestId, requester, timeout, context, timers)\n          .queryBehavior()\n      }\n    }\n  }\n}\n\nclass DeviceGroupQuery(\n    deviceIdToActor: Map[String, ActorRef[Device.Command]],\n    requestId: Long,\n    requester: ActorRef[RespondAllTemperatures],\n    timeout: FiniteDuration,\n    context: ActorContext[Command],\n    timers: TimerScheduler[Command]) {\n\n  private def queryBehavior(): Behavior[Command] = {\n    timers.startSingleTimer(CollectionTimeout, CollectionTimeout, timeout)\n    val respondTemperatureAdapter = context.messageAdapter(WrappedRespondTemperature.apply)\n    deviceIdToActor.foreach {\n      case (deviceId, device) =>\n        context.watchWith(device, DeviceTerminated(deviceId))\n        device ! Device.ReadTemperature(0, respondTemperatureAdapter)\n    }\n    waitingForReplies(deviceIdToActor.keySet, Map.empty)\n  }\n\n  // ... rest of the implementation\n}\n```\n\n----------------------------------------\n\nTITLE: Creating FileIO Sink for Writing ByteStrings to File in Java\nDESCRIPTION: This snippet shows how to create a sink using FileIO.toPath in Java to write ByteStrings to a file. It demonstrates the creation of a file path and the sink, which is then used in a stream to write data from a source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/FileIO/toPath.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.stream.javadsl.*;\nimport akka.util.ByteString;\nimport java.nio.file.Paths;\n\nfinal Path file = Paths.get(\"example.txt\");\nfinal Sink<ByteString, CompletionStage<IOResult>> sink = FileIO.toPath(file);\n\nSource.single(ByteString.fromString(\"Hello Akka Streams!\"))\n  .runWith(sink, system);\n```\n\n----------------------------------------\n\nTITLE: Watching Actor Termination with Custom Message in Akka Typed (Java)\nDESCRIPTION: Java version of using watchWith to monitor actor termination and receive a custom message, allowing for additional context to be included in the termination message.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nclass Master {\n  interface Command {}\n\n  public static class StartJob implements Command {\n    public final int jobId;\n    public final ActorRef<JobResult> replyToWhenDone;\n    public StartJob(int jobId, ActorRef<JobResult> replyToWhenDone) {\n      this.jobId = jobId;\n      this.replyToWhenDone = replyToWhenDone;\n    }\n  }\n\n  private static class JobTerminated implements Command {\n    public final int jobId;\n    public final ActorRef<JobResult> replyToWhenDone;\n    public JobTerminated(int jobId, ActorRef<JobResult> replyToWhenDone) {\n      this.jobId = jobId;\n      this.replyToWhenDone = replyToWhenDone;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(\n        context -> {\n          return Behaviors.receive(Command.class)\n              .onMessage(\n                  StartJob.class,\n                  message -> {\n                    ActorRef<Worker.Command> worker =\n                        context.spawn(Worker.create(), \"worker-\" + message.jobId);\n                    context.watchWith(\n                        worker, new JobTerminated(message.jobId, message.replyToWhenDone));\n                    worker.tell(new Worker.DoWork(message.jobId));\n                    return Behaviors.same();\n                  })\n              .onMessage(\n                  JobTerminated.class,\n                  message -> {\n                    System.out.println(\"Worker for job \" + message.jobId + \" finished\");\n                    message.replyToWhenDone.tell(new JobResult(message.jobId));\n                    return Behaviors.same();\n                  })\n              .build();\n        });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing with InitialStateAsEvents in Java\nDESCRIPTION: Java implementation for subscribing to cluster events using initialStateAsEvents mode, which provides initial events corresponding to current state instead of a single CurrentClusterState object.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\ncluster.subscribe(getSelf(), ClusterEvent.initialStateAsEvents(),\n    MemberEvent.class, UnreachableMember.class);\n```\n\n----------------------------------------\n\nTITLE: Scala alsoToAll Signature\nDESCRIPTION: Method signature for alsoToAll operator in Scala that accepts multiple Graph[SinkShape[Out]] arguments and returns the same type as the Flow/Source it's called on\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/alsoToAll.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nalsoToAll(that: akka.stream.Graph[akka.stream.SinkShape[Out],_]*): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Running KillrWeather Cluster Nodes with SBT - Bash\nDESCRIPTION: This set of bash code snippets demonstrates how to start Akka Cluster nodes for the KillrWeather application using sbt. Each command runs the application entry point on a specific port (or a random port when 0 is passed). This approach allows bootstrapping single or multiple actor systems either within a single JVM or across several JVMs for full-fledged cluster testing. The server must be correctly configured via application.conf before running these commands.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-scala/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsbt killrweather/run\n```\n\nLANGUAGE: bash\nCODE:\n```\nsbt \"killrweather/runMain sample.killrweather.KillrWeather 2553\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nsbt \"killrweather/runMain sample.killrweather.KillrWeather 2554\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nsbt \"killrweather/runMain sample.killrweather.KillrWeather 0\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Code Discipline in sbt for Akka - shell\nDESCRIPTION: This snippet shows how to set the 'akka.no.discipline' system property when launching sbt to temporarily opt out of Akka's strict code quality flags. Useful during early development or exploration phases; discipline warnings and checks are bypassed if any non-empty string is provided. Requires sbt and Akka project setup. Note that this should be enabled again before PR submission, as PR validation expects discipline flags active.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\nsbt -Dakka.no.discipline=youbet\n```\n\n----------------------------------------\n\nTITLE: Mutable State Handling with Lazy Flow in Scala\nDESCRIPTION: Shows how to safely handle mutable state using lazyFlow by creating an ArrayList within the flow factory.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/lazyFlow.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 3)\n  .via(\n    Flow.lazyFlow { () =>\n      // safe to create a new mutable object here\n      val list = new ArrayList[Int]()\n      Flow[Int].fold(list) { (l, element) =>\n        l.add(element)\n        l\n      }\n    })\n  .run()\n```\n\n----------------------------------------\n\nTITLE: Wrapping EventSourcedBehavior with Actor Setup in Akka\nDESCRIPTION: Shows how to wrap an EventSourcedBehavior in Behaviors.setup to access the ActorContext. This allows logging and other context-dependent operations during event sourcing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_41\n\nLANGUAGE: Scala\nCODE:\n```\nBehaviors.setup[Command] { ctx =>\\n  ctx.log.debug(\"Starting\")\\n  EventSourcedBehavior[Command, Event, State](\\n    persistenceId = persistenceId,\\n    emptyState = State.empty,\\n    commandHandler = (state, cmd) => Effect.persist(???),\\n    eventHandler = (state, evt) => ???).snapshotAdapter(new SnapshotAdapter {\\n      def toJournal(state: State): Any = state\\n      def fromJournal(from: Any): Option[State] = {\\n        ctx.log.debug2(\"Converting snapshot: {}\", from)\\n        from match {\\n          case s: State => Some(s)\\n          case _ => None\\n        }\\n      }\\n    })\\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nBehaviors.setup(ctx -> {\\n  ctx.getLog().debug(\"Starting\");\\n  return EventSourcedBehavior.create(\\n      PersistenceId.ofUniqueId(\"abc\"),\\n      State.empty(),\\n      (state, command) -> Effect().persist(commandToEvent(command)),\\n      (state, event) -> handleEvent(state, event))\\n    .snapshotAdapter(\\n      new SnapshotAdapter<State>() {\\n        public Object toJournal(State state) {\\n          return state;\\n        }\\n\\n        public Optional<State> fromJournal(Object from) {\\n          ctx.getLog().debug(\"Converting snapshot: {}\", from);\\n          return Optional.ofNullable((State) from);\\n        }\\n      });\\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Elements to GSet using Akka Distributed Data (Java)\nDESCRIPTION: Provides a Java example of using Akka's GSet CRDT for distributed grow-only set operations. The snippet shows how to add serialized values; merges occur via union. Dependencies: Akka Distributed Data for Java, running in a cluster; supports adding new elements as inputs and outputs the distributed set state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n// Java Example: Add to GSet\nGSetKey<String> setKey = GSetKey.create(\"exampleSet\");\ndistributedData.tell(new Update<>(setKey, GSet.create(), WriteLocal.instance(), curr -> curr.add(\"foo\")), self());\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Actor Deployment with Dispatcher in HOCON\nDESCRIPTION: HOCON configuration that defines a dispatcher for a specific actor path. This is used for deployment configuration to associate an actor with a custom dispatcher.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_4\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.deployment {\n  /my-service {\n    dispatcher = my-dispatcher\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Subscription Timeout Attribute in Akka StreamRefs - Scala\nDESCRIPTION: This Scala code snippet demonstrates how to set a custom stream reference subscription timeout attribute for a Flow in Akka Streams. The attribute allows fine-grained control per-stream instance over the duration a remote node has to materialize a referenced stream before timing out, overriding the global default set in configuration. Required dependencies include akka-stream and akka-actor-typed; modify the attribute using the .addAttributes method with ActorAttributes.subscriptionTimeout. Expected input/output: a Flow with an overridden subscription timeout constraint. Ensure to handle the timeout exception accordingly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-refs.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\nflow.addAttributes(ActorAttributes.subscriptionTimeout(FiniteDuration(10, TimeUnit.SECONDS)))\n```\n\n----------------------------------------\n\nTITLE: Setting Event Storage Policy in Java\nDESCRIPTION: Example demonstrating how to set a custom event storage policy in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n\"#set-event-storage-policy\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Pool Router for Akka Cluster\nDESCRIPTION: Configuration for a cluster-aware pool router using consistent hashing strategy. The router creates and deploys up to 3 routees per node with the \"compute\" role.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_10\n\nLANGUAGE: properties\nCODE:\n```\nakka.actor.deployment {\n  /statsService/singleton/workerRouter {\n      router = consistent-hashing-pool\n      cluster {\n        enabled = on\n        max-nr-of-instances-per-node = 3\n        allow-local-routees = on\n        use-roles = [\"compute\"]\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Singleton Message Classes\nDESCRIPTION: Example code showing how to define message classes for cluster singleton actor communication.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-singleton.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\ncase object End\ncase object GetCurrent\ncase class Response(address: Address)\n```\n\nLANGUAGE: Java\nCODE:\n```\npublic interface TestSingletonMessages {\n  public static End end() { return End.INSTANCE; }\n  public static GetCurrent getCurrent() { return GetCurrent.INSTANCE; }\n  public static Response response(Address address) { return new Response(address); }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Buffering for NACK-Based Write Back-Pressure in Java\nDESCRIPTION: The buffering state of the Java EchoHandler that manages write failures. It buffers data when writing is suspended and resumes writing when the connection is ready to accept more data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nprivate Receive buffering(final List<ByteString> buffer) {\n  return receiveBuilder()\n      .match(\n          Tcp.Received.class,\n          msg -> {\n            final ByteString data = msg.data();\n            buffer.add(data);\n            connection.tell(TcpMessage.write(data, Ack.getInstance()), getSelf());\n            return this;\n          })\n      .match(\n          Tcp.CommandFailed.class,\n          msg -> {\n            connection.tell(TcpMessage.resumeWriting(), getSelf());\n            return waitingForAck(buffer);\n          })\n      .match(\n          Ack.class,\n          msg -> {\n            if (buffer.isEmpty()) {\n              return createReceive();\n            } else {\n              connection.tell(\n                  TcpMessage.write(buffer.remove(0), Ack.getInstance()), getSelf());\n              return buffering(buffer);\n            }\n          })\n      .match(\n          Tcp.ConnectionClosed.class,\n          msg -> {\n            getContext().stop(getSelf());\n            return Stay();\n          })\n      .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Scala Source RecoverWithRetries Signature\nDESCRIPTION: API signature for recoverWithRetries method on Source class. Takes number of attempts and a partial function mapping throwables to alternative sources.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recoverWithRetries.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nrecoverWithRetries[T>:Out](attempts:Int,pf:PartialFunction[Throwable,akka.stream.Graph[akka.stream.SourceShape[T],akka.NotUsed]]):FlowOps.this.Repr[T]\n```\n\n----------------------------------------\n\nTITLE: Actor System Shutdown in Testing - Scala\nDESCRIPTION: Displays Scala code for properly shutting down the ActorSystem in ActorTestKit tests, ensuring that resources are released and the system is properly terminated after tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AsyncTestingExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/AsyncTestingExampleSpec.scala) { #test-shutdown }\n```\n\n----------------------------------------\n\nTITLE: Forward-Compatible ItemAdded Event Class in Java\nDESCRIPTION: Java code snippet showing the ItemAdded event class during the first deployment of a rolling update, with forward compatibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ItemAdded.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v1/ItemAdded.java) { #forward-one-rename }\n```\n\n----------------------------------------\n\nTITLE: Custom Configuration in Tests - Java\nDESCRIPTION: Java configuration snippet for ActorTestKit, detailing how to specify different configurations and their advantages in customizing test environments.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [TestConfigExample.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/TestConfigExample.java) { #default-application-conf }\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Messages in Akka Pool Router using Java\nDESCRIPTION: This snippet illustrates how to configure a pool router in Java to broadcast messages to all routees using a broadcast predicate. Ensure that Akka Typed is correctly configured in your Java project.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n/* Broadcasting configuration for pool router in RouterTest.java */\n```\n\n----------------------------------------\n\nTITLE: Source fromMaterializer API Definition in Scala\nDESCRIPTION: Scala API signature for Source.fromMaterializer that takes a factory function accepting Materializer and Attributes to create a Source\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/fromMaterializer.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nfromMaterializer[T,M](factory:(akka.stream.Materializer,akka.stream.Attributes)=>akka.stream.scaladsl.Source[T,M]):akka.stream.scaladsl.Source[T,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: Leveraging Java 21 Features for Event Sourcing\nDESCRIPTION: Demonstrates how to use Java 21's switch pattern matching and sealed types to create more concise and type-safe event-sourced behaviors in Akka Persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-style.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AccountBehavior.java](/akka-persistence-typed-tests/src/test/java-21+/jdocs21/akka/persistence/typed/javadsl/AccountBehavior.java) { #account-behavior }\n```\n\n----------------------------------------\n\nTITLE: Declaring prefixAndTail Operator for Akka Source in Java\nDESCRIPTION: Defines the prefixAndTail operator for Akka Source in Java. It takes an integer parameter specifying the number of elements to take as prefix.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/prefixAndTail.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.prefixAndTail(int)\n```\n\n----------------------------------------\n\nTITLE: Creating Akka Source from Java Stream (Scala)\nDESCRIPTION: Demonstrates how to create an Akka Stream `Source` from a Java 8 `java.util.stream.Stream` in Scala. It imports necessary classes (`akka.stream.scaladsl.StreamConverters`, `java.util.stream.Stream`) and uses `StreamConverters.fromJavaStream` with a lambda function `() =>` that provides the Java Stream instance (`Stream.of(1, 2, 3)`).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/fromJavaStream.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n//##import\nimport akka.stream.scaladsl.StreamConverters\nimport java.util.stream.Stream\n//##import\n\n//##fromJavaStream\nval source: Source[Int, NotUsed] = StreamConverters.fromJavaStream(() => Stream.of(1, 2, 3))\n//##fromJavaStream\n\n```\n\n----------------------------------------\n\nTITLE: Selecting Local Actors in Akka\nDESCRIPTION: Demonstrates how to select local actors using absolute and relative paths in Akka. The selection returns an ActorSelection that can be used to send messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\ncontext.actorSelection(\"/user/serviceA/actor\")\ncontext.actorSelection(\"../joe\")\n```\n\nLANGUAGE: Java\nCODE:\n```\ngetContext().actorSelection(\"/user/serviceA/actor\");\ngetContext().actorSelection(\"../joe\");\n```\n\n----------------------------------------\n\nTITLE: Converting Akka Stream to InputStream in Scala\nDESCRIPTION: Creates a sink that reads content from a source, converts it to uppercase, and materializes into a Java InputStream. The example demonstrates the usage of StreamConverters.asInputStream with transformation operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/asInputStream.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(List(\"chunk\"))\nval sink = StreamConverters.asInputStream()\nval inputStream = source\n  .map(s => ByteString(s.toUpperCase))\n  .runWith(sink)\n```\n\n----------------------------------------\n\nTITLE: Defining Message Classes for Type Collection in Java\nDESCRIPTION: Java implementation of the class hierarchy with Message class and Ping subclass, along with unrelated Pong class.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collectType.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#collect-elements\n```\n\n----------------------------------------\n\nTITLE: Configuring Distributed Data Mode for Akka Cluster Sharding State Store\nDESCRIPTION: Configuration snippet for enabling distributed data mode (the default) as the state store for Akka Cluster Sharding. This mode uses Akka Distributed Data to replicate shard coordinator state across the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_42\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.state-store-mode = ddata\n```\n\n----------------------------------------\n\nTITLE: Scheduling One-Off Function in Scala\nDESCRIPTION: Schedule a function to execute once after 50ms delay, sending current time to testActor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/scheduler.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nsystem.scheduler.scheduleOnce(50.milliseconds) {\n  testActor ! System.currentTimeMillis\n}(system.dispatcher)\n```\n\n----------------------------------------\n\nTITLE: Dropping Broadcast in Akka Streams (Java)\nDESCRIPTION: Java implementation of a broadcast that drops elements for slow consumers. Uses buffer with OverflowStrategy.dropHead.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_42\n\nLANGUAGE: Java\nCODE:\n```\npublic Graph<UniformFanOutShape<Integer, Integer>, NotUsed> createDroppyBroadcast() {\n    return GraphDSL.create(builder -> {\n        final int outputPorts = 2;\n        final UniformFanOutShape<Integer, Integer> bcast = builder.add(Broadcast.create(outputPorts));\n\n        final Flow<Integer, Integer, NotUsed> droppyFlow1 =\n                Flow.of(Integer.class).buffer(10, OverflowStrategy.dropHead());\n        final Flow<Integer, Integer, NotUsed> droppyFlow2 =\n                Flow.of(Integer.class).buffer(10, OverflowStrategy.dropHead());\n\n        builder.from(bcast.out(0)).via(builder.add(droppyFlow1));\n        builder.from(bcast.out(1)).via(builder.add(droppyFlow2));\n\n        return new UniformFanOutShape<>(bcast.in(), bcast.out(0), bcast.out(1));\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Using Log Operator in Java Streams\nDESCRIPTION: Shows the implementation of the log operator in Java for Akka streams. The operator provides logging capabilities for stream elements and events with customizable log levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/log.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#log\n```\n\n----------------------------------------\n\nTITLE: Handling Optional Fields when Deserializing Protobuf Messages in Akka Persistence (Java)\nDESCRIPTION: Demonstrates deserialization of a Protobuf event in Java, checking for the existence of an optional field and applying a default value or Optional empty if missing. Uses Protobuf-generated types and is designed for schema evolution scenarios in Akka Persistence projects. Maintains binary compatibility while evolving the schema.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nFlightAppModels.SeatReserved event = FlightAppModels.SeatReserved.parseFrom(bytes);\nOptional<String> seatType = event.hasSeatType() ? Optional.of(event.getSeatType()) : Optional.empty();\n```\n\n----------------------------------------\n\nTITLE: Defining Dispatcher in Actor Configuration in Scala\nDESCRIPTION: Shows how to create an actor and define its dispatcher through configuration in Scala. The dispatcher is specified in the deployment configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nval myActor = context.actorOf(Props[MyActor], \"myactor\")\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for Akka Actor Model\nDESCRIPTION: Detailed markdown documentation explaining the Actor model implementation in Akka, including core concepts like Actor References, State, Behavior, Mailbox, Child Actors, and Supervisor Strategy. The document provides in-depth explanations of how actors communicate, maintain state, and handle failures in a distributed computing environment.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/actors.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# What is an Actor?\n\nThe Actor Model as defined by Hewitt, Bishop and Steiger in 1973 is a computational model that expresses exactly what it means for computation to be distributed. The processing units—Actors—can only communicate by exchanging messages and upon reception of a message an Actor can do the following three fundamental actions:\n\n  1. send a finite number of messages to Actors it knows\n  2. create a finite number of new Actors\n  3. designate the behavior to be applied to the next message\n```\n\n----------------------------------------\n\nTITLE: Serializer Configuration in Scala\nDESCRIPTION: Configuration code for registering the TwoPhaseSet serializer in the Akka configuration, showing how to bind the serializer to a specific data type.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_31\n\nLANGUAGE: scala\nCODE:\n```\nval config = ConfigFactory.parseString(\"\"\"\n    akka.actor {\n      serializers {\n        twophaseset = \"docs.ddata.protobuf.TwoPhaseSetSerializer\"\n      }\n      serialization-bindings {\n        \"docs.ddata.TwoPhaseSet\" = twophaseset\n      }\n    }\n    \"\"\")\n```\n\n----------------------------------------\n\nTITLE: Implementing ByteBufferSerializer with String Manifest in Java\nDESCRIPTION: This snippet shows how to implement a ByteBufferSerializer with a string manifest in Java, including delegation to array-based methods for compatibility with non-ByteBuffer scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\npublic class MySerializer extends ByteBufferSerializer implements SerializerWithStringManifest {\n  private static final Charset UTF_8 = StandardCharsets.UTF_8;\n\n  @Override\n  public String manifest(Object o) {\n    return \"[class manifest]\";\n  }\n\n  // implement ByteBufferSerializer methods here\n\n  @Override\n  public byte[] toBinary(Object o) {\n    ByteBuffer buf = ByteBuffer.allocate(1024); // if you know the size you can use allocate\n    toBinary(o, buf);\n    return ByteBufferSerializer.getBytes(buf);\n  }\n\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) {\n    return fromBinary(ByteBuffer.wrap(bytes), manifest);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Sink.foreach in Scala\nDESCRIPTION: Example demonstrating how to use Sink.foreach to print each element from a stream to standard output in Scala. The sink materializes into a Future[Done] which completes when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/foreach.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 3).map(_.toString).runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Remote Routee Deployment in Scala\nDESCRIPTION: Example demonstrating how to deploy routees on remote hosts using RemoteRouterConfig\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RouterDocSpec.scala](/akka-docs/src/test/scala/docs/routing/RouterDocSpec.scala) { #remoteRoutees }\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Cluster Events in Java\nDESCRIPTION: Java implementation of an actor that subscribes to cluster membership events and logs them. Demonstrates the pattern for creating a listener in Java that reacts to cluster topology changes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.AbstractActor;\nimport akka.cluster.Cluster;\nimport akka.cluster.ClusterEvent;\nimport akka.cluster.ClusterEvent.MemberEvent;\nimport akka.cluster.ClusterEvent.MemberRemoved;\nimport akka.cluster.ClusterEvent.MemberUp;\nimport akka.cluster.ClusterEvent.UnreachableMember;\nimport akka.event.Logging;\nimport akka.event.LoggingAdapter;\n\npublic class SimpleClusterListener extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n  Cluster cluster = Cluster.get(getContext().getSystem());\n\n  //subscribe to cluster changes\n  @Override\n  public void preStart() {\n    cluster.subscribe(getSelf(), ClusterEvent.initialStateAsEvents(),\n        MemberEvent.class, UnreachableMember.class);\n  }\n\n  //re-subscribe when restart\n  @Override\n  public void postStop() {\n    cluster.unsubscribe(getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(MemberUp.class, mUp -> {\n          log.info(\"Member is Up: {}\", mUp.member().address());\n        })\n        .match(UnreachableMember.class, mUnreachable -> {\n          log.info(\"Member detected as unreachable: {}\", mUnreachable.member());\n        })\n        .match(MemberRemoved.class, mRemoved -> {\n          log.info(\"Member is Removed: {} after {}\",\n              mRemoved.member().address(), mRemoved.previousStatus());\n        })\n        .match(MemberEvent.class, message -> {\n          // ignore\n        })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Transforming incoming elements with contramap in Akka Streams (Scala)\nDESCRIPTION: This example demonstrates how to use the contramap operator to transform incoming elements in an Akka Streams Flow. It applies a function to each upstream element before it is passed to the Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/contramap.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.stream.scaladsl.Flow\n\nval flow = Flow[Int].map(_ * 2)\nval contraFlow = flow.contramap[String](_.toInt)\n```\n\n----------------------------------------\n\nTITLE: Creating a Tweet Source in Scala\nDESCRIPTION: Defines a Source of tweets for processing in Scala Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\nval tweets: Source[Tweet, NotUsed] = Source(???) // We'll assume that we got a Source of tweets from somewhere\n```\n\n----------------------------------------\n\nTITLE: Extracting ShardId from StartEntity in Scala\nDESCRIPTION: Demonstrates how to extract a ShardId from an EntityId when handling Shard.StartEntity messages for remembered entities in Akka Cluster Sharding.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [ClusterShardingSpec.scala](/akka-cluster-sharding/src/multi-jvm/scala/akka/cluster/sharding/ClusterShardingSpec.scala) { #extractShardId-StartEntity }\n```\n\n----------------------------------------\n\nTITLE: Logging Elements in Streams using println (Akka Streams Java)\nDESCRIPTION: Illustrates the use of map with println in a Java-based Akka Streams flow to output elements for diagnostic purposes. Particularly suitable for Java developers needing to debug streaming pipelines. No special dependencies outside Akka Streams; prints data as it flows through the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeLoggingElements.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeLoggingElements.java) { #println-debug }\n```\n\n----------------------------------------\n\nTITLE: Using initialDelay in Akka Streams (Java)\nDESCRIPTION: API signature for the initialDelay operator in Java, which takes a java.time.Duration parameter to delay the initial element in a Source or Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/initialDelay.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#initialDelay(java.time.Duration)\n```\n\n----------------------------------------\n\nTITLE: Reusing RunnableGraph in Akka Streams (Java)\nDESCRIPTION: Shows how a RunnableGraph can be reused and materialized multiple times in Java, producing different results for each materialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_41\n\nLANGUAGE: Java\nCODE:\n```\nfinal Sink<Integer, CompletionStage<Integer>> sumSink =\n    Sink.fold(0, (acc, elem) -> acc + elem);\n\nfinal RunnableGraph<CompletionStage<Integer>> counterRunnableGraph =\n    tweetsInMinuteFromNow\n        .filter(t -> t.hashtags().contains(AKKA_TAG))\n        .map(t -> 1)\n        .toMat(sumSink, Keep.right());\n\n// materialize the stream once in the morning\nCompletionStage<Integer> morningTweetsCount = counterRunnableGraph.run(mat);\nmorningTweetsCount.thenAccept(\n    c -> System.out.println(\"Tweets counted in the morning: \" + c));\n\n// and once in the evening, reusing the stream blueprint\nCompletionStage<Integer> eveningTweetsCount = counterRunnableGraph.run(mat);\neveningTweetsCount.thenAccept(\n    c -> System.out.println(\"Tweets counted in the evening: \" + c));\n```\n\n----------------------------------------\n\nTITLE: Expected Log Output for Kafka Server Start (Text)\nDESCRIPTION: This log snippet shows the expected output in the console when the embedded Kafka server starts successfully using the `sbt \"kafka / run\"` command. It confirms that Kafka is running on `localhost:9092` and the `user-events` topic with 128 partitions has been created.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n12:06:59.711 INFO  [run-main-0          ] s.s.embeddedkafka.KafkaBroker$        Kafka running: localhost:9092\n12:06:59.711 INFO  [run-main-0          ] s.s.embeddedkafka.KafkaBroker$        Topic 'user-events' with 128 partitions created\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Supervisor Strategy\nDESCRIPTION: Shows how to implement a basic supervisor actor that handles different types of failures with specific directives.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FaultHandlingDocSpec.scala](/akka-docs/src/test/scala/docs/actor/FaultHandlingDocSpec.scala) { #supervisor }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FaultHandlingTest.java](/akka-docs/src/test/java/jdocs/actor/FaultHandlingTest.java) { #supervisor }\n```\n\n----------------------------------------\n\nTITLE: Custom Configuration in Tests - Scala\nDESCRIPTION: Demonstrates configuring the ActorTestKit using a custom application.conf or other configurations in Scala. The approach allows dynamic configuration setup for specific tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [TestConfigExample.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/TestConfigExample.scala) { #default-application-conf }\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshot Predicate in Akka\nDESCRIPTION: Defines a predicate for when to take snapshots based on the current state and event.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-snapshot.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\n.withSnapshotWhen((state, event, sequenceNr) => event == BookingCompleted)\n```\n\n----------------------------------------\n\nTITLE: Handling Snapshot Deletion Signals in Akka\nDESCRIPTION: Demonstrates how to handle signals for successful or failed snapshot deletions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-snapshot.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n.receiveSignal {\n  case (state, DeleteSnapshotsCompleted) =>\n    effects.none\n  case (state, DeleteSnapshotsFailed(metadata, error)) =>\n    effects.none\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Distributed Data in Scala\nDESCRIPTION: Scala example of deleting data from distributed storage using Akka's Replicator. It demonstrates how to perform a delete operation with proper consistency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_28\n\nLANGUAGE: Scala\nCODE:\n```\nimplicit val timeout = Timeout(5.seconds)\\nreplicator ! Delete(dataKey, WriteLocal, request.replyTo)\n```\n\n----------------------------------------\n\nTITLE: Drop Head Buffer Strategy in Akka Streams\nDESCRIPTION: Creates a buffer that drops the oldest element when full. Ideal for scenarios where jobs can be retransmitted and older elements are less important.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\nexternalService.runWith(\n  Flow[Job].buffer(1000, OverflowStrategy.dropHead)\n)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(externalService)\n  .buffer(1000, OverflowStrategy.dropHead())\n  .run(system)\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor preStart Lifecycle Hook in Akka (Java)\nDESCRIPTION: Demonstrates overriding the preStart method in a Java Akka actor to perform initialization tasks when the actor is started. Requires extending AbstractActor and overriding preStart(). Used for resource allocation or setup logic that should run before an actor starts message processing. No input parameters; typically no direct output, but essential for runtime environment preparation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.AbstractActor;\n\npublic class MyActor extends AbstractActor {\n  @Override\n  public void preStart() {\n    // custom initialization code\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().build();\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Exception Handling with Ask Pattern - Java\nDESCRIPTION: Example of handling exceptions when using the ask pattern in Java by sending Status.Failure messages\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_29\n\nLANGUAGE: java\nCODE:\n```\n#reply-exception\n```\n\n----------------------------------------\n\nTITLE: Testing Exceptions in Akka Actors with TestActorRef in Java\nDESCRIPTION: Shows how to test exception handling in Java actors using TestActorRef. The example demonstrates catching exceptions thrown during message processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_42\n\nLANGUAGE: java\nCODE:\n```\nfinal TestActorRef<MyActor> actorRef = TestActorRef.create(system, Props.create(MyActor.class));\ntry {\n    actorRef.receive(\"unknown\");\n    fail(\"expected an exception to be thrown\");\n} catch (Exception e) {\n    // expected\n}\n```\n\n----------------------------------------\n\nTITLE: Actor Lifecycle Monitoring API Methods (Scala)\nDESCRIPTION: API methods for watching and unwatching actor lifecycle changes in Akka. These methods are used to monitor actor termination and receive Terminated messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/supervision.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nActorContext.watch[U](other:akka.actor.typed.ActorRef[U]):Unit\nActorContext.unwatch[U](other:akka.actor.typed.ActorRef[U]):Unit\n```\n\n----------------------------------------\n\nTITLE: Skipping Deleted Events in Akka Persistence Serializer (Java)\nDESCRIPTION: Java implementation of a SerializerWithStringManifest that skips deserialization of deleted event types in Akka Persistence. This optimizes recovery by avoiding unnecessary deserialization of removed events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\npublic class EventDeserializationSkipped {\n    public final String manifest;\n    public EventDeserializationSkipped(String manifest) {\n        this.manifest = manifest;\n    }\n}\n\npublic class RemovedEventsAwareSerializer extends SerializerWithStringManifest {\n    private final Set<String> removedTypes = new HashSet<>(Arrays.asList(\"customer-blinked\", \"customer-ignored\"));\n\n    @Override\n    public String manifest(Object o) {\n        return o.getClass().getName();\n    }\n\n    @Override\n    public Object fromBinary(byte[] bytes, String manifest) {\n        if (removedTypes.contains(manifest)) {\n            return new EventDeserializationSkipped(manifest);\n        } else {\n            // actual deserialization\n            ...\n        }\n    }\n\n    @Override\n    public byte[] toBinary(Object o) {\n        // implementation\n        ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Singleton Supervisor\nDESCRIPTION: Implementation of a supervisor actor for the cluster singleton to handle failure scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-singleton.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nclass SupervisorActor(childProps: Props) extends Actor {\n  override val supervisorStrategy = OneForOneStrategy() {\n    case _: Exception => Restart\n  }\n  override def receive = {\n    case \"stop\" => context.stop(self)\n    case other => context.child(\"supervised-singleton\") match {\n      case Some(child) => child.forward(other)\n      case None =>\n        val child = context.actorOf(childProps, name = \"supervised-singleton\")\n        child.forward(other)\n    }\n  }\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\npublic class SupervisorActor extends AbstractActor {\n  private final Props childProps;\n\n  public SupervisorActor(Props childProps) {\n    this.childProps = childProps;\n  }\n\n  private final SupervisorStrategy strategy = new OneForOneStrategy(\n    DeciderBuilder.match(Exception.class, e -> SupervisorStrategy.restart())\n      .build());\n\n  @Override\n  public SupervisorStrategy supervisorStrategy() {\n    return strategy;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n      .matchEquals(\"stop\", s -> context().stop(self()))\n      .matchAny(msg -> {\n        Optional<ActorRef> child = Optional.ofNullable(\n          context().child(\"supervised-singleton\"));\n        if (child.isPresent())\n          child.get().forward(msg, context());\n        else {\n          ActorRef supervisor = context().actorOf(\n            childProps, \"supervised-singleton\");\n          supervisor.forward(msg, context());\n        }\n      })\n      .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Empty Iterator Error in Java\nDESCRIPTION: Example demonstrating error handling when Source.cycle receives an empty iterator in Java, resulting in an IllegalArgumentException.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/cycle.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n#cycle-error\n```\n\n----------------------------------------\n\nTITLE: Markdown Index Structure for Akka Classic Networking\nDESCRIPTION: Markdown structure defining the documentation layout for Akka's classic networking components, including includes, TOC directives and links to subsections for IO, TCP, UDP and DNS documentation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-network.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Classic Networking\n\n@@include[includes.md](includes.md) { #actor-api }\nFIXME https://github.com/akka/akka/issues/27263\n\n@@toc { depth=2 }\n\n@@@ index\n\n* [io](io.md)\n* [io-tcp](io-tcp.md)\n* [io-udp](io-udp.md)\n* [io-dns](io-dns.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: ActorRef Serialization Implementation - Scala\nDESCRIPTION: Implementation of ActorRef serialization and deserialization logic in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-classic.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n#actorref-serializer\n```\n\n----------------------------------------\n\nTITLE: Performing Update Operation on Distributed Data in Java\nDESCRIPTION: Java example of sending an Update message to the Replicator to modify and replicate a data value. This demonstrates how to increment a counter in a distributed data structure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nKey<PNCounter> Counter1Key = PNCounterKey.create(\"counter1\");\nreplicator.tell(\n    new Update<PNCounter>(\n        Counter1Key,\n        PNCounter.create(),\n        WriteLocal.instance(),\n        curr -> curr.increment(node, 1)),\n    getSelf());\n```\n\n----------------------------------------\n\nTITLE: Implementing Fibonacci Consumer Actor in Scala\nDESCRIPTION: Example of a consumer actor that receives Fibonacci numbers through a ConsumerController and processes them reliably.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/reliable-delivery.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nobject FibonacciConsumer {\n  sealed trait Command\n  private case class WrappedDelivery(delivery: ConsumerController.Delivery[Int]) extends Command\n\n  def apply(): Behavior[Command] = {\n    Behaviors.setup { context =>\n      val deliveryAdapter: ActorRef[ConsumerController.Delivery[Int]] =\n        context.messageAdapter(WrappedDelivery.apply)\n\n      Behaviors.receiveMessage {\n        case WrappedDelivery(delivery) =>\n          context.log.info(\n            \"Consumer received {}, confirming\", \n            delivery.message)\n          delivery.confirmTo ! ConsumerController.Confirmed\n          Behaviors.same\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Java Reactive Database Integration Example\nDESCRIPTION: Example showing how to use Source.asSubscriber to integrate with a reactive database client in Java, demonstrating backpressure-aware streaming of database rows.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/asSubscriber.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nimport akka.stream.javadsl.JavaFlowSupport;\nimport java.util.concurrent.Flow;\n\nReactiveDB db = getDatabase();\n\nSource<String, Flow.Subscriber<Row>> rowSource =\n    JavaFlowSupport.Source.<Row>asSubscriber()\n        .map(row -> row.getString(\"name\"));\n\n// Each materialization will query the database\nFlow.Subscriber<Row> subscriber = rowSource.run(materializer);\ndb.query(\"SELECT * FROM TABLES\", subscriber);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating a Cycle with Materialized Value Feedback in Akka Streams (Scala)\nDESCRIPTION: This Scala snippet demonstrates a problematic pattern where the materialized value (e.g., a Future produced by a Sink fold) is fed back into the same computation, creating a potential cycle. This is intended as an anti-pattern. Dependency is akka.stream.scaladsl; key constraint is not to introduce feedback loops involving materialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_19\n\nLANGUAGE: Scala\nCODE:\n```\nval cycle = GraphDSL.create() { implicit b =>\n  val mat = b.materializedValue[Future[Int]]\n  // Use mat as input for a fold that would also produce this Future\n  // ...\n}\n\n```\n\n----------------------------------------\n\nTITLE: Creating RoundRobinGroup Programmatically (Scala/Java)\nDESCRIPTION: Demonstrates creating a RoundRobinGroup router actor programmatically. It specifies the round-robin routing logic and provides the list of routee actor paths directly in the code, instead of relying on external configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n//#round-robin-group-2\nimport akka.routing.RoundRobinGroup\n\nval router2: ActorRef = context.actorOf(RoundRobinGroup(paths).props(), \"router2\")\n//#round-robin-group-2\n```\n\nLANGUAGE: java\nCODE:\n```\n//#round-robin-group-2\nimport akka.routing.RoundRobinGroup;\n\nfinal ActorRef router2 = getContext().actorOf(new RoundRobinGroup(paths).props(), \"router2\");\n//#round-robin-group-2\n```\n\n----------------------------------------\n\nTITLE: Logging Stream Errors (Java)\nDESCRIPTION: Demonstrates logging errors in an Akka Stream using the `log()` operator in Java. The stream processes integers, intentionally causing an `ArithmeticException` when dividing by zero, which is then logged.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nfinal ActorSystem system = ActorSystem.create(\"log-error\");\n\nSource.from(Arrays.asList(0, 1, 2, 3, 4, 5))\n    .map(\n        n -> {\n          return 100 / n;\n        })\n    .log(\"error logging\")\n    .runWith(Sink.ignore(), system);\n```\n\n----------------------------------------\n\nTITLE: Defining Auction Commands in Scala\nDESCRIPTION: Command definitions for the auction system including placing bids, getting highest bid, and finishing the auction.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing-auction.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nsealed trait Command\nfinal case class OfferBid(offer: Int, replyTo: ActorRef[Bid]) extends Command\nfinal case class GetHighestBid(replyTo: ActorRef[Bid]) extends Command\ncase object AuctionFinished extends Command\n```\n\n----------------------------------------\n\nTITLE: Logging Elements in Streams using println (Akka Streams Scala)\nDESCRIPTION: Shows how to use the map operation with println to log elements traversing a stream for quick debugging. Useful for development-time introspection but should not be used in production environments. No additional dependencies beyond Akka Streams; simply outputs elements to the console as they pass through.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeLoggingElements.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeLoggingElements.scala) { #println-debug }\n```\n\n----------------------------------------\n\nTITLE: Configuring RandomGroup Router via HOCON\nDESCRIPTION: Defines an Akka RandomGroup router named 'router1' in HOCON. This router randomly selects one of the pre-existing actors specified in 'routees.paths' for each message.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_18\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-random-group\nakka.actor.deployment {\n  /parent/router1 {\n    router = random-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n  }\n}\n//#config-random-group\n```\n\n----------------------------------------\n\nTITLE: Testing Scanning Classification Bus in Scala\nDESCRIPTION: Test code for a Scanning Classification Bus in Scala. It demonstrates how to subscribe with a prefix string and receive all events that start with that prefix.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\nval scanningBus = new ScanningBusImpl\nscanningBus.subscribe(testActor, \"abc\")\nscanningBus.publish(\"abcdef\")\nexpectMsg(\"abcdef\")\n```\n\n----------------------------------------\n\nTITLE: Creating TestActorRef for Synchronous Actor Testing in Scala\nDESCRIPTION: Demonstrates how to create a TestActorRef for synchronous testing of an actor, allowing direct access to the underlying actor instance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_36\n\nLANGUAGE: scala\nCODE:\n```\nval actorRef = TestActorRef[MyActor](Props[MyActor])\nval actor = actorRef.underlyingActor\n```\n\n----------------------------------------\n\nTITLE: Configuring Thread Shutdown Timeouts in Akka Dispatcher\nDESCRIPTION: Configuration settings for controlling thread shutdown behavior in both fork-join-executor and thread-pool-executor implementations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\n#my-dispatcher-with-timeouts-config\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DeadLetters in Java\nDESCRIPTION: Java code showing how to subscribe an actor to receive DeadLetter notifications from the event stream. This is useful for diagnosing message delivery problems.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\nfinal ActorRef actor = system.actorOf(Props.create(DeadLetterActor.class));\nsystem.getEventStream().subscribe(actor, DeadLetter.class);\n```\n\n----------------------------------------\n\nTITLE: Querying Live Persistence IDs Stream in Scala\nDESCRIPTION: Demonstrates how to subscribe to a live stream of all persistence IDs in the system using the persistenceIds query. This stream will continuously emit new persistence IDs as they are created.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval persistenceIds = readJournal.persistenceIds()\n\npersistenceIds.runForeach { id =>\n  println(s\"We have persistence id: $id\")\n}\n```\n\n----------------------------------------\n\nTITLE: Inet-Address DNS Resolution in Java\nDESCRIPTION: Example of using the Inet-Address DNS provider directly through the actor API in Java. Uses Dns.Resolve and Dns.Resolved messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-dns.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n#actor-api-inet-address\n```\n\n----------------------------------------\n\nTITLE: Adding Tags to MDC in Scala\nDESCRIPTION: Demonstrates how to add tags to the Mapped Diagnostic Context (MDC) for an actor in Scala, which can be used for categorizing log entries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nval behavior: Behavior[String] =\n  Behaviors.setup { context =>\n    context.setTags(\"tag1\", \"tag2\")\n    // ...\n  }\n```\n\n----------------------------------------\n\nTITLE: Recovering from Stream Failure (Java)\nDESCRIPTION: Illustrates using the `recover` operator in Java to handle specific exceptions. If an `ArithmeticException` occurs, the stream emits a default value (0) and then completes gracefully. Other exceptions would still fail the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(0, 1, 2, 3, 4, 5, 6))\n    .map(\n        n -> {\n          if (n < 5) return n.toString();\n          else throw new RuntimeException(\"Boom!\");\n        })\n    .recover(\n        RuntimeException.class,\n        () -> {\n          return \"stream truncated\";\n        })\n    .runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Managing FSM Timers in Akka\nDESCRIPTION: Sets up mechanisms within Akka FSM to handle timers. Functions like startSingleTimer, startTimerWithFixedDelay, and cancelTimer allow creation, scheduling, and cancellation of timers. These functions ensure that timers with the same name are replaced and immediately cancel scheduled messages, respectively. Dependencies include Akka actor systems and knowledge of scheduling techniques like fixed-delay and fixed-rate.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\nstartSingleTimer(name, msg, interval)\nstartTimerWithFixedDelay(name, msg, interval)\n```\n\nLANGUAGE: Scala\nCODE:\n```\ncancelTimer(name)\n```\n\nLANGUAGE: Scala\nCODE:\n```\nisTimerActive(name)\n```\n\n----------------------------------------\n\nTITLE: Using ControlAwareMailbox in Scala\nDESCRIPTION: Scala code demonstrating how to create an actor that uses a control-aware mailbox. The actor will prioritize control messages over regular messages regardless of message queue ordering.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\n// We create a new Actor that just prints out what it processes\nval controlAwareActor = system.actorOf(Props[MyActor]().withMailbox(\"control-aware-mailbox\"))\n\n// We fill up the mailbox with messages, and the actor will prioritize MyControlMessage\nfor (i <- 1 to 100) {\n  controlAwareActor ! s\"msg $i\"\n  if (i == 50) controlAwareActor ! MyControlMessage\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Events by Tag in Akka Persistence - Scala\nDESCRIPTION: Example of querying events that are tagged using Akka Persistence in Scala. This demonstrates how to perform queries for events by specific tags and the use of stream operators on the resulting stream. It highlights the use of the optional offset parameter to implement resumable-streams. Dependencies include the Akka Persistence library and a supporting journal.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [PersistenceQueryDocSpec.scala](/akka-docs/src/test/scala/docs/persistence/query/PersistenceQueryDocSpec.scala) { #events-by-tag }\n```\n\n----------------------------------------\n\nTITLE: Error Recovery Output Example - Java\nDESCRIPTION: Displays the expected output when using the recover operator in Java, showing the stream elements before failure and the fallback value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recover.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n1\n2\nfallback\n```\n\n----------------------------------------\n\nTITLE: Creating a Tweet Source in Java\nDESCRIPTION: Defines a Source of tweets for processing in Java Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\nSource<Tweet, NotUsed> tweets = Source.from(Collections.emptySet()); // We'll assume we got a Source of tweets from somewhere\n```\n\n----------------------------------------\n\nTITLE: Testing Lookup Classification Bus in Scala\nDESCRIPTION: Test code that demonstrates how to work with a Lookup Classification Bus in Scala. It shows subscriber registration, event publishing, and message verification.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\nval lookupBus = new LookupBusImpl\nlookupBus.subscribe(testActor, \"greetings\")\nlookupBus.publish(MsgEnvelope(\"greetings\", \"hello\"))\nexpectMsg(\"hello\")\n```\n\n----------------------------------------\n\nTITLE: Illustrating Operator Immutability in Akka Streams (Java)\nDESCRIPTION: Highlights the immutability of Akka Streams operators in Java. Connecting an operator (e.g., using `via` or `to`) returns a new stream element instance. This snippet demonstrates that the original source remains unmodified and the result of the connection must be captured.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #source-immutable }\n```\n\n----------------------------------------\n\nTITLE: Combining Sources in Scala/Java\nDESCRIPTION: Demonstrates combining two sources into one using a simplified API without Graph DSL.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n#source-combine\n```\n\nLANGUAGE: Java\nCODE:\n```\n#source-combine\n```\n\n----------------------------------------\n\nTITLE: Implementing Abstract Multi-Node Cluster Test in Scala\nDESCRIPTION: Defines the abstract MultiNodeSpec test class that takes MultiNodeConfig as a constructor parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_25\n\nLANGUAGE: scala\nCODE:\n```\nabstract class StatsSampleSpec extends MultiNodeSpec(StatsSampleConfig)\n  with MultiNodeClusterSpec\n  with ImplicitSender {\n\n  import StatsSampleConfig._\n\n  \"The stats sample\" must {\n    \"illustrate how to startup cluster\" in within(15 seconds) {\n      // ...\n    }\n    \"show usage of the statsService\" in within(15 seconds) {\n      // ...\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Behavior Factory Method in Scala and Java\nDESCRIPTION: This illustrates the use of factory methods in Scala and Java Akka Typed for creating initial actor behaviors. It demonstrates the consistency in naming conventions and highlights the significance of using factory methods for resource retrieval and lifecycle management of actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nStyleGuideDocExamples.scala { #behavior-factory-method }\n```\n\nLANGUAGE: Java\nCODE:\n```\nStyleGuideDocExamples.java { #behavior-factory-method }\n```\n\n----------------------------------------\n\nTITLE: Creating Release Train Issue in Shell\nDESCRIPTION: This command creates a new issue from the Release Train Issue Template for a specific version.\nSOURCE: https://github.com/akka/akka/blob/main/RELEASING.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ sh ./scripts/create-release-issue.sh 0.x.y\n```\n\n----------------------------------------\n\nTITLE: Proper Usage of lazySource for Per-Materialization State in Java\nDESCRIPTION: Java example demonstrating the correct usage of lazySource to create separate stateful instances for each stream materialization, avoiding shared mutable state issues.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazySource.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n#one-per-materialization\n```\n\n----------------------------------------\n\nTITLE: Combining Materialized Values for a Composite Source in Java\nDESCRIPTION: This Java snippet demonstrates creating a composite Akka Stream `Source`. It combines `source1` (materializing a `CompletableFuture<Optional<Integer>>`) and `source2` using `Source.combine`. The `Keep.left()` combiner function selects the materialized value from `source1` for the resulting `nestedSource`. This allows interaction with the stream component represented by `source1`'s materialized value after the graph runs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\n// #mat-combine-1\n// Source requiring CompletableFuture materialized value\nSource<Integer, CompletableFuture<Optional<Integer>>> source1 = Source.maybe();\n\n// Source requiring NotUsed materialized value\nSource<Integer, NotUsed> source2 = Source.single(0);\n\n// Combine the sources, picking the CompletableFuture's materialized value\nSource<Integer, CompletableFuture<Optional<Integer>>> nestedSource =\n    Source.combine(source1, source2, i -> Merge.<Integer>create(i)).named(\"nestedSource\");\n// #mat-combine-1\n```\n\n----------------------------------------\n\nTITLE: Creating Source from Future in Akka Streams - Scala\nDESCRIPTION: Demonstrates how to create a Source from a Future that emits a single value when the Future completes. The Source will emit the Future's value when there is demand and complete afterwards. If the Future fails, the stream will fail with that exception.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/future.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source: Source[Int, NotUsed] = Source.future(Future(10))\n```\n\n----------------------------------------\n\nTITLE: Importing Akka Stream Tools in Java\nDESCRIPTION: The snippet illustrates how to import necessary Akka Stream tools in a Java file, setting up for stream creation and execution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [QuickStartDocTest.java](/akka-docs/src/test/java/jdocs/stream/QuickStartDocTest.java) { #stream-imports }\n```\n\n----------------------------------------\n\nTITLE: Retrieving ReadJournal in Scala\nDESCRIPTION: Shows how to get a ReadJournal instance for LevelDB persistence queries using PersistenceQuery extension.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query-leveldb.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval readJournal = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier)\n```\n\n----------------------------------------\n\nTITLE: Responding When All Temperatures Collected\nDESCRIPTION: Implements the logic for responding to the requester when all temperatures have been collected or timed out.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nprivate def respondWhenAllCollected(\n    stillWaiting: Set[String],\n    repliesSoFar: Map[String, TemperatureReading]): Behavior[Command] = {\n  if (stillWaiting.isEmpty) {\n    requester ! RespondAllTemperatures(requestId, repliesSoFar)\n    Behaviors.stopped\n  } else {\n    waitingForReplies(stillWaiting, repliesSoFar)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Tweet Data Model in Scala\nDESCRIPTION: Defines case classes for representing tweets and authors in Scala, used in the reactive tweets example.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\nfinal case class Author(handle: String)\n\nfinal case class Hashtag(name: String)\n\nfinal case class Tweet(author: Author, timestamp: Long, body: String) {\n  def hashtags: Set[Hashtag] =\n    body\n      .split(\" \")\n      .collect { case t if t.startsWith(\"#\") => Hashtag(t.replaceAll(\"[^#\\\\w]\", \"\")) }\n      .toSet\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Source of Tweet Authors (Java)\nDESCRIPTION: Defines an Akka Streams `Source` that emits `Author` objects, representing a stream of tweet authors extracted from a tweet source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nSource<Author, NotUsed> authors = tweets.filter(t -> t.hashtags().contains(akkaTag)).map(t -> t.author);\n```\n\n----------------------------------------\n\nTITLE: Using CompletionStageSource with Remote User Data Stream\nDESCRIPTION: Example demonstrating how to use CompletionStageSource to handle an asynchronous user data stream from a remote service. The source becomes available after establishing the connection, and the example shows how to handle both successful connection and error cases.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/completionStageSource.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\ninterface User {}\n\nclass RemoteService {\n    Source<User, NotUsed> connect() {\n        return Source.empty();\n    }\n}\n\nfinal RemoteService remoteService = new RemoteService();\nfinal CompletionStage<Source<User, NotUsed>> futureSource =\n    CompletableFuture.supplyAsync(() -> remoteService.connect());\n\nSource<User, NotUsed> source =\n    Source.completionStageSource(futureSource);\n// source is now a regular source which will emit the elements from the remote service\n// once the connection has been established\n```\n\n----------------------------------------\n\nTITLE: Implementing State Transitions in Scala\nDESCRIPTION: Transition handling logic between states in the FSM.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n#transition-elided\n```\n\n----------------------------------------\n\nTITLE: Setting the CallingThreadDispatcher for Akka Actors - Java\nDESCRIPTION: This Java code illustrates configuring an Akka actor to use CallingThreadDispatcher. This is useful in test environments to force synchronous actor execution. Set the dispatcher with the testkit and provide the correct settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_31\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [TestKitDocTest.java](/akka-docs/src/test/java/jdocs/testkit/TestKitDocTest.java) { #calling-thread-dispatcher }\n```\n\n----------------------------------------\n\nTITLE: Adding Mandatory Fields with Migration - Akka with Jackson\nDESCRIPTION: Illustrates the process of adding a mandatory field to a serialized class, requiring migration code to set default values. The migration involves configuring the JacksonMigration class and transforming JSON structures to ensure backward compatibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ItemAddedMigration.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v2b/ItemAddedMigration.scala) { #add-mandatory }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ItemAddedMigration.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v2b/ItemAddedMigration.java) { #add-mandatory }\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Reuse in Device Registration\nDESCRIPTION: Test that confirms the device group reuses existing device actors when a registration request is received for a device ID that already has an actor, avoiding duplicate actor creation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_4.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [DeviceGroupSpec.scala](/akka-docs/src/test/scala/typed/tutorial_4/DeviceGroupSpec.scala) { #device-group-test3 }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [DeviceGroupTest.java](/akka-docs/src/test/java/jdocs/typed/tutorial_4/DeviceGroupTest.java) { #device-group-test3 }\n```\n\n----------------------------------------\n\nTITLE: Implementing RetryFlow.withBackoff in Java\nDESCRIPTION: This snippet demonstrates how to use RetryFlow.withBackoff in Java to wrap a flow handling Integers, retrying elements unless the result is 0 or negative, or maxRetries is hit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RetryFlow/withBackoff.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nFlow<Integer, Integer, NotUsed> flow =\n    Flow.<Integer>create().map(x -> {\n      if (x < 5) return x;\n      else throw new RuntimeException(\"x too big\");\n    });\n\nFlow<Integer, Integer, NotUsed> retryFlow =\n    RetryFlow.withBackoff(\n        Duration.ofMillis(10),\n        Duration.ofSeconds(5),\n        0.2,\n        5,\n        flow,\n        (in, out) -> {\n          if (out.isSuccess()) {\n            Integer result = out.get();\n            if (result <= 0) {\n              return Optional.empty(); // don't retry on 0 or negative values\n            } else {\n              return Optional.of(in); // retry on positive values\n            }\n          } else {\n            return Optional.of(in); // retry on failure\n          }\n        });\n```\n\n----------------------------------------\n\nTITLE: Simple Lazy Flow Example in Java\nDESCRIPTION: Java implementation showing basic usage of lazyFlow with a sequence of numbers and execution order observation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/lazyFlow.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3))\n    .map(n -> {\n      System.out.println(\"Producing \" + n);\n      return n;\n    })\n    .via(\n        Flow.lazyFlow(\n            () -> {\n              System.out.println(\"Creating flow\");\n              return Flow.of(Integer.class)\n                  .map(\n                      n -> {\n                        System.out.println(\"Flow mapping \" + n);\n                        return n;\n                      });\n            }))\n    .run(system);\n```\n\n----------------------------------------\n\nTITLE: Source Setup Method Signature in Scala\nDESCRIPTION: Scala signature for the Source.setup method that takes a factory function accepting ActorMaterializer and Attributes, returning a Source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/setup.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsetup[T,M](factory:(akka.stream.ActorMaterializer,akka.stream.Attributes)=>akka.stream.scaladsl.Source[T,M]):akka.stream.scaladsl.Source[T,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: Scheduling One-Off Message in Scala\nDESCRIPTION: Schedule a single message 'foo' to be sent to testActor after 50ms delay using Akka Scheduler.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/scheduler.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsystem.scheduler.scheduleOnce(\n  50.milliseconds,\n  testActor,\n  \"foo\")(system.dispatcher)\n```\n\n----------------------------------------\n\nTITLE: Creating BalancingPool Programmatically (Scala/Java)\nDESCRIPTION: Illustrates creating a BalancingPool router actor ('router2') programmatically, specifying the balancing logic and the pool size (5). Be aware of BalancingPool constraints: routees share a mailbox and may not maintain distinct state effectively.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_23\n\nLANGUAGE: scala\nCODE:\n```\n//#balancing-pool-2\nimport akka.routing.BalancingPool\n\nval router2: ActorRef = context.actorOf(BalancingPool(5).props(Props[Worker]()), \"router2\")\n//#balancing-pool-2\n```\n\nLANGUAGE: java\nCODE:\n```\n//#balancing-pool-2\nimport akka.routing.BalancingPool;\n\nfinal ActorRef router2 = getContext().actorOf(new BalancingPool(5).props(Props.create(Worker.class)), \"router2\");\n//#balancing-pool-2\n```\n\n----------------------------------------\n\nTITLE: Collecting Stream Values Using Sink.seq in Java\nDESCRIPTION: Example showing how to collect numbers from a stream into a sequence using Sink.seq operator in Java. The result is available through a CompletionStage that completes when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/seq.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nList<Integer> input = Arrays.asList(1, 2, 3, 4, 5);\nCompletionStage<List<Integer>> result = Source.from(input)\n    .runWith(Sink.seq(), system);\nresult.thenAccept(System.out::println);\n```\n\n----------------------------------------\n\nTITLE: Implementing Graceful Actor Stop in Scala\nDESCRIPTION: Demonstrates gracefully stopping an actor with gracefulStop(), ensuring proper termination and execution of postStop hook.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_40\n\nLANGUAGE: scala\nCODE:\n```\ndef gracefulStop(actorRef: ActorRef): Unit = {\n  try {\n    gracefulStop(actorRef, timeout, Manager.Shutdown)\n      .map { _ => println(\"Actor stopped\") }\n      .recover { case _ => println(\"Actor failed to stop\") }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimal Size Exploring Resizer in Scala\nDESCRIPTION: Defines a router pool with an OptimalSizeExploringResizer in Scala configuration. This resizer attempts to find the optimal pool size for maximum throughput.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_39\n\nLANGUAGE: scala\nCODE:\n```\nakka.actor.deployment {\n  /parent/router3 {\n    router = round-robin-pool\n    optimal-size-exploring-resizer {\n      enabled = on\n      lower-bound = 1\n      upper-bound = 10\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Recovery Signal Handler in Java\nDESCRIPTION: Example of handling recovery completion signals in Java persistent actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_35\n\nLANGUAGE: Java\nCODE:\n```\n#recovery\n```\n\n----------------------------------------\n\nTITLE: Running AppOneMaster Client Node on Dynamic Port using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.AppOneMaster` main class using Maven's exec plugin in a separate process. This command starts a 'client' node for the Akka cluster sample application (Cluster Singleton example), allowing the system to assign a dynamic port (port 0). This node interacts with the cluster, likely sending requests to the singleton service.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.AppOneMaster\" -Dexec.args=\"client 0\"\n```\n\n----------------------------------------\n\nTITLE: Testing Subchannel Classification Bus in Scala\nDESCRIPTION: Test code for a Subchannel Classification Bus in Scala. It demonstrates subscribing to a parent topic and receiving events from both that topic and its subtopics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\nval subchannelBus = new SubchannelBusImpl\nsubchannelBus.subscribe(testActor, \"abc\")\nsubchannelBus.publish(MsgEnvelope(\"abc\", \"hello\"))\nsubchannelBus.publish(MsgEnvelope(\"abc.123\", \"hello2\"))\nexpectMsg(\"hello\")\nexpectMsg(\"hello2\")\n```\n\n----------------------------------------\n\nTITLE: Querying Events by Persistence ID in Scala\nDESCRIPTION: Demonstrates how to subscribe to a stream of events for a specific persistent actor using the eventsByPersistenceId query. This query is similar to replaying events for an actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nval events = readJournal.eventsByPersistenceId(\"some-persistence-id\", 0L, Long.MaxValue)\n\nevents.runForeach { event =>\n  println(s\"Got event: $event\")\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Subchannel Classification Bus in Scala\nDESCRIPTION: A Scala implementation of an EventBus using Subchannel Classification which allows subscribers to listen to groups of channels. It provides hierarchical event classification.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nclass SubchannelBusImpl extends\n  EventBus[MsgEnvelope, String, ActorRef] with SubchannelClassification {\n  // Subclassification means that subscribers won't get messages with\n  // a classifier that is more specific than what they have subscribed to\n  override protected def classify(event: MsgEnvelope): String = event.topic\n\n  // Topic can be freely defined by the publisher\n  // Subscribers receive messages if the topic is equal or a parent of\n  // what they have subscribed to\n  override protected def subclassification = {\n    new Subclassification[String] {\n      def isEqual(x: String, y: String) = x == y\n      def isSubclass(x: String, y: String) = x.startsWith(y)\n    }\n  }\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for the event's classifier\n  override protected def publish(event: MsgEnvelope, subscriber: ActorRef): Unit = {\n    subscriber ! event.payload\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Backoff Pattern\nDESCRIPTION: Demonstrates implementation of backoff supervisor with increasing retry intervals for actor restart.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BackoffSupervisorDocSpec.scala](/akka-docs/src/test/scala/docs/pattern/BackoffSupervisorDocSpec.scala) { #backoff-stop }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BackoffSupervisorDocTest.java](/akka-docs/src/test/java/jdocs/pattern/BackoffSupervisorDocTest.java) { #backoff-stop }\n```\n\n----------------------------------------\n\nTITLE: Selecting Routing Strategy in Akka Java\nDESCRIPTION: This snippet demonstrates different strategies for message routing in Akka using Java: round-robin, random, and consistent hashing. Configure strategy prior to spawning routers and ensure hashing mappings are provided if using consistent hashing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n/* Routing strategy configuration in RouterTest.java */\n```\n\n----------------------------------------\n\nTITLE: Handling Message Replies in Akka Actors\nDESCRIPTION: Shows how to handle message replies using sender() in Scala and getSender() in Java. Demonstrates proper pattern for replying to messages when there may not be a sender.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_34\n\nLANGUAGE: Scala\nCODE:\n```\ndef receive = {\n  case msg => sender() ! \"reply\"\n  // sender() is implicitly passed on by the scaladsl\n}\n```\n\n----------------------------------------\n\nTITLE: Using PubSub.sink in Akka Streams (Scala/Java)\nDESCRIPTION: The PubSub.sink operator creates a sink that publishes messages to an Akka Typed PubSub Topic. It has no backpressure support and will send messages to dead letters if there are no subscribers or if the topic actor is stopped.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/PubSub/sink.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nPubSub.sink[T](topic: akka.actor.typed.Toppic[T]): akka.stream.scaladsl.Sink[T, akka.NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nPubSub.sink(akka.actor.typed.Topic)\n```\n\n----------------------------------------\n\nTITLE: Programmatic Singleton Lease Settings in Java\nDESCRIPTION: Example of programmatically configuring lease settings for an Akka Cluster Singleton in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nLeaseUsageSettings leaseSettings = LeaseUsageSettings.create(\n    \"my-lease-implementation\",\n    Optional.of(\"custom-name\"), // defaults to <actor system name>-singleton-<singleton actor path>\n    system.settings().config().getConfig(\"pekko.coordination.lease.my-lease\"));\nClusterSingletonSettings singletonSettings = ClusterSingletonSettings.create(system)\n    .withLeaseSettings(Optional.of(leaseSettings));\n```\n\n----------------------------------------\n\nTITLE: Output of Recover with Retries Example (Java)\nDESCRIPTION: Shows the expected output of the Java `recoverWithRetries` example. The stream emits '0' through '4', encounters an exception, recovers using `planB`, emits 'five' and 'six', and then completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n0\n1\n2\n3\n4\nfive\nsix\n```\n\n----------------------------------------\n\nTITLE: Creating Source Stream from Iterable in Java\nDESCRIPTION: Example demonstrating how to create an Akka stream from an Iterable collection in Java. The source should be immutable or not modified after being used to avoid ConcurrentModificationException.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/from.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n#imports #source-from-example\n```\n\n----------------------------------------\n\nTITLE: Persistence Plugin Initialization in Scala\nDESCRIPTION: Example demonstrating how to initialize persistence plugins using PersistenceInit in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\n\"#imports #init\"\n```\n\n----------------------------------------\n\nTITLE: Java Source/Flow Backpressure Timeout API\nDESCRIPTION: API signature for backpressureTimeout operator in Java, which takes a java.time.Duration parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/backpressureTimeout.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nbackpressureTimeout(java.time.Duration)\n```\n\n----------------------------------------\n\nTITLE: Distributed Data Documentation\nDESCRIPTION: Documentation explaining the Distributed Data feature for sharing data between cluster nodes using a key-value store API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/includes/cluster.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n### Distributed Data\n\nDistributed Data is useful when you need to share data between nodes in an\nAkka Cluster. The data is accessed with an actor providing a key-value store like API.\n```\n\n----------------------------------------\n\nTITLE: Creating Akka Source from Java Stream (Java)\nDESCRIPTION: Shows how to create an Akka Stream `Source` from a Java 8 `java.util.stream.Stream` in Java. It imports relevant classes (`akka.stream.javadsl.StreamConverters`, `akka.stream.javadsl.Source`, `java.util.stream.Stream`) and uses `StreamConverters.fromJavaStream` accepting a `Creator` functional interface (`() -> Stream.of(1, 2, 3)`) to generate the Java Stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/fromJavaStream.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n//##import\nimport akka.stream.javadsl.StreamConverters;\nimport akka.stream.javadsl.Source;\n\nimport java.util.stream.Stream;\n//##import\n\n//##fromJavaStream\nSource<Integer, NotUsed> source = StreamConverters.fromJavaStream(() -> Stream.of(1, 2, 3));\n//##fromJavaStream\n\n```\n\n----------------------------------------\n\nTITLE: Markdown TOC and Index Structure\nDESCRIPTION: Markdown structure defining the table of contents and index for Akka actors documentation, including project description and navigation links to various actor-related topics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nproject.description: Using Akka to build reliable multi-core applications distributed across a network that scale up and scale out.\n---\n# Actors\n\n@@toc { depth=2 }\n\n@@@ index\n\n* [actors](actors.md)\n* [actor-lifecycle](actor-lifecycle.md)\n* [interaction patterns](interaction-patterns.md)\n* [fault-tolerance](fault-tolerance.md)\n* [actor-discovery](actor-discovery.md)\n* [routers](routers.md)\n* [stash](stash.md)\n* [fsm](fsm.md)\n* [coordinated-shutdown](../coordinated-shutdown.md)\n* [dispatchers](dispatchers.md)\n* [mailboxes](mailboxes.md)\n* [testing](testing.md)\n* [coexisting](coexisting.md)\n* [style-guide](style-guide.md)\n* [from-classic](from-classic.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Creating Partial Graph Component\nDESCRIPTION: Shows how to create a reusable partial graph component using GraphDSL.create(). The component has one input and one output port, making it compatible with the linear DSL.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\nval partial = GraphDSL.create() { implicit builder =>\n  import GraphDSL.Implicits._\n  val bcast = builder.add(Broadcast[Int](2))\n  val merge = builder.add(Merge[Int](2))\n\n  val f1, f2, f3, f4 = Flow[Int].map(_ + 10)\n\n  bcast ~> f2 ~> merge\n  bcast ~> f4 ~> merge\n  FlowShape(bcast.in, merge.out)\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nfinal Graph<FlowShape<Integer, Integer>, NotUsed> partial =\n    GraphDSL.create(\n        builder -> {\n          final UniformFanOutShape<Integer, Integer> bcast = builder.add(Broadcast.create(2));\n          final UniformFanInShape<Integer, Integer> merge = builder.add(Merge.create(2));\n\n          final Flow<Integer, Integer, NotUsed> f2 = Flow.of(Integer.class).map(elem -> elem + 10);\n          final Flow<Integer, Integer, NotUsed> f4 = Flow.of(Integer.class).map(elem -> elem + 10);\n\n          builder.from(bcast).via(builder.add(f2)).toFanIn(merge);\n          builder.from(bcast).via(builder.add(f4)).toFanIn(merge);\n\n          return FlowShape.of(bcast.in(), merge.out());\n        });\n```\n\n----------------------------------------\n\nTITLE: Actor Path Examples in Akka\nDESCRIPTION: Examples of actor paths in different contexts, showing both local and remote actor path formats with their URI-based addressing schemes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/addressing.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\"akka://my-sys/user/service-a/worker1\"               // purely local\n\"akka://my-sys@host.example.com:5678/user/service-b\" // remote\n```\n\n----------------------------------------\n\nTITLE: Implementing Synchronous Journal Plugin\nDESCRIPTION: Implementation example for storage backends that only support synchronous, blocking writes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-journals.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\ndef asyncWriteMessages(messages: Seq[AtomicWrite]): Future[Seq[Try[Unit]]] = {\n  Future.fromTry(Try {\n    // blocking call here\n    messages.map(_.payload.map(doSyncWriteMessage))\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Get Response Pattern 2 in Scala\nDESCRIPTION: Alternative approach to handling Get responses in Scala by matching on the key with if-guards and extracting the response type.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\nreplicator ! Get(Counter1Key, ReadLocal)\n\nreceive {\n  case g @ GetSuccess(key, _) if key == Counter1Key =>\n    val value = g.get(Counter1Key).value\n    ...\n  case NotFound(key, _) if key == Counter1Key =>\n    // key counter1 does not exist\n  case GetFailure(key, _) if key == Counter1Key =>\n    // read failed, try again later?\n}\n```\n\n----------------------------------------\n\nTITLE: Using ZipWith for Programmatic Triggering in Akka Streams\nDESCRIPTION: An alternative implementation of the programmatic trigger pattern using ZipWith instead of Zip followed by map. This avoids creating intermediate pairs by directly defining the output element creation function.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_34\n\nLANGUAGE: Scala\nCODE:\n```\n// This is the stream of triggers\nval triggerSource: Source[Trigger, NotUsed] = Source.fromPublisher(triggersForMessages)\n\n// This is the stream of messages\nval messageSource: Source[Message, NotUsed] = Source.fromPublisher(messages)\n\nval controllableSource: Source[Message, NotUsed] =\n  messageSource\n    // We pair each message with a trigger\n    .zipWith(triggerSource)((msg, trigger) => msg)\n```\n\n----------------------------------------\n\nTITLE: Initializing Sink.fromSubscriber in Java\nDESCRIPTION: Creates a Sink from a Reactive Streams Subscriber in Java. The method takes a subscriber as a parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/fromSubscriber.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSink.fromSubscriber(org.reactivestreams.Subscriber)\n```\n\n----------------------------------------\n\nTITLE: Collecting Stream Values into a List Collection using Akka Streams in Scala\nDESCRIPTION: This example demonstrates how to use Sink.collection to collect numbers from a source into a List collection. The source emits numbers 1 through 10, which are then collected into a List using the collection sink, and finally printed to the console when complete.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/collection.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.ActorSystem\nimport akka.stream.scaladsl._\n\nimport scala.concurrent.Future\nimport scala.concurrent.ExecutionContext.Implicits.global\n\nval system = ActorSystem()\n\nval source = Source(1 to 10)\n\n// #collection\nval result: Future[List[Int]] = source.runWith(Sink.collection[Int, List[Int]])\n// #collection\n\nresult.foreach(list => println(s\"result: $list\"))\n\n```\n\n----------------------------------------\n\nTITLE: Joining Cluster in Scala Multi-Node Test\nDESCRIPTION: Shows how to use the Cluster extension to join a node to the cluster in a multi-node test.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_27\n\nLANGUAGE: scala\nCODE:\n```\nCluster(system).join(node(first).address)\n```\n\n----------------------------------------\n\nTITLE: Passing Explicit Parent Reference to Child Actor - Java\nDESCRIPTION: This snippet demonstrates, in Java, passing an explicit parent reference to a child actor instead of relying on getContext().getParent(). Used for decoupling actors in testing scenarios. Requires Akka Actors (Java) and test infrastructure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #test-dependentchild }\n```\n\n----------------------------------------\n\nTITLE: Scala Database Integration Example\nDESCRIPTION: Demonstrates creating a Source from a reactive database publisher and processing database rows in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromPublisher.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.stream.javadsl.JavaFlowSupport\nimport java.util.concurrent.Flow\n\nval databasePublisher: Flow.Publisher[DatabaseRow] = getDatabasePublisher()\nval source = JavaFlowSupport.Source.fromPublisher(databasePublisher)\nval names: Source[String, NotUsed] = source.map(row => row.name)\n```\n\n----------------------------------------\n\nTITLE: Splitting Streams using splitWhen and splitAfter in Java\nDESCRIPTION: Compares the `splitWhen` and `splitAfter` operators in Akka Streams. `splitWhen` starts a new substream *with* the element satisfying the predicate, while `splitAfter` starts a new substream *after* the element satisfying the predicate. Both merge the substreams sequentially using `concatSubstreams`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n//#splitWhenAfter\nSource.range(1, 10)\n    .splitWhen(i -> i % 3 == 0)\n    .map(\n        i -> {\n          System.out.println(\"splitWhen: \" + i);\n          return i;\n        })\n    .concatSubstreams()\n    .runWith(Sink.ignore(), system);\n\nSystem.out.println(\"---------------------------\");\n\nSource.range(1, 10)\n    .splitAfter(i -> i % 3 == 0)\n    .map(\n        i -> {\n          System.out.println(\"splitAfter: \" + i);\n          return i;\n        })\n    .concatSubstreams()\n    .runWith(Sink.ignore(), system);\n//#splitWhenAfter\n```\n\n----------------------------------------\n\nTITLE: Building Akka Documentation Locally with Paradox - Shell\nDESCRIPTION: This snippet describes the sbt commands needed to generate the Akka documentation site using the Paradox plugin. The process involves running 'sbt' to initialize the shell and then executing 'akka-docs/paradox' to build the HTML documentation in the specified output directory. Prerequisites include having sbt, Paradox, and all documentation dependencies installed. The result is a static site in 'akka-docs/target/paradox/site/main/index.html'.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nsbt\\nakka-docs/paradox\n```\n\n----------------------------------------\n\nTITLE: Handling Update Response Pattern 1 in Java\nDESCRIPTION: Java example showing how to handle the response from an Update operation by checking for different response types like UpdateSuccess and UpdateTimeout.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nreplicator.tell(\n    new Update<PNCounter>(\n        Counter1Key,\n        PNCounter.create(),\n        WriteLocal.instance(),\n        curr -> curr.increment(node, 1)),\n    getSelf());\n\n// handle response\nreceiveBuilder()\n    .match(\n        UpdateSuccess.class,\n        a -> {\n          if (a.key().equals(Counter1Key)) {\n            // ok\n          }\n        })\n    .match(\n        UpdateTimeout.class,\n        a -> {\n          if (a.key().equals(Counter1Key)) {\n            // will eventually be replicated, but read from other nodes might not\n            // see the update yet\n          }\n        })\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Publishing Akka Artifacts Locally\nDESCRIPTION: Command to deploy Akka artifacts to local Ivy repository for local development use.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsbt publishLocal\n```\n\n----------------------------------------\n\nTITLE: Mounting TLS Secret Volume into Akka Container (YAML)\nDESCRIPTION: Specifies the 'volumeMounts' section in a Kubernetes container spec, mounting the TLS certificate secret at the default path expected by 'RotatingKeysSSLEngineProvider'. This path ('/var/run/secrets/akka-tls/rotating-keys-engine') is required for Akka's automated credential reload support. Must correspond with the 'volumes' entry for consistency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n        volumeMounts:\\n        - name: akka-tls\\n          mountPath: /var/run/secrets/akka-tls/rotating-keys-engine\n```\n\n----------------------------------------\n\nTITLE: Looking up a Dispatcher in Java\nDESCRIPTION: Demonstrates how to look up an Executor from an actor system in Java, which can be used to run CompletableFuture invocations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// this is a shorthand for system.dispatchers().lookup(\"my-dispatcher\")\nExecutor ex = system.dispatchers().lookup(\"my-dispatcher\");\nCompletableFuture.runAsync(() -> {}, ex);\n```\n\n----------------------------------------\n\nTITLE: Implementing Stats Service in Scala\nDESCRIPTION: Scala implementation of the statistics service that receives text, splits it into words, delegates counting to worker routees, and aggregates results using a cluster-aware router.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nclass StatsService extends Actor {\n  import StatsMessages._\n  import akka.cluster.metrics.sample.StatsMessages._\n\n  // This router is used both with lookup and deploy of routees. If you\n  // have a router with only lookup of routees you can use Props.empty\n  // instead of Props[StatsWorker.class].\n  var workerRouter = context.actorOf(\n    FromConfig.props(Props[StatsWorker]),\n    name = \"workerRouter\")\n\n  def receive = {\n    case StatsJob(text) if text != \"\" =>\n      val words = text.split(\"\\\\s+\").toList\n      val replyTo = sender() // important to not close over sender()\n      val aggregator = context.actorOf(\n        Props(new StatsAggregator(words.size, replyTo)))\n\n      words foreach { word =>\n        workerRouter.tell(ProcessWord(word), aggregator)\n      }\n  }\n}\n\nclass StatsAggregator(expectedResults: Int, replyTo: ActorRef) extends Actor {\n  import StatsMessages._\n  import akka.cluster.metrics.sample.StatsMessages._\n\n  var results = IndexedSeq.empty[Int]\n  context.setReceiveTimeout(3.seconds)\n\n  def receive = {\n    case WordCount(word, count) =>\n      results = results :+ count\n      if (results.size == expectedResults) {\n        val meanWordLength = results.sum.toDouble / results.size\n        replyTo ! StatsResult(meanWordLength)\n        context.stop(self)\n      }\n\n    case ReceiveTimeout =>\n      replyTo ! JobFailed(\"Service unavailable, try again later\")\n      context.stop(self)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RandomPool Router via HOCON\nDESCRIPTION: Defines an Akka RandomPool router named 'router1' in HOCON. This router randomly selects a routee from its pool of 5 instances for each message.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_15\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-random-pool\nakka.actor.deployment {\n  /parent/router1 {\n    router = random-pool\n    nr-of-instances = 5\n  }\n}\n//#config-random-pool\n```\n\n----------------------------------------\n\nTITLE: Using drop operator in Akka Streams with Scala\nDESCRIPTION: Example of using the drop operator to skip the first 3 elements of a source in Scala. This creates a stream of numbers from 1 to 5, drops the first 3 elements, and passes only 4 and 5 downstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/drop.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n// #drop\nSource(1 to 5).drop(3).runWith(Sink.foreach(println))\n// prints: 4\n// prints: 5\n// #drop\n```\n\n----------------------------------------\n\nTITLE: Creating a Source from a Function (Akka Streams Scala)\nDESCRIPTION: Shows how to generate a source that continuously evaluates a function as long as there is demand, employing Source.repeat followed by map. Assumes the builder function is side-effect free and thread-safe; for sources touching mutable state, see Source.unfold/unfoldResource. Produces a stream of function outputs on demand.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeSourceFromFunction.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeSourceFromFunction.scala) { #source-from-function }\n```\n\n----------------------------------------\n\nTITLE: Reusing Custom Akka Streams Components (Java)\nDESCRIPTION: Shows how custom, nested `Source` (`nestedSource`) and `Sink` (`nestedSink`) components, created using modularization techniques like `named()`, can be seamlessly connected to form a `RunnableGraph` in Java, just like using built-in Akka components.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n//##reuse\n// Create a RunnableGraph from the composite modules\nfinal RunnableGraph<CompletionStage<Integer>> runnableGraph2 =\n    nestedSource.toMat(nestedSink, Keep.right());\n\nfinal RunnableGraph<CompletionStage<Integer>> runnableGraph3 =\n    Source.single(0).toMat(Sink.<Integer>head(), Keep.right());\n//##reuse\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Serializer for a Domain Event in Akka Persistence (Java)\nDESCRIPTION: Implements a custom Java serializer class for the 'Person' entity, usually via extending Akka's SerializerWithStringManifest interface. This serializer provides binary conversion for the 'Person' model, and includes a unique identifier for Akka's serialization framework. Relies on Akka core and the relevant Java domain model.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.serialization.SerializerWithStringManifest;\nimport jdocs.persistence.Person;\nimport java.nio.charset.StandardCharsets;\n\npublic class PersonSerializer extends SerializerWithStringManifest {\n  @Override\n  public int identifier() {\n    return 1234567;\n  }\n  @Override\n  public byte[] toBinary(Object o) {\n    Person p = (Person) o;\n    return (p.getName() + \",\" + p.getAge()).getBytes(StandardCharsets.UTF_8);\n  }\n  @Override\n  public String manifest(Object o) {\n    return \"Person\";\n  }\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) {\n    String str = new String(bytes, StandardCharsets.UTF_8);\n    String[] parts = str.split(\",\");\n    return new Person(parts[0], Integer.parseInt(parts[1]));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Routing Logic in Akka\nDESCRIPTION: Creates a RedundancyRoutingLogic class that implements the RoutingLogic interface. This logic selects multiple destinations for each message using round-robin selection, effectively replicating each message to several routees.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_42\n\nLANGUAGE: Scala\nCODE:\n```\nclass RedundancyRoutingLogic(nbrCopies: Int) extends RoutingLogic {\n  private val roundRobin = RoundRobinRoutingLogic()\n  def select(message: Any, routees: immutable.IndexedSeq[Routee]): Routee = {\n    if (routees.isEmpty)\n      NoRoutee\n    else {\n      val targets = (1 to nbrCopies).map(_ => roundRobin.select(message, routees))\n      SeveralRoutees(targets)\n    }\n  }\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\npublic class RedundancyRoutingLogic implements RoutingLogic {\n  private final int nbrCopies;\n  private final RoundRobinRoutingLogic roundRobin = new RoundRobinRoutingLogic();\n\n  public RedundancyRoutingLogic(int nbrCopies) {\n    this.nbrCopies = nbrCopies;\n  }\n\n  @Override\n  public Routee select(Object message, IndexedSeq<Routee> routees) {\n    if (routees.isEmpty()) {\n      return Routees.noRoutee();\n    } else {\n      final List<Routee> targets = new ArrayList<Routee>();\n      for (int i = 0; i < nbrCopies; i++) {\n        targets.add(roundRobin.select(message, routees));\n      }\n      return new SeveralRoutees(targets);\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Problematic Blocking in Akka Actor (Java)\nDESCRIPTION: This snippet demonstrates a problematic implementation of blocking calls within an Akka actor in Java, which can cause thread starvation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BlockingActor.java](/akka-docs/src/test/java/jdocs/actor/typed/BlockingActor.java) { #blocking-in-actor }\n```\n\n----------------------------------------\n\nTITLE: Non-blocking Print Actor (Java)\nDESCRIPTION: This snippet shows a non-blocking Print Actor implementation in Java, used to demonstrate the impact of blocking operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [PrintActor.java](/akka-docs/src/test/java/jdocs/actor/typed/PrintActor.java) { #print-actor }\n```\n\n----------------------------------------\n\nTITLE: Loading Akka Discovery Extension in Java\nDESCRIPTION: Code snippet demonstrating how to load the Akka Discovery extension in a Java application. This is the first step to use service discovery features.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nServiceDiscovery discovery = Discovery.get(system).discovery();\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Lookup in Java\nDESCRIPTION: Java code for creating a cluster-aware router that uses ActorSelection to look up routees by path on cluster nodes with the \"compute\" role.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nActorRef workerRouter = getContext().actorOf(\n  new ClusterRouterGroup(new ConsistentHashingGroup(Collections.<String>emptyList()),\n    new ClusterRouterGroupSettings(100, \n      Arrays.asList(\"/user/statsWorker\"), true, \"compute\"))\n      .props(), \"workerRouter3\");\n\n```\n\n----------------------------------------\n\nTITLE: Exception Handling with Ask Pattern - Scala\nDESCRIPTION: Example of handling exceptions when using the ask pattern in Scala by sending Status.Failure messages\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_28\n\nLANGUAGE: scala\nCODE:\n```\n#reply-exception\n```\n\n----------------------------------------\n\nTITLE: Accessing TCP Manager Actor in Scala\nDESCRIPTION: Shows how to obtain the TCP manager actor reference in Scala using the IO entry point, which serves as the primary access point for I/O operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.io.{ IO, Tcp }\nimport akka.actor.ActorSystem\n\nimplicit val system: ActorSystem = ActorSystem()\nval manager = IO(Tcp)\n```\n\n----------------------------------------\n\nTITLE: Drop Elements Using dropWhile in Java\nDESCRIPTION: Java implementation demonstrating the usage of dropWhile operator to filter out negative numbers from a source stream until encountering the first non-negative number. Subsequent elements are emitted regardless of the predicate.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/dropWhile.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(-3, -2, -1, 0, 1, -1, 2))\n  .dropWhile(x -> x < 0)\n  .runWith(Sink.foreach(System.out::println), system);\n// prints: 0, 1, -1, 2\n```\n\n----------------------------------------\n\nTITLE: Creating RandomGroup from Configuration (Scala/Java)\nDESCRIPTION: Shows how to create a RandomGroup router actor ('router1') using settings defined in the HOCON configuration file under the path '/parent/router1', including the routee paths.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\n//#random-group-1\nimport akka.routing.FromConfig\n\nval router1: ActorRef = context.actorOf(FromConfig.props(Props[Worker]()), \"router1\")\n//#random-group-1\n```\n\nLANGUAGE: java\nCODE:\n```\n//#random-group-1\nimport akka.routing.FromConfig;\n\nfinal ActorRef router1 = getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)),\n    \"router1\");\n//#random-group-1\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Flag Changes in Akka Voting Service with Scala\nDESCRIPTION: This code snippet illustrates how the VotingService actor in Akka subscribes to changes in a flag key. By listening to changes of the OpenedKey, nodes are notified when the voting flag is toggled. The implementation requires Akka's distributed data features and involves Subscribe messages handling Changed events.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-scala/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nreplicator ! Subscribe(OpenedKey, self)\n\ncase c @ Changed(OpenedKey) if c.get(OpenedKey).enabled\n```\n\n----------------------------------------\n\nTITLE: Selecting a Mailbox Type for an Actor in Scala\nDESCRIPTION: Demonstrates how to select a specific mailbox for an actor using MailboxSelector to create a Props instance for actor spawning in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/mailboxes.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval boundedMailbox = spawn(\n  behavior,\n  \"bounded-mailbox-actor\",\n  // create a bounded mailbox with a capacity of 100 messages and a default overflow strategy\n  MailboxSelector.bounded(100))\n\nval unboundedMailbox = spawn(\n  behavior,\n  \"unbounded-mailbox-actor\",\n  // default is an unbounded mailbox, same as not specifying\n  MailboxSelector.unbounded())\n\nval fromConfig = spawn(\n  behavior,\n  \"from-config-mailbox-actor\",\n  // select the mailbox-type from configuration\n  MailboxSelector.fromConfig(\"my-app.my-special-mailbox\"))\n```\n\n----------------------------------------\n\nTITLE: Anonymous Actor Spawning - Scala\nDESCRIPTION: Provides a method for anonymously spawning actors during tests in Scala with ActorTestKit, a technique useful for handling dynamic actor creation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AsyncTestingExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/AsyncTestingExampleSpec.scala) { #test-spawn-anonymous }\n```\n\n----------------------------------------\n\nTITLE: Defining Query Protocol Messages in Scala\nDESCRIPTION: Defines the message types used for querying device temperatures, including possible response states.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsealed trait DeviceGroupQuery\nfinal case class WrappedRespondTemperature(response: RespondTemperature) extends DeviceGroupQuery\nfinal case object CollectionTimeout extends DeviceGroupQuery\n\nfinal case class RespondAllTemperatures(requestId: Long, temperatures: Map[String, TemperatureReading])\n\nsealed trait TemperatureReading\nfinal case class Temperature(value: Double) extends TemperatureReading\ncase object TemperatureNotAvailable extends TemperatureReading\ncase object DeviceNotAvailable extends TemperatureReading\ncase object DeviceTimedOut extends TemperatureReading\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data with splitAfter in Java\nDESCRIPTION: Java implementation demonstrating how to split a stream of time series data into substreams for each second using the splitAfter operator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/splitAfter.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass TimeSeriesEvent {\n    private final long timestamp;\n\n    public TimeSeriesEvent(long timestamp) {\n        this.timestamp = timestamp;\n    }\n\n    public long getTimestamp() {\n        return timestamp;\n    }\n}\n\nSource.from(Arrays.asList(\n    new TimeSeriesEvent(1000),\n    new TimeSeriesEvent(2000),\n    new TimeSeriesEvent(3000),\n    new TimeSeriesEvent(4000)))\n    .sliding(2)\n    .splitAfter(\n        SubstreamCancelStrategy.drain(),\n        pair -> {\n            long current = pair.get(0).getTimestamp() / 1000;\n            long next = pair.get(1).getTimestamp() / 1000;\n            return current != next;\n        });\n```\n\n----------------------------------------\n\nTITLE: Implementing Conflate Operator in Java\nDESCRIPTION: Shows how to use the conflate operator in Java to handle backpressure by aggregating elements. The example demonstrates summing elements when downstream cannot keep up with upstream production.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/conflate.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#conflate\n```\n\n----------------------------------------\n\nTITLE: Consistent Hashing Shard Allocation Initialization in Scala\nDESCRIPTION: Scala example showing how to initialize entities with consistent hashing shard allocation\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_21\n\nLANGUAGE: scala\nCODE:\n```\nShardedDaemonProcess(system).init(\\n  Name = \\\"BuildingDevices\\\",\\n  numberOfInstances = 30,\\n  behaviorFactory = instanceId => {\\n    val region = ClusterSharding(system).init(\\n      Entity(Building.TypeKey)(\\n        createBehavior = _ => Building())\\n        .withAllocationStrategy(\\n          new ConsistentHashingShardAllocationStrategy(\\n            numberOfShards = 30,\\n            maxSimultaneousRebalance = 5))\\n        .withMessageExtractor(\\n          ShardingMessageExtractor\\n            .noEnvelope[Building.Command, Building.BuildingEnvelope](30) {\\n              case cmd => Building.BuildingEnvelope(cmd.buildingId)\\n            }))\\n\\n    val deviceRegion = ClusterSharding(system).init(\\n      Entity(Device.TypeKey)(\\n        createBehavior = _ => Device())\\n        .withAllocationStrategy(\\n          new ConsistentHashingShardAllocationStrategy(\\n            numberOfShards = 30,\\n            maxSimultaneousRebalance = 5))\\n        .withMessageExtractor(\\n          ShardingMessageExtractor\\n            .noEnvelope[Device.Command, Device.DeviceEnvelope](30) {\\n              case cmd => Device.DeviceEnvelope(cmd.buildingId, cmd.deviceId)\\n            }))\\n\\n    // ... behavior using the regions\\n    ???\\n  })\n```\n\n----------------------------------------\n\nTITLE: Illustrating Potential Deadlock with asInputStream in mapMaterializedValue (Scala)\nDESCRIPTION: This Scala snippet demonstrates a potential deadlock scenario in Akka Streams when using blocking I/O converters. It shows using `StreamConverters.asInputStream()` combined with `mapMaterializedValue`. Because `asInputStream` materializes into a blocking `InputStream`, calling a blocking method like `inputStream.read()` within `mapMaterializedValue` can block the stream materialization process itself, leading to a timeout or deadlock. This pattern is explicitly warned against in the documentation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/index.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n...\n.toMat(StreamConverters.asInputStream().mapMaterializedValue { inputStream =>\n        inputStream.read()  // this could block forever\n        ...\n}).run()\n```\n\n----------------------------------------\n\nTITLE: Custom Mailbox Marker Interface in Java\nDESCRIPTION: A marker interface implementation that indicates a custom mailbox queue semantics in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/mailboxes.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n// Marker interface used for mailbox requirements mapping\npublic interface MyUnboundedMessageQueueSemantics {}\n```\n\n----------------------------------------\n\nTITLE: Factorial Backend Implementation in Scala\nDESCRIPTION: Example Scala implementation of a backend worker that performs factorial calculations. This actor is used as a routee for the adaptive load balancing example.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [FactorialBackend.scala](/akka-docs/src/test/scala/docs/cluster/FactorialBackend.scala) { #backend }\n```\n\n----------------------------------------\n\nTITLE: Implementing Request-Response with Scala 3 Union Types in Akka Typed\nDESCRIPTION: This snippet demonstrates how to use Scala 3 union types for response message types in Akka Typed, allowing for ad hoc combinations of types instead of message adapters. The behavior is internally declared as a union of its own protocol and any response messages it may accept.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/interaction-patterns.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [InteractionPatternsSpec.scala](/akka-actor-typed-tests/src/test/scala-3/docs/akka/typed/InteractionPatternsScala3Spec.scala) { #adapted-response }\n```\n\n----------------------------------------\n\nTITLE: Actor Testing with Test Headers - Java\nDESCRIPTION: This Java snippet outlines setting up test headers when using ActorTestKit for actor testing. It includes methods for creating test probes and handling the actors under test for smooth test execution.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AsyncTestingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/AsyncTestingExampleTest.java) { #test-header }\n```\n\n----------------------------------------\n\nTITLE: Applying reduce Operator in Scala Akka Streams\nDESCRIPTION: Example of using the reduce operator in Scala to add up all integers in a stream. The reduce function takes a binary operation that combines elements, and only emits the final result when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/reduce.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(1 to 100)\n\nval result: Future[Int] =\n  source.reduce((a, b) => a + b).runWith(Sink.head) // Result: 5050\n```\n\n----------------------------------------\n\nTITLE: Calculating Phi Value in Failure Detector\nDESCRIPTION: This snippet shows the formula used to calculate the phi value in the Phi Accrual Failure Detector. It uses the cumulative distribution function of a normal distribution based on historical heartbeat inter-arrival times.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/failure-detector.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\nphi = -log10(1 - F(timeSinceLastHeartbeat))\n```\n```\n\n----------------------------------------\n\nTITLE: Buffer Operation Java API Signature\nDESCRIPTION: Java API signature for buffer operation in Source and Flow components. Takes buffer size and overflow strategy as parameters.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/buffer.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nbuffer(int size, akka.stream.OverflowStrategy overflowStrategy)\n```\n\n----------------------------------------\n\nTITLE: Named Test Probes\nDESCRIPTION: Demonstrates creating test probes with custom names for better test logging and assertions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n#test-probe-with-custom-name\n```\n\n----------------------------------------\n\nTITLE: Handling Blocking Operations with map and Dedicated Dispatcher (Scala)\nDESCRIPTION: An alternative way to handle blocking calls using a simple `map` operation. The `withAttributes` and `ActorAttributes.dispatcher` are used to ensure the blocking `lookupEmailBlocking` function runs on the dedicated `blocking-dispatcher`. Note that this processes elements one by one, unlike the concurrent nature of `mapAsync`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\nval emailAddressesMapBlocking: Source[String, NotUsed] =\n  authors\n    .map(author => lookupEmailBlocking(author.handle))\n    .withAttributes(ActorAttributes.dispatcher(\"blocking-dispatcher\"))\n    .collect { case Some(emailAddress) => emailAddress }\n```\n\n----------------------------------------\n\nTITLE: Akka Source Ask Pattern Signature\nDESCRIPTION: API signatures for the ask operator on Source, which sends request-reply messages to an ActorRef. Available for both Scala and Java with different type parameter handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/ask.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.ask[S](ref:akka.actor.ActorRef)(implicittimeout:akka.util.Timeout,implicittag:scala.reflect.ClassTag[S]):FlowOps.this.Repr[S]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.ask(akka.actor.ActorRef,java.lang.Class,akka.util.Timeout)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using ORMultiMap in Java\nDESCRIPTION: Example of creating and using an ORMultiMap (observed-remove multi-map) in Java. It shows how to add key-value pairs and retrieve values from the map.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\nORMultiMap<String, Integer> multiMap = ORMultiMap.create();\nORMultiMap<String, Integer> updated = multiMap.addBinding(\"a\", 1).addBinding(\"b\", 2);\nSet<Integer> result = updated.get(\"a\");\nSet<Integer> result2 = updated.getOrElse(\"c\", new HashSet<>());\n```\n\n----------------------------------------\n\nTITLE: Factorial Frontend Implementation in Java\nDESCRIPTION: Example Java implementation of a frontend actor that receives user jobs and delegates to the backends via the metrics-based router.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n@@snip [FactorialFrontend.java](/akka-docs/src/test/java/jdocs/cluster/FactorialFrontend.java) { #frontend }\n```\n\n----------------------------------------\n\nTITLE: Providing a Child Maker Function in Tests - Scala\nDESCRIPTION: This code sample shows the implementation of a child actor factory function for use in testing scenarios. Typically, this function will return a Props configuration for child actors within Scala Akka tests. Akka Actors and ScalaTest are required.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_26\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ParentChildSpec.scala](/akka-docs/src/test/scala/docs/testkit/ParentChildSpec.scala) { #child-maker-test }\n```\n\n----------------------------------------\n\nTITLE: Custom Akka Application Configuration\nDESCRIPTION: Example of a custom application.conf file showing common Akka settings including logging, actor system, and remote configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/configuration.md#2025-04-22_snippet_1\n\nLANGUAGE: hocon\nCODE:\n```\n# In this file you can override any option defined in the reference files.\n# Copy in parts of the reference files and modify as you please.\n\nakka {\n\n  # Logger config for Akka internals and classic actors, the new API relies\n  # directly on SLF4J and your config for the logger backend.\n\n  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs\n  # to STDOUT)\n  loggers = [\"akka.event.slf4j.Slf4jLogger\"]\n\n  # Log level used by the configured loggers (see \"loggers\") as soon\n  # as they have been started; before that, see \"stdout-loglevel\"\n  # Options: OFF, ERROR, WARNING, INFO, DEBUG\n  loglevel = \"DEBUG\"\n\n  # Log level for the very basic logger activated during ActorSystem startup.\n  # This logger prints the log messages to stdout (System.out).\n  # Options: OFF, ERROR, WARNING, INFO, DEBUG\n  stdout-loglevel = \"DEBUG\"\n\n  # Filter of log events that is used by the LoggingAdapter before\n  # publishing log events to the eventStream.\n  logging-filter = \"akka.event.slf4j.Slf4jLoggingFilter\"\n\n  actor {\n    provider = \"cluster\"\n\n    default-dispatcher {\n      # Throughput for default Dispatcher, set to 1 for as fair as possible\n      throughput = 10\n    }\n  }\n\n  remote.artery {\n    # The port clients should connect to.\n    canonical.port = 4711\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Merging Prioritized Sources in Scala\nDESCRIPTION: Example showing how to merge sources with priority weights in Scala using Akka Streams mergePrioritized operator. The priorities determine the probability of selecting from each source when multiple sources have available elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergePrioritized.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval sourceA = Source(1 to 3)\nval sourceB = Source(4 to 6)\nval priorityMerged = sourceA.mergePrioritized(sourceB, 2, 1, eagerComplete = false)\nprioritizedMerged.runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Concatenating Multiple Sources Using Source.combine in Scala\nDESCRIPTION: Example showing how to concatenate three sources of integers using Source.combine with Concat strategy. Sources emit elements sequentially - each source completes before the next begins.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/combine.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n#source-combine-concat\n```\n\n----------------------------------------\n\nTITLE: Referencing StreamOperatorsIndexGenerator in Scala\nDESCRIPTION: This snippet shows the file path for the StreamOperatorsIndexGenerator, which is used to automatically generate and enforce documentation for Akka Streams operators. It's important when adding new top-level objects or classes containing operators.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\nproject/StreamOperatorsIndexGenerator.scala\n```\n\n----------------------------------------\n\nTITLE: Implementing DurableStateUpdateWithChangeEventStore Interface in Scala\nDESCRIPTION: Implementation of the extended DurableStateUpdateWithChangeEventStore interface in Scala, which adds functionality for storing additional change events alongside the state updates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n#plugin-api-change-event\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Actor Typed Dependency\nDESCRIPTION: Dependency configuration for adding the core Akka actor library (akka-actor-typed) to your project using SBT, Maven, or Gradle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_1\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-actor-typed_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Secret Volume for Akka TLS Certificates in Kubernetes (YAML)\nDESCRIPTION: Adds a 'volumes' entry in a Kubernetes Pod/Deployment spec to define a volume sourced from the previously created secret ('my-service-akka-tls-certificate'). This makes the certificate data available for mounting into the container. The volume name will be referenced in the 'volumeMounts' section.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n      volumes:\\n      - name: akka-tls\\n        secret:\\n          secretName: my-service-akka-tls-certificate\n```\n\n----------------------------------------\n\nTITLE: Configuring Virtual Threads Dispatcher for Blocking Operations\nDESCRIPTION: This snippet demonstrates how to configure a virtual threads dispatcher for blocking operations in Java 21 or later, which allows for efficient handling of blocking tasks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_15\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.actor.default-blocking-io-dispatcher {\n  executor = \"virtual-thread-executor\"\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing Replicator Update Message using Curried Syntax in Scala\nDESCRIPTION: Illustrates an alternative, curried Scala syntax for creating a `Replicator.Update` message. This approach separates the key, initial value, consistency level, reply target, and modification function into distinct parameter lists. The code is referenced from `ReplicatorCompileOnlyTest.scala`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ReplicatorSpec.scala](/akka-cluster-typed/src/test/scala/akka/cluster/ddata/typed/scaladsl/ReplicatorCompileOnlyTest.scala) { #curried-update }\n```\n\n----------------------------------------\n\nTITLE: Setting Dispatcher for Akka Pool Router in Scala\nDESCRIPTION: This snippet demonstrates how to configure dispatchers for pool routers in Akka using Scala. It addresses configuring the Props used by the PoolRouter for its routees. Make sure to have Akka and relevant dispatcher settings in your configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/routers.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n/* Dispatcher configuration for pool router in RouterSpec.scala */\n```\n\n----------------------------------------\n\nTITLE: Sorting Elements to Multiple Groups with groupBy in Akka Streams\nDESCRIPTION: This pattern allows mapping elements to multiple groups simultaneously, unlike the standard groupBy which puts each element in exactly one group. It's useful for scenarios like message routing to multiple topics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_28\n\nLANGUAGE: Scala\nCODE:\n```\n// For every topic, a separate group of elements:\nSource(Message :: Nil)\n  .mapConcat(msg =>\n    // Get the topics a message belongs to\n    topicMapper(msg)\n      // Create a group for each topic\n      .map(topic => (msg, topic))\n  )\n  // now we group by the topic, which gives us a separate substream for each topic\n  .groupBy(maxTopics, _._2)\n  // from each Message-Topic pair we only are interested in the Message\n  .map(_._1)\n```\n\n----------------------------------------\n\nTITLE: Sink.fromMaterializer API Signatures\nDESCRIPTION: API signatures for Sink.fromMaterializer in both Scala and Java. The operator takes a factory function that receives a Materializer and Attributes to create a Sink.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/fromMaterializer.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nfromMaterializer[T,M](factory:(akka.stream.Materializer,akka.stream.Attributes)=>akka.stream.scaladsl.Sink[T,M]):akka.stream.scaladsl.Sink[T,scala.concurrent.Future[M]]\n```\n\nLANGUAGE: java\nCODE:\n```\nfromMaterializer(java.util.function.BiFunction)\n```\n\n----------------------------------------\n\nTITLE: Implementing Singleton Messages using Enum Pattern in Java\nDESCRIPTION: A pattern for defining parameter-less messages using Java enums. This approach is recommended for messages that don't require additional parameters.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nenum Command {\n  Increment,\n  Decrement\n}\n```\n\n----------------------------------------\n\nTITLE: Cancellable Shutdown Tasks in Java\nDESCRIPTION: Shows how to create cancellable tasks in the coordinated shutdown process in Java. The task can later be cancelled by calling cancel() on the returned object.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nCancellable task = CoordinatedShutdown.get(system).addCancellableTask(\n    CoordinatedShutdown.PhaseBeforeServiceUnbind(),\n    \"cleanup\",\n    () -> {\n      CompletableFuture<Done> promise = new CompletableFuture<>();\n      actorRef.tell(new Cleanup(promise), ActorRef.noSender());\n      return promise;\n    });\n\n// To cancel task\ntask.cancel();\n```\n\n----------------------------------------\n\nTITLE: Creating Flow from Partial Graph in Scala/Java\nDESCRIPTION: Shows how to create a Flow by exposing both inlet and outlet using GraphDSL.create and FlowShape.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n#flow-from-partial-graph-dsl\n```\n\nLANGUAGE: Java\nCODE:\n```\n#flow-from-partial-graph-dsl\n```\n\n----------------------------------------\n\nTITLE: Storing State in Java FSM Implementation\nDESCRIPTION: Java equivalent of storing state in a Finite State Machine implementation. Includes a queue for objects and a reference to the target actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fsm.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nenum Timeout implements Event {\n  INSTANCE\n}\n\nclass Batcher extends AbstractBehavior<Event> {\n  private List<Object> queue = new ArrayList<>();\n  private Optional<ActorRef<Batch>> target = Optional.empty();\n\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing TCP Manager Actor in Java\nDESCRIPTION: Shows how to obtain the TCP manager actor reference in Java by querying the ActorSystem, which serves as the primary access point for I/O operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nfinal ActorRef tcpManager = Tcp.get(system).manager();\n```\n\n----------------------------------------\n\nTITLE: Configuring Event Deletion with Snapshot Predicate in Scala\nDESCRIPTION: Demonstrates how to enable event deletion when using predicate-based snapshots in Scala. This configuration allows events to be deleted when snapshots are taken based on custom predicates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-snapshot.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nwithDeleteEventsOnSnapshot(true)\n```\n\n----------------------------------------\n\nTITLE: Source.recoverWith API Signature in Scala and Java\nDESCRIPTION: The API signature for the recoverWith operator on Source, which allows switching to an alternative Source when a failure occurs upstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recoverWith.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nrecoverWith[T>:Out](pf:PartialFunction[Throwable,akka.stream.Graph[akka.stream.SourceShape[T],akka.NotUsed]]):FlowOps.this.Repr[T]\n```\n\nLANGUAGE: java\nCODE:\n```\nrecoverWith(java.lang.Class,java.util.function.Supplier)\n```\n\n----------------------------------------\n\nTITLE: Handling CRDT Flag Changes in Akka Typed Actor (Java)\nDESCRIPTION: Matches 'Changed' messages when the data key matches the expected openedKey, then invokes the internal handler for flag changes. This pattern illustrates actor message handling for data change events using Akka's match method in Java; primary dependency is Akka Typed.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-java/README.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n.match(Changed.class, c -> c.key().equals(openedKey), c -> receiveOpenedChanged((Changed<Flag>) c))\n```\n\n----------------------------------------\n\nTITLE: Initializing Flow.fromSinkAndSourceCoupled in Akka Streams (Java)\nDESCRIPTION: Creates a Flow by coupling a Sink and a Source, with coupled termination behavior. The method takes a Sink and a Source as parameters, returning a Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/fromSinkAndSourceCoupled.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nFlow.fromSinkAndSourceCoupled(akka.stream.Graph, akka.stream.Graph)\n```\n\n----------------------------------------\n\nTITLE: Setting TTL for Topic Actor in Akka Cluster PubSub - Scala\nDESCRIPTION: Shows how to configure a time-to-live (TTL) for a distributed topic actor in Scala so that it is automatically stopped if inactive. Uses the PubSub registry (akka.actor.typed.pubsub.PubSub) with an added TTL duration parameter. Requires the topic name, message type, and desired duration. The topic actor is removed if it has no subscribers or messages over the specified period.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nval topicWithTTL: ActorRef[Message] = \n  PubSub(context.system).topic[Message](\"my-topic\", 30.seconds)\n```\n\n----------------------------------------\n\nTITLE: Extending Flow Operators with Custom Extension Methods - Akka Streams - Scala\nDESCRIPTION: Illustrates enhancing Akka Streams' 'Flow' type by attaching custom extension methods through Scala implicits, facilitating fluent APIs. The technique relies on Scala's ability to add methods to classes via implicit value classes. Inputs: a Flow instance; Outputs: Flow with additional methods. Dependencies: Scala, Akka Streams, usage of implicit classes. This pattern is simpler for Flow/Source than SubFlow, due to Scala's type parameter limitations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-customize.md#2025-04-22_snippet_30\n\nLANGUAGE: Scala\nCODE:\n```\n// @@snip [GraphStageDocSpec.scala](/akka-docs/src/test/scala/docs/stream/GraphStageDocSpec.scala) { #extending-flow }\n\n```\n\n----------------------------------------\n\nTITLE: Implementing an Actor That Supports Backpressure in Scala\nDESCRIPTION: A Scala implementation of an actor that processes messages and acknowledges them to support backpressure. The actor handles various message types including stream elements, initialization, completion, and failure messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/actorRefWithBackpressure.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nclass Ref extends Actor {\n  override def receive: Receive = {\n    case StreamInit => \n      println(\"Stream initialized!\")\n      sender() ! Continue\n    case StreamElement(element) =>\n      println(s\"Stream element: $element\")\n      sender() ! Continue\n    case StreamComplete =>\n      println(\"Stream completed!\")\n      context.stop(self)\n    case StreamFailure(ex) =>\n      println(s\"Stream failed with: $ex\")\n      context.stop(self)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Actor with Custom Mailbox via Code in Java\nDESCRIPTION: Java code demonstrating how to create an actor with a specific mailbox type set programmatically. This allows direct mailbox specification in the actor creation code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n// this actor will have a custom mailbox applied programmatically\nfinal ActorRef otherActor = system.actorOf(Props.create(MyActor.class).withMailbox(\"prio-mailbox\"));\n```\n\n----------------------------------------\n\nTITLE: Using Java Flight Recorder Configuration\nDESCRIPTION: The akka.java-flight-recorder.enabled config has been removed. JFR events are now recorded directly and controlled via JVM flags.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guide-2.9.x-2.10.x.md#2025-04-22_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nakka.java-flight-recorder.enabled\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Incorrect Usage of asInputStream in Akka Streams\nDESCRIPTION: This example shows an anti-pattern that causes a deadlock when using asInputStream in mapMaterializedValue. The code will fail with a timeout exception because the read() method blocks indefinitely, preventing stream materialization from completing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/categories/additional-sink-and-source-converters.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n...\n.toMat(StreamConverters.asInputStream().mapMaterializedValue { inputStream =>\n        inputStream.read()  // this could block forever\n        ...\n}).run()\n```\n\n----------------------------------------\n\nTITLE: Error Recovery Output Example - Scala\nDESCRIPTION: Shows the expected output when using the recover operator in Scala, demonstrating the sequence of elements before and after failure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recover.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n1\n2\nfallback\n```\n\n----------------------------------------\n\nTITLE: Structural Changes in Serialization - Akka with Jackson\nDESCRIPTION: Explains how to implement arbitrary structural changes to serialized classes using Jackson with Akka. Changes are managed through a migration class that modifies the JSON structure accordingly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [CustomerMigration.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v2a/CustomerMigration.scala) { #structural }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [CustomerMigration.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v2a/CustomerMigration.java) { #structural }\n```\n\n----------------------------------------\n\nTITLE: Configuring Keep Oldest Strategy in Akka Cluster (HOCON)\nDESCRIPTION: Sets the `active-strategy` property within `akka.cluster.split-brain-resolver` to `keep-oldest` in the Akka configuration. This strategy prioritizes keeping the partition containing the oldest cluster node, which is often where the Cluster Singleton runs. The other partition(s) will be downed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_6\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.split-brain-resolver.active-strategy=keep-oldest\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for NACK-Based Write Back-Pressure in Java\nDESCRIPTION: Helper functions for the Java EchoHandler that support the back-pressure mechanism. These include specific states for handling write acknowledgments and waiting for TCP connection writing to resume.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\nprivate Receive waitingForAck(final List<ByteString> buffer) {\n  return receiveBuilder()\n      .match(\n          Tcp.WritingResumed.class,\n          msg -> {\n            if (!buffer.isEmpty()) {\n              connection.tell(\n                  TcpMessage.write(buffer.remove(0), Ack.getInstance()), getSelf());\n              return buffering(buffer);\n            } else {\n              return createReceive();\n            }\n          })\n      .match(\n          Tcp.Received.class,\n          msg -> {\n            final ByteString data = msg.data();\n            buffer.add(data);\n            return this;\n          })\n      .match(\n          Tcp.ConnectionClosed.class,\n          msg -> {\n            getContext().stop(getSelf());\n            return Stay();\n          })\n      .build();\n}\n\nprivate Receive waitingForAckAndClosing(\n    final Tcp.ConnectionClosed reason, final List<ByteString> buffer) {\n  return receiveBuilder()\n      .match(\n          Tcp.WritingResumed.class,\n          msg -> {\n            if (!buffer.isEmpty()) {\n              connection.tell(\n                  TcpMessage.write(buffer.remove(0), Ack.getInstance()), getSelf());\n              return closing(reason, buffer);\n            } else {\n              getContext().stop(getSelf());\n              return Stay();\n            }\n          })\n      .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring Stream Elements with FlowMonitor in Scala\nDESCRIPTION: Example showing how to use monitorMat operator to observe stream elements and completion state. Uses Keep.right to retain only the FlowMonitor materialization value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/monitor.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#monitor\n```\n\n----------------------------------------\n\nTITLE: Configuring Split Brain Resolver Provider in Akka\nDESCRIPTION: Configuration to enable the Split Brain Resolver by setting it as the downing provider in the ActorSystem configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.downing-provider-class = \"akka.cluster.sbr.SplitBrainResolverProvider\"\n```\n\n----------------------------------------\n\nTITLE: Using CompletionTimeout in Akka Streams - Scala\nDESCRIPTION: Example demonstrating how to use completionTimeout operator in Scala to process numbers with a 10ms completion timeout. The stream will fail if processing doesn't complete within the timeout period.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/completionTimeout.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#completionTimeout\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in Gradle\nDESCRIPTION: Specifies the Akka library repository configuration within a Gradle build script. This allows Gradle to resolve Akka dependencies from the official repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_2\n\nLANGUAGE: gradle\nCODE:\n```\nrepositories {\n  maven {\n    url = \"https://repo.akka.io/maven\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Application Settings Extension in Scala\nDESCRIPTION: Implements an Extension that loads application-specific settings from configuration. The Settings class parses the configuration while the SettingsExtension provides access to the settings instance.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor._\nimport com.typesafe.config.Config\n\nclass SettingsImpl(config: Config) extends Extension {\n  \n  val Greeting: String = config.getString(\"myapp.greeting\")\n  val Threads: Int = config.getInt(\"myapp.threads\")\n\n}\n\nobject Settings extends ExtensionId[SettingsImpl] {\n\n  override def createExtension(system: ExtendedActorSystem): SettingsImpl = \n    new SettingsImpl(system.settings.config)\n\n  // Java API\n  override def get(system: ActorSystem): SettingsImpl = super.get(system)\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Flow.flattenOptional in Java\nDESCRIPTION: Shows how to import and use the Flow.flattenOptional operation in Java. This method collects the value of Optional from all elements passing through the flow, filtering out empty Optional instances.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/flattenOptional.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nFlow.flattenOptional(akka.stream.javadsl.Flow)\n```\n\n----------------------------------------\n\nTITLE: Adding Optional Fields in Serialization - Akka with Jackson\nDESCRIPTION: Demonstrates how to add an optional field to a serialized class in Akka using the Jackson library. No migration code is required for optional fields as they default to `None` in Scala or `Optional.empty` in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ItemAdded.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v2a/ItemAdded.scala) { #add-optional }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ItemAdded.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v2a/ItemAdded.java) { #add-optional }\n```\n\n----------------------------------------\n\nTITLE: Referencing Akka Streams Operators in Markdown\nDESCRIPTION: References to Akka Streams operators like mapConcat and filter that transform element rates based on data, along with a link to backpressure-aware operators.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/categories/simple-operators.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`mapConcat`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`filter`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n[detached operators](#backpressure-aware-operators)\n```\n\n----------------------------------------\n\nTITLE: Starting Supervisor for Entity in Java\nDESCRIPTION: Demonstrates how to start a supervisor actor for an entity in Java using Cluster Sharding.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n@@snip [ClusterShardingTest.java](/akka-docs/src/test/java/jdocs/sharding/ClusterShardingTest.java) { #counter-supervisor-start }\n```\n\n----------------------------------------\n\nTITLE: Scala Sink.never Method Signature\nDESCRIPTION: API signature for creating a Sink that always backpressures in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/never.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nSink.never()\n```\n\n----------------------------------------\n\nTITLE: Querying Shard Region State in Scala\nDESCRIPTION: Code snippet demonstrating how to request the current state of a shard region using GetShardRegionState in Scala. This returns information about shards and entities in the region.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_51\n\nLANGUAGE: scala\nCODE:\n```\nval shardRegionState: Future[ShardRegion.CurrentShardRegionState] =\n  sharding.shardState(\"user\")\nshardRegionState.map { state =>\n  state.shards.foreach { case (shardId, entityIds) =>\n    println(s\"Shard: $shardId Entity IDs: $entityIds\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running gRPC Client\nDESCRIPTION: Command to start the gRPC client for querying user statistics.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nsbt \"client / run\"\n```\n\n----------------------------------------\n\nTITLE: Configuring RoundRobinPool Router via HOCON\nDESCRIPTION: Defines an Akka RoundRobinPool router named 'router1' within the application configuration (HOCON). This router uses the round-robin strategy and creates its own pool of 5 routee actors ('nr-of-instances').\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_12\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-round-robin-pool\nakka.actor.deployment {\n  /parent/router1 {\n    router = round-robin-pool\n    nr-of-instances = 5\n  }\n}\n//#config-round-robin-pool\n```\n\n----------------------------------------\n\nTITLE: Implementing FSM Tests in Scala\nDESCRIPTION: Test implementation for verifying FSM behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n#test-code\n```\n\n----------------------------------------\n\nTITLE: Waiting for Cluster Size Before Startup\nDESCRIPTION: Demonstrates how to configure an Akka cluster to wait for a minimum number of members before transitioning nodes from 'Joining' to 'Up' status. The configuration is set in the Akka configuration file and supports role-specific conditions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FactorialFrontendMain.java](/akka-docs/src/test/java/jdocs/cluster/FactorialFrontendMain.java) { #registerOnUp }\n```\n\n----------------------------------------\n\nTITLE: Advanced Journal Query Types in Akka Persistence - Java\nDESCRIPTION: Java implementation of advanced journal query types using Materialized Values. Shows how to provide additional materialization information like the nature of the stream, whether strictly ordered, finite, or infinite. Requires Akka Persistence and Streams libraries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [PersistenceQueryDocTest.java](/akka-docs/src/test/java/jdocs/persistence/PersistenceQueryDocTest.java) { #advanced-journal-query-types }\n```\n\n----------------------------------------\n\nTITLE: Implementing Read-Write Majority Consistency in Scala\nDESCRIPTION: Scala example demonstrating the use of WriteMajority and ReadMajority for consistency in distributed data operations. It shows how to define these consistency levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\nimplicit val timeout = Timeout(5.seconds)\\nval writeMajority = WriteMajority(timeout)\\nval readMajority = ReadMajority(timeout)\n```\n\n----------------------------------------\n\nTITLE: JVM Options for Node2\nDESCRIPTION: Example of node-specific JVM options file content for SampleMultiJvmNode2, setting remote port and memory options.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_13\n\nLANGUAGE: none\nCODE:\n```\n-Dakka.remote.port=9992 -Xmx256m\n```\n\n----------------------------------------\n\nTITLE: Implementing an Object-Oriented Style Counter Actor in Akka Typed (Java)\nDESCRIPTION: Illustrates an object-oriented Akka Typed (Java) counter actor, maintaining mutable state as fields within a class and creating behaviors via static factory methods. This style depends on the Akka Typed Java API, typically using `AbstractBehavior` for actor bodies. Accepts messages as input, updates class fields, and produces new or same behavior for subsequent processing. Follows the conventional OO approach and fits scenarios requiring mutable state or direct correspondence with classic actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// @@snip [StyleGuideDocExamples.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/StyleGuideDocExamples.java) { #oo-style }\n\n```\n\n----------------------------------------\n\nTITLE: Creating Child Actors in Scala Context\nDESCRIPTION: Shows how to create child actors using context.actorOf() within an existing actor. Demonstrates parent-child actor hierarchy creation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\n#context-actorOf\n```\n\n----------------------------------------\n\nTITLE: Initializing TCP Manager - Scala\nDESCRIPTION: Code showing how to import required dependencies and acquire a reference to the Akka TCP manager.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.{ Actor, ActorRef, Props }\nimport akka.io.{ IO, Tcp }\nimport java.net.InetSocketAddress\n```\n\n----------------------------------------\n\nTITLE: Cancellable Shutdown Tasks in Scala\nDESCRIPTION: Demonstrates how to create cancellable tasks in the coordinated shutdown process in Scala. Returns a Cancellable object that can be used to remove the task from the shutdown process later.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval task = CoordinatedShutdown(system).addCancellableTask(CoordinatedShutdown.PhaseBeforeServiceUnbind, \"cleanup\") { () =>\n  val promise = Promise[Done]()\n  actorRef ! Cleanup(promise)\n  promise.future\n}\n\n// To cancel task\ntask.cancel()\n```\n\n----------------------------------------\n\nTITLE: Changing Actor Behavior in Functional Style (Java)\nDESCRIPTION: Illustrates a common functional programming pattern in Akka Typed (Java) where an actor changes its behavior in response to messages (e.g., from `idle` to `workInProgress`) by returning a new behavior instance. This is relevant when applying supervision.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n// #wrap\nstatic Behavior<String> worker(int inProgress) {\n  return Behaviors.receiveMessage(\n      (String message) -> {\n        if (message.equals(\"step\")) {\n          return worker(inProgress + 1);\n        } else if (message.equals(\"reset\")) {\n          return worker(0);\n        } else {\n          return Behaviors.unhandled();\n        }\n      });\n}\n\nstatic Behavior<String> idle() {\n  return Behaviors.receiveMessage(\n      (String message) -> {\n        if (message.equals(\"step\")) {\n          return workInProgress(1);\n        } else {\n          return Behaviors.unhandled();\n        }\n      });\n}\n\nstatic Behavior<String> workInProgress(int count) {\n  return Behaviors.receiveMessage(\n      (String message) -> {\n        if (message.equals(\"step\")) {\n          return workInProgress(count + 1);\n        } else if (message.equals(\"done\")) {\n          return idle();\n        } else {\n          return Behaviors.unhandled();\n        }\n      });\n}\n// #wrap\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Scala Logging Listener\nDESCRIPTION: This Scala code snippet shows how to create a logging adapter using Akka's event stream, allowing customization of log output.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nval log = Logging(system.eventStream, \"my.nice.string\")\n```\n\n----------------------------------------\n\nTITLE: Obtaining a Read Journal Instance in Java\nDESCRIPTION: Shows how to get a reference to a read journal implementation using PersistenceQuery in Java. This is the first step to access persistence query capabilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nMyJavadslReadJournal readJournal =\n  PersistenceQuery.get(system).getReadJournalFor(MyJavadslReadJournal.class, \"akka.persistence.query.my-read-journal\");\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in Build Tools (sbt, Maven, Gradle)\nDESCRIPTION: Specifies the Akka library repository URL required to fetch Akka dependencies. This configuration needs to be added to the respective build tool's configuration file (sbt, Maven, or Gradle).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/stash.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\nrepositories {\n  mavenCentral()\n  maven {\n    url = \"https://repo.akka.io/maven\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing BackoffSupervisor for Sharded Actors\nDESCRIPTION: Shows configuration of BackoffSupervisor for sharded actors with termination message handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BackoffSupervisorDocSpec.scala](/akka-docs/src/test/scala/docs/pattern/BackoffSupervisorDocSpec.scala) { #backoff-sharded }\n```\n\n----------------------------------------\n\nTITLE: Starting Compute Node for Stats Cluster (Random Port)\nDESCRIPTION: Starts an additional compute node for the `sample.cluster.stats.App` example using SBT in a separate terminal. It specifies the role 'compute' and port 0 (random).\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.stats.App compute  0\"\n```\n\n----------------------------------------\n\nTITLE: Removing Item from Cart with Consistency in Scala\nDESCRIPTION: Scala example of removing an item from a cart in distributed storage. It demonstrates fetching the latest data before performing the removal to ensure consistency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_26\n\nLANGUAGE: Scala\nCODE:\n```\nprivate def removeItem(userId: String, itemId: String): Future[Done] = {\\n  implicit val timeout = Timeout(5.seconds)\\n  val update = Update(DataKey(userId), Cart.empty, writeMajority, None) { cart =>\\n    cart.removeItem(itemId)\\n  }\\n  (replicator ? Get(DataKey(userId), ReadMajority(timeout))).flatMap {\\n    case GetSuccess(_, _) => (replicator ? update).map(_ => Done)\\n    case NotFound(_, _) => (replicator ? update).map(_ => Done)\\n    case GetFailure(_, _) =>\\n      Future.failed(new IllegalStateException(\\\"Get failed\\\"))\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Stop Supervision Strategy Example\nDESCRIPTION: Shows the default stop supervision strategy where the stream fails when an exception occurs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\nval source = Source(0 to 5).map(100 / _)\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Logger Name in Java\nDESCRIPTION: Shows how to set a custom logger name for an actor in Java. This allows for more precise control over log output identification.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nclass MyActor extends AbstractBehavior<String> {\n  private final ActorContext<String> context;\n\n  public MyActor(ActorContext<String> context) {\n    this.context = context;\n    context.setLoggerName(\"my.custom.logger.name\");\n  }\n\n  @Override\n  public Behavior<String> onMessage(String msg) {\n    context.getLog().debug(\"Received message {}\", msg);\n    return this;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Periodic Ticking in Scala\nDESCRIPTION: Demonstrates basic usage of Source.tick to print a message every second with 0 initial delay.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/tick.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource\n  .tick(0.seconds, 1.second, \"tick\")\n  .runForeach(println)\n```\n\n----------------------------------------\n\nTITLE: Creating Materializer from ActorContext in Java\nDESCRIPTION: This Java snippet creates a materializer from akka.actor.ActorContext, connecting the stream's lifecycle to the actor, showcasing how actor-bound lifecycles are managed in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowDocTest.java) { #materializer-from-actor-context }\n```\n\n----------------------------------------\n\nTITLE: Initializing ClusterSingletonProxy in Akka Scala\nDESCRIPTION: This Scala snippet sets up a ClusterSingletonProxy, allowing nodes to communicate with the singleton actor by tracking the current master node. It uses ClusterSingletonProxy from Akka’s cluster-tools with settings to define system roles and the path to the singleton manager. The proxy facilitates message delegation to the singleton actor, and it’s reliant on path settings and proper initialization of the Akka system and actor paths.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\nsystem.actorOf(\n  ClusterSingletonProxy.props(\n    singletonManagerPath = \"/user/statsService\",\n    settings = ClusterSingletonProxySettings(system).withRole(\"compute\")),\n  name = \"statsServiceProxy\")\n```\n\n----------------------------------------\n\nTITLE: Formatting Java Source Code\nDESCRIPTION: SBT commands to format Java source code in Akka actor module.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsbt\nproject akka-actor\njavafmtAll\n```\n\n----------------------------------------\n\nTITLE: Running the Fog Network with SBT - Bash\nDESCRIPTION: This bash code snippet shows how to start the Fog application, simulating remote weather stations submitting data to a running KillrWeather cluster via HTTP. The sbt runMain command launches the Fog entry point and accepts a variable number of port numbers corresponding to accessible WeatherServer HTTP endpoints. The cluster should be started beforehand so that endpoints are available on the specified ports.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-scala/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsbt \"killrweather-fog/runMain sample.killrweather.fog.Fog 8081 8033 8056\"\n```\n\n----------------------------------------\n\nTITLE: Buffer Until Changed Pattern in Scala\nDESCRIPTION: Implementation that buffers elements until the incoming element changes, then emits the buffered elements downstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMap.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nval numbers = Source(List(1, 1, 1, 2, 2, 3, 3, 3))\n  .statefulMap(() => (Vector.empty[Int], Option.empty[Int]))(\n    (state, element) => {\n      val (buffer, prev) = state\n      prev match {\n        case Some(p) if p == element =>\n          ((buffer :+ element, prev), Vector.empty)\n        case _ =>\n          ((Vector(element), Some(element)), buffer)\n      }\n    },\n    state => Some(state._1)\n  )\n```\n\n----------------------------------------\n\nTITLE: Helper Methods for Java Echo Handler (ACK-Based Back-Pressure)\nDESCRIPTION: This Java snippet provides helper methods for the `SimpleEchoHandler`. The `buffer` method adds incoming data to a storage vector and initiates writing the first chunk if the buffer was empty. The `acknowledge` method removes the acknowledged data chunk from storage and sends the next available chunk, managing the `Ack` sequence number. It also handles suspending/resuming reading based on buffer size and closing the connection.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nprivate void buffer(ByteString data) {\n    storage.add(data);\n    stored += data.size();\n\n    if (stored > maxStored) {\n      log.warning(\"New data {}, total stored: {} exceeds max: {}\", data.size(), stored, maxStored);\n      connection.tell(TcpMessage.suspendReading(), getSelf());\n    }\n\n    if (stored == data.size()) {\n      // storage was empty, start writing\n      connection.tell(TcpMessage.write(storage.get(0), Ack.create(nextAck)), getSelf());\n    }\n  }\n\n  private void acknowledge(int ack) {\n    assert (!storage.isEmpty());\n    assert (ack == nextAck);\n\n    final ByteString acknowledged = storage.get(0);\n    final int size = acknowledged.size();\n\n    storage.remove(0);\n\n    stored -= size;\n    transferred += size;\n    nextAck += 1;\n\n    if (stored < maxStored) {\n      connection.tell(TcpMessage.resumeReading(), getSelf());\n    }\n\n    if (storage.isEmpty()) {\n      if (closed) {\n        getContext().stop(getSelf());\n      }\n    } else {\n      connection.tell(TcpMessage.write(storage.get(0), Ack.create(nextAck)), getSelf());\n    }\n  }\n\n  private Receive closing() {\n    return receiveBuilder()\n        .match(\n            Ack.class,\n            ack -> {\n              acknowledge(ack.ack);\n            })\n        .match(\n            CommandFailed.class,\n            failed -> {\n              // write failed, retry\n              connection.tell(failed.cmd(), getSelf());\n            })\n        .match(\n            ConnectionClosed.class,\n            closed -> {\n              log.warning(\"Received {} closes after pending write(s) {}}\", closed, storage.size());\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Domain Model Class for Custom Serialization in Akka Persistence (Java)\nDESCRIPTION: Defines a simple Java class ('Person'), intended for use as the payload in Akka Persistence events. This model is necessary as the target for custom serialization logic and demonstrates field encapsulation via Java bean conventions. This snippet requires standard Java and does not depend on Akka by itself.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic class Person {\n    private final String name;\n    private final int age;\n\n    public Person(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    public String getName() { return name; }\n    public int getAge() { return age; }\n}\n```\n\n----------------------------------------\n\nTITLE: Wrapping Blocking Call in Future (Scala)\nDESCRIPTION: This snippet demonstrates a non-solution of wrapping a blocking call in a Future, which can still lead to bottlenecks under increased load.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BlockingDispatcherSample.scala](/akka-docs/src/test/scala/docs/actor/typed/BlockingDispatcherSample.scala) { #blocking-in-future }\n```\n\n----------------------------------------\n\nTITLE: Blocking Dispatcher Test (Java)\nDESCRIPTION: This snippet shows how to set up an application with blocking and non-blocking actors in Java to demonstrate the issue of thread starvation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [BlockingDispatcherTest.java](/akka-docs/src/test/java/jdocs/actor/typed/BlockingDispatcherTest.java) { #blocking-main }\n```\n\n----------------------------------------\n\nTITLE: Restarting Source on Completion in Java\nDESCRIPTION: This example demonstrates that a Source is not restarted if it completes, only if it fails. The 'tick' is only printed three times as the take(3) means the inner source completes successfully after emitting the first 3 elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartSource/onFailuresWithBackoff.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nRestartSource.onFailuresWithBackoff(\n    Duration.ofSeconds(1),\n    Duration.ofSeconds(10),\n    0.2,\n    () ->\n        Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), \"tick\")\n            .take(3)\n            .map(\n                s -> {\n                  System.out.println(s);\n                  return s;\n                }))\n    .run(system);\n```\n\n----------------------------------------\n\nTITLE: Creating RandomPool Programmatically (Scala/Java)\nDESCRIPTION: Illustrates creating a RandomPool router actor ('router2') programmatically, specifying the random routing logic and the pool size (5) directly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\n//#random-pool-2\nimport akka.routing.RandomPool\n\nval router2: ActorRef = context.actorOf(RandomPool(5).props(Props[Worker]()), \"router2\")\n//#random-pool-2\n```\n\nLANGUAGE: java\nCODE:\n```\n//#random-pool-2\nimport akka.routing.RandomPool;\n\nfinal ActorRef router2 = getContext().actorOf(new RandomPool(5).props(Props.create(Worker.class)), \"router2\");\n//#random-pool-2\n```\n\n----------------------------------------\n\nTITLE: Defining ItemAdded Event Class with Optional Field in Java\nDESCRIPTION: Java code snippet showing the initial ItemAdded event class with an optional field before renaming.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ItemAdded.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v1/ItemAdded.java) { #add-optional }\n```\n\n----------------------------------------\n\nTITLE: Flattening a Stream of Lists (Akka Streams Java)\nDESCRIPTION: Covers utilizing mapConcat in Java Akka Streams to flatten List-valued streams into a flow of individual elements. Common when upstream producers aggregate data which must then be split. Expects lists as input; outputs their elements as the new stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeFlattenList.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeFlattenList.java) { #flattening-lists }\n```\n\n----------------------------------------\n\nTITLE: Creating an Actor with Custom Mailbox via Deployment in Scala\nDESCRIPTION: Scala code showing how to create an actor that uses a mailbox configured in deployment configuration. This approach allows separating actor creation from mailbox selection.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n// this actor will have a custom mailbox applied in deployment config\nval myActor = context.actorOf(Props[MyActor](), \"priomailboxactor\")\n```\n\n----------------------------------------\n\nTITLE: Hiding Extension Behind Traits in Scala\nDESCRIPTION: Demonstrates how to hide extension implementation details behind traits in Scala. This approach improves code organization and encapsulation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\ntrait CounterComponent { this: Actor =>\n  val counter = Counter(context.system)\n}\n\nclass MyCounterActor extends Actor with CounterComponent {\n  def receive = {\n    case someMessage =>\n      val counterValue = counter.increment()\n      sender() ! counterValue\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing RetryFlow.withBackoff in Scala\nDESCRIPTION: This snippet demonstrates how to use RetryFlow.withBackoff in Scala to wrap a flow handling Ints, retrying elements unless the result is 0 or negative, or maxRetries is hit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RetryFlow/withBackoff.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval flow: Flow[Int, Int, NotUsed] = Flow[Int].map(x => if (x < 5) x else throw new RuntimeException(\"x too big\"))\n\nval retryFlow: Flow[Int, Int, NotUsed] = RetryFlow.withBackoff(\n  minBackoff = 10.millis,\n  maxBackoff = 5.seconds,\n  randomFactor = 0.2,\n  maxRetries = 5,\n  flow\n) { (in, out) =>\n  out match {\n    case Success(x) if x <= 0 => None // don't retry on 0 or negative values\n    case Success(_)           => Some(in) // retry on positive values\n    case Failure(_)           => Some(in) // retry on failure\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Snapshot Storage Policy in Scala\nDESCRIPTION: Example showing how to configure a custom snapshot storage policy in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n\"#set-snapshot-storage-policy\"\n```\n\n----------------------------------------\n\nTITLE: Merging Substreams after groupBy in Java\nDESCRIPTION: Shows how to merge the elements from all substreams created by `groupBy` back into a single stream using `mergeSubstreams`. The order of elements in the merged stream depends on the emission order of the substreams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n//#groupBy3\nsource\n    .groupBy(2, element -> element % 2 == 0)\n    .mergeSubstreams()\n    .runWith(Sink.fold(0, (acc, i) -> acc + i), system);\n//#groupBy3\n```\n\n----------------------------------------\n\nTITLE: Defining HelloWorld Actor in Akka Classic (Java)\nDESCRIPTION: Example of a classic Akka actor extending the AbstractActor class to create a simple HelloWorld actor in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/from-classic.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass HelloWorldClassic extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"hello\", s -> {\n          System.out.println(\"Hello World!\");\n        })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Manually Creating Kafka Topic (Shell)\nDESCRIPTION: This shell command provides an alternative way to create the necessary Kafka topic if not using the embedded Kafka server from the sample project. It uses the standard `kafka-topics.sh` script (part of a Kafka installation) to create the `user-events` topic with 128 partitions and a replication factor of 1 on a Kafka broker running at `localhost:9092`. This is an optional step if the embedded Kafka is not used.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 128 --topic user-events\n```\n\n----------------------------------------\n\nTITLE: External Shard Allocation Entity Configuration in Java\nDESCRIPTION: Java example showing how to configure an entity with external shard allocation strategy\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\nEntity.of(\\n    typeKey,\\n    entityContext -> Counter.create(entityContext.getEntityId())\\n).withAllocationStrategy(\\n    new ExternalShardAllocationStrategy(system, typeKey.name())\\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Storage Policy Test in Scala\nDESCRIPTION: Example showing how to test a custom storage policy implementation in Scala, demonstrating policy behavior testing\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\n\"#policy-test\"\n```\n\n----------------------------------------\n\nTITLE: Drop Tail Buffer Strategy in Akka Streams\nDESCRIPTION: Implements a buffer that drops the youngest element (from tail) when full to make space for new elements. Useful for maintaining fairness for older jobs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\nexternalService.runWith(\n  Flow[Job].buffer(1000, OverflowStrategy.dropTail)\n)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(externalService)\n  .buffer(1000, OverflowStrategy.dropTail())\n  .run(system)\n```\n\n----------------------------------------\n\nTITLE: Child Actor Implementation\nDESCRIPTION: Simple no-op child actor implementation used for testing parent-child relationships.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-sync.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nobject Child {\n  def apply(): Behavior[String] =\n    Behaviors.empty\n}\n```\n\nLANGUAGE: java\nCODE:\n```\nclass Child {\n  static Behavior<String> create() {\n    return Behaviors.empty();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Tell Protection Pattern with CircuitBreaker (Java)\nDESCRIPTION: This example shows how to implement the Tell Protection pattern using the low-level CircuitBreaker API in Java, manually tracking success and failure for operations that expect replies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nstatic class DangerousJavaActor extends AbstractActor {\n\n  private final CircuitBreaker circuitBreaker;\n\n  public DangerousJavaActor() {\n    this.circuitBreaker =\n        new CircuitBreaker(\n            getContext().getSystem().getScheduler(),\n            5,\n            Duration.ofSeconds(10),\n            Duration.ofMinutes(1));\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            message -> {\n              if (\"is-closed\".equals(message)) {\n                getSender().tell(circuitBreaker.isClosed(), getSelf());\n              } else if (\"dangerous-call\".equals(message)) {\n                if (circuitBreaker.isClosed() || circuitBreaker.isHalfOpen()) {\n                  final ActorRef client = getSender();\n                  getContext()\n                      .getSystem()\n                      .getScheduler()\n                      .scheduleOnce(\n                          Duration.ofSeconds(5),\n                          () -> client.tell(\"call-result\", getSelf()),\n                          getContext().getDispatcher());\n\n                  // We know that the call succeeded so we tell the circuit breaker\n                  circuitBreaker.succeed();\n                } else {\n                  getSender().tell(\"circuit-breaker-open\", getSelf());\n                }\n              } else if (\"exception-call\".equals(message)) {\n                if (circuitBreaker.isClosed() || circuitBreaker.isHalfOpen()) {\n                  final ActorRef client = getSender();\n                  getContext()\n                      .getSystem()\n                      .getScheduler()\n                      .scheduleOnce(\n                          Duration.ofSeconds(5),\n                          () ->\n                              client.tell(\n                                  new Status.Failure(new RuntimeException(\"Bad response\")),\n                                  getSelf()),\n                          getContext().getDispatcher());\n\n                  // We know that the call failed so we tell the circuit breaker\n                  circuitBreaker.fail();\n                } else {\n                  getSender().tell(\"circuit-breaker-open\", getSelf());\n                }\n              }\n            })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Snapshot Selection Criteria in Java\nDESCRIPTION: Demonstrates configuring snapshot selection criteria in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_31\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic Recovery recovery() {\n  return Recovery.create(\n      SnapshotSelectionCriteria.create(\n          457L,\n          System.currentTimeMillis()));\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Update Response Pattern 2 in Scala\nDESCRIPTION: Alternative approach to handling Update responses in Scala by matching on the key with if-guards and extracting the response type.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nreplicator ! Update(Counter1Key, PNCounter.empty, WriteLocal)(_ :+ 1)\n\nreceive {\n  case success @ UpdateSuccess(key, _) if key == Counter1Key =>\n    // ok\n  case UpdateTimeout(key, _) if key == Counter1Key =>\n    // will eventually be replicated, but read from other nodes might not\n    // see the update yet\n}\n```\n\n----------------------------------------\n\nTITLE: JVM Shutdown Hook Registration in Java\nDESCRIPTION: Demonstrates how to register a JVM shutdown hook via Coordinated Shutdown in Java. This ensures application hooks run in a controlled order relative to Akka's internal shutdown processes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nCoordinatedShutdown.get(system).addJvmShutdownHook(() -> {\n  System.out.println(\"custom JVM shutdown hook...\");\n});\n```\n\n----------------------------------------\n\nTITLE: Cluster Owner Configuration in Scala\nDESCRIPTION: Demonstrates how to set up a cluster-specific lease owner in Scala using cluster host port.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordination.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n#cluster-owner\n```\n\n----------------------------------------\n\nTITLE: Actor Lifecycle Hooks in Scala\nDESCRIPTION: Shows the default lifecycle hook implementations in the Actor trait, including preStart, postStop, preRestart, and postRestart methods.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n#lifecycle-hooks\n```\n\n----------------------------------------\n\nTITLE: Full Service Lookup with Port and Protocol in Scala\nDESCRIPTION: Advanced example of service lookup in Scala that includes service name, port name, and protocol. This is used for more specific service discovery queries, particularly useful for SRV DNS record lookups.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval lookup: Future[Resolved] =\n  discovery.lookup(Lookup(\"service-name\").withPortName(\"http\").withProtocol(\"tcp\"), resolveTimeout)\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Snapshot Repository in Maven\nDESCRIPTION: XML configuration for adding Akka snapshot repository and dependencies to Maven pom.xml file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/links.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<repositories>\n  <repositories>\n    <repository>\n      <id>akka-repository</id>\n      <name>Akka library snapshot repository</name>\n      <url>https://repo.akka.io/snapshots</url>\n    </repository>\n  </repositories>\n</repositories>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-cluster_$scala.binary.version$</artifactId>\n    <version>2.9.0+72-53943d99-SNAPSHOT</version>\n  </dependency>\n</dependencies>\n```\n\n----------------------------------------\n\nTITLE: Running Coordinated Shutdown in Scala\nDESCRIPTION: Demonstrates how to manually trigger the coordinated shutdown process in Scala. The run method takes a reason parameter that describes why the shutdown was initiated.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nCoordinatedShutdown(system).run(UnknownReason)\n```\n\n----------------------------------------\n\nTITLE: Log Output Showing Kafka Partition Assignment (Text)\nDESCRIPTION: This log snippet illustrates the output seen when the processor node starts and connects to Kafka. As it's initially the only consumer in its group, it gets assigned all Kafka partitions for the `user-events` topic. The logs show the processor acknowledging the assignment of specific partitions (e.g., 1, 25, 116) and indicating that it will update the shard allocation based on these assignments, which is key to the external shard allocation strategy.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n[info] [2020-01-16 09:48:51,040] [INFO] [akka://KafkaToSharding/user/kafka-event-processor/rebalancerRef] - Partition [1] assigned to current node. Updating shard allocation\n[info] [2020-01-16 09:48:51,040] [INFO] [akka://KafkaToSharding/user/kafka-event-processor/rebalancerRef] - Partition [25] assigned to current node. Updating shard allocation\n[info] [2020-01-16 09:48:51,043] [INFO] [akka://KafkaToSharding/user/kafka-event-processor/rebalancerRef] - Partition [116] assigned to current node. Updating shard allocation\n```\n\n----------------------------------------\n\nTITLE: RestartFlow.withBackoff API Signature in Scala and Java\nDESCRIPTION: API signature for RestartFlow.withBackoff that accepts RestartSettings and a factory function to create the flow that should be restarted upon completion or failure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartFlow/withBackoff.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nRestartFlow.withBackoff[In,Out](settings:akka.stream.RestartSettings)(flowFactory:()=&gt;akka.stream.scaladsl.Flow[In,Out,_]):akka.stream.scaladsl.Flow[In,Out,akka.NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nRestartFlow.withBackoff(akka.stream.RestartSettings,akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Example of Network Message Ordering\nDESCRIPTION: Illustrative example showing how message ordering can be affected in a distributed actor system across multiple nodes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/message-delivery-reliability.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nActor `A` on node-1 sends message `M1` to actor `C` on node-3\nActor `A` on node-1 then sends message `M2` to actor `B` on node-2\nActor `B` on node-2 forwards message `M2` to actor `C` on node-3\nActor `C` may receive `M1` and `M2` in any order\n```\n\n----------------------------------------\n\nTITLE: Using Behaviors.withMdc for Custom MDC in Scala\nDESCRIPTION: Demonstrates how to use Behaviors.withMdc to add custom Mapped Diagnostic Context (MDC) attributes to log entries in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nval behavior: Behavior[Protocol] = Behaviors.setup { context =>\n  Behaviors.withMdc(\n    Protocol.classTag,\n    Map(\"startTime\" -> System.currentTimeMillis.toString),\n    {      \n      case LoggedProtocol(body) => Map(\"messageId\" -> body.id)\n      case _                    => Map.empty[String, String]\n    }) {\n    Behaviors.receiveMessage {\n      case LoggedProtocol(body) =>\n        context.log.info(\"Received message {}\", body.payload)\n        Behaviors.same\n      case other =>\n        context.log.info(\"Received other message\")\n        Behaviors.same\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Sink.last Operator in Java\nDESCRIPTION: Example demonstrating how to use the Sink.last operator to obtain the last element of a stream in Java. It materializes into a CompletionStage which will complete with the last value when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/last.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [SinkDocExamples.java](/akka-docs/src/test/java/jdocs/stream/operators/SinkDocExamples.java) { #last-operator-example }\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-JVM in build.sbt\nDESCRIPTION: Configuration for enabling the MultiJvmPlugin and setting the MultiJvm config in your build.sbt or project/Build.scala file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\nlazy val root = (project in file(\".\"))\n  .enablePlugins(MultiJvmPlugin)\n  .configs(MultiJvm)\n```\n\n----------------------------------------\n\nTITLE: Git Commands for Documentation Update\nDESCRIPTION: Shell commands for updating and committing documentation changes on gustav.akka.io server as akkarepo user.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/release-train-issue-template.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd ~/www\ngit status\ngit add libraries/akka-core/current libraries/akka-core/$VERSION$\ngit add api/akka-core/current api/akka-core/$VERSION$\ngit add japi/akka-core/current japi/akka-core/$VERSION$\ngit commit -m \"Akka core $VERSION$\"\n```\n\n----------------------------------------\n\nTITLE: Auction Setup Configuration in Scala\nDESCRIPTION: Initial setup and configuration for the auction entity including minimum bid and replica settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing-auction.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\nEventSourcedBehavior[\n  Command,\n  Event,\n  AuctionState\n](\n  persistenceId = PersistenceId.ofUniqueId(\"auction\"),\n  emptyState = AuctionState(isOpen = true, Bid(initialBid, 0L, \"\"), highestCounterOffer = 0),\n  commandHandler = commandHandler(),\n  eventHandler = eventHandler()\n).withReplication(ReplicationId(\"AuctionEntity\", context.self.path.name, replicaId))\n```\n\n----------------------------------------\n\nTITLE: Running Broadcast Outputs Asynchronously in Scala\nDESCRIPTION: This Scala example shows how to introduce asynchronous boundaries using `.async` after the `Broadcast` stage. This allows the downstream processing steps (sum and count calculations) to potentially run in parallel on different actors or threads, which can improve throughput, especially for computationally intensive tasks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Broadcast.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval source = Source(1 to 10)\n\nval graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n  import GraphDSL.Implicits._\n\n  val broadcast = builder.add(Broadcast[Int](2))\n  val merge = builder.add(Zip[BigInt, Int]())\n\n  val sumSink = Sink.fold[BigInt, Int](0)(_ + _)\n  val countSink = Sink.fold[Int, Int](0)((c, _) => c + 1)\n\nsource ~> broadcast\n\n  broadcast.out(0).async ~> Flow[Int].map(BigInt(_)) ~> merge.in0\n  broadcast.out(1).async ~> merge.in1\n\n  merge.out ~> Sink.foreach(println)\n\n  ClosedShape\n})\n\ngraph.run()\n```\n\n----------------------------------------\n\nTITLE: Specifying JVM Options for Forked JVMs\nDESCRIPTION: sbt configuration for specifying JVM options that will be applied to all forked JVMs in multi-JVM tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\njvmOptions in MultiJvm := Seq(\"-Xmx256M\")\n```\n\n----------------------------------------\n\nTITLE: Terminating FSM from Inside in Akka Scala\nDESCRIPTION: Defines the procedure to stop a finite state machine (FSM) within Akka when a specific state is reached. The stop function takes an optional reason and data parameters for contextual termination. Utilizes the onTermination function to specify custom code execution upon stopping. Applicable in Scala-based Akka implementations, leveraging state and event management capabilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\nstop([reason[, data]])\n```\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FSMDocSpec.scala](/akka-docs/src/test/scala/docs/actor/FSMDocSpec.scala) { #stop-syntax }\n```\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FSMDocSpec.scala](/akka-docs/src/test/scala/docs/actor/FSMDocSpec.scala) { #termination-syntax }\n```\n\n----------------------------------------\n\nTITLE: Implementing Identity Event Adapter in Scala\nDESCRIPTION: Shows how to implement a basic identity event adapter in Scala that doesn't transform events. This adapter simply passes events through without modification during journaling and recovery.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_34\n\nLANGUAGE: Scala\nCODE:\n```\nclass IdentityEventAdapter extends EventAdapter {\n  override def fromJournal(event: Any, manifest: String): EventSeq = EventSeq.single(event)\n  override def toJournal(event: Any): Any = event\n  override def manifest(event: Any): String = \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Output of Stream Recovery Example (Scala)\nDESCRIPTION: Shows the expected output of the Scala `recover` example. The stream emits elements '0' through '4', then catches the `RuntimeException`, emits the recovery element 'stream truncated', and completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n0\n1\n2\n3\n4\nstream truncated\n```\n\n----------------------------------------\n\nTITLE: Final ItemAdded Migration in Scala\nDESCRIPTION: Scala code snippet showing the final migration class for ItemAdded without forward-compatibility code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ItemAddedMigration.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v2c/ItemAddedMigration.scala) { #rename }\n```\n\n----------------------------------------\n\nTITLE: Subscribing to All DeadLetters in Scala\nDESCRIPTION: Scala code demonstrating how to subscribe to all DeadLetters including suppressed ones. This provides complete visibility into message delivery failures.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_25\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.AllDeadLetters\n// subscribe to all dead letters\nactorSystem.eventStream.subscribe(actor, classOf[AllDeadLetters])\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Actor Deployment in HOCON\nDESCRIPTION: Configuration for remote actor deployment specifying the remote system address and actor path.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_21\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  actor {\n    deployment {\n      /sampleActor {\n        remote = \"akka.tcp://sampleActorSystem@127.0.0.1:2553\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Akka Actor using message passing in Java\nDESCRIPTION: This snippet shows how to initialize an Akka actor using message passing in Java. It uses the become method to transition between uninitialized and initialized states.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_47\n\nLANGUAGE: Java\nCODE:\n```\nstatic class InitializeMsg {\n    public final String data;\n    public InitializeMsg(String data) {\n        this.data = data;\n    }\n}\n\nstatic class Initialized {\n    private final String data;\n    public Initialized(String data) {\n        this.data = data;\n    }\n    // getters, equals, hashCode\n}\n\npublic static class InitializeViaMsgActor extends AbstractActor {\n    private String initializeMe = null;\n\n    @Override\n    public Receive createReceive() {\n        return receiveBuilder()\n            .match(InitializeMsg.class, this::initializeMe)\n            .match(String.class, this::replyIfInitialized)\n            .build();\n    }\n\n    private void initializeMe(InitializeMsg msg) {\n        initializeMe = msg.data;\n        getContext().become(receiveBuilder()\n            .match(String.class, this::replyIfInitialized)\n            .build());\n    }\n\n    private void replyIfInitialized(String msg) {\n        if (\"get\".equals(msg) && initializeMe != null)\n            getSender().tell(new Initialized(initializeMe), getSelf());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Optional Initial State (Scala)\nDESCRIPTION: Demonstrates how to use Option[State] as the state type and None as the emptyState in Scala, allowing for an optional initial state in Akka Persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-style.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [AccountExampleWithOptionState.scala](/akka-cluster-sharding-typed/src/test/scala/docs/akka/cluster/sharding/typed/AccountExampleWithOptionState.scala) { #account-entity }\n```\n\n----------------------------------------\n\nTITLE: Deferred Subscription until MemberUp in Scala\nDESCRIPTION: Example of deferring cluster event subscription until the MemberUp event for the own node is received. This helps avoid handling an initial empty CurrentClusterState.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nval selfAddress = Cluster(system).selfAddress\nCluster(system).join(selfAddress)\n\nCluster(system).registerOnMemberUp {\n  println(s\"Member is up!\")\n  Cluster(system).subscribe(self, classOf[ClusterEvent.MemberEvent])\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Configuration Compatibility Checks in Akka Cluster\nDESCRIPTION: Configures Akka Cluster to disable configuration compatibility checks when new nodes join. This is part of a two-step approach for migrating Cluster Sharding from Classic to Typed Actors during a rolling update.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/rolling-updates.md#2025-04-22_snippet_1\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.configuration-compatibility-check.enforce-on-join = off\n```\n\n----------------------------------------\n\nTITLE: Terminating FSM from Inside in Akka Java\nDESCRIPTION: Details the termination process of FSM in Java implementation of Akka, using the stop method with potential reason and data parameters. The approach involves using the onTermination function to handle stoppage events. Designed for Java developers using Akka framework to manage actor states effectively.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FSMDocTest.java](/akka-docs/src/test/java/jdocs/actor/fsm/FSMDocTest.java) { #stop-syntax }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FSMDocTest.java](/akka-docs/src/test/java/jdocs/actor/fsm/FSMDocTest.java) { #termination-syntax }\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Cluster App Version for Rolling Updates\nDESCRIPTION: Sets the app-version configuration property used by rolling update features to distinguish between old and new nodes. This is important for features like the LeastShardAllocationStrategy to work correctly during updates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/rolling-updates.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.app-version = 1.2.3\n```\n\n----------------------------------------\n\nTITLE: Using BoundedSourceQueue in Scala\nDESCRIPTION: Example demonstrating how to use a BoundedSourceQueue with synchronous feedback in Scala. Shows queue creation, offering elements, and handling the queue offer results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/queue.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval queueSize = 10\nval queue = Source\n  .queue[Int](queueSize)\n  .map(x => x * x)\n  .toMat(Sink.foreach(x => println(s\"completed $x\")))(Keep.left)\n  .run()\n\nqueue.offer(1) match {\n  case QueueOfferResult.Enqueued    => println(\"enqueued\")\n  case QueueOfferResult.Dropped     => println(\"dropped\")\n  case QueueOfferResult.Failure(ex) => println(s\"Offer failed: $ex\")\n  case QueueOfferResult.QueueClosed => println(\"queue closed\")\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring BalancingPool with Custom Mailbox (HOCON)\nDESCRIPTION: Configures the dispatcher for a BalancingPool to use a custom mailbox type (e.g., 'prio-mailbox'). This allows for custom message handling, such as priority queuing. The custom mailbox must implement `akka.dispatch.MultipleConsumerSemantics`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_26\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-balancing-pool4\nmy-dispatcher {\n  mailbox-type = \"docs.routing.PriorityMailbox\"\n  //Other dispatcher configuration properties\n}\n\nakka.actor.deployment {\n  /parent/router1 {\n    router = balancing-pool\n    nr-of-instances = 5\n    pool-dispatcher {\n      id = \"my-dispatcher\"\n    }\n  }\n}\n//#config-balancing-pool4\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Router Group Configuration in Akka\nDESCRIPTION: Implements a RedundancyGroup class that extends Group to create a configurable router that uses the custom RedundancyRoutingLogic. This allows the router to be used as a standard Akka router component.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_44\n\nLANGUAGE: Scala\nCODE:\n```\ncase class RedundancyGroup(routeePaths: immutable.Iterable[String], nbrCopies: Int = 3)\n    extends Group {\n\n  def this(config: Config) =\n    this(routeePaths = immutableSeq(config.getStringList(\"routees.paths\")),\n      nbrCopies = config.getInt(\"nbr-copies\"))\n\n  override def createRouter(system: ActorSystem): Router =\n    new Router(new RedundancyRoutingLogic(nbrCopies))\n\n  override def routerDispatcher: String = Dispatchers.DefaultDispatcherId\n\n  override def supervisorStrategy: SupervisorStrategy = SupervisorStrategy.defaultStrategy\n\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\npublic class RedundancyGroup extends Group {\n  private final int nbrCopies;\n\n  public RedundancyGroup(List<String> paths) {\n    this(paths, 3);\n  }\n\n  public RedundancyGroup(List<String> paths, int nbrCopies) {\n    super(paths);\n    this.nbrCopies = nbrCopies;\n  }\n\n  public RedundancyGroup(Config config) {\n    this(JavaConverters.asJavaCollectionConverter(config.getStringList(\"routees.paths\")).asJava(),\n        config.getInt(\"nbr-copies\"));\n  }\n\n  @Override\n  public Router createRouter(ActorSystem system) {\n    return new Router(new RedundancyRoutingLogic(nbrCopies));\n  }\n\n  @Override\n  public String routerDispatcher() {\n    return Dispatchers.DefaultDispatcherId();\n  }\n\n  @Override\n  public SupervisorStrategy supervisorStrategy() {\n    return SupervisorStrategy.defaultStrategy();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering and Mapping Tweet Stream in Scala\nDESCRIPTION: Demonstrates filtering tweets containing '#akka' and mapping to author handles in Scala Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_26\n\nLANGUAGE: Scala\nCODE:\n```\nval authors: Source[Author, NotUsed] =\n  tweets\n    .filter(_.hashtags.contains(Hashtag(\"#akka\")))\n    .map(_.author)\n```\n\n----------------------------------------\n\nTITLE: Defining Commands for a Counter Actor (Java)\nDESCRIPTION: Defines the command protocol for a simple counter actor using a Java interface `Command`. It includes commands for incrementing (`Increment`, `IncrementBy`) and retrieving the current value (`GetValue`, which includes an `ActorRef` for the reply).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.typed.ActorRef;\n\ninterface Command {}\n\npublic enum Increment implements Command {\n  INSTANCE\n}\n\npublic static class IncrementBy implements Command {\n  public final int value;\n\n  public IncrementBy(int value) {\n    this.value = value;\n  }\n}\n\npublic static class GetValue implements Command {\n  public final ActorRef<State> replyTo;\n\n  public GetValue(ActorRef<State> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Round Robin Pool Configuration\nDESCRIPTION: Configuration snippet for creating a round-robin router with five Worker routees as children\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_1\n\nLANGUAGE: Configuration\nCODE:\n```\n@@snip [RouterDocSpec.scala](/akka-docs/src/test/scala/docs/routing/RouterDocSpec.scala) { #config-round-robin-pool }\n```\n\n----------------------------------------\n\nTITLE: Configuring ScatterGatherFirstCompletedPool Router\nDESCRIPTION: Configuration for ScatterGatherFirstCompletedPool router that sends messages to all routees and returns the first response received.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_29\n\nLANGUAGE: scala\nCODE:\n```\nakka.actor.deployment {\n  /parent/router5 {\n    router = scatter-gather-pool\n    nr-of-instances = 5\n    within = 10 seconds\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging of Event Stream Subscriptions - HOCON\nDESCRIPTION: This snippet allows Akka to log subscription changes (subscribe/unsubscribe) on the event stream at DEBUG level. Located in akka.actor.debug, set event-stream to 'on'. Useful for auditing actor event stream usage, especially in dynamic systems. Requires debug-level logging to be active.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_22\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  actor {\n    debug {\n      # enable DEBUG logging of subscription changes on the eventStream\n      event-stream = on\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using TestKitBase Trait in Scala for Akka Testing\nDESCRIPTION: Shows how to use the TestKitBase trait when inheriting from TestKit is not possible, requiring an implicit lazy val system declaration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_35\n\nLANGUAGE: scala\nCODE:\n```\nclass MySpec extends TestKitBase with WordSpecLike with Matchers {\n  implicit lazy val system = ActorSystem(\"MySpec\")\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Fork Join Pool Dispatcher in Configuration File\nDESCRIPTION: Provides an example of configuring a Fork Join Pool dispatcher. Configurations affect how dispatchers manage threads and execute tasks. Constraints include the need for proper configuration within your application.conf file for it to be effective.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [DispatcherDocSpec.scala](/akka-docs/src/test/scala/docs/dispatcher/DispatcherDocSpec.scala) { #my-dispatcher-config }\n```\n\n----------------------------------------\n\nTITLE: Implementing Control Messages in Scala\nDESCRIPTION: Scala implementation of control messages that will be prioritized by a control-aware mailbox. Messages extending the ControlMessage trait will be processed before regular messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.dispatch.ControlMessage\n// control messages\ncase object MyControlMessage extends ControlMessage\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster Metrics Extension in application.conf\nDESCRIPTION: Basic configuration required to enable the Cluster Metrics Extension in your Akka application. This stanza needs to be added to the application.conf file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka.extensions = [ \"akka.cluster.metrics.ClusterMetricsExtension\" ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository\nDESCRIPTION: Repository configuration block for accessing Akka dependencies from their library repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/futures.md#2025-04-22_snippet_0\n\nLANGUAGE: markup\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the Akka Extension for Database Connection Pool in Scala\nDESCRIPTION: Defines the DatabaseConnectionPool class that extends akka.actor.typed.Extension. This class manages the SharedResource instance and provides a method to access it.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nclass DatabaseConnectionPool(system: ActorSystem[_]) extends Extension {\n  private val sharedResource = new SharedResource\n  def queryDatabase(id: String): Unit = sharedResource.query(id)\n}\n```\n\n----------------------------------------\n\nTITLE: Scala Source.asSubscriber API Definition\nDESCRIPTION: API definition for the Source.asSubscriber operator in Scala that creates a Source which materializes into a Flow.Subscriber.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/asSubscriber.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ndef asSubscriber[T]: Source[T, Subscriber[T]]\n```\n\n----------------------------------------\n\nTITLE: RestartSource.withBackoff API Signature\nDESCRIPTION: Method signature showing how to create a RestartSource with backoff settings in both Scala and Java. Takes RestartSettings and a source factory function as parameters, returning a Source[T, NotUsed].\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartSource/withBackoff.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nwithBackoff[T](settings: akka.stream.RestartSettings)(sourceFactory: () => akka.stream.scaladsl.Source[T,_]): akka.stream.scaladsl.Source[T,akka.NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nwithBackoff(akka.stream.RestartSettings, akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Lifted Configuration Example\nDESCRIPTION: Example of the resulting configuration after lifting a subtree, showing the effective settings accessible within the actor system\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/configuration.md#2025-04-22_snippet_6\n\nLANGUAGE: ruby\nCODE:\n```\nakka.loglevel = \"WARNING\"\nmy.own.setting = 43\nmy.other.setting = \"hello\"\n// plus myapp1 and myapp2 subtrees\n```\n\n----------------------------------------\n\nTITLE: Converting Akka Stream to OutputStream in Scala\nDESCRIPTION: Creates a Source that materializes to an OutputStream and connects it to a ByteString-concatenating Sink. The example demonstrates writing bytes to the stream and handling completion.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/asOutputStream.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval outputStream: OutputStream = \n  StreamConverters\n    .asOutputStream()\n    .toMat(Sink.foreach(bytes => println(bytes.utf8String)))(Keep.left)\n    .run()\n\noutputStream.write(\"Hello World\\n\".getBytes(StandardCharsets.UTF_8))\noutputStream.close()\n```\n\n----------------------------------------\n\nTITLE: Testing Parent-Child Actor Relationships - Java\nDESCRIPTION: This snippet shows testing strategies for parent-child relationships in Akka using Java, corresponding to different ways to decouple actor creation for testing purposes. Java Akka TestKit is a required dependency. Various configurations of parent and child actors are demonstrated for better test isolation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #test-example }\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshot Selection Criteria in Akka\nDESCRIPTION: Specifies criteria for selecting which snapshot to use during recovery.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-snapshot.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nSnapshotSelectionCriteria.latest\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Cluster Events with CurrentClusterState in Scala\nDESCRIPTION: Example of subscribing to cluster events that receives CurrentClusterState as first message. This shows how to handle current cluster state and subsequent incremental updates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nCluster(system).subscribe(self, classOf[ClusterEvent.MemberEvent])\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Durable State Store Plugin in Java\nDESCRIPTION: Configuration example for activating a custom durable state store plugin in Java. This shows the minimal configuration needed to use a custom plugin implementation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\n#plugin-config-java\n```\n\n----------------------------------------\n\nTITLE: Git Push Command for Documentation\nDESCRIPTION: Shell command for pushing documentation changes to the remote repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/release-train-issue-template.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd ~/www\ngit push origin master\n```\n\n----------------------------------------\n\nTITLE: Creating RandomGroup Programmatically (Scala/Java)\nDESCRIPTION: Illustrates creating a RandomGroup router actor ('router2') programmatically, specifying the random routing logic and the list of routee actor paths directly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\n//#random-group-2\nimport akka.routing.RandomGroup\n\nval router2: ActorRef = context.actorOf(RandomGroup(paths).props(), \"router2\")\n//#random-group-2\n```\n\nLANGUAGE: java\nCODE:\n```\n//#random-group-2\nimport akka.routing.RandomGroup;\n\nfinal ActorRef router2 = getContext().actorOf(new RandomGroup(paths).props(), \"router2\");\n//#random-group-2\n```\n\n----------------------------------------\n\nTITLE: Applying a Sink to Substreams after groupBy in Java\nDESCRIPTION: Illustrates how to apply a processing stage (a `Sink.fold` in this case, summing elements) independently to each substream created by the `groupBy` operator. The result of the fold for each substream is then ignored.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n//#groupBy2\nsource\n    .groupBy(2, element -> element % 2 == 0)\n    .to(Sink.fold(0, (acc, i) -> acc + i))\n    // From Sink.fold(0)(_ + _), even substream will emit 30 and odd 25\n    // It means this will be applied independently for each substream\n    .run(system);\n//#groupBy2\n```\n\n----------------------------------------\n\nTITLE: Monitoring Stream Termination in Java\nDESCRIPTION: Shows the usage of watchTermination operator in Java with CompletionStage to track stream completion status. The operator allows monitoring stream termination while passing through elements unchanged.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/watchTermination.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#watchTermination\n```\n\n----------------------------------------\n\nTITLE: Composing Behaviors with PartialFunction in Scala\nDESCRIPTION: Combining multiple partial functions with orElse to create complete behaviors for different actor states. This approach allows for modular and reusable message handling logic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_19\n\nLANGUAGE: Scala\nCODE:\n```\ndef zeroBehavior(value: Int): Behavior[Command] =\n  Behaviors.receiveMessagePartial(getHandler\n    .orElse(setHandlerNonZero))\n\ndef nonZeroBehavior(value: Int): Behavior[Command] =\n  Behaviors.receiveMessagePartial(getHandler\n    .orElse(setHandlerNonZero)\n    .orElse(setHandlerZero))\n```\n\n----------------------------------------\n\nTITLE: Enabling Config Logging on System Start - HOCON\nDESCRIPTION: With this configuration, Akka will log the complete config at INFO level when the ActorSystem starts. Useful for debugging configuration and understanding what settings are loaded. Set log-config-on-start to 'on' in the HOCON configuration; requires appropriate log level to be active.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_20\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  # Log the complete configuration at INFO level when the actor system is started.\n  # This is useful when you are uncertain of what configuration is used.\n  log-config-on-start = on\n}\n```\n\n----------------------------------------\n\nTITLE: Defining askWithStatusAndContext Signature for ActorFlow - Akka Streams API Declaration (Scala)\nDESCRIPTION: Shows the Scala API signature for ActorFlow.askWithStatusAndContext, defining the parameters and expected flow types for integrating an actor ask pattern with a stream. Parameters include parallelism, the actor reference, a function to make the message, and an implicit timeout. Inputs are tuples of (input, context), and outputs are tuples of (actor reply, context). Prerequisites include importing Akka Streams and Akka Typed Actor libraries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithStatusAndContext.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nActorFlow.askWithStatusAndContext[I, Q, A, Ctx](parallelism: Int)(ref: akka.actor.typed.ActorRef[Q])(makeMessage: (I, akka.actor.typed.ActorRef[akka.pattern.StatusReply[A]]) => Q)(implicit timeout: akka.util.Timeout): akka.stream.scaladsl.Flow[(I, Ctx), (A, Ctx), akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Setting Akka Log Level to DEBUG - HOCON\nDESCRIPTION: This HOCON configuration example sets Akka's internal log level to 'DEBUG', enabling verbose diagnostic output from Akka internals and classic actors. It should be included in application.conf or another HOCON-compatible config file. Changing this property impacts which messages are passed to SLF4J and subsequently to your backend logger.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_17\n\nLANGUAGE: hocon\nCODE:\n```\nakka.loglevel = \"DEBUG\"\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Suppressed DeadLetters in Java\nDESCRIPTION: Java code showing how to subscribe to suppressed DeadLetters that are normally excluded from the default logging. Used for detailed system debugging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.SuppressedDeadLetter;\n// subscribe to all dead letters\nactorSystem.getEventStream().subscribe(actor, SuppressedDeadLetter.class);\n```\n\n----------------------------------------\n\nTITLE: Default Jackson Serialization Features\nDESCRIPTION: Configuration snippet showing the default Jackson serialization features enabled in Akka.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_27\n\nLANGUAGE: HOCON\nCODE:\n```\n@@snip [reference.conf](/akka-serialization-jackson/src/main/resources/reference.conf) { #features }\n```\n\n----------------------------------------\n\nTITLE: Default Jackson Modules Configuration\nDESCRIPTION: Configuration snippet showing the default Jackson modules enabled in Akka serialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_22\n\nLANGUAGE: HOCON\nCODE:\n```\n@@snip [reference.conf](/akka-serialization-jackson/src/main/resources/reference.conf) { #jackson-modules }\n```\n\n----------------------------------------\n\nTITLE: Decompressing Gzipped ByteString Streams (Akka Streams Scala)\nDESCRIPTION: Shows usage of the Compression helper object in Scala Akka Streams to decompress a stream of gzipped ByteStrings, e.g., when reading compressed files from disk or network. Supports Gzip and Deflate and can be chained with further stream processing steps. Requires Akka Streams Compression module.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeDecompress.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeDecompress.scala) { #decompress-gzip }\n```\n\n----------------------------------------\n\nTITLE: Actor with Value Class Arguments in Scala\nDESCRIPTION: Example of handling value class constructor arguments when creating Props, showing both manual Props construction and argument unpacking approaches.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n#actor-with-value-class-argument\n```\n\n----------------------------------------\n\nTITLE: Non-blocking Print Actor (Scala)\nDESCRIPTION: This snippet shows a non-blocking Print Actor implementation in Scala, used to demonstrate the impact of blocking operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [PrintActor.scala](/akka-docs/src/test/scala/docs/actor/typed/PrintActor.scala) { #print-actor }\n```\n\n----------------------------------------\n\nTITLE: Using orElse Operator in Akka Streams (Scala)\nDESCRIPTION: Example of using the orElse operator in Scala to provide a fallback source when the primary source completes without emitting elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/orElse.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval source1 = Source.empty\nval source2 = Source.single(\"yes\")\nval source3 = Source.from(List(\"no1\", \"no2\", \"no3\"))\n\nval s1 = source1.orElse(source2)\n// s1 will emit \"yes\"\n\nval s2 = source3.orElse(source2)\n// s2 will emit \"no1\", \"no2\", \"no3\"\n```\n\n----------------------------------------\n\nTITLE: Using Flow.futureFlow with prefixAndTail in Scala\nDESCRIPTION: This example demonstrates how to create a deferred stream based on the initial element by combining futureFlow with prefixAndTail in Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/futureFlow.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nSource(1 to 10)\n  .prefixAndTail(1)\n  .flatMapConcat { case (Seq(prefix), tail) =>\n    // create a future flow based on the first element\n    val futureFlow = createFlowBasedOnFirstElement(prefix)\n    // use the future flow for the rest of the elements\n    tail.via(Flow.futureFlow(futureFlow))\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradle Shadow Plugin with Kotlin DSL for Akka Fat Jar\nDESCRIPTION: Kotlin DSL configuration for Gradle that uses the Shadow plugin to merge reference.conf files when creating an Akka application fat jar.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/packaging.md#2025-04-22_snippet_4\n\nLANGUAGE: kotlin\nCODE:\n```\ntasks.withType<ShadowJar> {\n    val newTransformer = AppendingTransformer()\n    newTransformer.resource = \"reference.conf\"\n    transformers.add(newTransformer)\n}\n```\n\n----------------------------------------\n\nTITLE: Nesting Supervise with Other Behaviors in Java\nDESCRIPTION: The Java version of nesting supervise with other behavior wrappers, demonstrating the restart boundary. Only the behavior inside supervise is restarted when a failure occurs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nBehaviors.setup(context -> {\n  // this is not restarted\n  return Behaviors.supervise(\n    Behaviors.withTimers(timers -> {\n      // this is restarted, including the timer setup\n      // ...\n      return Behaviors.empty();\n    })\n  ).onFailure(SupervisorStrategy.restart());\n})\n```\n\n----------------------------------------\n\nTITLE: Custom Serialization Configuration for Persistence\nDESCRIPTION: Configuration example for setting up custom serializers for specific persistent event and snapshot types. Shows how to configure serialization bindings for Akka Persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_37\n\nLANGUAGE: Scala\nCODE:\n```\nakka.actor {\n  serializers {\n    my-payload = \"docs.persistence.MyPayloadSerializer\"\n    my-snapshot = \"docs.persistence.MySnapshotSerializer\"\n  }\n  serialization-bindings {\n    \"docs.persistence.MyPayload\" = my-payload\n    \"docs.persistence.MySnapshot\" = my-snapshot\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Tweet Authors using Akka Streams in Java\nDESCRIPTION: Similar to the Scala snippet, this Java code extracts a stream of authors from tweets using Akka Streams. It prepares the data for subsequent async operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntegrationDocTest.java](/akka-docs/src/test/java/jdocs/stream/IntegrationDocTest.java) { #tweet-authors }\n```\n\n----------------------------------------\n\nTITLE: Logger Operations Path\nDESCRIPTION: Reference to the deprecated LoggerOps path in Akka typed scaladsl that is being removed in favor of direct slf4j Logger APIs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guide-2.9.x-2.10.x.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nakka.actor.typed.scaladsl.LoggerOps\n```\n\n----------------------------------------\n\nTITLE: Defining Reply-Enabled Commands for Akka DurableStateBehavior (Scala)\nDESCRIPTION: Depicts the creation of command classes/traits that include an ActorRef replyTo field in Scala, enabling the actor behavior to send replies to the command sender. Requires Akka Typed ActorRef and appropriate serialization for messages. Input is the command with replyTo reference; output is a reply being sent. Constraints pertain to serialization of commands and the correct use of generics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_25\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [AccountExampleWithCommandHandlersInDurableState.scala](/akka-cluster-sharding-typed/src/test/scala/docs/akka/cluster/sharding/typed/AccountExampleWithCommandHandlersInDurableState.scala) { #reply-command }\n```\n\n----------------------------------------\n\nTITLE: Source.empty API Signatures\nDESCRIPTION: API signatures for Source.empty operator in both Scala and Java. Creates a source that completes immediately without emitting any elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/empty.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.empty[T]: Source[T, NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.empty()\nSource.empty(Class)\n```\n\n----------------------------------------\n\nTITLE: Polymorphic Type Handling in Scala\nDESCRIPTION: Example showing how to handle polymorphic types with Jackson annotations in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME)\n@JsonSubTypes(Array(\n  new JsonSubTypes.Type(value = classOf[Lion], name = \"lion\"),\n  new JsonSubTypes.Type(value = classOf[Elephant], name = \"elephant\")))\nsealed trait Animal extends JsonSerializable\ncase class Lion(name: String) extends Animal\ncase class Elephant(name: String, age: Int) extends Animal\n```\n\n----------------------------------------\n\nTITLE: Configuring Mailbox Stash Capacity in Akka Actors (HOCON)\nDESCRIPTION: Sets the maximum size of the stash for actors using the default mailbox configuration via `akka.actor.default-mailbox.stash-capacity`. This is particularly important for persistent actors, which internally stash messages during recovery, to prevent unbounded growth and potential `OutOfMemoryError`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_15\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.default-mailbox.stash-capacity=10000\n```\n\n----------------------------------------\n\nTITLE: Using Lambdas with ReceiveBuilder in Java\nDESCRIPTION: Using lambda expressions to delegate message handling to methods in Akka Typed. This pattern keeps the message matching code clean and concise.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nBehaviors.receive(Command.class)\n    .onMessage(GetValue.class, command -> {\n      getSender(command).tell(value);\n      return this;\n    })\n    .onMessage(Increment.class, command -> onIncrement(command))\n    .build()\n```\n\n----------------------------------------\n\nTITLE: NAT/Docker Network Configuration\nDESCRIPTION: HOCON configuration for Akka remoting when running behind NAT or in Docker containers, specifying both logical and bind addresses.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_19\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  remote {\n    artery {\n      canonical.hostname = my.domain.com      # external (logical) hostname\n      canonical.port = 8000                   # external (logical) port\n\n      bind.hostname = local.address # internal (bind) hostname\n      bind.port = 25520              # internal (bind) port\n    }\n }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining an Asynchronous Email Lookup Method (Java)\nDESCRIPTION: Defines an asynchronous method `lookupEmail` that simulates looking up an email address for a given handle, returning a `CompletionStage<Optional<String>>`. It includes a simulated delay.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\npublic static CompletionStage<Optional<String>> lookupEmail(String handle) {\n  return CompletableFuture.supplyAsync(() -> {\n    // simulate DB lookup\n    System.out.println(\"Looking up email for \" + handle);\n    try {\n      Thread.sleep(100);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    Optional<String> email;\n    switch (handle) {\n      case \"rolandkuhn\":\n        email = Optional.of(\"rk@example.com\");\n        break;\n      case \"patriknw\":\n        email = Optional.of(\"pn@example.com\");\n        break;\n      case \"konradmalawski\":\n        email = Optional.of(\"km@example.com\");\n        break;\n      default:\n        email = Optional.empty();\n    }\n    System.out.println(\"Email for \" + handle + \": \" + email.orElse(\"None\"));\n    return email;\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Resume Supervision Strategy Example\nDESCRIPTION: Demonstrates how to configure a resume supervision strategy to handle arithmetic exceptions by dropping failed elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_16\n\nLANGUAGE: Scala\nCODE:\n```\nval decider: Supervision.Decider = {\n  case _: ArithmeticException => Supervision.Resume\n  case _ => Supervision.Stop\n}\n\nSource(0 to 5).map(100 / _).withAttributes(ActorAttributes.supervisionStrategy(decider))\n```\n\n----------------------------------------\n\nTITLE: Initializing initialTimeout Operator in Akka Streams (Scala)\nDESCRIPTION: Applies an initial timeout to a Source or Flow in Akka Streams. The method takes a FiniteDuration parameter and returns a new representation of the stream with the timeout applied.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/initialTimeout.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ninitialTimeout(timeout: scala.concurrent.duration.FiniteDuration): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Source.failed API Definition in Scala and Java\nDESCRIPTION: API signature for creating a failing source stream in both Scala and Java. The operator takes a Throwable parameter and returns a Source that will fail immediately with that exception.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/failed.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nfailed[T](cause: Throwable): Source[T, NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nfailed(Throwable cause)\n```\n\n----------------------------------------\n\nTITLE: Describing Flow.lazyCompletionStageFlow Operator in Akka Streams\nDESCRIPTION: This snippet provides a detailed description of the Flow.lazyCompletionStageFlow operator. It explains how the operator defers creation and materialization of a Flow until the first element arrives, and describes its behavior in various scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/lazyCompletionStageFlow.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Flow.lazyCompletionStageFlow\n\nDefers creation and materialization of a `Flow` until there is a first element.\n\n@ref[Simple operators](../index.md#simple-operators)\n\n\n## Description\n\nWhen the first element comes from upstream the actual `CompletionStage<Flow>` is created and when that completes it is materialized\nand inserted in the stream.\nThe internal `Flow` will not be created if there are no elements on completion or failure of up or downstream.\n\nThe materialized value of the `Flow` will be the materialized value of the created internal flow if it is materialized\nand failed with a `akka.stream.NeverMaterializedException` if the stream fails or completes without the flow being materialized.\n\nSee also @ref:[lazyFlow](lazyFlow.md).\n\nCan be combined with `prefixAndTail(1)` to base the flow construction on the initial element triggering creation.\nSee @ref:[lazyFlow](lazyFlow.md) for sample.\n```\n\n----------------------------------------\n\nTITLE: Implementing Countdown with Source.unfold in Java\nDESCRIPTION: Creates a source that counts down from a given number to zero using Source.unfold in Java. Uses Optional to control stream termination when counter reaches zero.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfold.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.unfold(\n    10,\n    (Integer n) -> {\n      if (n == 0) {\n        return Optional.empty();\n      } else {\n        return Optional.of(new Pair<>(n - 1, n));\n      }\n    });\n```\n\n----------------------------------------\n\nTITLE: Basic Akka Actor Test Setup with TestKit in Scala\nDESCRIPTION: Illustrates a basic Scala test class structure for testing Akka actors using `akka-testkit`. It integrates with ScalaTest's `WordSpec`, mixes in `TestKit` for testing utilities (like `testActor`), `ImplicitSender` for convenience, and `BeforeAndAfterAll` for actor system lifecycle management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [PlainWordSpec.scala](/akka-docs/src/test/scala/docs/testkit/PlainWordSpec.scala) { #plain-spec }\n```\n\n----------------------------------------\n\nTITLE: Application-Specific Settings Configuration\nDESCRIPTION: Example configuration for application-specific settings that can be loaded through an Extension. This provides a clean way to access application configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\nmyapp {\n  greeting = \"Hello\"\n  threads = 4\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Cluster Seed Nodes in application.conf\nDESCRIPTION: Defines seed nodes for an Akka Cluster in the configuration file. Seed nodes are used as initial contact points for new nodes joining the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster.md#2025-04-22_snippet_6\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.seed-nodes = [\n  \"akka://ClusterSystem@host1:2552\",\n  \"akka://ClusterSystem@host2:2552\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Version and Snapshot Status for Documentation Release in Scala\nDESCRIPTION: This Scala code sets the version number and snapshot status for a documentation release. It's added to the version.sbt file when updating docs for an existing release.\nSOURCE: https://github.com/akka/akka/blob/main/RELEASING.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nThisBuild / version := \"2.6.4\"\nThisBuild / isSnapshot := false\n```\n\n----------------------------------------\n\nTITLE: Signature of Flow.lazyInitAsync in Scala and Java\nDESCRIPTION: The API documentation signature for the Flow.lazyInitAsync operator in both Scala and Java. It shows how to create a Flow that defers initialization until the first element arrives.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/lazyInitAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFlow.lazyInitAsync[I,O,M](flowFactory:()=&gt;scala.concurrent.Future[akka.stream.scaladsl.Flow[I,O,M]]):akka.stream.scaladsl.Flow[I,O,scala.concurrent.Future[Option[M]]]\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow.lazyInitAsync(akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Configuring Adaptive Load Balancing Router\nDESCRIPTION: Configuration for the cluster-metrics-adaptive-group router used in the factorial example. This config specifies metrics selector and cluster roles to use.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_7\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.deployment {\n  /factorialFrontend/factorialBackendRouter = {\n    # Router type provided by metrics extension.\n    router = cluster-metrics-adaptive-group\n    # Router parameter specific for metrics extension.\n    # metrics-selector = heap\n    # metrics-selector = load\n    # metrics-selector = cpu\n    metrics-selector = mix\n    #\n    routees.paths = [\"/user/factorialBackend\"]\n    cluster {\n      enabled = on\n      use-roles = [\"backend\"]\n      allow-local-routees = off\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Sink.ignore with Database Operations in Java\nDESCRIPTION: Example showing how to use Sink.ignore in Java when reading lines from a file, saving them to a database, and storing the database identifiers in another file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/ignore.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// #ignore\nSource<ByteString, CompletionStage<IOResult>> fileSource =\n    FileIO.fromPath(Paths.get(\"lines.txt\"));\n\nSource<String, NotUsed> strings =\n    fileSource\n        .via(Framing.delimiter(ByteString.fromString(\"\\n\"), 1024))\n        .map(bytes -> bytes.utf8String().trim());\n\nFlow<String, Integer, NotUsed> databaseFlow =\n    Flow.of(String.class).mapAsync(4, line -> saveToDatabase(line));\n\nSink<Integer, CompletionStage<IOResult>> idsFileSink =\n    Flow.of(Integer.class)\n        .map(id -> ByteString.fromString(id.toString() + \"\\n\"))\n        .toMat(FileIO.toPath(Paths.get(\"ids.txt\")), Keep.right());\n\n// run stream and ignore the outcome\nstrings.via(databaseFlow).to(Sink.ignore()).run(materializer);\n\n// or alternatively\nstrings.via(databaseFlow).toMat(idsFileSink, Keep.right()).run(materializer);\n// #ignore\n```\n\n----------------------------------------\n\nTITLE: Merging Sources in Java using Akka Streams\nDESCRIPTION: Example showing how to merge multiple sources in Akka Streams using Java. The merge operator combines elements from different sources randomly when elements are available.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/merge.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer> source1 = Source.single(1);\\nSource<Integer> source2 = Source.single(2);\\nSource<Integer> source3 = Source.single(3);\\n\\nSource<Integer> sources = Source.mergeSorted(\\n    Arrays.asList(source1, source2, source3),\\n    Integer::compareTo);\n```\n\n----------------------------------------\n\nTITLE: Creating RoundRobinPool from Configuration (Scala/Java)\nDESCRIPTION: Demonstrates creating a RoundRobinPool router actor named 'router1' within a parent actor. The router type and number of instances are loaded from the application configuration under the deployment path '/parent/router1'. It uses `FromConfig` to create the router based on HOCON settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\n//#round-robin-pool-1\nimport akka.routing.FromConfig\n\nval router1: ActorRef = context.actorOf(FromConfig.props(Props[Worker]()), \"router1\")\n//#round-robin-pool-1\n```\n\nLANGUAGE: java\nCODE:\n```\n//#round-robin-pool-1\nimport akka.routing.FromConfig;\n\nfinal ActorRef router1 = getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)),\n    \"router1\");\n//#round-robin-pool-1\n```\n\n----------------------------------------\n\nTITLE: Generating JavaDoc Style API Documentation - Shell\nDESCRIPTION: This snippet provides the sbt command to enable 'genjavadoc' and generate JavaDoc-style API documentation for Akka's Scala sources. The command passes a system property and runs 'Javaunidoc/doc', outputting the generated documentation to './target/javaunidoc/index.html'. Java JDK 11+ and the 'genjavadoc' sbt plugin are required. This is useful for checking formatting and link correctness in generated JavaDoc.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nsbt -Dakka.genjavadoc.enabled=true Javaunidoc/doc\n```\n\n----------------------------------------\n\nTITLE: Using Null as Initial State (Java)\nDESCRIPTION: Shows how to use null as the emptyState in Java for Akka Persistence, allowing for an optional initial state. This approach requires careful handling of null states in command and event handlers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-style.md#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AccountExampleWithNullState.java](/akka-cluster-sharding-typed/src/test/java/jdocs/akka/cluster/sharding/typed/AccountExampleWithNullState.java) { #account-entity }\n```\n\n----------------------------------------\n\nTITLE: Converting Java Stream to Akka Source in Scala\nDESCRIPTION: Example showing how to create an Akka Source from a Java Stream in Scala. The operator creates a new stream for each materialization and processes elements based on demand.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromJavaStream.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#from-javaStream\n```\n\n----------------------------------------\n\nTITLE: Combining Custom Strategy with Default Strategy in Scala\nDESCRIPTION: Shows how to combine a custom supervision strategy with Akka's default strategy for handling uncovered exception types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nval strategy =\n  OneForOneStrategy(maxNrOfRetries = 10, withinTimeRange = 1.minute) {\n    case _: ArithmeticException => Resume\n    case e => super.decider.applyOrElse(e, (_: Any) => Escalate)\n  }\n```\n\n----------------------------------------\n\nTITLE: SRV Record Resolution in Java\nDESCRIPTION: Example of resolving DNS SRV records using the Async DNS provider in Java. Shows how to specifically request SRV record types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-dns.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n#srv\n```\n\n----------------------------------------\n\nTITLE: Merging Substreams with Parallelism Limit in Java\nDESCRIPTION: Demonstrates merging substreams created by `groupBy` while limiting the number of concurrently active substreams using `mergeSubstreamsWithParallelism`. This can control resource usage but might lead to deadlocks if not handled carefully.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n//#groupBy4\nsource\n    .groupBy(3, element -> element % 3)\n    .mergeSubstreamsWithParallelism(2)\n    .runWith(Sink.fold(0, (acc, i) -> acc + i), system);\n//#groupBy4\n```\n\n----------------------------------------\n\nTITLE: Implementing Send Destination Actor in Scala\nDESCRIPTION: Example of implementing a destination actor that receives messages via DistributedPubSub.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\nclass Destination extends Actor {\n  def receive = {\n    case s: String =>\n      // do something ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Shade Plugin for Akka Fat Jar\nDESCRIPTION: Maven configuration that uses the Shade Plugin with Resource Transformers to merge all reference.conf files into a single fat jar for Akka applications.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/packaging.md#2025-04-22_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<plugin>\n <groupId>org.apache.maven.plugins</groupId>\n <artifactId>maven-shade-plugin</artifactId>\n <version>1.5</version>\n <executions>\n  <execution>\n   <id>shade-my-jar</id>\n   <phase>package</phase>\n   <goals>\n    <goal>shade</goal>\n   </goals>\n   <configuration>\n    <shadedArtifactAttached>true</shadedArtifactAttached>\n    <shadedClassifierName>allinone</shadedClassifierName>\n    <artifactSet>\n     <includes>\n      <include>*:*</include>\n     </includes>\n    </artifactSet>\n    <transformers>\n      <transformer\n       implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">\n       <resource>reference.conf</resource>\n      </transformer>\n      <transformer\n       implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">\n       <resource>version.conf</resource>\n      </transformer>\n      <transformer\n       implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n       <manifestEntries>\n        <Main-Class>myapp.Main</Main-Class>\n       </manifestEntries>\n      </transformer>\n    </transformers>\n   </configuration>\n  </execution>\n </executions>\n</plugin>\n```\n\n----------------------------------------\n\nTITLE: Java Flow RecoverWithRetries Signature\nDESCRIPTION: API signature for recoverWithRetries method on Flow class in Java. Takes retry attempts count, exception class, and supplier for alternative source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recoverWithRetries.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nrecoverWithRetries(int,java.lang.Class,java.util.function.Supplier)\n```\n\n----------------------------------------\n\nTITLE: Adding sbt-native-packager Plugin for Akka Packaging\nDESCRIPTION: Adds the sbt-native-packager plugin to the project/plugins.sbt file for creating distributions of Akka applications.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/packaging.md#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.1.5\")\n```\n\n----------------------------------------\n\nTITLE: Pipelined and Parallelized Operators with Java\nDESCRIPTION: This Java snippet exhibits the integration of pipelining and parallel processing in Akka Streams, utilizing the analogy of four chefs to optimize workflow. The application manages multiple processing stages concurrently, benefiting from Akka Streams' capabilities. Dependencies involve Akka Streams' Java API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-parallelism.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FlowParallelismDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowParallelismDocTest.java) { #pipelined-parallel }\n```\n\n----------------------------------------\n\nTITLE: Configuring Core-Based Thread Pool Dispatcher in Akka\nDESCRIPTION: Configuration for a dispatcher that scales its thread pool based on the number of CPU cores, optimized for CPU-bound tasks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\n#my-thread-pool-dispatcher-config\n```\n\n----------------------------------------\n\nTITLE: Applying Delay to Source in Akka Streams (Java)\nDESCRIPTION: Applies a delay to elements in a Source. The delay duration and overflow strategy are specified as parameters. This operator is part of the timer-driven operators in Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/delay.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.delay(java.time.Duration, akka.stream.DelayOverflowStrategy)\n```\n\n----------------------------------------\n\nTITLE: Configuring Retention Criteria with Event Deletion in Scala\nDESCRIPTION: Demonstrates how to set up retention criteria with event deletion enabled in Scala. This configuration allows automatic deletion of events after successful snapshot creation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-snapshot.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nRetentionCriteria.snapshotEvery(100, 2).withDeleteEventsOnSnapshot()\n```\n\n----------------------------------------\n\nTITLE: Defining Routee Paths Programmatically (Scala/Java)\nDESCRIPTION: Shows how to define a list of actor paths programmatically. This list will be used to specify the routees for a group router created in code, rather than through configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n//#paths\nval paths = List(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\")\n//#paths\n```\n\nLANGUAGE: java\nCODE:\n```\n//#paths\nList<String> paths = Arrays.asList(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\");\n//#paths\n```\n\n----------------------------------------\n\nTITLE: Selecting Remote Actors in Akka\nDESCRIPTION: Shows how to select remote actors in Akka by specifying the full actor path including the remote address.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_23\n\nLANGUAGE: Scala\nCODE:\n```\ncontext.actorSelection(\"akka://app@10.0.0.1:2552/user/serviceB\")\n```\n\nLANGUAGE: Java\nCODE:\n```\ngetContext().actorSelection(\"akka://app@10.0.0.1:2552/user/serviceB\");\n```\n\n----------------------------------------\n\nTITLE: Defining Tweet Data Model in Java\nDESCRIPTION: Defines classes for representing tweets and authors in Java, used in the reactive tweets example.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nclass Author {\n  private final String handle;\n\n  public Author(String handle) {\n    this.handle = handle;\n  }\n\n  public String handle() {\n    return handle;\n  }\n}\n\nclass Hashtag {\n  private final String name;\n\n  public Hashtag(String name) {\n    this.name = name;\n  }\n\n  public String name() {\n    return name;\n  }\n}\n\nclass Tweet {\n  private final Author author;\n  private final long timestamp;\n  private final String body;\n\n  public Tweet(Author author, long timestamp, String body) {\n    this.author = author;\n    this.timestamp = timestamp;\n    this.body = body;\n  }\n\n  public Author author() {\n    return author;\n  }\n\n  public long timestamp() {\n    return timestamp;\n  }\n\n  public String body() {\n    return body;\n  }\n\n  public Set<Hashtag> hashtags() {\n    return Arrays.stream(body.split(\"\\\\s\"))\n        .filter(a -> a.startsWith(\"#\"))\n        .map(a -> new Hashtag(a.replaceAll(\"[^#\\\\w]\", \"\")))\n        .collect(Collectors.toSet());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Snapshot Selection Criteria in Scala\nDESCRIPTION: Shows how to specify criteria for selecting snapshots during recovery\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_30\n\nLANGUAGE: scala\nCODE:\n```\noverride def recovery = Recovery(\n  fromSnapshot = SnapshotSelectionCriteria(\n    maxSequenceNr = 457L,\n    maxTimestamp = System.currentTimeMillis()))\n```\n\n----------------------------------------\n\nTITLE: Async DNS Resolution in Scala\nDESCRIPTION: Example of using the Async DNS provider directly through the actor API in Scala. Uses DnsProtocol.Resolve and DnsProtocol.Resolved messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-dns.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n#actor-api-async\n```\n\n----------------------------------------\n\nTITLE: Querying Default Internal Stash Overflow Strategy (Scala)\nDESCRIPTION: Demonstrates how to programmatically access the configured default internal stash overflow strategy within an actor's context. It uses the `Persistence` extension obtained via `Persistence(context.system)` and accesses its `defaultInternalStashOverflowStrategy` property.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\nPersistence(context.system).defaultInternalStashOverflowStrategy\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for Durable State Plugin in Scala\nDESCRIPTION: The necessary imports required to implement a custom durable state storage plugin in Scala. These imports provide access to the Akka persistence extension's plugin APIs and related utilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#plugin-imports\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Cluster Sharding Dependency\nDESCRIPTION: Dependency configuration for adding the Akka cluster sharding library to your project using SBT, Maven, or Gradle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_4\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-cluster-sharding-typed_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: JVM Shutdown Hook Registration in Scala\nDESCRIPTION: Shows how to register a JVM shutdown hook via Coordinated Shutdown in Scala. This ensures the hook runs before Akka's internal shutdown hooks and is executed in the specified phase.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nCoordinatedShutdown(system).addJvmShutdownHook {\n  println(\"custom JVM shutdown hook...\")\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Logging with log() Operation (Akka Streams Java)\nDESCRIPTION: Demonstrates log() usage within an Akka Streams Java pipeline for fine-grained logging of elements, stream completion, and errors. Integrates with standard Akka logging and allows setting log categories. Requires Akka Streams and its logging configuration to output structured logs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeLoggingElements.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeLoggingElements.java) { #log-custom }\n```\n\n----------------------------------------\n\nTITLE: Registering and Binding a Custom Serializer for Event Classes in Akka Persistence (Scala)\nDESCRIPTION: Demonstrates how to register the created custom serializer in Akka configuration, binding it to a specific event class such as 'docs.persistence.Person'. This is necessary step for Akka to use the custom implementation for serialization and deserialization of events. Typically placed in 'application.conf' or reference configuration, this snippet lists serializer class mappings and bindings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nakka {\n  actor {\n    serializers {\n      person = \"docs.persistence.PersonSerializer\"\n    }\n    serialization-bindings {\n      \"docs.persistence.Person\" = person\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Java Publisher Integration\nDESCRIPTION: Shows the API signature for creating a Source from a Flow.Publisher in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromPublisher.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<T, NotUsed> fromPublisher(Publisher<T> publisher)\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in Gradle\nDESCRIPTION: Specifies the Akka library repository URL in a Gradle build configuration file to enable fetching Akka dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_2\n\nLANGUAGE: gradle\nCODE:\n```\nrepositories {\n  maven {\n    url = \"https://repo.akka.io/maven\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using SpawnProtocol to Spawn Actors with Akka System in Java\nDESCRIPTION: Explains spawning additional actors using SpawnProtocol within an Akka ActorSystem in Java. Utilizes ask pattern for actor creation in an asynchronous manner.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/SpawnProtocolDocTest.java) { #imports2 #system-spawn }\n```\n\n----------------------------------------\n\nTITLE: Testing Timed Expectations with Akka TestKit - Java\nDESCRIPTION: This snippet demonstrates Akka TestKit's timing assertions in Java, specifically how `expectMsgEquals` uses the default timeout within probe-based tests. Dependencies include Akka TestKit for Java. Inputs and outputs are analogous to the Scala version and timing rules are local to each test probe.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [TestKitDocTest.java](/akka-docs/src/test/java/jdocs/testkit/TestKitDocTest.java) { #test-within-probe }\n```\n\n----------------------------------------\n\nTITLE: Application Settings Extension in Java\nDESCRIPTION: Java implementation of an Extension for loading application-specific settings from configuration. It provides type-safe access to configuration values.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.AbstractExtensionId;\nimport akka.actor.ActorSystem;\nimport akka.actor.ExtendedActorSystem;\nimport akka.actor.Extension;\nimport com.typesafe.config.Config;\n\npublic class SettingsImpl implements Extension {\n\n  public final String greeting;\n  public final int threads;\n\n  public SettingsImpl(Config config) {\n    greeting = config.getString(\"myapp.greeting\");\n    threads = config.getInt(\"myapp.threads\");\n  }\n}\n\npublic class Settings extends AbstractExtensionId<SettingsImpl> {\n\n  public static final Settings SettingsProvider = new Settings();\n\n  private Settings() {}\n\n  public SettingsImpl createExtension(ExtendedActorSystem system) {\n    return new SettingsImpl(system.settings().config());\n  }\n\n  public static SettingsImpl get(ActorSystem system) {\n    return SettingsProvider.get(system);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Read-Write Majority Consistency in Java\nDESCRIPTION: Java example showing how to use WriteMajority and ReadMajority for consistency in distributed data operations. It demonstrates defining these consistency levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nDuration t = Duration.ofSeconds(5);\\nWriteConsistency writeMajority = new WriteMajority(t);\\nReadConsistency readMajority = new ReadMajority(t);\n```\n\n----------------------------------------\n\nTITLE: Limiting Elements from Untrusted Source in Akka Streams (Java)\nDESCRIPTION: This Java example shows how to use the limit operator to protect against memory issues by restricting the number of elements processed from an untrusted source to 10,000. The stream will fail with StreamLimitReachedException if more elements arrive.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/limit.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(untrustedSource)\n  .limit(10000) // at most 10000 elements will be allowed through\n  .toMat(Sink.seq(), Keep.right()); // collect the elements into a list\n```\n\n----------------------------------------\n\nTITLE: Sample gRPC Client Session Output\nDESCRIPTION: Example interaction with the gRPC client showing user purchase statistics queries.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n7\nUser 7 has made 2 for a total of 3096p\nEnter user id or :q to quit\n3\nUser 3 has made 1 for a total of 12060p\nEnter user id or :q to quit\n4\nUser 4 has made 1 for a total of 7876p\nEnter user id or :q to quit\n5\nUser 5 has made 0 for a total of 0p\nEnter user id or :q to quit\n1\nUser 1 has made 0 for a total of 0p\nEnter user id or :q to quit\n```\n\n----------------------------------------\n\nTITLE: Source.lazySingle API Signature\nDESCRIPTION: API signatures for the Source.lazySingle operator in both Scala and Java. The operator takes a factory function that creates a single element and returns a Source that will emit that element when downstream demand arrives.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazySingle.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.lazySingle[T](create: () => T): Source[T, NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.lazySingle(akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Collecting Stream Values Using Sink.seq in Scala\nDESCRIPTION: Example demonstrating how to collect numbers from a stream into a sequence using Sink.seq operator in Scala. The result is available through a Future that completes when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/seq.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval input = List(1, 2, 3, 4, 5)\nval result = Source(input).runWith(Sink.seq)\nresult.map(println)\n```\n\n----------------------------------------\n\nTITLE: Handling Email Lookup Failures with mapAsync in Java\nDESCRIPTION: This Java snippet demonstrates using mapAsync with supervision to handle exceptions during email lookups in Akka Streams. It applies a resuming decider to manage errors without stopping the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [IntegrationDocTest.java](/akka-docs/src/test/java/jdocs/stream/IntegrationDocTest.java) { #email-addresses-mapAsync-supervision }\n```\n\n----------------------------------------\n\nTITLE: Offering to Receive Data: SinkRef Example in Java\nDESCRIPTION: Demonstrates how to set up a `SinkRef` in Java to receive data streams from a remote system, using flow-control mechanisms.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-refs.md#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nsnip [FlowStreamRefsDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowStreamRefsDocTest.java) { #offer-sink }\n```\n\nLANGUAGE: Java\nCODE:\n```\nsnip [FlowStreamRefsDocTest.java](/akka-docs/src/test/java/jdocs/stream/FlowStreamRefsDocTest.java) { #offer-sink-use }\n```\n\n----------------------------------------\n\nTITLE: Specific Configuration for Jackson Bindings\nDESCRIPTION: Configuration snippet demonstrating how to override default Jackson serialization settings for specific bindings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_24\n\nLANGUAGE: HOCON\nCODE:\n```\n@@snip [config](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/SerializationDocSpec.scala) { #specific-config }\n```\n\n----------------------------------------\n\nTITLE: Configuring Event Adapters in Akka Persistence\nDESCRIPTION: Configuration snippet for binding event adapters to event classes in Akka Persistence. This demonstrates how to register adapters that will be applied to specific event types during journaling and recovery.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_36\n\nLANGUAGE: Scala\nCODE:\n```\nakka.persistence.journal {\n  inmem {\n    event-adapters {\n      tagging = \"docs.persistence.MyTaggingEventAdapter\"\n      adapter2 = \"docs.persistence.MyOtherEventAdapter\"\n    }\n\n    event-adapter-bindings {\n      \"docs.persistence.MyEvent\" = tagging\n      \"docs.persistence.MyOtherEvent\" = [tagging, adapter2]\n      \"docs.persistence.MyThirdEvent\" = adapter2\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Library Repository\nDESCRIPTION: This snippet sets up the Akka library repository in sbt, Maven, or Gradle, which is essential to access the Akka Streams dependencies. It specifies the repository ID, name, and URL, ensuring projects can retrieve Akka's library components as needed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-flows-and-basics.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt,Maven,Gradle\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Programmatically Setting Target Location in Akka Persistence Plugin Proxy\nDESCRIPTION: Method call to programmatically set the target location for the persistence plugin proxy in Akka.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\nPersistencePluginProxy.setTargetLocation\n```\n\n----------------------------------------\n\nTITLE: Using Sink.cancelled to Immediately Cancel a Stream in Scala\nDESCRIPTION: In this example, a source generating numbers from 1 to 5 is connected to Sink.cancelled, which immediately cancels the stream and returns NotUsed as the materialized value, preventing any elements from being processed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/cancelled.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.NotUsed\nimport akka.actor.ActorSystem\nimport akka.stream.scaladsl.{Sink, Source}\n\nobject CancelledExample {\n  implicit val system: ActorSystem = ActorSystem()\n\n  def main(args: Array[String]): Unit = {\n    // #cancelled\n    // numbers will be produced until the stream is cancelled\n    val cancelled: NotUsed =\n      Source(1 to 5).runWith(Sink.cancelled)\n    // #cancelled\n\n    system.terminate()\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Eager Initialization for Akka Persistence Plugins (HOCON)\nDESCRIPTION: Demonstrates how to configure eager initialization for specific persistence plugins (LevelDB journal and local snapshot store) in Akka. This requires adding `akka.persistence.Persistence` to `akka.extensions` and listing the desired plugin IDs under `akka.persistence.journal.auto-start-journals` and `akka.persistence.snapshot-store.auto-start-snapshot-stores`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_1\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n\n  extensions = [akka.persistence.Persistence]\n\n  persistence {\n\n    journal {\n      plugin = \"akka.persistence.journal.leveldb\"\n      auto-start-journals = [\"akka.persistence.journal.leveldb\"]\n    }\n\n    snapshot-store {\n      plugin = \"akka.persistence.snapshot-store.local\"\n      auto-start-snapshot-stores = [\"akka.persistence.snapshot-store.local\"]\n    }\n\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Akka Local Message Send Method Reference\nDESCRIPTION: Reference to Akka's tell method used for local message sending, showing both Scala and Java signatures.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/message-delivery-reliability.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\ntell(msg:Any,sender:akka.actor.ActorRef):Unit\n```\n\nLANGUAGE: java\nCODE:\n```\ntell(java.lang.Object,akka.actor.ActorRef)\n```\n\n----------------------------------------\n\nTITLE: Sliding Window with Custom Step Size\nDESCRIPTION: Shows sliding window operation with window size 3 and step size 2, demonstrating how the step parameter affects window generation and handling of incomplete final windows.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/sliding.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval numbers = Source(1 to 4)\nnumbers.sliding(3, 2).runWith(Sink.foreach(println))\n// prints: Vector(1, 2, 3)\n// Vector(3, 4)\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3, 4))\n    .sliding(3, 2)\n    .runForeach(window -> System.out.println(window), system);\n// prints: [1, 2, 3]\n// [3, 4]\n```\n\n----------------------------------------\n\nTITLE: Safe Shutdown Example Using Custom Message\nDESCRIPTION: Shows the recommended approach for safely shutting down persistent actors using custom shutdown messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_25\n\nLANGUAGE: scala\nCODE:\n```\n#safe-shutdown-example-good\n```\n\nLANGUAGE: java\nCODE:\n```\n#safe-shutdown-example-good\n```\n\n----------------------------------------\n\nTITLE: Joining Seed Nodes Programmatically in Java\nDESCRIPTION: Java implementation for programmatically joining a cluster by specifying seed nodes. Useful when dynamically discovering other nodes at startup using external tools or APIs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.Address;\nimport akka.cluster.Cluster;\n\nimport java.util.Arrays;\nimport java.util.List;\n\nList<Address> seedNodes = Arrays.asList(\n  Address.parse(\"akka://ClusterSystem@host1:2552\"),\n  Address.parse(\"akka://ClusterSystem@host2:2552\"));\n\nCluster.get(system).joinSeedNodes(seedNodes);\n```\n\n----------------------------------------\n\nTITLE: Calculating Phi Value for Failure Detection in Akka Remote\nDESCRIPTION: This snippet shows the formula used to calculate the phi value in Akka's Phi Accrual Failure Detector. The phi value represents the suspicion level of failure for remote actors based on heartbeat inter-arrival times.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nphi = -log10(1 - F(timeSinceLastHeartbeat))\n```\n\n----------------------------------------\n\nTITLE: Defining a Codec Operator as a Bidirectional Flow in Akka Streams (Java)\nDESCRIPTION: This Java snippet shows how to construct a BidiFlow as a codec operator, wrapping two Flows for translating between objects and ByteString representations. It relies on Akka Streams Java DSL and expects Flows to handle encoding and decoding. The main parameters are the input Java String, ByteString, and the reverse data types, mapping outbound data to binary and inbound binary to decoded messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nBidiFlow<String, ByteString, ByteString, String, NotUsed> codec = BidiFlow.fromFunctions(\n  str -> ByteString.fromString(str),\n  bytes -> bytes.utf8String()\n);\n\n```\n\n----------------------------------------\n\nTITLE: Starting Send Destinations in Scala\nDESCRIPTION: Code showing how to start destination actors on cluster nodes using DistributedPubSub mediator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\nval mediator = DistributedPubSub(system).mediator\nmediator ! DistributedPubSubMediator.Put(self)\n```\n\n----------------------------------------\n\nTITLE: Implementing State Definitions in Scala\nDESCRIPTION: State definitions for the FSM actor including Idle and Active states with associated data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n#simple-state\n```\n\n----------------------------------------\n\nTITLE: Scheduling One-Off Runnable in Java\nDESCRIPTION: Schedule a Runnable to execute once after 50ms delay, sending current time to testActor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/scheduler.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nsystem.scheduler().scheduleOnce(\n  Duration.ofMillis(50),\n  new Runnable() {\n    @Override\n    public void run() {\n      testActor.tell(System.currentTimeMillis(), null);\n    }\n  },\n  system.dispatcher());\n```\n\n----------------------------------------\n\nTITLE: Lease Usage Implementation in Java\nDESCRIPTION: Shows lease acquisition and usage patterns in Java, with proper error handling and lease verification.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordination.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n#lease-usage\n```\n\n----------------------------------------\n\nTITLE: Providing a Child Maker Function in Tests - Java\nDESCRIPTION: Implements a child actor creation function for Java Akka test scenarios. Typically used in conjunction with parent actors accepting a child factory as constructor argument. Akka Actors (Java) and Java testing utilities are cell dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #child-maker-test }\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency in Maven\nDESCRIPTION: Declares the Akka Streams library dependency in a Maven pom.xml file, utilizing the Akka Bill of Materials (BOM) to manage dependency versions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<properties>\n  <scala.binary.version>$scala.binary.version$</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_${scala.binary.version}</artifactId>\n      <version>$akka.version$</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies>\n```\n\n----------------------------------------\n\nTITLE: Akka Streams zipLatestWith Flow Signature\nDESCRIPTION: API signature for the zipLatestWith operator on Flow class, showing both Scala and Java variants. Takes a Graph source and combine function to produce combined output.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipLatestWith.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nFlow.zipLatestWith[Out2,Out3](that:akka.stream.Graph[akka.stream.SourceShape[Out2],_])(combine:(Out,Out2)=>Out3):FlowOps.this.Repr[Out3]\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow.zipLatestWith(akka.stream.Graph,akka.japi.function.Function2)\n```\n\n----------------------------------------\n\nTITLE: Protobuf Schema for TwoPhaseSet\nDESCRIPTION: Protobuf representation of a TwoPhaseSet data type, showing how to define the structure with two GSet components.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_28\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage TwoPhaseSetMessages {\n  message TwoPhaseSet {\n    repeated string adds = 1;\n    repeated string removals = 2;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing LogSource in Scala\nDESCRIPTION: Shows how to create a custom LogSource implementation in Scala that mimics traditional Java logger behavior by using the class name as the log category.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.event.LogSource\nimport org.slf4j.LoggerFactory\n\nimplicit val logSource: LogSource[AnyRef] = new LogSource[AnyRef] {\n  def genString(o: AnyRef): String = o.getClass.getName\n  override def getClazz(o: AnyRef): Class[_] = o.getClass\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Cluster Metrics Events in Java\nDESCRIPTION: Java code for subscribing a metrics listener actor to cluster metrics events. This enables custom handling of metrics data for node lifecycle management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-metrics.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nClusterMetricsExtension.get(system).subscribe(metricsListenerActor);\n```\n\n----------------------------------------\n\nTITLE: Output of Recover with Retries Example (Scala)\nDESCRIPTION: Shows the expected output of the Scala `recoverWithRetries` example. The stream emits '0' through '4', encounters an exception, recovers using `planB`, emits 'five' and 'six', and then completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n0\n1\n2\n3\n4\nfive\nsix\n```\n\n----------------------------------------\n\nTITLE: Scala onErrorComplete API Signatures\nDESCRIPTION: API signatures for onErrorComplete operator in Scala for Source and Flow types. Supports both partial function and class tag based error handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/onErrorComplete.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.onErrorComplete(pf: PartialFunction[Throwable, Boolean]): FlowOps.this.Repr[T]\nSource.onErrorComplete[T <: Throwable]()(implicit tag: ClassTag[T]): FlowOps.this.Repr[T]\nFlow.onErrorComplete(pf: PartialFunction[Throwable, Boolean]): FlowOps.this.Repr[T]\nFlow.onErrorComplete[T <: Throwable]()(implicit tag: ClassTag[T]): FlowOps.this.Repr[T]\n```\n\n----------------------------------------\n\nTITLE: Creating JavaCollector Sink in Akka Streams\nDESCRIPTION: API signature for creating a sink that uses Java 8 Collector operations. The sink materializes into a Future in Scala or CompletionStage in Java containing the collector's transformation result.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/javaCollector.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nStreamConverters.javaCollector[T,R](collectorFactory:()=>java.util.stream.Collector[T,_,R]):akka.stream.scaladsl.Sink[T,scala.concurrent.Future[R]]\n```\n\nLANGUAGE: java\nCODE:\n```\nStreamConverters.javaCollector(akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Publishing Local Akka with sbt\nDESCRIPTION: This snippet demonstrates publishing Akka locally using sbt. It is a prerequisite for building test projects and running the native image.\nSOURCE: https://github.com/akka/akka/blob/main/native-image-tests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsbt publishLocal\n```\n\n----------------------------------------\n\nTITLE: Using logWithMarker in Java Akka Streams\nDESCRIPTION: Example showing how to use logWithMarker operator in Java to log stream elements with custom markers. The operator logs elements, completion, and error signals with configurable log levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/logWithMarker.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n#logWithMarker\n```\n\n----------------------------------------\n\nTITLE: Applying Top-Level Supervision to Functional Actor Behavior (Java)\nDESCRIPTION: Demonstrates applying `Behaviors.supervise` only once at the top level in Java when using the functional style of changing behaviors (like the `worker` example). The supervisor automatically re-wraps subsequent behaviors returned by the actor, ensuring consistent supervision.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n// #top-level\nBehavior<String> supervisedWorker =\n    Behaviors.supervise(worker(0)).onFailure(Exception.class, SupervisorStrategy.restart());\n// #top-level\n\n```\n\n----------------------------------------\n\nTITLE: Custom Lease Implementation Example in Scala\nDESCRIPTION: Template for implementing a custom lease system in Scala extending the Lease trait.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordination.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n#lease-example\n```\n\n----------------------------------------\n\nTITLE: Java Source RecoverWithRetries Signature\nDESCRIPTION: API signature for recoverWithRetries method on Source class in Java. Takes retry attempts count, exception class, and supplier for alternative source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recoverWithRetries.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nrecoverWithRetries(int,java.lang.Class,java.util.function.Supplier)\n```\n\n----------------------------------------\n\nTITLE: Applying Code Style\nDESCRIPTION: Command to automatically apply code style formatting to Akka project.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nsbt\napplyCodeStyle\n```\n\n----------------------------------------\n\nTITLE: Starting Embedded Kafka Server via sbt (Shell)\nDESCRIPTION: This command uses sbt (Simple Build Tool) to run the `kafka` sub-project. This specific task starts an embedded Kafka server locally, which is pre-configured for the sample. It also automatically creates the required `user-events` topic with 128 partitions.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"kafka / run\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Lookup Classification Bus in Java\nDESCRIPTION: A Java implementation of an EventBus using Lookup Classification, which maps events to subscribers based on event topics. It demonstrates efficient event classification and publishing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\npublic class LookupBusImpl extends EventBus<MsgEnvelope, String, ActorRef>\n    implements LookupClassification {\n  // is used for extracting the classifier from the incoming events\n  @Override\n  public String classify(MsgEnvelope event) {\n    return event.topic;\n  }\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for the event's classifier\n  @Override\n  public void publish(MsgEnvelope event, ActorRef subscriber) {\n    subscriber.tell(event.payload, ActorRef.noSender());\n  }\n\n  // must define the classifier to subscriber mapping\n  @Override\n  public int compareSubscribers(ActorRef a, ActorRef b) {\n    return a.compareTo(b);\n  }\n\n  // must define how the mapping is stored\n  @Override\n  public <A> void mapSize() {\n    return 128;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Final ItemAdded Migration in Java\nDESCRIPTION: Java code snippet showing the final migration class for ItemAdded without forward-compatibility code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ItemAddedMigration.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v2c/ItemAddedMigration.java) { #rename }\n```\n\n----------------------------------------\n\nTITLE: DNS Discovery Configuration\nDESCRIPTION: Configuration snippet to set DNS as the service discovery implementation. This configures Akka to use the DNS resolver for looking up services.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_6\n\nLANGUAGE: conf\nCODE:\n```\nakka {\n  discovery {\n    method = akka-dns\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Compute Node for Stats Cluster (Port 25252)\nDESCRIPTION: Starts a second compute node for the `sample.cluster.stats.App` example using SBT in a separate terminal. It specifies the role 'compute' and port 25252, acting as another seed node.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.stats.App compute 25252\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Events in Scala\nDESCRIPTION: Definition of messages and events handled by the Buncher actor in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n#simple-events\n```\n\n----------------------------------------\n\nTITLE: Looking Up a Dispatcher Interface Implementation in Scala\nDESCRIPTION: Demonstrates how to look up a Dispatcher implementing the ExecutionContext interface in Scala. Dependencies: akka-actor-typed library. Uses Future invocations with dispatchers. Outputs the dispatcher retrieved for executing actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [DispatcherDocSpec.scala](/akka-docs/src/test/scala/docs/actor/typed/DispatcherDocSpec.scala) { #lookup }\n```\n\n----------------------------------------\n\nTITLE: MapAsync Strict Order Processing in Java\nDESCRIPTION: Java implementation of strict order event processing using mapAsync with parallelism of 1. Demonstrates sequential processing while maintaining event order.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapAsync.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.range(1, 100)\n    .map(Event::new)\n    .mapAsync(1, event -> CompletableFuture.supplyAsync(() -> {\n        System.out.println(\"Processing event number \" + event + \"...\");\n        try {\n            Thread.sleep(500); // processing takes time\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n        System.out.println(\"Completed processing \" + event.getNumber());\n        return event;\n    }))\n    .map(event -> {\n        System.out.println(\"`mapAsync` emitted event number: \" + event.getNumber());\n        return event;\n    })\n```\n\n----------------------------------------\n\nTITLE: Defining a Worker Actor (Scala/Java)\nDESCRIPTION: Defines a simple worker actor class used as a routee in the router examples. This actor receives messages and performs some work, potentially logging the reception.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n//#create-workers\nimport akka.actor.Actor\nimport akka.actor.ActorRef\nimport akka.actor.Props\n\nclass Worker extends Actor {\n  def receive = {\n    case msg: String =>\n      println(s\"I received message '$msg' in ${self.path.name}\")\n      // ... process message\n  }\n}\n//#create-workers\n```\n\nLANGUAGE: java\nCODE:\n```\n//#create-workers\nstatic public class Worker extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n      .match(String.class, msg -> {\n          System.out.println(String.format(\"I received message '%s' in %s\", msg, getSelf().path().name()));\n          // ... process message\n      })\n      .build();\n  }\n}\n//#create-workers\n```\n\n----------------------------------------\n\nTITLE: Frontend Operation with Clustered Backends in Akka\nDESCRIPTION: Shows how the frontend node receives user jobs and delegates processing to registered backend workers. The frontend monitors backends using Akka's death-watch mechanism to manage node availability in the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [TransformationFrontend.java](/akka-docs/src/test/java/jdocs/cluster/TransformationFrontend.java) { #frontend }\n```\n\n----------------------------------------\n\nTITLE: One-line Tweet Counting in Akka Streams (Scala)\nDESCRIPTION: Demonstrates a concise way to count tweets using runWith in Scala, which is equivalent to the multi-line version.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_42\n\nLANGUAGE: Scala\nCODE:\n```\nval sum: Future[Int] = tweets.map(_ => 1).runWith(Sink.fold(0)(_ + _))\n```\n\n----------------------------------------\n\nTITLE: Working but Discouraged Infix Operator Pattern in Scala\nDESCRIPTION: A syntax that works but is subjectively less readable than the recommended ask pattern. This shows the needed parentheses to make the infix operator work with placeholders.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_23\n\nLANGUAGE: Scala\nCODE:\n```\n(actorRef ? (GetValue(_)))\n```\n\n----------------------------------------\n\nTITLE: Getting DurableStateStoreQuery in Scala\nDESCRIPTION: Example showing how to retrieve DurableStateStoreQuery from the DurableStateStoreRegistry extension in Scala. This is used for implementing tag-based searches in Akka Projections.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/persistence-query.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval durableStateStoreQuery = DurableStateStoreRegistry(system).durableStateStoreFor[DurableStateStoreQuery[String]](pluginId)\n```\n\n----------------------------------------\n\nTITLE: Configuring BroadcastPool Router\nDESCRIPTION: Configuration example for BroadcastPool router that broadcasts messages to all routees in the pool.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_28\n\nLANGUAGE: scala\nCODE:\n```\nakka.actor.deployment {\n  /parent/router3 {\n    router = broadcast-pool\n    nr-of-instances = 5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling GetSuccess for PNCounterMap in Akka Typed Actors (Java)\nDESCRIPTION: Handles successful Get responses from Replicator for the countersKey, invoking a custom method to process the received map and send the results to the requestor. This approach separates fetch logic from response construction in Akka Typed. Dependencies: Akka, Replicator. Input: GetSuccess for PNCounterMap. Output: triggers a reply to the original requestor.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-java/README.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n.match(GetSuccess.class, g -> g.key().equals(countersKey),\n   g -> receiveGetSuccess(open, (GetSuccess<PNCounterMap>) g))\n```\n\n----------------------------------------\n\nTITLE: Adding Tasks to Coordinated Shutdown in Java\nDESCRIPTION: Shows how to add a custom task to the 'before-service-unbind' phase of the coordinated shutdown process in Java. The task sends a message to an actor and returns a CompletionStage that completes when the actor replies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nCoordinatedShutdown.get(system).addTask(\n    CoordinatedShutdown.PhaseBeforeServiceUnbind(),\n    \"someTaskName\",\n    () -> {\n      CompletableFuture<Done> promise = new CompletableFuture<>();\n      actorRef.tell(new DoSomething(promise), ActorRef.noSender());\n      return promise;\n    });\n```\n\n----------------------------------------\n\nTITLE: Running AppOneMaster Compute Node on Dynamic Port using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.AppOneMaster` main class using Maven's exec plugin in a separate process. This command starts a 'compute' node for the Akka cluster sample application (Cluster Singleton example), allowing the system to assign a dynamic port (port 0).\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.AppOneMaster\" -Dexec.args=\"compute 0\"\n```\n\n----------------------------------------\n\nTITLE: Adding sbt-multi-jvm Plugin to Project\nDESCRIPTION: Instructions for adding the sbt-multi-jvm plugin to your project's plugins.sbt file. This is the first step in setting up multi-JVM testing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\naddSbtPlugin(\"com.github.sbt\" % \"sbt-multi-jvm\" % \"0.6.0\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Tell Protection Pattern with CircuitBreaker (Scala)\nDESCRIPTION: This example shows how to implement the Tell Protection pattern using the low-level CircuitBreaker API in Scala, manually tracking success and failure for operations that expect replies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\nclass DangerousActor extends Actor {\n  import scala.concurrent.duration._\n\n  val circuitBreaker = new CircuitBreaker(\n    context.system.scheduler,\n    maxFailures = 5,\n    callTimeout = 10.seconds,\n    resetTimeout = 1.minute)\n\n  def receive: Receive = {\n    case \"is-closed\" => sender() ! circuitBreaker.isClosed\n    case \"dangerous-call\" =>\n      if (circuitBreaker.isClosed || circuitBreaker.isHalfOpen) {\n        // The following call could be anything that\n        // could potentially fail and you want to protect against\n        val client = sender()\n        context.system.scheduler.scheduleOnce(5.seconds) {\n          client ! \"call-result\"\n        }(context.dispatcher)\n\n        // We know that the call succeeded so we tell the circuit breaker\n        circuitBreaker.succeed()\n      } else {\n        sender() ! \"circuit-breaker-open\"\n      }\n    case \"exception-call\" =>\n      if (circuitBreaker.isClosed || circuitBreaker.isHalfOpen) {\n        // The following call could be anything that\n        // could potentially fail and you want to protect against\n        val client = sender()\n        context.system.scheduler.scheduleOnce(5.seconds) {\n          client ! Status.Failure(new RuntimeException(\"Bad response\"))\n        }(context.dispatcher)\n\n        // We know that the call failed so we tell the circuit breaker\n        circuitBreaker.fail()\n      } else {\n        sender() ! \"circuit-breaker-open\"\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Lookup Classification Bus in Java\nDESCRIPTION: Test code that demonstrates using a Lookup Classification Bus in Java. It shows how to subscribe an actor to the bus, publish a message, and verify its reception.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nLookupBusImpl lookupBus = new LookupBusImpl();\nlookupBus.subscribe(getTestActor(), \"greetings\");\nlookupBus.publish(new MsgEnvelope(\"greetings\", \"hello\"));\nexpectMsgEquals(\"hello\");\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Durable State Store Plugin in Scala\nDESCRIPTION: Configuration example for activating a custom durable state store plugin in Scala. This shows the minimal configuration needed to use a custom plugin implementation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n#plugin-config-scala\n```\n\n----------------------------------------\n\nTITLE: Java Flow.keepAlive API Signature\nDESCRIPTION: Java API signature for the keepAlive operator on Flow, taking Duration and Creator parameters\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/keepAlive.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nkeepAlive(java.time.Duration,akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Basic State Definition in Akka FSM\nDESCRIPTION: Basic syntax for defining a state in Akka FSM using the when() method with optional state timeout.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nwhen(<name>[, stateTimeout = <timeout>])(stateFunction)\n```\n\n----------------------------------------\n\nTITLE: Simple Periodic Ticking in Java\nDESCRIPTION: Shows basic usage of Source.tick in Java to print a message every second with 0 initial delay.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/tick.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.tick(Duration.ZERO, Duration.ofSeconds(1), \"tick\")\n    .runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Interacting with HTTP Endpoint via Curl\nDESCRIPTION: This snippet exemplifies the use of curl to interact with the HTTP endpoint hosted by the Akka Cluster. It includes commands for posting weather data and querying the average temperature from the server.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-java/README.md#2025-04-22_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -XPOST http://localhost:12553/weather/62 -H \"Content-Type: application/json\" --data '{\"eventTime\": 1579106781, \"dataType\": \"temperature\", \"value\": 10.3}'\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncurl \"http://localhost:12553/weather/62?type=temperature&function=average\"\n```\n\n----------------------------------------\n\nTITLE: Selecting Remote Actor with Akka in Java\nDESCRIPTION: This Java code snippet provides an example of using Akka's actorSelection method to select an actor on a remote node. The Akka path format specifies the actor system, hostname, port, and actor path. This is useful for applications that need to send messages to remote actors in a networked environment. It requires the Akka library. The method takes an Akka actor path as a string and returns an ActorSelection object.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nActorSelection selection =\n  context.actorSelection(\"akka://actorSystemName@10.0.0.1:25520/user/actorName\");\n```\n\n----------------------------------------\n\nTITLE: Configuring BalancingPool with Thread Pool Executor (HOCON)\nDESCRIPTION: Configures the dispatcher for a BalancingPool to use a 'thread-pool-executor' instead of the default 'fork-join-executor'. This is useful when routees perform blocking operations, allowing explicit control over the number of threads.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_25\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-balancing-pool3\nakka.actor.deployment {\n  /parent/router1 {\n    router = balancing-pool\n    nr-of-instances = 5\n    pool-dispatcher {\n      executor = \"thread-pool-executor\"\n      thread-pool-executor {\n        fixed-pool-size = 5\n      }\n    }\n  }\n}\n//#config-balancing-pool3\n```\n\n----------------------------------------\n\nTITLE: Scala Sink.lazyFutureSink API Signature\nDESCRIPTION: API signature for the lazyFutureSink operator that creates a Sink[T, Future[M]] from a function returning Future[Sink[T,M]].\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/lazyFutureSink.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSink.lazyFutureSink[T,M](create: () => scala.concurrent.Future[akka.stream.scaladsl.Sink[T,M]]): akka.stream.scaladsl.Sink[T,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: Cycling Iterator Elements in Java\nDESCRIPTION: Example showing the usage of Source.cycle in Java to create an infinite stream from an iterator. The operator repeatedly iterates through the elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/cycle.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#cycle\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Recovery in Akka Streams - Java\nDESCRIPTION: Shows the Java implementation of error recovery in Akka Streams using the recover operator. Demonstrates handling stream failures with a fallback value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recover.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<String> source =\n    Source.<Integer>from(Arrays.asList(1, 2, 3, 4))\n        .map(\n            n -> {\n              if (n < 3) return String.valueOf(n);\n              else throw new RuntimeException(\"Boom!\");\n            });\n\nsource\n    .recover(\n        RuntimeException.class,\n        () -> \"fallback\")\n    .runForeach(System.out::println, system);\n```\n\n----------------------------------------\n\nTITLE: Default Behavior: Stopping Child Actors on Parent Restart (Scala)\nDESCRIPTION: Illustrates the default Akka Typed supervision behavior in Scala. When a parent actor supervised with `SupervisorStrategy.restart` restarts, any child actors it created within its `Behaviors.setup` block are stopped automatically.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/fault-tolerance.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\nval child: Behavior[String] = Behaviors.empty // dummy\n\n// #restart-stop-children\nBehaviors.supervise(\n  Behaviors.setup[String] { context =>\n    val child1 = context.spawn(child, \"child1\")\n    val child2 = context.spawn(child, \"child2\")\n\n    Behaviors.receiveMessage { msg =>\n      // processing messages\n      // ...\n      // restart occurs, children will be stopped\n      Behaviors.same\n    }\n  }\n)\n// #restart-stop-children\n```\n\n----------------------------------------\n\nTITLE: Java Source.asSubscriber API Definition\nDESCRIPTION: API definition for the Source.asSubscriber operator in Java that creates a Source which materializes into a Flow.Subscriber.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/asSubscriber.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<T, Subscriber<T>> asSubscriber()\n```\n\n----------------------------------------\n\nTITLE: Using CollectType Operator in Java\nDESCRIPTION: Alternative approach using collectType operator in Java, which can be more straightforward than using PFBuilder.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collect.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nmessageSource\n    .collectType(Ping.class)\n    .filter(ping -> ping.id() != 0)\n    .map(ping -> new Pong(ping.id()));\n```\n\n----------------------------------------\n\nTITLE: Declaring Akka Stream TestKit Dependency (sbt/Maven/Gradle) - Configuration\nDESCRIPTION: This build configuration snippet demonstrates how to declare a test-scoped dependency for the akka-stream-testkit module using sbt, Maven, or Gradle. The configuration references a BOM for version alignment, uses version symbols, and sets the correct group and artifact coordinates for the desired Scala binary version. Inputs include explicit Akka version and Scala binary version symbols. Output: a resolved build dependency that provides stream test utilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-testkit.md#2025-04-22_snippet_1\n\nLANGUAGE: configuration\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\\\"$akka.version$\\\"\n  group=\\\"com.typesafe.akka\\\"\n  artifact=\\\"akka-stream-testkit_$scala.binary.version$\\\"\n  version=AkkaVersion\n  scope=\\\"test\\\"\n}\n```\n\n----------------------------------------\n\nTITLE: Reusing Custom Akka Streams Components (Scala)\nDESCRIPTION: Shows how custom, nested `Source` (`nestedSource`) and `Sink` (`nestedSink`) components, created using modularization techniques like `named()`, can be seamlessly connected to form a `RunnableGraph` in Scala, just like using built-in Akka components.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\n//##reuse\n// Create a RunnableGraph from the composite modules\nval runnableGraph2 = nestedSource.toMat(nestedSink)(Keep.right)\n\n// Plug the sources and sinks together\nval runnableGraph3 = Source.single(0).toMat(Sink.head[Int])(Keep.right)\n//##reuse\n```\n\n----------------------------------------\n\nTITLE: Defining Commands for Blog Post Entity (Scala)\nDESCRIPTION: Defines the commands for a blog post entity using sealed traits and case classes in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\nsealed trait Command\n\nfinal case class AddPost(content: PostContent) extends Command\n\nfinal case class GetPost(replyTo: ActorRef[PostContent]) extends Command\n\nfinal case class ChangeBody(newBody: String) extends Command\n\ncase object Publish extends Command\n\nfinal case class AddComment(user: String, comment: String) extends Command\n```\n\n----------------------------------------\n\nTITLE: Implementing TwoPhaseSet Serializer with Embedded Types in Scala\nDESCRIPTION: Scala implementation that shows how to serialize a TwoPhaseSet by leveraging existing serializers for embedded GSet instances, using otherMessageToProto and otherMessageFromBinary methods.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_36\n\nLANGUAGE: scala\nCODE:\n```\nclass TwoPhaseSetSerializer2 extends SerializerWithStringManifest with SerializationSupport {\n  private val TwoPhaseSetManifest = \"A\"\n\n  def manifest(o: AnyRef): String = o match {\n    case _: TwoPhaseSet => TwoPhaseSetManifest\n    case _ => throw new IllegalArgumentException(s\"Can't serialize object of type ${o.getClass}\")\n  }\n\n  def toBinary(o: AnyRef): Array[Byte] = o match {\n    case m: TwoPhaseSet => twoPhaseSetToProto(m).toByteArray\n    case _ => throw new IllegalArgumentException(s\"Can't serialize object of type ${o.getClass}\")\n  }\n\n  def fromBinary(bytes: Array[Byte], manifest: String): AnyRef = manifest match {\n    case TwoPhaseSetManifest => twoPhaseSetFromBinary(bytes)\n    case _ => throw new IllegalArgumentException(\n      s\"Unknown manifest [${manifest}], known manifests [${TwoPhaseSetManifest}]\")\n  }\n\n  def twoPhaseSetToProto(twoPhaseSet: TwoPhaseSet): TwoPhaseSetMessages2.TwoPhaseSet = {\n    val b = TwoPhaseSetMessages2.TwoPhaseSet.newBuilder()\n    b.setAdds(otherMessageToProto(twoPhaseSet.adds))\n    b.setRemovals(otherMessageToProto(twoPhaseSet.removals))\n    b.build()\n  }\n\n  def twoPhaseSetFromBinary(bytes: Array[Byte]): TwoPhaseSet = {\n    val msg = TwoPhaseSetMessages2.TwoPhaseSet.parseFrom(bytes)\n    val adds = otherMessageFromBinary(msg.getAdds).asInstanceOf[GSet[String]]\n    val removals = otherMessageFromBinary(msg.getRemovals).asInstanceOf[GSet[String]]\n    TwoPhaseSet(adds, removals)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Large Message Payload Warnings - HOCON\nDESCRIPTION: This HOCON configuration monitors and logs Akka remote message types whose serialized payload size exceeds the specified threshold. The parameter log-frame-size-exceeding accepts a byte size (e.g., 10000b) and helps in managing network overhead by identifying oversized messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_25\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery {\n  log-frame-size-exceeding = 10000b\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Fixed Pool Size Dispatcher in Akka\nDESCRIPTION: Configuration for a dispatcher with a fixed thread pool size, suitable for actors performing blocking IO operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\n#fixed-pool-size-dispatcher-config\n```\n\n----------------------------------------\n\nTITLE: Referencing Static Quorum Configuration Defaults (Akka Docs Snippet)\nDESCRIPTION: This directive includes the default HOCON configuration for the `static-quorum` split-brain resolver strategy from Akka's `reference.conf`. It displays configurable parameters (like `quorum-size` and `role`) and their defaults for this strategy by referencing the `#static-quorum` section.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n@@snip [reference.conf](/akka-cluster/src/main/resources/reference.conf) { #static-quorum }\n```\n\n----------------------------------------\n\nTITLE: License Key Validation in Scala\nDESCRIPTION: Test code to verify Akka license key validity before expiration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/configuration.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nConfigDocSpec.scala\n```\n\n----------------------------------------\n\nTITLE: Looking up a Dispatcher in Scala\nDESCRIPTION: Demonstrates how to look up an ExecutionContext from an actor system in Scala, which can be used to run Future invocations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n// this is a shorthand for system.dispatchers.lookup(\"my-dispatcher\")\nimplicit val ec = system.dispatchers.lookup(\"my-dispatcher\")\nFuture {}\n// Uses the ExecutionContext automatically\n```\n\n----------------------------------------\n\nTITLE: Implementing ExtensionId in Scala\nDESCRIPTION: Creates an ExtensionId for the Counter extension. The ExtensionId provides methods to create and access the extension instance, ensuring it's loaded only once per ActorSystem.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nobject Counter extends ExtensionId[Counter] {\n  // The lookup method is required by ExtensionId\n  // and is used by Akka to create an instance of the Extension\n  override def createExtension(system: ExtendedActorSystem): Counter = new Counter\n\n  // Get the Counter extension from the system\n  // this Java version uses the Extension identity object directly to get the extension\n  def get(system: ActorSystem): Counter = super.get(system)\n}\n```\n\n----------------------------------------\n\nTITLE: Circuit Breaker Initialization in Java\nDESCRIPTION: Java implementation for initializing a circuit breaker with specified parameters including max failures, call timeout, and reset timeout.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nprivate final CircuitBreaker breaker = new CircuitBreaker(\n    getContext().getDispatcher(),\n    getContext().getSystem().getScheduler(),\n    5,\n    Duration.ofSeconds(10),\n    Duration.ofMinutes(1)\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository\nDESCRIPTION: Repository configuration block for accessing Akka libraries from the official Akka Maven repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-actors.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for NACK-Based Write Back-Pressure in Scala\nDESCRIPTION: Helper functions for the Scala EchoHandler implementation that support the back-pressure mechanism. These include specific states for handling acknowledgments and waiting for writing to resume.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\ndef waitingForAck(buffer: Vector[ByteString]): Receive = {\n  case WritingResumed =>\n    if (buffer.nonEmpty) {\n      connection ! Write(buffer.head, Ack)\n      context.become(buffering(buffer.tail))\n    } else {\n      context.become(writing)\n    }\n\n  case Received(data) =>\n    context.become(waitingForAck(buffer :+ data))\n\n  case _: ConnectionClosed =>\n    context.stop(self)\n}\n\ndef waitingForAckAndClosing(reason: ConnectionClosed, buffer: Vector[ByteString]): Receive = {\n  case WritingResumed =>\n    if (buffer.nonEmpty) {\n      connection ! Write(buffer.head, Ack)\n      context.become(closing(reason, buffer.tail))\n    } else {\n      context.stop(self)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Ask Pattern with Producer in Scala\nDESCRIPTION: Example of using the ask pattern in an image converter work manager to receive confirmation when messages are handled or stored.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/reliable-delivery.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\nWorkPullingDocExample.scala\n```\n\n----------------------------------------\n\nTITLE: Defining Stats Service Messages in Scala\nDESCRIPTION: Scala case classes defining the messages used in the statistics service example, including requests, worker results, and response messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nobject StatsMessages {\n  case class StatsJob(text: String) extends ConsistentHashable {\n    override def consistentHashKey: Any = text\n  }\n  case class StatsResult(meanWordLength: Double)\n  case class JobFailed(reason: String)\n\n  case class ProcessText(text: String)\n  case class ProcessWord(word: String)\n  case class WordCount(word: String, count: Int)\n\n  case object Start\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logback Appender with MDC Values\nDESCRIPTION: XML configuration for a Logback appender that includes MDC values in the log pattern. This allows custom fields like requestId and visitorId to be included in log outputs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_13\n\nLANGUAGE: xml\nCODE:\n```\n<appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n  <encoder>\n    <pattern>\n      %-5level %logger{36} [req: %X{requestId}, visitor: %X{visitorId}] - %msg%n\n    </pattern>\n  </encoder>\n</appender>\n```\n\n----------------------------------------\n\nTITLE: Running Simple Akka Cluster Example (Single JVM)\nDESCRIPTION: Executes the main class `sample.cluster.simple.App` using SBT. This command starts three actor systems (cluster members) within the same JVM process to demonstrate basic cluster formation and membership event logging.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.simple.App\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Logging in Java Actors\nDESCRIPTION: Shows how to create a LoggingAdapter and use logging methods in a Java actor. The example demonstrates importing necessary classes and implementing logging at different severity levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.AbstractActor;\nimport akka.actor.ActorRef;\nimport akka.actor.ActorSystem;\nimport akka.actor.Props;\nimport akka.event.Logging;\nimport akka.event.LoggingAdapter;\n```\n\nLANGUAGE: java\nCODE:\n```\npublic class MyActor extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  @Override\n  public void preStart() {\n    log.debug(\"Starting\");\n  }\n\n  @Override\n  public void preRestart(Throwable reason, Optional<Object> message) {\n    log.error(reason, \"Restarting due to [{}] when processing [{}]\",\n        reason.getMessage(),\n        message.isPresent() ? message.get() : \"(no message)\");\n    super.preRestart(reason, message);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"test\", s -> {\n          log.info(\"Received test\");\n        })\n        .matchAny(x -> {\n          log.warning(\"Received unknown message: {}\", x);\n        })\n        .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Joining a Single Node in Java\nDESCRIPTION: Java implementation for joining a specific single node in a cluster. This is less preferred than joinSeedNodes as it lacks built-in redundancy and retry mechanisms.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.Address;\nimport akka.cluster.Cluster;\n\nfinal Address selfAddress = Cluster.get(system).selfAddress();\nCluster.get(system).join(selfAddress);\n```\n\n----------------------------------------\n\nTITLE: RestartSource with KillSwitch in Scala\nDESCRIPTION: Demonstrates combining RestartSource with a KillSwitch to allow manual termination of the supervised stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\nval killSwitch = RestartSource.onFailuresWithBackoff(\n  minBackoff = 3.seconds,\n  maxBackoff = 30.seconds,\n  randomFactor = 0.2,\n  maxRestarts = -1\n) { () =>\n  Source.fromGraph(ServerSentEvent(\"http://example.com/events\"))\n}.viaMat(KillSwitches.single)(Keep.right)\n  .toMat(Sink.foreach(println))(Keep.left)\n  .run()\n```\n\n----------------------------------------\n\nTITLE: Implementing dropWithin in Akka Streams (Java)\nDESCRIPTION: Signature for the dropWithin operator in Java for Akka Streams Source and Flow. It takes a java.time.Duration parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/dropWithin.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.dropWithin(java.time.Duration)\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow.dropWithin(java.time.Duration)\n```\n\n----------------------------------------\n\nTITLE: Configuring ControlAwareMailbox\nDESCRIPTION: HOCON configuration for a control-aware mailbox. This mailbox allows actors to receive control messages with higher priority than regular messages, enabling immediate handling of critical control signals.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_14\n\nLANGUAGE: hocon\nCODE:\n```\ncontrol-aware-mailbox {\n  mailbox-type = \"akka.dispatch.UnboundedControlAwareMailbox\"\n  //Other mailbox configuration can go here\n}\n```\n\n----------------------------------------\n\nTITLE: Draining a Stream to a List Without Limits (Akka Streams Java, UNSAFE)\nDESCRIPTION: Shows how to collect an entire unbounded stream into a List using Sink.seq in Java Akka Streams, risking out-of-memory errors. Provided as an example of unsafe usage; always restrict the amount of data collected in practice.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeSeq.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeSeq.java) { #draining-to-list-unsafe }\n```\n\n----------------------------------------\n\nTITLE: Runtime Persistence Plugin Configuration in Scala\nDESCRIPTION: Example of a persistent actor that provides plugin configurations at runtime. This approach allows dynamic configuration of persistence plugins instead of using static configuration files.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_46\n\nLANGUAGE: Scala\nCODE:\n```\nclass RuntimePluginActor(id: String) extends PersistentActor {\n  override def persistenceId: String = id\n\n  // Config provided at runtime\n  override def journalPluginConfig: Config = ConfigFactory.parseString(\"\"\"\n    |dir = \"target/journal-runtime-conf\"\n    |native = off\n    \"\"\".stripMargin).withFallback(context.system.settings.config.getConfig(journalPluginId))\n\n  // Config provided at runtime\n  override def snapshotPluginConfig: Config = ConfigFactory.parseString(\"\"\"\n    |dir = \"target/snapshot-runtime-conf\"\n    \"\"\".stripMargin).withFallback(context.system.settings.config.getConfig(snapshotPluginId))\n\n  // ... rest of the actor implementation ...\n\n  override def receiveCommand: Receive = { case _ => }\n\n  override def receiveRecover: Receive = { case _ => }\n}\n```\n\n----------------------------------------\n\nTITLE: Limiting ByteString Stream Size in Java\nDESCRIPTION: Implements a GraphStage to fail the stream if more than a given maximum of bytes has been consumed. It updates a counter in onPush() and signals failure if the limit is exceeded.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_52\n\nLANGUAGE: Java\nCODE:\n```\npublic class ByteLimiter extends GraphStage<FlowShape<ByteString, ByteString>> {\n  private final long maximumBytes;\n\n  private final Inlet<ByteString> in = Inlet.create(\"ByteLimiter.in\");\n  private final Outlet<ByteString> out = Outlet.create(\"ByteLimiter.out\");\n\n  private final FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);\n\n  public ByteLimiter(long maximumBytes) {\n    this.maximumBytes = maximumBytes;\n  }\n\n  @Override\n  public FlowShape<ByteString, ByteString> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape()) {\n      private long count = 0;\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() throws Exception {\n                ByteString chunk = grab(in);\n                count += chunk.size();\n                if (count > maximumBytes) {\n                  failStage(new IllegalStateException(\"Too many bytes!\"));\n                } else {\n                  push(out, chunk);\n                }\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                pull(in);\n              }\n            });\n      }\n    };\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Unhandled Event Logic in Scala\nDESCRIPTION: Logic for handling unmatched events in the FSM.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n#unhandled-elided\n```\n\n----------------------------------------\n\nTITLE: Signature of groupedWeightedWithin for Source and Flow in Scala and Java\nDESCRIPTION: The API signatures for the groupedWeightedWithin operator in Akka Streams for both Source and Flow, in Scala and Java. It shows the method parameters and return types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/groupedWeightedWithin.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.groupedWeightedWithin(maxWeight:Long,d:scala.concurrent.duration.FiniteDuration)(costFn:Out=>Long):FlowOps.this.Repr[scala.collection.immutable.Seq[Out]]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.groupedWeightedWithin(long,akka.japi.function.Function,java.time.Duration)\n```\n\nLANGUAGE: scala\nCODE:\n```\nFlow.groupedWeightedWithin(maxWeight:Long,d:scala.concurrent.duration.FiniteDuration)(costFn:Out=>Long):FlowOps.this.Repr[scala.collection.immutable.Seq[Out]]\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow.groupedWeightedWithin(long,akka.japi.function.Function,java.time.Duration)\n```\n\n----------------------------------------\n\nTITLE: Adding ScalaTest Dependencies\nDESCRIPTION: Maven/SBT/Gradle dependency configuration for adding ScalaTest as the recommended testing framework with Akka TestKit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\ngroup=org.scalatest\nartifact=scalatest_$scala.binary.version$\nversion=$scalatest.version$\nscope=test\n```\n\n----------------------------------------\n\nTITLE: Configuring a Dedicated Blocking Dispatcher (HOCON)\nDESCRIPTION: Provides an example HOCON configuration for a dedicated dispatcher named `blocking-dispatcher`. It uses a thread-pool-executor suitable for blocking I/O tasks, preventing them from interfering with other application tasks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_16\n\nLANGUAGE: hocon\nCODE:\n```\nblocking-dispatcher {\n  type = Dispatcher\n  executor = \"thread-pool-executor\"\n  thread-pool-executor {\n    fixed-pool-size = 16\n  }\n  throughput = 1\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Exceptions in Akka Actors with TestActorRef in Scala\nDESCRIPTION: Demonstrates how to test exception handling in actors using TestActorRef's receive method. This approach allows exceptions to propagate rather than being swallowed by the actor system.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_41\n\nLANGUAGE: scala\nCODE:\n```\nval actor = TestActorRef[MyActor]\ntry {\n  actor.receive(\"unknown\")\n  fail(\"expected an exception to be thrown\")\n} catch {\n  case e: Exception => // expected\n}\n```\n\n----------------------------------------\n\nTITLE: Alternative Protobuf Schema for TwoPhaseSet with Embedded Types\nDESCRIPTION: Improved protobuf schema for TwoPhaseSet that leverages existing serializers by embedding serialized GSet instances as bytes fields, providing better composition of data types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_35\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage TwoPhaseSetMessages2 {\n  message TwoPhaseSet {\n    optional bytes adds = 1;\n    optional bytes removals = 2;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RoundRobinGroup Router via HOCON\nDESCRIPTION: Defines an Akka RoundRobinGroup router named 'myrouter' within the application configuration (HOCON). This router uses the round-robin strategy and routes messages to actors specified by the 'routees.paths' list. This configuration is typically placed in application.conf.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_4\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-round-robin-group\nakka.actor.deployment {\n  /parent/router1 {\n    router = round-robin-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n  }\n}\n//#config-round-robin-group\n```\n\n----------------------------------------\n\nTITLE: Scala Flow RecoverWithRetries Signature\nDESCRIPTION: API signature for recoverWithRetries method on Flow class. Takes number of attempts and partial function mapping throwables to alternative sources.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recoverWithRetries.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nrecoverWithRetries[T>:Out](attempts:Int,pf:PartialFunction[Throwable,akka.stream.Graph[akka.stream.SourceShape[T],akka.NotUsed]]):FlowOps.this.Repr[T]\n```\n\n----------------------------------------\n\nTITLE: Passing Explicit Parent Reference to Child Actor - Scala\nDESCRIPTION: This Scala snippet shows how to introduce a child actor to its parent by explicitly passing a reference, bypassing context.parent. This allows for more controlled and testable actor hierarchies in Akka. Requires Akka Actors and standard test setup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ParentChildSpec.scala](/akka-docs/src/test/scala/docs/testkit/ParentChildSpec.scala) { #test-dependentchild }\n```\n\n----------------------------------------\n\nTITLE: PersistenceIds Query Implementation in Scala\nDESCRIPTION: Shows how to query all persistenceIds from the journal to get IDs of all persistent actors.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query-leveldb.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval persistenceIds = readJournal.persistenceIds()\n```\n\n----------------------------------------\n\nTITLE: Creating Pull Mode Outbound TCP Connections in Java\nDESCRIPTION: A Java example showing how to establish an outbound TCP connection in pull mode. The pullMode parameter of the TcpMessage.connect() method is set to true to enable pull-based reading.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_21\n\nLANGUAGE: java\nCODE:\n```\nfinal ActorRef tcp = Tcp.get(system).manager();\n// connect to server using pull mode reading\ntcp.tell(\n    TcpMessage.connect(new InetSocketAddress(\"127.0.0.1\", 9999), null, Option.<Iterable<Inet.SocketOption>>empty(), Duration.Inf(), true),\n    getSelf());\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging of Unhandled Messages - HOCON\nDESCRIPTION: This snippet enables DEBUG logging for messages that are not handled by Akka actors, useful for identifying uncaptured or mistyped messages. Must be inserted under akka.actor.debug in your configuration, and general loglevel should be set to DEBUG for these entries to appear.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_21\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  actor {\n    debug {\n      # enable DEBUG logging of unhandled messages\n      unhandled = on\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Index Structure for Akka Documentation\nDESCRIPTION: Table of contents and index structure using special documentation markers (@@@) to organize links to major Akka framework components and modules\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@toc { depth=2 }\n\n@@@ index\n\n* [security/index](security/index.md)\n* [guide/index](typed/guide/index.md)\n* [general/index](general/index.md)\n* [index-actors](typed/index.md)\n* [index-cluster](typed/index-cluster.md)\n* [index-persistence](typed/index-persistence.md)\n* [index-persistence-durable-state](typed/index-persistence-durable-state.md)\n* [stream/index](stream/index.md)\n* [discovery](discovery/index.md)\n* [index-utilities](index-utilities.md)\n* [common/other-modules](common/other-modules.md)\n* [additional/deploy](additional/deploy.md)\n* [project/index](project/index.md)\n* [classic](index-classic.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Retrieving Distributed PNCounterMap State from Akka Replicator in Java\nDESCRIPTION: Issues a Get request to the distributed Replicator actor to retrieve the current state of a PNCounterMap CRDT, passing a context (the sender) for later identification. Responds asynchronously with the map contents. Dependencies: Akka Replicator. Inputs: countersKey and readAll consistency. Output: a replicated counter map reflecting aggregated votes.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-java/README.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nOptional<Object> ctx = Optional.of(sender());\nreplicator.tell(new Replicator.Get<PNCounterMap>(countersKey, readAll, ctx), self());\n```\n\n----------------------------------------\n\nTITLE: Logging Unreachable Node in Akka Cluster\nDESCRIPTION: This snippet demonstrates the log message generated when the Cluster failure detector observes another node as unreachable.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/failure-detector.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```\nMarking node(s) as UNREACHABLE\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing DeadLetter Listener in Scala\nDESCRIPTION: Scala code for an actor that listens to DeadLetter messages from the event stream. It logs received dead letters for debugging purposes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\nimport akka.actor.{ Actor, DeadLetter, Props }\n\nclass DeadLetterListener extends Actor {\n  def receive = {\n    case d: DeadLetter => println(d)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Event Adapter Implementation for Human-Readable Storage\nDESCRIPTION: Implementation of an EventAdapter that serializes events to/from JSON format for human-readable storage in the journal. Shows how to convert domain events to JSON representation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\nclass JsonEventAdapter extends EventAdapter {\n  override def toJournal(event: Any): Any = event match {\n    case event: UserDetailsChanged => JsonParser.parse(event)\n  }\n  override def fromJournal(event: Any, manifest: String): EventSeq = event match {\n    case json: JsValue => EventSeq.single(json.convertTo[UserDetailsChanged])\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Current Persistence IDs Snapshot in Java\nDESCRIPTION: Shows how to get a non-live snapshot of all current persistence IDs using the currentPersistenceIds query in Java. This stream completes when all current IDs have been delivered.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nreadJournal.getCurrentPersistenceIds().runForeach(id -> {\n  System.out.println(\"We have persistence id: \" + id);\n}, system);\n```\n\n----------------------------------------\n\nTITLE: Scala divertTo API Signature\nDESCRIPTION: API signature for divertTo operator in Scala for Source and Flow types. Takes a sink graph and predicate function to determine where elements should be diverted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/divertTo.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource#divertTo(that:akka.stream.Graph[akka.stream.SinkShape[Out],_],when:Out=>Boolean):FlowOps.this.Repr[Out]\n```\n\nLANGUAGE: scala\nCODE:\n```\nFlow#divertTo(that:akka.stream.Graph[akka.stream.SinkShape[Out],_],when:Out=>Boolean):FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Using intersperse to format a stream of integers in Java\nDESCRIPTION: This example takes a stream of integers, converts them to strings, and adds formatting elements: '[' at the start, ', ' between each element, and ']' at the end.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/intersperse.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3))\n    .map(Object::toString)\n    .intersperse(\"[\", \", \", \"]\")\n    .runForeach(System.out::print, system);\n// prints: [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Source.lazyFutureSource API Signature in Scala\nDESCRIPTION: API signature for creating a lazy Future Source that defers materialization until downstream demand exists. The operator takes a factory function that returns a Future[Source] and produces a Source that will emit elements from the created source once the future completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazyFutureSource.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.lazyFutureSource[T,M](create:()=>scala.concurrent.Future[akka.stream.scaladsl.Source[T,M]]):akka.stream.scaladsl.Source[T,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: Using grouped Operator in Scala\nDESCRIPTION: Demonstrates how to use the grouped operator to accumulate elements into Seq collections in Scala Akka Streams. The grouped operator collects elements until reaching the specified size before emitting downstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/grouped.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 7)\n  .grouped(3)\n  .map(group => group.map(_ * 2))\n  .runWith(Sink.foreach(println))\n// prints\n// Vector(2, 4, 6)\n// Vector(8, 10, 12)\n// Vector(14)\n```\n\n----------------------------------------\n\nTITLE: Tracking Actor State in DeviceGroupQuery\nDESCRIPTION: Implements state tracking for the DeviceGroupQuery actor, handling different response scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nprivate def waitingForReplies(\n    stillWaiting: Set[String],\n    repliesSoFar: Map[String, TemperatureReading]): Behavior[Command] =\n  Behaviors.receiveMessage {\n    case WrappedRespondTemperature(response) =>\n      val reading = response.value match {\n        case Some(value) => Temperature(value)\n        case None        => TemperatureNotAvailable\n      }\n      val deviceId = response.deviceId\n      respondWhenAllCollected(stillWaiting - deviceId, repliesSoFar + (deviceId -> reading))\n\n    case DeviceTerminated(deviceId) =>\n      respondWhenAllCollected(stillWaiting - deviceId, repliesSoFar + (deviceId -> DeviceNotAvailable))\n\n    case CollectionTimeout =>\n      val timedOutReplies =\n        stillWaiting.map(deviceId => deviceId -> DeviceTimedOut).toMap\n      requester ! RespondAllTemperatures(requestId, repliesSoFar ++ timedOutReplies)\n      Behaviors.stopped\n  }\n```\n\n----------------------------------------\n\nTITLE: Running Integer Range Source in Akka Streams\nDESCRIPTION: Demonstrates how to execute the range Source and process its output. The example shows printing each emitted integer value to the console.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/range.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nsource.runForeach(i -> System.out.println(i), system);\n```\n\n----------------------------------------\n\nTITLE: Creating a Java Logging Listener\nDESCRIPTION: This Java code snippet demonstrates how to get a logger from Akka's event stream, facilitating custom log management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nfinal LoggingAdapter log = Logging.getLogger(system.eventStream(), \"my.string\");\n```\n\n----------------------------------------\n\nTITLE: Importing Serialization Dependencies - Scala\nDESCRIPTION: Required imports for implementing ActorRef serialization in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-classic.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#imports\n```\n\n----------------------------------------\n\nTITLE: Using Akka Extensions in Scala\nDESCRIPTION: Demonstrates how to use the Counter extension directly in Scala code. The extension is accessed through its ExtensionId, and its methods can be called directly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/extending-akka.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n// typically you would use static imports here\nval counter1 = Counter(system)\nval counter2 = Counter(system)\nassert(counter1 == counter2)\ncounter1.increment()\nsystem.log.info(\"counter incremented: {}\", counter1.increment())\n```\n\n----------------------------------------\n\nTITLE: Renaming Fields in Serialization - Akka with Jackson\nDESCRIPTION: Details the approach to rename fields in a serialized class with corresponding migration code to handle the change. The migration code ensures the JSON transformation accounts for the field name change.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ItemAddedMigration.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v2c/ItemAddedMigration.scala) { #rename }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ItemAddedMigration.java](/akka-serialization-jackson/src/test/java/jdoc/akka/serialization/jackson/v2c/ItemAddedMigration.java) { #rename }\n```\n\n----------------------------------------\n\nTITLE: Using drop operator in Akka Streams with Java\nDESCRIPTION: Example of using the drop operator to skip elements at the beginning of a stream in Java. This demonstrates how to drop a specified number of elements before processing the remaining ones.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/drop.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// #drop\n// drop the first two elements\nSource.range(1, 10).drop(2);\n// #drop\n```\n\n----------------------------------------\n\nTITLE: Enabling Shared LevelDB Journal Plugin (HOCON - Deprecated)\nDESCRIPTION: Provides the configuration snippet needed to enable the shared LevelDB journal plugin (`akka.persistence.journal.leveldb-shared`) by setting the `akka.persistence.journal.plugin` property. This plugin requires a separate `SharedLeveldbStore` actor and is deprecated.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_9\n\nLANGUAGE: hocon\nCODE:\n```\n# Assuming the snippet sets the shared leveldb journal plugin\nakka.persistence.journal.plugin = \"akka.persistence.journal.leveldb-shared\"\n```\n\n----------------------------------------\n\nTITLE: Referencing Keep Oldest Configuration Defaults (Akka Docs Snippet)\nDESCRIPTION: This directive incorporates the default HOCON configuration settings for the `keep-oldest` split-brain resolver strategy from Akka's `reference.conf`. It shows specific parameters (like `down-if-alone` and `role`) and their default values by referencing the `#keep-oldest` section.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\n@@snip [reference.conf](/akka-cluster/src/main/resources/reference.conf) { #keep-oldest }\n```\n\n----------------------------------------\n\nTITLE: Creating Immutable User Class with Lombok\nDESCRIPTION: Demonstrates how to create an immutable class using Lombok's @Value annotation. This generates private final fields, getters, equals, hashCode, and toString methods automatically.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/immutable.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n@Value\npublic class LombokUser {\n\n  String name;\n\n  String email;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom PriorityMailbox\nDESCRIPTION: HOCON configuration for a custom priority mailbox. This associates a mailbox identifier with a specific implementation class, allowing actors to reference this mailbox by name.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_8\n\nLANGUAGE: hocon\nCODE:\n```\n\"prio-mailbox\" {\n  mailbox-type = \"docs.dispatcher.DispatcherDocSpec$MyPrioMailbox\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Sink.fold to accumulate elements in Akka Streams (Scala)\nDESCRIPTION: This snippet demonstrates using Sink.fold to read numbers from a source, perform calculations in the flow, and then fold over the results by adding the incoming elements together in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/fold.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n// #fold\nval source = Source(1 to 100)\n\n// Apply a fold to sum up all numbers\nval result: Future[Int] = source.runWith(Sink.fold(0)(_ + _))\n\nresult.map(println)\n// 5050\n// #fold\n```\n\n----------------------------------------\n\nTITLE: Updated ItemAdded Event Class with Renamed Field in Scala\nDESCRIPTION: Scala code snippet showing the final version of the ItemAdded event class after the field renaming.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ItemAdded.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v2c/ItemAdded.scala) { #rename }\n```\n\n----------------------------------------\n\nTITLE: Creating Deflate Compression Flow - Akka Streams - Scala\nDESCRIPTION: This snippet creates an Akka Streams Flow that applies deflate compression to a stream of akka.util.ByteString elements using the Scala API. It guarantees that each output ByteString can be individually decompressed due to SYNC_FLUSH after processing each chunk, which may reduce performance for small chunks. Requires Akka Streams and akka.util.ByteString as dependencies. The input is a stream of ByteStrings, and the output is a stream of individually decompressible, deflate-compressed ByteStrings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Compression/deflate.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nCompression.deflate: akka.stream.scaladsl.Flow[akka.util.ByteString, akka.util.ByteString, akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor Lifecycle in Java\nDESCRIPTION: Shows how to handle actor lifecycle events, specifically the PostStop signal, in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nclass StartStopActor1 {\n  static Behavior<String> create() {\n    return Behaviors.setup(\n        context -> {\n          System.out.println(\"first started\");\n          context.spawn(StartStopActor2.create(), \"second\");\n\n          return Behaviors.receiveMessage(\n              message -> {\n                System.out.println(\"first received: \" + message);\n                return Behaviors.stopped();\n              })\n              .receiveSignal(\n                  PostStop.class,\n                  (context1, signal) -> {\n                    System.out.println(\"first stopped\");\n                    return Behaviors.same();\n                  });\n        });\n  }\n}\n\nclass StartStopActor2 {\n  static Behavior<String> create() {\n    return Behaviors.setup(\n        context -> {\n          System.out.println(\"second started\");\n\n          return Behaviors.receiveSignal(\n              PostStop.class,\n              (context1, signal) -> {\n                System.out.println(\"second stopped\");\n                return Behaviors.same();\n              });\n        });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing DurableStateUpdateStore Interface in Scala\nDESCRIPTION: Implementation of the core DurableStateUpdateStore interface in Scala, which defines the essential methods required for a durable state storage plugin. This interface handles state read, write, and query operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n#plugin-api\n```\n\n----------------------------------------\n\nTITLE: Testing Child Actor Spawning\nDESCRIPTION: Tests for verifying that child actors are properly spawned with names and anonymously using BehaviorTestKit.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-sync.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n\"create a child\" in {\n  val testKit = BehaviorTestKit(parent())\n  testKit.run(SpawnChild(\"child\"))\n  testKit.expectEffect(Spawned(Child(), \"child\"))\n}\n```\n\nLANGUAGE: java\nCODE:\n```\n@Test\npublic void testSpawnChild() {\n  BehaviorTestKit<Command> testKit = BehaviorTestKit.create(Parent.create());\n  testKit.run(new SpawnChild(\"child\"));\n  testKit.expectEffect(Effects.spawned(Child.create(), \"child\"));\n}\n```\n\n----------------------------------------\n\nTITLE: Java takeWithin Signature\nDESCRIPTION: Method signature for the takeWithin operator in Java, which takes a Duration parameter and returns a transformed Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/takeWithin.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ntakeWithin(java.time.Duration)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stats Worker in Scala\nDESCRIPTION: Scala implementation of a worker actor that counts the characters in words and returns the result, used as a routee in the cluster router example.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nclass StatsWorker extends Actor {\n  import StatsMessages._\n  import akka.cluster.metrics.sample.StatsMessages._\n\n  def receive = {\n    case ProcessWord(word) =>\n      sender() ! WordCount(word, word.length)\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Using grouped Operator in Java\nDESCRIPTION: Shows how to use the grouped operator to accumulate elements into List collections in Java Akka Streams. The grouped operator collects elements until reaching the specified size before emitting downstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/grouped.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7))\n    .grouped(3)\n    .map(group -> group.stream().map(x -> x * 2).collect(Collectors.toList()))\n    .runWith(Sink.foreach(System.out::println), system)\n// prints\n// [2, 4, 6]\n// [8, 10, 12]\n// [14]\n```\n\n----------------------------------------\n\nTITLE: Defining Blocking Database API in Java\nDESCRIPTION: Example code showing a blocking database API interface and implementation in Java that will be used with mapWithResource.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapWithResource.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ninterface BlockingDatabase {\n  DatabaseConnection newConnection();\n}\nclass DatabaseConnection {\n  List<String> queryById(int id) {\n    // Let's simulate a blocking call with a simple implementation\n    return Collections.singletonList(\"Result for id: \" + id);\n  }\n  List<String> queryByName(String name) {\n    // Let's simulate a blocking call with a simple implementation\n    return Collections.singletonList(\"Result for name: \" + name);\n  }\n  void close() {} // close connection\n}\n```\n\n----------------------------------------\n\nTITLE: Problematic Blocking in Akka Actor (Scala)\nDESCRIPTION: This snippet shows a problematic implementation of blocking calls within an Akka actor, which can lead to thread starvation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [BlockingDispatcherSample.scala](/akka-docs/src/test/scala/docs/actor/typed/BlockingActor.scala) { #blocking-in-actor }\n```\n\n----------------------------------------\n\nTITLE: Consuming Tweet Stream with Sink in Scala\nDESCRIPTION: Shows how to attach a Sink to the stream to print author handles in Scala Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_28\n\nLANGUAGE: Scala\nCODE:\n```\nauthors.runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Java JsonSerializable Type Example\nDESCRIPTION: Example showing how to implement JsonSerializable marker trait for serialization in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/native-image.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass MyClass implements JsonSerializable\n```\n\n----------------------------------------\n\nTITLE: Starting Backend Node for Transformation Cluster (Random Port)\nDESCRIPTION: Starts an additional backend worker node for the `sample.cluster.transformation.App` example using SBT in a separate terminal. It specifies the role 'backend' and port 0, allowing Akka to use a random available port.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.transformation.App backend 0\"\n```\n\n----------------------------------------\n\nTITLE: Scheduling Recurring Message in Java\nDESCRIPTION: Schedule a 'Tick' message to be sent to tickActor repeatedly every 50ms with initial delay of 0ms in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/scheduler.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nfinal Cancellable cancellable = system.scheduler().scheduleWithFixedDelay(\n  Duration.ZERO,\n  Duration.ofMillis(50),\n  tickActor,\n  \"Tick\",\n  system.dispatcher(),\n  null);\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Classification Bus in Java\nDESCRIPTION: Test code for an Actor Classification Bus in Java. It demonstrates subscribing an actor to itself and receiving events where it is the sender.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nActorBusImpl actorBus = new ActorBusImpl();\nactorBus.subscribe(getTestActor(), getTestActor());\nactorBus.publish(new ActorEvent(getTestActor(), \"hello\"));\nexpectMsgEquals(\"hello\");\n```\n\n----------------------------------------\n\nTITLE: Dedicated Blocking Dispatcher Configuration\nDESCRIPTION: This snippet shows the configuration for a dedicated dispatcher used for blocking operations, which helps isolate the blocking behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/dispatchers.md#2025-04-22_snippet_12\n\nLANGUAGE: HOCON\nCODE:\n```\n@@snip [BlockingDispatcherSample.scala](/akka-docs/src/test/scala/docs/actor/typed/BlockingDispatcherSample.scala) { #my-blocking-dispatcher-config }\n```\n\n----------------------------------------\n\nTITLE: Using detach() in Akka Streams Flow (Java)\nDESCRIPTION: Applies the detach operator to a Flow in Akka Streams using Java. This separates upstream demand from downstream demand without affecting stream rates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/detach.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nFlow.detach()\n```\n\n----------------------------------------\n\nTITLE: Specifying a Custom Dispatcher for File IO in Scala\nDESCRIPTION: Explains how to run Akka Streams file IO operations on a custom dispatcher in Scala. By using `.withAttributes(ActorAttributes.dispatcher(\"custom-blocking-io-dispatcher\"))` on the `FileIO` source or sink, blocking file operations can be isolated from other actor processing, preventing thread pool starvation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-io.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n// Code for [StreamFileDocSpec.scala](/akka-docs/src/test/scala/docs/stream/io/StreamFileDocSpec.scala) { #custom-dispatcher-code } not available in input\n```\n\n----------------------------------------\n\nTITLE: Running Broadcast Outputs Asynchronously in Java\nDESCRIPTION: This Java snippet demonstrates adding asynchronous boundaries using `.async()` within the `GraphDSL` immediately after the `Broadcast` outputs. This configuration enables the separate downstream flows (sum and count reduction) to execute concurrently, potentially leveraging multiple CPU cores.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Broadcast.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nfinal Source<Integer, NotUsed> source = Source.range(1, 10);\n\nfinal Sink<BigInteger, CompletionStage<BigInteger>> sumSink =\n    Sink.<BigInteger, Integer>fold(BigInteger.ZERO, (acc, el) -> acc.add(BigInteger.valueOf(el)));\nfinal Sink<Integer, CompletionStage<Integer>> countSink =\n    Sink.<Integer, Integer>fold(0, (acc, el) -> acc + 1);\n\nfinal RunnableGraph<Pair<CompletionStage<BigInteger>, CompletionStage<Integer>>> graph =\n    RunnableGraph.<Pair<CompletionStage<BigInteger>, CompletionStage<Integer>>>fromGraph(\n        GraphDSL.create(\n            sumSink, // S1\n            countSink, // S2\n            Keep.both(),\n            (builder, s1, s2) -> {\n              final UniformFanOutShape<Integer, Integer> broadcast =\n                  builder.add(Broadcast.create(2));\n\n              builder.from(builder.add(source)).toFanOut(broadcast);\n\n              builder\n                  .from(broadcast.out(0))\n                  .async()\n                  .via(builder.add(Flow.of(Integer.class).map(BigInteger::valueOf)))\n                  .to(s1);\n              builder.from(broadcast.out(1)).async().to(s2);\n              return ClosedShape.getInstance();\n            }));\n\nfinal Pair<CompletionStage<BigInteger>, CompletionStage<Integer>> result = graph.run(system);\n\nresult\n    .first()\n    .thenAcceptAsync(s -> System.out.println(\"Sum: \" + s), system.dispatcher());\nresult\n    .second()\n    .thenAcceptAsync(c -> System.out.println(\"Count: \" + c), system.dispatcher());\n```\n\n----------------------------------------\n\nTITLE: Implementing DeadLetter Listener in Java\nDESCRIPTION: Java implementation of an actor that listens to DeadLetter messages from the event stream. It logs information about messages that couldn't be delivered.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nstatic class DeadLetterActor extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(DeadLetter.class, this::printDeadLetter).build();\n  }\n\n  private void printDeadLetter(DeadLetter letter) {\n    System.out.println(letter);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: License Key Validation in Java\nDESCRIPTION: Java test code to verify Akka license key validity before expiration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/configuration.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nConfigDocTest.java\n```\n\n----------------------------------------\n\nTITLE: Creating Index Links in Markdown\nDESCRIPTION: This snippet creates an index of links to various sections of the Akka Getting Started Guide using custom Markdown syntax. It includes links to the introduction, actor motivation, modules, and a five-part tutorial.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n@@@ index\n\n * [introduction](introduction.md)\n * [actors-motivation](actors-motivation.md)\n * [actors-intro](actors-intro.md)\n * [modules](modules.md)\n * [tutorial](tutorial.md)\n * [part1](tutorial_1.md)\n * [part2](tutorial_2.md)\n * [part3](tutorial_3.md)\n * [part4](tutorial_4.md)\n * [part5](tutorial_5.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Logging Delayed Heartbeat Sending in Akka Cluster\nDESCRIPTION: This snippet shows the log message generated when the scheduled sending of heartbeat is delayed in the Akka Cluster, which typically requires investigation of the root cause.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/failure-detector.md#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n```\nScheduled sending of heartbeat was delayed\n```\n```\n\n----------------------------------------\n\nTITLE: Running Multi-JVM Tests\nDESCRIPTION: Command to run the multi-JVM tests for the Akka Cluster example, testing the distributed functionality across multiple JVMs.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nsbt multi-jvm:test\n```\n\n----------------------------------------\n\nTITLE: Programmatic Remote System Configuration\nDESCRIPTION: Java code example showing how to programmatically configure remote system properties.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\n@@snip [RemoteDeploymentDocTest.java](/akka-docs/src/test/java/jdocs/remoting/RemoteDeploymentDocTest.java) { #programmatic-artery }\n```\n\n----------------------------------------\n\nTITLE: Creating Cluster Singleton Proxy\nDESCRIPTION: Setup of ClusterSingletonProxy for accessing the singleton actor from any cluster node.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-singleton.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\nsystem.actorOf(\n  ClusterSingletonProxy.props(\n    singletonManagerPath = \"/user/consumer\",\n    settings = ClusterSingletonProxySettings(system).withRole(\"worker\")),\n  name = \"consumerProxy\")\n```\n\nLANGUAGE: Java\nCODE:\n```\nsystem.actorOf(\n  ClusterSingletonProxy.props(\n    \"/user/consumer\",\n    ClusterSingletonProxySettings.create(system).withRole(\"worker\")),\n  \"consumerProxy\");\n```\n\n----------------------------------------\n\nTITLE: Applying reduce Operator in Java Akka Streams\nDESCRIPTION: Example of using the reduce operator in Java to combine elements in a stream. The operator takes the first element and then combines it with each subsequent element using the provided function.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/reduce.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.range(1, 100)\n    .reduce((a, b) -> a + b)\n    .runWith(Sink.head(), system)\n```\n\n----------------------------------------\n\nTITLE: Creating a Balanced but Deadlocked Cycle Using ZipWith in Akka Streams (Scala)\nDESCRIPTION: An attempt to create a balanced cycle by replacing Merge with ZipWith. While this should maintain the balance of elements, it creates a chicken-and-egg problem where no processing can start because each junction needs an element from the other.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_27\n\nLANGUAGE: Scala\nCODE:\n```\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n  val zip = b.add(ZipWith[Int, Int, Int]((left, right) => right))\n  val bcast = b.add(Broadcast[Int](2))\n\n  source ~> zip.in0\n  zip.out.map { s => println(s); s } ~> bcast ~> Sink.ignore\n                                            bcast ~> zip.in1\n\n  ClosedShape\n})\n```\n\n----------------------------------------\n\nTITLE: Source.fromFuture API Signature in Scala\nDESCRIPTION: The type signature for the deprecated Source.fromFuture operator that creates a Source from a Future[T]. Returns a Source that emits the Future's value when completed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromFuture.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.fromFuture[T](future: scala.concurrent.Future[T]): akka.stream.scaladsl.Source[T,akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Example Log Output for Actor Restart Supervision\nDESCRIPTION: This text block shows sample log output from running the Akka supervision experiment defined in the accompanying Scala/Java code. It clearly shows the lifecycle events: the supervised actor starts, receives the command to fail, is restarted by the supervising parent (indicated by 'supervised actor will be restarted'), and starts again. The ERROR log entry displays the exception stack trace captured by the supervisor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nsupervised actor started\nsupervised actor fails now\nsupervised actor will be restarted\nsupervised actor started\n[ERROR] [11/12/2018 12:03:27.171] [ActorHierarchyExperiments-akka.actor.default-dispatcher-2] [akka://ActorHierarchyExperiments/user/supervising-actor/supervised-actor] Supervisor akka.actor.typed.internal.RestartSupervisor@1c452254 saw failure: I failed!\njava.lang.Exception: I failed!\n\tat typed.tutorial_1.SupervisedActor.onMessage(ActorHierarchyExperiments.scala:113)\n\tat typed.tutorial_1.SupervisedActor.onMessage(ActorHierarchyExperiments.scala:106)\n\tat akka.actor.typed.scaladsl.AbstractBehavior.receive(AbstractBehavior.scala:59)\n\tat akka.actor.typed.Behavior$.interpret(Behavior.scala:395)\n\tat akka.actor.typed.Behavior$.interpretMessage(Behavior.scala:369)\n\tat akka.actor.typed.internal.InterceptorImpl$$anon$2.apply(InterceptorImpl.scala:49)\n\tat akka.actor.typed.internal.SimpleSupervisor.aroundReceive(Supervision.scala:85)\n\tat akka.actor.typed.internal.InterceptorImpl.receive(InterceptorImpl.scala:70)\n\tat akka.actor.typed.Behavior$.interpret(Behavior.scala:395)\n\tat akka.actor.typed.Behavior$.interpretMessage(Behavior.scala:369)\n```\n\n----------------------------------------\n\nTITLE: Selecting Remote Actor with Akka in Scala\nDESCRIPTION: This Scala code snippet demonstrates how to use Akka's actorSelection method to obtain an ActorSelection for an Actor on a remote node using the actor system's path format. It is crucial for sending messages to remote actors within an Akka system. No external dependencies are required apart from Akka itself. The input is an Akka actor path as a string, and the output is an ActorSelection object.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval selection =\n  context.actorSelection(\"akka://actorSystemName@10.0.0.1:25520/user/actorName\")\n```\n\n----------------------------------------\n\nTITLE: LMDB Directory Configuration in Java\nDESCRIPTION: Configuration for specifying the directory location for LMDB files when using durable storage in Java, with options for relative or absolute paths.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_41\n\nLANGUAGE: text\nCODE:\n```\n# Directory of LMDB file. There are two options:\n# 1. A relative or absolute path to a directory that ends with 'ddata'\n#    the full name of the directory will contain name of the ActorSystem\n#    and its remote port.\n# 2. Otherwise the path is used as is, as a relative or absolute path to\n#    a directory.\nakka.cluster.distributed-data.durable.lmdb.dir = \"ddata\"\n```\n\n----------------------------------------\n\nTITLE: Defining Message Classes for Stream Elements in Scala\nDESCRIPTION: Defines the class hierarchy for stream elements including Message base class, Ping subclass, and unrelated Pong class.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collect.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsealed trait Message\ncase class Ping(id: Int) extends Message\ncase class Pong(id: Int)\n```\n\n----------------------------------------\n\nTITLE: Manifest-less Serialization Configuration\nDESCRIPTION: Configuration snippet demonstrating how to set up manifest-less serialization for improved performance in certain scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_26\n\nLANGUAGE: HOCON\nCODE:\n```\n@@snip [config](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/SerializationDocSpec.scala) { #manifestless }\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Serialization in Akka\nDESCRIPTION: This configuration snippet toggles Java serialization on or off in Akka. Dependencies include a running Akka environment and appropriate access to the configuration files. The primary parameters are 'allow-java-serialization' to enable Java serialization and 'warn-about-java-serializer-usage' to suppress warnings. Inputs include boolean values for the configuration options. Constraints involve using this only for prototyping or backward compatibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization.md#2025-04-22_snippet_4\n\nLANGUAGE: Ruby\nCODE:\n```\nakka.actor.allow-java-serialization = on\n```\n\n----------------------------------------\n\nTITLE: Auto-Pilot Test Probe Configuration\nDESCRIPTION: Shows how to configure auto-pilot behavior for test probes to automatically handle message forwarding.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\n#autopilot\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Classification Bus in Scala\nDESCRIPTION: Test code for an Actor Classification Bus in Scala. It demonstrates registering actors as subscribers and matching events based on the actor references.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\nval actorBus = new ActorBusImpl\nactorBus.subscribe(testActor, testActor)\nactorBus.publish(ActorEvent(testActor, \"hello\"))\nexpectMsg(\"hello\")\n```\n\n----------------------------------------\n\nTITLE: Creating Resizable Router Pool in Java\nDESCRIPTION: Creates a router pool with a default resizer programmatically in Java. The resizer adjusts the pool size between 2 and 15 routees based on load.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_38\n\nLANGUAGE: java\nCODE:\n```\nDefaultResizer resizer = new DefaultResizer(2, 15);\nint nrOfInstances = 5;\nPool pool = new RoundRobinPool(nrOfInstances).withResizer(resizer);\nActorRef router1 = getContext().actorOf(pool.props(Props.create(Worker.class)), \"router1\");\n```\n\n----------------------------------------\n\nTITLE: Running the Fog Application\nDESCRIPTION: The command shows how to start the Fog application in the context of the KillrWeather project. It connects to previously started cluster nodes, utilizing predefined HTTP ports for communication.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-java/README.md#2025-04-22_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nmvn -pl killrweather exec:java -Dexec.args=\"2553\"\nsbt \"killrweather-fog/runMain sample.killrweather.fog.Fog 8081 8033 8056\"\n```\n\n----------------------------------------\n\nTITLE: Parameter Management in Akka with Java\nDESCRIPTION: This snippet illustrates the handling of parameters in Akka Typed using Java by grouping them into a class for better structure and scalability. It progresses from passing parameters separately to encapsulating them into a single object to maintain cleaner code and ease of extensions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nStyleGuideDocExamples.java { #fun-style-setup-params1 }\n```\n\nLANGUAGE: Java\nCODE:\n```\nStyleGuideDocExamples.java { #fun-style-setup-params2 }\n```\n\nLANGUAGE: Java\nCODE:\n```\nStyleGuideDocExamples.java { #fun-style-setup-params3 }\n```\n\n----------------------------------------\n\nTITLE: Creating Pull Mode Outbound TCP Connections in Scala\nDESCRIPTION: A Scala example showing how to establish an outbound TCP connection in pull mode. The pullMode parameter of the Connect command is set to true to enable pull-based reading.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\nIOManager(system) ! Connect(remoteAddress, pullMode = true)\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote RoundRobinGroup Router via HOCON\nDESCRIPTION: Configures an Akka RoundRobinGroup router that routes messages to actors running on remote systems. The 'routees.paths' include the Akka protocol, system name, host, and port. Requires the 'akka-remote' module.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_10\n\nLANGUAGE: hocon\nCODE:\n```\n//#config-remote-round-robin-group\nakka.actor.deployment {\n  /parent/remoteGroup {\n    router = round-robin-group\n    routees.paths = [\n      \"akka://app@10.0.0.2:2552/user/workers/w1\",\n      \"akka://app@10.0.0.3:2552/user/workers/w1\"]\n  }\n}\n//#config-remote-round-robin-group\n```\n\n----------------------------------------\n\nTITLE: API Reference for Sink.futureSink in Scala\nDESCRIPTION: The signature of the Sink.futureSink operator in Akka Streams. This operator takes a Future of a Sink and returns a Sink that will stream elements to the provided sink once the Future completes successfully.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/futureSink.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSink.futureSink[T,M](future:scala.concurrent.Future[akka.stream.scaladsl.Sink[T,M]]):akka.stream.scaladsl.Sink[T,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: Starting Additional Node for Simple Cluster (Random Port)\nDESCRIPTION: Starts an additional node for the `sample.cluster.simple.App` Akka cluster example using SBT in a separate terminal. Specifying port 0 allows Akka to pick a random available port. This node will join the cluster by contacting one of the configured seed nodes.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.simple.App 0\"\n```\n\n----------------------------------------\n\nTITLE: Avoiding Shared Mutable State in Scala Actors\nDESCRIPTION: A code example demonstrating incorrect usage of shared mutable state in Akka actors. This snippet shows how not to expose internal actor state to external threads, which could lead to concurrency issues.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/jmm.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#mutable-state\n```\n\n----------------------------------------\n\nTITLE: Frontend Operation with Clustered Backends in Akka\nDESCRIPTION: Shows how the frontend node receives user jobs and delegates processing to registered backend workers. The frontend monitors backends using Akka's death-watch mechanism to manage node availability in the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_19\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [TransformationFrontend.scala](/akka-docs/src/test/scala/docs/cluster/TransformationFrontend.scala) { #frontend }\n```\n\n----------------------------------------\n\nTITLE: Using Sink.takeLast in Java\nDESCRIPTION: Example showing the usage of Sink.takeLast operator in Java to collect the last n elements from a stream into a List. The materialized CompletionStage will complete when the stream completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/takeLast.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#takeLast-operator-example\n```\n\n----------------------------------------\n\nTITLE: Implementing DurableStateUpdateStore Interface in Java\nDESCRIPTION: Implementation of the core DurableStateUpdateStore interface in Java, which defines the essential methods required for a durable state storage plugin. This interface handles state read, write, and query operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n#state-store-plugin-api\n```\n\n----------------------------------------\n\nTITLE: RoundRobin Class for Stateful Routing in Java\nDESCRIPTION: This snippet defines a RoundRobin class used for stateful routing in the PartitionHub example for Java. It maintains a counter to implement round-robin behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-dynamic.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\nclass RoundRobin implements BiFunction<PartitionHub.ConsumerInfo, String, Long> {\n  private int i = -1;\n\n  @Override\n  public Long apply(PartitionHub.ConsumerInfo info, String elem) {\n    i++;\n    long idx = info.consumerIds().get(i % info.size());\n    return idx;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Message Classes for Type Collection in Scala\nDESCRIPTION: Example showing the class hierarchy setup with Message as parent class and Ping as a subclass, while Pong is unrelated.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collectType.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#collect-elements\n```\n\n----------------------------------------\n\nTITLE: Subscribing to All DeadLetters in Java\nDESCRIPTION: Java code showing how to subscribe to all DeadLetters including suppressed ones. This provides comprehensive monitoring of message delivery failures.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.AllDeadLetters;\n// subscribe to all dead letters\nactorSystem.getEventStream().subscribe(actor, AllDeadLetters.class);\n```\n\n----------------------------------------\n\nTITLE: Calculating a Digest of a ByteString Stream (Akka Streams Java, Part 1)\nDESCRIPTION: Shows part of a Java custom Akka Streams operator to process ByteStrings through a MessageDigest, updating the digest for each incoming element. Suited for when a hash or signature is required for the entire stream payload. Integrates with the Java Cryptography API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [RecipeDigest.java](/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeDigest.java) { #calculating-digest }\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistence Mode for Akka Cluster Sharding State Store\nDESCRIPTION: Configuration snippet for enabling the deprecated persistence mode as the state store for Akka Cluster Sharding. This mode uses Akka Persistence with a distributed journal to store shard coordinator state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_43\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.state-store-mode = persistence\n```\n\n----------------------------------------\n\nTITLE: Using the Database Connection Pool Extension in Scala\nDESCRIPTION: Demonstrates how to access and use the DatabaseConnectionPool extension within an actor or other components of the ActorSystem.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval connectionPool = DatabaseConnectionPool(system)\nconnectionPool.queryDatabase(\"id1\")\n```\n\n----------------------------------------\n\nTITLE: Grouping Elements by Weight in Akka Streams (Scala)\nDESCRIPTION: This snippet demonstrates how to use the groupedWeighted operator in Scala to accumulate elements based on their weight and perform operations on the grouped results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/groupedWeighted.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nSource(1 to 10)\n  .groupedWeighted(10)(identity)\n  .map(group => group.fold(0)(_ + _))\n  .runWith(Sink.foreach(println))\n\n// prints:\n// 10\n// 26\n// 19\n```\n\n----------------------------------------\n\nTITLE: Configuring External Aeron Media Driver in Akka\nDESCRIPTION: HOCON configuration for using an external Aeron media driver instead of the embedded one.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_16\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery.advanced.aeron {\n  embedded-media-driver = off\n  aeron-dir = /dev/shm/aeron\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Persistence ID in Scala\nDESCRIPTION: Example of overriding the persistenceId method in a Scala PersistentActor. The persistenceId must be unique to a given entity in the journal and remain consistent across actor incarnations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\noverride def persistenceId: String = \"abc\"\n```\n\n----------------------------------------\n\nTITLE: Handling Update Response Pattern 2 in Java\nDESCRIPTION: Alternative approach to handling Update responses in Java by checking for different response types and then verifying if the response key matches the expected key.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nreplicator.tell(\n    new Update<PNCounter>(\n        Counter1Key,\n        PNCounter.create(),\n        WriteLocal.instance(),\n        curr -> curr.increment(node, 1)),\n    getSelf());\n\n// handle response\nreceiveBuilder()\n    .matchEquals(\n        new UpdateSuccess<>(Counter1Key, Optional.empty()),\n        a -> {\n          // ok\n        })\n    .matchEquals(\n        new UpdateTimeout<>(Counter1Key, Optional.empty()),\n        a -> {\n          // will eventually be replicated, but read from other nodes might not\n          // see the update yet\n        })\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Subscriber Method Reference - Reactive Streams API\nDESCRIPTION: Reference to the onNext method signature from the Reactive Streams Subscriber interface, used to illustrate error handling patterns.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/stream/stream-design.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nonNext(T)\n```\n\n----------------------------------------\n\nTITLE: Advanced Logging Criteria in Java\nDESCRIPTION: Demonstrates advanced logging criteria for event verification using LoggingTestKit in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [LoggingDocExamples.java](/akka-actor-typed-tests/src/test/java/jdocs/akka/typed/LoggingDocExamples.java) { #test-logging-criteria }\n```\n\n----------------------------------------\n\nTITLE: Transforming Arithmetic Exceptions with mapError in Akka Streams (Java)\nDESCRIPTION: This snippet demonstrates how to use the mapError operator in Java to transform an ArithmeticException into a UnsupportedOperationException when the element 0 goes through a map operator that divides by the element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapError.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.ActorSystem;\nimport akka.japi.function.Function;\nimport akka.stream.javadsl.*;\nimport java.util.Arrays;\n\nActorSystem system = ActorSystem.create(\"mapError-system\");\n\nSource<Integer> source = Source.from(Arrays.asList(0, 1, 2));\n// #map-error\nSource<Integer> stream =\n    source\n        .map(n -> 1 / n) // will throw ArithmeticException for n = 0\n        .mapError(\n            ArithmeticException.class,\n            ex -> new UnsupportedOperationException(\"Operation not supported\"));\n// #map-error\n```\n\n----------------------------------------\n\nTITLE: Documentation Note on Message Delivery in Akka\nDESCRIPTION: A markdown note block highlighting Akka's guarantee about message ordering between actor pairs.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/terminology.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@@ note\n\nThe only guarantee that Akka provides about messages sent between a given pair of actors is that their order is\nalways preserved. see @ref:[Message Delivery Reliability](message-delivery-reliability.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Source.fromCompletionStage API Signature\nDESCRIPTION: API signature for the deprecated Source.fromCompletionStage operator that converts a CompletionStage to a Source. The operator emits a single value when the CompletionStage completes and there is demand. Handles null completion and failure cases appropriately.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromCompletionStage.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.fromCompletionStage[T](future: java.util.concurrent.CompletionStage[T]): akka.stream.scaladsl.Source[T,akka.NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.fromCompletionStage(java.util.concurrent.CompletionStage)\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Remoting Dependency\nDESCRIPTION: Dependency configuration for adding the Akka remoting library to your project using SBT, Maven, or Gradle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_2\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-remote_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logback Encoder with Markers and MDC Properties\nDESCRIPTION: XML configuration for a Logback encoder that includes markers and all MDC properties in the log pattern. This setup is useful for advanced logging scenarios with both markers and MDC data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_15\n\nLANGUAGE: xml\nCODE:\n```\n  <encoder>\n    <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n  </encoder>\n```\n\n----------------------------------------\n\nTITLE: Loading Akka Discovery Extension in Scala\nDESCRIPTION: Code snippet demonstrating how to load the Akka Discovery extension in a Scala application. This is the first step to use service discovery features.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/discovery/index.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval discovery = Discovery(system).discovery\n```\n\n----------------------------------------\n\nTITLE: Custom ADT Serialization in Scala\nDESCRIPTION: Example of implementing custom serialization for ADTs with case objects in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n@JsonSerialize(using = classOf[ConnectionStatusSerializer])\n@JsonDeserialize(using = classOf[ConnectionStatusDeserializer])\nsealed trait ConnectionStatus\ncase object Disconnected extends ConnectionStatus\ncase object Connected extends ConnectionStatus\ncase object Connecting extends ConnectionStatus\n```\n\n----------------------------------------\n\nTITLE: Implementing mapConcat in Java\nDESCRIPTION: Java implementation example showing how to use mapConcat to emit each integer twice in the stream. The operator transforms each input element into an List containing two copies of the element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapConcat.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3))\n    .mapConcat(n -> Arrays.asList(n, n))\n    .runWith(Sink.foreach(System.out::println), system);\n\n// prints:\n// 1\n// 1\n// 2\n// 2\n// 3\n// 3\n```\n\n----------------------------------------\n\nTITLE: Providing a Child Maker Function in Production - Scala\nDESCRIPTION: This version of the child maker function constructs the appropriate Props or child actor for use in production Scala Akka code, as opposed to testing. Inputs typically parameterize child creation; outputs are Akka Props for actor instantiation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_28\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ParentChildSpec.scala](/akka-docs/src/test/scala/docs/testkit/ParentChildSpec.scala) { #child-maker-prod }\n```\n\n----------------------------------------\n\nTITLE: Stateful MapConcat Implementation Pattern in Scala\nDESCRIPTION: Example showing how to combine statefulMap and mapConcat to implement statefulMapConcat-like behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMap.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\nval numbers = Source(List(1, 2, 3, 4, 5))\n  .statefulMap(() => 0)(\n    (state, element) => {\n      val newState = state + 1\n      (newState, List.fill(state)(element))\n    },\n    _ => None\n  )\n  .mapConcat(identity)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Storage Policy Test in Java\nDESCRIPTION: Example showing how to test a custom storage policy implementation in Java, demonstrating policy behavior testing\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n\"#policy-test\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Node Addresses in Scala Multi-Node Test\nDESCRIPTION: Shows how to access the addresses of specific nodes in a multi-node cluster test.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_29\n\nLANGUAGE: scala\nCODE:\n```\nval firstAddress = node(first).address\nval secondAddress = node(second).address\nval thirdAddress = node(third).address\n```\n\n----------------------------------------\n\nTITLE: Router Kill Message Handling\nDESCRIPTION: Shows how to send Kill messages to routers and routees for immediate termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_35\n\nLANGUAGE: scala\nCODE:\n```\nrouter ! Kill\n```\n\n----------------------------------------\n\nTITLE: Signature of RestartFlow.onFailuresWithBackoff in Scala and Java\nDESCRIPTION: The method signature for RestartFlow.onFailuresWithBackoff in both Scala and Java. It takes RestartSettings and a flow factory function as parameters, returning a Flow that wraps the original with restart capabilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartFlow/onFailuresWithBackoff.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nRestartFlow.onFailuresWithBackoff[In,Out](settings:akka.stream.RestartSettings)(flowFactory:()=&gt;akka.stream.scaladsl.Flow[In,Out,_]):akka.stream.scaladsl.Flow[In,Out,akka.NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nRestartFlow.onFailuresWithBackoff(akka.stream.RestartSettings,akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Defining an Asynchronous Email Lookup Function (Scala)\nDESCRIPTION: Defines an asynchronous function `lookupEmail` that simulates looking up an email address for a given handle, returning a `Future[Option[String]]`. It includes a simulated delay.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\ndef lookupEmail(handle: String): Future[Option[String]] = Future {\n  // simulate DB lookup\n  println(s\"Looking up email for $handle\")\n  Thread.sleep(100)\n  val email = handle match {\n    case \"rolandkuhn\" => Some(\"rk@example.com\")\n    case \"patriknw\"   => Some(\"pn@example.com\")\n    case \"konradmalawski\" => Some(\"km@example.com\")\n    case _            => None\n  }\n  println(s\"Email for $handle: ${email.getOrElse(\"None\")}\")\n  email\n}\n```\n\n----------------------------------------\n\nTITLE: ActorContext Access in Scala\nDESCRIPTION: Shows how to access ActorContext in a DurableStateBehavior using Behaviors.setup in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\n#actor-context\n```\n\n----------------------------------------\n\nTITLE: Configuring Full Actor Logging in Akka\nDESCRIPTION: Configuration snippet to enable comprehensive logging of actor activities including message receives, special messages, and lifecycle events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_34\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  loglevel = \"DEBUG\"\n  actor {\n    debug {\n      receive = on\n      autoreceive = on\n      lifecycle = on\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency in sbt\nDESCRIPTION: Declares the Akka Streams library dependency in an sbt build file, using the Akka Bill of Materials (BOM) for consistent version management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_3\n\nLANGUAGE: sbt\nCODE:\n```\nval AkkaVersion = \"$akka.version$\"\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-stream\" % AkkaVersion\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Array Arguments in Scala\nDESCRIPTION: Demonstrates how to log array elements as individual arguments in Scala by passing an array as the single substitution argument.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval args = Array(\"The\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\")\nlog.info(\"Logging in colors: {}\", args.mkString(\", \"))\n```\n\n----------------------------------------\n\nTITLE: Running StatsSample Compute Node on Dynamic Port using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.App` main class using Maven's exec plugin in a separate process. This command starts a 'compute' node for the Akka cluster sample application, allowing the system to assign a dynamic port (port 0). Used for the multi-process group router example.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.App\" -Dexec.args=\"compute 0\"\n```\n\n----------------------------------------\n\nTITLE: Scala API Signature for Source.fromFutureSource\nDESCRIPTION: Method signature for the deprecated fromFutureSource operator that takes a Future[Graph] and returns a Source. The operator streams elements from the future source once it completes successfully, or fails if the future fails.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromFutureSource.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.fromFutureSource[T,M](future: scala.concurrent.Future[akka.stream.Graph[akka.stream.SourceShape[T],M]]): akka.stream.scaladsl.Source[T,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: Using Source.futureSource with HTTP Streaming in Scala\nDESCRIPTION: Example demonstrating how to use Source.futureSource to handle a remote service streaming user data over HTTP/2 or WebSocket. The source becomes available after establishing the connection.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/futureSource.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ndef establishConnection(): Future[Source[User, NotUsed]] = ???\n\nval source: Source[User, Future[NotUsed]] =\n  Source.futureSource(establishConnection())\n```\n\n----------------------------------------\n\nTITLE: Scala Source Subscriber Code Reference\nDESCRIPTION: Scala code reference showing how to obtain a Subscriber using Source.asSubscriber\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/stream/stream-design.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nSource.asSubscriber[T]:akka.stream.scaladsl.Source[T,org.reactivestreams.Subscriber[T]]\n```\n\n----------------------------------------\n\nTITLE: Initializing Sharding with Custom Stop Message (Java)\nDESCRIPTION: This Java snippet demonstrates initializing Akka Cluster Sharding for an entity type (`TypeKey`). It provides an `Entity` definition that includes the behavior factory (`createBehavior`) and sets `GoodByeCounter` as the custom `stopMessage` to be sent to the entity upon passivation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_26\n\nLANGUAGE: java\nCODE:\n```\nEntityTypeKey<CounterCommand> TypeKey = EntityTypeKey.create(CounterCommand.class, \"Counter\");\n\nActorRef<ShardingEnvelope<CounterCommand>> shardRegion =\n    ClusterSharding.get(system)\n        .init(\n            Entity.of(\n                    TypeKey,\n                    entityContext -> counter(entityContext.getShard(), entityContext.getEntityId()))\n                // specify the GoodByeCounter message adapts the ShardCommand to the CounterCommand\n                // but typically you would have a dedicated stop message\n                .withStopMessageAdapter(shardCommand -> new GoodByeCounter(shardCommand.entityId())));\n```\n\n----------------------------------------\n\nTITLE: Manual Construction of CircuitBreaker (Java)\nDESCRIPTION: This snippet demonstrates how to manually create a CircuitBreaker instance in Java with custom parameters instead of looking it up by name from configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nCircuitBreaker circuitBreaker =\n    new CircuitBreaker(\n        system.getScheduler(),\n        5,\n        Duration.ofSeconds(10),\n        Duration.ofMinutes(1));\n```\n\n----------------------------------------\n\nTITLE: Using Sink.lastOption in Scala\nDESCRIPTION: Example demonstrating how to use the lastOption sink operator in Scala to capture the last element of a stream within a Future[Option[T]].\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/lastOption.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nfinal class LastSinkSpec extends AnyWordSpec with ScalaFutures with Matchers {\n\n  implicit val system: ActorSystem = ActorSystem()\n  implicit val materializer: Materializer = SystemMaterializer(system).materializer\n\n  \"A Last sink\" should {\n\n    \"yield the last element from a finite stream\" in {\n      //#lastOption-operator-example\n      val source = Source(1 to 3)\n      val result: Future[Option[Int]] = source.runWith(Sink.lastOption)\n      //#lastOption-operator-example\n\n      result.futureValue should be(Some(3))\n    }\n\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating RandomPool from Configuration (Scala/Java)\nDESCRIPTION: Shows how to create a RandomPool router actor ('router1') using settings defined in the HOCON configuration file under the path '/parent/router1'.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\n//#random-pool-1\nimport akka.routing.FromConfig\n\nval router1: ActorRef = context.actorOf(FromConfig.props(Props[Worker]()), \"router1\")\n//#random-pool-1\n```\n\nLANGUAGE: java\nCODE:\n```\n//#random-pool-1\nimport akka.routing.FromConfig;\n\nfinal ActorRef router1 = getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)),\n    \"router1\");\n//#random-pool-1\n```\n\n----------------------------------------\n\nTITLE: Scala Flow.delayWith Signature\nDESCRIPTION: API signature for delayWith operator in Akka Streams Flow, enabling dynamic delay control with DelayStrategy.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/delayWith.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\ndelayWith(delayStrategySupplier:()=>akka.stream.scaladsl.DelayStrategy[Out],overFlowStrategy:akka.stream.DelayOverflowStrategy):FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Counting Tweets Using Fold in Akka Streams (Scala)\nDESCRIPTION: Demonstrates how to count the number of tweets in a stream using Sink.fold. The result is made available as a Future[Int].\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_38\n\nLANGUAGE: Scala\nCODE:\n```\nval count: Flow[Tweet, Int, NotUsed] = Flow[Tweet].map(_ => 1)\n\nval sumSink: Sink[Int, Future[Int]] = Sink.fold[Int, Int](0)(_ + _)\n\nval counterGraph: RunnableGraph[Future[Int]] =\n  tweets\n    .via(count)\n    .toMat(sumSink)(Keep.right)\n\nval sum: Future[Int] = counterGraph.run()\n\nsum.foreach(c => println(s\"Total tweets processed: $c\"))\n```\n\n----------------------------------------\n\nTITLE: Creating RoundRobinGroup from Configuration (Scala/Java)\nDESCRIPTION: Demonstrates creating a RoundRobinGroup router actor named 'router1' within a parent actor. The router's behavior and routee paths are loaded from the application configuration under the deployment path '/parent/router1'. This requires a corresponding HOCON configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n//#round-robin-group-1\nimport akka.routing.FromConfig\n\nval router1: ActorRef = context.actorOf(FromConfig.props(Props[Worker]()), \"router1\")\n//#round-robin-group-1\n```\n\nLANGUAGE: java\nCODE:\n```\n//#round-robin-group-1\nimport akka.routing.FromConfig;\n\nfinal ActorRef router1 = getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)),\n    \"router1\");\n//#round-robin-group-1\n```\n\n----------------------------------------\n\nTITLE: Building Native Image with sbt\nDESCRIPTION: This code snippet describes how to build a test project with Akka using sbt, specifying a local snapshot version. The nativeImage command is used to build the native image from the test project.\nSOURCE: https://github.com/akka/akka/blob/main/native-image-tests/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsbt -Dakka.version=[local-snapshot-version] nativeImage\n```\n\n----------------------------------------\n\nTITLE: Duration Dilation for Slow Test Systems\nDESCRIPTION: Demonstrates scaling test durations to account for slower test environments using the timefactor configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\n#duration-dilation\n```\n\n----------------------------------------\n\nTITLE: Implementing a Publisher Actor in Scala\nDESCRIPTION: Scala example of an actor that publishes messages to a specific topic using the DistributedPubSubMediator. This publisher sends messages to the 'content' topic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\nclass Publisher extends Actor {\n  import DistributedPubSubMediator.Publish\n  // activate the extension\n  val mediator = DistributedPubSub(context.system).mediator\n\n  def receive = {\n    case in: String =>\n      mediator ! Publish(\"content\", in)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Compression Configuration for Jackson JSON\nDESCRIPTION: Configuration snippet showing the default compression settings for jackson-json binding in Akka serialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_23\n\nLANGUAGE: HOCON\nCODE:\n```\n@@snip [reference.conf](/akka-serialization-jackson/src/main/resources/reference.conf) { #compression }\n```\n\n----------------------------------------\n\nTITLE: Changing to Project and Running Tests\nDESCRIPTION: Commands for changing to the akka-remote-tests project first and then running all multi-JVM tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_3\n\nLANGUAGE: none\nCODE:\n```\nproject akka-remote-tests\nmulti-jvm:test\n```\n\n----------------------------------------\n\nTITLE: Executing KillrWeather in Maven\nDESCRIPTION: This snippet shows how to start a three-node Akka Cluster, configured to run within a single JVM process using Maven commands. It demonstrates how to compile and run the project files for KillrWeather.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-java/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmvn compile\nmvn -pl killrweather exec:java\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Maven Repository\nDESCRIPTION: Configuration for accessing Akka dependencies from the official Akka library repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guide-2.8.x-2.9.x.md#2025-04-22_snippet_0\n\nLANGUAGE: markup\nCODE:\n```\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n```\n\n----------------------------------------\n\nTITLE: Scheduling Recurring Message in Scala\nDESCRIPTION: Schedule a 'Tick' message to be sent to tickActor repeatedly every 50ms with initial delay of 0ms.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/scheduler.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval cancellable = system.scheduler.scheduleWithFixedDelay(\n  Duration.Zero,\n  50.milliseconds,\n  tickActor,\n  \"Tick\")(system.dispatcher)\n```\n\n----------------------------------------\n\nTITLE: Enabling Logging of Inbound Remoting Messages at DEBUG - HOCON\nDESCRIPTION: This snippet enables logging of all inbound messages received via Akka remoting (Artery) at DEBUG level. Configurable under akka.remote.artery with log-received-messages parameter. Should be applied for detailed network diagnostics and only with debug-level logging enabled.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_24\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery {\n  # If this is \"on\", Akka will log all inbound messages at DEBUG level,\n  # if off then they are not logged\n  log-received-messages = on\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Circuit Breaker in Scala\nDESCRIPTION: Configuration example for initializing a circuit breaker named 'data-access' with specific timeout and failure threshold settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval breaker = CircuitBreaker(\n  context.system.scheduler,\n  maxFailures = 5,\n  callTimeout = 10.seconds,\n  resetTimeout = 1.minute\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistence Plugin Proxy in Akka\nDESCRIPTION: Configuration entries for setting up the journal and snapshot store proxies in Akka. These settings control the target plugins, instantiation, and addressing of the shared persistence plugin.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nakka.persistence.journal.proxy\nakka.persistence.snapshot-store.proxy\ntarget-journal-plugin\ntarget-snapshot-store-plugin\nstart-target-journal\nstart-target-snapshot-store\ntarget-journal-address\ntarget-snapshot-store-address\n```\n\n----------------------------------------\n\nTITLE: External Address Configuration - Java\nDESCRIPTION: Configuration for external address handling in Java for actor serialization\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-classic.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n#external-address-default\n```\n\n----------------------------------------\n\nTITLE: Providing a Child Maker Function in Production - Java\nDESCRIPTION: This snippet provides an implementation for the child factory function in production Java Akka code. It returns a new Props or Actor for the parent. Used where flexibility in child creation is desired for larger systems.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #child-maker-prod }\n```\n\n----------------------------------------\n\nTITLE: Guardian Actor Initialization with SpawnProtocol in Scala\nDESCRIPTION: Shows how to use SpawnProtocol to initialize task and actor creation from the guardian actor in Scala. Useful for managing simple applications where actors are spawned from outside the guardian actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-lifecycle.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntroSpec.scala](/akka-actor-typed-tests/src/test/scala/docs/akka/typed/SpawnProtocolDocSpec.scala) { #imports1 #main }\n```\n\n----------------------------------------\n\nTITLE: External Shard Allocation Entity Configuration in Scala\nDESCRIPTION: Scala example showing how to configure an entity with external shard allocation strategy\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\nEntity(TypeKey[Command](\"Counter\"))\\n  .withAllocationStrategy(new ExternalShardAllocationStrategy(system, typeName))\n```\n\n----------------------------------------\n\nTITLE: Akka Streams zipLatestWith Source Signature\nDESCRIPTION: API signature for the zipLatestWith operator on Source class, showing both Scala and Java variants. Takes a Graph source and combine function to produce combined output.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipLatestWith.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.zipLatestWith[Out2,Out3](that:akka.stream.Graph[akka.stream.SourceShape[Out2],_])(combine:(Out,Out2)=>Out3):FlowOps.this.Repr[Out3]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.zipLatestWith(akka.stream.Graph,akka.japi.function.Function2)\n```\n\n----------------------------------------\n\nTITLE: Implementing TwoPhaseSet Serializer with Embedded Types in Java\nDESCRIPTION: Java implementation that shows how to serialize a TwoPhaseSet by leveraging existing serializers for embedded GSet instances, using otherMessageToProto and otherMessageFromBinary methods.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_37\n\nLANGUAGE: java\nCODE:\n```\npublic class TwoPhaseSetSerializer2 extends AbstractSerializationSupport\n    implements SerializerWithStringManifest {\n\n  private static final String TWOPHASESET_MANIFEST = \"A\";\n\n  @Override\n  public String manifest(Object o) {\n    if (o instanceof TwoPhaseSet)\n      return TWOPHASESET_MANIFEST;\n    else\n      throw new IllegalArgumentException(\"Can't serialize object of type \" + o.getClass());\n  }\n\n  @Override\n  public byte[] toBinary(Object o) {\n    if (o instanceof TwoPhaseSet)\n      return twoPhaseSetToProto((TwoPhaseSet) o).toByteArray();\n    else\n      throw new IllegalArgumentException(\"Can't serialize object of type \" + o.getClass());\n  }\n\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) {\n    if (TWOPHASESET_MANIFEST.equals(manifest))\n      return twoPhaseSetFromBinary(bytes);\n    else\n      throw new IllegalArgumentException(\"Unknown manifest: \" + manifest);\n  }\n\n  private TwoPhaseSetMessages2.TwoPhaseSet twoPhaseSetToProto(TwoPhaseSet twoPhaseSet) {\n    TwoPhaseSetMessages2.TwoPhaseSet.Builder b = TwoPhaseSetMessages2.TwoPhaseSet.newBuilder();\n    b.setAdds(otherMessageToProto(twoPhaseSet.getAdds()));\n    b.setRemovals(otherMessageToProto(twoPhaseSet.getRemovals()));\n    return b.build();\n  }\n\n  @SuppressWarnings(\"unchecked\")\n  private TwoPhaseSet twoPhaseSetFromBinary(byte[] bytes) {\n    try {\n      TwoPhaseSetMessages2.TwoPhaseSet msg = TwoPhaseSetMessages2.TwoPhaseSet.parseFrom(bytes);\n      GSet<String> adds =\n          (GSet<String>) otherMessageFromBinary(msg.getAdds());\n      GSet<String> removals =\n          (GSet<String>) otherMessageFromBinary(msg.getRemovals());\n      return new TwoPhaseSet(adds, removals);\n    } catch (InvalidProtocolBufferException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Forward-Compatible ItemAdded Event Class in Scala\nDESCRIPTION: Scala code snippet showing the ItemAdded event class during the first deployment of a rolling update, with forward compatibility.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_14\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ItemAdded.scala](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v1/ItemAdded.scala) { #forward-one-rename }\n```\n\n----------------------------------------\n\nTITLE: Custom Mailbox Implementation in Java\nDESCRIPTION: Complete example of a custom mailbox implementation with unbounded queue in Java, showing how to define the mailbox type and message queue.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/mailboxes.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.ActorRef;\nimport akka.actor.ActorSystem;\nimport akka.dispatch.Envelope;\nimport akka.dispatch.MailboxType;\nimport akka.dispatch.MessageQueue;\nimport akka.dispatch.ProducesMessageQueue;\nimport com.typesafe.config.Config;\n\nimport java.util.Queue;\nimport java.util.concurrent.ConcurrentLinkedQueue;\n\n// This is the MessageQueue implementation\npublic class MyUnboundedMailbox implements MailboxType, ProducesMessageQueue<MyUnboundedMessageQueue.MyMessageQueue> {\n\n    // This is the MessageQueue implementation\n    public static class MyMessageQueue implements MessageQueue, MyUnboundedMessageQueueSemantics {\n        private final Queue<Envelope> queue = new ConcurrentLinkedQueue<Envelope>();\n\n        // these must be implemented\n        public void enqueue(ActorRef receiver, Envelope handle) {\n            queue.offer(handle);\n        }\n\n        public Envelope dequeue() {\n            return queue.poll();\n        }\n\n        public int numberOfMessages() {\n            return queue.size();\n        }\n\n        public boolean hasMessages() {\n            return !queue.isEmpty();\n        }\n\n        public void cleanUp(ActorRef owner, MessageQueue deadLetters) {\n            for (Envelope handle : queue) {\n                deadLetters.enqueue(owner, handle);\n            }\n        }\n    }\n\n    // This constructor signature must exist, it will be called by Akka\n    public MyUnboundedMailbox(ActorSystem.Settings settings, Config config) {\n        // put your initialization code here\n    }\n\n    // The create method is called to create the MessageQueue\n    public MessageQueue create(Option<ActorRef> owner, Option<ActorSystem> system) {\n        return new MyMessageQueue();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Array Arguments in Java\nDESCRIPTION: Demonstrates how to log array elements as individual arguments in Java by passing an array as the single substitution argument.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nObject[] args = new Object[] { \"The\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\" };\nlog.info(\"Logging in colors: {}\", args);\n```\n\n----------------------------------------\n\nTITLE: Manual Scheduling for Timing-Sensitive Tests in Java\nDESCRIPTION: Java version of the manual scheduling example, showing how to control timing in Akka tests by explicitly advancing the clock.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ManualTimerExampleTest.scala](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/ManualTimerExampleTest.java) { #manual-scheduling-simple }\n```\n\n----------------------------------------\n\nTITLE: Scala Source.delayWith Signature\nDESCRIPTION: API signature for delayWith operator in Akka Streams Source, allowing dynamic delay control with a DelayStrategy supplier and overflow handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/delayWith.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ndelayWith(delayStrategySupplier:()=>akka.stream.scaladsl.DelayStrategy[Out],overFlowStrategy:akka.stream.DelayOverflowStrategy):FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Actor for Chatroom System in Scala\nDESCRIPTION: This code sets up the main actor that coordinates the chat room system. It spawns both the ChatRoom and Gabbler actors, initiates the chat session, and watches for termination of the gabbler to shut down the system. It demonstrates actor hierarchy and lifecycle management.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actors.md#2025-04-22_snippet_22\n\nLANGUAGE: scala\nCODE:\n```\nobject Main {\n  def apply(): Behavior[NotUsed] =\n    Behaviors.setup { context =>\n      val chatRoom = context.spawn(ChatRoom(), \"chatroom\")\n      val gabblerRef = context.spawn(Gabbler(), \"gabbler\")\n      context.watch(gabblerRef)\n      chatRoom ! GetSession(\"ol' Gabbler\", gabblerRef)\n\n      Behaviors.receiveSignal {\n        case (_, Terminated(_)) =>\n          Behaviors.stopped\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Scala idleTimeout API Definition\nDESCRIPTION: Scala API signature for idleTimeout operator that can be applied to Source and Flow types. Takes a FiniteDuration parameter and returns the same type preserving the output type.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/idleTimeout.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nidleTimeout(timeout: scala.concurrent.duration.FiniteDuration): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Log Output Verifying Local Processing on Multiple Nodes (Text)\nDESCRIPTION: This log snippet demonstrates the effectiveness of the external shard allocation strategy with multiple processor nodes. It shows pairs of log messages: one for forwarding a message for a specific entity (e.g., entity 29) and another immediately following it showing the entity actor processing the message. Crucially, both logs in each pair originate from the *same* Akka node (identified by `akka://KafkaToSharding` without a remote address), confirming that the message was processed locally on the node that consumed it from Kafka, even after cluster scaling and rebalancing.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n[info] [2020-01-17 08:27:58,199] [INFO] [akka://KafkaToSharding/user/kafka-event-processor] - Forwarding message for entity 29 to cluster sharding\n[info] [2020-01-17 08:27:58,204] [INFO] [akka://KafkaToSharding/system/sharding/user-processing/45/29] - user 29 purchase cat t-shirt, quantity 1, price 2093\n[info] [2020-01-17 08:28:08,218] [INFO] [akka://KafkaToSharding/user/kafka-event-processor] - Forwarding message for entity 56 to cluster sharding\n[info] [2020-01-17 08:28:08,218] [INFO] [akka://KafkaToSharding/system/sharding/user-processing/6/56] - user 56 purchase akka t-shirt, quantity 3, price 8576\n[info] [2020-01-17 08:28:28,288] [INFO] [akka://KafkaToSharding/user/kafka-event-processor] - Forwarding message for entity 44 to cluster sharding\n[info] [2020-01-17 08:28:28,296] [INFO] [akka://KafkaToSharding/system/sharding/user-processing/59/44] - user 44 purchase cat t-shirt, quantity 3, price 9716\n```\n\n----------------------------------------\n\nTITLE: Actor System Shutdown in Testing - Java\nDESCRIPTION: Java code snippet illustrating the proper method for shutting down the ActorSystem when testing with ActorTestKit, ensuring tests are completed cleanly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [AsyncTestingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/AsyncTestingExampleTest.java) { #test-shutdown }\n```\n\n----------------------------------------\n\nTITLE: Using Ask Pattern with ActorFlow.askWithContext in Akka Streams (Java)\nDESCRIPTION: Shows the Java API for ActorFlow.askWithContext, where each stream element (paired with context) invokes a BiFunction to create the message for a provided typed ActorRef and expects a reply within a specified timeout (java.time.Duration). Output tuples combine the reply and original context per stream element. This requires 'akka-stream-typed' Java artifact and the Akka typed actor system. Proper parallelism and backpressure handling must be observed, and the Flow fails on timeout or actor termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithContext.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nActorFlow.askWithContext(actorRef, java.time.Duration timeout, java.util.function.BiFunction<(I, Ctx), akka.actor.typed.ActorRef<A>, Q>)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Causal Delivery and Concurrent Events in Replicated Event Sourcing - Generic\nDESCRIPTION: This generic code snippet provides illustrative pseudocode for causal delivery and concurrent event handling in replicated event sourcing, independent of a specific language. The examples show sequences of write and read operations across data centers (DC-1, DC-2, DC-3), modeling how events propagate and are observed in potentially different orders. There are no special dependencies; the input is a sequence of labeled operations, and the output is an explanation of event order seen from different replicas. This serves as a conceptual explanation rather than runnable code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing.md#2025-04-22_snippet_6\n\nLANGUAGE: \nCODE:\n```\nDC-1: write e1\\nDC-2: read e1, write e2\\nDC-1: read e2, write e3\n```\n\nLANGUAGE: \nCODE:\n```\nDC1: write e1\\nDC2: read e1, write e2\\nDC1: write e3 (e2 and e3 are concurrent)\\nDC1: read e2\\nDC2: read e3\n```\n\n----------------------------------------\n\nTITLE: Reactive Streams Semantics Description for lazyCompletionStage\nDESCRIPTION: Describes the emitting and completion behavior of the lazyCompletionStage operator in reactive streams context. The operator emits when downstream demand exists and the future completes, then completes itself after emission.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazyCompletionStage.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n**emits** when there is downstream demand and the element factory returned future has completed\n\n**completes** after emitting the single element\n```\n\n----------------------------------------\n\nTITLE: Run Source in Java\nDESCRIPTION: The Java snippet shows executing a source with a consumer function to process the data from the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [QuickStartDocTest.java](/akka-docs/src/test/java/jdocs/stream/QuickStartDocTest.java) { #run-source }\n```\n\n----------------------------------------\n\nTITLE: Implementing mapConcat in Scala\nDESCRIPTION: Example showing how to use mapConcat to emit each integer twice in the stream. The operator transforms each input element into an Iterable containing two copies of the element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapConcat.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 3)\n  .mapConcat(n => List(n, n))\n  .runWith(Sink.foreach(println))\n\n// prints:\n// 1\n// 1\n// 2\n// 2\n// 3\n// 3\n```\n\n----------------------------------------\n\nTITLE: Setting TTL for Topic Actor in Akka Cluster PubSub - Java\nDESCRIPTION: Demonstrates configuring a TTL for a topic actor to auto-stop after a period of inactivity in Java. Relies on akka.actor.typed.pubsub.PubSub and requires a topic name, message class, and TTL duration. If the topic receives no messages or local subscriptions during this period, it is removed from the registry and stopped.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-pub-sub.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nActorRef<Message> topicWithTTL =\n  PubSub.get(system).topic(\"my-topic\", Message.class, \n    Duration.ofSeconds(30));\n```\n\n----------------------------------------\n\nTITLE: Java onErrorComplete API Signatures\nDESCRIPTION: API signatures for onErrorComplete operator in Java for Source and Flow types. Supports predicate and class-based error handling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/onErrorComplete.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.onErrorComplete(java.util.function.Predicate)\nSource.onErrorComplete(java.lang.Class)\nFlow.onErrorComplete(java.util.function.Predicate)\nFlow.onErrorComplete(java.lang.Class)\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Messages to Router Example\nDESCRIPTION: Shows how to broadcast a warning message to all routees in a router using Broadcast wrapper.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_33\n\nLANGUAGE: scala\nCODE:\n```\nrouter ! Broadcast(\"Watch out for Davy Jones' locker\")\n```\n\n----------------------------------------\n\nTITLE: Testing Missing Temperature Reading\nDESCRIPTION: Test case for handling scenarios where devices cannot provide temperature measurements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\n#query-test-no-reading\n```\n\nLANGUAGE: Java\nCODE:\n```\n#query-test-no-reading\n```\n\n----------------------------------------\n\nTITLE: Restart Section Supervision Example\nDESCRIPTION: Demonstrates restart supervision strategy that resets operator state on failure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_18\n\nLANGUAGE: Scala\nCODE:\n```\nval decider: Supervision.Decider = {\n  case _: ArithmeticException => Supervision.Restart\n  case _ => Supervision.Stop\n}\n\nFlow[Int].map(100 / _).withAttributes(ActorAttributes.supervisionStrategy(decider))\n```\n\n----------------------------------------\n\nTITLE: Sharding Persistence Mode Deprecation Warning\nDESCRIPTION: Warning notice about the deprecation of persistence state store mode and migration recommendations to ddata and eventsourced modes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/includes/cluster.md#2025-04-22_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n@@@ warning\n\nPersistence for state store mode is deprecated. It is recommended to migrate to `ddata` for the coordinator state and if using replicated entities\nmigrate to `eventsourced` for the replicated entities state.\n\nThe data written by the deprecated `persistence` state store mode for remembered entities can be read by the new remember entities `eventsourced` mode.\n\nOnce you've migrated you can not go back to `persistence` mode.\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Generating Akka Documentation TOC in Markdown\nDESCRIPTION: Markdown code that generates a table of contents and index for Akka documentation, using special directives @@toc and @@@ index to organize the content structure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@toc { depth=2 }\n\n@@@ index\n\n* [terminology](terminology.md)\n* [actor-systems](actor-systems.md)\n* [actors](actors.md)\n* [supervision](supervision.md)\n* [addressing](addressing.md)\n* [remoting](remoting.md)\n* [jmm](jmm.md)\n* [message-delivery-reliability](message-delivery-reliability.md)\n* [configuration](configuration.md)\n* [configuration-reference](configuration-reference.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Configuring SingleConsumerOnlyUnboundedMailbox as Default Mailbox\nDESCRIPTION: HOCON configuration that sets SingleConsumerOnlyUnboundedMailbox as the default mailbox type for all actors. This mailbox is more efficient than the standard unbounded mailbox but cannot be used with a BalancingDispatcher.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.default-mailbox {\n  mailbox-type = \"akka.dispatch.SingleConsumerOnlyUnboundedMailbox\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Output for Word Count Snippet\nDESCRIPTION: Shows the expected standard output when running the Scala or Java word count examples, displaying the character count for each line.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\n23\n16\n26\n```\n\n----------------------------------------\n\nTITLE: Watching ActorRef Termination with Source/Flow in Akka Streams (Scala)\nDESCRIPTION: This snippet demonstrates how to use the 'watch' operator on an Akka Streams Source or Flow in Scala to monitor the lifecycle of an ActorRef. When the watched actor terminates, the stream fails with a WatchedActorTerminatedException. Dependencies include Akka Streams and a materialized ActorRef to be watched. Key parameters: the ActorRef to watch. Input is any upstream stream element, output is identical elements unless the actor terminates, in which case the stream fails.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/watch.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n\"\"\"@@snip [Watch.scala](/akka-docs/src/test/scala/docs/stream/operators/sourceorflow/Watch.scala) { #watch }\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Recommended Default Strategy with Limit (HOCON)\nDESCRIPTION: This HOCON configuration snippet enables the recommended default passivation strategy (`default-strategy`) and explicitly configures the `limit` for active entities per shard region to 100,000. Entities will be passivated based on the strategy's replacement policy when this limit is exceeded.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_29\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.sharding {\n  passivation {\n    strategy = default-strategy\n    # configure the active entity limit for the default strategy\n    default-strategy {\n      # approximately 100k entities per node\n      limit = 100000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Snapshot Store Directory (HOCON)\nDESCRIPTION: Illustrates configuring the file system path for the local snapshot store using the `akka.persistence.snapshot-store.local.dir` setting. The specified path can be relative or absolute; if not set, it defaults to a 'snapshots' directory in the current working directory.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_13\n\nLANGUAGE: hocon\nCODE:\n```\n# Assuming the snippet configures the snapshot store directory\nakka.persistence.snapshot-store.local {\n  # Path to the directory where snapshots are stored\n  dir = \"target/snapshots\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing ScalaTest Integration for MultiNodeSpec in Scala\nDESCRIPTION: This snippet defines a trait named STMultiNodeSpec that integrates ScalaTest with Akka's MultiNodeSpec. It uses ScalaTest's BeforeAndAfterAll to properly start and stop MultiNodeSpec before and after all tests are run.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-node-testing.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ntrait STMultiNodeSpec extends Suite with BeforeAndAfterAll { this: MultiNodeSpec =>\n\n  override def beforeAll() = multiNodeSpecBeforeAll()\n\n  override def afterAll() = multiNodeSpecAfterAll()\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling All Akka Internal Logging - HOCON\nDESCRIPTION: This HOCON snippet disables all logging from Akka by setting both stdout-loglevel and loglevel to 'OFF'. Useful for environments where no Akka-internal logs are desired, such as production silencing or suppressing logs during sensitive operations. Place within your Akka config, however, understand that logs related to startup, shutdown, errors, and warnings will no longer be emitted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_18\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  stdout-loglevel = \"OFF\"\n  loglevel = \"OFF\"\n}\n```\n\n----------------------------------------\n\nTITLE: Java Sink.never Method Signature\nDESCRIPTION: API signature for creating a Sink that always backpressures in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/never.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nSink.never()\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Temperature Reading Protocol in Java\nDESCRIPTION: Java implementation of a request-respond message protocol for device actors to handle temperature reading requests. Includes a ReadTemperature message class for requests and a RespondTemperature class for responses with an optional temperature value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_3.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class ReadTemperature {\n  public final ActorRef<RespondTemperature> replyTo;\n\n  public ReadTemperature(ActorRef<RespondTemperature> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic class RespondTemperature {\n  public final Optional<Double> value;\n\n  public RespondTemperature(Optional<Double> value) {\n    this.value = value;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Router with Consistent Hashing in Akka Clusters\nDESCRIPTION: This configuration sets up a router using consistent-hashing-pool for distributing work among nodes in the cluster. It specifies cluster settings like enabling the cluster, setting the maximum number of instances per node, allowing local routees, and using specific roles. This configuration is written in HOCON and applies to actors within the Akka framework to ensure scalability and resource management across the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_15\n\nLANGUAGE: HOCON\nCODE:\n```\nakka.actor.deployment {\n  /statsService/singleton/workerRouter {\n    router = consistent-hashing-pool\n    cluster {\n      enabled = on\n      max-nr-of-instances-per-node = 3\n      allow-local-routees = on\n      use-roles = [\"compute\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Actor with Custom Mailbox via Deployment in Java\nDESCRIPTION: Java code demonstrating how to create an actor that uses a mailbox defined in deployment configuration. The actor will use the mailbox specified in the configuration file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n// this actor will have a custom mailbox applied in deployment config\nfinal ActorRef myActor = system.actorOf(Props.create(MyActor.class), \"priomailboxactor\");\n```\n\n----------------------------------------\n\nTITLE: Using interleaveAll in Akka Streams (Scala)\nDESCRIPTION: Demonstrates how to use the interleaveAll operator in Scala to combine elements from multiple sources in an interleaved pattern. It shows the creation of three sources and their interleaving with a segment size of 2.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/interleaveAll.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval sourceA = Source(List(1, 2, 3))\nval sourceB = Source(List(4, 5, 6))\nval sourceC = Source(List(7, 8, 9))\n\nval result = sourceA.interleaveAll(List(sourceB, sourceC), 2, eagerClose = false).runWith(Sink.seq)\n\nresult.futureValue should contain theSameElementsAs Seq(1, 2, 4, 5, 7, 8, 3, 6, 9)\n```\n\n----------------------------------------\n\nTITLE: Implementing conflateWithSeed in Scala\nDESCRIPTION: Example showing how to use conflateWithSeed operator in Scala to handle backpressure and transform elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/conflateWithSeed.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#conflateWithSeed\n```\n\n----------------------------------------\n\nTITLE: Merging Prioritized Sources in Java\nDESCRIPTION: Example demonstrating the mergePrioritized operator usage in Java with Akka Streams. Shows how to combine two sources with different priorities to control element selection probability.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergePrioritized.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer> sourceA = Source.from(Arrays.asList(1, 2, 3));\nSource<Integer> sourceB = Source.from(Arrays.asList(4, 5, 6));\nSource<Integer> priorityMerged = sourceA.mergePrioritized(sourceB, 2, 1, false);\nprioritizedMerged.runWith(Sink.foreach(System.out::println), system);\n```\n\n----------------------------------------\n\nTITLE: Scala takeWithin Signature\nDESCRIPTION: Method signature for the takeWithin operator in Scala, which takes a FiniteDuration parameter and returns a transformed Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/takeWithin.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ntakeWithin(d: scala.concurrent.duration.FiniteDuration): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Running AppOneMaster Compute Node on Port 25252 using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.AppOneMaster` main class using Maven's exec plugin in a separate process. This command starts another node of the Akka cluster sample application (Cluster Singleton example) configured as a 'compute' node listening on port 25252.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.AppOneMaster\" -Dexec.args=\"compute 25252\"\n```\n\n----------------------------------------\n\nTITLE: JVM Options for Node1\nDESCRIPTION: Example of node-specific JVM options file content for SampleMultiJvmNode1, setting remote port and memory options.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_12\n\nLANGUAGE: none\nCODE:\n```\n-Dakka.remote.port=9991 -Xmx256m\n```\n\n----------------------------------------\n\nTITLE: Using Request Context with Update in Java\nDESCRIPTION: Java example showing how to pass a request context (original sender) with an Update message, which can be used to route the response back to the original requester.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nfinal Key<PNCounter> Counter1Key = PNCounterKey.create(\"counter1\");\nreplicator.tell(\n    new Update<PNCounter>(\n        Counter1Key,\n        PNCounter.create(),\n        WriteLocal.instance(),\n        curr -> curr.increment(node, 1),\n        getContext().getSender()),\n    getSelf());\n\n// handle response\nreceiveBuilder()\n    .match(\n        UpdateSuccess.class,\n        a -> {\n          if (a.key().equals(Counter1Key) && a.getRequest().isPresent()) {\n            ActorRef replyTo = (ActorRef) a.getRequest().get();\n            replyTo.tell(\"updated\", getSelf());\n          }\n        })\n    .match(\n        UpdateTimeout.class,\n        a -> {\n          if (a.key().equals(Counter1Key) && a.getRequest().isPresent()) {\n            ActorRef replyTo = (ActorRef) a.getRequest().get();\n            replyTo.tell(\"not updated, but will eventually be replicated\", getSelf());\n          }\n        })\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Using mapAsyncUnordered for Unordered Processing (Java)\nDESCRIPTION: Demonstrates using `mapAsyncUnordered` instead of `mapAsync` when the order of elements downstream does not need to match the upstream order. This can improve performance as results are emitted as soon as they become available.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nSource<Tweet, NotUsed> otherTweets = tweets; // imagine ordering is not important\n\nSource<Author, NotUsed> authors = otherTweets.filter(t -> t.hashtags().contains(akkaTag)).map(t -> t.author);\n\nint parallelism = 4;\nSource<String, NotUsed> emailAddresses = authors\n    .mapAsyncUnordered(parallelism, author -> lookupEmail(author.handle))\n    .filter(Optional::isPresent)\n    .map(Optional::get);\n```\n\n----------------------------------------\n\nTITLE: Implementing an Actor That Supports Backpressure in Java\nDESCRIPTION: A Java implementation of an actor that processes messages and acknowledges them to support backpressure. The actor handles various message types including stream elements, initialization, completion, and failure messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/actorRefWithBackpressure.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nstatic class Ref extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n      .match(StreamInit.class, s -> {\n        System.out.println(\"Stream initialized!\");\n        sender().tell(Continue.instance, self());\n      })\n      .match(StreamElement.class, s -> {\n        System.out.println(\"Stream element: \" + s.element);\n        sender().tell(Continue.instance, self());\n      })\n      .match(StreamComplete.class, s -> {\n        System.out.println(\"Stream completed!\");\n        context().stop(self());\n      })\n      .match(StreamFailure.class, s -> {\n        System.out.println(\"Stream failed with: \" + s.ex.getMessage());\n        context().stop(self());\n      })\n      .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Scala Source/Flow Backpressure Timeout API\nDESCRIPTION: API signature for backpressureTimeout operator in Scala, which takes a FiniteDuration parameter and returns the same type as the original Source/Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/backpressureTimeout.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nbackpressureTimeout(timeout: scala.concurrent.duration.FiniteDuration): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Implementing RequiresMessageQueue in Java\nDESCRIPTION: Java actor implementation that requires a bounded mailbox by implementing the RequiresMessageQueue interface. The bounded mailbox ensures that the actor can handle message surges without consuming unlimited memory.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nimport akka.actor.AbstractActor;\nimport akka.dispatch.BoundedMessageQueueSemantics;\nimport akka.dispatch.RequiresMessageQueue;\n\npublic class MyBoundedActor extends AbstractActor implements RequiresMessageQueue<BoundedMessageQueueSemantics> {\n    @Override\n    public Receive createReceive() {\n        return receiveBuilder().matchAny(o -> {\n            // Do something useful with the message\n        }).build();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Logger Name in Scala\nDESCRIPTION: Demonstrates how to set a custom logger name for an actor in Scala. This allows for more precise control over log output identification.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nclass MyActor extends AbstractBehavior[String](context) {\n  context.setLoggerName(\"my.custom.logger.name\")\n\n  override def onMessage(msg: String): Behavior[String] = {\n    context.log.debug(\"Received message {}\", msg)\n    this\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cluster Singleton Documentation\nDESCRIPTION: Documentation explaining the Cluster Singleton feature which ensures only one actor of a certain type runs in the cluster.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/includes/cluster.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Cluster Singleton\n\nFor some use cases it is convenient or necessary to ensure only one \nactor of a certain type is running somewhere in the cluster.\nThis can be implemented by subscribing to member events, but there are several corner\ncases to consider. Therefore, this specific use case is covered by the Cluster Singleton.\n```\n\n----------------------------------------\n\nTITLE: Example Output from Stream Monitoring\nDESCRIPTION: Sample console output showing the stream's state at different stages of processing.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/monitor.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nStream is initialized but hasn't processed any element\n0\n1\n2\nLast element processed: 2\n3\n4\n5\nStream completed already\n```\n\n----------------------------------------\n\nTITLE: Initializing DiagnosticLoggingAdapter in Scala\nDESCRIPTION: Shows how to create a DiagnosticLoggingAdapter within an Actor in Scala. This adapter allows setting custom MDC values for logging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\n// Within your Actor\nval log: DiagnosticLoggingAdapter = Logging(this);\n```\n\n----------------------------------------\n\nTITLE: Scala JsonSerializable Type Example\nDESCRIPTION: Example showing how to implement JsonSerializable marker trait for serialization in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/native-image.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass MyClass() extends JsonSerializable\n```\n\n----------------------------------------\n\nTITLE: Basic Sliding Window with Default Step\nDESCRIPTION: Demonstrates basic sliding window operation with window size 2 and default step of 1, showing how elements are grouped into overlapping windows.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/sliding.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval numbers = Source(1 to 4)\nnumbers.sliding(2).runWith(Sink.foreach(println))\n// prints: Vector(1, 2)\n// Vector(2, 3)\n// Vector(3, 4)\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3, 4))\n    .sliding(2, 1)\n    .runForeach(window -> System.out.println(window), system);\n// prints: [1, 2]\n// [2, 3]\n// [3, 4]\n```\n\n----------------------------------------\n\nTITLE: Using TestProbe as Parent in Akka Actor Tests - Java\nDESCRIPTION: In Akka's Java TestKit, this snippet constructs a child actor with the test kit as parent, ensuring all parent-related messages go to the probe. Required dependencies are Akka TestKit and relevant test infrastructure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [ParentChildTest.java](/akka-docs/src/test/java/jdocs/testkit/ParentChildTest.java) { #test-TestProbe-parent }\n```\n\n----------------------------------------\n\nTITLE: Starting Client Node for Stats Cluster (Random Port)\nDESCRIPTION: Starts a client node for the `sample.cluster.stats.App` example using SBT in a separate terminal. It specifies the role 'client' and port 0 (random). Client nodes use a group router to send tasks to available `StatsService` instances on compute nodes.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.stats.App client 0\"\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents in Markdown\nDESCRIPTION: This snippet generates a table of contents for the Akka Getting Started Guide using custom Markdown syntax. It specifies a depth of 2 for the table of contents.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@toc { depth=2 }\n```\n\n----------------------------------------\n\nTITLE: Defining Async Database API in Java\nDESCRIPTION: Java interface for an asynchronous database API with methods for querying, checking more results, and closing connections.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldResourceAsync.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\ninterface AsyncDatabaseApi {\n  // Initial async query\n  CompletionStage<QueryResult> runQuery(String query);\n  // Check if has more results\n  CompletionStage<Optional<Data>> hasMore(QueryResult result);\n  // Close the query/connection\n  CompletionStage<Done> close(QueryResult result);\n}\n```\n\n----------------------------------------\n\nTITLE: Running Akka Test Suite\nDESCRIPTION: Command to execute all Akka test suites using sbt.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsbt test\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency (SBT, Maven, Gradle)\nDESCRIPTION: Specifies the dependency declaration for the Akka Streams module (`akka-stream`) using SBT, Maven, and Gradle formats. It utilizes an Akka Bill of Materials (BOM) for consistent version management across Akka modules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\nval AkkaVersion = \"$akka.version$\"\ndefine Akka Streams dependency\nlazy val akkaHttpVersion = \"10.5.0\"\nlazy val akkaVersion     = \"2.8.0\"\nlazy val scalaBinaryVersion = \"$scala.binary.version$\"\n\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-stream\" % akkaVersion,\n  \"com.typesafe.akka\" %% \"akka-stream-testkit\" % akkaVersion % Test\n)\nenable Akka Typed settings\nlazy val example = project.in(file(\".\"))\n  .settings(\n    scalaVersion := $scala.version$,\n    run / fork := true,\n    Global / cancelable := true,\n    libraryDependencies ++= Seq(\n      \"com.typesafe.akka\" %% \"akka-stream\" % AkkaVersion\n    )\n  )\n```\n\nLANGUAGE: maven\nCODE:\n```\n<properties>\n  <scala.binary.version>$scala.binary.version$</scala.binary.version>\n  <akka.version>$akka.version$</akka.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_${scala.binary.version}</artifactId>\n      <version>${akka.version}</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies>\n```\n\nLANGUAGE: gradle\nCODE:\n```\ndef versions = [\n  ScalaBinary: \"$scala.binary.version$\",\n  AkkaVersion: \"$akka.version$\"\n]\n\n\ndependencies {\n  implementation platform(\"com.typesafe.akka:akka-bom_${versions.ScalaBinary}:\" + versions.AkkaVersion)\n\n  implementation \"com.typesafe.akka:akka-stream_${versions.ScalaBinary}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running StatsSample Application (Single JVM) using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.App` main class using Maven's exec plugin. This command starts the Akka cluster sample application with 4 actor systems (cluster members) running within the same JVM process. This demonstrates the group router example.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.App\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Monitored Shard Regions in Ruby\nDESCRIPTION: Configuration snippet to enable monitoring of specific shard regions by defining their entity type names. This allows health checks to be performed on selected shard regions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_50\n\nLANGUAGE: ruby\nCODE:\n```\nakka.cluster.sharding.healthcheck.names = [\"counter-1\", \"HelloWorld\"]\n```\n\n----------------------------------------\n\nTITLE: Starting Frontend Node for Transformation Cluster (Random Port)\nDESCRIPTION: Starts a frontend node for the `sample.cluster.transformation.App` example using SBT in a separate terminal. It specifies the role 'frontend' and port 0 (random). Frontend nodes discover backend workers via the Receptionist to delegate tasks.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.transformation.App frontend 0\"\n```\n\n----------------------------------------\n\nTITLE: Debugging FSM with Event Tracing in Akka Java\nDESCRIPTION: Outlines how to implement event tracing for FSMs in Java using Akka's debug settings. The mechanism logs detailed events including state changes and scheduled messages, leveraging AbstractLoggingFSM for insight into FSM processes. Great for Java developers aiming to understand FSM operation intricacies during debugging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FSMDocTest.java](/akka-docs/src/test/java/jdocs/actor/fsm/FSMDocTest.java) { #logging-fsm }\n```\n\n----------------------------------------\n\nTITLE: Printing Actor References in Java\nDESCRIPTION: Shows the Java equivalent of creating actors and printing their references, demonstrating the actor hierarchy.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_1.md#2025-04-22_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic class ActorHierarchyExperiments {\n  public static void main(String[] args) {\n    ActorSystem<String> testSystem = ActorSystem.create(Behaviors.setup(context -> {\n      ActorRef<String> firstRef = context.spawn(PrintMyActorRefActor.create(), \"first-actor\");\n      System.out.println(\"First: \" + firstRef);\n      firstRef.tell(\"printit\");\n      return Behaviors.empty();\n    }), \"testSystem\");\n  }\n}\n\nclass PrintMyActorRefActor {\n  static Behavior<String> create() {\n    return Behaviors.setup(context -> Behaviors.receiveMessage(message -> {\n      ActorRef<String> secondRef = context.spawn(Behaviors.empty(), \"second-actor\");\n      System.out.println(\"Second: \" + secondRef);\n      return Behaviors.same();\n    }));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Backpressure-Aware Operators Table in Markdown\nDESCRIPTION: Table documenting operators that respond to downstream backpressure signals and adapt their behavior accordingly.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/index.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| |Operator|Description|\n|--|--|--|\n|Source/Flow|<a name=\"aggregatewithboundary\"></a>@ref[aggregateWithBoundary](Source-or-Flow/aggregateWithBoundary.md)|Aggregate and emit until custom boundary condition met.|\n|Source/Flow|<a name=\"batch\"></a>@ref[batch](Source-or-Flow/batch.md)|Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum number of batched elements is not yet reached.|\n```\n\n----------------------------------------\n\nTITLE: Configuring Max Concurrent Recoveries in Akka Persistence (HOCON)\nDESCRIPTION: Sets the maximum number of persistent actor recoveries that can run concurrently using the `akka.persistence.max-concurrent-recoveries` configuration key. This helps prevent overloading the system and the backend data store. Actors attempting to recover beyond this limit will wait.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_4\n\nLANGUAGE: hocon\nCODE:\n```\nakka.persistence.max-concurrent-recoveries = 50\n```\n\n----------------------------------------\n\nTITLE: Compacting ByteStrings in a Stream in Scala\nDESCRIPTION: Uses a simple map operation to call the compact() method on ByteString elements, creating clean copies that no longer reference original ByteStrings. This should be the last element in a long chain due to copying of underlying arrays.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_53\n\nLANGUAGE: Scala\nCODE:\n```\nval compactFlow: Flow[ByteString, ByteString, NotUsed] = Flow[ByteString].map(_.compact)\n```\n\n----------------------------------------\n\nTITLE: Running TransformationApp in Java\nDESCRIPTION: This snippet demonstrates launching the TransformationApp, which starts five actor systems. The systems use different roles and dynamic discovery for message processing tasks.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\\\"sample.cluster.transformation.App\\\" -Dexec.args=\\\"backend 25251\\\"\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\\\"sample.cluster.transformation.App\\\" -Dexec.args=\\\"backend 25252\\\"\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\\\"sample.cluster.transformation.App\\\" -Dexec.args=\\\"backend 0\\\"\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\\\"sample.cluster.transformation.App\\\" -Dexec.args=\\\"frontend 0\\\"\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\\\"sample.cluster.transformation.App\\\" -Dexec.args=\\\"frontend 0\\\"\n```\n\n----------------------------------------\n\nTITLE: EventStream Superclass Subscription in Scala\nDESCRIPTION: Scala code demonstrating how to subscribe to a group of events by subscribing to their common superclass on the event stream. This allows receiving multiple event types with one subscription.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_27\n\nLANGUAGE: Scala\nCODE:\n```\nabstract class AllKindsOfMusic { def artist: String }\ncase class Jazz(artist: String) extends AllKindsOfMusic\ncase class Electronic(artist: String) extends AllKindsOfMusic\n\nval musicListener = system.actorOf(Props(new Actor {\n  def receive = {\n    case m: Jazz         => println(s\"${self.path.name} is listening to: ${m.artist}\")\n    case m: Electronic   => println(s\"${self.path.name} is listening to: ${m.artist}\")\n    case m: AllKindsOfMusic => println(s\"${self.path.name} heard some music but didn't recognize what kind it is\")\n  }\n}))\nsystem.eventStream.subscribe(musicListener, classOf[AllKindsOfMusic])\n\nsystem.eventStream.publish(Electronic(\"Parov Stelar\"))\n```\n\n----------------------------------------\n\nTITLE: Creating a Parallel Unordered Java Collector Sink with Akka Streams - Scala\nDESCRIPTION: This Scala signature defines javaCollectorParallelUnordered, a factory method to create a Sink that consumes elements of type T, collecting them in parallel and unordered manner using a provided java.util.stream.Collector. The sink materializes to a scala.concurrent.Future[R], representing the reduced result. Requires Akka Streams, Scala, and a compatible Java 8 Collector.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/javaCollectorParallelUnordered.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nscala=\"#javaCollectorParallelUnordered[T,R](parallelism:Int)(collectorFactory:()=>java.util.stream.Collector[T,_,R]):akka.stream.scaladsl.Sink[T,scala.concurrent.Future[R]]\"\n```\n\n----------------------------------------\n\nTITLE: Multi-JVM Test Naming Pattern\nDESCRIPTION: The naming convention pattern for multi-JVM tests, showing how tests are discovered and grouped.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_6\n\nLANGUAGE: none\nCODE:\n```\n{TestName}MultiJvm{NodeName}\n```\n\n----------------------------------------\n\nTITLE: Java Source.delayWith Signature\nDESCRIPTION: Java API signature for delayWith operator in Akka Streams Source, using Java's Supplier interface for delay strategy.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/delayWith.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ndelayWith(java.util.function.Supplier,akka.stream.DelayOverflowStrategy)\n```\n\n----------------------------------------\n\nTITLE: Transform Source in Java\nDESCRIPTION: The Java snippet shows how to apply transformations to a source stream using the scan operator to calculate factorials and then process the results.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [QuickStartDocTest.java](/akka-docs/src/test/java/jdocs/stream/QuickStartDocTest.java) { #transform-source }\n```\n\n----------------------------------------\n\nTITLE: Draining a Stream to a Seq Without Limits (Akka Streams Scala, UNSAFE)\nDESCRIPTION: Demonstrates use of Sink.seq to collect an unbounded stream into a collection, which can quickly exhaust system memory and is not recommended in production. Intended to illustrate what not to do; users should always constrain stream size for collecting. Input is stream of elements; output is Future[Seq[T]] of all stream elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RecipeSeq.scala](/akka-docs/src/test/scala/docs/stream/cookbook/RecipeSeq.scala) { #draining-to-seq-unsafe }\n```\n\n----------------------------------------\n\nTITLE: Implementing Slow Service Simulator in Scala\nDESCRIPTION: A service implementation that simulates varying processing times based on input string characteristics. Lower case characters trigger slower processing times to demonstrate async behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_21\n\nLANGUAGE: scala\nCODE:\n```\nclass SometimesSlowService {\n  def convert(s: String): Future[String] = {\n    val delayed = if (s.head.isLower) 2.seconds else 1.second\n    println(s\"running: $s (${ Thread.currentThread().getName })\")\n    after(delayed) {\n      println(s\"completed: $s (${ Thread.currentThread().getName })\")\n      s.toUpperCase\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Snapshot Repository in sbt\nDESCRIPTION: Configuration for adding Akka snapshot repository to sbt resolvers and defining library dependencies with snapshot versions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/links.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nresolvers += \"Akka library snapshot repository\".at(\"https://repo.akka.io/snapshots\")\n```\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies += \"com.typesafe.akka\" % \"akka-cluster_$scala.binary.version$\" % \"2.9.0+72-53943d99-SNAPSHOT\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository (sbt, Maven, Gradle)\nDESCRIPTION: Defines the repository URL (`https://repo.akka.io/maven`) required to fetch Akka dependencies. This configuration needs to be added to the build tool (sbt, Maven, or Gradle) settings to enable fetching Akka libraries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\nrepositories {\n  mavenCentral()\n  maven {\n    url = \"https://repo.akka.io/maven\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Counter in Akka Voting Service with Scala\nDESCRIPTION: This snippet demonstrates how to increment counters in Akka using a PNCounterMap for distributed voting services. The increment operation leverages WriteLocal for fast updates on the local node, and changes are later synchronized with other nodes. Dependencies include Akka's replication utilities, and parameters involve the local update key and participant identification.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-scala/README.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nval update = Update(CountersKey, PNCounterMap(), WriteLocal, request = Some(v)) {\n  _.increment(participant, 1)\n}\nreplicator ! update\n```\n\n----------------------------------------\n\nTITLE: Setting Subscription Timeout Attribute in Akka StreamRefs - Java\nDESCRIPTION: This Java code snippet shows how to configure a stream reference's subscription timeout using Akka Streams in Java. By adding a stream attribute (ActorAttributes.subscriptionTimeout) to a Flow, developers can control how long the origin node waits for a remote materialization before aborting the stream. Dependencies required are akka-stream and akka-actor-typed libraries. Inputs include a Flow and a customized timeout attribute; output is an augmented Flow with the subscription deadline. Make sure to handle resulting failures if the timeout triggers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-refs.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nflow.addAttributes(ActorAttributes.subscriptionTimeout(Duration.ofSeconds(10)));\n```\n\n----------------------------------------\n\nTITLE: Secondary Configuration Example\nDESCRIPTION: Another example of lifted configuration showing different effective settings\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/configuration.md#2025-04-22_snippet_7\n\nLANGUAGE: ruby\nCODE:\n```\nakka.loglevel = \"ERROR\"\nmy.own.setting = 42\nmy.other.setting = \"hello\"\n// plus myapp1 and myapp2 subtrees\n```\n\n----------------------------------------\n\nTITLE: Creating Resizable Router Pool in Scala\nDESCRIPTION: Creates a router pool with a default resizer programmatically in Scala. The resizer adjusts the pool size between 2 and 15 routees based on load.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_37\n\nLANGUAGE: scala\nCODE:\n```\nval resizer = DefaultResizer(lowerBound = 2, upperBound = 15)\nval router1 = context.actorOf(\n  RoundRobinPool(5, Some(resizer)).props(Props[Worker]),\n  \"router1\")\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Sources with MergeAll in Java\nDESCRIPTION: Shows how to merge multiple source streams using the mergeAll operator in Java. Illustrates the random element selection behavior when multiple sources have available elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mergeAll.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSourceOrFlow.java\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running Akka Java Project with Maven - Bash\nDESCRIPTION: This snippet provides the Maven command to compile and run the Akka Hello World Java project. It requires Maven to be installed and properly configured with a working pom.xml in the project root directory. The command sequences the compilation of source files and executes the main class as defined in the Maven project descriptor. Input: none, Output: application runs from main class. This is suited for users familiar with Maven-based Java projects.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-quickstart-java/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmvn compile exec:exec\n```\n\n----------------------------------------\n\nTITLE: Running AppOneMaster Application (Single JVM) using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.AppOneMaster` main class using Maven's exec plugin. This command starts the Akka cluster sample application featuring a Cluster Singleton pattern, with 4 actor systems (cluster members) running within the same JVM process.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.AppOneMaster\"\n```\n\n----------------------------------------\n\nTITLE: Defining Message Classes for Stream Elements in Java\nDESCRIPTION: Java implementation of the message classes used for stream processing examples.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/collect.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass Message {}\n\nclass Ping extends Message {\n    private final int id;\n\n    public Ping(int id) {\n        this.id = id;\n    }\n\n    public int id() {\n        return id;\n    }\n}\n\nclass Pong {\n    private final int id;\n\n    public Pong(int id) {\n        this.id = id;\n    }\n\n    public int id() {\n        return id;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Configuration for SnapshotTestKit\nDESCRIPTION: Configuration snippet for setting up the test environment with SnapshotTestKit plugin\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n\"akka.persistence.snapshot-store.plugin\" = \"akka.persistence.snapshot-store.test\"\n\"akka.persistence.snapshot-store.test\" = \"akka.persistence.testkit.internal.snapshot.TestSnapshotStore\"\n```\n\n----------------------------------------\n\nTITLE: Running MiMa Binary Compatibility Report - Shell\nDESCRIPTION: This snippet shows how to manually trigger the MiMa plugin to perform a binary compatibility check on all Scala versions using an sbt command. The '+mimaReportBinaryIssues' command should be run in a terminal within the Akka project root directory. Prerequisites include the sbt build tool and properly set up MiMa configuration files. The output details any backward incompatibilities introduced in the current branch.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nsbt +mimaReportBinaryIssues\n```\n\n----------------------------------------\n\nTITLE: Referencing Keep Majority Configuration Defaults (Akka Docs Snippet)\nDESCRIPTION: This directive is used in Akka documentation to include the default HOCON configuration settings for the `keep-majority` split-brain resolver strategy. It sources the relevant section (`#keep-majority`) from the `reference.conf` file within the `akka-cluster` module, showing available parameters and their default values.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n@@snip [reference.conf](/akka-cluster/src/main/resources/reference.conf) { #keep-majority }\n```\n\n----------------------------------------\n\nTITLE: Timer-Driven Operators Table in Markdown\nDESCRIPTION: Table documenting timer-based stream operators that handle delaying, dropping, or grouping elements based on time durations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/index.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| |Operator|Description|\n|--|--|--|\n|Source/Flow|<a name=\"delay\"></a>@ref[delay](Source-or-Flow/delay.md)|Delay every element passed through with a specific duration.|\n|Source/Flow|<a name=\"delaywith\"></a>@ref[delayWith](Source-or-Flow/delayWith.md)|Delay every element passed through with a duration that can be controlled dynamically.|\n```\n\n----------------------------------------\n\nTITLE: Project Branch Structure in Markdown\nDESCRIPTION: Lists the main development branches used in the Akka project, showing version control organization.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* `main` – active development branch of Akka 2.9.x\n* `release-2.8` – maintenance branch of Akka 2.8.x\n* similarly `release-2.#` branches contain legacy versions of Akka\n```\n\n----------------------------------------\n\nTITLE: Running All Multi-JVM Tests in Akka Remote\nDESCRIPTION: Command for running all multi-JVM tests in the akka-remote-tests project from the sbt prompt.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_2\n\nLANGUAGE: none\nCODE:\n```\nakka-remote-tests/multi-jvm:test\n```\n\n----------------------------------------\n\nTITLE: Creating BalancingPool from Configuration (Scala/Java)\nDESCRIPTION: Shows how to create a BalancingPool router actor ('router1') using settings defined in the HOCON configuration file under the path '/parent/router1'. Note the limitations of BalancingPool regarding routee state.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_22\n\nLANGUAGE: scala\nCODE:\n```\n//#balancing-pool-1\nimport akka.routing.FromConfig\n\nval router1: ActorRef = context.actorOf(FromConfig.props(Props[Worker]()), \"router1\")\n//#balancing-pool-1\n```\n\nLANGUAGE: java\nCODE:\n```\n//#balancing-pool-1\nimport akka.routing.FromConfig;\n\nfinal ActorRef router1 = getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)),\n    \"router1\");\n//#balancing-pool-1\n```\n\n----------------------------------------\n\nTITLE: Running Akka Cluster Stats Sample with One Master\nDESCRIPTION: Command to run the StatsSampleOneMaster example, which starts 4 actor systems (cluster members) in a single JVM process.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsbt \"runMain sample.cluster.stats.AppOneMaster\"\n```\n\n----------------------------------------\n\nTITLE: Enabling the Local LevelDB Journal Plugin (HOCON)\nDESCRIPTION: Provides the configuration snippet required to enable the local LevelDB journal plugin in Akka by setting the `akka.persistence.journal.plugin` property to `\"akka.persistence.journal.leveldb\"`. Note that this plugin is deprecated and not suitable for cluster environments.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_2\n\nLANGUAGE: hocon\nCODE:\n```\n# Assuming the snippet sets the leveldb journal plugin\nakka.persistence.journal.plugin = \"akka.persistence.journal.leveldb\"\n```\n\n----------------------------------------\n\nTITLE: Setting Akka Default Logger\nDESCRIPTION: This code snippet sets up Akka’s default logger to log to STDOUT with a DEBUG log level. The default logger is not recommended for production use.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_7\n\nLANGUAGE: ruby\nCODE:\n```\nakka {\n  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs\n  # to STDOUT)\n  loggers = [\"akka.event.Logging$DefaultLogger\"]\n  # Options: OFF, ERROR, WARNING, INFO, DEBUG\n  loglevel = \"DEBUG\"\n}\n```\n\n----------------------------------------\n\nTITLE: Source Prematerialization API - Java\nDESCRIPTION: Java API signatures for prematerializing a Source using either a ClassicActorSystemProvider or Materializer.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/preMaterialize.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npreMaterialize(akka.actor.ClassicActorSystemProvider)\npreMaterialize(akka.stream.Materializer)\n```\n\n----------------------------------------\n\nTITLE: Enabling Logging for Actor Message Receives in Scala\nDESCRIPTION: Demonstrates how to enable logging of message invocations on actors using the akka.actor.debug.receive configuration setting and the loggable statement.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_33\n\nLANGUAGE: scala\nCODE:\n```\ndef receive = loggable { case msg =>\n  // handle message\n}\n```\n\n----------------------------------------\n\nTITLE: Sink.lazyInitAsync Scala Signature\nDESCRIPTION: Scala API signature for the lazyInitAsync operator that creates a Sink which initializes lazily upon receiving the first element. Returns a Future[Option[M]] as materialized value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/lazyInitAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nlazyInitAsync[T,M](sinkFactory: () => scala.concurrent.Future[akka.stream.scaladsl.Sink[T,M]]): akka.stream.scaladsl.Sink[T,scala.concurrent.Future[Option[M]]]\n```\n\n----------------------------------------\n\nTITLE: Implementing Scanning Classification Bus in Java\nDESCRIPTION: A Java implementation of an EventBus using Scanning Classification. It demonstrates processing each event by scanning through all subscribers and determining matches based on custom criteria.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\npublic class ScanningBusImpl extends EventBus<String, String, ActorRef>\n    implements ScanningClassification {\n  // is needed for determining matching classifiers and subscribers\n  @Override\n  public boolean matches(String classifier, String event) {\n    // publish String messages to subscribers that handle events starting with\n    // the given classifier\n    return event.startsWith(classifier);\n  }\n\n  // will be invoked for each event for all subscribers\n  @Override\n  public void publish(String event, ActorRef subscriber) {\n    subscriber.tell(event, ActorRef.noSender());\n  }\n\n  // may define the subscriber order which is used throughout the lifetime of the EventBus\n  @Override\n  public int compareSubscribers(ActorRef a, ActorRef b) {\n    return a.compareTo(b);\n  }\n\n  // determines the initial size of the index data structure\n  // used internally (i.e. the expected number of different classifiers)\n  @Override\n  public int mapSize() {\n    return 128;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Buffer Operation Scala API Signature\nDESCRIPTION: Scala API signature for buffer operation in Source and Flow components. Takes buffer size and overflow strategy as parameters.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/buffer.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nbuffer(size: Int, overflowStrategy: akka.stream.OverflowStrategy): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.maybe in Java\nDESCRIPTION: Example showing how to use Source.maybe operator in Java to create a source that emits a single value when a CompletableFuture<Optional<T>> is completed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/maybe.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource<Integer, CompletableFuture<Optional<Integer>>> source = Source.maybe();\nCompletableFuture<Optional<Integer>> future = source.run(materializer);\n// complete with value\nfuture.complete(Optional.of(42));\n// or complete with no value\nfuture.complete(Optional.empty());\n// or fail the future\nfuture.completeExceptionally(new RuntimeException());\n```\n\n----------------------------------------\n\nTITLE: Using Take Operator in Scala\nDESCRIPTION: Demonstrates using the take operator to limit stream elements in Scala. Takes only the first 3 elements from a source of numbers 1 through 10.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/take.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#take\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared LevelDB Store Directory (HOCON - Deprecated)\nDESCRIPTION: Illustrates configuring the file system path for the shared LevelDB store via the `akka.persistence.journal.leveldb-shared.store.dir` property. The default location is a 'journal' directory in the current working directory. This configuration applies to the deprecated shared LevelDB journal.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_8\n\nLANGUAGE: hocon\nCODE:\n```\n# Assuming the snippet configures the shared store directory\nakka.persistence.journal.leveldb-shared.store {\n  # Path to the shared journal directory. This directory is created if it\n  # doesn't exist.\n  dir = \"target/shared\"\n}\n```\n\n----------------------------------------\n\nTITLE: Error Handling Method Reference - Reactive Streams API\nDESCRIPTION: Reference to the onError method signature from the Reactive Streams Subscriber interface, used to demonstrate failure handling patterns.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/stream/stream-design.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nonError(java.lang.Throwable)\n```\n\n----------------------------------------\n\nTITLE: Testing Scanning Classification Bus in Java\nDESCRIPTION: Test code for a Scanning Classification Bus in Java, demonstrating how to subscribe to a prefix pattern and receive all events that match that pattern.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nScanningBusImpl scanningBus = new ScanningBusImpl();\nscanningBus.subscribe(getTestActor(), \"abc\");\nscanningBus.publish(\"abcdef\");\nexpectMsgEquals(\"abcdef\");\n```\n\n----------------------------------------\n\nTITLE: Deprecated Props Creation Patterns in Java\nDESCRIPTION: Shows deprecated approaches to creating Props objects in Java that should be avoided as they can lead to race conditions and break actor encapsulation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n// NOT RECOMMENDED within another actor:\n// encourages to close over enclosing scope\nProps props7 = Props.create(MyActorC.class, () -> new MyActorC(\"arg\"));\n```\n\n----------------------------------------\n\nTITLE: Markdown Note Block for Pull Request Contribution\nDESCRIPTION: A formatted note block in Markdown highlighting the importance of pull request contributions using a Klangian proverb.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/issue-tracking.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@@ note\n\n*A pull request is worth a thousand +1's.* -- Old Klangian Proverb\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Running StatsSample Compute Node on Port 25252 using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.App` main class using Maven's exec plugin in a separate process. This command starts another node of the Akka cluster sample application configured as a 'compute' node listening on port 25252. Used for the multi-process group router example.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.App\" -Dexec.args=\"compute 25252\"\n```\n\n----------------------------------------\n\nTITLE: Programmatic Singleton Lease Settings in Scala\nDESCRIPTION: Example of programmatically configuring lease settings for an Akka Cluster Singleton in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-singleton.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nval leaseSettings = LeaseUsageSettings(\n  leaseImplementation = \"my-lease-implementation\",\n  leaseName = Some(\"custom-name\"),  // defaults to <actor system name>-singleton-<singleton actor path>\n  leaseConfig = system.settings.config.getConfig(\"pekko.coordination.lease.my-lease\"))\nval singletonSettings = ClusterSingletonSettings(system)\n  .withLeaseSettings(Option(leaseSettings))\n```\n\n----------------------------------------\n\nTITLE: Router Supervision Configuration\nDESCRIPTION: Example showing how to configure supervision strategy for router actors\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [RoutingSpec.scala](/akka-actor-tests/src/test/scala/akka/routing/RoutingSpec.scala) { #supervision }\n```\n\n----------------------------------------\n\nTITLE: Implementing Lookup Classification Bus in Scala\nDESCRIPTION: A Scala implementation of an EventBus using Lookup Classification which maps from event type to a set of subscribers. This allows efficient subscriber lookup based on exact event type matching.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nclass LookupBusImpl extends\n  EventBus[MsgEnvelope, String, ActorRef] with LookupClassification {\n  // is used for extracting the classifier from the incoming events\n  override protected def classify(event: MsgEnvelope): String = event.topic\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for the event's classifier\n  override protected def publish(event: MsgEnvelope, subscriber: ActorRef): Unit = {\n    subscriber ! event.payload\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Posting Weather Data using Curl - Bash\nDESCRIPTION: This bash code snippet demonstrates how to interact with the running KillrWeather Akka HTTP server to record weather data for a specific station. It uses the curl command-line tool to send an HTTP POST request to the server with the station ID, specifying JSON data for event time, data type, and measured value. The request requires that the server is running and reachable at the specified port, with correct content type and endpoint path. The expected output is a successful recording of the provided data on the server for the given station.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-scala/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XPOST http://localhost:12553/weather/62 -H \"Content-Type: application/json\" --data '{\"eventTime\": 1579106781, \"dataType\": \"temperature\", \"value\": 10.3}'\n```\n\n----------------------------------------\n\nTITLE: Log Output Showing Message Processing on Single Node (Text)\nDESCRIPTION: This log snippet displays the flow of messages within the single running processor node. It shows a message being consumed from Kafka (identified by `entityId->partition`), forwarded to Akka Cluster Sharding, and then processed by the corresponding sharded entity actor (identified by its path including the shard ID and entity ID). Critically, both the forwarding log and the entity processing log originate from the same Akka system (`akka://KafkaToSharding`), demonstrating local processing.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n[info] [2020-01-16 09:51:38,672] [INFO] [sample.sharding.kafka.UserEventsKafkaProcessor$] [KafkaToSharding-akka.actor.default-dispatcher-26] [akka://KafkaToSharding/user/kafka-event-processor] - entityId->partition 29->45\n[info] [2020-01-16 09:51:38,672] [INFO] [sample.sharding.kafka.UserEventsKafkaProcessor$] [KafkaToSharding-akka.actor.default-dispatcher-26] [akka://KafkaToSharding/user/kafka-event-processor] - Forwarding message for entity 29 to cluster sharding\n[info] [2020-01-16 09:51:38,673] [INFO] [sample.sharding.kafka.UserEvents$] [KafkaToSharding-akka.actor.default-dispatcher-26] [akka://KafkaToSharding/system/sharding/user-processing/75/29] - user 29 purchase cat t-shirt, quantity 0, price 8874\n[info] [2020-01-16 09:51:39,702] [INFO] [sample.sharding.kafka.UserEventsKafkaProcessor$] [KafkaToSharding-akka.actor.default-dispatcher-17] [akka://KafkaToSharding/user/kafka-event-processor] - entityId->partition 60->111\n[info] [2020-01-16 09:51:39,703] [INFO] [sample.sharding.kafka.UserEventsKafkaProcessor$] [KafkaToSharding-akka.actor.default-dispatcher-17] [akka://KafkaToSharding/user/kafka-event-processor] - Forwarding message for entity 60 to cluster sharding\n[info] [2020-01-16 09:51:39,706] [INFO] [sample.sharding.kafka.UserEvents$] [KafkaToSharding-akka.actor.default-dispatcher-17] [akka://KafkaToSharding/system/sharding/user-processing/2/60] - user 60 purchase cat t-shirt, quantity 2, price 9375\n[info] [2020-01-16 09:51:40,732] [INFO] [sample.sharding.kafka.UserEventsKafkaProcessor$] [KafkaToSharding-akka.actor.default-dispatcher-17] [akka://KafkaToSharding/user/kafka-event-processor] - entityId->partition 75->1\n[info] [2020-01-16 09:51:40,732] [INFO] [sample.sharding.kafka.UserEventsKafkaProcessor$] [KafkaToSharding-akka.actor.default-dispatcher-17] [akka://KafkaToSharding/user/kafka-event-processor] - Forwarding message for entity 75 to cluster sharding\n```\n\n----------------------------------------\n\nTITLE: Initializing ClusterSingletonManager in Akka Scala\nDESCRIPTION: This Scala code initializes the ClusterSingletonManager on each node. It is used to manage singleton actors within an Akka cluster. Dependencies include the ClusterSingletonManager from Akka’s cluster-tools module, Props for creating actors, and necessary settings like terminationMessage and system roles. Inputs are the Props of the actor and manager settings, while outputs involve managing the singleton lifecycle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\nsystem.actorOf(\n  ClusterSingletonManager.props(\n    singletonProps = Props[StatsService],\n    terminationMessage = PoisonPill,\n    settings = ClusterSingletonManagerSettings(system).withRole(\"compute\")),\n  name = \"statsService\")\n```\n\n----------------------------------------\n\nTITLE: Flow.lazyInitAsync Scala Signature\nDESCRIPTION: Scala API signature for the Flow variant of lazyInitAsync operator that creates a Flow which initializes lazily upon receiving the first element.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/lazyInitAsync.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nlazyInitAsync[I,O,M](flowFactory: () => scala.concurrent.Future[akka.stream.scaladsl.Flow[I,O,M]]): akka.stream.scaladsl.Flow[I,O,scala.concurrent.Future[Option[M]]]\n```\n\n----------------------------------------\n\nTITLE: Using Sink.cancelled to Immediately Cancel a Stream in Java\nDESCRIPTION: This Java example demonstrates using Sink.cancelled to immediately cancel a stream. A source generating numbers from 1 to 5 is connected to Sink.cancelled, returning NotUsed as the materialized value and preventing any elements from being processed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/cancelled.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.NotUsed;\nimport akka.actor.ActorSystem;\nimport akka.stream.javadsl.Sink;\nimport akka.stream.javadsl.Source;\n\npublic class SinkDocExamples {\n\n  public static void main(String[] args) {\n    ActorSystem system = ActorSystem.create(\"25-sink-cancelled\");\n\n    // #cancelled\n    final NotUsed cancelled = Source.range(1, 5).runWith(Sink.cancelled(), system);\n    // #cancelled\n\n    system.terminate();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Akka Stream Sources in Scala and Java\nDESCRIPTION: Shows how to access the built-in Source classes in both Scala and Java DSLs through their respective packages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/categories/source-operators.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nakka.stream.scaladsl.Source\n```\n\nLANGUAGE: java\nCODE:\n```\nakka.stream.javadsl.Source\n```\n\n----------------------------------------\n\nTITLE: Starting Backend Node for Transformation Cluster (Port 25252)\nDESCRIPTION: Starts a second backend worker node for the `sample.cluster.transformation.App` example using SBT in a separate terminal. It specifies the role 'backend' and port 25252, acting as another seed node.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.transformation.App backend 25252\"\n```\n\n----------------------------------------\n\nTITLE: Processing Aggregated Vote Counts and Replying in Akka Typed Actor (Java)\nDESCRIPTION: Extracts the aggregated vote mappings from the PNCounterMap, retrieves the original requestor from context, and sends a typed Votes message as a reply. Ensures that each Get response is correctly attributed and delivered. Dependencies: Akka Actors, Replicator API. Inputs: open status, GetSuccess<PNCounterMap>. Output: reply to the requestor with computed vote counts.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-java/README.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nprivate void receiveGetSuccess(boolean open, GetSuccess<PNCounterMap> g) {\n  Map<String, BigInteger> result = g.dataValue().getEntries();\n  ActorRef replyTo = (ActorRef) g.getRequest().get();\n  replyTo.tell(new Votes(result, open), self());\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Router with OptimalSizeExploringResizer in Java\nDESCRIPTION: Creates a router pool with an OptimalSizeExploringResizer programmatically in Java. This resizer attempts to find the optimal pool size between default bounds.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_41\n\nLANGUAGE: java\nCODE:\n```\nOptimalSizeExploringResizer resizer = new OptimalSizeExploringResizer();\nint nrOfInstances = 5;\nPool pool = new RoundRobinPool(nrOfInstances).withResizer(resizer);\nActorRef router3 = getContext().actorOf(pool.props(Props.create(Worker.class)), \"router3\");\n```\n\n----------------------------------------\n\nTITLE: Starting the First Processor Node via sbt (Shell)\nDESCRIPTION: This command uses sbt to run the `processor` sub-project, starting the first node of the Akka Cluster application. It requires three arguments: the Akka remoting port (2551, a seed node), the Akka management port (8551), and the gRPC port for the front-end service (8081). This node will join the cluster, connect to Kafka as a consumer, and start processing messages.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"processor / run 2551 8551 8081\"\n```\n\n----------------------------------------\n\nTITLE: Full Kubernetes Deployment Example for Akka with TLS (YAML)\nDESCRIPTION: Presents a comprehensive deployment spec for an Akka cluster node container, including secret volume definition and mounting, management and HTTP ports, health probes, and resource requests. This ties together all prior resource definitions, ensuring Akka picks up and rotates TLS certificates via Kubernetes. Replace image and naming conventions as needed for your own deployments.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    app: appka\\n  name: appka\\n  namespace: appka-1\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: appka\\n  template:\\n    metadata:\\n      labels:\\n        app: appka\\n    spec:\\n      containers:\\n      - name: appka\\n        image: akka-sample-cluster-kubernetes-scala:latest\\n        readinessProbe:\\n          httpGet:\\n            path: /ready\\n            port: management\\n        livenessProbe:\\n          httpGet:\\n            path: /alive\\n            port: management\\n        ports:\\n        - name: management\\n          containerPort: 8558\\n          protocol: TCP\\n        - name: http\\n          containerPort: 8080\\n          protocol: TCP\\n        resources:\\n          limits:\\n            memory: 1024Mi\\n          requests:\\n            cpu: 2\\n            memory: 1024Mi\\n        volumeMounts:\\n        - name: akka-tls\\n          mountPath: /var/run/secrets/akka-tls/rotating-keys-engine\\n      volumes:\\n      - name: akka-tls\\n        secret:\\n          secretName: my-service-akka-tls-certificate\n```\n\n----------------------------------------\n\nTITLE: Integrating SLF4J with Akka Logging\nDESCRIPTION: This configuration snippet sets up SLF4J as the logger in Akka with a DEBUG log level and a specific logging filter. SLF4J requires the SLF4J API jar and a backend like Logback.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_8\n\nLANGUAGE: ruby\nCODE:\n```\nakka {\n  loggers = [\"akka.event.slf4j.Slf4jLogger\"]\n  loglevel = \"DEBUG\"\n  logging-filter = \"akka.event.slf4j.Slf4jLoggingFilter\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Jackson Serializable Marker Interface in Scala\nDESCRIPTION: Example of using predefined marker traits for Jackson serialization in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsealed trait Animal extends JsonSerializable\ncase class Lion(name: String) extends Animal\ncase class Elephant(name: String, age: Int) extends Animal\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Testkit Dependency in sbt\nDESCRIPTION: Declares the `akka-testkit` dependency for an sbt project, using the Akka Bill of Materials (BOM) for consistent version management across Akka modules. The dependency is added to the 'test' scope.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_3\n\nLANGUAGE: sbt\nCODE:\n```\nval AkkaVersion = \"$akka.version$\"\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-testkit\" % AkkaVersion % Test\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Stash Buffer Configuration in Scala\nDESCRIPTION: Example of configuring a custom stash buffer capacity for individual entities in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_46\n\nLANGUAGE: scala\nCODE:\n```\nRefer to file: BasicPersistentBehaviorCompileOnly.scala\n```\n\n----------------------------------------\n\nTITLE: Setting Up a CA Issuer referencing CA Secret with cert-manager (YAML)\nDESCRIPTION: Establishes a namespaced Issuer of type CA within Kubernetes, referencing the CA certificate secret generated earlier ('akka-tls-ca-certificate'). This issuer is responsible for signing per-service certificates within the namespace. Requires the referenced secret to be present and valid.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1\\nkind: Issuer\\nmetadata:\\n  name: akka-tls-ca-issuer\\n  namespace: default\\nspec:\\n  ca:\\n    secretName: akka-tls-ca-certificate\n```\n\n----------------------------------------\n\nTITLE: Importing DeadLetter Classes in Java\nDESCRIPTION: Java imports necessary for working with DeadLetters in Akka. These imports are used to access the appropriate classes for subscribing to and handling dead letter messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nimport akka.actor.ActorRef;\nimport akka.actor.ActorSystem;\nimport akka.actor.DeadLetter;\nimport akka.actor.Props;\n```\n\n----------------------------------------\n\nTITLE: FSM Unhandled Event Handler\nDESCRIPTION: Definition of how to handle unhandled events in the FSM using whenUnhandled.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nwhenUnhandled(stateFunction)\n```\n\n----------------------------------------\n\nTITLE: Defining Dispatcher in Code in Java\nDESCRIPTION: Demonstrates how to programmatically specify a custom dispatcher for an actor in Java using the withDispatcher method on Props.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/dispatchers.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nActorRef myActor = context.actorOf(Props.create(MyActor.class).withDispatcher(\"my-dispatcher\"), \"myactor\");\n```\n\n----------------------------------------\n\nTITLE: Querying Average Temperature using Curl - Bash\nDESCRIPTION: This bash code snippet illustrates how to query the average temperature for a weather station through the KillrWeather Akka HTTP endpoint. The curl command sends an HTTP GET request to the '/weather/{stationId}' endpoint with URL parameters specifying the data type and the computation function (average). The server should be running and have previously recorded data for the station. The response will typically be a JSON object representing the calculated average value or relevant error data.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-scala/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"http://localhost:12553/weather/62?type=temperature&function=average\"\n```\n\n----------------------------------------\n\nTITLE: Java Source Subscriber Code Reference\nDESCRIPTION: Java code reference showing how to obtain a Subscriber using Source.asSubscriber\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/stream/stream-design.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSource.asSubscriber()\n```\n\n----------------------------------------\n\nTITLE: Selecting Actors with Wildcards in Akka\nDESCRIPTION: Shows how to use wildcard patterns in actor selection to broadcast messages to multiple actors matching the pattern.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_21\n\nLANGUAGE: Scala\nCODE:\n```\ncontext.actorSelection(\"/user/services/*/*\")\n```\n\nLANGUAGE: Java\nCODE:\n```\ngetContext().actorSelection(\"/user/services/*/*\");\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Input Buffer Size in Akka Streams (HOCON)\nDESCRIPTION: Sets the default input buffer size for all asynchronous operators within an Akka application using the HOCON configuration format. This value influences the batching strategy used for backpressure across asynchronous boundaries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_8\n\nLANGUAGE: hocon\nCODE:\n```\nakka.stream.materializer.max-input-buffer-size = 16\n```\n\n----------------------------------------\n\nTITLE: Enabling TLS Transport Configuration in Akka Remote\nDESCRIPTION: Basic configuration to enable TLS as the transport protocol for Akka Remote communication.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery {\n  transport = tls-tcp\n}\n```\n\n----------------------------------------\n\nTITLE: Defining State for a Counter Actor (Java)\nDESCRIPTION: Defines the immutable state representation for the counter actor in Java using a class `State`. It holds the current integer `value` of the counter and provides a helper method `increment()`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\npublic static class State {\n  public final int value;\n\n  public State(int value) {\n    this.value = value;\n  }\n\n  State increment(int amount) {\n    return new State(value + amount);\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: LevelDB JNI Dependency Declaration (sbt/Maven/Gradle)\nDESCRIPTION: Shows the necessary dependency declaration for `org.fusesource.leveldbjni:leveldbjni-all:1.8` in sbt, Maven, and Gradle build files. This dependency is required for using LevelDB-based Akka Persistence plugins like the local or shared LevelDB journal.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_3\n\nLANGUAGE: sbt\nCODE:\n```\nlibraryDependencies += \"org.fusesource.leveldbjni\" % \"leveldbjni-all\" % \"1.8\"\n```\n\nLANGUAGE: maven\nCODE:\n```\n<dependency>\n  <groupId>org.fusesource.leveldbjni</groupId>\n  <artifactId>leveldbjni-all</artifactId>\n  <version>1.8</version>\n</dependency>\n```\n\nLANGUAGE: gradle\nCODE:\n```\nimplementation group: 'org.fusesource.leveldbjni', name: 'leveldbjni-all', version: '1.8'\n```\n\n----------------------------------------\n\nTITLE: JVM Options for Node3\nDESCRIPTION: Example of node-specific JVM options file content for SampleMultiJvmNode3, setting remote port and memory options.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_14\n\nLANGUAGE: none\nCODE:\n```\n-Dakka.remote.port=9993 -Xmx256m\n```\n\n----------------------------------------\n\nTITLE: Configuring Frequency Sketch Admission Filter in Akka Cluster Sharding\nDESCRIPTION: Configuration snippet for enabling the frequency sketch admission filter in Akka Cluster Sharding, which determines whether entities should be admitted to the main tracking area based on access frequency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_41\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.sharding.admission.filter = frequency-sketch\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cart Data with Consistency in Java\nDESCRIPTION: Java example of retrieving cart data from distributed storage using ReadMajority consistency. It shows how to handle different response types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nprivate CompletionStage<Cart> getCart(String userId) {\\n  return askGet(new Get<>(DataKey.create(userId), readMajority))\\n      .thenApply(response -> {\\n        if (response instanceof GetSuccess) {\\n          return ((GetSuccess<Cart>) response).get(Cart.empty());\\n        } else if (response instanceof NotFound) {\\n          return Cart.empty();\\n        } else {\\n          throw new IllegalStateException(\\\"Get failed\\\");\\n        }\\n      });\\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Elements in Akka Streams (Scala)\nDESCRIPTION: Demonstrates how to use the filter operator in Akka Streams to select longer words from a source of words. The predicate function checks if the word length is greater than 5.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/filter.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nSource(words)\n  .filter(_.length > 5)\n  .runWith(Sink.foreach(println))\n```\n\n----------------------------------------\n\nTITLE: Unsafe Shutdown Example Using PoisonPill\nDESCRIPTION: Demonstrates the incorrect way of shutting down persistent actors using PoisonPill, which can lead to premature shutdown.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_24\n\nLANGUAGE: scala\nCODE:\n```\n#safe-shutdown-example-bad\n```\n\nLANGUAGE: java\nCODE:\n```\n#safe-shutdown-example-bad\n```\n\n----------------------------------------\n\nTITLE: Importing Multi JVM TestKit Classes in Akka\nDESCRIPTION: Shows the package changes for Direction and ThrottleMode classes when using Multi JVM TestKit. The classes have been moved from the remote transport adapter package to the testkit package.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guide-2.7.x-2.8.x.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nakka.remote.transport.ThrottlerTransportAdapter.*\n```\n\nLANGUAGE: java\nCODE:\n```\nakka.remote.testkit.*\n```\n\n----------------------------------------\n\nTITLE: Markdown Index Structure for Akka Migration Guides\nDESCRIPTION: Markdown structure defining the table of contents and index links for Akka version migration guides. Uses custom Akka documentation syntax with @@@ and @@toc markers.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guides.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@toc { depth=1 }\n\n@@@ index\n\n* [migration-guide-2.9.x-2.10.x](migration-guide-2.9.x-2.10.x.md)\n* [migration-guide-2.8.x-2.9.x](migration-guide-2.8.x-2.9.x.md)\n* [migration-guide-2.7.x-2.8.x](migration-guide-2.7.x-2.8.x.md)\n* [migration-guide-2.6.x-2.7.x](migration-guide-2.6.x-2.7.x.md)\n* [migration-guide-2.5.x-2.6.x](migration-guide-2.5.x-2.6.x.md)\n* [migration-guide-old](migration-guide-old.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Basic Akka Actor Test Setup with TestKit in Java\nDESCRIPTION: Shows a fundamental Java test class setup for testing Akka actors using `akka-testkit`. It utilizes JUnit for test structure (`@Test`, `@AfterClass`), creates an `ActorSystem` and a `TestKit` instance, manages the actor system lifecycle, defines a simple actor, and demonstrates sending/receiving messages via the `testActor` obtained using `getRef()`.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n@@snip [TestKitSampleTest.java](/akka-docs/src/test/java/jdocs/testkit/TestKitSampleTest.java) { #fullsample }\n```\n\n----------------------------------------\n\nTITLE: Enabling Untrusted Mode in Akka Remote Configuration (HOCON)\nDESCRIPTION: This snippet shows how to enable untrusted mode in Akka's remote configuration. It disallows sending of system messages and potentially harmful messages to the remote system.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_10\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery.untrusted-mode = on\n```\n\n----------------------------------------\n\nTITLE: WeatherServer Dynamic Port Log Example - Plaintext\nDESCRIPTION: This plaintext log snippet shows the informational messages output by KillrWeather actor systems on startup, indicating which dynamic HTTP ports the WeatherServer has bound to. These logs are critical for discovering which endpoints clients or fog devices should target for data communication. The log output exemplifies Akka's dispatcher and binding reporting.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-sharding-scala/README.md#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n[2020-01-16 13:44:58,842] [INFO] [] [akka.actor.typed.ActorSystem] [KillrWeather-akka.actor.default-dispatcher-3] [] - WeatherServer online at http://127.0.0.1:12553/\n[2020-01-16 13:44:58,842] [INFO] [] [akka.actor.typed.ActorSystem] [KillrWeather-akka.actor.default-dispatcher-19] [] - WeatherServer online at http://127.0.0.1:53937/\n[2020-01-16 13:44:58,843] [INFO] [] [akka.actor.typed.ActorSystem] [KillrWeather-akka.actor.default-dispatcher-15] [] - WeatherServer online at http://127.0.0.1:12554/\n```\n\n----------------------------------------\n\nTITLE: Verifying Code Style\nDESCRIPTION: Command to verify code style compliance in Akka project.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsbt\nverifyCodeStyle\n```\n\n----------------------------------------\n\nTITLE: Running Tests with JVM Options\nDESCRIPTION: Command for running specific multi-JVM tests with additional JVM options specified after -- separator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_5\n\nLANGUAGE: none\nCODE:\n```\nmulti-jvm:testOnly akka.remote.RandomRoutedRemoteActor -- -Dsome.option=something\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Testkit Dependency in Maven\nDESCRIPTION: Declares the `akka-testkit` dependency for a Maven project using the `pom.xml`. It utilizes the Akka Bill of Materials (BOM) for version management and specifies the 'test' scope.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<properties>\n  <scala.binary.version>$scala.binary.version$</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_${scala.binary.version}</artifactId>\n      <version>$akka.version$</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies>\n```\n\n----------------------------------------\n\nTITLE: Joining a Single Node in Scala\nDESCRIPTION: Example of joining a specific single node in an Akka cluster. This approach is less preferred than joinSeedNodes as it lacks the built-in redundancy and retry mechanisms.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval selfAddress = Cluster(system).selfAddress\nCluster(system).join(selfAddress)\n```\n\n----------------------------------------\n\nTITLE: Lease Configuration Example\nDESCRIPTION: Shows how to configure lease settings including timeout and heartbeat parameters.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordination.md#2025-04-22_snippet_6\n\nLANGUAGE: Config\nCODE:\n```\n#lease-config\n```\n\n----------------------------------------\n\nTITLE: Demonstrating a Deadlocking Cycle in Akka Streams (Scala)\nDESCRIPTION: An example showing a naïve cycle implementation in Akka Streams that will deadlock. The graph broadcasts elements to both a consumer and a feedback arc that merges back into the main stream, causing an element build-up that eventually backpressures the source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_21\n\nLANGUAGE: Scala\nCODE:\n```\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n  val merge = b.add(Merge[Int](2))\n  val bcast = b.add(Broadcast[Int](2))\n\n  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore\n           merge <~ bcast\n\n  ClosedShape\n})\n```\n\n----------------------------------------\n\nTITLE: Enabling Configuration Compatibility Checks in Akka Cluster\nDESCRIPTION: Configures Akka Cluster to enable configuration compatibility checks when new nodes join. This is the second step in the approach for migrating Cluster Sharding from Classic to Typed Actors during a rolling update.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/rolling-updates.md#2025-04-22_snippet_2\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.configuration-compatibility-check.enforce-on-join = on\n```\n\n----------------------------------------\n\nTITLE: EventsByTag Query Implementation in Scala\nDESCRIPTION: Demonstrates how to query events that were tagged with a specific tag using an offset.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query-leveldb.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval events = readJournal.eventsByTag(\"bid\", NoOffset)\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Projection Dependency\nDESCRIPTION: Dependency configuration for including Akka Projection Core module in build systems.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_8\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-projection-core_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Usage vs Latency in Akka\nDESCRIPTION: HOCON configuration for tuning the tradeoff between CPU usage and latency when using Aeron transport.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_17\n\nLANGUAGE: hocon\nCODE:\n```\n# Values can be from 1 to 10, where 10 strongly prefers low latency\n# and 1 strongly prefers less CPU usage\nakka.remote.artery.advanced.aeron.idle-cpu-level = 1\n```\n\n----------------------------------------\n\nTITLE: Running Coordinated Shutdown in Java\nDESCRIPTION: Shows how to manually trigger the coordinated shutdown process in Java. The runAll method takes a reason parameter and returns a CompletionStage that completes when the shutdown process is finished.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordinated-shutdown.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nCoordinatedShutdown.get(system).runAll(new UnknownReason());\n```\n\n----------------------------------------\n\nTITLE: Setting the CallingThreadDispatcher for Akka Actors - Scala\nDESCRIPTION: Demonstrates how to configure an Akka actor in Scala to use the CallingThreadDispatcher. This dispatcher runs actions on the calling thread, enabling deterministic execution during testing. Set the dispatcher in the actor's Props configuration; Akka Actors required.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_30\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [TestkitDocSpec.scala](/akka-docs/src/test/scala/docs/testkit/TestkitDocSpec.scala) { #calling-thread-dispatcher }\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Stream Dependencies in Scala\nDESCRIPTION: This snippet shows how to configure Akka library dependencies for sbt, Maven, or Gradle, including the Akka Stream module, essential for utilizing Akka Streams in a project.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-refs.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\ndependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=\"com.typesafe.akka\"\n  artifact=\"akka-stream_$scala.binary.version$\"\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents in Markdown\nDESCRIPTION: This snippet uses Markdown syntax to generate a table of contents with a depth of 2 levels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/index-persistence-durable-state.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@toc { depth=2 }\n```\n\n----------------------------------------\n\nTITLE: Shared LevelDB Store Configuration for Akka Persistence\nDESCRIPTION: Configuration for using a shared LevelDB Java implementation with Akka Persistence. This is used when multiple actor systems need to share the same persistence store.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_39\n\nLANGUAGE: Scala\nCODE:\n```\nakka.persistence.journal.leveldb-shared {\n  store {\n    native = off // switch to using the Java port of leveldb\n    dir = \"target/shared-journal-native-java\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Message Forwarding in Akka Actors - Scala\nDESCRIPTION: Shows how to forward messages between actors while preserving the original sender reference in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_30\n\nLANGUAGE: scala\nCODE:\n```\n#forward\n```\n\n----------------------------------------\n\nTITLE: Log Output Showing Akka Shard Rebalancing (Text)\nDESCRIPTION: This log snippet originates from the Akka Cluster Sharding coordinator on one of the nodes (in this case, the first node running on port 2551). It indicates that a shard rebalancing process is being initiated for several shards (e.g., 45, 33, 16). This rebalancing is triggered by the external shard allocation strategy, which detects changes in Kafka partition assignments and moves shards to the nodes consuming the corresponding partitions.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n[info] [2020-01-16 09:59:39,923] [INFO] [akka://KafkaToSharding@127.0.0.1:2551/system/sharding/user-processingCoordinator/singleton/coordinator] - Starting rebalance for shards [45,33,16,2,3,15,11,6,36]. Current shards rebalancing: []\n```\n\n----------------------------------------\n\nTITLE: Checking Binary Compatibility with sbt and MiMa - Shell\nDESCRIPTION: This snippet demonstrates how to use sbt to check for binary compatibility issues in the Akka project. It invokes the MiMa plugin's report command, which examines discrepancies in the project's public API against previous versions. A successful run returns compatibility status and any problems. This command requires sbt and the appropriate MiMa configuration. The output example includes info and error messages identifying compatibility issues.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n[info] akka-stream: found 1 potential binary incompatibilities while checking against com.typesafe.akka:akka-stream_2.12:2.4.2  (filtered 222)\\n[error]  * method foldAsync(java.lang.Object,scala.Function2)akka.stream.scaladsl.FlowOps in trait akka.stream.scaladsl.FlowOps is present only in current version\\n[error]    filter with: ProblemFilters.exclude[ReversedMissingMethodProblem](\\\"akka.stream.scaladsl.FlowOps.foldAsync\\\")\n```\n\n----------------------------------------\n\nTITLE: Running Akka Benchmarks Using Interactive SBT Shell\nDESCRIPTION: Instructions for running Akka microbenchmarks using the SBT interactive shell. The command runs the ActorCreationBenchmark with 3 iterations, 3 warmup iterations, and 1 fork.\nSOURCE: https://github.com/akka/akka/blob/main/akka-bench-jmh/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsbt shell\nakka > project akka-bench-jmh\nsbt:akka-bench-jmh> jmh:run -i 3 -wi 3 -f 1 .*ActorCreationBenchmark\n```\n\n----------------------------------------\n\nTITLE: Creating Main Application Entry Point in Java\nDESCRIPTION: Defines the main entry point for the Java IoT application. It creates the ActorSystem and starts the IotSupervisor actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_2.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic class IotMain {\n  public static void main(String[] args) {\n    // Create ActorSystem and top level supervisor\n    ActorSystem.create(IotSupervisor.create(), \"iot-system\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Cluster Sharding Health Check in Ruby\nDESCRIPTION: Configuration snippet to disable the Akka Management compatible health check for Cluster Sharding. This health check returns healthy once the local shard region has registered with the coordinator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_49\n\nLANGUAGE: ruby\nCODE:\n```\nakka.management.health-checks.readiness-checks {\n  sharding = \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Java Source.keepAlive API Signature\nDESCRIPTION: Java API signature for the keepAlive operator on Source, taking Duration and Creator parameters\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/keepAlive.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nkeepAlive(java.time.Duration,akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Issue Tags Structure in Markdown\nDESCRIPTION: Documents the tag categorization system used for Akka issues, including topic prefixes and development phases.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n- [t:core](https://github.com/akka/akka/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aopen%20label%3At%3Acore)\n- [t:stream](https://github.com/akka/akka/issues?q=is%3Aissue+is%3Aopen+label%3At%3Astream)\n- see [all tags here](https://github.com/akka/akka/labels)\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Testkit Dependency in Gradle\nDESCRIPTION: Declares the `akka-testkit` dependency for a Gradle project within the build script. It leverages the Akka Bill of Materials (BOM) for version consistency and assigns the dependency to the 'testImplementation' configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_5\n\nLANGUAGE: gradle\nCODE:\n```\next { // or save in gradle.properties\n  ScalaBinaryVersion = \"$scala.binary.version$\"\n}\ndef AkkaVersion = \"$akka.version$\"\ndependencies {\n  implementation platform(\"com.typesafe.akka:akka-bom_${ScalaBinaryVersion}:${AkkaVersion}\")\n\n  testImplementation \"com.typesafe.akka:akka-testkit_${ScalaBinaryVersion}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Tweet Authors using Akka Streams in Scala\nDESCRIPTION: This snippet demonstrates how to extract a stream of authors from tweets using Akka Streams in Scala. It serves as an initial step before performing asynchronous operations like email lookup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_19\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [IntegrationDocSpec.scala](/akka-docs/src/test/scala/docs/stream/IntegrationDocSpec.scala) { #tweet-authors }\n```\n\n----------------------------------------\n\nTITLE: Running the Dining Hakkers Sample with sbt (Shell)\nDESCRIPTION: This shell command utilizes sbt (Simple Build Tool) to compile and run the main class `sample.DiningHakkers` within the Akka FSM sample project. Executing this command starts the 'Dining Hakkers' simulation, allowing observation of the Hakker actors' state transitions and actions in the log output.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-fsm-scala/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nsbt \"runMain sample.DiningHakkers\"\n```\n\n----------------------------------------\n\nTITLE: Scala alsoTo API Signature\nDESCRIPTION: Method signature for the alsoTo operator in Scala, showing how to attach a Sink to a Flow or Source\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/alsoTo.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nalsoTo(that: akka.stream.Graph[akka.stream.SinkShape[Out],_]): FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Sink.queue API Signature\nDESCRIPTION: API signature for creating a SinkQueue with configurable concurrent pulls. Returns a Sink that materializes to a SinkQueueWithCancel.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/queue.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSink.queue[T](maxConcurrentPulls: Int): Sink[T, SinkQueueWithCancel[T]]\n```\n\nLANGUAGE: java\nCODE:\n```\nSink.queue(int maxConcurrentPulls)\n```\n\n----------------------------------------\n\nTITLE: Java Props Constructor Example\nDESCRIPTION: Example of defining Props for classic actors using both reflection-based and lambda factory approaches in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/native-image.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nProps.create(T.getClass, ...)\nProps.create(T.getClass, () -> new T())\n```\n\n----------------------------------------\n\nTITLE: Selecting a Mailbox Type for an Actor in Java\nDESCRIPTION: Demonstrates how to select a specific mailbox for an actor using MailboxSelector to create a Props instance for actor spawning in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/mailboxes.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nActorRef<Command> boundedMailbox =\n    context.spawn(\n        behavior,\n        \"bounded-mailbox-actor\",\n        // create a bounded mailbox with a capacity of 100 messages and a default overflow strategy\n        MailboxSelector.bounded(100));\n\nActorRef<Command> unboundedMailbox =\n    context.spawn(\n        behavior,\n        \"unbounded-mailbox-actor\",\n        // default is an unbounded mailbox, same as not specifying\n        MailboxSelector.unbounded());\n\nActorRef<Command> fromConfig =\n    context.spawn(\n        behavior,\n        \"from-config-mailbox-actor\",\n        // select the mailbox-type from config\n        MailboxSelector.fromConfig(\"my-app.my-special-mailbox\"));\n```\n\n----------------------------------------\n\nTITLE: Serializer Configuration in Java\nDESCRIPTION: Configuration code for registering the TwoPhaseSet serializer when using Java, showing how to bind the serializer to a specific data type.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_32\n\nLANGUAGE: scala\nCODE:\n```\nConfig config = ConfigFactory.parseString(\"\"\"\n    akka.actor {\n      serializers {\n        twophaseset = \"jdocs.ddata.protobuf.TwoPhaseSetSerializer\"\n      }\n      serialization-bindings {\n        \"jdocs.ddata.TwoPhaseSet\" = twophaseset\n      }\n    }\n    \"\"\");\n```\n\n----------------------------------------\n\nTITLE: Running Compute Nodes in Separate Processes\nDESCRIPTION: Commands to run different components of the cluster in separate terminal windows for a more realistic deployment scenario.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nsbt \"runMain sample.cluster.stats.AppOneMaster compute 25251\"\n\nsbt \"runMain sample.cluster.stats.AppOneMaster compute 25252\"\n\nsbt \"runMain sample.cluster.stats.AppOneMaster compute 0\"\n\nsbt \"runMain sample.cluster.stats.AppOneMaster client 0\"\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Dependencies\nDESCRIPTION: Dependency configuration block for adding Akka Utilities to your project, including both the main actor module and testkit dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-utilities-classic.md#2025-04-22_snippet_1\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=\"com.typesafe.akka\"\n  artifact=\"akka-actor_$scala.binary.version$\"\n  version=AkkaVersion\n  group2=\"com.typesafe.akka\"\n  artifact2=\"akka-testkit_$scala.binary.version$\"\n  scope2=test\n  version2=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Large Message Channel Paths in Akka\nDESCRIPTION: HOCON configuration for specifying actor paths that should use the dedicated large message channel. Supports exact paths and wildcard patterns.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_12\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery.large-message-destinations = [\n   \"/user/largeMessageActor\",\n   \"/user/largeMessagesGroup/*\",\n   \"/user/anotherGroup/*/largeMesssages\",\n   \"/user/thirdGroup/**\",\n   \"/temp/session-ask-actor*\"\n]\n```\n\n----------------------------------------\n\nTITLE: Converting Between Java IO Streams using Akka Streams - Java\nDESCRIPTION: Example demonstrating how to read from InputStream, uppercase the content, and write to OutputStream using Akka Streams in Java. Uses StreamConverters.fromInputStream and StreamConverters.fromOutputStream to handle stream conversion.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/StreamConverters/fromOutputStream.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#tofromJavaIOStream\n```\n\n----------------------------------------\n\nTITLE: Querying Live Persistence IDs Stream in Java\nDESCRIPTION: Shows how to subscribe to a live stream of all persistence IDs in the system using the persistenceIds query in Java. This stream will continuously emit new persistence IDs as they are created.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nreadJournal.getPersistenceIds().runForeach(id -> {\n  System.out.println(\"We have persistence id: \" + id);\n}, system);\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency in Gradle\nDESCRIPTION: Declares the Akka Streams library dependency in a Gradle build file, leveraging the Akka Bill of Materials (BOM) for coordinated versioning.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_5\n\nLANGUAGE: gradle\nCODE:\n```\ndef versions = [\n  AkkaVersion: \"$akka.version$\",\n  ScalaBinary: \"$scala.binary.version$\"\n]\n\ndependencies {\n  implementation platform(\"com.typesafe.akka:akka-bom_${versions.ScalaBinary}:${versions.AkkaVersion}\")\n\n  implementation \"com.typesafe.akka:akka-stream_${versions.ScalaBinary}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Cluster Owner Configuration in Java\nDESCRIPTION: Shows how to configure cluster-specific lease ownership in Java using cluster host port.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/coordination.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n#cluster-owner\n```\n\n----------------------------------------\n\nTITLE: Querying Cluster Shards Distribution\nDESCRIPTION: Shell commands to check the number of shards on each node in the cluster using curl and jq.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl -s localhost:8551/cluster/shards/user-processing | jq -r \".\" | grep shardId | wc -l\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -s localhost:8552/cluster/shards/user-processing | jq -r \".\" | grep shardId | wc -l\n```\n\n----------------------------------------\n\nTITLE: Running StatsSample Compute Node on Port 25251 using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.App` main class using Maven's exec plugin in a separate process. This command starts one node of the Akka cluster sample application configured as a 'compute' node listening on port 25251. Used for the multi-process group router example.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.App\" -Dexec.args=\"compute 25251\"\n```\n\n----------------------------------------\n\nTITLE: Using detach() in Akka Streams Source (Scala)\nDESCRIPTION: Applies the detach operator to a Source in Akka Streams using Scala. This separates upstream demand from downstream demand without affecting stream rates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/detach.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.detach\n```\n\n----------------------------------------\n\nTITLE: Configuring Event Adapter for Migrating Remembered Entities in Akka Cluster Sharding\nDESCRIPTION: Configuration example for setting up an event adapter to migrate existing remembered entities from the old persistence mode to the new event sourced mode. This example uses Cassandra as the journal.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_48\n\nLANGUAGE: conf\nCODE:\n```\nakka.persistence.cassandra.journal {\n  event-adapters {\n    coordinator-migration = \"akka.cluster.sharding.OldCoordinatorStateMigrationEventAdapter\"\n  }\n\n  event-adapter-bindings {\n    \"akka.cluster.sharding.ShardCoordinator$Internal$DomainEvent\" = coordinator-migration\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Jackson Serializable Marker Interface in Java\nDESCRIPTION: Example of using predefined marker interfaces for Jackson serialization in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic interface MySerializable extends JsonSerializable {}\n```\n\n----------------------------------------\n\nTITLE: Using Sink.headOption with an Empty Source in Java\nDESCRIPTION: Example of using Sink.headOption with an empty source in Java, which handles the case when no elements are emitted by completing with an empty Optional.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/headOption.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n@@snip [SinkDocExamples.java](/akka-docs/src/test/java/jdocs/stream/operators/SinkDocExamples.java) { #headoption }\n```\n\n----------------------------------------\n\nTITLE: Configuring Journal Plugin\nDESCRIPTION: Minimal configuration required to activate a custom journal plugin in Akka Persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-journals.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nyour-journal {\n  class = \"docs.persistence.MyJournal\"\n  plugin-dispatcher = \"akka.actor.default-dispatcher\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Stats Worker in Java\nDESCRIPTION: Java implementation of a worker actor that counts the characters in words and returns the result, used as a routee in the cluster router example.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\npublic class StatsWorker extends AbstractActor {\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n      .match(StatsMessages.ProcessWord.class, message -> {\n        String word = message.getWord();\n        // Very naive implementation\n        getSender().tell(\n          new StatsMessages.WordCount(word, word.length()), getSelf());\n      })\n      .build();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto-Import in IntelliJ and Eclipse for Akka Development\nDESCRIPTION: Advises developers to disable auto-import suggestions that mix javadsl and scaladsl imports when working with Akka. For IntelliJ, the settings are under \"Editor/General/Auto Import\" and for Eclipse under \"Window/Preferences/Java/Appearance/Type Filters\".\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/ide.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nakka.stream.javadsl*\nakka.stream.scaladsl*\n*javadsl*\n*scaladsl*\n```\n\n----------------------------------------\n\nTITLE: Java Flow.delayWith Signature\nDESCRIPTION: Java API signature for delayWith operator in Akka Streams Flow, using Java's Supplier interface.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/delayWith.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\ndelayWith(java.util.function.Supplier,akka.stream.DelayOverflowStrategy)\n```\n\n----------------------------------------\n\nTITLE: Scala Enumeration Serialization\nDESCRIPTION: Example of serializing Scala Enumerations with Jackson\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nobject Weekday extends Enumeration {\n  type Weekday = Value\n  val Mon, Tue, Wed, Thu, Fri, Sat, Sun = Value\n}\n\nclass WeekdayType {\n  @JsonScalaEnumeration(classOf[WeekdayType])\n  var weekday: Weekday.Weekday = _\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring sbt to Exclude Specific Test Tags - shell\nDESCRIPTION: This shell snippet demonstrates how to launch sbt with options that exclude particular categories of tests (such as performance, timing, and long-running tests) from the test suite. It also disables running multi-node tests inline. Requires sbt installed and proper project structure. Key parameters: 'akka.test.tags.exclude' (comma-separated list of tags to exclude) and 'akka.test.multi-in-test' (boolean flag). Input is given as JVM system properties; output is a filtered test run without the excluded tests.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nsbt -Dakka.test.tags.exclude=performance,timing,long-running -Dakka.test.multi-in-test=false\n```\n\n----------------------------------------\n\nTITLE: Demonstrating a Deadlocking Cycle in Akka Streams (Java)\nDESCRIPTION: A Java implementation showing a naïve cycle in Akka Streams that will deadlock. The graph broadcasts elements to both a consumer and a feedback arc that merges back into the main stream, causing an element build-up that eventually backpressures the source.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-graphs.md#2025-04-22_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\nfinal RunnableGraph<NotUsed> deadlocking =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final UniformFanInShape<Integer, Integer> merge = b.add(Merge.create(2));\n              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n\n              b.from(b.add(source))\n                  .viaFanIn(merge)\n                  .via(b.add(Flow.of(Integer.class).map(s -> { System.out.println(s); return s; })))\n                  .viaFanOut(bcast)\n                  .to(b.add(Sink.ignore()));\n              b.from(bcast).toFanIn(merge);\n              return ClosedShape.getInstance();\n            }));\n```\n\n----------------------------------------\n\nTITLE: Generating ECDSA Key and Converting to PKCS#8 with OpenSSL (Shell)\nDESCRIPTION: This shell script uses OpenSSL to first generate an Elliptic Curve private key (`ecdsa.pem`) based on the `secp256r1` curve. It then converts this key into the unencrypted PKCS#8 format (`pkcs8-ecdsa.pem`) suitable for testing environments. Requires OpenSSL to be installed.\nSOURCE: https://github.com/akka/akka/blob/main/akka-pki/src/test/resources/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Eliptic curve key\n$ openssl ecparam -out ecdsa.pem -name secp256r1 -genkey\n$ openssl pkcs8 -topk8 -nocrypt -in ecdsa.pem -out pkcs8-ecdsa.pem \n```\n\n----------------------------------------\n\nTITLE: Validating Pull Request Changes\nDESCRIPTION: Example output of validatePullRequest task showing changed modules detection.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n> validatePullRequest\n[info] Diffing [HEAD] to determine changed modules in PR...\n[info] Detected uncomitted changes in directories (including in dependency analysis): [akka-protobuf,project]\n[info] Detected changes in directories: [akka-actor-tests, project, akka-stream, akka-docs, akka-persistence]\n```\n\n----------------------------------------\n\nTITLE: Wrapping DurableStateBehavior with ActorContext Setup for Logging (Scala)\nDESCRIPTION: Shows how to wrap a DurableStateBehavior in Behaviors.setup in Scala, granting access to ActorContext features such as logging for debugging command handlers. Requires Akka Typed and access to the Scala DSL for composing behaviors. Inputs are the actor context and durable state handler; output is a context-enriched actor behavior. The snippet demonstrates clean separation of concerns and is useful for tracing and debugging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_31\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [DurableStatePersistentActorCompileOnly.scala](/akka-persistence-typed/src/test/scala/docs/akka/persistence/typed/DurableStatePersistentBehaviorCompileOnly.scala) { #wrapPersistentBehavior }\n```\n\n----------------------------------------\n\nTITLE: Java alsoToAll Signature\nDESCRIPTION: Method signature for alsoToAll operator in Java that accepts variable number of Graph arguments\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/alsoToAll.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nalsoToAll(akka.stream.Graph*)\n```\n\n----------------------------------------\n\nTITLE: Source Queue Implementation in Java\nDESCRIPTION: Java implementation of Source.queue showing how to offer elements to a stream with backpressure and throttling.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/actor-interop.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nfinal SourceQueue<Integer> queue =\n    Source.<Integer>queue(10, OverflowStrategy.backpressure())\n        .throttle(5, Duration.ofSeconds(3))\n        .to(Sink.foreach(System.out::println))\n        .run(materializer);\n\n// push element to queue\nCompletionStage<QueueOfferResult> result = queue.offer(1);\n```\n\n----------------------------------------\n\nTITLE: Stash Capacity Configuration\nDESCRIPTION: Configuration setting for the maximum capacity of the stash buffer in Akka Persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_45\n\nLANGUAGE: conf\nCODE:\n```\nakka.persistence.typed.stash-capacity = 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Logging to OFF\nDESCRIPTION: This Ruby configuration snippet demonstrates how to set Akka's logging levels to OFF during startup and shutdown. This ensures no log output during these phases.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_6\n\nLANGUAGE: ruby\nCODE:\n```\nakka {\n  stdout-loglevel = \"OFF\"\n  loglevel = \"OFF\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive Streams Publishers with Sink.asPublisher in Scala\nDESCRIPTION: Demonstrates how to create a Publisher from a Sink in Scala, showing the difference between fanout=true (allowing multiple subscribers) and fanout=false (allowing only the first subscriber).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/asPublisher.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval source = Source(1 to 5)\n\n// When fanout is set to true, multiple subscribers can attach to this publisher\nval (publisherWithFanout, notUsed) = source.toMat(Sink.asPublisher(fanout = true))(Keep.both).run()\n\npublisherWithFanout.subscribe(createSubscriber(\"WithFanout-A\"))\npublisherWithFanout.subscribe(createSubscriber(\"WithFanout-B\"))\npublisherWithFanout.subscribe(createSubscriber(\"WithFanout-C\"))\n\n// Await for verification\n// expect 5 elements for each of the three subscribers\n\n// When fanout is set to false, the first subscriber will trigger publisher,\n// and the rest will be rejected\nval (publisherWithoutFanout, notUsed2) = source.toMat(Sink.asPublisher(fanout = false))(Keep.both).run()\n\npublisherWithoutFanout.subscribe(createSubscriber(\"WithoutFanout-A\"))\npublisherWithoutFanout.subscribe(createSubscriber(\"WithoutFanout-B\")) // will receive onError\npublisherWithoutFanout.subscribe(createSubscriber(\"WithoutFanout-C\")) // will receive onError\n\n// Await for verification\n// expect 5 elements for subscriber \"WithoutFanout-A\", but error for \"WithoutFanout-B/C\"\n```\n\n----------------------------------------\n\nTITLE: Compacting ByteStrings in a Stream in Java\nDESCRIPTION: Uses a simple map operation to call the compact() method on ByteString elements, creating clean copies that no longer reference original ByteStrings. This should be the last element in a long chain due to copying of underlying arrays.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_54\n\nLANGUAGE: Java\nCODE:\n```\nFlow<ByteString, ByteString, NotUsed> compactFlow =\n    Flow.of(ByteString.class).map(ByteString::compact);\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Imports for FSM in Scala\nDESCRIPTION: Required imports for implementing FSM functionality in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fsm.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#simple-imports\n```\n\n----------------------------------------\n\nTITLE: Creating Shared LevelDB Store Actor (Scala - Deprecated)\nDESCRIPTION: Demonstrates the Scala code needed to create an instance of the `SharedLeveldbStore` actor using `system.actorOf`. This actor serves as the central, shared LevelDB instance for the deprecated shared LevelDB journal plugin.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n// Assuming the snippet creates the SharedLeveldbStore actor\nimport akka.persistence.shared.SharedLeveldbStore\nimport akka.actor.{ ActorSystem, Props }\n\nval system: ActorSystem = ???\nval store = system.actorOf(Props[SharedLeveldbStore](), \"store\")\n```\n\n----------------------------------------\n\nTITLE: Describing Reactive Streams Semantics for Flow.lazyCompletionStageFlow\nDESCRIPTION: This snippet outlines the reactive streams semantics of the Flow.lazyCompletionStageFlow operator. It describes when the operator emits, backpressures, and completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/lazyCompletionStageFlow.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Reactive Streams semantics\n\n@@@div { .callout }\n\n**emits** when the internal flow is successfully created and it emits\n\n**backpressures** when the internal flow is successfully created and it backpressures\n\n**completes** when upstream completes and all elements have been emitted from the internal flow\n\n**completes** when upstream completes and all futures have been completed and all elements have been emitted\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Configuring Jackson Serialization in Akka\nDESCRIPTION: HOCON configuration for enabling Jackson serialization while maintaining backwards compatibility with Java serialization during migration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/rolling-updates.md#2025-04-22_snippet_3\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.allow-java-serialization=on\nakka.serialization.jackson.allowed-class-prefix=[\"com.myapp\"]\nakka.actor.serialization-bindings\n```\n\n----------------------------------------\n\nTITLE: Changing Multi-JVM Source Directory\nDESCRIPTION: sbt configuration for changing the default source directory for multi-JVM test files.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_10\n\nLANGUAGE: none\nCODE:\n```\nunmanagedSourceDirectories in MultiJvm :=\n   Seq(baseDirectory(_ / \"src/some_directory_here\")).join.value\n```\n\n----------------------------------------\n\nTITLE: Formatting Scala Source Code\nDESCRIPTION: SBT commands to format Scala source code in specific Akka modules.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nsbt\nakka-cluster/scalafmtAll\nakka-persistence/scalafmtAll\n```\n\n----------------------------------------\n\nTITLE: Executing Akka Java Project Using Gradle Wrapper - Bash\nDESCRIPTION: This snippet provides the Gradle wrapper command for running the Akka Hello World Java application. It assumes that the project includes a gradlew script and a correct build.gradle setup specifying the main class and dependencies. Upon execution, Gradle compiles the source code if necessary and starts the main application. Input: none, Output: application runs from main class. This is targeted at users leveraging Gradle for Java project management.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-quickstart-java/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew run\n```\n\n----------------------------------------\n\nTITLE: Starting Subscriber Actors in Java\nDESCRIPTION: Java implementation showing how to start subscriber actors on multiple nodes in an Akka cluster. All these subscribers will receive messages published to the 'content' topic.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// start a subscriber on each node\nsystem.actorOf(Props.create(Subscriber.class), \"subscriber1\");\nsystem2.actorOf(Props.create(Subscriber.class), \"subscriber2\");\nsystem3.actorOf(Props.create(Subscriber.class), \"subscriber3\");\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Router in Configuration File\nDESCRIPTION: Shows how to define the custom RedundancyGroup router in the Akka configuration. This allows the router to be created from config without programmatic setup.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_46\n\nLANGUAGE: Scala\nCODE:\n```\nakka.actor.deployment {\n  /redundancy2 {\n    router = \"docs.routing.RedundancyGroup\"\n    routees.paths = [\"/user/a1\", \"/user/a2\", \"/user/a3\"]\n    nbr-copies = 5\n  }\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nakka.actor.deployment {\n  /redundancy2 {\n    router = \"jdocs.routing.RedundancyGroup\"\n    routees.paths = [\"/user/a1\", \"/user/a2\", \"/user/a3\"]\n    nbr-copies = 5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Failure Counting in Akka CircuitBreaker (Java)\nDESCRIPTION: This Java example shows how to define custom failure criteria for a circuit breaker by treating even numbers as failures, regardless of whether the operation succeeds or fails.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/circuitbreaker.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n// We define a function that will determine whether to count the result as a failure.\n// In this example, we treat even numbers as failure, regardless whether\n// the operation succeeded or not\nBiFunction<Optional<Object>, Optional<Throwable>, Boolean> defineEvenNumberAsFailure =\n    (result, err) -> {\n      if (err.isPresent()) return true;\n      if (!result.isPresent()) return false;\n      if (result.get() instanceof Integer) {\n        return ((Integer) result.get()) % 2 == 0;\n      }\n      return false;\n    };\n\nCircuitBreaker circuitBreaker =\n    CircuitBreaker.create(\n        system.scheduler(),\n        1,\n        Duration.ofSeconds(1),\n        Duration.ofSeconds(1));\n\n// Protect the even-as-failure logic\nList<Integer> oddAndEvenNumbers = Arrays.asList(1, 2, 3, 4, 5);\n\nList<CompletionStage<Integer>> futures =\n    oddAndEvenNumbers.stream()\n        .map(\n            n ->\n                circuitBreaker.callWithCircuitBreaker(\n                    () -> {\n                      try {\n                        Thread.sleep(10);\n                        return n;\n                      } catch (InterruptedException e) {\n                        throw new CompletionException(e);\n                      }\n                    },\n                    defineEvenNumberAsFailure))\n        .collect(Collectors.toList());\n```\n\n----------------------------------------\n\nTITLE: Generating Fibonacci Sequence with Source.unfold in Scala\nDESCRIPTION: Implements an infinite Fibonacci sequence generator using Source.unfold. The state maintains the last two numbers in the sequence to calculate the next value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfold.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval fibonacciSource = Source.unfold((0, 1)) { case (n1, n2) =>\n  Some((n2, n2 + n1), n1)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing TakeWhile Operator in Java\nDESCRIPTION: Example showing the usage of takeWhile operator in Java to filter stream elements based on a predicate condition. The operator continues passing elements until the predicate evaluates to false.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/takeWhile.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#take-while\n```\n\n----------------------------------------\n\nTITLE: Combining Tick with ZipLatest in Scala\nDESCRIPTION: Shows how to combine Source.tick with zipLatest to update values periodically in a stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/tick.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\ndef streamWithUpdatingValue[T](actor: ActorRef[Query]): Source[(T, Response), NotUsed] =\n  val responseStream = Source\n    .tick(0.seconds, 1.second, \"tick\")\n    .mapAsync(1)(_ => actor.ask(Query(_)))\n  elements.zipLatest(responseStream)\n```\n\n----------------------------------------\n\nTITLE: Changing Multi-JVM Marker\nDESCRIPTION: sbt configuration for changing the default 'MultiJvm' identifier to a custom marker like 'ClusterTest'.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_11\n\nLANGUAGE: none\nCODE:\n```\nmultiJvmMarker in MultiJvm := \"ClusterTest\"\n```\n\n----------------------------------------\n\nTITLE: Durable Storage Configuration\nDESCRIPTION: Configuration example for enabling durable storage in Akka Distributed Data, showing how to specify which keys should be stored persistently.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_38\n\nLANGUAGE: text\nCODE:\n```\nakka.cluster.distributed-data.durable.keys = [\"a\", \"b\", \"durable*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in sbt\nDESCRIPTION: Specifies the Akka library repository URL in an sbt build configuration to enable fetching Akka dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\n----------------------------------------\n\nTITLE: Building Documentation Locally with sbt in Shell\nDESCRIPTION: This shell command builds the Akka documentation locally using sbt and opens it in a browser for review.\nSOURCE: https://github.com/akka/akka/blob/main/RELEASING.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsbt akka-docs/paradoxBrowse\n```\n\n----------------------------------------\n\nTITLE: Deploying Actor with Custom Mailbox via Configuration\nDESCRIPTION: HOCON configuration for deploying an actor with a specific mailbox type. This allows associating mailbox implementations with actors through deployment configuration rather than code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_9\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.deployment {\n  /priomailboxactor {\n    mailbox = prio-mailbox\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: ConsistentHashingRouter Configuration Example\nDESCRIPTION: HOCON configuration example for setting up a ConsistentHashingPool router in Akka.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_32\n\nLANGUAGE: hocon\nCODE:\n```\nakka.actor.deployment {\n  /parent/router1 {\n    router = consistent-hashing-pool\n    nr-of-instances = 5\n    virtual-nodes-factor = 10\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Simple TCP Handler - Scala\nDESCRIPTION: Basic implementation of a TCP connection handler that echoes received data back to the sender.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-tcp.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nclass SimplisticHandler extends Actor {\n  import Tcp._\n  def receive = {\n    case Received(data) => sender() ! Write(data)\n    case PeerClosed     => context.stop(self)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Logging of Outbound Remoting Messages at DEBUG - HOCON\nDESCRIPTION: This snippet configures Akka remote (Artery) to log all outbound messages at DEBUG level. Place under akka.remote.artery, setting log-sent-messages to 'on'. It logs network layer events, not actor logic, and should be used for diagnosing distributed messaging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_23\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery {\n  # If this is \"on\", Akka will log all outbound messages at DEBUG level,\n  # if off then they are not logged\n  log-sent-messages = on\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Lazy Flow Example in Scala\nDESCRIPTION: Demonstrates basic usage of lazyFlow by producing a sequence of numbers and observing the order of execution through side effects.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/lazyFlow.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 3)\n  .map { n =>\n    println(s\"Producing $n\")\n    n\n  }\n  .via(\n    Flow.lazyFlow { () =>\n      println(\"Creating flow\")\n      Flow[Int].map { n =>\n        println(s\"Flow mapping $n\")\n        n\n      }\n    })\n  .run()\n```\n\n----------------------------------------\n\nTITLE: Using LWWRegister with Custom Clock in Java\nDESCRIPTION: Example of using LWWRegister with a custom clock in Java. It shows how to create a register with a custom timestamp based on a version number.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\npublic static LWWRegister<String> versionedLWWRegister(String name, long version) {\n  return LWWRegister.create(name, version, LWWRegister.defaultClock());\n}\n\nLWWRegister<String> r1 = versionedLWWRegister(\"value1\", 1L);\nLWWRegister<String> r2 = versionedLWWRegister(\"value2\", 2L);\n\nLWWRegister<String> merged = r1.merge(r2);\nString result = merged.getValue();\n```\n\n----------------------------------------\n\nTITLE: Making All Data Durable Configuration\nDESCRIPTION: Configuration example showing how to make all Distributed Data entries durable by using a wildcard pattern for keys.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_39\n\nLANGUAGE: text\nCODE:\n```\nakka.cluster.distributed-data.durable.keys = [\"*\"]\n```\n\n----------------------------------------\n\nTITLE: Enabling Akka Stream Materializer Fuzzing Mode - HOCON\nDESCRIPTION: This configuration snippet enables a special fuzzer setting in the Akka Stream materializer for testing. With fuzzing mode on, the stream materializer introduces additional thread interleavings to stress test concurrency, helping to expose race conditions. There are no parameters; simply set the value under akka.stream.materializer.debug. Use only during testing, as it severely affects performance and logs a warning.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-testkit.md#2025-04-22_snippet_2\n\nLANGUAGE: hocon\nCODE:\n```\nakka.stream.materializer.debug.fuzzing-mode = on\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Version Compatibility in Akka Scala\nDESCRIPTION: This code snippet illustrates the binary compatibility rules for Akka versions, showing which version upgrades are allowed and which are not. It covers both the old epoch.major.minor era and the new major.minor.patch era.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/common/binary-compatibility-rules.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n# [epoch.major.minor] era\nOK:  2.2.0 --> 2.2.1 --> ... --> 2.2.x\nNO:  2.2.y --x 2.3.y\nOK:  2.3.0 --> 2.3.1 --> ... --> 2.3.x\nOK:  2.3.x --> 2.4.x (special case, migration to new versioning scheme)\n# [major.minor.patch] era\nOK:  2.4.0 --> 2.5.x\nOK:  2.5.0 --> 2.6.x\nNO:  2.x.y --x 3.x.y\nOK:  3.0.0 --> 3.0.1 --> ... --> 3.0.n\nOK:  3.0.n --> 3.1.0 --> ... --> 3.1.n\nOK:  3.1.n --> 3.2.0 ...\n     ...\n```\n\n----------------------------------------\n\nTITLE: Markdown Navigation Reference\nDESCRIPTION: Reference link to the new Cluster API documentation using Markdown syntax.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-cluster.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n@ref[Cluster](typed/index-cluster.md)\n```\n\n----------------------------------------\n\nTITLE: Starting a Second Processor Node via sbt (Shell)\nDESCRIPTION: This command starts a second instance of the `processor` application, adding another node to the Akka Cluster. It uses different ports (2552 for remoting, 8552 for management, 8082 for gRPC) to avoid conflicts with the first node. This node will join the existing cluster and also join the Kafka consumer group, causing a rebalance of Kafka partitions.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"processor / run 2552 8552 8082\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Vote Counts with Akka in Scala\nDESCRIPTION: This piece of code retrieves the total counts of votes in a distributed manner using Akka. It sends a Get request for the counter key with ReadAll consistency, ensuring all remote replicas are considered, then processes the successful response. Requires Akka's replication features and identifies the sender for response purposes.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-scala/README.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\ncase GetVotes ⇒\n  replicator ! Get(CountersKey, ReadAll(3.seconds), Some(GetVotesReq(sender())))\n\ncase g @ GetSuccess(CountersKey, Some(GetVotesReq(replyTo))) ⇒\n  val data = g.get(CountersKey)\n  replyTo ! Votes(data.entries, open)\n```\n\n----------------------------------------\n\nTITLE: Configuring Acceptable Heartbeat Pause in Akka Cluster\nDESCRIPTION: This snippet demonstrates how to configure the acceptable-heartbeat-pause setting in the Akka Cluster failure detector to handle false positives in unstable environments.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/failure-detector.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n```\nakka.cluster.failure-detector.acceptable-heartbeat-pause = 7s\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring TailChoppingPool Router\nDESCRIPTION: Configuration for TailChoppingPool router that sends messages to random routees with delays to optimize response time.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_30\n\nLANGUAGE: scala\nCODE:\n```\nakka.actor.deployment {\n  /parent/router7 {\n    router = tail-chopping-pool\n    nr-of-instances = 5\n    within = 10 seconds\n    tail-chopping-router.interval = 20 milliseconds\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Concrete Test Classes for Multi-Node Cluster Test in Scala\nDESCRIPTION: Creates concrete test classes for each role/node in the multi-node cluster test, extending an abstract test class.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_24\n\nLANGUAGE: scala\nCODE:\n```\nclass StatsSampleSpecMultiJvmNode1 extends StatsSampleSpec\nclass StatsSampleSpecMultiJvmNode2 extends StatsSampleSpec\nclass StatsSampleSpecMultiJvmNode3 extends StatsSampleSpec\n```\n\n----------------------------------------\n\nTITLE: Defining ItemAdded Event Class with Optional Field in Scala\nDESCRIPTION: Scala code snippet showing the initial ItemAdded event class with an optional field before renaming.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-jackson.md#2025-04-22_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ItemAdded.java](/akka-serialization-jackson/src/test/scala/doc/akka/serialization/jackson/v1/ItemAdded.scala) { #add-optional }\n```\n\n----------------------------------------\n\nTITLE: Using zipAll Operator in Java\nDESCRIPTION: Shows the usage of zipAll operator to combine elements from two sources in Java, handling early completion with default values.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipAll.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// #zipAll-simple\nSource.from(Arrays.asList(1, 2, 3))\n    .zipAll(Source.from(Arrays.asList(\"A\", \"B\")), 0, \"MISSING\")\n    .runWith(Sink.foreach(System.out::println), system);\n// prints:\n// (1,A)\n// (2,B)\n// (3,MISSING)\n// #zipAll-simple\n```\n\n----------------------------------------\n\nTITLE: Scala Props Constructor Example\nDESCRIPTION: Example of defining Props for classic actors using reflection-based approach in Scala\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/native-image.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nProps[T]()\nProps(classOf[T], ...)\nProps(new T)\n```\n\n----------------------------------------\n\nTITLE: Extracting Context from Flow Elements in Java\nDESCRIPTION: This Java example shows how to use asFlowWithContext to extract correlation numbers as context from flow elements, focusing on the text message content. It demonstrates the conversion of a regular flow to a FlowWithContext and back.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/asFlowWithContext.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nFlow<Pair<String, Integer>, Pair<String, Integer>, NotUsed> flow =\n    Flow.<Pair<String, Integer>>create()\n        .map(pair -> Pair.create(pair.first().toUpperCase(), pair.second()))\n        .asFlowWithContext(\n            (text, correlationNumber) -> Pair.create(text, correlationNumber),\n            outgoingPair -> outgoingPair.second())\n        .map(String::toLowerCase);\n\nList<Pair<String, Integer>> input =\n    Arrays.asList(\n        Pair.create(\"abc\", 1), Pair.create(\"def\", 2), Pair.create(\"ghi\", 3));\nCompletionStage<List<Pair<String, Integer>>> result =\n    Source.from(input).via(flow).runWith(Sink.seq(), system);\n\nresult.thenAccept(list -> list.forEach(System.out::println));\n```\n\n----------------------------------------\n\nTITLE: Expected Log Output from Producer (Text)\nDESCRIPTION: This log snippet shows example output from the producer application console after it starts. It indicates that the producer is actively sending messages to the Kafka topic, specifying the user ID for each message (e.g., user 29, 60, 75).\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n[INFO] [01/16/2020 09:51:38.639] [UserEventProducer(akka://UserEventProducer)] Sending message to user 29\n[INFO] [01/16/2020 09:51:39.660] [UserEventProducer(akka://UserEventProducer)] Sending message to user 60\n[INFO] [01/16/2020 09:51:40.680] [UserEventProducer(akka://UserEventProducer)] Sending message to user 75\n```\n\n----------------------------------------\n\nTITLE: Excluding Named Tests from sbt Test Execution - shell\nDESCRIPTION: This shell snippet provides an sbt launch command that excludes any tests containing a specified name pattern (e.g., 'akka.cluster.Stress'). The 'akka.test.names.exclude' system property allows for customization of which tests to skip. Requires sbt and applies to Akka's build/test environment. Input is the excluded name pattern; the output omits the matching tests during the test phase.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nsbt -Dakka.test.names.exclude=akka.cluster.Stress\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL/TLS Parameters for Akka Remote\nDESCRIPTION: Detailed SSL/TLS configuration including keystore, truststore settings, passwords, and enabled algorithms. Passwords should be provided through environment variables for security.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remote-security.md#2025-04-22_snippet_1\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.artery {\n  transport = tls-tcp\n\n  ssl.config-ssl-engine {\n    key-store = \"/example/path/to/mykeystore.jks\"\n    trust-store = \"/example/path/to/mytruststore.jks\"\n\n    key-store-password = ${SSL_KEY_STORE_PASSWORD}\n    key-password = ${SSL_KEY_PASSWORD}\n    trust-store-password = ${SSL_TRUST_STORE_PASSWORD}\n\n    protocol = \"TLSv1.2\"\n\n    enabled-algorithms = [TLS_DHE_RSA_WITH_AES_128_GCM_SHA256]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Compression for TwoPhaseSet Serialization in Scala\nDESCRIPTION: Scala example showing how to add Gzip compression to a serializer using the SerializationSupport trait, which can reduce data size for network transmission.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/distributed-data.md#2025-04-22_snippet_33\n\nLANGUAGE: scala\nCODE:\n```\ndef toBinary(o: AnyRef): Array[Byte] = o match {\n  case m: TwoPhaseSet =>\n    compress(twoPhaseSetToProto(m).toByteArray)\n  case _ => throw new IllegalArgumentException(s\"Can't serialize object of type ${o.getClass}\")\n}\n\ndef fromBinary(bytes: Array[Byte], manifest: String): AnyRef = manifest match {\n  case TwoPhaseSetManifest =>\n    twoPhaseSetFromBinary(uncompress(bytes))\n  case _ =>\n    throw new IllegalArgumentException(s\"Unknown manifest [${manifest}]\")\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Node Cluster Test in Scala\nDESCRIPTION: Defines the participating roles and configuration for a multi-node cluster test by extending MultiNodeConfig.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-usage.md#2025-04-22_snippet_23\n\nLANGUAGE: scala\nCODE:\n```\nval first = role(\"first\")\nval second = role(\"second\")\nval third = role(\"third\")\n\ncommonConfig(debugConfig(on = false)\n  .withFallback(ConfigFactory.parseString(\"\"\"\n    akka.cluster.metrics.enabled=off\n    akka.actor.provider=cluster\n    akka.remote.log-remote-lifecycle-events=off\n    akka.cluster.roles=[\"backend\"]\n    \"\"\"))\n  .withFallback(MultiNodeClusterSpec.clusterConfig))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for Durable State Plugin in Java\nDESCRIPTION: The necessary imports required to implement a custom durable state storage plugin in Java. These imports provide access to the Akka persistence extension's plugin APIs and related utilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#plugin-imports\n```\n\n----------------------------------------\n\nTITLE: Scala Source.keepAlive API Signature\nDESCRIPTION: Scala API signature for the keepAlive operator on Source, taking maxIdle duration and element injection function parameters\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/keepAlive.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nkeepAlive[U>:Out](maxIdle:scala.concurrent.duration.FiniteDuration,injectedElem:()=>U):FlowOps.this.Repr[U]\n```\n\n----------------------------------------\n\nTITLE: Log Capturing with ScalaTest\nDESCRIPTION: Shows how to use the LogCapturing utility with ScalaTest to buffer log events and flush them only on test failure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [ScalaTestIntegrationExampleSpec.scala](/akka-actor-testkit-typed/src/test/scala/docs/akka/actor/testkit/typed/scaladsl/ScalaTestIntegrationExampleSpec.scala) { #log-capturing }\n```\n\n----------------------------------------\n\nTITLE: Removing Item from Cart with Consistency in Java\nDESCRIPTION: Java example of removing an item from a cart in distributed storage. It shows fetching the latest data before performing the removal to ensure consistency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nprivate CompletionStage<Done> removeItem(String userId, String itemId) {\\n  Update<Cart> update =\\n      new Update<>(\\n          DataKey.create(userId),\\n          Cart.empty(),\\n          writeMajority,\\n          curr -> curr.removeItem(itemId));\\n  return askGet(new Get<Cart>(DataKey.create(userId), readMajority))\\n      .thenCompose(\\n          response -> {\\n            if (response instanceof GetSuccess || response instanceof NotFound) {\\n              return askUpdate(update);\\n            } else {\\n              throw new IllegalStateException(\\\"Get failed\\\");\\n            }\\n          })\\n      .thenApply(updateDone -> Done.getInstance());\\n}\n```\n\n----------------------------------------\n\nTITLE: Injecting Shared LevelDB Store Reference (Java - Deprecated)\nDESCRIPTION: Demonstrates how to inject the `ActorRef` of the running `SharedLeveldbStore` actor into the shared LevelDB journal plugin using `SharedLeveldbJournal.setStore` in Java. This initialization step is required before the deprecated shared journal plugin can be used.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n// Assuming the snippet injects the store reference\nimport akka.actor.ActorRef;\nimport akka.actor.ActorSystem;\nimport akka.persistence.journal.leveldb.SharedLeveldbJournal;\n\nActorSystem system = ActorSystem.create();\nActorRef store = null; // The SharedLeveldbStore actor reference\nSharedLeveldbJournal.setStore(store, system);\n```\n\n----------------------------------------\n\nTITLE: Journal TCK Test Implementation - Scala\nDESCRIPTION: Example of implementing the Journal Technology Compatibility Kit tests in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-journals.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\nclass MyJournalSpec extends JournalSpec(\n    config = ConfigFactory.parseString(\n      \"\"\"\n        akka.persistence.journal.plugin = \"my-journal\"\n        my-journal {\n          class = \"docs.persistence.MyJournal\"\n          plugin-dispatcher = \"akka.actor.default-dispatcher\"\n        }\n      \"\"\"),\n    pluginConfig = \"my-journal\")\n```\n\n----------------------------------------\n\nTITLE: Batch Operation API Signatures in Akka Streams\nDESCRIPTION: API signatures for batch operations in Source and Flow components. The batch operation takes a maximum size, seed function, and aggregate function to batch elements while handling backpressure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/batch.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.batch[S](max: Long, seed: Out => S)(aggregate: (S, Out) => S): FlowOps.this.Repr[S]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.batch(long, akka.japi.function.Function, akka.japi.function.Function2)\n```\n\nLANGUAGE: scala\nCODE:\n```\nFlow.batch[S](max: Long, seed: Out => S)(aggregate: (S, Out) => S): FlowOps.this.Repr[S]\n```\n\nLANGUAGE: java\nCODE:\n```\nFlow.batch(long, akka.japi.function.Function, akka.japi.function.Function2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Termination Message Handler\nDESCRIPTION: Example of how to handle termination messages in the singleton actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-singleton.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\ncase End =>\n  queue ! UnregisterConsumer\n  context.stop(self)\n```\n\nLANGUAGE: Java\nCODE:\n```\n@Override\npublic void onReceive(Object msg) {\n  if (msg == TestSingletonMessages.end()) {\n    queue.tell(UnregisterConsumer.getInstance(), getSelf());\n    getContext().stop(getSelf());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Subchannel Classification Bus in Java\nDESCRIPTION: Test code for a Subchannel Classification Bus in Java, showing how to subscribe to a parent topic and receive messages from both that topic and its subtopics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nSubchannelBusImpl subchannelBus = new SubchannelBusImpl();\nsubchannelBus.subscribe(getTestActor(), \"abc\");\nsubchannelBus.publish(new MsgEnvelope(\"abc\", \"hello\"));\nsubchannelBus.publish(new MsgEnvelope(\"abc.123\", \"hello2\"));\nexpectMsgEquals(\"hello\");\nexpectMsgEquals(\"hello2\");\n```\n\n----------------------------------------\n\nTITLE: Scala batchWeighted Method Signature\nDESCRIPTION: Method signature for batchWeighted operator in Scala that takes maximum weight limit, cost function, seed function, and aggregation function as parameters. Returns a transformed flow with batched elements.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/batchWeighted.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nbatchWeighted[S](max: Long, costFn: Out => Long, seed: Out => S)(aggregate: (S, Out) => S): FlowOps.this.Repr[S]\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Extensions in application.conf for Scala\nDESCRIPTION: Shows how to configure the Akka extension to be loaded at ActorSystem creation time using the application.conf file in Scala.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_8\n\nLANGUAGE: ruby\nCODE:\n```\nakka.actor.typed {\n  extensions = [\"docs.akka.extensions.DatabaseConnectionPool\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Using detach() in Akka Streams Flow (Scala)\nDESCRIPTION: Applies the detach operator to a Flow in Akka Streams using Scala. This separates upstream demand from downstream demand without affecting stream rates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/detach.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nFlow.detach\n```\n\n----------------------------------------\n\nTITLE: Starting Standalone Aeron Media Driver\nDESCRIPTION: Java command for launching a standalone Aeron media driver process.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\njava io.aeron.driver.MediaDriver\n```\n\n----------------------------------------\n\nTITLE: Suppress Java Serialization Warnings in Akka\nDESCRIPTION: This configuration snippet turns off warnings about Java serializer usage. It requires an Akka setup and access to configuration files. The key parameter is 'warn-about-java-serializer-usage', which is set to 'off' to disable warnings. Expected behavior is the suppression of log warnings about the deprecated or insecure use of Java serialization. This setting is suggested only for development stages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization.md#2025-04-22_snippet_5\n\nLANGUAGE: Ruby\nCODE:\n```\nakka.actor.warn-about-java-serializer-usage = off\n```\n\n----------------------------------------\n\nTITLE: Using detach() in Akka Streams Source (Java)\nDESCRIPTION: Applies the detach operator to a Source in Akka Streams using Java. This separates upstream demand from downstream demand without affecting stream rates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/detach.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSource.detach()\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Extensions in application.conf for Java\nDESCRIPTION: Shows how to configure the Akka extension to be loaded at ActorSystem creation time using the application.conf file in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/extending.md#2025-04-22_snippet_9\n\nLANGUAGE: ruby\nCODE:\n```\nakka.actor.typed {\n  extensions = [\"jdocs.akka.extensions.ExtensionDocTest$DatabaseConnectionPool$Id\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Empty Iterator Error in Scala\nDESCRIPTION: Example showing how Source.cycle handles an empty iterator in Scala, which results in an IllegalArgumentException.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/cycle.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n#cycle-error\n```\n\n----------------------------------------\n\nTITLE: Implementing dropWithin in Akka Streams (Scala)\nDESCRIPTION: Signature for the dropWithin operator in Scala for Akka Streams Source and Flow. It takes a FiniteDuration parameter and returns a transformed stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/dropWithin.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.dropWithin(d:scala.concurrent.duration.FiniteDuration):FlowOps.this.Repr[Out]\n```\n\nLANGUAGE: scala\nCODE:\n```\nFlow.dropWithin(d:scala.concurrent.duration.FiniteDuration):FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Using intersperse to format a stream of integers in Scala\nDESCRIPTION: This example takes a stream of integers, converts them to strings, and adds formatting elements: '[' at the start, ', ' between each element, and ']' at the end.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/intersperse.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource(1 to 3)\n  .map(_.toString)\n  .intersperse(\"[\", \", \", \"]\")\n  .runWith(Sink.foreach(print))\n// prints: [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Flow Creation for Gzip Decompression in Akka Streams - Java\nDESCRIPTION: This Java definition creates a gzip decompression flow using Akka Streams, suitable for processing streams of ByteStrings. It takes an integer parameter to configure the maximum byte length for decompressed output chunks. The code handles various errors such as invalid compression methods. Dependencies include the Akka Streams library.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Compression/gunzip.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n\"java=\\\"#gunzip(int)\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Querying Shard Allocation on Node 2 via Akka Management (Shell)\nDESCRIPTION: Similar to the previous command, this uses `curl` and `jq` to query the Akka Management endpoint, but targets the second processor node (running on management port 8552). This allows inspection of the shard distribution on the second node, verifying the results of the rebalancing.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n# Node 2:\ncurl -v localhost:8552/cluster/shards/user-processing | jq\n```\n\n----------------------------------------\n\nTITLE: Using RetryFlow.withBackoffAndContext in Java\nDESCRIPTION: Java implementation example of RetryFlow.withBackoffAndContext with a FlowWithContext handling Integers with SomeContext, retrying elements based on output value and maxRetries conditions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RetryFlow/withBackoffAndContext.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nFlowWithContext<Integer, SomeContext, Integer, SomeContext, NotUsed> flow =\n    // business logic\n    FlowWithContext.<Integer, SomeContext>create().map(i -> i - 1);\n\nFlowWithContext<Integer, SomeContext, Integer, SomeContext, NotUsed> retryFlow =\n    RetryFlow.withBackoffAndContext(\n        Duration.ofMillis(10),\n        Duration.ofSeconds(5),\n        0.2,\n        3,\n        flow,\n        (Pair<Integer, SomeContext> in, Pair<Integer, SomeContext> out) -> {\n          if (out.first() <= 0) {\n            // do not retry on 0 or negative\n            return Optional.empty();\n          } else if (in.first() - 1 == out.first()) {\n            // do not retry on expected outcome\n            return Optional.empty();\n          } else {\n            // otherwise retry with the original value\n            return Optional.of(in);\n          }\n        });\n```\n\n----------------------------------------\n\nTITLE: Testing for Deadlock with CallingThreadDispatcher and CountDownLatch - Scala\nDESCRIPTION: This Scala snippet demonstrates a synchronization deadlock scenario when using the CallingThreadDispatcher combined with CountDownLatch for coordination. Shows that sending a message which blocks on a latch can hang if both sender and actor share the dispatcher. Akka Actors and java.util.concurrent.CountDownLatch are required.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/testing.md#2025-04-22_snippet_32\n\nLANGUAGE: Scala\nCODE:\n```\nval latch = new CountDownLatch(1)\\nactor ! startWorkAfter(latch)   // actor will call latch.await() before proceeding\\ndoSomeSetupStuff()\\nlatch.countDown()\n```\n\n----------------------------------------\n\nTITLE: Defining an Asynchronous Email Sending Service (Java)\nDESCRIPTION: Defines an `EmailServer` class with an asynchronous `send` method. This method simulates sending an email and returns a `CompletionStage<Void>`, indicating the completion of the asynchronous operation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nstatic class EmailServer {\n  // Sends an email, returns a CompletionStage<Void>\n  public CompletionStage<Void> send(Email email) {\n    return CompletableFuture.runAsync(() -> {\n      // simulate sending email\n      System.out.println(\"Sending email to \" + email.to);\n      try {\n        Thread.sleep(100); // simulate latency\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n      System.out.println(\"Email to \" + email.to + \" sent\");\n    });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Sender Actor in Scala\nDESCRIPTION: Example of an actor that sends messages using the DistributedPubSub mediator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\nclass Sender extends Actor {\n  val mediator = DistributedPubSub(context.system).mediator\n\n  def receive = {\n    case in: String =>\n      val out = in.toUpperCase\n      mediator ! DistributedPubSubMediator.Send(\"/user/destination\", out)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Request Context with Update in Scala\nDESCRIPTION: Example of passing a request context with an Update message, which is included in the reply messages. This is useful for passing contextual information without maintaining local correlation data structures.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nval Counter1Key = PNCounterKey(\"counter1\")\nreplicator ! Update(Counter1Key, PNCounter.empty, WriteLocal, request = Some(sender()))(_ :+ 1)\n\n// reply will be sent to the original sender\nreceive {\n  case UpdateSuccess(Counter1Key, Some(replyTo: ActorRef)) =>\n    replyTo ! \"updated\"\n  case UpdateTimeout(Counter1Key, Some(replyTo: ActorRef)) =>\n    replyTo ! \"not updated, but will eventually be replicated\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Flow.fromSinkAndSourceCoupled in Akka Streams (Scala)\nDESCRIPTION: Creates a Flow by coupling a Sink and a Source, with coupled termination behavior. The method takes a Sink of type I and a Source of type O as parameters, returning a Flow[I,O,akka.NotUsed].\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Flow/fromSinkAndSourceCoupled.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFlow.fromSinkAndSourceCoupled[I,O](sink: akka.stream.Graph[akka.stream.SinkShape[I],_], source: akka.stream.Graph[akka.stream.SourceShape[O],_]): akka.stream.scaladsl.Flow[I,O,akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Starting Cluster Sharding\nDESCRIPTION: Shows how to initialize cluster sharding by registering entity types using ClusterSharding.start method. The code demonstrates setup for the ShardRegion.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\n#counter-start\n```\n\nLANGUAGE: Java\nCODE:\n```\n#counter-start\n```\n\n----------------------------------------\n\nTITLE: Event Adapter for Skipping Deleted Events in Akka Persistence (Scala)\nDESCRIPTION: Scala implementation of an EventAdapter that handles EventDeserializationSkipped objects, emitting an empty EventSeq for removed event types in Akka Persistence.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_15\n\nLANGUAGE: Scala\nCODE:\n```\nclass RemovedEventsAwareAdapter extends EventAdapter {\n  override def fromJournal(event: Any, manifest: String): EventSeq = event match {\n    case s: EventDeserializationSkipped => EventSeq.empty\n    case _ => EventSeq.single(event)\n  }\n\n  override def manifest(event: Any): String = \"\"\n\n  override def toJournal(event: Any): Any = event\n}\n```\n\n----------------------------------------\n\nTITLE: Running SimpleClusterApp in Java\nDESCRIPTION: This snippet demonstrates how to start the SimpleClusterApp, which sets up three actor systems as cluster members. You can stop the application and run them in separate processes using specific ports or a random available port.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\\\"sample.cluster.simple.App\\\" -Dexec.args=25251\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\\\"sample.cluster.simple.App\\\" -Dexec.args=25252\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\\\"sample.cluster.simple.App\\\" -Dexec.args=0\n```\n\n----------------------------------------\n\nTITLE: Generating and Opening Akka Documentation in the Browser - Shell\nDESCRIPTION: This snippet shows a one-liner sbt command, 'akka-docs/paradoxBrowse', which builds the Akka Paradox documentation and attempts to open the generated index in the system's default web browser. The command should be run from the project root. This requires sbt, Paradox, and system browser configuration.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nakka-docs/paradoxBrowse\n```\n\n----------------------------------------\n\nTITLE: Setting Snapshot Storage Policy in Java\nDESCRIPTION: Example showing how to configure a custom snapshot storage policy in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n\"#set-snapshot-storage-policy\"\n```\n\n----------------------------------------\n\nTITLE: Detaching Domain Model from Data Model in Akka Persistence\nDESCRIPTION: Example showing how to implement domain and data models separately along with an EventAdapter to map between them. The adapter converts between domain events and data model events while maintaining complete separation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\n// Domain model - a simple user event\nfinal case class UserDetailsChanged(name: String, address: String)\n\n// Data model - the protobuf generated class\nfinal case class UserDetailsChangedDataModel(name: String, address: String)\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository\nDESCRIPTION: Maven repository configuration for accessing Akka libraries\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_0\n\nLANGUAGE: markup\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Upper Replay Bound During Recovery in Akka Persistence (Scala)\nDESCRIPTION: Customizes recovery by defining an upper bound on the sequence number of events to replay using `toSequenceNr`. This causes the actor to only recover its state up to that specific point, potentially useful for debugging. Persisting new events after such a partial recovery is discouraged as it may lead to inconsistencies in subsequent full recoveries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\noverride def recovery: Recovery = Recovery(toSequenceNr = 457L)\n```\n\n----------------------------------------\n\nTITLE: Using Request Context with Get in Java\nDESCRIPTION: Java example showing how to pass a request context (original sender) with a Get message, which can be used to route the response back to the original requester.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\nfinal Key<PNCounter> Counter1Key = PNCounterKey.create(\"counter1\");\nreplicator.tell(\n    new Get<>(Counter1Key, ReadLocal.instance(), getContext().getSender()), getSelf());\n\n// handle response\nreceiveBuilder()\n    .match(\n        GetSuccess.class,\n        g -> {\n          if (g.key().equals(Counter1Key) && g.getRequest().isPresent()) {\n            ActorRef replyTo = (ActorRef) g.getRequest().get();\n            long value = ((PNCounter) g.dataValue()).getValue().intValue();\n            replyTo.tell(value, getSelf());\n          }\n        })\n    .match(\n        NotFound.class,\n        n -> {\n          if (n.key().equals(Counter1Key) && n.getRequest().isPresent()) {\n            ActorRef replyTo = (ActorRef) n.getRequest().get();\n            replyTo.tell(new Integer(0), getSelf());\n          }\n        })\n    .match(\n        GetFailure.class,\n        f -> {\n          if (f.key().equals(Counter1Key) && f.getRequest().isPresent()) {\n            ActorRef replyTo = (ActorRef) f.getRequest().get();\n            replyTo.tell(new Integer(-1), getSelf());\n          }\n        })\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Using CompletionTimeout in Akka Streams - Java\nDESCRIPTION: Example showing CompletionTimeout usage in Java to process numbers with a 10ms completion timeout. The stream will fail if processing doesn't complete within the timeout period.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/completionTimeout.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#completionTimeout\n```\n\n----------------------------------------\n\nTITLE: Actor Under Test Implementation\nDESCRIPTION: Example actor implementation that demonstrates different effects to be tested including spawning children, sending messages and logging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-sync.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass Protocol {\n  sealed trait Command\n  case class SpawnChild(name: String) extends Command\n  case class SpawnAnonymousChild() extends Command\n  case class SendMessageToChild(name: String) extends Command\n  case class SendMessageToAnonymousChild() extends Command\n  case class SendMessageToTarget(target: ActorRef[String]) extends Command\n  case class LogAndSayHello() extends Command\n}\n```\n\nLANGUAGE: java\nCODE:\n```\ninterface Protocol {\n  interface Command {}\n  public class SpawnChild implements Command {\n    public final String name;\n    public SpawnChild(String name) {\n      this.name = name;\n    }\n  }\n  // Other command classes...\n}\n```\n\n----------------------------------------\n\nTITLE: Repeating Last Element Stream Processor in Akka Streams (Scala)\nDESCRIPTION: Custom GraphStage that repeats the last seen element if the upstream is slower than the downstream. Includes an initial value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-cookbook.md#2025-04-22_snippet_45\n\nLANGUAGE: Scala\nCODE:\n```\nclass HoldWithInitial[T](initial: T) extends GraphStage[FlowShape[T, T]] {\n  val in = Inlet[T](\"HoldWithInitial.in\")\n  val out = Outlet[T](\"HoldWithInitial.out\")\n  override val shape = FlowShape(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      private var currentValue: T = initial\n\n      setHandlers(\n        in,\n        out,\n        new InHandler with OutHandler {\n          override def onPush(): Unit = {\n            currentValue = grab(in)\n            pull(in)\n          }\n\n          override def onPull(): Unit = {\n            push(out, currentValue)\n          }\n        }\n      )\n\n      override def preStart(): Unit = {\n        pull(in)\n      }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating RoundRobinPool Programmatically (Scala/Java)\nDESCRIPTION: Demonstrates creating a RoundRobinPool router actor programmatically. It specifies the round-robin logic and the number of routee instances (5) directly in the code.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\n//#round-robin-pool-2\nimport akka.routing.RoundRobinPool\n\nval router2: ActorRef = context.actorOf(RoundRobinPool(5).props(Props[Worker]()), \"router2\")\n//#round-robin-pool-2\n```\n\nLANGUAGE: java\nCODE:\n```\n//#round-robin-pool-2\nimport akka.routing.RoundRobinPool;\n\nfinal ActorRef router2 = getContext().actorOf(new RoundRobinPool(5).props(Props.create(Worker.class)), \"router2\");\n//#round-robin-pool-2\n```\n\n----------------------------------------\n\nTITLE: Partitioning Integers into Even and Odd Streams in Java\nDESCRIPTION: This example shows how to use the Partition operator in Java to split a Source of integers into two Sinks: one for even numbers and another for odd numbers. It demonstrates the usage of the partition method and how to connect the resulting substreams to their respective sinks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Partition.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource<Integer> source = Source.range(1, 11);\nint numOutputs = 2;\n\nPartition<Integer> partition = Partition.create(numOutputs, element -> element % 2);\n\nSource<Integer> even = source.via(partition.out(0));\nSource<Integer> odd = source.via(partition.out(1));\n\nSink<Integer> sink1 = Sink.foreach(x -> System.out.println(\"Even number: \" + x));\nSink<Integer> sink2 = Sink.foreach(x -> System.out.println(\"Odd number: \" + x));\n\neven.runWith(sink1, materializer);\nodd.runWith(sink2, materializer);\n```\n\n----------------------------------------\n\nTITLE: Defining an Asynchronous Email Sending Service (Scala)\nDESCRIPTION: Defines an `EmailServer` object with an asynchronous `send` method. This method simulates sending an email and returns a `Future[Unit]`, indicating the completion of the operation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nobject EmailServer {\n  // Sends an email, returns a Future of Unit\n  def send(email: Email): Future[Unit] = Future {\n    // simulate sending email\n    println(s\"Sending email to ${email.to}\")\n    Thread.sleep(100) // simulate latency\n    println(s\"Email to ${email.to} sent\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository and Dependencies\nDESCRIPTION: Maven repository configuration and dependency declarations for Akka Persistence TestKit\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_0\n\nLANGUAGE: markup\nCODE:\n```\nurl=\"https://repo.akka.io/maven\"\n```\n\n----------------------------------------\n\nTITLE: Sink.preMaterialize Method Signatures\nDESCRIPTION: Method signatures for preMaterialize in both Scala and Java. The operator materializes a Sink and returns both its materialized value and a new Sink instance. It's implemented using a reactive streams Subscriber, introducing buffering and converting upstream errors into cancellations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/preMaterialize.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\npreMaterialize()(implicit materializer: akka.stream.Materializer): (Mat, akka.stream.scaladsl.Sink[In,akka.NotUsed])\n```\n\nLANGUAGE: java\nCODE:\n```\npreMaterialize(akka.actor.ClassicActorSystemProvider)\npreMaterialize(akka.stream.Materializer)\n```\n\n----------------------------------------\n\nTITLE: Source.lazily API Signature in Scala and Java\nDESCRIPTION: API signatures for the lazily operator in both Scala and Java. The operator takes a creation function that returns a Source and produces a Source with a Future materialized value. This operator has been deprecated in Akka 2.6.0.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazily.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nlazily[T,M](create:()=>akka.stream.scaladsl.Source[T,M]):akka.stream.scaladsl.Source[T,scala.concurrent.Future[M]]\n```\n\nLANGUAGE: java\nCODE:\n```\nlazily(akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Basic ActorSystem Configuration Structure\nDESCRIPTION: Example configuration structure showing how to separate settings for multiple actor systems using configuration hierarchy\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/general/configuration.md#2025-04-22_snippet_5\n\nLANGUAGE: hocon\nCODE:\n```\nmyapp1 {\n  akka.loglevel = \"WARNING\"\n  my.own.setting = 43\n}\nmyapp2 {\n  akka.loglevel = \"ERROR\"\n  app2.setting = \"appname\"\n}\nmy.own.setting = 42\nmy.other.setting = \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Key Changes in Akka Replicator (Java)\nDESCRIPTION: Registers the current actor to receive updates when the specified distributed data key changes. This allows actors across all nodes to respond immediately to replicated flag changes. The snippet uses the Replicator actor's tell method and Java Akka API; dependencies include Akka Actors and Distributed Data.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-distributed-data-java/README.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nreplicator.tell(new Subscribe<>(openedKey, self()), ActorRef.noSender());\n```\n\n----------------------------------------\n\nTITLE: Using Sink.headOption with an Empty Source in Scala\nDESCRIPTION: Example of using Sink.headOption with an empty source in Scala, which handles the case when no elements are emitted by completing with None.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/headOption.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [HeadOption.scala](/akka-docs/src/test/scala/docs/stream/operators/sink/HeadOption.scala) { #headoption }\n```\n\n----------------------------------------\n\nTITLE: Sending Messages via PubSub in Scala\nDESCRIPTION: Example showing how to send messages using the DistributedPubSub mediator.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-pub-sub.md#2025-04-22_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\nmediator ! DistributedPubSubMediator.Send(\"/user/destination\", \"hello\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Event Deletion with Snapshot Predicate in Java\nDESCRIPTION: Shows how to enable event deletion when using predicate-based snapshots in Java. This setting allows events to be deleted when snapshots are taken based on custom predicates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-snapshot.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nwithDeleteEventsOnSnapshot(true)\n```\n\n----------------------------------------\n\nTITLE: Deleting Distributed Data in Java\nDESCRIPTION: Java example of deleting data from distributed storage using Akka's Replicator. It shows how to execute a delete operation with proper consistency.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/distributed-data.md#2025-04-22_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\nreplicator.tell(new Delete<>(dataKey, WriteLocal.getInstance(), request.replyTo), getSelf());\n```\n\n----------------------------------------\n\nTITLE: Combining Sinks with Fan-out Junction in Scala\nDESCRIPTION: This example demonstrates how to combine multiple sinks using a Fan-out Junction in Akka Streams with Scala. It shows the implementation of Sink.combine with a custom fan-out strategy.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/combine.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval sink1 = Sink.foreach[String](println)\nval sink2 = Sink.foreach[String](println)\nval sink3 = Sink.foreach[String](println)\n\nval fanOutSink = Sink.combine(sink1, sink2, sink3)(Broadcast[String](_))\n\nSource(List(\"a\", \"b\", \"c\"))\n  .runWith(fanOutSink)\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Contents and Index Configuration\nDESCRIPTION: Markdown configuration for generating table of contents with depth 2 and index section containing links to various documentation pages\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@toc { depth=2 }\n\n@@@ index\n\n* [../common/binary-compatibility-rules](../common/binary-compatibility-rules.md)\n* [downstream-upgrade-strategy](downstream-upgrade-strategy.md)\n* [../common/may-change](../common/may-change.md)\n* [ide] (../additional/ide.md)\n* [lombok](immutable.md)\n* [migration-guides](migration-guides.md)\n* [rolling-update](rolling-update.md)\n* [issue-tracking](issue-tracking.md)\n* [licenses](licenses.md)\n* [faq](../additional/faq.md)\n* [books](../additional/books.md)\n* [examples](examples.md)\n* [links](links.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository in Maven\nDESCRIPTION: Specifies the Akka library repository URL in a Maven pom.xml file to enable fetching Akka dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-rate.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\n----------------------------------------\n\nTITLE: Using ask Method for Actor Interactions in Scala\nDESCRIPTION: The recommended pattern for using the AskPattern in Scala, which provides better readability than alternative syntaxes. This is used for request-response interactions from outside an actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_20\n\nLANGUAGE: Scala\nCODE:\n```\nask(actorRef, replyTo => GetValue(replyTo))\n```\n\n----------------------------------------\n\nTITLE: Matching Enum Messages in a Java ReceiveBuilder\nDESCRIPTION: How to match against enum singleton messages in an Akka Typed ReceiveBuilder. This demonstrates the pattern matching syntax for enum-based messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nBehaviors.receive(Command.class)\n    .onMessage(Command.Increment.class, command -> {\n      // increment logic here\n      return this;\n    })\n    .onMessage(Command.Decrement.class, command -> {\n      // decrement logic here\n      return this;\n    })\n    .build()\n```\n\n----------------------------------------\n\nTITLE: Akka Streams Source.lazyFuture API Definition in Scala\nDESCRIPTION: API signature for lazyFuture operator that creates a source which defers future creation until demand arrives. Takes a factory function returning Future[T] and produces a Source[T, NotUsed].\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazyFuture.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.lazyFuture[T](create: () => scala.concurrent.Future[T]): akka.stream.scaladsl.Source[T, akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Querying Shard Allocation on Node 1 via Akka Management (Shell)\nDESCRIPTION: This command uses `curl` to query the Akka Management HTTP endpoint exposed by the first processor node (running on management port 8551). It specifically requests information about the shard allocation for the `user-processing` entity type. The output is piped to `jq` for pretty-printing the JSON response. This allows inspection of which shards are currently hosted on this node.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n# Node 1:\ncurl -v localhost:8551/cluster/shards/user-processing | jq\n```\n\n----------------------------------------\n\nTITLE: Implementing zipWithIndex in Java\nDESCRIPTION: Example demonstrating the usage of zipWithIndex operator in Java to combine stream elements with their indices.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/zipWithIndex.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSourceOrFlow.java\n```\n\n----------------------------------------\n\nTITLE: Initializing DiagnosticLoggingAdapter in Java\nDESCRIPTION: Demonstrates the creation of a DiagnosticLoggingAdapter within an AbstractActor in Java. This adapter enables setting custom MDC values for logging.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\n// Within your AbstractActor\nfinal DiagnosticLoggingAdapter log = Logging.getLogger(this);\n```\n\n----------------------------------------\n\nTITLE: Blocking Database API Interface Example\nDESCRIPTION: Sample blocking database API interface that demonstrates a resource requiring explicit opening, reading and closing operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldResource.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass BlockingDatabase {\n  def query(queryString: String): BlockingDatabaseApi = ???\n}\n\ntrait BlockingDatabaseApi {\n  def hasMore: Boolean\n  def nextRow(): String  \n  def close(): Unit\n}\n```\n\nLANGUAGE: java\nCODE:\n```\nclass BlockingDatabase {\n  BlockingDatabaseApi query(String queryString) {\n    return null; // not a real implementation\n  }\n}\n\ninterface BlockingDatabaseApi {\n  boolean hasMore();\n  String nextRow();\n  void close();\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Late Device Termination\nDESCRIPTION: Test case verifying behavior when receiving a normal reply followed by actor termination.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\n#query-test-stopped-later\n```\n\nLANGUAGE: Java\nCODE:\n```\n#query-test-stopped-later\n```\n\n----------------------------------------\n\nTITLE: Entity Supervisor Strategy in Java\nDESCRIPTION: Shows how to implement a custom supervisor strategy for entity actors in Java using an intermediate parent actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n@@snip [ClusterShardingTest.java](/akka-docs/src/test/java/jdocs/sharding/ClusterShardingTest.java) { #supervisor }\n```\n\n----------------------------------------\n\nTITLE: Defining Sealed Traits for Command Composition in Scala\nDESCRIPTION: Creating a message hierarchy using sealed traits to enable composition of message handlers in Scala. This structure allows for selective handling of different message types.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_17\n\nLANGUAGE: Scala\nCODE:\n```\nsealed trait Command\nfinal case class GetValue(replyTo: ActorRef[Int]) extends Command\nfinal case class SetValue(value: Int) extends Command\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository (sbt, Maven, Gradle)\nDESCRIPTION: Specifies the Akka library repository URL required to fetch Akka dependencies. Configuration examples are provided for sbt, Maven, and Gradle build tools.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/futures-interop.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\nrepositories {\n    mavenCentral()\n    maven {\n        url 'https://repo.akka.io/maven'\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Blocking Database API in Scala\nDESCRIPTION: Example code showing a blocking database API interface and implementation that will be used with mapWithResource.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/mapWithResource.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ntrait BlockingDatabase {\n  def newConnection(): DatabaseConnection\n}\nclass DatabaseConnection {\n  def queryById(id: Int): Vector[String] = {\n    // Let's simulate a blocking call with a simple implementation\n    Vector(s\"Result for id: $id\")\n  }\n  def queryByName(name: String): Vector[String] = {\n    // Let's simulate a blocking call with a simple implementation\n    Vector(s\"Result for name: $name\")\n  }\n  def close(): Unit = () // close connection\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Logging for Akka Cluster Sharding in Logback\nDESCRIPTION: A Logback configuration example that enables DEBUG level logging specifically for the Cluster Sharding module while keeping the root logger at INFO level. This helps with troubleshooting Cluster Sharding issues.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_27\n\nLANGUAGE: xml\nCODE:\n```\n   <logger name=\"akka.cluster.sharding\" level=\"DEBUG\" />\n\n    <root level=\"INFO\">\n        <appender-ref ref=\"ASYNC\"/>\n    </root>\n```\n\n----------------------------------------\n\nTITLE: Java batchWeighted Method Signature\nDESCRIPTION: Method signature for batchWeighted operator in Java that takes maximum weight limit and three functions for cost calculation, seed generation, and element aggregation using Akka's Java API.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/batchWeighted.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nbatchWeighted(long, akka.japi.function.Function, akka.japi.function.Function, akka.japi.function.Function2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Capability in Device Group\nDESCRIPTION: Implementation of query functionality in the DeviceGroup actor by creating query actors with appropriate parameters.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/tutorial_5.md#2025-04-22_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\n#query-added\n```\n\nLANGUAGE: Java\nCODE:\n```\n#query-added\n```\n\n----------------------------------------\n\nTITLE: Anti-pattern: Complex Logic in Lambda Handlers in Java\nDESCRIPTION: An example of what to avoid: putting extensive logic inside lambda expressions in the ReceiveBuilder. This makes the code harder to read and debug.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nBehaviors.receive(Command.class)\n    .onMessage(GetValue.class, command -> {\n      getSender(command).tell(value);\n      return this;\n    })\n    .onMessage(Increment.class, command -> {\n      value++;\n      logValue();\n      return this;\n    })\n    .onMessage(Decrement.class, command -> {\n      value--;\n      logValue();\n      return this;\n    })\n    .build()\n```\n\n----------------------------------------\n\nTITLE: Building and Device Entity Definitions in Java\nDESCRIPTION: Java example showing entity definitions with custom shard allocation for buildings and devices\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_20\n\nLANGUAGE: java\nCODE:\n```\npublic class Building {\\n  public static final EntityTypeKey<Command> TypeKey =\\n      EntityTypeKey.create(Command.class, \\\"Building\\\");\\n\\n  interface Command extends CborSerializable {}\\n\\n  public static class BuildingEnvelope {\\n    public final String buildingId;\\n    public final String shardId;\\n\\n    public BuildingEnvelope(String buildingId) {\\n      this.buildingId = buildingId;\\n      this.shardId = String.valueOf(Math.abs(buildingId.hashCode() % 30));\\n    }\\n  }\\n}\\n\\npublic class Device {\\n  public static final EntityTypeKey<Command> TypeKey =\\n      EntityTypeKey.create(Command.class, \\\"Device\\\");\\n\\n  interface Command extends CborSerializable {}\\n\\n  public static class DeviceEnvelope {\\n    public final String buildingId;\\n    public final String deviceId;\\n    public final String shardId;\\n\\n    public DeviceEnvelope(String buildingId, String deviceId) {\\n      this.buildingId = buildingId;\\n      this.deviceId = deviceId;\\n      this.shardId = String.valueOf(Math.abs(buildingId.hashCode() % 30));\\n    }\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Sources Using Source.combine in Scala\nDESCRIPTION: Example showing how to merge three sources of integers using Source.combine with Merge strategy. The sources will emit elements concurrently with non-deterministic ordering between sources.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/combine.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#imports #source-combine-merge\n```\n\n----------------------------------------\n\nTITLE: Defining Akka Stream Operators in Markdown Tables\nDESCRIPTION: This snippet demonstrates how to create markdown tables to document Akka Stream operators. It includes columns for the operator type, name, and description.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| |Operator|Description|\n|--|--|--|\n|Flow|<a name=\"asflowwithcontext\"></a>@ref[asFlowWithContext](Flow/asFlowWithContext.md)|Extracts context data from the elements of a `Flow` so that it can be turned into a `FlowWithContext` which can propagate that context per element along a stream.|\n|Source/Flow|<a name=\"collect\"></a>@ref[collect](Source-or-Flow/collect.md)|Apply a partial function to each incoming element, if the partial function is defined for a value the returned value is passed downstream.|\n```\n\n----------------------------------------\n\nTITLE: Querying Cluster Sharding Stats in Scala\nDESCRIPTION: Code snippet showing how to retrieve cluster-wide sharding statistics using GetClusterShardingStats in Scala. This provides information about shards and entity counts across all regions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_53\n\nLANGUAGE: scala\nCODE:\n```\nval shardingStats: Future[ShardRegion.ClusterShardingStats] =\n  sharding.shardingStats(\"user\")\nshardingStats.map { stats =>\n  stats.regions.foreach { case (region, regionStats) =>\n    println(s\"Region: $region\")\n    regionStats.stats.foreach { case (shardId, entityCount) =>\n      println(s\"  Shard: $shardId Count: $entityCount\")\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Supervisor for Entity in Scala\nDESCRIPTION: Shows how to start a supervisor actor for an entity in Scala using Cluster Sharding.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-sharding.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [ClusterShardingSpec.scala](/akka-cluster-sharding/src/multi-jvm/scala/akka/cluster/sharding/ClusterShardingSpec.scala) { #counter-supervisor-start }\n```\n\n----------------------------------------\n\nTITLE: Scala API - AggregateWithBoundary Signature\nDESCRIPTION: Scala method signature for aggregateWithBoundary operator that takes allocation, aggregation, harvest functions and optional timer condition.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/aggregateWithBoundary.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\naggregateWithBoundary[Agg,Emit](allocate:()=>Agg)(aggregate:(Agg,Out)=>(Agg,Boolean),harvest:Agg=>Emit,emitOnTimer:Option[(Agg=>Boolean,scala.concurrent.duration.FiniteDuration)]):FlowOps.this.Repr[Emit]\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Cluster Singleton Dependency\nDESCRIPTION: Dependency configuration for adding the Akka cluster singleton library to your project using SBT, Maven, or Gradle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_5\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-cluster-singleton_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Concatenating Multiple Sources Using Source.combine in Java\nDESCRIPTION: Java implementation showing how to concatenate three sources of integers using Source.combine with Concat strategy. Sources emit elements sequentially - each source completes before the next begins.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/combine.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n#source-combine-concat\n```\n\n----------------------------------------\n\nTITLE: Importing PubSub.source from Akka Stream Typed\nDESCRIPTION: Shows the API signature for the PubSub.source operator which creates a Source that subscribes to an Akka Typed Topic. Available for both Scala and Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/PubSub/source.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nPubSub.source[T](topic:akka.actor.typed.Toppic[T]):akka.stream.scaladsl.Source[T,akka.NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nPubSub.source(akka.actor.typed.Topic)\n```\n\n----------------------------------------\n\nTITLE: Configuring Resizable Router Pool in Scala\nDESCRIPTION: Defines a router pool with a default resizer in Scala configuration. The resizer dynamically adjusts the number of routees based on system pressure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/routing.md#2025-04-22_snippet_36\n\nLANGUAGE: scala\nCODE:\n```\nakka.actor.deployment {\n  /parent/router1 {\n    router = round-robin-pool\n    resizer {\n      lower-bound = 2\n      upper-bound = 15\n      messages-per-resize = 100\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Public vs Private Messages in Akka Actors\nDESCRIPTION: The code illustrates managing public and private messages within Akka actors. It demonstrates the usage of visibility constraints to limit access to certain messages and offers alternative approaches using type hierarchies for message organization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\nStyleGuideDocExamples.scala { #public-private-messages-1 }\n```\n\nLANGUAGE: Java\nCODE:\n```\nStyleGuideDocExamples.java { #public-private-messages-1 }\n```\n\nLANGUAGE: Scala\nCODE:\n```\nStyleGuideDocExamples.scala { #public-private-messages-2 }\n```\n\nLANGUAGE: Java\nCODE:\n```\nStyleGuideDocExamples.java { #public-private-messages-2 }\n```\n\n----------------------------------------\n\nTITLE: Output of Stream Recovery Example (Java)\nDESCRIPTION: Shows the expected output of the Java `recover` example. The stream emits elements '0' through '4', then catches the `RuntimeException`, emits the recovery element 'stream truncated', and completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-error.md#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n0\n1\n2\n3\n4\nstream truncated\n```\n\n----------------------------------------\n\nTITLE: Sink.onComplete API Signatures\nDESCRIPTION: API signatures for Sink.onComplete operator in both Scala and Java. The Scala version takes a callback function that receives a Try[Done], while the Java version takes a Procedure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/onComplete.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSink.onComplete[T](callback: Try[Done] => Unit): Sink[T, NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nSink.onComplete(akka.japi.function.Procedure)\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Logging in Akka\nDESCRIPTION: Configuration snippet showing how to enable debug level logging in an Akka actor system by modifying the loglevel setting in the configuration file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/additional/faq.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka.loglevel = DEBUG\n```\n\n----------------------------------------\n\nTITLE: Configuring LevelDB Journal Compaction (HOCON)\nDESCRIPTION: Provides an example configuration for tuning the LevelDB journal compaction intervals using `akka.persistence.journal.leveldb.compaction-intervals`. This feature helps manage journal file size by potentially removing deleted entries marked with tombstones, addressing issues with continuously growing journals due to LevelDB's delete mechanism.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_5\n\nLANGUAGE: hocon\nCODE:\n```\n# Assuming the snippet configures compaction intervals\nakka.persistence.journal.leveldb {\n  # Automatically run compaction potentially deleting tombstones every interval.\n  compaction-intervals {\n    # Set interval to 0 to disable automatic compaction.\n    # default: 10m (10 minutes)\n    # default-compaction-<pid> = 10m\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Java idleTimeout API Definition\nDESCRIPTION: Java API signature for idleTimeout operator that can be applied to Source and Flow types. Takes a java.time.Duration parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/idleTimeout.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nidleTimeout(java.time.Duration)\n```\n\n----------------------------------------\n\nTITLE: Skipping Deleted Events in Akka Persistence Serializer (Scala)\nDESCRIPTION: Scala implementation of a SerializerWithStringManifest that skips deserialization of deleted event types in Akka Persistence. This optimizes recovery by avoiding unnecessary deserialization of removed events.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\nclass EventDeserializationSkipped(val manifest: String)\n\nclass RemovedEventsAwareSerializer extends SerializerWithStringManifest {\n  val removedTypes = Set(\"customer-blinked\", \"customer-ignored\")\n\n  override def manifest(o: AnyRef): String = o.getClass.getName\n\n  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef = {\n    manifest match {\n      case removed if removedTypes.contains(removed) =>\n        new EventDeserializationSkipped(manifest)\n      case other =>\n        // actual deserialization\n        ...\n    }\n  }\n\n  override def toBinary(o: AnyRef): Array[Byte] = ...\n}\n```\n\n----------------------------------------\n\nTITLE: Log Capturing with JUnit\nDESCRIPTION: Demonstrates how to use the LogCapturing utility with JUnit to buffer log events and flush them only on test failure.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [LogCapturingExampleTest.java](/akka-actor-testkit-typed/src/test/java/jdocs/akka/actor/testkit/typed/javadsl/LogCapturingExampleTest.java) { #log-capturing }\n```\n\n----------------------------------------\n\nTITLE: Generating Markdown Index with TOC\nDESCRIPTION: Markdown code that generates a table of contents and index for Akka utility documentation sections using special directives.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-utilities.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Utilities\n\n@@toc { depth=2 }\n\n@@@ index\n\n* [event-bus](typed/event-stream.md)\n* [logging](typed/logging.md)\n* [common/circuitbreaker](common/circuitbreaker.md)\n* [futures](futures.md)\n* [extensions](typed/extending.md) \n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Implementing Source.unfoldResource with Database API\nDESCRIPTION: Example showing how to safely wrap the blocking database API into an Akka Stream source using Source.unfoldResource.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldResource.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval queryString = \"select * from people\"\nSource.unfoldResource[String, BlockingDatabaseApi](\n  create = () => db.query(queryString),\n  read = api => if (api.hasMore) Some(api.nextRow()) else None,\n  close = _.close()\n)\n```\n\nLANGUAGE: java\nCODE:\n```\nString queryString = \"select * from people\";\nSource.unfoldResource(\n    () -> db.query(queryString),\n    api -> {\n      if (api.hasMore()) {\n        return Optional.of(api.nextRow());\n      } else {\n        return Optional.empty();\n      }\n    },\n    BlockingDatabaseApi::close\n);\n```\n\n----------------------------------------\n\nTITLE: Using Sink.lastOption in Java\nDESCRIPTION: Example showing how to use the lastOption sink operator in Java to retrieve the last element of a stream within a CompletionStage<Optional<T>>.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/lastOption.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#lastOption-operator-example\nSource<Integer, NotUsed> source = Source.from(Arrays.asList(1, 2, 3));\nCompletionStage<Optional<Integer>> result = source.runWith(Sink.lastOption(), system);\n// result will be completed with Optional.of(3)\n```\n\n----------------------------------------\n\nTITLE: Referencing Akka Stream Sink Package Paths\nDESCRIPTION: Shows the package paths for accessing built-in Akka Stream sinks in both Scala and Java DSLs. The Scala DSL uses akka.stream.scaladsl.Sink while the Java DSL uses akka.stream.javadsl.Sink.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/categories/sink-operators.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nakka.stream.scaladsl.Sink\n```\n\nLANGUAGE: java\nCODE:\n```\nakka.stream.javadsl.Sink\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Element Source - Java\nDESCRIPTION: Example showing how to create a Source that emits a single element once using Akka Streams in Java. The source completes immediately after emitting the single value.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/single.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport akka.stream.javadsl.Source;\n\nSource.single(42)\n```\n\n----------------------------------------\n\nTITLE: Not Recommended Usage of lazySource with Queue Sink in Java\nDESCRIPTION: Java example showing how using lazySource with Sink.queue may not achieve expected lazy initialization due to queue's immediate buffering behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazySource.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#not-a-good-example\n```\n\n----------------------------------------\n\nTITLE: Defining Auction Events in Scala\nDESCRIPTION: Event definitions that represent state changes in the auction system including bid placement and auction completion.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/replicated-eventsourcing-auction.md#2025-04-22_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nsealed trait Event\nfinal case class BidRegistered(offer: Int, timestamp: Long, fromReplica: String) extends Event\nfinal case class AuctionFinished(winner: Bid, highestCounterOffer: Int) extends Event\n```\n\n----------------------------------------\n\nTITLE: Handling Snapshot Offers in Java\nDESCRIPTION: Demonstrates snapshot restoration handling in Java implementation\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_29\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic Receive createReceiveRecover() {\n  return receiveBuilder()\n      .match(SnapshotOffer.class, s -> state = (ExampleState) s.snapshot())\n      .match(Event.class, this::updateState)\n      .build();\n}\n```\n\n----------------------------------------\n\nTITLE: Command Class Example in Scala\nDESCRIPTION: Example of a command class containing an ActorRef for replies in Scala. Shows how to structure commands with reply channels.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence.md#2025-04-22_snippet_28\n\nLANGUAGE: Scala\nCODE:\n```\n#reply-command\n```\n\n----------------------------------------\n\nTITLE: Flow Prematerialization API - Java\nDESCRIPTION: Java API signatures for prematerializing a Flow using either a ClassicActorSystemProvider or Materializer.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/preMaterialize.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npreMaterialize(akka.actor.ClassicActorSystemProvider)\npreMaterialize(akka.stream.Materializer)\n```\n\n----------------------------------------\n\nTITLE: Configuring sbt-pgp for GPG Setup in Scala\nDESCRIPTION: These sbt commands set up GPG for signing releases, including generating a new key and verifying the setup.\nSOURCE: https://github.com/akka/akka/blob/main/RELEASING.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nsbt> set pgpReadOnly := false\nsbt> pgp-cmd gen-key\n```\n\n----------------------------------------\n\nTITLE: Scala Flow.keepAlive API Signature\nDESCRIPTION: Scala API signature for the keepAlive operator on Flow, taking maxIdle duration and element injection function parameters\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/keepAlive.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nkeepAlive[U>:Out](maxIdle:scala.concurrent.duration.FiniteDuration,injectedElem:()=>U):FlowOps.this.Repr[U]\n```\n\n----------------------------------------\n\nTITLE: Flow fromMaterializer API Definition in Scala\nDESCRIPTION: Scala API signature for Flow.fromMaterializer that takes a factory function accepting Materializer and Attributes to create a Flow\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/fromMaterializer.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nfromMaterializer[T,U,M](factory:(akka.stream.Materializer,akka.stream.Attributes)=>akka.stream.scaladsl.Flow[T,U,M]):akka.stream.scaladsl.Flow[T,U,scala.concurrent.Future[M]]\n```\n\n----------------------------------------\n\nTITLE: RestartSink.withBackoff Signature in Scala and Java\nDESCRIPTION: The method signature for RestartSink.withBackoff in both Scala and Java. It takes RestartSettings and a factory function to create the Sink to be wrapped.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartSink/withBackoff.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nRestartSink.withBackoff[T](settings: akka.stream.RestartSettings)(sinkFactory: () => akka.stream.scaladsl.Sink[T,_]): akka.stream.scaladsl.Sink[T,akka.NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nRestartSink.withBackoff(akka.stream.RestartSettings, akka.japi.function.Creator)\n```\n\n----------------------------------------\n\nTITLE: Setting Up TestKit Environment\nDESCRIPTION: Shows the setup of testing utilities including TestProbe for testing actor interactions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FaultHandlingDocSpec.scala](/akka-docs/src/test/scala/docs/actor/FaultHandlingDocSpec.scala) { #testkit }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FaultHandlingTest.java](/akka-docs/src/test/java/jdocs/actor/FaultHandlingTest.java) { #testkit }\n```\n\n----------------------------------------\n\nTITLE: Connected UDP Implementation\nDESCRIPTION: Demonstrates connection-based UDP implementation that can only send to and receive from a specific remote address. Offers better performance when SecurityManager is enabled.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-udp.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nclass Connected extends Actor {\n  IO(Udp) ! Connect(self, remote)\n  def receive = {\n    case Connected =>\n      context.become(ready(sender()))\n  }\n  def ready(connection: ActorRef): Receive = {\n    case data: ByteString =>\n      connection ! Send(data)\n    case Received(data) =>\n      // process data...\n    case CommandFailed(w: Send) =>\n      // handle failed send\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Histogram Aggregation with Fold in Java\nDESCRIPTION: This snippet shows how to use the fold operator in Java to create a histogram from incoming stream values. It uses a HashMap to store the counts of each unique value in the stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/fold.md#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nSource.range(1, 10)\n    .fold(\n        new HashMap<Integer, Integer>(),\n        (map, next) -> {\n          map.put(next, map.getOrDefault(next, 0) + 1);\n          return map;\n        })\n    .runWith(Sink.head(), mat);\n```\n\n----------------------------------------\n\nTITLE: Signature of FileIO.fromFile in Scala and Java\nDESCRIPTION: The API documentation signature for the FileIO.fromFile operator in both Scala and Java. It shows the method parameters and return types for different overloads.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/FileIO/fromFile.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFileIO.fromFile(f:java.io.File,chunkSize:Int):akka.stream.scaladsl.Source[akka.util.ByteString,scala.concurrent.Future[akka.stream.IOResult]]\n```\n\nLANGUAGE: java\nCODE:\n```\nFileIO.fromFile(java.io.File)\n```\n\nLANGUAGE: java\nCODE:\n```\nFileIO.fromFile(java.io.File,int)\n```\n\n----------------------------------------\n\nTITLE: Flow Prematerialization API - Scala\nDESCRIPTION: Scala API signature for prematerializing a Flow, returning a tuple of materialized value and new Flow with NotUsed type parameter.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/preMaterialize.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\npreMaterialize()(implicit materializer: akka.stream.Materializer): (Mat, akka.stream.scaladsl.Flow[Int,Out,akka.NotUsed])\n```\n\n----------------------------------------\n\nTITLE: Initializing Sink.fromSubscriber in Scala\nDESCRIPTION: Creates a Sink from a Reactive Streams Subscriber in Scala. The method takes a subscriber of type T and returns a Sink[T, akka.NotUsed].\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/fromSubscriber.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSink.fromSubscriber[T](subscriber:org.reactivestreams.Subscriber[T]):akka.stream.scaladsl.Sink[T,akka.NotUsed]\n```\n\n----------------------------------------\n\nTITLE: Reactive Streams Semantics Documentation\nDESCRIPTION: Documentation block describing the reactive streams behavior of the lazilyAsync operator, specifically when it emits values and completes.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/lazilyAsync.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@@div { .callout }\n\n**emits** the future completes\n\n**completes** after the future has completed\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Actor Polling with Tick in Java\nDESCRIPTION: Demonstrates periodic actor querying using Source.tick in Java with ask pattern.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/tick.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSource<Response, NotUsed> queryActor(ActorRef<Query> actor) {\n  return Source.tick(Duration.ZERO, Duration.ofSeconds(1), \"tick\")\n      .mapAsync(1, tick -> Patterns.ask(actor, Query::new, Duration.ofSeconds(3), system.scheduler())\n          .thenApply(Response.class::cast));\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing ByteBufferSerializer with String Manifest in Scala\nDESCRIPTION: This snippet demonstrates how to implement a ByteBufferSerializer with a string manifest in Scala, including delegation to array-based methods for compatibility with non-ByteBuffer scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nclass MySerializer extends ByteBufferSerializer with SerializerWithStringManifest {\n  private val UTF_8 = StandardCharsets.UTF_8\n\n  override def manifest(o: AnyRef): String = \"[class manifest]\"\n\n  // implement ByteBufferSerializer methods here\n\n  override def toBinary(o: AnyRef): Array[Byte] = {\n    val buf = ByteBuffer.allocate(1024) // if you know the size you can use allocate\n    toBinary(o, buf)\n    ByteBufferSerializer.getBytes(buf)\n  }\n\n  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =\n    fromBinary(ByteBuffer.wrap(bytes), manifest)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing RequiresMessageQueue in Scala\nDESCRIPTION: Scala actor implementation that requires a specific mailbox type by extending the RequiresMessageQueue trait. This enforces that the actor will use a mailbox with the specified characteristics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/mailboxes.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nimport akka.actor.Actor\nimport akka.dispatch.RequiresMessageQueue\nimport akka.dispatch.BoundedMessageQueueSemantics\n\nclass MyBoundedActor extends Actor with RequiresMessageQueue[BoundedMessageQueueSemantics] {\n  def receive = {\n    case x => // Do something\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using LoggerFactory in Scala\nDESCRIPTION: Demonstrates how to use LoggerFactory to obtain a logger outside of an actor context in Scala. This is useful for logging in non-actor classes or in Future callbacks.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nimport org.slf4j.LoggerFactory\n\nclass MyClass {\n  private val log = LoggerFactory.getLogger(this.getClass)\n\n  def doSomething(msg: String): Unit = {\n    log.debug(\"Doing something with {}\", msg)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Child Actor Implementation\nDESCRIPTION: Demonstrates the implementation of a child actor that will be supervised and tested for different failure scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/fault-tolerance.md#2025-04-22_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n@@snip [FaultHandlingDocSpec.scala](/akka-docs/src/test/scala/docs/actor/FaultHandlingDocSpec.scala) { #child }\n```\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [FaultHandlingTest.java](/akka-docs/src/test/java/jdocs/actor/FaultHandlingTest.java) { #child }\n```\n\n----------------------------------------\n\nTITLE: Skipping Snapshot Loading During Recovery in Akka Persistence (Java)\nDESCRIPTION: Customizes the recovery process within an AbstractPersistentActor to skip loading from snapshots and replay all journaled events. This is done by overriding the `recovery` method to return a `Recovery` object configured using `SnapshotSelectionCriteria.none()`. This can be useful when snapshot formats are incompatible, but caution is advised if events might have been deleted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic Recovery recovery() {\n  return Recovery.create(SnapshotSelectionCriteria.none());\n}\n```\n\n----------------------------------------\n\nTITLE: Event Tagger Implementation in Scala\nDESCRIPTION: Shows implementation of an event adapter that wraps events with tags for event querying.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-query-leveldb.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nclass MyTaggingEventAdapter extends WriteEventAdapter {\n  override def toJournal(event: Any): Any = event match {\n    case OrderPlaced(orderId) => Tagged(event, Set(\"order\"))\n    case OrderBilled(orderId) => Tagged(event, Set(\"bill\"))\n    case _ => event\n  }\n  def manifest(event: Any): String = \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding SSL Config Core Dependency\nDESCRIPTION: Maven/sbt/Gradle dependency configuration for adding ssl-config-core when needed for older Akka HTTP versions.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guide-2.8.x-2.9.x.md#2025-04-22_snippet_1\n\nLANGUAGE: markup\nCODE:\n```\ngroup=com.typesafe\nartifact=ssl-config-core_$scala.binary.version$\nversion=\"0.6.1\"\n```\n\n----------------------------------------\n\nTITLE: Source.never API Signatures\nDESCRIPTION: API signatures for the Source.never operator in both Scala and Java. This operator creates a source that never emits any elements and never terminates.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/never.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nSource.never[T]: Source[T, NotUsed]\n```\n\nLANGUAGE: java\nCODE:\n```\nSource.never()\n```\n\n----------------------------------------\n\nTITLE: Common Chained Effects in Scala\nDESCRIPTION: Example of reusable side effects implementation in Scala using thenRun method for multiple commands.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\n#commonChainedEffects\n```\n\n----------------------------------------\n\nTITLE: Running Multi-JVM Sample Test\nDESCRIPTION: Command output when running a multi-JVM test, showing how three JVMs are spawned for each node in the test.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_8\n\nLANGUAGE: none\nCODE:\n```\n> multi-jvm:run sample.Sample\n...\n[info] * sample.Sample\n[JVM-1] Hello from node 1\n[JVM-2] Hello from node 2\n[JVM-3] Hello from node 3\n[success] Total time: ...\n```\n\n----------------------------------------\n\nTITLE: Implementing ByteBufferSerializer for Akka Remote Serialization in Java\nDESCRIPTION: This code snippet shows the ByteBufferSerializer interface in Java for Akka remote serialization. It allows direct writing into a shared ByteBuffer for improved performance in high-throughput messaging scenarios.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\npublic interface ByteBufferSerializer {\n  void toBinary(Object o, ByteBuffer buf);\n  Object fromBinary(ByteBuffer buf, String manifest);\n}\n```\n\n----------------------------------------\n\nTITLE: Sink.setup API Signatures\nDESCRIPTION: The Sink.setup operator signatures for both Scala and Java APIs, allowing deferred Sink creation with access to ActorMaterializer and Attributes. Returns a Sink[T, Future[M]].\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/setup.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nsetup[T,M](factory:(akka.stream.ActorMaterializer,akka.stream.Attributes)=>akka.stream.scaladsl.Sink[T,M]):akka.stream.scaladsl.Sink[T,scala.concurrent.Future[M]]\n```\n\nLANGUAGE: java\nCODE:\n```\nsetup(java.util.function.BiFunction)\n```\n\n----------------------------------------\n\nTITLE: Using Sink.ignore with Database Operations in Scala\nDESCRIPTION: Example showing how to use Sink.ignore in Scala when reading lines from a file, saving them to a database, and storing the database identifiers in another file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Sink/ignore.md#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n// #ignore\nval fileSource = FileIO.fromPath(Paths.get(\"lines.txt\"))\n  .via(Framing.delimiter(ByteString(\"\\n\"), 1024))\n  .map(_.utf8String.trim)\n\nval databaseFlow = Flow[String]\n  .mapAsync(4)(line => saveToDatabase(line))\n\nval idsFileSink = Flow[Int]\n  .map(id => ByteString(id.toString + \"\\n\"))\n  .to(FileIO.toPath(Paths.get(\"ids.txt\")))\n\n// run stream and ignore the outcome\nfileSource.via(databaseFlow).to(Sink.ignore).run()\n\n// or alternatively\nfileSource.via(databaseFlow).toMat(idsFileSink)(Keep.right).run()\n// #ignore\n```\n\n----------------------------------------\n\nTITLE: Reactive Streams Behavior Documentation in Markdown\nDESCRIPTION: Documents the reactive streams behavior of the operator, specifically its emission and completion semantics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/fromSourceCompletionStage.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n**emits** the next value from the asynchronous source, once its *completion operator* has completed\n\n**completes** after the asynchronous source completes\n```\n\n----------------------------------------\n\nTITLE: Configuring Logback Encoder with All MDC Properties\nDESCRIPTION: XML configuration for a Logback encoder that includes all MDC properties in the log pattern using %mdc. This allows for flexible inclusion of all available MDC data.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/logging.md#2025-04-22_snippet_14\n\nLANGUAGE: xml\nCODE:\n```\n  <encoder>\n    <pattern>%date{ISO8601} %-5level %logger{36} - %msg MDC: {%mdc}%n</pattern>\n  </encoder>\n```\n\n----------------------------------------\n\nTITLE: Handling Recovery Completion Event in Akka Persistence (Java)\nDESCRIPTION: Demonstrates how to perform actions upon the completion of the recovery process in an AbstractPersistentActor. This involves matching the `RecoveryCompleted` message within the `createReceiveRecover` method's builder. This allows for initialization or logging after state restoration but before processing live messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_14\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic Receive createReceiveRecover() {\n  return receiveBuilder()\n      .match(Evt.class, this::updateState)\n      .match(SnapshotOffer.class, ss -> state = (State) ss.snapshot())\n      .match(RecoveryCompleted.class, this::onRecoveryComplete)\n      .build();\n}\n\nprivate void onRecoveryComplete(RecoveryCompleted completed) {\n  log().info(\n      \"MyPersistentActor [{}] recovery completed. Current state: [{}]\", persistenceId(), state);\n}\n```\n\n----------------------------------------\n\nTITLE: Restarting Source with Kill Switch in Java\nDESCRIPTION: This example demonstrates how to stop the restarting of a source using a kill switch. The kill switch is inserted right after the restart source. The inner source emits 3 elements and then fails. A killswitch is used to be able to stop the source from being restarted.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/RestartSource/onFailuresWithBackoff.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nPair<UniqueKillSwitch, CompletionStage<Done>> stream =\n    RestartSource.onFailuresWithBackoff(\n            Duration.ofSeconds(1),\n            Duration.ofSeconds(10),\n            0.2,\n            () ->\n                Source.from(Arrays.asList(1, 2, 3, 4))\n                    .map(\n                        n -> {\n                          if (n == 4) {\n                            throw new RuntimeException(\"Boom!\");\n                          } else {\n                            System.out.println(n);\n                            return n;\n                          }\n                        }))\n        .viaMat(KillSwitches.single(), Keep.right())\n        .toMat(Sink.ignore(), Keep.both())\n        .run(system);\n\n// Stop the restarts:\nstream.first().shutdown();\n```\n\n----------------------------------------\n\nTITLE: Testing Configuration for PersistenceTestKit\nDESCRIPTION: Configuration snippet for setting up the test environment with PersistenceTestKit plugin\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/persistence-testing.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n\"akka.persistence.journal.plugin\" = \"akka.persistence.journal.test\"\n\"akka.persistence.journal.test\" = \"akka.persistence.testkit.internal.journal.TestJournal\"\n```\n\n----------------------------------------\n\nTITLE: Running Single Multi-JVM Test\nDESCRIPTION: Commands to run a specific multi-jvm test for Akka cluster module using sbt.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsbt\nproject akka-cluster\nMultiJvm/testOnly akka.cluster.SunnyWeather\n```\n\n----------------------------------------\n\nTITLE: Configuring Serialization Bindings\nDESCRIPTION: Configuration showing how to map specific classes to serializer implementations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nakka {\n  actor {\n    serialization-bindings {\n      \"com.myproject.MySerializable\" = myown\n      \"java.lang.String\" = java\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling Akka Core Modules with SBT\nDESCRIPTION: Basic sbt command to compile all Akka core modules from the command line.\nSOURCE: https://github.com/akka/akka/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsbt compile\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Contents Include\nDESCRIPTION: Markdown include directive that imports actor API documentation from an external file.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-cluster.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@include[includes.md](includes.md) { #actor-api }\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository Access\nDESCRIPTION: Specifies the Akka library repository URL required to fetch Akka dependencies. Configuration examples are provided for sbt, Maven, and Gradle build tools.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/cluster-sharding.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\nrepositories {\n  maven {\n    url = uri(\"https://repo.akka.io/maven\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using initialDelay in Akka Streams (Scala)\nDESCRIPTION: API signature for the initialDelay operator in Scala, which takes a FiniteDuration parameter to delay the initial element in a Source or Flow.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/initialDelay.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n#initialDelay(delay:scala.concurrent.duration.FiniteDuration):FlowOps.this.Repr[Out]\n```\n\n----------------------------------------\n\nTITLE: Implementing Stats Service in Java\nDESCRIPTION: Java implementation of the statistics service that receives text, splits it into words, delegates counting to worker routees via a cluster-aware router.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/cluster-routing.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\npublic class StatsService extends AbstractActor {\n\n  @Override\n  public void preStart() {\n    // the service can re-used from several systems (Master/Worker)\n    context().actorOf(Props.create(StatsWorker.class), \"statsWorker\");\n  }\n\n  // This router is used both with lookup and deploy of routees. If you\n  // have a router with only lookup of routees you can use Props.empty\n  // instead of Props.create(StatsWorker.class).\n  private final ActorRef workerRouter = getContext().actorOf(\n    FromConfig.getInstance().props(Props.create(StatsWorker.class)),\n    \"workerRouter\");\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n      .match(StatsMessages.StatsJob.class, job -> {\n        if (job.getText().equals(\"\"))\n          return;\n        String[] words = job.getText().split(\"\\\\s+\");\n\n        // create actor that collects replies from workers\n        ActorRef aggregator = getContext().actorOf(\n          Props.create(StatsAggregator.class, words.length, getSender()));\n\n        // send each word to a worker\n        for (String word : words) {\n          workerRouter.tell(new StatsMessages.ProcessWord(word), aggregator);\n        }\n      })\n      .build();\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Non-typechecking Infix Operator Pattern in Scala\nDESCRIPTION: An example of a syntax that won't typecheck due to binding scope rules for wildcard parameters. This pattern should be avoided.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/style-guide.md#2025-04-22_snippet_22\n\nLANGUAGE: Scala\nCODE:\n```\nactorRef ? GetValue(_) // won't compile\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging of Dead Letters in Akka - HOCON\nDESCRIPTION: This code sample configures how Akka logs dead letters and whether to do so during shutdown. Parameters log-dead-letters and log-dead-letters-during-shutdown control the number of dead letter logs before silencing and whether dead letters are recorded while shutting down. Place in your Akka HOCON config to manage log verbosity related to undeliverable messages.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/logging.md#2025-04-22_snippet_19\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  log-dead-letters = 10\n  log-dead-letters-during-shutdown = on\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Source from CompletionStage in Java\nDESCRIPTION: Demonstrates creating an Akka Stream Source from a CompletionStage. Shows how to create a CompletionStage that completes with a string value and convert it into a Source that will emit that value when there is demand.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/completionStage.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport akka.stream.javadsl.Source;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\n\nclass SourceFromCompletionStage {\n    public static void example() {\n        final CompletionStage<String> future = CompletableFuture.completedFuture(\"hello\");\n        final Source<String, ?> source = Source.completionStage(future);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Quorum Strategy in Akka Cluster (HOCON)\nDESCRIPTION: Sets the `active-strategy` property within `akka.cluster.split-brain-resolver` to `static-quorum` in the Akka configuration. This activates the strategy where a partition survives only if it contains at least the configured `quorum-size` number of nodes; otherwise, the partition downs itself.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_4\n\nLANGUAGE: hocon\nCODE:\n```\nakka.cluster.split-brain-resolver.active-strategy=static-quorum\n```\n\n----------------------------------------\n\nTITLE: Markdown Index Structure\nDESCRIPTION: Index section defining the documentation structure and links to individual cluster-related topics.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-cluster.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n* [cluster-usage](cluster-usage.md)   \n* [cluster-routing](cluster-routing.md)\n* [cluster-singleton](cluster-singleton.md)\n* [distributed-pub-sub](distributed-pub-sub.md)\n* [cluster-sharding](cluster-sharding.md)\n* [cluster-metrics](cluster-metrics.md)\n* [distributed-data](distributed-data.md)\n* [serialization](serialization-classic.md)\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Akka Remoting with HOCON\nDESCRIPTION: Minimal configuration required to enable Akka remoting in an application.conf file. Specifies the actor provider, transport type, and network settings.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/remoting-artery.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\nakka {\n  actor {\n    # provider=remote is possible, but prefer cluster\n    provider = cluster \n  }\n  remote {\n    artery {\n      transport = tcp # See Selecting a transport below\n      canonical.hostname = \"127.0.0.1\"\n      canonical.port = 25520\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initialize Akka Actor System in Java\nDESCRIPTION: This snippet shows creating a class to start an Akka ActorSystem in a Java application, facilitating stream operations.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n@@snip [Main.java](/akka-docs/src/test/java/jdocs/stream/Main.java) { #main-app }\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency (sbt, Maven, Gradle)\nDESCRIPTION: Specifies the dependency coordinates required to include the Akka Streams module in a project using sbt, Maven, or Gradle. It utilizes the Akka Bill of Materials (BOM) for consistent version management across Akka modules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-stream\" % AkkaVersion\n)\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_${scala.binary.version}</artifactId>\n      <version>${akka.version}</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\ndependencies {\n  implementation platform(\"com.typesafe.akka:akka-bom_$scala.binary.version:$akka.version\")\n\n  implementation \"com.typesafe.akka:akka-stream_$scala.binary.version$\"\n}\n```\n\n----------------------------------------\n\nTITLE: Deprecated Props Creation Patterns in Scala\nDESCRIPTION: Shows deprecated approaches to creating Props objects in Scala that should be avoided as they can lead to race conditions and break actor encapsulation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/actors.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n// NOT RECOMMENDED within another actor:\n// encourages to close over enclosing scope\nval props7 = Props { new MyActorC(\"arg\") }\n```\n\n----------------------------------------\n\nTITLE: Stateful MapConcat Implementation Pattern in Java\nDESCRIPTION: Java implementation showing how to combine statefulMap and mapConcat to implement statefulMapConcat-like behavior.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/statefulMap.md#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nSource.from(Arrays.asList(1, 2, 3, 4, 5))\n    .statefulMap(\n        () -> 0,\n        (Integer state, Integer element) -> {\n          ArrayList<Integer> list = new ArrayList<>();\n          for (int i = 0; i < state; i++) {\n            list.add(element);\n          }\n          return new Pair<>(state + 1, list);\n        },\n        state -> Optional.empty())\n    .mapConcat(ints -> ints);\n```\n\n----------------------------------------\n\nTITLE: Applying Delay to Flow in Akka Streams (Java)\nDESCRIPTION: Applies a delay to elements in a Flow. The delay duration and overflow strategy are specified as parameters. This operator is part of the timer-driven operators in Akka Streams.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/delay.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nFlow.delay(java.time.Duration, akka.stream.DelayOverflowStrategy)\n```\n\n----------------------------------------\n\nTITLE: Project YAML Configuration\nDESCRIPTION: YAML frontmatter defining the project description for the migration guide.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guide-2.5.x-2.6.x.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nproject.description: Migrating to Akka 2.6.\n```\n\n----------------------------------------\n\nTITLE: Starting Backend Node for Transformation Cluster (Port 25251)\nDESCRIPTION: Starts a backend worker node for the `sample.cluster.transformation.App` example using SBT in a separate terminal. It specifies the role 'backend' and port 25251. Backend nodes register themselves with the Receptionist.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.transformation.App backend 25251\"\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Cluster Dependency\nDESCRIPTION: Dependency configuration for including Akka Cluster Typed module for distributed data capabilities.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_9\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=com.typesafe.akka\n  artifact=akka-cluster-typed_$scala.binary.version$\n  version=AkkaVersion\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Repository in Build Tool\nDESCRIPTION: This snippet shows how to add the Akka library repository in your build tool (sbt, Maven, Gradle) configuration to access Akka dependencies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-quickstart.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt,Maven,Gradle\nCODE:\n```\nid=\\\"akka-repository\\\"\\nname=\\\"Akka library repository\\\"\\nurl=\\\"https://repo.akka.io/maven\\\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Domain Model Class for Custom Serialization in Akka Persistence (Scala)\nDESCRIPTION: Defines a simple domain model class ('Person') in Scala, which will be used as the event payload for Akka Persistence. Required as a prerequisite for creating and binding a custom serializer. Key parameters are the case class's fields, and the output is a reusable event type. No external dependencies are required beyond Scala language support.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\ncase class Person(name: String, age: Int)\n```\n\n----------------------------------------\n\nTITLE: Starting Second Seed Node for Simple Cluster (Separate JVM)\nDESCRIPTION: Starts the second seed node for the `sample.cluster.simple.App` Akka cluster example using SBT in a separate terminal. It specifies port 25252, corresponding to the second seed node configuration. This node will join the cluster formed by the first seed node.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.simple.App 25252\"\n```\n\n----------------------------------------\n\nTITLE: Testing Scanning Classification Implementation\nDESCRIPTION: Test cases for the scanning classification implementation showing how to verify the behavior of overlapping classifiers in the EventBus.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/event-stream.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n@@snip [EventBusDocSpec.scala](/akka-docs/src/test/scala/docs/event/EventBusDocSpec.scala) { #scanning-bus-test }\n```\n\nLANGUAGE: java\nCODE:\n```\n@@snip [EventBusDocTest.java](/akka-docs/src/test/java/jdocs/event/EventBusDocTest.java) { #scanning-bus-test }\n```\n\n----------------------------------------\n\nTITLE: Starting Compute Node for Stats Cluster (Port 25251)\nDESCRIPTION: Starts a compute node for the `sample.cluster.stats.App` example using SBT in a separate terminal. It specifies the role 'compute' and port 25251. Compute nodes run the `StatsService` and `StatsWorker` actors.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.stats.App compute 25251\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Navigation Structure\nDESCRIPTION: Defines the table of contents and index structure for the security documentation using Markdown and custom Akka documentation syntax.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/security/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n@@toc { .list depth=1 }\n\n@@@ index\n\n* [2017-02-10-java-serialization](2017-02-10-java-serialization.md)\n* [2017-08-09-camel](2017-08-09-camel.md)\n* [2018-08-29-aes-rng](2018-08-29-aes-rng.md)\n\n@@@\n```\n\n----------------------------------------\n\nTITLE: Nesting and Flattening Operators Table in Markdown\nDESCRIPTION: Table documenting operators that handle nested streams, either by nesting a stream into a stream of streams or flattening nested streams into a single stream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/index.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| |Operator|Description|\n|--|--|--|\n|Source/Flow|<a name=\"flatmapconcat\"></a>@ref[flatMapConcat](Source-or-Flow/flatMapConcat.md)|Transform each input element into a `Source` whose elements are then flattened into the output stream through concatenation.|\n|Source/Flow|<a name=\"flatmapmerge\"></a>@ref[flatMapMerge](Source-or-Flow/flatMapMerge.md)|Transform each input element into a `Source` whose elements are then flattened into the output stream through merging.|\n```\n\n----------------------------------------\n\nTITLE: Importing Serialization Dependencies - Java\nDESCRIPTION: Required imports for implementing ActorRef serialization in Java\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/serialization-classic.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n#imports\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Maven Repository - sbt, Maven, Gradle\nDESCRIPTION: This configuration snippet sets up the Akka library repository URL for build tools such as sbt, Maven, or Gradle. Required for fetching Akka dependencies, it must be added to the project build configuration file. The core parameter is the repository URL ('https://repo.akka.io/maven'). It does not handle dependency resolution on its own but enables access to any Akka artifacts published in this repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/actor-discovery.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nid=\\\"akka-repository\\\"\\nname=\\\"Akka library repository\\\"\\nurl=\\\"https://repo.akka.io/maven\\\"\n```\n\n----------------------------------------\n\nTITLE: Running AppOneMaster Compute Node on Port 25251 using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.AppOneMaster` main class using Maven's exec plugin in a separate process. This command starts one node of the Akka cluster sample application (Cluster Singleton example) configured as a 'compute' node listening on port 25251.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.AppOneMaster\" -Dexec.args=\"compute 25251\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Recovery Completely in Akka Persistence (Scala)\nDESCRIPTION: Disables the entire recovery process for a PersistentActor. This is achieved by overriding the `recovery` method to return `Recovery.none()`. When recovery is disabled, the actor will always start with an initial, empty state, ignoring any previously persisted events or snapshots.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\noverride def recovery: Recovery = Recovery.none\n```\n\n----------------------------------------\n\nTITLE: Running Stats Cluster Example (Single JVM)\nDESCRIPTION: Executes the main class `sample.cluster.stats.App` using SBT. This command starts four actor systems (compute nodes and client nodes) within the same JVM process to demonstrate cluster-aware routing for a text statistics calculation service.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.stats.App\"\n```\n\n----------------------------------------\n\nTITLE: Cluster Router Documentation\nDESCRIPTION: Documentation explaining cluster-aware routers for distributing messages across cluster nodes using various routing strategies.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/includes/cluster.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n### Cluster aware routers\n\nDistribute messages to actors on different nodes in the cluster with routing strategies\nlike round-robin and consistent hashing.\n```\n\n----------------------------------------\n\nTITLE: Time-Aware Operators Table in Markdown\nDESCRIPTION: Table documenting operators that consider time in their operations, handling timeouts and keeping streams alive.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/index.md#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n| |Operator|Description|\n|--|--|--|\n|Source/Flow|<a name=\"backpressuretimeout\"></a>@ref[backpressureTimeout](Source-or-Flow/backpressureTimeout.md)|If the time between the emission of an element and the following downstream demand exceeds the provided timeout, the stream is failed with a `TimeoutException`.|\n|Source/Flow|<a name=\"completiontimeout\"></a>@ref[completionTimeout](Source-or-Flow/completionTimeout.md)|If the completion of the stream does not happen until the provided timeout, the stream is failed with a `TimeoutException`.|\n```\n\n----------------------------------------\n\nTITLE: Displaying Akka Classic API Documentation Note in Markdown\nDESCRIPTION: A markdown note block explaining that Akka Classic APIs are still supported but new projects should use the new typed Actor APIs. It also mentions the possibility of coexistence between classic and new APIs in the same ActorSystem.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/includes.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--- #actor-api --->\n@@@ note\n\nAkka Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. \nAkka Classic is still fully supported and existing applications can continue to use the classic APIs. It is also\npossible to use the new Actor APIs together with classic actors in the same ActorSystem, see @ref:[coexistence](typed/coexisting.md).\nFor new projects we recommend using @ref:[the new Actor API](typed/actors.md).\n       \n@@@\n<!--- #actor-api --->\n```\n\n----------------------------------------\n\nTITLE: Running StatsSample Client Node on Dynamic Port using Maven (Shell)\nDESCRIPTION: Executes the `sample.cluster.stats.App` main class using Maven's exec plugin in a separate process. This command starts a 'client' node for the Akka cluster sample application, allowing the system to assign a dynamic port (port 0). This node likely sends requests to the compute nodes in the multi-process group router example.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-java/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nmvn exec:java -Dexec.mainClass=\"sample.cluster.stats.App\" -Dexec.args=\"client 0\"\n```\n\n----------------------------------------\n\nTITLE: Flow.recoverWith API Signature in Scala and Java\nDESCRIPTION: The API signature for the recoverWith operator on Flow, which allows switching to an alternative Source when a failure occurs upstream.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source-or-Flow/recoverWith.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nrecoverWith[T>:Out](pf:PartialFunction[Throwable,akka.stream.Graph[akka.stream.SourceShape[T],akka.NotUsed]]):FlowOps.this.Repr[T]\n```\n\nLANGUAGE: java\nCODE:\n```\nrecoverWith(java.lang.Class,java.util.function.Supplier)\n```\n\n----------------------------------------\n\nTITLE: Running Transformation Cluster Example (Single JVM)\nDESCRIPTION: Executes the main class `sample.cluster.transformation.App` using SBT. This command starts five actor systems (both backend workers and frontend nodes) within the same JVM process to demonstrate worker registration and discovery using the Akka Receptionist.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.transformation.App\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CapturingAppender in Logback\nDESCRIPTION: XML configuration for setting up the CapturingAppender and CapturingAppenderDelegate in logback-test.xml for log capturing in tests.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/testing-async.md#2025-04-22_snippet_26\n\nLANGUAGE: XML\nCODE:\n```\n@@snip [logback-test.xml](/akka-actor-typed-tests/src/test/resources/logback-doc-test.xml)\n```\n\n----------------------------------------\n\nTITLE: Adding Akka Streams Dependency (sbt, Maven, Gradle)\nDESCRIPTION: Declares the necessary dependency (`com.typesafe.akka:akka-stream`) to include the Akka Streams library in a project. It uses the Akka Bill of Materials (BOM) via `akka-bom` for consistent version management across Akka modules.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-composition.md#2025-04-22_snippet_1\n\nLANGUAGE: sbt\nCODE:\n```\nval AkkaVersion = \"$akka.version$\"\nlibraryDependencies ++= Seq(\n  \"com.typesafe.akka\" %% \"akka-stream\" % AkkaVersion\n)\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<properties>\n  <scala.binary.version>$scala.binary.version$</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.typesafe.akka</groupId>\n      <artifactId>akka-bom_${scala.binary.version}</artifactId>\n      <version>$akka.version$</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>com.typesafe.akka</groupId>\n    <artifactId>akka-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\ndef versions = [\n  ScalaBinary: \"$scala.binary.version$\"\n]\ndef akkaVersion = \"$akka.version$\"\n\ndependencies {\n  implementation platform(\"com.typesafe.akka:akka-bom_${versions.ScalaBinary}:\" + akkaVersion)\n\n  implementation \"com.typesafe.akka:akka-stream_${versions.ScalaBinary}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Akka Persistence Dependencies\nDESCRIPTION: Dependency configuration for Akka Persistence and test modules using version variables\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-schema-evolution.md#2025-04-22_snippet_1\n\nLANGUAGE: markup\nCODE:\n```\n@@dependency[sbt,Maven,Gradle] {\n  bomGroup=com.typesafe.akka bomArtifact=akka-bom_$scala.binary.version$ bomVersionSymbols=AkkaVersion\n  symbol1=AkkaVersion\n  value1=\"$akka.version$\"\n  group=\"com.typesafe.akka\"\n  artifact=\"akka-persistence_$scala.binary.version$\"\n  version=AkkaVersion\n  group2=\"com.typesafe.akka\"\n  artifact2=\"akka-persistence-testkit_$scala.binary.version$\"\n  version2=AkkaVersion\n  scope2=test\n}\n```\n\n----------------------------------------\n\nTITLE: Starting First Seed Node for Simple Cluster (Separate JVM)\nDESCRIPTION: Starts the first seed node for the `sample.cluster.simple.App` Akka cluster example using SBT in a separate terminal. It specifies port 25251, corresponding to the first seed node configuration in `application.conf`. This allows running cluster members in distinct processes.\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-cluster-scala/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"runMain sample.cluster.simple.App 25251\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Down-All-When-Unstable for Akka Split Brain Resolver\nDESCRIPTION: Configuration example for the down-all-when-unstable feature in Akka's Split Brain Resolver. This setting determines how long to wait after stability timeout before downing all nodes when the cluster remains unstable.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/split-brain-resolver.md#2025-04-22_snippet_9\n\nLANGUAGE: conf\nCODE:\n```\nakka.cluster.split-brain-resolver {\n  down-all-when-unstable = 15s\n  stable-after = 20s\n}\n```\n\n----------------------------------------\n\nTITLE: Starting the Kafka Producer via sbt (Shell)\nDESCRIPTION: This command uses sbt to run the `producer` sub-project. This application connects to the Kafka broker and starts sending sample user events to the `user-events` topic. These messages will then be consumed by the running processor node(s).\nSOURCE: https://github.com/akka/akka/blob/main/samples/akka-sample-kafka-to-sharding-scala/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nsbt \"producer / run\"\n```\n\n----------------------------------------\n\nTITLE: Creating Gzip Compression Flow in Akka - Java\nDESCRIPTION: This snippet demonstrates how to create a flow that gzip-compresses a stream of ByteStrings in Akka Streams using Java. Like the Scala version, this operation flushes after processing every ByteString to ensure independent decompression capabilities at the cost of compression efficiency for small chunks. Reactive stream behaviors such as emissions and backpressure are described.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Compression/gzip.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\njava=\\\"#gzip()\\\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository\nDESCRIPTION: Repository configuration block for accessing Akka's library repository across different build tools (sbt, Maven, Gradle).\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-utilities-classic.md#2025-04-22_snippet_0\n\nLANGUAGE: markup\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Secure Random Number Generator in Akka TLS\nDESCRIPTION: Configuration snippet showing how to explicitly set the SecureRandom RNG for both classic and Artery remoting TLS in Akka. This is the recommended secure configuration.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/security/2018-08-29-aes-rng.md#2025-04-22_snippet_0\n\nLANGUAGE: hocon\nCODE:\n```\n# Set `SecureRandom` RNG explicitly (but it is also the default)\nakka.remote.classic.netty.ssl.random-number-generator = \"SecureRandom\"\nakka.remote.artery.ssl.config-ssl-engine.random-number-generator = \"SecureRandom\"\n```\n\n----------------------------------------\n\nTITLE: Injecting Shared LevelDB Store Reference (Scala - Deprecated)\nDESCRIPTION: Demonstrates how to inject the `ActorRef` of the running `SharedLeveldbStore` actor into the shared LevelDB journal plugin using `SharedLeveldbJournal.setStore` in Scala. This initialization step is required before the deprecated shared journal plugin can be used.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence-plugins.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n// Assuming the snippet injects the store reference\nimport akka.persistence.journal.leveldb.SharedLeveldbJournal\nimport akka.actor.{ ActorRef, ActorSystem }\n\nval system: ActorSystem = ???\nval store: ActorRef = ??? // The SharedLeveldbStore actor reference\nSharedLeveldbJournal.setStore(store, system)\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository (sbt, Maven, Gradle)\nDESCRIPTION: Defines the repository details needed to fetch Akka dependencies. This configuration specifies the ID, name, and URL for the Akka library repository, required for build tools like sbt, Maven, and Gradle.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/stream-substream.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\nresolvers += \"Akka library repository\" at \"https://repo.akka.io/maven\"\n```\n\nLANGUAGE: Maven\nCODE:\n```\n<repositories>\n  <repository>\n    <id>akka-repository</id>\n    <name>Akka library repository</name>\n    <url>https://repo.akka.io/maven</url>\n  </repository>\n</repositories>\n```\n\nLANGUAGE: Gradle\nCODE:\n```\nrepositories {\n  maven {\n    url \"https://repo.akka.io/maven\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Specific Multi-JVM Tests\nDESCRIPTION: Command for running individual multi-JVM tests using the testOnly task with a specific test name.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/multi-jvm-testing.md#2025-04-22_snippet_4\n\nLANGUAGE: none\nCODE:\n```\nmulti-jvm:testOnly akka.remote.RandomRoutedRemoteActor\n```\n\n----------------------------------------\n\nTITLE: UDP Multicast Configuration\nDESCRIPTION: Shows how to configure UDP for multicast messaging using IPv6 protocol, including protocol family selection and multicast group joining.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/io-udp.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nclass Inet6ProtocolFamily extends DatagramChannelCreator {\n  override def create() =\n    DatagramChannel.open(StandardProtocolFamily.INET6)\n}\n```\n\nLANGUAGE: scala\nCODE:\n```\nclass JoinMulticastGroup(group: InetAddress, interface: NetworkInterface)\n  extends SocketOption {\n  override def afterBind(c: DatagramChannel) {\n    c.join(group, interface)\n  }\n}\n```\n\nLANGUAGE: scala\nCODE:\n```\nIO(Udp) ! Bind(self, new InetSocketAddress(\"localhost\", 0),\n  options = List(new Inet6ProtocolFamily,\n    new JoinMulticastGroup(group, interface)))\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Library Repository with sbt, Maven, or Gradle - Build System Configuration\nDESCRIPTION: Configures the custom Akka Maven repository in the build tool to enable downloading Akka dependencies. The configuration snippet should be added to your sbt, Maven, or Gradle project configuration as appropriate so that Akka modules can be resolved. This is required before specifying Akka as a dependency in your build, and the URL used must match Akka's public Maven repository.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/ActorFlow/askWithStatusAndContext.md#2025-04-22_snippet_0\n\nLANGUAGE: sbt\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\\\"akka-repository\\\"\nname=\\\"Akka library repository\\\"\nurl=\\\"https://repo.akka.io/maven\\\"\n}\n```\n\nLANGUAGE: Maven\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\\\"akka-repository\\\"\nname=\\\"Akka library repository\\\"\nurl=\\\"https://repo.akka.io/maven\\\"\n}\n```\n\nLANGUAGE: Gradle\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\\\"akka-repository\\\"\nname=\\\"Akka library repository\\\"\nurl=\\\"https://repo.akka.io/maven\\\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing UnfoldAsync Stream in Java\nDESCRIPTION: Creates a stream of ByteStrings using unfoldAsync by repeatedly requesting chunks from an actor.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/stream/operators/Source/unfoldAsync.md#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic Source<ByteString, NotUsed> createSource(ActorRef actor) {\n    return Source.unfoldAsync(\n        0L,\n        (Long offset) -> {\n          CompletionStage<Chunk> fut =\n              PatternsCS.ask(actor, new GetChunk(offset), Duration.ofSeconds(3))\n                  .thenApply(resp -> (Chunk) resp);\n\n          return fut.thenApply(\n              chunk -> {\n                if (chunk.getBytes().isEmpty())\n                  return Optional.empty();\n                else\n                  return Optional.of(\n                      new Pair<>(\n                          offset + chunk.getBytes().length(),\n                          chunk.getBytes()));\n              });\n        });\n}\n```\n\n----------------------------------------\n\nTITLE: Vulnerable Akka RNG Configurations\nDESCRIPTION: Configuration patterns that indicate vulnerable random number generator settings in Akka versions 2.5.0-2.5.15. These configurations should be avoided.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/security/2018-08-29-aes-rng.md#2025-04-22_snippet_1\n\nLANGUAGE: hocon\nCODE:\n```\nakka.remote.netty.ssl.random-number-generator = \"AES128CounterSecureRNG\"\nakka.remote.netty.ssl.random-number-generator = \"AES256CounterSecureRNG\"\nakka.remote.artery.ssl.config-ssl-engine.random-number-generator = \"AES128CounterSecureRNG\"\nakka.remote.artery.ssl.config-ssl-engine.random-number-generator = \"AES256CounterSecureRNG\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Title and Link for Akka Migration Guide\nDESCRIPTION: Markdown content containing a header and a link to the official Akka migration documentation for versions 2.4.x to 2.5.x.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/project/migration-guide-2.4.x-2.5.x.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Migration Guide 2.4.x to 2.5.x\n\nMigration from 2.4.x to 2.5.x is described in the\n[documentation of 2.5](https://doc.akka.io/libraries/akka-core/2.5/project/migration-guide-2.4.x-2.5.x.html).\n```\n\n----------------------------------------\n\nTITLE: ActorContext Access in Java\nDESCRIPTION: Demonstrates accessing ActorContext in a DurableStateBehavior using Behaviors.setup in Java.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/durable-state/persistence.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\n#actor-context\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DeadLetters in Scala\nDESCRIPTION: Scala code demonstrating how to subscribe an actor to the DeadLetter channel of the event stream. This allows monitoring messages that couldn't be delivered to their recipients.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/event-bus.md#2025-04-22_snippet_19\n\nLANGUAGE: Scala\nCODE:\n```\nval listener = system.actorOf(Props(classOf[DeadLetterListener]))\nsystem.eventStream.subscribe(listener, classOf[DeadLetter])\n```\n\n----------------------------------------\n\nTITLE: Setting Upper Replay Bound During Recovery in Akka Persistence (Java)\nDESCRIPTION: Customizes recovery by setting an upper bound on the sequence number of events to replay using `toSequenceNr` within the `recovery` method override. This replays the actor's state only up to a specific point in the past, which can be useful for debugging. It's generally a bad idea to persist new events after a partial recovery, as it might confuse later full recoveries.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/persistence.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic Recovery recovery() {\n  return Recovery.create(457L);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Akka Repository\nDESCRIPTION: Repository configuration block for accessing Akka libraries through sbt, Maven, or Gradle build systems.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/typed/guide/modules.md#2025-04-22_snippet_6\n\nLANGUAGE: markup\nCODE:\n```\n@@repository [sbt,Maven,Gradle] {\nid=\"akka-repository\"\nname=\"Akka library repository\"\nurl=\"https://repo.akka.io/maven\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a DurableStateStoreProvider in Scala\nDESCRIPTION: Implementation of the DurableStateStoreProvider in Scala, which is responsible for creating instances of the custom plugin. This provider handles the plugin instantiation and initialization.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/durable-state/state-store-plugin.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n#plugin-provider\n```\n\n----------------------------------------\n\nTITLE: Including Documentation Components in Markdown\nDESCRIPTION: Documentation structure using Markdown with special include directives and table of contents formatting for Akka Classic documentation. Uses custom include syntax and depth-based TOC generation.\nSOURCE: https://github.com/akka/akka/blob/main/akka-docs/src/main/paradox/index-classic.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Akka Classic\n\n@@include[includes.md](includes.md) { #actor-api }\n\n@@toc { depth=2 }\n\n@@@ index\n\n* [index-actors](index-actors.md)\n* [index-cluster](index-cluster.md)\n* [index-network](index-network.md)\n* [index-utilities](index-utilities-classic.md)\n\n@@@\n```"
  }
]
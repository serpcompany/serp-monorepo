[
  {
    "owner": "huggingface",
    "repo": "optimum-intel",
    "content": "TITLE: Quantize Language Model to INT8 with OpenVINO (Python)\nDESCRIPTION: This Python code snippet demonstrates how to quantize a language model to INT8 using the `OVModelForCausalLM` class and `OVWeightQuantizationConfig` from the `optimum.intel` library.  It loads the model, applies the quantization, and saves the resulting INT8 model to a specified directory, reducing its size by a factor of four compared to the FP32 version.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig\n\nmodel_id = \"helenai/gpt2-ov\"\nquantization_config = OVWeightQuantizationConfig(bits=8)\nmodel = OVModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n\n# Saves the int8 model that will be x4 smaller than its fp32 counterpart\nmodel.save_pretrained(saving_directory)\n```\n\n----------------------------------------\n\nTITLE: Data-Aware 4-bit Quantization\nDESCRIPTION: This example demonstrates how to combine multiple data-aware quantization methods (AWQ, scale estimation, GPTQ) when quantizing a model. It requires a dataset for calibration and specifies the desired methods in the `OVWeightQuantizationConfig`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nquantization_config = OVWeightQuantizationConfig(\n    bits=4,\n    sym=False,\n    ratio=0.8,\n    quant_method=\"awq\",\n    scale_estimation=True,\n    gptq=True,\n    dataset=\"wikitext2\"\n)\n```\n\n----------------------------------------\n\nTITLE: Full Quantization of DistilBERT\nDESCRIPTION: This snippet shows how to perform full quantization (weights and activations) on a fine-tuned DistilBERT model using `OVQuantizer`, `OVModelForSequenceClassification`, `OVConfig`, and `OVQuantizationConfig`. It involves creating a quantizer, defining a quantization configuration, applying quantization, and saving the quantized model and tokenizer.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\nfrom optimum.intel import OVQuantizer, OVModelForSequenceClassification, OVConfig, OVQuantizationConfig\n\nmodel_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# The directory where the quantized model will be saved\nsave_dir = \"ptq_model\"\n\nquantizer = OVQuantizer.from_pretrained(model)\n\n# Apply full quantization and export the resulting quantized model to OpenVINO IR format\nov_config = OVConfig(quantization_config=OVQuantizationConfig())\nquantizer.quantize(ov_config=ov_config, calibration_dataset=calibration_dataset, save_directory=save_dir)\n# Save the tokenizer\ntokenizer.save_pretrained(save_dir)\n```\n\n----------------------------------------\n\nTITLE: Quantize CodeBERT with NF4/F8E4M3 OpenVINO Quantization and WikiText2 Dataset (Python)\nDESCRIPTION: This Python code quantizes the microsoft/codebert-base model using a mixed quantization configuration (NF4 for weights, F8E4M3 for activations) and calibrates it with the wikitext2 dataset.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nOVModelForFeatureExtraction.from_pretrained('microsoft/codebert-base', quantization_config=OVMixedQuantizationConfig(OVWeightQuantizationConfig(bits=4, dtype='nf4'), OVQuantizationConfig(dtype='f8e4m3', dataset='wikitext2'))).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Static Quantization with Optimum-Intel API\nDESCRIPTION: Quantizes a Sentence Transformers model statically to 8-bit using the Optimum-Intel API and NNCF.  It involves loading the model and tokenizer, creating a calibration dataset, and applying quantization using an `OVQuantizer` and `OVConfig`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nimport datasets\nfrom transformers import AutoTokenizer\nfrom optimum.intel import OVModelForFeatureExtraction, OVQuantizer, OVQuantizationConfig, OVConfig\n\nMODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\nbase_model_path = \"all-MiniLM-L6-v2\"\nint8_ptq_model_path = \"all-MiniLM-L6-v2_int8\"\n\nmodel = OVModelForFeatureExtraction.from_pretrained(MODEL_ID)\nmodel.save_pretrained(base_model_path)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\ntokenizer.save_pretrained(base_model_path)\n\n\nquantizer = OVQuantizer.from_pretrained(model)\n\ndef preprocess_function(examples, tokenizer):\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=384, truncation=True)\n\n\ncalibration_dataset = quantizer.get_calibration_dataset(\n    \"glue\",\n    dataset_config_name=\"sst2\",\n    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n    num_samples=300,\n    dataset_split=\"train\",\n)\n\nov_config = OVConfig(quantization_config=OVQuantizationConfig())\n\nquantizer.quantize(ov_config=ov_config, calibration_dataset=calibration_dataset, save_directory=int8_ptq_model_path)\ntokenizer.save_pretrained(int8_ptq_model_path)\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image with OpenVINO LCM Pipeline in Python\nDESCRIPTION: Loads a Latent Consistency Model (LCM) pipeline from Hugging Face Hub, exports it to OpenVINO format, and runs inference using the OpenVINO execution provider. It requires the `optimum-intel` library.  The `from_pretrained` method loads the model and exports it. The `pipeline` object is then called with a prompt and inference parameters to generate images.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVLatentConsistencyModelPipeline\n\nmodel_id = \"SimianLuo/LCM_Dreamshaper_v7\"\npipeline = OVLatentConsistencyModelPipeline.from_pretrained(model_id, export=True)\nprompt = \"sailing ship in storm by Leonardo da Vinci\"\nimages = pipeline(prompt, num_inference_steps=4, guidance_scale=8.0).images\n```\n\n----------------------------------------\n\nTITLE: Mixed Quantization with CLI\nDESCRIPTION: This command-line instruction shows how to apply mixed quantization using the `optimum-cli` tool. It specifies the quantization mode using the `--quant-mode` argument, setting it to `nf4_f8e4m3` to combine NF4 weight quantization with f8e4m3 full quantization. The command also includes the dataset for calibration and specifies the save directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_50\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 --quant-mode nf4_f8e4m3 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Full Quantization - Text Generation - Python\nDESCRIPTION: Loads a TinyLlama model, applies full quantization with 8 bits using the wikitext2 dataset, and saves the quantized model. This uses the `OVQuantizationConfig` class.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOVModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0', quantization_config=OVQuantizationConfig(bits=8, dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-free) - Text Generation - Python\nDESCRIPTION: Loads a TinyLlama model, applies weight-only quantization with 8 bits, and saves the quantized model. This uses the `OVModelForCausalLM` class and `OVWeightQuantizationConfig` to perform the quantization. The `bits` parameter is set to 8.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOVModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0', quantization_config=OVWeightQuantizationConfig(bits=8)).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Quantized U-Net Stable Diffusion Model\nDESCRIPTION: This bash command benchmarks the performance of the Stable Diffusion model with a quantized U-Net component using `run_diffusion_post_training.py`. It specifies the model, sets the output directory, provides paths to ground truth images, input text, enables the benchmark flag, and activates int8 quantization with the `--int8` flag.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-to-image/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run_diffusion_post_training.py \\\n    --model_name_or_path sd-pokemon-diffusers \\\n    --output_dir /tmp/diffusion_output \\\n    --base_images base_images \\   # The path of the ground truth pictures\n    --input_text \"a drawing of a gray and black dragon\" \\\n    --benchmark \\\n    --int8\n```\n\n----------------------------------------\n\nTITLE: Hybrid Quantization for Stable Diffusion\nDESCRIPTION: This snippet shows how to apply hybrid quantization to a Stable Diffusion pipeline.  It uses `OVStableDiffusionPipeline.from_pretrained` with `OVWeightQuantizationConfig` to quantize the U-Net with full quantization and the remaining components with weight-only quantization.  A quantization dataset is required for hybrid quantization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionPipeline, OVWeightQuantizationConfig\n\nmodel = OVStableDiffusionPipeline.from_pretrained(\n    model_id,\n    export=True,\n    quantization_config=OVWeightQuantizationConfig(bits=8, dataset=\"conceptual_captions\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Mixed Quantization - Text Generation - CLI\nDESCRIPTION: Exports a TinyLlama model to OpenVINO format with mixed quantization (nf4_f8e4m3) using the optimum-cli. This is a data-aware quantization method, which uses the wikitext2 dataset. It quantizes weights and activations to different datatypes.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 --quant-mode nf4_f8e4m3 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Export CodeBERT Model to OpenVINO with INT8 Quantization (CLI)\nDESCRIPTION: This command uses optimum-cli to export the microsoft/codebert-base model to OpenVINO format, applying INT8 weight-only quantization and saving the converted model to the ./save_dir directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m microsoft/codebert-base --weight-format int8 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Loading a Transformers Model with OVModelForCausalLM in Python\nDESCRIPTION: Demonstrates how to load a pre-exported Transformers model using `OVModelForCausalLM` from the `optimum.intel` library for inference with OpenVINO. It replaces the standard `AutoModelForCausalLM` class from `transformers`. Requires `optimum-intel` and `transformers` libraries. The model is loaded using `from_pretrained`, a tokenizer is initialized, and a pipeline is created for text generation. The input is a text prompt, and the output is the generated text.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\n- from transformers import AutoModelForCausalLM\n+ from optimum.intel import OVModelForCausalLM\n  from transformers import AutoTokenizer, pipeline\n\n  model_id = \"helenai/gpt2-ov\"\n- model = AutoModelForCausalLM.from_pretrained(model_id)\n  # here the model was already exported so no need to set export=True\n+ model = OVModelForCausalLM.from_pretrained(model_id)\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n  results = pipe(\"He's a dreadful magician and\")\n```\n\n----------------------------------------\n\nTITLE: Quantize SentenceTransformer with NF4/F8E4M3 Quantization and WikiText2 Dataset (Python)\nDESCRIPTION: Quantizes the sentence-transformers/all-mpnet-base-v2 model to a mixed precision format (NF4 and F8E4M3) using the specified quantization configuration and the wikitext2 dataset for calibration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nOVSentenceTransformer.from_pretrained('sentence-transformers/all-mpnet-base-v2', quantization_config=OVMixedQuantizationConfig(OVWeightQuantizationConfig(bits=4, dtype='nf4'), OVQuantizationConfig(dtype='f8e4m3', dataset='wikitext2'))).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-free) - Image-Text-to-Text - Python\nDESCRIPTION: Loads an InternVL2 model, applies weight-only quantization with 4 bits, and saves the quantized model. The `trust_remote_code` flag is set to True due to custom code in the model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOVModelForVisualCausalLM.from_pretrained('OpenGVLab/InternVL2-1B', trust_remote_code=True, quantization_config=OVWeightQuantizationConfig(bits=4)).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Full Quantization - Text Generation - CLI\nDESCRIPTION: Exports a TinyLlama model to OpenVINO format with int8 full quantization using the optimum-cli. This is a data-aware quantization method, which uses the wikitext2 dataset. It quantizes both the weights and activations to 8-bit integers using a dataset.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 --quant-mode int8 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Inference with Stable Diffusion and OpenVINO\nDESCRIPTION: This snippet demonstrates how to load an OpenVINO Stable Diffusion model, compile it, and perform text-to-image inference using the `OVStableDiffusionPipeline` class from the `optimum.intel` library. It initializes the pipeline with a specified model ID, sets a prompt, and generates images. The resulting images are then saved.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionPipeline\n\nmodel_id = \"echarlaix/stable-diffusion-v1-5-openvino\"\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id)\nprompt = \"sailing ship in storm by Rembrandt\"\nimages = pipeline(prompt).images\n```\n\n----------------------------------------\n\nTITLE: Post-training Static Quantization on DistilBERT in Bash\nDESCRIPTION: This bash script applies post-training static quantization on a DistilBERT model fine-tuned on the sst-2 task. It uses the `run_glue_post_training.py` script with specific parameters to enable quantization and evaluation. The script defines the model, task, quantization approach, and output directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_glue_post_training.py \\\n    --model_name_or_path distilbert-base-uncased-finetuned-sst-2-english \\\n    --task_name sst2 \\\n    --apply_quantization \\\n    --quantization_approach static \\\n    --num_calibration_samples 50 \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/sst2_output\n```\n\n----------------------------------------\n\nTITLE: Full Model Quantization - Optimum CLI\nDESCRIPTION: Performs full model quantization, including both weights and activations, using the specified quantization mode (int8 in this example). Requires a calibration dataset (librispeech) and sets a SmoothQuant alpha value. The output model is saved in the ./whisper-large-v3-turbo directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m openai/whisper-large-v3-turbo --quant-mode int8 --dataset librispeech --num-samples 32 --smooth-quant-alpha 0.9 ./whisper-large-v3-turbo\n```\n\n----------------------------------------\n\nTITLE: 4-bit Weight Quantization with OVModelForVisualCausalLM\nDESCRIPTION: This code shows how to apply 4-bit weight quantization to a vision-language model using `OVModelForVisualCausalLM`.  It uses `OVWeightQuantizationConfig` to specify the quantization parameters. The `model_id` should be replaced with the identifier of the vision-language model you want to quantize.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nmodel = OVModelForVisualCausalLM.from_pretrained(\n    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n    quantization_config=quantization_config\n)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Quantization with Optimum CLI (Bash)\nDESCRIPTION: Applies dynamic quantization to a pre-trained model using the `optimum-cli` command. This command quantizes the specified model and saves the quantized version to the output directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli inc quantize --model distilbert-base-cased-distilled-squad --output quantized_distilbert\n```\n\n----------------------------------------\n\nTITLE: 4-bit Weight Quantization with OVModelForCausalLM\nDESCRIPTION: This snippet demonstrates how to perform 4-bit weight quantization on a causal language model using `OVModelForCausalLM` and `OVWeightQuantizationConfig` from the `optimum.intel` library.  It initializes the quantization configuration with the desired number of bits and then loads the pre-trained model with the quantization config applied.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig\n\nquantization_config = OVWeightQuantizationConfig(bits=4)\nmodel = OVModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n```\n\n----------------------------------------\n\nTITLE: Loading a Diffusers Model with OVStableDiffusionPipeline in Python\nDESCRIPTION: Illustrates loading a Stable Diffusion model using `OVStableDiffusionPipeline` from `optimum.intel` for optimized inference with OpenVINO. It replaces the standard `StableDiffusionPipeline` from `diffusers`. Requires `optimum-intel` and `diffusers` libraries. The model is loaded, a prompt is defined, and the pipeline generates images based on the prompt.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- from diffusers import StableDiffusionPipeline\n+ from optimum.intel import OVStableDiffusionPipeline\n\n  model_id = \"echarlaix/stable-diffusion-v1-5-openvino\"\n- pipeline = StableDiffusionPipeline.from_pretrained(model_id)\n+ pipeline = OVStableDiffusionPipeline.from_pretrained(model_id)\n  prompt = \"sailing ship in storm by Rembrandt\"\n  images = pipeline(prompt).images\n```\n\n----------------------------------------\n\nTITLE: Quantize SentenceTransformer to INT8 with OpenVINO Quantization and WikiText2 (Python)\nDESCRIPTION: This Python code snippet quantizes the sentence-transformers/all-mpnet-base-v2 model to INT8 precision using the OVQuantizationConfig with the wikitext2 dataset for calibration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nOVSentenceTransformer.from_pretrained('sentence-transformers/all-mpnet-base-v2', quantization_config=OVQuantizationConfig(bits=8, dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Post-Training Static Quantization for Summarization (BART Model)\nDESCRIPTION: This script demonstrates how to apply post-training static quantization on a BART model fine-tuned on the CNN/DailyMail dataset using the Intel Neural Compressor. The `quantization_approach` is set to `static`, and the `--apply_quantization` flag enables quantization. It also includes evaluation and verification of the loading process of the quantized model. The `num_calibration_samples` specifies the number of samples used for calibration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/summarization/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_summarization_post_training.py \\\n    --model_name_or_path sshleifer/distilbart-cnn-12-6 \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\"\n    --apply_quantization \\\n    --quantization_approach static\n    --num_calibration_samples 50 \\\n    --do_eval \\\n    --verify_loading \\\n    --predict_with_generate \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --output_dir /tmp/test_summarization\n```\n\n----------------------------------------\n\nTITLE: Quantize SentenceTransformer to INT8 with OpenVINO Weight Quantization (Python)\nDESCRIPTION: Quantizes the sentence-transformers/all-mpnet-base-v2 model to INT8 using OVWeightQuantizationConfig and saves the quantized model to a specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nOVSentenceTransformer.from_pretrained('sentence-transformers/all-mpnet-base-v2', quantization_config=OVWeightQuantizationConfig(bits=8)).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Quantize Whisper Model to INT8 with OpenVINO (Python)\nDESCRIPTION: This Python code snippet quantizes the whisper-large-v3-turbo model to INT8 precision using the OVModelForSpeechSeq2Seq class. It uses the librispeech dataset with 10 samples for calibration and saves the quantized model to a specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nOVModelForSpeechSeq2Seq.from_pretrained('openai/whisper-large-v3-turbo', quantization_config=OVQuantizationConfig(bits=8, dataset='librispeech', num_samples=10)).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Pruning Configuration with INCTrainer in Optimum Intel\nDESCRIPTION: This code demonstrates how to configure and apply weight pruning using `WeightPruningConfig` and `INCTrainer` from the `optimum-intel` library. The `WeightPruningConfig` specifies pruning parameters like pruning type, start/end steps, target sparsity, and pruning scope. The `INCTrainer` integrates this pruning configuration during the training process.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_7\n\nLANGUAGE: diff\nCODE:\n```\n- from transformers import Trainer\n+ from optimum.intel import INCTrainer\n+ from neural_compressor import WeightPruningConfig\n\n  # The configuration detailing the pruning process\n+ pruning_config = WeightPruningConfig(\n+     pruning_type=\"magnitude\",\n+     start_step=0,\n+     end_step=15,\n+     target_sparsity=0.2,\n+     pruning_scope=\"local\",\n+ )\n\n- trainer = Trainer(\n+ trainer = INCTrainer(\n      model=model,\n+     pruning_config=pruning_config,\n      args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=False),\n      train_dataset=dataset[\"train\"].select(range(300)),\n      eval_dataset=dataset[\"validation\"],\n      compute_metrics=compute_metrics,\n      tokenizer=tokenizer,\n      data_collator=default_data_collator,\n  )\n\n  train_result = trainer.train()\n  metrics = trainer.evaluate()\n  trainer.save_model()\n\n  model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n```\n\n----------------------------------------\n\nTITLE: Full Quantization - Text-to-Image - CLI\nDESCRIPTION: Exports a Stable Diffusion model to OpenVINO format with full quantization (int8) using the optimum-cli and conceptual_captions dataset.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m dreamlike-art/dreamlike-anime-1.0 --quant-mode int8 --dataset conceptual_captions ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Quantize Vision-Language Model to INT8 with OpenVINO (Python)\nDESCRIPTION: This Python code snippet quantizes the weights of a vision-language model to INT8, enabling memory-efficient deployment. It uses the `OVModelForVisualCausalLM` class and applies the specified quantization configuration using the `OVWeightQuantizationConfig` class, reducing the size of the model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nmodel = OVModelForVisualCausalLM.from_pretrained(\n    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n    quantization_config=quantization_config\n)\n```\n\n----------------------------------------\n\nTITLE: Export Whisper Model to OpenVINO with INT8 Quantization (CLI)\nDESCRIPTION: This command exports the Whisper large-v3-turbo model to OpenVINO format using the optimum-cli, applying INT8 quantization. It utilizes the librispeech dataset for quantization calibration with 10 samples, saving the converted model to the specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m openai/whisper-large-v3-turbo --quant-mode int8 --dataset librispeech --num-samples 10 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Mixed Quantization - Text Generation - Python\nDESCRIPTION: Loads a TinyLlama model, applies mixed quantization with nf4 and f8e4m3 datatypes using the wikitext2 dataset, and saves the quantized model. This uses `OVMixedQuantizationConfig` to define different quantization configurations for weights and activations.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOVModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0', quantization_config=OVMixedQuantizationConfig(OVWeightQuantizationConfig(bits=4, dtype='nf4'), OVQuantizationConfig(dtype='f8e4m3', dataset='wikitext2'))).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Inference with Stable Diffusion XL and OpenVINO\nDESCRIPTION: This code demonstrates how to perform image-to-image generation using Stable Diffusion XL (SDXL) with OpenVINO. It initializes the `OVStableDiffusionXLImg2ImgPipeline`, loads an initial image from a URL, and generates a new image based on the prompt. The OpenVINO model is exported on-the-fly with `export=True`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionXLImg2ImgPipeline\nfrom diffusers.utils import load_image\n\nmodel_id = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\npipeline = OVStableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, export=True)\n\nurl = \"https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/sd_xl/castle_friedrich.png\"\nimage = load_image(url).convert(\"RGB\")\nprompt = \"medieval castle by Caspar David Friedrich\"\nimage = pipeline(prompt, image=image).images[0]\n# Don't forget to save your OpenVINO model so that you can load it without exporting it with `export=True`\npipeline.save_pretrained(\"openvino-sd-xl-refiner-1.0\")\n```\n\n----------------------------------------\n\nTITLE: Quantizing the Model with Post Training Quantization\nDESCRIPTION: This code quantizes the loaded model using the OpenVINO Quantizer. It initializes the quantizer, configures quantization settings (including optional overflow fix), and applies the quantization process using a calibration dataset. The resulting quantized model is then saved to the specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Hide PyTorch warnings about missing shape inference\nwarnings.simplefilter(\"ignore\")\n\n# Quantize the model\nquantizer = OVQuantizer.from_pretrained(model)\nov_config = OVConfig(quantization_config=OVQuantizationConfig())\nquantizer.quantize(calibration_dataset=train_dataset, ov_config=ov_config, save_directory=int8_ptq_model_path)\n```\n\n----------------------------------------\n\nTITLE: Quantize SentenceTransformer to INT4 with Hybrid Quantization and WikiText2 Dataset (Python)\nDESCRIPTION: Quantizes sentence-transformers/all-mpnet-base-v2 to INT4 using hybrid quantization with the wikitext2 dataset for calibration and saves the resulting model to the designated save directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nOVSentenceTransformer.from_pretrained('sentence-transformers/all-mpnet-base-v2', quantization_config=OVWeightQuantizationConfig(bits=4, quant_method='hybrid', dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Inference with IPEX using IPEXModelForCausalLM\nDESCRIPTION: This code demonstrates how to load and use an IPEX-optimized model for causal language modeling. It replaces `AutoModelForCausalLM` with `IPEXModelForCausalLM` to load the model and utilize IPEX optimizations.  It also sets the `torch_dtype` to `torch.bfloat16` for improved performance on Intel hardware.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nfrom optimum.intel import IPEXModelForCausalLM\n\n\n  model_id = \"gpt2\"\n  model = IPEXModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n  results = pipe(\"He's a dreadful magician and\")\n```\n\n----------------------------------------\n\nTITLE: Exporting and Saving Stable Diffusion model to OpenVINO\nDESCRIPTION: This code shows how to export a PyTorch Stable Diffusion model to OpenVINO format on-the-fly using the `export=True` flag in `from_pretrained`. After exporting, the model is saved for future use using `save_pretrained`.  This eliminates the need to re-export the model every time.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\n# Don't forget to save the exported model\npipeline.save_pretrained(\"openvino-sd-v1-5\")\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-aware) - Text Generation - Python\nDESCRIPTION: Loads a TinyLlama model, applies weight-only quantization with 4 bits using the wikitext2 dataset, and saves the quantized model.  The `bits` parameter is set to 4 and the `dataset` parameter is set to 'wikitext2'.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOVModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0', quantization_config=OVWeightQuantizationConfig(bits=4, dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Static Quantization (Python)\nDESCRIPTION: Applies static quantization to a model using Optimum Intel and Neural Compressor. It loads a pre-trained model, generates a calibration dataset, and performs static quantization based on the specified configuration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom neural_compressor.config import PostTrainingQuantConfig\nfrom optimum.intel import INCQuantizer\n\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# The directory where the quantized model will be saved\nsave_dir = \"static_quantization\"\n\ndef preprocess_function(examples, tokenizer):\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=128, truncation=True)\n\n# Load the quantization configuration detailing the quantization we wish to apply\nquantization_config = PostTrainingQuantConfig(approach=\"static\")\nquantizer = INCQuantizer.from_pretrained(model)\n# Generate the calibration dataset needed for the calibration step\ncalibration_dataset = quantizer.get_calibration_dataset(\n    \"glue\",\n    dataset_config_name=\"sst2\",\n    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n    num_samples=100,\n    dataset_split=\"train\",\n)\nquantizer = INCQuantizer.from_pretrained(model)\n# Apply static quantization and save the resulting model\nquantizer.quantize(\n    quantization_config=quantization_config,\n    calibration_dataset=calibration_dataset,\n    save_directory=save_dir,\n)\n```\n\n----------------------------------------\n\nTITLE: Quantization Aware Training and Pruning for Translation with INC\nDESCRIPTION: This example shows how to fine-tune a T5 model while applying magnitude pruning and quantization aware training using the Intel Neural Compressor. It requires setting parameters such as the model name, source and target languages, dataset, pruning target sparsity, number of training epochs, and maximum training samples. The example also includes options to verify the loading of the quantized model and set batch sizes.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/translation/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_translation.py \\\n    --model_name_or_path t5-small \\\n    --source_lang en \\\n    --target_lang ro \\\n    --dataset_name wmt16 \\\n    --dataset_config_name ro-en \\\n    --source_prefix \"translate English to Romanian: \" \\\n    --apply_quantization \\\n    --apply_pruning \\\n    --target_sparsity 0.1 \\\n    --num_train_epochs 4 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --predict_with_generate \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --output_dir /tmp/test_translation\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Function for Tokenization\nDESCRIPTION: This function preprocesses the dataset by tokenizing the question and context using the provided tokenizer. It applies padding and truncation to ensure the inputs are of a consistent length (384 tokens).\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_fn(examples, tokenizer):\n    \"\"\"convert the text from the dataset into tokens in the format that the model expects\"\"\"\n    return tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        padding=True,\n        truncation=True,\n        max_length=384,\n    )\n```\n\n----------------------------------------\n\nTITLE: Quantization Aware Training with Knowledge Distillation\nDESCRIPTION: This example fine-tunes a DistilBERT model on the SWAG dataset while applying knowledge distillation and quantization-aware training.  It uses the `run_swag.py` script with the `--apply_distillation`, `--teacher_model_name_or_path`, and `--apply_quantization` flags. The teacher model is specified using `--teacher_model_name_or_path`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/multiple-choice/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_swag.py \\\n    --model_name_or_path distilbert-base-uncased \\\n    --apply_distillation \\\n    --teacher_model_name_or_path ehdwns1516/bert-base-uncased_SWAG \\\n    --apply_quantization \\\n    --num_train_epochs 1 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/swag_output\n```\n\n----------------------------------------\n\nTITLE: Export SentenceTransformer to OpenVINO with INT4 Weight Quantization and WikiText2 (CLI)\nDESCRIPTION: Exports sentence-transformers/all-mpnet-base-v2 to OpenVINO using optimum-cli with INT4 weight quantization, calibrating using the wikitext2 dataset and saving the result to ./save_dir.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --library sentence_transformers -m sentence-transformers/all-mpnet-base-v2 --weight-format int4 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Quantization Aware Training with Knowledge Distillation (Bash)\nDESCRIPTION: This command fine-tunes a DistilBERT model on the CoNLL-2003 dataset while applying knowledge distillation with quantization aware training. It utilizes the `run_ner.py` script, requiring `model_name_or_path`, `dataset_name`, and `teacher_model_name_or_path`.  The `--apply_distillation` flag enables knowledge distillation, and `--apply_quantization` enables quantization aware training.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/token-classification/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_ner.py \\\n    --model_name_or_path distilbert-base-uncased \\\n    --dataset_name conll2003 \\\n    --apply_distillation \\\n    --teacher_model_name_or_path elastic/distilbert-base-uncased-finetuned-conll03-english \\\n    --apply_quantization \\\n    --num_train_epochs 1 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/conll03_output\n```\n\n----------------------------------------\n\nTITLE: Load Model on GPU Directly\nDESCRIPTION: This snippet shows how to load an OpenVINO model directly onto the GPU using the `device` parameter in the `from_pretrained()` method. This avoids the need to move the model to the GPU separately after loading it. This ensures that the model is loaded directly onto the GPU for inference.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n# Set the device directly with `.from_pretrained()`\nif \"GPU\" in Core().available_devices:\n    model = OVModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad-ov-fp16\", device=\"GPU\")\n```\n\n----------------------------------------\n\nTITLE: Create Gradio Interface\nDESCRIPTION: This code creates the Gradio Blocks interface for the chatbot. It defines the layout, including the chatbot display, text input, buttons, and sliders for controlling generation parameters. It sets up event listeners to connect user interactions to the chatbot's logic.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown('<h1 style=\"text-align: center;\">Chat with Phi-2 on Meteor Lake iGPU</h1>')\n    chatbot = gr.Chatbot()\n    with gr.Row():\n        assisted = gr.Checkbox(value=False, label=\"Assisted Generation\", scale=10)\n        msg = gr.Textbox(placeholder=\"Enter message here...\", show_label=False, autofocus=True, scale=75)\n        status = gr.Textbox(\"Status: Idle\", show_label=False, max_lines=1, scale=15)\n    with gr.Row():\n        submit = gr.Button(\"Submit\", variant='primary')\n        regenerate = gr.Button(\"Regenerate\")\n        clear = gr.Button(\"Clear\")\n    with gr.Accordion(\"Advanced Options:\", open=False):\n        with gr.Row():\n            with gr.Column():\n                temperature = gr.Slider(\n                    label=\"Temperature\",\n                    value=0.0,\n                    minimum=0.0,\n                    maximum=1.0,\n                    step=0.05,\n                    interactive=True,\n                )\n                max_new_tokens = gr.Slider(\n                    label=\"Max new tokens\",\n                    value=128,\n                    minimum=0,\n                    maximum=512,\n                    step=32,\n                    interactive=True,\n                )\n            with gr.Column():\n                top_p = gr.Slider(\n                    label=\"Top-p (nucleus sampling)\",\n                    value=1.0,\n                    minimum=0.0,\n                    maximum=1.0,\n                    step=0.05,\n                    interactive=True,\n                )\n                repetition_penalty = gr.Slider(\n                    label=\"Repetition penalty\",\n                    value=1.0,\n                    minimum=1.0,\n                    maximum=2.0,\n                    step=0.1,\n                    interactive=True,\n                )\n    gr.Examples(\n        EXAMPLES, inputs=msg, label=\"Click on any example and press the 'Submit' button\"\n    )\n\n    # Sets generate function to be triggered when the user submit a new message\n    gr.on(\n        triggers=[submit.click, msg.submit],\n        fn=add_user_text,\n        inputs=[msg, chatbot],\n        outputs=[msg, chatbot],\n        queue=False,\n    ).then(\n        fn=generate,\n        inputs=[chatbot, temperature, max_new_tokens, top_p, repetition_penalty, assisted],\n        outputs=[chatbot, status, msg, submit, regenerate, clear],\n        concurrency_limit=1,\n        queue=True\n    )\n    regenerate.click(\n        fn=prepare_for_regenerate,\n        inputs=chatbot,\n        outputs=chatbot,\n        queue=True,\n        concurrency_limit=1\n    ).then(\n        fn=generate,\n        inputs=[chatbot, temperature, max_new_tokens, top_p, repetition_penalty, assisted],\n        outputs=[chatbot, status, msg, submit, regenerate, clear],\n        concurrency_limit=1,\n        queue=True\n    )\n    clear.click(fn=lambda: (None, \"Status: Idle\"), inputs=None, outputs=[chatbot, status], queue=False)\n```\n\n----------------------------------------\n\nTITLE: Hybrid Quantization - Text Generation - Python\nDESCRIPTION: Loads a TinyLlama model, applies hybrid quantization with 4 bits using the wikitext2 dataset, and saves the quantized model. The `quant_method` is set to 'hybrid'.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOVModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0', quantization_config=OVWeightQuantizationConfig(bits=4, quant_method='hybrid', dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Post-training static quantization with OpenVINO\nDESCRIPTION: This code demonstrates post-training static quantization using OpenVINO. It involves creating a calibration dataset and using `OVQuantizer` to quantize a model, and then loads the quantized model for inference.  The quantized model is saved in the OpenVINO IR format.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfrom functools import partial\nfrom optimum.intel import OVQuantizer, OVModelForSequenceClassification, OVConfig, OVQuantizationConfig\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ndef preprocess_fn(examples, tokenizer):\n    return tokenizer(\n        examples[\"sentence\"], padding=True, truncation=True, max_length=128\n    )\n\nquantizer = OVQuantizer.from_pretrained(model)\ncalibration_dataset = quantizer.get_calibration_dataset(\n    \"glue\",\n    dataset_config_name=\"sst2\",\n    preprocess_function=partial(preprocess_fn, tokenizer=tokenizer),\n    num_samples=100,\n    dataset_split=\"train\",\n    preprocess_batch=True,\n)\n# The directory where the quantized model will be saved\nsave_dir = \"nncf_results\"\n# Apply static quantization and save the resulting model in the OpenVINO IR format\nov_config = OVConfig(quantization_config=OVQuantizationConfig())\nquantizer.quantize(ov_config=ov_config, calibration_dataset=calibration_dataset, save_directory=save_dir)\n# Load the quantized model\noptimized_model = OVModelForSequenceClassification.from_pretrained(save_dir)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning and Pruning with Dynamic Quantization (T5 Model)\nDESCRIPTION: This script shows how to fine-tune a T5 model on the CNN/DailyMail dataset while applying magnitude pruning and then applying dynamic quantization as a second step. The `--apply_pruning` flag enables pruning, and `--target_sparsity` specifies the desired sparsity level. The `--apply_quantization` flag is used to apply dynamic quantization after pruning. Training, evaluation and verification of model loading are also performed.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/summarization/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_summarization.py \\\n    --model_name_or_path t5-small \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\"\n    --source_prefix \"summarize: \" \\\n    --apply_quantization \\\n    --apply_pruning \\\n    --target_sparsity 0.1 \\\n    --num_train_epochs 4 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --predict_with_generate \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --output_dir /tmp/test_summarization\n```\n\n----------------------------------------\n\nTITLE: Masked Language Modeling with Optimum Intel\nDESCRIPTION: This snippet demonstrates masked language modeling using a pre-trained BERT model and the Optimum Intel library. It involves loading a model, tokenizer, creating a pipeline, and predicting the masked word in a sentence. The code requires `optimum.intel` and `transformers` libraries. The `model_id` variable specifies the model to be used.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForMaskedLM\nfrom transformers import AutoTokenizer, pipeline\n\nmodel_id = \"helenai/bert-base-uncased-ov\"\nmodel = OVModelForMaskedLM.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nov_pipe = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n\nsentence = f\"I am a {tokenizer.mask_token} model\"\nov_pipe(sentence, top_k=2)\n```\n\n----------------------------------------\n\nTITLE: Quantize CodeBERT Model to INT4 with OpenVINO Weight Quantization and WikiText2 Dataset (Python)\nDESCRIPTION: This Python code quantizes the microsoft/codebert-base model to INT4 precision using OVWeightQuantizationConfig with the wikitext2 dataset for calibration. The quantized model is then saved to the specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nOVModelForFeatureExtraction.from_pretrained('microsoft/codebert-base', quantization_config=OVWeightQuantizationConfig(bits=4, dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Post-Training Static Quantization for Question Answering in Bash\nDESCRIPTION: This example demonstrates how to apply post-training static quantization to a DistilBERT model fine-tuned on the SQuAD1.0 dataset using the `run_qa_post_training.py` script. It utilizes the Intel Neural Compressor with IPEX for quantization. The `apply_quantization` flag enables quantization, and `quantization_approach` is set to `static` for static quantization. Calibration samples are used to determine quantization parameters.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/question-answering/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_qa_post_training.py \\\n    --model_name_or_path distilbert-base-uncased-distilled-squad \\\n    --dataset_name squad \\\n    --apply_quantization \\\n    --quantization_approach static \\\n    --num_calibration_samples 40 \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/squad_output\n```\n\n----------------------------------------\n\nTITLE: Generate with Speculative Decoding\nDESCRIPTION: This snippet demonstrates how to generate text using speculative decoding with the main model and the assistant model. It passes the assistant model to the `generate` method, along with other generation parameters such as `max_new_tokens`, `streamer`, and `pad_token_id`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nout = stateless_model.generate(\n    **inputs,\n    max_new_tokens=128,\n    streamer=TextStreamer(tokenizer=tokenizer, skip_special_tokens=True),\n    pad_token_id=tokenizer.eos_token_id,\n    assistant_model=asst_model,\n)\n```\n\n----------------------------------------\n\nTITLE: Sequence-to-Sequence Translation with Optimum Intel\nDESCRIPTION: This snippet showcases how to use the `t5-small` model for translation from English to German using both the `model.generate()` method and the pipeline approach. It loads the model and tokenizer, then translates a sentence using `model.generate()`. The code requires `optimum.intel` and `transformers` libraries. `trust_remote_code=True` is specified as it might be needed for certain models.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForSeq2SeqLM\nfrom transformers import AutoTokenizer, pipeline\n\nmodel_id = \"helenai/t5-small-ov\"\nmodel = OVModelForSeq2SeqLM.from_pretrained(model_id, compile=False, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n\ninput_ids = tokenizer(\"translate English to German: This house is wonderful.\", return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids, max_new_tokens=10)\ntokenizer.decode(outputs[0], skip_special_tokens=True)\n```\n\n----------------------------------------\n\nTITLE: Load and Use IPEX Model for Generation\nDESCRIPTION: Loads a GPT-2 model optimized with IPEX using `IPEXModelForCausalLM.from_pretrained`. The code then prepares an input sentence using the tokenizer and generates text using the model's `generate` method with specified generation parameters, such as `max_new_tokens`, `do_sample`, and `num_beams`. Finally, it decodes and prints the generated text.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/text_generation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = IPEXModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ninput_sentence = [\"Answer the following yes/no question by reasoning step-by-step please. Can you write a whole Haiku in a single tweet?\"]\nmodel_inputs = tokenizer(input_sentence, return_tensors=\"pt\")\ngeneration_kwargs = dict(max_new_tokens=32, do_sample=False, num_beams=4, num_beam_groups=1, no_repeat_ngram_size=2, use_cache=True)\n\ngenerated_ids = model.generate(**model_inputs, **generation_kwargs)\noutput = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Quantize CodeBERT Model to INT8 with OpenVINO Weight Quantization (Python)\nDESCRIPTION: This Python code quantizes the microsoft/codebert-base model to INT8 using the OVModelForFeatureExtraction class. It employs OVWeightQuantizationConfig and saves the quantized model to a specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nOVModelForFeatureExtraction.from_pretrained('microsoft/codebert-base', quantization_config=OVWeightQuantizationConfig(bits=8)).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Dynamic Quantization with Neural Compressor in Bash\nDESCRIPTION: This command applies dynamic quantization to a MiniLM model fine-tuned on the CLINC150 dataset. It uses the `run_clinc.py` script with specified parameters for model path, dataset, quantization approach, evaluation, loading verification, and output directory. The script leverages the Intel Neural Compressor library to perform the dynamic quantization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/intent-classification/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_clinc.py \\\n    --model_name_or_path SetFit/MiniLM_L3_clinc_oos_plus_distilled \\\n    --dataset_name clinc_oos \\\n    --apply_quantization \\\n    --quantization_approach dynamic \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/clinc_output\n```\n\n----------------------------------------\n\nTITLE: Load and Save OpenVINO Model\nDESCRIPTION: This snippet demonstrates how to load a PyTorch model from the Hugging Face Hub and export it to the OpenVINO format, saving the converted model to a local directory. It also showcases loading an existing OpenVINO model directly from a local directory. The `OVModelForQuestionAnswering` class from `optimum.intel` is used for this purpose.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom optimum.intel import OVModelForQuestionAnswering\n\n# Load PyTorch model from the Hub and export to OpenVINO in the background\nmodel = OVModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\", export=True)\n\n# Save the converted model to a local directory\nmodel.save_pretrained(\"distilbert-base-uncased-distilled-squad-ov-fp32\")\n\n# Load the OpenVINO model directly from the directory\nmodel = OVModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad-ov-fp32\")\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Inference with Stable Diffusion and OpenVINO\nDESCRIPTION: This code snippet shows how to perform image-to-image generation using the `OVStableDiffusionImg2ImgPipeline` with OpenVINO. It loads an initial image from a URL, preprocesses it, and then uses it as a base for generating a new image based on the provided prompt, strength, and guidance scale.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\nfrom optimum.intel import OVStableDiffusionImg2ImgPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipeline = OVStableDiffusionImg2ImgPipeline.from_pretrained(model_id, export=True)\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((768, 512))\nprompt = \"A fantasy landscape, trending on artstation\"\nimage = pipeline(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images[0]\nimage.save(\"fantasy_landscape.png\")\n```\n\n----------------------------------------\n\nTITLE: Export and quantize model\nDESCRIPTION: This code loads, exports (if necessary), and quantizes the Phi-2 model using OpenVINO and  Optimum. It configures the loading parameters, including device, OpenVINO configuration, quantization configuration, and compilation settings. The exported and quantized model is then saved locally.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load kwargs\nload_kwargs = {\n    \"device\": device,\n    \"ov_config\": {\n        \"PERFORMANCE_HINT\": \"LATENCY\",\n        \"INFERENCE_PRECISION_HINT\": precision,\n        \"CACHE_DIR\": os.path.join(save_name, \"model_cache\"),  # OpenVINO will use this directory as cache\n    },\n    \"compile\": False,\n    \"quantization_config\": quantization_config\n}\n\n# Check whether the model was already exported\nsaved = os.path.exists(save_name)\n\nmodel = OVModelForCausalLM.from_pretrained(\n    model_name if not saved else save_name,\n    export=not saved,\n    **load_kwargs,\n)\n\n# Load tokenizer to be used with the model\ntokenizer = AutoTokenizer.from_pretrained(model_name if not saved else save_name)\n\n# Save the exported model locally\nif not saved:\n    model.save_pretrained(save_name)\n    tokenizer.save_pretrained(save_name)\n\n# TODO Optional: export to huggingface/hub\n\nmodel_size = os.stat(os.path.join(save_name, \"openvino_model.bin\")).st_size / 1024 ** 3\nprint(f'Model size in FP32: ~5.4GB, current model size in 4bit: {model_size:.2f}GB')\n```\n\n----------------------------------------\n\nTITLE: Mixed Quantization Configuration with Python API\nDESCRIPTION: This code snippet demonstrates how to configure and apply mixed quantization using the OpenVINO Python API. It utilizes `OVMixedQuantizationConfig` to specify different quantization configurations for weights and activations, setting the weight quantization to 4-bit NF4 and the full quantization to f8e4m3. A dataset is also provided for calibration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nmodel = OVModelForCausalLM.from_pretrained(\n    'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n    quantization_config=OVMixedQuantizationConfig(\n        weight_quantization_config=OVWeightQuantizationConfig(bits=4, dtype='nf4'),\n        full_quantization_config=OVQuantizationConfig(dtype='f8e4m3', dataset='wikitext2')\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Running Post-Training Static Quantization on TrOCR (Python)\nDESCRIPTION: This script applies post-training static quantization on a TrOCR small model fine-tuned on the IAM dataset. It uses the `run_ocr_post_training.py` script and requires the `model_name_or_path`, `datasets_dir`, `--apply_quantization`, `--quantization_approach`, `--verify_loading`, and `output_dir` parameters. The `quantization_approach` is set to `dynamic` in this example. The `--verify_loading` flag ensures the quantized model can be loaded correctly.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/optical-character-recognition/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython run_ocr_post_training.py \\\n    --model_name_or_path microsoft/trocr-small-handwritten \\\n    --datasets_dir IAM \\\n    --apply_quantization \\\n    --quantization_approach dynamic \\\n    --verify_loading \\\n    --output_dir /tmp/trocr_output\n```\n\n----------------------------------------\n\nTITLE: Inference with OpenVINO Runtime using OVModelForSeq2SeqLM\nDESCRIPTION: This code snippet demonstrates how to load and use an OpenVINO-optimized model for sequence-to-sequence tasks. It replaces `AutoModelForSeq2SeqLM` with `OVModelForSeq2SeqLM` to load the model and perform translation using the Hugging Face `pipeline` API.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom optimum.intel import OVModelForSeq2SeqLM\n  from transformers import AutoTokenizer, pipeline\n\n  model_id = \"echarlaix/t5-small-openvino\"\n  model = OVModelForSeq2SeqLM.from_pretrained(model_id)\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n  pipe = pipeline(\"translation_en_to_fr\", model=model, tokenizer=tokenizer)\n  results = pipe(\"He never went out without a book under his arm, and he often came back with two.\")\n\n  [{'translation_text': \"Il n'est jamais sorti sans un livre sous son bras, et il est souvent revenu avec deux.\"}]\n```\n\n----------------------------------------\n\nTITLE: Token Classification with Optimum Intel and OpenVINO\nDESCRIPTION: This code snippet demonstrates how to perform token classification using `OVModelForTokenClassification` from the `optimum.intel` library. It loads a pre-trained named entity recognition model, creates a pipeline, and classifies tokens in a sample sentence. The results are then printed.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForTokenClassification\nfrom transformers import AutoTokenizer, pipeline\n\nmodel_id = \"helenai/dslim-bert-base-NER-ov-fp32\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = OVModelForTokenClassification.from_pretrained(model_id)\nov_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\nresult = ov_pipe(\"My name is Wolfgang and I live in Berlin\")\nfor item in result:\n    print(item)\n```\n\n----------------------------------------\n\nTITLE: Exporting a Model to OpenVINO IR Format using optimum-cli\nDESCRIPTION: This command exports a model hosted on the Hugging Face Hub to the OpenVINO IR format using the `optimum-cli`. It specifies the model ID and the output directory for the exported model. This process converts the model into a format optimized for OpenVINO inference.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Meta-Llama-3-8B ov_model/\n```\n\n----------------------------------------\n\nTITLE: Quantization Aware Training with Knowledge Distillation in Bash\nDESCRIPTION: This example shows how to fine-tune DistilBERT on the SQuAD1.0 dataset while applying knowledge distillation and quantization-aware training using `run_qa.py`.  The `apply_distillation` flag enables knowledge distillation from a teacher model.  `apply_quantization` enables quantization aware training. The process can be accelerated by the flag `--generate_teacher_logits`, which will add an additional step where the teacher outputs will be computed and saved in the training dataset, removing the need to compute the teacher outputs at every training step.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/question-answering/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_qa.py \\\n    --model_name_or_path distilbert-base-uncased \\\n    --dataset_name squad \\\n    --apply_distillation \\\n    --generate_teacher_logits \\\n    --teacher_model_name_or_path distilbert-base-uncased-distilled-squad \\\n    --apply_quantization \\\n    --do_train \\\n    --do_eval \\\n    --num_train_epochs 1 \\\n    --max_train_samples 100 \\\n    --verify_loading \\\n    --output_dir /tmp/squad_output\n```\n\n----------------------------------------\n\nTITLE: Exporting Stable Diffusion XL with Hybrid Quantization - Optimum CLI\nDESCRIPTION: Exports a Stable Diffusion XL model to OpenVINO with hybrid quantization (INT8 weights).  Uses the conceptual_captions dataset for calibration. The output models are saved in the ov_sdxl/ directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model stabilityai/stable-diffusion-xl-base-1.0 \\\n    --weight-format int8 --dataset conceptual_captions ov_sdxl/\n```\n\n----------------------------------------\n\nTITLE: Applying Post-Training Static Quantization with SmoothQuant (CLM)\nDESCRIPTION: This bash script demonstrates how to apply post-training static quantization using the SmoothQuant methodology on a GPT-Neo model. It utilizes the WikiText-2 dataset and saves the output to /tmp/clm_output. It includes evaluation and verifies loading of the quantized model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/language-modeling/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_clm.py \\\n    --model_name_or_path EleutherAI/gpt-neo-125M \\\n    --dataset_name wikitext \\\n    --dataset_config_name wikitext-2-raw-v1 \\\n    --apply_quantization \\\n    --quantization_approach static \\\n    --smooth_quant \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/clm_output\n```\n\n----------------------------------------\n\nTITLE: Exporting a Local Model to OpenVINO IR Format using optimum-cli\nDESCRIPTION: This command exports a local model to the OpenVINO IR format using the `optimum-cli`. It specifies the path to the local model, the task for which the model should be loaded (text generation with past key values), and the output directory for the exported model. The `--task` argument is required for local models to correctly load and export them.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model local_llama --task text-generation-with-past ov_model/\n```\n\n----------------------------------------\n\nTITLE: Quantization Aware Training (Diff)\nDESCRIPTION: Demonstrates how to integrate quantization-aware training into a Hugging Face Transformers training pipeline using Optimum Intel. It shows the necessary code changes to switch from the standard `Trainer` to `INCTrainer` and incorporate quantization configuration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\n  import evaluate\n  import numpy as np\n  from datasets import load_dataset\n  from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, default_data_collator\n- from transformers import Trainer\n+ from optimum.intel import INCModelForSequenceClassification, INCTrainer\n+ from neural_compressor import QuantizationAwareTrainingConfig\n\n  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n  model = AutoModelForSequenceClassification.from_pretrained(model_id)\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n  dataset = load_dataset(\"glue\", \"sst2\")\n  dataset = dataset.map(lambda examples: tokenizer(examples[\"sentence\"], padding=True, max_length=128), batched=True)\n  metric = evaluate.load(\"glue\", \"sst2\")\n  compute_metrics = lambda p: metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n\n  # The directory where the quantized model will be saved\n  save_dir = \"quantized_model\"\n\n  # The configuration detailing the quantization process\n+ quantization_config = QuantizationAwareTrainingConfig()\n\n- trainer = Trainer(\n+ trainer = INCTrainer(\n      model=model,\n+     quantization_config=quantization_config,\n      args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=False),\n      train_dataset=dataset[\"train\"].select(range(300)),\n      eval_dataset=dataset[\"validation\"],\n      compute_metrics=compute_metrics,\n      tokenizer=tokenizer,\n      data_collator=default_data_collator,\n  )\n\n  train_result = trainer.train()\n  metrics = trainer.evaluate()\n  trainer.save_model()\n\n- model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n+ model = INCModelForSequenceClassification.from_pretrained(save_dir)\n```\n\n----------------------------------------\n\nTITLE: Question Answering with Optimum Intel\nDESCRIPTION: This snippet demonstrates question answering using a pre-trained DistilBERT model and the Optimum Intel library. It involves loading a model, tokenizer, creating a pipeline, and predicting the answer to a question given a context. The code requires `optimum.intel` and `transformers` libraries. The `model_id` variable specifies the model to be used.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForQuestionAnswering\nfrom transformers import AutoTokenizer, pipeline\n\n# Load the model and tokenizer saved in Part 1 of this notebook. Or use the line below to load them from the hub\n# model_id = \"helenai/distilbert-base-uncased-distilled-squad-ov-fp32\"\nmodel_id = \"distilbert-base-uncased-distilled-squad-ov-fp32\"\nmodel = OVModelForQuestionAnswering.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nov_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\nov_pipe(\"What is OpenVINO?\", \"OpenVINO is an open source toolkit for deep learning inference optimization.\")\n```\n\n----------------------------------------\n\nTITLE: Loading a PyTorch checkpoint and exporting to OpenVINO IR format\nDESCRIPTION: This code loads a PyTorch checkpoint using `OVModelForCausalLM` and exports it to the OpenVINO IR format by setting `export=True`.  The model is then saved to a specified directory. This allows loading a standard PyTorch model and automatically converting it to an OpenVINO-compatible format.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom optimum.intel import OVModelForCausalLM\n\nmodel = OVModelForCausalLM.from_pretrained(\"gpt2\", export=True)\nmodel.save_pretrained(\"./ov_model\")\n```\n\n----------------------------------------\n\nTITLE: Audio Classification with Optimum Intel\nDESCRIPTION: This snippet demonstrates how to perform audio classification using a pre-trained model from Hugging Face Hub and the Optimum Intel library. It involves loading a model, feature extractor, creating a pipeline, loading the dataset, and running inference. The code requires `IPython`, `optimum.intel`, `transformers`, and `datasets` libraries. It utilizes the `speech_commands` dataset and a fine-tuned AST model. The `audio_sample` variable determines the audio data source (either a dataset item or a path to an audio file).\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Audio\nfrom optimum.intel import OVModelForAudioClassification\nfrom transformers import AutoFeatureExtractor, pipeline\nfrom datasets import load_dataset\n\nmodel_id = \"helenai/MIT-ast-finetuned-speech-commands-v2-ov\"\nmodel = OVModelForAudioClassification.from_pretrained(model_id)\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\nov_pipe = pipeline(\"audio-classification\", model=model, feature_extractor=feature_extractor)\n\n# streaming=true enables loading one item from the dataset without downloading the full dataset\ndataset = load_dataset(\"speech_commands\", \"v0.02\", streaming=True, trust_remote_code=True)\naudio_sample = next(iter(dataset[\"test\"]))\n```\n\n----------------------------------------\n\nTITLE: Exporting with INT4 Weights and AWQ/Scale Estimation - Optimum CLI\nDESCRIPTION: Exports a model with INT4 weight compression, applying AWQ and Scale Estimation algorithms for improved accuracy.  Uses the wikitext2 dataset for calibration. The output model is saved in the ov_model/ directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Meta-Llama-3-8B \\\n    --weight-format int4 --awq --scale-estimation --dataset wikitext2 ov_model/\n```\n\n----------------------------------------\n\nTITLE: Magnitude Pruning and Quantization Aware Training in Bash\nDESCRIPTION: This example demonstrates fine-tuning DistilBERT on the SQuAD1.0 dataset while applying magnitude pruning and quantization-aware training using `run_qa.py`. `apply_pruning` enables pruning, and `target_sparsity` specifies the desired sparsity level. `apply_quantization` activates quantization-aware training in conjunction with pruning.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/question-answering/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run_qa.py \\\n    --model_name_or_path distilbert-base-uncased-distilled-squad \\\n    --dataset_name squad \\\n    --apply_quantization \\\n    --apply_pruning \\\n    --target_sparsity 0.1 \\\n    --num_train_epochs 4 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/squad_output\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image with Textual Inversion in Stable Diffusion XL\nDESCRIPTION: This snippet demonstrates how to integrate textual inversion into Stable Diffusion XL using OpenVINO.  It runs the pipeline first without textual inversion, then it loads the textual inversion embedding, and runs the pipeline again with the embedding.  The `load_textual_inversion` method incorporates the embedding. The `clear_requests` method clears the pipeline before loading textual inversion.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionXLPipeline\nimport numpy as np\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nprompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround wearing a red jacket and black shirt, best quality, intricate details.\"\n# Set a random seed for better comparison\nnp.random.seed(112)\n\nbase = OVStableDiffusionXLPipeline.from_pretrained(model_id, export=False, compile=False)\nbase.compile()\nimage1 = base(prompt, num_inference_steps=50).images[0]\nimage1.save(\"sdxl_without_textual_inversion.png\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Reset stable diffusion pipeline\nbase.clear_requests()\n\n# Load textual inversion into stable diffusion pipeline\nbase.load_textual_inversion(\"./charturnerv2.pt\", \"charturnerv2\")\n\n# Compile the model before the first inference\nbase.compile()\nimage2 = base(prompt, num_inference_steps=50).images[0]\nimage2.save(\"sdxl_with_textual_inversion.png\")\n```\n\n----------------------------------------\n\nTITLE: Prune Once For All - Stage 2 in Bash\nDESCRIPTION: This bash script executes the second stage of the Prune Once For All (POFA) fine-tuning process. It fine-tunes the model from stage 1, additionally applying quantization-aware training to obtain a fine-tuned quantized model with the same sparsity. It uses `run_glue_during_training_optimization.py` with specified configuration and training parameters.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# for stage 2\npython run_glue_during_training_optimization.py \\\n    --model_name_or_path /tmp/sst2_output_stage1 \\\n    --task_name sst2 \\\n    --apply_distillation \\\n    --teacher_model_name_or_path distilbert-base-uncased-finetuned-sst-2-english \\\n    --apply_pruning \\\n    --pruning_config ../config/prune_pattern_lock.yml \\\n    --apply_quantization \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 1e-6 \\\n    --num_train_epochs 3 \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size 32 \\\n    --per_device_eval_batch_size 32 \\\n    --pad_to_max_length \\\n    --warmup_ratio 0.01 \\\n    --weight_decay 0.01 \\\n    --verify_loading \\\n    --output_dir /tmp/sst2_output_stage2\n```\n\n----------------------------------------\n\nTITLE: Export SentenceTransformer to OpenVINO with INT8 Quantization and WikiText2 (CLI)\nDESCRIPTION: Exports the sentence-transformers/all-mpnet-base-v2 model to OpenVINO format using optimum-cli and INT8 quantization, using the wikitext2 dataset for quantization calibration, saving the converted model to ./save_dir.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --library sentence_transformers -m sentence-transformers/all-mpnet-base-v2 --quant-mode int8 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Magnitude Pruning and Quantization Aware Training\nDESCRIPTION: This example fine-tunes a BERT model on the SWAG dataset while applying magnitude pruning and quantization-aware training. The script `run_swag.py` is used with the `--apply_quantization` and `--apply_pruning` flags. The `--target_sparsity` parameter specifies the desired sparsity level.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/multiple-choice/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run_swag.py \\\n    --model_name_or_path ehdwns1516/bert-base-uncased_SWAG \\\n    --apply_quantization \\\n    --apply_pruning \\\n    --target_sparsity 0.1 \\\n    --num_train_epochs 4 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/swag_output\n```\n\n----------------------------------------\n\nTITLE: Comparing FP32 and INT8 prediction scores for question answering in Python\nDESCRIPTION: This code snippet compares the F1 scores of FP32 and INT8 question answering models by iterating through a validation dataset. It identifies instances where the prediction scores differ and stores them for further analysis. The `evaluate` library is used for calculating F1 scores, and the `squad_v2` or `squad` dataset is used depending on the `VERSION_2_WITH_NEGATIVE` flag. It depends on hf_qa_pipeline and ov_qa_pipeline_ptq being defined elsewhere.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresults = []\nmetric = evaluate.load(\"squad_v2\" if VERSION_2_WITH_NEGATIVE else \"squad\")\n\nfor item in validation_examples:\n    id, title, context, question, answers = item.values()\n    fp32_answer = hf_qa_pipeline(question, context)[\"answer\"]\n    int8_answer = ov_qa_pipeline_ptq(question, context)[\"answer\"]\n\n    references = [{\"id\": id, \"answers\": answers}]\n    fp32_predictions = [{\"id\": id, \"prediction_text\": fp32_answer}]\n    int8_predictions = [{\"id\": id, \"prediction_text\": int8_answer}]\n\n    fp32_score = round(metric.compute(references=references, predictions=fp32_predictions)[\"f1\"], 2)\n    int8_score = round(metric.compute(references=references, predictions=int8_predictions)[\"f1\"], 2)\n\n    if int8_score != fp32_score:\n        results.append((question, answers[\"text\"], fp32_answer, fp32_score, int8_answer, int8_score))\n\npd.set_option(\"display.max_colwidth\", None)\npd.DataFrame(\n    results,\n    columns=[\"Question\", \"Answer\", \"FP32 answer\", \"FP32 F1\", \"INT8 answer\", \"INT8 F1\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Magnitude Pruning on DistilBERT in Bash\nDESCRIPTION: This bash script fine-tunes a DistilBERT model on the sst-2 task while applying magnitude pruning. It uses the `run_glue.py` script with specific parameters to enable pruning and training/evaluation. The script defines model paths, task, target sparsity, epochs, and output directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython run_glue.py \\\n    --model_name_or_path distilbert-base-uncased-finetuned-sst-2-english \\\n    --task_name sst2 \\\n    --apply_pruning \\\n    --target_sparsity 0.1 \\\n    --num_train_epochs 4 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/sst2_output \\\n    --overwrite_output_dir\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-aware) - Image-Text-to-Text - Python\nDESCRIPTION: Loads an InternVL2 model, applies weight-only quantization with 4 bits using the 'contextual' dataset, and saves the quantized model. Both `trust_remote_code` in `from_pretrained` and inside `OVWeightQuantizationConfig` are set to `True`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOVModelForVisualCausalLM.from_pretrained('OpenGVLab/InternVL2-1B', trust_remote_code=True, quantization_config=OVWeightQuantizationConfig(bits=4, dataset='contextual', trust_remote_code=True)).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Export SentenceTransformer to OpenVINO with INT8 Weight Quantization (CLI)\nDESCRIPTION: Exports the sentence-transformers/all-mpnet-base-v2 model to OpenVINO using optimum-cli, applying INT8 weight-only quantization and saving the converted model to the ./save_dir directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --library sentence_transformers -m sentence-transformers/all-mpnet-base-v2 --weight-format int8 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Knowledge Distillation with Quantization Aware Training in Bash\nDESCRIPTION: This bash script fine-tunes a DistilBERT model on the sst-2 task while applying knowledge distillation with quantization-aware training. It uses the `run_glue.py` script with specific parameters to enable distillation, quantization and training/evaluation. The script defines model paths, task, epochs, and output directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run_glue.py \\\n    --model_name_or_path distilbert-base-uncased \\\n    --task_name sst2 \\\n    --apply_distillation \\\n    --teacher_model_name_or_path distilbert-base-uncased-finetuned-sst-2-english \\\n    --apply_quantization \\\n    --num_train_epochs 1 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/sst2_output\n```\n\n----------------------------------------\n\nTITLE: Calibration Dataset Creation\nDESCRIPTION: This code shows how to create a calibration dataset for full quantization using the `OVQuantizer`.  It defines a preprocessing function and uses `quantizer.get_calibration_dataset` to create the dataset from a Hugging Face dataset.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\ndef preprocess_function(examples, tokenizer):\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=128, truncation=True)\n\n# Create the calibration dataset used to perform full quantization\ncalibration_dataset = quantizer.get_calibration_dataset(\n    \"glue\",\n    dataset_config_name=\"sst2\",\n    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n    num_samples=300,\n    dataset_split=\"train\",\n)\n```\n\n----------------------------------------\n\nTITLE: Prune Once For All - Stage 1 Fine-tuning in Bash\nDESCRIPTION: This example reproduces the first stage of the Prune Once For All approach by fine-tuning a pre-trained DistilBERT model with 90% sparsity on the SQuAD1.0 dataset using `run_qa.py`.  It applies distillation and pattern lock pruning during fine-tuning. The `pruning_config` specifies the configuration file for pattern lock pruning.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/question-answering/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# for stage 1\npython run_qa.py \\\n    --model_name_or_path Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa \\\n    --dataset_name squad \\\n    --apply_distillation \\\n    --generate_teacher_logits \\\n    --teacher_model_name_or_path distilbert-base-uncased-distilled-squad \\\n    --apply_pruning \\\n    --pruning_config ../config/prune_pattern_lock.yml \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 1.8e-4 \\\n    --num_train_epochs 8 \\\n    --max_seq_length 384 \\\n    --per_device_train_batch_size 12 \\\n    --per_device_eval_batch_size 12 \\\n    --pad_to_max_length \\\n    --warmup_ratio 0.05 \\\n    --weight_decay 0.01 \\\n    --output_dir /tmp/squad_output_stage1\n```\n\n----------------------------------------\n\nTITLE: Exporting Model using export_from_model\nDESCRIPTION: Exports a pre-trained model to OpenVINO format using the `export_from_model` function from `optimum.exporters.openvino`. This is an alternative to exporting during model loading. It requires specifying the model and the task for which it will be used.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM\nfrom optimum.exporters.openvino import export_from_model\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\nexport_from_model(model, output=\"ov_model\", task=\"text-generation-with-past\")\n```\n\n----------------------------------------\n\nTITLE: Knowledge Distillation with INCTrainer in Optimum Intel\nDESCRIPTION: This code demonstrates how to perform knowledge distillation using `DistillationConfig` and `INCTrainer` from the `optimum-intel` library.  A teacher model is loaded, and `DistillationConfig` is used to configure the distillation process.  The `INCTrainer` is then used to train the student model while leveraging the knowledge from the teacher model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_8\n\nLANGUAGE: diff\nCODE:\n```\n- from transformers import Trainer\n+ from optimum.intel import INCTrainer\n+ from neural_compressor import DistillationConfig\n\n+ teacher_model_id = \"textattack/bert-base-uncased-SST-2\"\n+ teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_id)\n+ distillation_config = DistillationConfig(teacher_model=teacher_model)\n\n- trainer = Trainer(\n+ trainer = INCTrainer(\n      model=model,\n+     distillation_config=distillation_config,\n      args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=False),\n      train_dataset=dataset[\"train\"].select(range(300)),\n      eval_dataset=dataset[\"validation\"],\n      compute_metrics=compute_metrics,\n      tokenizer=tokenizer,\n      data_collator=default_data_collator,\n  )\n\n  train_result = trainer.train()\n  metrics = trainer.evaluate()\n  trainer.save_model()\n\n  model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n```\n\n----------------------------------------\n\nTITLE: Compile Model on GPU Conditionally\nDESCRIPTION: This snippet conditionally reshapes and compiles the model on the GPU if a GPU device is available. The code checks for the presence of \"GPU\" in the list of available devices obtained from `Core().available_devices`. If a GPU is found, the model is reshaped, moved to the GPU using `model.to(\"gpu\")`, and then compiled.  The device of the model is then printed.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# Use `model.to()` to compile the model on GPU if a GPU is found\nif \"GPU\" in Core().available_devices:\n    model.reshape(1, 28)\n    model.to(\"gpu\")\n    model.compile()\n    print(ov_pipe.model._device)\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-free) - Text-to-Image - Python\nDESCRIPTION: Loads a Stable Diffusion model, applies weight-only quantization with 8 bits, and saves the quantized model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nOVStableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', quantization_config=OVWeightQuantizationConfig(bits=8)).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Prune Once For All - Stage 2 Quantization Aware Training in Bash\nDESCRIPTION: This example reproduces the second stage of the Prune Once For All approach, taking the model fine-tuned in stage 1 and applying quantization-aware training. This stage uses distillation, pruning (pattern lock), and quantization to obtain a fine-tuned quantized model with the same sparsity as the pre-trained model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/question-answering/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# for stage 2\npython run_qa.py \\\n    --model_name_or_path /tmp/squad_output_stage1 \\\n    --dataset_name squad \\\n    --apply_distillation \\\n    --generate_teacher_logits \\\n    --teacher_model_name_or_path distilbert-base-uncased-distilled-squad \\\n    --apply_pruning \\\n    --pruning_config ../config/prune_pattern_lock.yml \\\n    --apply_quantization \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 384 \\\n    --per_device_train_batch_size 12 \\\n    --per_device_eval_batch_size 12 \\\n    --pad_to_max_length \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0 \\\n    --verify_loading \\\n    --output_dir /tmp/squad_output_stage2\n```\n\n----------------------------------------\n\nTITLE: Moving a Model to GPU for Inference in Python\nDESCRIPTION: Illustrates how to move an OpenVINO model to an Intel GPU device for inference. This leverages the GPU's parallel processing capabilities for potentially faster inference. Requires the model to be an instance of `OVModel` and proper OpenVINO GPU drivers installed.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel.to(\"gpu\")\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning RoBERTa with Quantization Aware Training and Pruning (MLM)\nDESCRIPTION: This bash script fine-tunes a RoBERTa model on the WikiText-2 dataset, applying quantization aware training and snip_momentum pruning with a target sparsity of 0.02. The script trains for 4 epochs with a maximum of 100 training samples. It uses masked language modeling (MLM) and saves the output to /tmp/mlm_output.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/language-modeling/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run_mlm.py \\\n    --model_name_or_path bert-base-uncased \\\n    --dataset_name wikitext \\\n    --dataset_config_name wikitext-2-raw-v1 \\\n    --apply_quantization \\\n    --quantization_approach aware_training \\\n    --apply_pruning \\\n    --target_sparsity 0.02 \\\n    --num_train_epochs 4 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/mlm_output\n```\n\n----------------------------------------\n\nTITLE: Export Stateless Model\nDESCRIPTION: This code exports the Phi-2 model again, this time with `stateful=False` to support speculative decoding methods. The model is saved in a different directory to differentiate it from the stateful model. The stateless model is then compiled for efficient use.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Save the model in a different directory to set it apart from the stateful model\nsave_name = model_name.split(\"/\")[-1] + \"_openvino_stateless\"\n\nload_kwargs[\"ov_config\"][\"CACHE_DIR\"] = os.path.join(save_name, \"model_cache\")\n\n# Check whether the model was already exported\nsaved = os.path.exists(save_name)\n\n# We can use the same loading attributes, the only differece is the stateful attribute\nstateless_model = OVModelForCausalLM.from_pretrained(\n    model_name if not saved else save_name,\n    export=not saved,\n    stateful=False,\n    **load_kwargs,\n)\n\n# Save the exported model locally\nif not saved:\n    stateless_model.save_pretrained(save_name)\n    tokenizer.save_pretrained(save_name)\n\nstateless_model.compile()\n```\n\n----------------------------------------\n\nTITLE: Post-Training Static Quantization for Translation with INC\nDESCRIPTION: This example demonstrates how to apply post-training static quantization to a T5 model for translation using the `run_translation_post_training.py` script and the Intel Neural Compressor. It involves setting parameters such as the model name, source and target languages, dataset, quantization approach, and number of calibration samples. The script evaluates the quantized model and specifies batch sizes and an output directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/translation/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_translation_post_training.py \\\n    --model_name_or_path t5-small \\\n    --source_lang en \\\n    --target_lang ro \\\n    --dataset_name wmt16 \\\n    --dataset_config_name ro-en \\\n    --source_prefix \"translate English to Romanian: \" \\\n    --quantization_approach static \\\n    --num_calibration_samples 50 \\\n    --do_eval \\\n    --verify_loading \\\n    --predict_with_generate \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --output_dir /tmp/test_translation\n```\n\n----------------------------------------\n\nTITLE: Loading Quantized Model with INCModelForSequenceClassification\nDESCRIPTION: This code shows how to load a quantized model using `INCModelForSequenceClassification` from the `optimum-intel` library. The `from_pretrained` method is used to load the quantized model from a specified model name or path, which can be either a local directory or a Hugging Face Model Hub ID.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import INCModelForSequenceClassification\n\nmodel_name = \"Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic\"\nmodel = INCModelForSequenceClassification.from_pretrained(model_name)\n```\n\n----------------------------------------\n\nTITLE: Load CodeGen with OpenVINO\nDESCRIPTION: This code snippet loads the CodeGen model using OpenVINO, specifying configurations such as device, performance hint, inference precision hint, cache directory, and quantization configuration. It checks if the model has already been exported and either loads it or exports it. The model is then compiled and saved locally.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Load kwargs\nload_kwargs = {\n    \"device\": device,\n    \"ov_config\": {\n        \"PERFORMANCE_HINT\": \"LATENCY\",\n        \"INFERENCE_PRECISION_HINT\": precision,\n        \"CACHE_DIR\": os.path.join(save_name, \"model_cache\"),  # OpenVINO will use this directory as cache\n    },\n    \"compile\": False,\n    \"quantization_config\": quantization_config\n}\n\n# Check whether the model was already exported\nsaved = os.path.exists(save_name)\n\nasst_model = OVModelForCausalLM.from_pretrained(\n    model_name if not saved else save_name,\n    export=not saved,\n    stateful=False,\n    **load_kwargs,\n)\n\n# Save the exported model locally\nif not saved:\n    asst_model.save_pretrained(save_name)\n    tokenizer.save_pretrained(save_name)\n\nasst_model.compile()\n```\n\n----------------------------------------\n\nTITLE: Reshaping and Compiling Stable Diffusion model in OpenVINO\nDESCRIPTION: This snippet demonstrates how to statically reshape and compile a Stable Diffusion model for faster inference using OpenVINO. The `reshape` method allows specifying the batch size, height, width, and number of images per prompt.  Compilation is done via the `compile()` method.  These steps optimize the model for specific input shapes.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define the shapes related to the inputs and desired outputs\nbatch_size, num_images, height, width = 1, 1, 512, 512\n# Statically reshape the model\npipeline.reshape(batch_size=batch_size, height=height, width=width, num_images_per_prompt=num_images)\n# Compile the model before the first inference\npipeline.compile()\n\n# Run inference\nimages = pipeline(prompt, height=height, width=width, num_images_per_prompt=num_images).images\n```\n\n----------------------------------------\n\nTITLE: Exporting Stable Diffusion XL - Optimum CLI\nDESCRIPTION: Exports a Stable Diffusion XL model to the OpenVINO IR format. This decomposes the model into its component parts (text encoder, U-Net, VAE encoder, VAE decoder). The output models are saved in the ov_sdxl/ directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model stabilityai/stable-diffusion-xl-base-1.0 ov_sdxl/\n```\n\n----------------------------------------\n\nTITLE: Setting Quantization Level for Distributed Tuning in Python\nDESCRIPTION: This python snippet sets the `quant_level` to 1 in `run_glue_post_training.py` for distributed tuning. It imports `PostTrainingQuantConfig` and initializes it with the quantization approach and quant level. It's a part of enabling accuracy-aware tuning process with multi-node setup.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom neural_compressor import PostTrainingQuantConfig\nquantization_config = PostTrainingQuantConfig(approach=optim_args.quantization_approach, quant_level=1)\n```\n\n----------------------------------------\n\nTITLE: Hybrid Quantization (Data-aware) - Text-to-Image - CLI\nDESCRIPTION: Exports a Stable Diffusion model to OpenVINO format with weight-only quantization in hybrid mode using the optimum-cli and conceptual_captions dataset.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m dreamlike-art/dreamlike-anime-1.0 --weight-format int8 --dataset conceptual_captions ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Quantize CodeBERT to INT8 with OpenVINO Quantization and WikiText2 Dataset (Python)\nDESCRIPTION: This Python code quantizes the microsoft/codebert-base model to INT8 precision using OVQuantizationConfig and calibrates with the wikitext2 dataset before saving to a specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nOVModelForFeatureExtraction.from_pretrained('microsoft/codebert-base', quantization_config=OVQuantizationConfig(bits=8, dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Loading Model and Tokenizer\nDESCRIPTION: This code loads a pre-trained question-answering model and its corresponding tokenizer from the Hugging Face Hub. It then prints the tokenized output of a sample sentence to demonstrate the tokenizer's functionality.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModelForQuestionAnswering.from_pretrained(MODEL_ID)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# See how the tokenizer for the given model converts input text to model input values\nprint(tokenizer(\"hello world!\"))\n```\n\n----------------------------------------\n\nTITLE: Quantizing Stable Diffusion with Dynamic Quantization\nDESCRIPTION: This bash command demonstrates how to apply dynamic post-training quantization to a Stable Diffusion model using the `run_diffusion_post_training.py` script. It specifies the model, enables quantization, sets the quantization approach to `dynamic`, verifies loading, provides paths to ground truth images, input and calibration texts, and sets the output directory. The `lambdalabs/sd-pokemon-diffusers` model is used here.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-to-image/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_diffusion_post_training.py \\\n    --model_name_or_path lambdalabs/sd-pokemon-diffusers \\\n    --apply_quantization \\\n    --quantization_approach dynamic \\\n    --verify_loading \\\n    --base_images base_images \\ # The path of the ground truth pictures\n    --input_text \"a drawing of a gray and black dragon\" \\\n    --calibration_text \"a drawing of a green pokemon with red eyes\" \\ # The prompt to calibrate for static quantization\n    --output_dir /tmp/diffusion_output\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-free) - Text Generation - CLI\nDESCRIPTION: Exports a TinyLlama model to OpenVINO format with int8 weight-only quantization using the optimum-cli. This is a data-free quantization method. It quantizes the weights of the model to 8-bit integers without using any training data.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 --weight-format int8 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Prepare CodeGen Draft Model\nDESCRIPTION: This snippet prepares the CodeGen-350M-Multi model as a draft model for speculative decoding. It sets the model name, save name, precision, quantization configuration, and device. The quantization configuration specifies the bits, symmetry, group size, and ratio for weight quantization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"Salesforce/codegen-350M-multi\"\nsave_name = model_name.split(\"/\")[-1] + \"_openvino_stateless\"\nprecision = \"f32\"\nquantization_config = OVWeightQuantizationConfig(\n    bits=4,\n    sym=False,\n    group_size=128,\n    ratio=0.8,\n)\ndevice = \"cpu\"\n```\n\n----------------------------------------\n\nTITLE: Compile the model\nDESCRIPTION: This code compiles the loaded OpenVINO model for efficient inference.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel.compile()\n```\n\n----------------------------------------\n\nTITLE: Language Generation with Static Quantization (bash)\nDESCRIPTION: This command applies post-training static quantization to a GPT-J model. It utilizes the `run_generation.py` script, specifying the model type, path, quantization approach, and smooth quantization parameters. The model is quantized statically with a specified alpha value for smooth quantization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-generation/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_generation.py \\\n    --model_type=gptj \\\n    --model_name_or_path=EleutherAI/gpt-j-6b \\\n    --apply_quantization \\\n    --quantization_approach static\\\n    --smooth_quant \\\n    --smooth_quant_alpha 0.7\n```\n\n----------------------------------------\n\nTITLE: Distillation for Quantization\nDESCRIPTION: This command performs distillation for quantization using an FP32 Textual Inversion model to generate an INT8 Textual Inversion model.  It utilizes parameters like the pretrained model path, training data directory, placeholder token, resolution, batch size, learning rate, and output directory, along with flags for quantization and distillation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/textual-inversion/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport FP32_MODEL_NAME=\"./dicoo_model\"\nexport DATA_DIR=\"./dicoo\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$FP32_MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --use_ema --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=300 \\\n  --learning_rate=5.0e-04 --max_grad_norm=3 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"int8_model\" \\\n  --do_quantization --do_distillation --verify_loading\n```\n\n----------------------------------------\n\nTITLE: Full Quantization - Text-to-Image - Python\nDESCRIPTION: Loads a Stable Diffusion model, applies full quantization with 8 bits using the conceptual_captions dataset, and saves the quantized model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nOVStableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', quantization_config=OVQuantizationConfig(bits=8, dataset='conceptual_captions')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Static Quantization on BERT with Neural Compressor\nDESCRIPTION: This example demonstrates how to perform post-training static quantization on a BERT model fine-tuned on the SWAG dataset using the Intel Neural Compressor. The script `run_swag_post_training.py` is used with the `--apply_quantization` and `--quantization_approach static` flags. The `--num_calibration_samples` parameter specifies the number of samples to use for calibration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/multiple-choice/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_swag_post_training.py \\\n    --model_name_or_path ehdwns1516/bert-base-uncased_SWAG \\\n    --apply_quantization \\\n    --quantization_approach static \\\n    --num_calibration_samples 50 \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/swag_output\n```\n\n----------------------------------------\n\nTITLE: Preparing History for Model Input\nDESCRIPTION: This function `prepare_history_for_model` converts the dialogue history into a tokenized prompt that the model expects. It formats the history into a list of messages with 'User' and 'Assistant' roles and applies the chat template using `tokenizer.apply_chat_template`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_history_for_model(history):\n    \"\"\"\n    Converts the history to a tokenized prompt in the format expected by the model.\n    Params:\n      history: dialogue history\n    Returns:\n      Tokenized prompt\n    \"\"\"\n    messages = []\n    for idx, (user_msg, model_msg) in enumerate(history):\n        # skip the last assistant message if its empty, the tokenizer will do the formating\n        if idx == len(history) - 1 and not model_msg:\n            messages.append({\"role\": \"User\", \"content\": user_msg})\n            break\n        if user_msg:\n            messages.append({\"role\": \"User\", \"content\": user_msg})\n        if model_msg:\n            messages.append({\"role\": \"Assistant\", \"content\": model_msg})\n    input_token = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_tensors=\"pt\",\n        return_dict=True\n    )\n    return input_token\n```\n\n----------------------------------------\n\nTITLE: Magnitude Pruning for Token Classification (Bash)\nDESCRIPTION: This command fine-tunes a DistilBERT model on the CoNLL-2003 dataset while applying magnitude pruning.  It uses the `run_ner.py` script with the `--apply_pruning` flag to enable pruning.  The `--target_sparsity` parameter controls the desired sparsity level. The `--apply_quantization` flag is also set, though the description mainly focuses on pruning.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/token-classification/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run_ner.py \\\n    --model_name_or_path elastic/distilbert-base-uncased-finetuned-conll03-english \\\n    --dataset_name conll2003 \\\n    --apply_quantization \\\n    --apply_pruning \\\n    --target_sparsity 0.1 \\\n    --num_train_epochs 4 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/conll03_output\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-aware) - Image-Text-to-Text - CLI\nDESCRIPTION: Exports an InternVL2 model to OpenVINO format with int4 weight-only quantization using the optimum-cli and the 'contextual' dataset.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --task image-text-to-text -m OpenGVLab/InternVL2-1B --trust-remote-code --weight-format int4 --dataset contextual ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Export CodeBERT to OpenVINO with INT8 Quantization and WikiText2 Dataset (CLI)\nDESCRIPTION: This command exports the microsoft/codebert-base model using optimum-cli to OpenVINO, employing INT8 quantization calibrated on the wikitext2 dataset and saving the model to the specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m microsoft/codebert-base --quant-mode int8 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Inference on Audio Data with OpenVINO Pipeline\nDESCRIPTION: This code snippet showcases how to perform inference on audio data using an OpenVINO pipeline. It checks if `audio_sample` is a dictionary (dataset item) or a path to an audio file. It extracts the audio data and sampling rate accordingly, then displays the audio using IPython. Finally, it runs inference on the audio data using the `ov_pipe` pipeline, returning the top 3 predictions.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nif isinstance(audio_sample, dict):\n    audio_data = audio_sample[\"audio\"][\"array\"]\n    sampling_rate = audio_sample[\"audio\"][\"sampling_rate\"]\nelse:\n    # if audio_sample is not a dataset item, it should be the path to an audio file\n    audio_data = audio_sample\n    sampling_rate = None\n\ndisplay(Audio(audio_data, rate=sampling_rate))\n\nov_pipe(audio_data, top_k=3)\n```\n\n----------------------------------------\n\nTITLE: Generating Assistant Response with Model\nDESCRIPTION: The `generate` function generates the assistant's response given the chatbot history and generation parameters such as temperature, max_new_tokens, top_p and repetition_penalty. It prepares the input, handles streaming output, applies stopping criteria, and updates the chatbot history. It supports assisted generation with speculative decoding.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef generate(history, temperature, max_new_tokens, top_p, repetition_penalty, assisted):\n    \"\"\"\n    Generates the assistant's reponse given the chatbot history and generation parameters\n\n    Params:\n      history: conversation history formated in pairs of user and assistant messages `[user_message, assistant_message]`\n      temperature:  parameter for control the level of creativity in AI-generated text.\n                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n      max_new_tokens: The maximum number of tokens we allow the model to generate as a response.\n      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n      assisted: boolean parameter to enable/disable assisted generation with speculative decoding.\n    Yields:\n      Updated history and generation status.\n    \"\"\"\n    start = time.perf_counter()\n    # Construct the input message string for the model by concatenating the current system message and conversation history\n    # Tokenize the messages string\n    inputs = prepare_history_for_model(history)\n    input_length = inputs['input_ids'].shape[1]\n    # truncate input in case it is too long.\n    # TODO improve this\n    if input_length > 2000:\n        history = [history[-1]]\n        inputs = prepare_history_for_model(history)\n        input_length = inputs['input_ids'].shape[1]\n\n    prompt_char = \"\"\n    history[-1][1] = prompt_char\n    yield history, \"Status: Generating...\", *([gr.update(interactive=False)] * 4)\n    \n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    # Create a stopping criteria to prevent the model from playing the role of the user aswell.\n    stop_str = [\"\\nUser:\", \"\\nAssistant:\", \"\\nRules:\", \"\\nQuestion:\"]\n    stopping_criteria = StoppingCriteriaList([SuffixCriteria(input_length, stop_str, tokenizer)])\n    # Prepare input for generate\n    generation_config = GenerationConfig(\n        max_new_tokens=max_new_tokens,\n        do_sample=temperature > 0.0,\n        temperature=temperature if temperature > 0.0 else 1.0,\n        repetition_penalty=repetition_penalty,\n        top_p=top_p,\n        eos_token_id=[tokenizer.eos_token_id],\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    generate_kwargs = dict(\n        streamer=streamer,\n        generation_config=generation_config,\n        stopping_criteria=stopping_criteria,\n    ) | inputs\n\n    if assisted:\n        target_generate = stateless_model.generate\n        generate_kwargs[\"assistant_model\"] = asst_model\n    else:\n        target_generate = model.generate\n\n    t1 = Thread(target=target_generate, kwargs=generate_kwargs)\n    t1.start()\n\n    # Initialize an empty string to store the generated text.\n    partial_text = \"\"\n    for new_text in streamer:\n        partial_text += new_text\n        history[-1][1] = partial_text + prompt_char\n        for s in stop_str:\n            if (pos := partial_text.rfind(s)) != -1:\n                break\n        if pos != -1:\n            partial_text = partial_text[:pos]\n            break\n        elif any([is_partial_stop(partial_text, s) for s in stop_str]):\n            continue\n        yield history, \"Status: Generating...\", *([gr.update(interactive=False)] * 4)\n    history[-1][1] = partial_text\n    generation_time = time.perf_counter() - start\n    yield history, f'Generation time: {generation_time:.2f} sec', *([gr.update(interactive=True)] * 4)\n```\n\n----------------------------------------\n\nTITLE: Post-Training Static Quantization for Token Classification (Bash)\nDESCRIPTION: This command applies post-training static quantization to a DistilBERT model fine-tuned on the CoNLL-2003 dataset using the Intel Neural Compressor. It utilizes the `run_ner_post_training.py` script and requires the `model_name_or_path` and `dataset_name` parameters.  The `--apply_quantization` flag enables quantization, and `--quantization_approach static` specifies static quantization.  `--num_calibration_samples` sets the number of samples for calibration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/token-classification/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_ner_post_training.py \\\n    --model_name_or_path elastic/distilbert-base-uncased-finetuned-conll03-english \\\n    --dataset_name conll2003 \\\n    --apply_quantization \\\n    --quantization_approach static \\\n    --num_calibration_samples 50 \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/conll03_output\n```\n\n----------------------------------------\n\nTITLE: Text Classification Pipeline with Static Shapes (Optimum Intel)\nDESCRIPTION: Demonstrates how to initialize a text classification pipeline with a maximum sequence length to enable static shapes for OpenVINO inference. This can improve performance by pre-allocating memory for tensors.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npipeline(\"text-classification\", model=model, tokenizer=tokenizer, max_length=128, padding=\"max_length\", truncation=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Model and Dataset Parameters\nDESCRIPTION: This snippet defines the model ID, dataset name, and whether the dataset uses version 2 with negative answers. It also sets up the file paths for storing the FP32 and INT8 quantized models.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nMODEL_ID = \"csarron/bert-base-uncased-squad-v1\"\nDATASET_NAME = \"squad\"\nVERSION_2_WITH_NEGATIVE = False\n\nbase_model_path = Path(f\"models/{MODEL_ID}\")\nfp32_model_path = base_model_path.with_name(base_model_path.name + \"_FP32\")\nint8_ptq_model_path = base_model_path.with_name(base_model_path.name + \"_INT8_PTQ\")\n```\n\n----------------------------------------\n\nTITLE: Fine-tune for FP32 Textual Inversion\nDESCRIPTION: This command fine-tunes a Stable Diffusion model on a given dataset to obtain an FP32 Textual Inversion model.  Key parameters include the pretrained model path, training data directory, learnable property, placeholder token, resolution, batch size, learning rate, and output directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/textual-inversion/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"./dicoo\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"dicoo_model\"\n```\n\n----------------------------------------\n\nTITLE: Acceptance Rate Recorder\nDESCRIPTION: This code defines a utility class `AcceptanceRateRecorder` to measure the acceptance rate of the draft model. It wraps the `forward` and `generate` methods of the model to record the sequence lengths and window sizes. The `acceptance_rate` method calculates the acceptance rate based on the recorded data.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import wraps\nimport numpy as np\n\n\nclass AcceptanceRateRecorder:\n    def __init__(self, model):\n        self.model = model\n        self.model_forward = None\n        self.model_generate = None\n        self.seq_lens = []\n        self.win_sizes = []\n\n    def __enter__(self):\n        # wrap forward method\n        if len(self.seq_lens) > 0 or len(self.win_sizes) > 0:\n            raise RuntimeError(\"Always use a new instance, don't reuse!\")\n        self.model_forward = self.model.forward\n        \n        @wraps(self.model_forward)\n        def forward_wrapper(**kwargs):\n            self.seq_lens[-1].append(kwargs.get(\"attention_mask\").shape[-1])\n            self.win_sizes[-1].append(kwargs.get(\"input_ids\").shape[-1] - 1)\n            return self.model_forward(**kwargs)\n        \n        self.model.forward = forward_wrapper\n        \n        # wrap generate method\n        self.model_generate = self.model.generate\n\n        @wraps(self.model_generate)\n        def generate_wrapper(*args, **kwargs):\n            self.seq_lens.append([])\n            self.win_sizes.append([])\n            input_ids = args[0] if len(args) > 0 else kwargs.get(\"input_ids\")\n            self.seq_lens[-1].append(input_ids.shape[-1])\n            out = self.model_generate(*args, **kwargs)\n            self.seq_lens[-1].append(out.shape[-1])\n            return out\n        self.model.generate = generate_wrapper\n        return self\n\n    def __exit__(self,  type, value, traceback):\n        self.model.forward = self.model_forward\n        self.model.generate = self.model_generate\n        self.model_forward = None\n        self.model_generate = None\n        # Fix first window size\n        for ws, sl in zip(self.win_sizes, self.seq_lens):\n            ws[0] -= sl[0] - 1\n        # Delete first seq_len, not needed anymore\n        self.seq_lens = [sl[1:] for sl in self.seq_lens]\n        # Add window size for output to ease calculation later\n        for ws, sl in zip(self.win_sizes, self.seq_lens):\n            ws.append(0)    \n\n    def acceptance_rate(self, return_mean=True, normalize=False):\n        # ar_per_win = ((cur_seq_len - cur_win_size) - (prev_seq_len - prev_win_size) - 1) / prev_win_size\n        ar_per_win = []\n        for sl, ws in zip(self.seq_lens, self.win_sizes):\n            sl = np.array(sl, dtype=np.float64)\n            ws = np.array(ws, dtype=np.float64)\n            out_lens = sl - ws\n            accepted = (out_lens[1:] - out_lens[:-1] - 1)\n            ar_per_win.append(np.divide(accepted, ws[:-1],\n                                   out=np.zeros_like(accepted),where=ws[:-1] != 0))\n        ar_per_win = np.hstack(ar_per_win)\n        # Normalized AR doesn't take into account windows with size 0\n        if normalize:\n            ar_per_win = ar_per_win[np.nonzero(np.hstack([ws[:-1] for ws in self.win_sizes]))]\n        return np.mean(ar_per_win) if return_mean else ar_per_win\n```\n\n----------------------------------------\n\nTITLE: Dynamic Quantization with Accuracy Tolerance (Python)\nDESCRIPTION: Demonstrates dynamic quantization with accuracy tolerance and evaluation function using Optimum Intel and Neural Compressor. It defines an evaluation function, sets accuracy and tuning criteria, and performs quantization with the specified configuration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport evaluate\nfrom optimum.intel import INCQuantizer\nfrom datasets import load_dataset\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nfrom neural_compressor.config import AccuracyCriterion, TuningCriterion, PostTrainingQuantConfig\n\nmodel_name = \"distilbert-base-cased-distilled-squad\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\neval_dataset = load_dataset(\"squad\", split=\"validation\").select(range(64))\ntask_evaluator = evaluate.evaluator(\"question-answering\")\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\ndef eval_fn(model):\n    qa_pipeline.model = model\n    metrics = task_evaluator.compute(model_or_pipeline=qa_pipeline, data=eval_dataset, metric=\"squad\")\n    return metrics[\"f1\"]\n\n# Set the accepted accuracy loss to 5%\naccuracy_criterion = AccuracyCriterion(tolerable_loss=0.05)\n# Set the maximum number of trials to 10\ntuning_criterion = TuningCriterion(max_trials=10)\nquantization_config = PostTrainingQuantConfig(\n    approach=\"dynamic\", accuracy_criterion=accuracy_criterion, tuning_criterion=tuning_criterion\n)\nquantizer = INCQuantizer.from_pretrained(model, eval_fn=eval_fn)\nquantizer.quantize(quantization_config=quantization_config, save_directory=\"dynamic_quantization\")\n```\n\n----------------------------------------\n\nTITLE: Sequence Classification with Optimum Intel and OpenVINO\nDESCRIPTION: This code snippet shows how to perform sequence classification using `OVModelForSequenceClassification` from the `optimum.intel` library with OpenVINO. It loads a pre-trained language detection model, creates a pipeline, and classifies a Dutch sentence.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForSequenceClassification\nfrom transformers import AutoTokenizer, pipeline\n\nmodel_id = \"helenai/papluca-xlm-roberta-base-language-detection-ov\"\nmodel = OVModelForSequenceClassification.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nov_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n\n# this is a Dutch (nl) sentence, it means \"hello world!\"\nov_pipe(\"hallo wereld!\")\n```\n\n----------------------------------------\n\nTITLE: Single-Node Distributed Training with oneCCL\nDESCRIPTION: This command launches a distributed training job with 2 processes on a single node using `mpirun`.  It sets environment variables like `CCL_WORKER_COUNT` and `MASTER_ADDR`, and then executes a PyTorch training script (`run_qa.py`) with specific arguments to configure the model, dataset, quantization, and distributed backend.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/distributed_training.mdx#_snippet_3\n\nLANGUAGE: shell script\nCODE:\n```\nexport CCL_WORKER_COUNT=1\nexport MASTER_ADDR=127.0.0.1\nmpirun -n 2 -genv OMP_NUM_THREADS=23 \\\npython3 run_qa.py \\\n    --model_name_or_path distilbert-base-uncased-distilled-squad \\\n    --dataset_name squad \\\n    --apply_quantization \\\n    --quantization_approach static \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/squad_output \\\n    --no_cuda \\\n    --xpu_backend ccl\n```\n\n----------------------------------------\n\nTITLE: Language Generation with JIT and Static Quantization (bash)\nDESCRIPTION: This command applies post-training static quantization to a GPT-J model and uses JIT compilation for further optimization. It utilizes the `run_generation.py` script, specifying the model type, path, quantization approach, smooth quantization parameters, and enabling JIT compilation. This command demonstrates how to combine static quantization with JIT for performance improvements.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-generation/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_generation.py \\\n    --model_type=gptj \\\n    --model_name_or_path=EleutherAI/gpt-j-6b \\\n    --apply_quantization \\\n    --quantization_approach static\\\n    --smooth_quant \\\n    --smooth_quant_alpha 0.7 \\\n    --jit\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Original Stable Diffusion Model\nDESCRIPTION: This bash command benchmarks the performance of the original (FP32) Stable Diffusion model using `run_diffusion_post_training.py`. It specifies the model, sets the output directory, provides paths to ground truth images, input text, and enables the benchmark flag.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-to-image/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_diffusion_post_training.py \\\n    --model_name_or_path sd-pokemon-diffusers \\\n    --output_dir /tmp/diffusion_output \\\n    --base_images base_images \\   # The path of the ground truth pictures\n    --input_text \"a drawing of a gray and black dragon\" \\\n    --benchmark\n```\n\n----------------------------------------\n\nTITLE: Benchmarking FP32 Model Performance\nDESCRIPTION: Uses OpenVINO's `benchmark_app` to measure the performance of the FP32 baseline model. The command specifies the model path, input shape, API type, and number of iterations.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!benchmark_app -m all-MiniLM-L6-v2/openvino_model.xml -shape \"input_ids[1,384],attention_mask[1,384],token_type_ids[1,384]\" -api sync -niter 200\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image with Textual Inversion in Stable Diffusion\nDESCRIPTION: This example demonstrates how to incorporate textual inversion into the Stable Diffusion pipeline using OpenVINO. It first runs the pipeline without textual inversion, then loads a textual inversion embedding, and runs the pipeline again with the embedding. The `load_textual_inversion` method is used to inject the embedding into the pipeline. `clear_requests` is called to clear the pipeline before loading the textual inversion.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionPipeline\nimport numpy as np\n\nmodel_id = \"echarlaix/stable-diffusion-v1-5-openvino\"\nprompt = \"A <cat-toy> back-pack\"\n# Set a random seed for better comparison\nnp.random.seed(42)\n\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=False, compile=False)\npipeline.compile()\nimage1 = pipeline(prompt, num_inference_steps=50).images[0]\nimage1.save(\"stable_diffusion_v1_5_without_textual_inversion.png\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Reset stable diffusion pipeline\npipeline.clear_requests()\n\n# Load textual inversion into stable diffusion pipeline\npipeline.load_textual_inversion(\"sd-concepts-library/cat-toy\", \"<cat-toy>\")\n\n# Compile the model before the first inference\npipeline.compile()\nimage2 = pipeline(prompt, num_inference_steps=50).images[0]\nimage2.save(\"stable_diffusion_v1_5_with_textual_inversion.png\")\n```\n\n----------------------------------------\n\nTITLE: Exporting with INT8 Weights - Optimum CLI\nDESCRIPTION: Exports a model with INT8 weight compression using the optimum-cli. This reduces the model size and can improve inference speed, especially on hardware that supports INT8 operations. The output model is saved in the ov_model/ directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Meta-Llama-3-8B --weight-format int8 ov_model/\n```\n\n----------------------------------------\n\nTITLE: Loading Model with HuggingFacePipeline\nDESCRIPTION: This code loads a language model using the `HuggingFacePipeline.from_model_id` method from the `langchain_huggingface.llms` module.  It specifies the model ID, task, pipeline keyword arguments (max_new_tokens), and backend (ipex). The `ipex` backend enables Intel extension for PyTorch for faster inference.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/langchain_hf_pipelines.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_huggingface.llms import HuggingFacePipeline\n\nhf = HuggingFacePipeline.from_model_id(\n    model_id=\"gpt2\",\n    task=\"text-generation\",\n    pipeline_kwargs={\"max_new_tokens\": 10},\n    backend=\"ipex\",\n)\n```\n\n----------------------------------------\n\nTITLE: Save Model in FP16 Format\nDESCRIPTION: This snippet shows how to convert a model's weights to FP16 precision using `model.half()` and then save the model. This reduces the model size and can improve inference speed, often with a minimal impact on accuracy. Saving the model in FP16 format is beneficial for deployment scenarios with limited resources.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nmodel.half()\nmodel.save_pretrained(\"distilbert-base-uncased-distilled-squad-ov-fp16\")\n```\n\n----------------------------------------\n\nTITLE: Creating Question-Answering Pipelines\nDESCRIPTION: This code creates two question-answering pipelines: one for the quantized OpenVINO model and one for the original FP32 model.  These pipelines simplify inference by combining the model, tokenizer, and task into a single object.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nquantized_model_ptq = OVModelForQuestionAnswering.from_pretrained(int8_ptq_model_path)\noriginal_model = AutoModelForQuestionAnswering.from_pretrained(MODEL_ID)\nov_qa_pipeline_ptq = pipeline(\"question-answering\", model=quantized_model_ptq, tokenizer=tokenizer)\nhf_qa_pipeline = pipeline(\"question-answering\", model=original_model, tokenizer=tokenizer)\n```\n\n----------------------------------------\n\nTITLE: Compiling an OpenVINO Model in Python\nDESCRIPTION: Shows how to explicitly compile an OpenVINO model after it has been loaded and potentially reshaped or moved to a different device.  Calling `model.compile()` ensures the model is optimized for the target device before running inference.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel.compile()\n```\n\n----------------------------------------\n\nTITLE: Reshaping an OpenVINO Model with Static Shapes in Python\nDESCRIPTION: Demonstrates how to reshape an OpenVINO model to use static input shapes for potentially faster inference. It uses the `reshape()` method to fix the batch size and sequence length. Requires an instance of `OVModel`. The shapes must be provided as arguments to `reshape()`.  Inference with inputs of different shapes after reshaping is not allowed.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Fix the batch size to 1 and the sequence length to 40\nbatch_size, seq_len = 1, 40\nmodel.reshape(batch_size, seq_len)\n```\n\n----------------------------------------\n\nTITLE: Generate Code Completion\nDESCRIPTION: This code uses the quantized Phi-2 model to generate a code completion for a given function signature and docstring. It tokenizes the input, calls the generate method with specified parameters (max_new_tokens, streamer, pad_token_id), and prints the generated code. TextStreamer is used for printing the tokens during generation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import TextStreamer\n\n# Tokenize the sample\ninputs = tokenizer([sample], return_tensors='pt')\n\n# Call generate on the inputs\nout = model.generate(\n    **inputs,\n    max_new_tokens=128,\n    streamer=TextStreamer(tokenizer=tokenizer, skip_special_tokens=True),\n    pad_token_id=tokenizer.eos_token_id,\n)\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-aware) - Text Generation - CLI\nDESCRIPTION: Exports a TinyLlama model to OpenVINO format with int4 weight-only quantization using the optimum-cli. This is a data-aware quantization method, which uses the wikitext2 dataset. It quantizes the weights of the model to 4-bit integers using a dataset.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 --weight-format int4 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Run Inference with Transformers Pipeline\nDESCRIPTION: This snippet demonstrates how to perform inference using a Transformers pipeline.  It loads an OpenVINO model and a tokenizer, creates a question-answering pipeline, and then uses the pipeline to answer a question given a context. The `pipeline` function from `transformers` is used to simplify the inference process.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom optimum.intel import OVModelForQuestionAnswering\nfrom transformers import AutoTokenizer, pipeline\n\nmodel = OVModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad-ov-fp32\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\nov_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\nov_pipe(\"What is OpenVINO?\", \"OpenVINO is a framework for deep learning inference optimization\")\n```\n\n----------------------------------------\n\nTITLE: Loading Baseline and Quantized Models\nDESCRIPTION: Loads the baseline FP32 model and the quantized INT8 model using `OVModelForFeatureExtraction.from_pretrained` and creates `SentenceEmbeddingPipeline` instances for each.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = OVModelForFeatureExtraction.from_pretrained(base_model_path)\nvanilla_emb = SentenceEmbeddingPipeline(model=model, tokenizer=tokenizer)\n\nq_model = OVModelForFeatureExtraction.from_pretrained(int8_ptq_model_path)\nq8_emb = SentenceEmbeddingPipeline(model=q_model, tokenizer=tokenizer)\n```\n\n----------------------------------------\n\nTITLE: Install Optimum and Dependencies\nDESCRIPTION: This cell installs the necessary dependencies, including `optimum[openvino]`, `ipywidgets`, `pillow`, `torchaudio`, `soundfile`, and `librosa`.  These packages are required for running the OpenVINO inference examples provided in the notebook. The audio classification example also requires ffmpeg.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# %pip install optimum[openvino] ipywidgets pillow torchaudio soundfile librosa\n```\n\n----------------------------------------\n\nTITLE: Hybrid Quantization (Data-aware) - Text-to-Image - Python\nDESCRIPTION: Loads a Stable Diffusion model, applies weight-only quantization with 8 bits in hybrid mode using the conceptual_captions dataset, and saves the quantized model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nOVStableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0', quantization_config=OVWeightQuantizationConfig(bits=8, quant_method='hybrid', dataset='conceptual_captions')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel from source\nDESCRIPTION: This command installs the Optimum Intel library directly from the GitHub repository.  It fetches the latest version of the code, allowing users to access the most recent features and bug fixes. Requires git to be installed.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/installation.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install git+https://github.com/huggingface/optimum-intel.git\n```\n\n----------------------------------------\n\nTITLE: Prune Once For All - Stage 1 in Bash\nDESCRIPTION: This bash script executes the first stage of the Prune Once For All (POFA) fine-tuning process. It fine-tunes a pre-trained DistilBERT model with a sparsity of 90% on the sst-2 task, applying distillation and pattern lock pruning. It uses `run_glue_during_training_optimization.py` with specified configuration and training parameters.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# for stage 1\npython run_glue_during_training_optimization.py \\\n    --model_name_or_path Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa \\\n    --task_name sst2 \\\n    --apply_distillation \\\n    --teacher_model_name_or_path distilbert-base-uncased-finetuned-sst-2-english \\\n    --apply_pruning \\\n    --pruning_config ../config/prune_pattern_lock.yml \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 1.5e-4 \\\n    --num_train_epochs 9 \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size 32 \\\n    --per_device_eval_batch_size 32 \\\n    --pad_to_max_length \\\n    --warmup_ratio 0 \\\n    --weight_decay 0 \\\n    --output_dir /tmp/sst2_output_stage1\n```\n\n----------------------------------------\n\nTITLE: Import IPEX Model Classes\nDESCRIPTION: Imports the necessary modules from `transformers` and `optimum.intel` to work with IPEX-optimized models.  Specifically, it imports `AutoTokenizer` from transformers and `IPEXModelForCausalLM` from `optimum.intel`.  These are required to load and utilize the IPEX-optimized model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/text_generation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import AutoTokenizer\nfrom optimum.intel import IPEXModelForCausalLM\n```\n\n----------------------------------------\n\nTITLE: Disabling Model Compilation during Loading in Python\nDESCRIPTION: Demonstrates how to load an `OVModelForQuestionAnswering` and disable the automatic model compilation during the loading process. This prevents immediate compilation which can inflate initial inference latency. Requires `optimum-intel`. The `compile=False` argument in `from_pretrained` disables the compilation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForQuestionAnswering\n\nmodel_id = \"distilbert/distilbert-base-cased-distilled-squad\"\n# Load the model and disable the model compilation\nmodel = OVModelForQuestionAnswering.from_pretrained(model_id, compile=False)\n```\n\n----------------------------------------\n\nTITLE: Custom Stopping Criteria for Generation\nDESCRIPTION: This code defines a custom stopping criteria class `SuffixCriteria` for text generation. It inherits from `StoppingCriteria` and checks if the generated sequence ends with any of the specified stop strings. This is used to prevent the model from generating unwanted continuations such as role playing both user and assistant.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass SuffixCriteria(StoppingCriteria):\n    def __init__(self, start_length, eof_strings, tokenizer, check_fn=None):\n        self.start_length = start_length\n        self.eof_strings = eof_strings\n        self.tokenizer = tokenizer\n        if check_fn is None:\n            check_fn = lambda decoded_generation: any(\n                [decoded_generation.endswith(stop_string) for stop_string in self.eof_strings]\n            )\n        self.check_fn = check_fn\n\n    def __call__(self, input_ids, scores, **kwargs):\n        \"\"\"Returns True if generated sequence ends with any of the stop strings\"\"\"\n        decoded_generations = self.tokenizer.batch_decode(input_ids[:, self.start_length :])\n        return all([self.check_fn(decoded_generation) for decoded_generation in decoded_generations])\n```\n\n----------------------------------------\n\nTITLE: Export CodeBERT Model to OpenVINO with NF4/F8E4M3 Quantization and WikiText2 (CLI)\nDESCRIPTION: Exports microsoft/codebert-base to OpenVINO using optimum-cli with mixed precision quantization (nf4_f8e4m3) and calibrates on the wikitext2 dataset, saving the resulting model to the specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m microsoft/codebert-base --quant-mode nf4_f8e4m3 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Computing Sentence Similarity and Evaluating STSB\nDESCRIPTION: Defines functions to compute cosine similarity between sentence embeddings and to evaluate the models on the STSB task. The `evaluate_stsb` function computes the similarity for both the baseline and quantized models and returns the results.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef compute_sentence_similarity(sentence_1, sentence_2, pipeline):\n    embedding_1 = pipeline(sentence_1)\n    embedding_2 = pipeline(sentence_2)\n    # compute cosine similarity between two sentences\n    return torch.nn.functional.cosine_similarity(embedding_1, embedding_2, dim=1)\n\n\ndef evaluate_stsb(example):\n    default = compute_sentence_similarity(example[\"sentence1\"], example[\"sentence2\"], vanilla_emb)\n    quantized = compute_sentence_similarity(example[\"sentence1\"], example[\"sentence2\"], q8_emb)\n    return {\n        \"reference\": (example[\"label\"] - 1) / (5 - 1),  # rescale to [0,1]\n        \"default\": float(default),\n        \"quantized\": float(quantized),\n    }\n\n\nresult = eval_dataset.map(evaluate_stsb)\n```\n\n----------------------------------------\n\nTITLE: Exporting a model with INT8 weight-only quantization using CLI\nDESCRIPTION: This command exports a GPT-2 model to the OpenVINO IR format and applies 8-bit weight-only quantization. The `--weight-format int8` argument specifies that the model's linear, embedding, and convolution weights should be quantized to INT8 precision.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_8\n\nLANGUAGE: Plain Text\nCODE:\n```\noptimum-cli export openvino --model gpt2 --weight-format int8 ov_model\n```\n\n----------------------------------------\n\nTITLE: List Available OpenVINO Devices\nDESCRIPTION: This snippet retrieves and prints the available devices for OpenVINO inference. The `Core().available_devices` property is used to get a list of devices, and `Core().get_property` is used to retrieve the full device name for each device. This helps identify available hardware for running inference.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.runtime import Core\n\nfor device in Core().available_devices:\n    print(device, Core().get_property(device, \"FULL_DEVICE_NAME\"))\n```\n\n----------------------------------------\n\nTITLE: Benchmarking INT8 Model Performance\nDESCRIPTION: Uses OpenVINO's `benchmark_app` to measure the performance of the INT8 quantized model. The command specifies the model path, input shape, API type, and number of iterations.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n!benchmark_app -m all-MiniLM-L6-v2_int8/openvino_model.xml -shape \"input_ids[1,384],attention_mask[1,384],token_type_ids[1,384]\" -api sync -niter 200\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset\nDESCRIPTION: This code loads the specified dataset using the `datasets` library and then prints a specific item from the training split to preview the data structure. This includes the question, context, and answer information.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndataset = datasets.load_dataset(DATASET_NAME)\ndataset[\"train\"][31415]\n```\n\n----------------------------------------\n\nTITLE: Calculating and Printing Evaluation Metrics\nDESCRIPTION: Computes the Pearson correlation coefficient for both the baseline and quantized models using the loaded metric. Then, prints the results and the accuracy of the quantized model relative to the FP32 model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndefault_acc = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\nquantized = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])\n\nprint(\"vanilla model: pearson=\", default_acc[\"pearson\"])\nprint(\"quantized model: pearson=\", quantized[\"pearson\"])\nprint(\n    \"The quantized model achieves \",\n    round(quantized[\"pearson\"] / default_acc[\"pearson\"], 2) * 100,\n    \"% accuracy of the fp32 model\",\n)\n```\n\n----------------------------------------\n\nTITLE: Image Classification with Optimum Intel\nDESCRIPTION: This code snippet demonstrates image classification using a pre-trained Swin Transformer model and the Optimum Intel library. It involves loading a model, image processor, creating a pipeline, and classifying an image from a URL.  The code requires `IPython`, `optimum.intel`, and `transformers` libraries. The model is reshaped to match the expected input size before compilation for static shape optimization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image\nfrom optimum.intel import OVModelForImageClassification\nfrom transformers import AutoImageProcessor, pipeline\n\nmodel_id = \"helenai/microsoft-swin-tiny-patch4-window7-224-ov\"\nmodel = OVModelForImageClassification.from_pretrained(model_id, compile=False)\nimage_processor = AutoImageProcessor.from_pretrained(model_id)\n\nmodel.reshape(1, 3, image_processor.size[\"height\"], image_processor.size[\"width\"])\nmodel.compile()\nov_pipe = pipeline(\"image-classification\", model=model, feature_extractor=image_processor)\n\nimage_url_or_path = \"https://live.staticflickr.com/65535/51120373723_431ea5d1f5_w_d.jpg\"\ndisplay(Image(image_url_or_path))\nov_pipe(image_url_or_path, top_k=3)\n```\n\n----------------------------------------\n\nTITLE: Import libraries\nDESCRIPTION: This code imports necessary libraries for working with transformers and optimum, including AutoTokenizer for tokenization and OVModelForCausalLM and OVWeightQuantizationConfig for model loading and quantization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom transformers import AutoTokenizer\nfrom optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-free) - Text-to-Image - CLI\nDESCRIPTION: Exports a Stable Diffusion model to OpenVINO format with int8 weight-only quantization using the optimum-cli.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m dreamlike-art/dreamlike-anime-1.0 --weight-format int8 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: optimum-cli export openvino Command Help\nDESCRIPTION: This shows the help message and options for the `optimum-cli export openvino` command. It lists the required and optional arguments, including model selection, task specification, framework selection, quantization options, and dataset configuration. The help message provides details about data-aware quantization, AWQ, GPTQ, and other advanced compression techniques for optimizing models for OpenVINO.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nusage: optimum-cli export openvino [-h] -m MODEL [--task TASK] [--framework {pt,tf}] [--trust-remote-code]\n                                   [--weight-format {fp32,fp16,int8,int4,mxfp4,nf4}]\n                                   [--quant-mode {int8,f8e4m3,f8e5m2,nf4_f8e4m3,nf4_f8e5m2,int4_f8e4m3,int4_f8e5m2}]\n                                   [--library {transformers,diffusers,timm,sentence_transformers,open_clip}]\n                                   [--cache_dir CACHE_DIR] [--pad-token-id PAD_TOKEN_ID] [--ratio RATIO] [--sym]\n                                   [--group-size GROUP_SIZE] [--backup-precision {none,int8_sym,int8_asym}]\n                                   [--dataset DATASET] [--all-layers] [--awq] [--scale-estimation] [--gptq]\n                                   [--lora-correction] [--sensitivity-metric SENSITIVITY_METRIC]\n                                   [--num-samples NUM_SAMPLES] [--disable-stateful] [--disable-convert-tokenizer]\n                                   [--smooth-quant-alpha SMOOTH_QUANT_ALPHA]\n                                   output\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired arguments:\n  -m MODEL, --model MODEL\n                        Model ID on huggingface.co or path on disk to load model from.\n  output                Path indicating the directory where to store the generated OV model.\n\nOptional arguments:\n  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on\n                        the model. Available tasks depend on the model, but are among: ['image-to-image',\n                        'image-segmentation', 'inpainting', 'sentence-similarity', 'text-to-audio', 'image-to-text',\n                        'automatic-speech-recognition', 'token-classification', 'text-to-image', 'audio-classification',\n                        'feature-extraction', 'semantic-segmentation', 'masked-im', 'audio-xvector',\n                        'audio-frame-classification', 'text2text-generation', 'multiple-choice', 'depth-estimation',\n                        'image-classification', 'fill-mask', 'zero-shot-object-detection', 'object-detection',\n                        'question-answering', 'zero-shot-image-classification', 'mask-generation', 'text-generation',\n                        'text-classification']. For decoder models, use 'xxx-with-past' to export the model using past\n                        key values in the decoder.\n  --framework {pt,tf}   The framework to use for the export. If not provided, will attempt to use the local\n                        checkpoint's original framework or what is available in the environment.\n  --trust-remote-code   Allows to use custom code for the modeling hosted in the model repository. This option should\n                        only be set for repositories you trust and in which you have read the code, as it will execute\n                        on your local machine arbitrary code present in the model repository.\n  --weight-format {fp32,fp16,int8,int4,mxfp4,nf4}\n                        The weight format of the exported model.\n  --quant-mode {int8,f8e4m3,f8e5m2,nf4_f8e4m3,nf4_f8e5m2,int4_f8e4m3,int4_f8e5m2}\n                        Quantization precision mode. This is used for applying full model quantization including\n                        activations.\n  --library {transformers,diffusers,timm,sentence_transformers,open_clip}\n                        The library used to load the model before export. If not provided, will attempt to infer the\n                        local checkpoint's library\n  --cache_dir CACHE_DIR\n                        The path to a directory in which the downloaded model should be cached if the standard cache\n                        should not be used.\n  --pad-token-id PAD_TOKEN_ID\n                        This is needed by some models, for some tasks. If not provided, will attempt to use the\n                        tokenizer to guess it.\n  --ratio RATIO         A parameter used when applying 4-bit quantization to control the ratio between 4-bit and 8-bit\n                        quantization. If set to 0.8, 80% of the layers will be quantized to int4 while 20% will be\n                        quantized to int8. This helps to achieve better accuracy at the sacrifice of the model size\n                        and inference latency. Default value is 1.0. Note: If dataset is provided, and the ratio is\n                        less than 1.0, then data-aware mixed precision assignment will be applied.\n  --sym                 Whether to apply symmetric quantization. This argument is related to integer-typed\n                        --weight-format and --quant-mode options. In case of full or mixed quantization (--quant-mode)\n                        symmetric quantization will be applied to weights in any case, so only activation quantization\n                        will be affected by --sym argument. For weight-only quantization (--weight-format) --sym\n                        argument does not affect backup precision. Examples: (1) --weight-format int8 --sym => int8\n                        symmetric quantization of weights; (2) --weight-format int4 => int4 asymmetric quantization of\n                        weights; (3) --weight-format int4 --sym --backup-precision int8_asym => int4 symmetric\n                        quantization of weights with int8 asymmetric backup precision; (4) --quant-mode int8 --sym =>\n                        weights and activations are quantized to int8 symmetric data type; (5) --quant-mode int8 =>\n                        activations are quantized to int8 asymmetric data type, weights -- to int8 symmetric data type;\n                        (6) --quant-mode int4_f8e5m2 --sym => activations are quantized to f8e5m2 data type, weights --\n                        to int4 symmetric data type.\n  --group-size GROUP_SIZE\n                        The group size to use for quantization. Recommended value is 128 and -1 uses per-column\n                        quantization.\n  --backup-precision {none,int8_sym,int8_asym}\n                        Defines a backup precision for mixed-precision weight compression. Only valid for 4-bit weight\n                        formats. If not provided, backup precision is int8_asym. 'none' stands for original floating-\n                        point precision of the model weights, in this case weights are retained in their original\n                        precision without any quantization. 'int8_sym' stands for 8-bit integer symmetric quantization\n                        without zero point. 'int8_asym' stands for 8-bit integer asymmetric quantization with zero\n                        points per each quantization group.\n  --dataset DATASET     The dataset used for data-aware compression or quantization with NNCF. For language models you\n                        can use the one from the list ['auto','wikitext2','c4','c4-new']. With 'auto' the dataset will\n                        be collected from model's generations. For diffusion models it should be on of\n                        ['conceptual_captions','laion/220k-GPT4Vision-captions-from-LIVIS','laion/filtered-wit']. For\n                        visual language models the dataset must be set to 'contextual'. Note: if none of the data-aware\n                        compression algorithms are selected and ratio parameter is omitted or equals 1.0, the dataset\n                        argument will not have an effect on the resulting model.\n  --all-layers          Whether embeddings and last MatMul layers should be compressed to INT4. If not provided an\n                        weight compression is applied, they are compressed to INT8.\n  --awq                 Whether to apply AWQ algorithm. AWQ improves generation quality of INT4-compressed LLMs, but\n                        requires additional time for tuning weights on a calibration dataset. To run AWQ, please also\n                        provide a dataset argument. Note: it is possible that there will be no matching patterns in\n                        the model to apply AWQ, in such case it will be skipped.\n  --scale-estimation    Indicates whether to apply a scale estimation algorithm that minimizes the L2 error between\n                        the original and compressed layers. Providing a dataset is required to run scale estimation.\n                        Please note, that applying scale estimation takes additional memory and time.\n  --gptq                Indicates whether to apply GPTQ algorithm that optimizes compressed weights in a layer-wise\n                        fashion to minimize the difference between activations of a compressed and original layer.\n                        Please note, that applying GPTQ takes additional memory and time.\n```\n\n----------------------------------------\n\nTITLE: Calculating Model Size for PyTorch and OpenVINO in Python\nDESCRIPTION: This function `get_model_size` calculates the size of a PyTorch or OpenVINO model given the model's directory. It checks the `framework` parameter to determine whether the model is PyTorch or OpenVINO and calculates the total size based on the `model.safetensors` file for PyTorch and `openvino_model.xml` and `openvino_model.bin` files for OpenVINO.  It requires the `Path` module from the `pathlib` library.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_model_size(model_folder, framework):\n    \"\"\"\n    Return OpenVINO or PyTorch model size in Mb.\n    Arguments:\n        model_folder:\n            Directory containing a model.safetensors for a PyTorch model, and an openvino_model.xml/.bin for an OpenVINO model.\n        framework:\n            Define whether the model is a PyTorch or an OpenVINO model.\n    \"\"\"\n    if framework.lower() == \"openvino\":\n        model_path = Path(model_folder) / \"openvino_model.xml\"\n        model_size = model_path.stat().st_size + model_path.with_suffix(\".bin\").stat().st_size\n    elif framework.lower() == \"pytorch\":\n        model_path = Path(model_folder) / \"model.safetensors\"\n        model_size = model_path.stat().st_size\n    model_size /= 1000 * 1000\n    return model_size\n\n\nmodel.save_pretrained(fp32_model_path)\n\nfp32_model_size = get_model_size(fp32_model_path, \"pytorch\")\nint8_model_size = get_model_size(int8_ptq_model_path, \"openvino\")\nprint(f\"FP32 model size: {fp32_model_size:.2f} MB\")\nprint(f\"INT8 model size: {int8_model_size:.2f} MB\")\nprint(f\"INT8 size decrease: {fp32_model_size / int8_model_size:.2f}x\")\n```\n\n----------------------------------------\n\nTITLE: Exporting a model to OpenVINO IR format using CLI\nDESCRIPTION: This command uses the Optimum CLI to export a GPT-2 model to the OpenVINO Intermediate Representation (IR) format. The `--model` argument specifies the input model (`gpt2`), and `ov_model` is the directory where the exported model will be saved.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_7\n\nLANGUAGE: Plain Text\nCODE:\n```\noptimum-cli export openvino --model gpt2 ov_model\n```\n\n----------------------------------------\n\nTITLE: Disabling Model Caching on GPU with OpenVINO Config in Python\nDESCRIPTION: Demonstrates how to disable model caching on GPU when using OpenVINO through `optimum-intel`. This is achieved by setting the `CACHE_DIR` to an empty string in the `ov_config` dictionary. This dictionary is then passed to the `from_pretrained` function when loading the model. This avoids writing compiled models to disk.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nov_config = {\"CACHE_DIR\": \"\"}\nmodel = OVModelForSequenceClassification.from_pretrained(model_id, device=\"gpu\", ov_config=ov_config)\n```\n\n----------------------------------------\n\nTITLE: Streaming Response from Chain\nDESCRIPTION: This code demonstrates how to stream the response from the chain using the `stream` method. It iterates over the chunks of the response and prints each chunk to the console, flushing the output to ensure immediate display. This allows for real-time or near real-time display of the generated text.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/langchain_hf_pipelines.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor chunk in chain.stream(question):\n    print(chunk, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Quantize CodeBERT Model to INT4 with Hybrid Quantization and WikiText2 Dataset (Python)\nDESCRIPTION: This Python snippet quantizes microsoft/codebert-base to INT4 using a hybrid quantization method with the wikitext2 dataset and saves the resulting model to the designated save directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nOVModelForFeatureExtraction.from_pretrained('microsoft/codebert-base', quantization_config=OVWeightQuantizationConfig(bits=4, quant_method='hybrid', dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Exporting Stable Diffusion with hybrid quantization to OpenVINO\nDESCRIPTION: This command exports the Stable Diffusion 2-1 model with hybrid quantization to OpenVINO IR format. It uses the `conceptual_captions` dataset for calibration and applies 8-bit weight-only quantization for the non-UNet components of the pipeline. This approach quantizes weights in MatMul and Embedding layers, as well as activations of other layers.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_9\n\nLANGUAGE: Plain Text\nCODE:\n```\noptimum-cli export openvino --model stabilityai/stable-diffusion-2-1 --dataset conceptual_captions --weight-format int8 ov_model\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Optimum and Evaluation\nDESCRIPTION: Installs the necessary Python packages: `optimum` with the `openvino` extra for OpenVINO support, and `evaluate` for evaluating model performance.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install optimum[openvino]\n%pip install evaluate\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO Inference Precision in Python\nDESCRIPTION: Demonstrates how to configure the inference precision of an OpenVINO model using the `ov_config` parameter. It sets the `INFERENCE_PRECISION_HINT` to `f32` for full precision inference. Requires `optimum-intel`. The `ov_config` dictionary is passed to the `from_pretrained` method.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nov_config = {\"INFERENCE_PRECISION_HINT\": \"f32\"}\nmodel = OVModelForSequenceClassification.from_pretrained(model_id, ov_config=ov_config)\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel with IPEX\nDESCRIPTION: This command installs Optimum Intel with the 'ipex' extra to enable optimizations using Intel Extension for PyTorch (IPEX). The `--upgrade` and `--upgrade-strategy eager` flags ensure that `optimum-intel` and its dependencies are upgraded to the latest versions.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npip install --upgrade --upgrade-strategy eager \"optimum[ipex]\"\n```\n\n----------------------------------------\n\nTITLE: Image Refinement with Stable Diffusion XL and OpenVINO\nDESCRIPTION: This code demonstrates how to refine an image output from a base SDXL model using a refiner model like `stabilityai/stable-diffusion-xl-refiner-1.0`. The base model's output is set to 'latent', and this latent representation is then passed to the refiner model for enhanced image generation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionXLImg2ImgPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\nrefiner = OVStableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, export=True)\n\nimage = base(prompt=prompt, output_type=\"latent\").images[0]\nimage = refiner(prompt=prompt, image=image[None, :]).images[0]\n```\n\n----------------------------------------\n\nTITLE: Creating and Invoking Chain with Prompt\nDESCRIPTION: This code defines a prompt template using `PromptTemplate` from `langchain_core.prompts`.  It then creates a chain by combining the prompt and the HuggingFacePipeline model (`hf`). Finally, it invokes the chain with a question and prints the generated answer.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/langchain_hf_pipelines.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate.from_template(template)\n\nchain = prompt | hf\n\nquestion = \"What is electroencephalography?\"\n\nprint(chain.invoke({\"question\": question}))\n\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel from source with extras\nDESCRIPTION: This command installs Optimum Intel from the GitHub repository along with specified extra dependencies such as 'neural-compressor', 'openvino', or 'ipex'. This allows installing from source with all the required dependencies.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/installation.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \"optimum-intel[extras]\"@git+https://github.com/huggingface/optimum-intel.git\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel and other dependencies\nDESCRIPTION: This code cell installs the necessary Python packages including `optimum-intel` with `openvino` and `nncf` support, `datasets`, `evaluate`, and `ipywidgets`. These packages are used for model quantization, dataset loading, and evaluation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# %pip install \"optimum-intel[openvino, nncf]\" datasets evaluate[evaluator] ipywidgets\n```\n\n----------------------------------------\n\nTITLE: Run Inference Manually\nDESCRIPTION: This snippet showcases how to perform inference manually by defining the pre-processing and post-processing steps.  It loads an OpenVINO model and a tokenizer, prepares the input using the tokenizer, performs inference, and then extracts the answer from the model's output. This approach provides more control over the inference process compared to using a pipeline.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom optimum.intel import OVModelForQuestionAnswering\nfrom transformers import AutoTokenizer, pipeline\n\nmodel = OVModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad-ov-fp32\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad-ov-fp32\")\n\nquestion, text = \"What is OpenVINO?\", \"OpenVINO is a framework for deep learning inference optimization\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\noutputs = model(**inputs)\n\nanswer_start_index = torch.argmax(outputs.start_logits, axis=-1).item()\nanswer_end_index = torch.argmax(outputs.end_logits, axis=-1).item()\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\n----------------------------------------\n\nTITLE: Inference with Reshaped OpenVINO Model in Python\nDESCRIPTION: Illustrates performing inference with a reshaped OpenVINO model. Includes loading a tokenizer, reshaping the model, compiling it, and running inference. Requires `transformers` and `optimum-intel`. The model is reshaped before compilation to fix input shapes.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\nfrom optimum.intel import OVModelForQuestionAnswering\n\nmodel_id = \"distilbert/distilbert-base-cased-distilled-squad\"\nmodel = OVModelForQuestionAnswering.from_pretrained(model_id, compile=False)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nbatch_size, seq_len = 1, 40\nmodel.reshape(batch_size, seq_len)\n# Compile the model before the first inference\nmodel.compile()\n\nquestion = \"Which name is also used to describe the Amazon rainforest ?\"\ncontext = \"The Amazon rainforest, also known as Amazonia or the Amazon Jungle\"\ntokens = tokenizer(question, context, max_length=seq_len, padding=\"max_length\", return_tensors=\"np\")\n\noutputs = model(**tokens)\n```\n\n----------------------------------------\n\nTITLE: Inference Example with Wav2Vec2 in Python\nDESCRIPTION: This code snippet demonstrates how to perform speech-to-text transcription using the Wav2Vec2 model from the Hugging Face Transformers library. It involves loading a dataset, preprocessing audio data, using a pre-trained model for inference, and decoding the predicted output. Dependencies include transformers, datasets, and torch.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n\n>>> # transcribe speech\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription[0]\n'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n```\n\n----------------------------------------\n\nTITLE: Disabling Torch Distribution Initialization in Python\nDESCRIPTION: This python snippet disables the initialization of torch distribution. It sets the environment variable `OMPI_COMM_WORLD_SIZE` to '-1' if it's not already set. This is a prerequisite for distributed tuning.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nos.environ.setdefault('OMPI_COMM_WORLD_SIZE', '-1') if os.environ.get('OMPI_COMM_WORLD_SIZE', -1) != -1 else None\n```\n\n----------------------------------------\n\nTITLE: Static Quantization with Neural Compressor in Bash\nDESCRIPTION: This command applies post-training static quantization to a MiniLM model fine-tuned on the CLINC150 dataset. It uses the `run_clinc.py` script with specified parameters for model path, dataset, quantization approach, number of calibration samples, evaluation, loading verification, and output directory. The script utilizes the Intel Neural Compressor library for quantization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/text-classification/intent-classification/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_clinc.py \\\n    --model_name_or_path SetFit/MiniLM_L3_clinc_oos_plus_distilled \\\n    --dataset_name clinc_oos \\\n    --apply_quantization \\\n    --quantization_approach static \\\n    --num_calibration_samples 50 \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/clinc_output\n```\n\n----------------------------------------\n\nTITLE: Sentence Embedding Pipeline Definition\nDESCRIPTION: Defines a custom `SentenceEmbeddingPipeline` class that extends `transformers.Pipeline`. It includes preprocessing with padding and truncation, embedding generation using the model, mean pooling, and normalization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import Pipeline\nimport torch.nn.functional as F\nimport torch\n\n\n# copied from the model card \"sentence-transformers/all-MiniLM-L6-v2\"\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\nclass SentenceEmbeddingPipeline(Pipeline):\n    def _sanitize_parameters(self, **kwargs):\n        # we don\"t have any hyperameters to sanitize\n        preprocess_kwargs = {}\n        return preprocess_kwargs, {}, {}\n\n    def preprocess(self, inputs):\n        encoded_inputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n        return encoded_inputs\n\n    def _forward(self, model_inputs):\n        outputs = self.model(**model_inputs)\n        return {\"outputs\": outputs, \"attention_mask\": model_inputs[\"attention_mask\"]}\n\n    def postprocess(self, model_outputs):\n        # Perform pooling\n        sentence_embeddings = mean_pooling(model_outputs[\"outputs\"], model_outputs[\"attention_mask\"])\n        # Normalize embeddings\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n        return sentence_embeddings\n```\n\n----------------------------------------\n\nTITLE: Example hostfile for Multi-Node Distributed Training\nDESCRIPTION: This shows an example hostfile, which is required for multi-node distributed training with `mpirun`. It contains the IP addresses of each node participating in the training process, allowing `mpirun` to distribute the workload.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/distributed_training.mdx#_snippet_4\n\nLANGUAGE: shell script\nCODE:\n```\n cat hostfile\n xxx.xxx.xxx.xxx #node0 ip\n xxx.xxx.xxx.xxx #node1 ip\n```\n\n----------------------------------------\n\nTITLE: Tuning Quantization Parameters\nDESCRIPTION: This code illustrates how to tune quantization parameters for 4-bit weight quantization using `OVWeightQuantizationConfig`.  It sets parameters like `sym` (symmetric quantization), `ratio` (ratio between 4-bit and 8-bit quantization), `quant_method` (quantization algorithm), and `dataset` (dataset used for tuning).\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nquantization_config = OVWeightQuantizationConfig(\n    bits=4,\n    sym=False,\n    ratio=0.8,\n    quant_method=\"awq\",\n    dataset=\"wikitext2\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up environment variables for oneCCL (>= 1.12.0)\nDESCRIPTION: This snippet retrieves the path to `oneccl_bindings_for_pytorch` and sources the `setvars.sh` script to set up the necessary environment variables for oneCCL. It uses python to find the current working directory of the `oneccl_bindings_for_pytorch` package and executes the `setvars.sh` script.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/distributed_training.mdx#_snippet_1\n\nLANGUAGE: shell script\nCODE:\n```\noneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bindings_for_pytorch import cwd; print(cwd)\")\nsource $oneccl_bindings_for_pytorch_path/env/setvars.sh\n```\n\n----------------------------------------\n\nTITLE: Chatbot Core Function Imports\nDESCRIPTION: This imports necessary modules for implementing a chatbot, including time, threading, and classes from the transformers library like TextIteratorStreamer, StoppingCriteria, StoppingCriteriaList and GenerationConfig. These are used for handling streaming output, defining stopping criteria and configuring text generation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom threading import Thread\n\nfrom transformers import (\n    TextIteratorStreamer,\n    StoppingCriteria,\n    StoppingCriteriaList,\n    GenerationConfig,\n)\n```\n\n----------------------------------------\n\nTITLE: SmoothQuant Recipes (Diff)\nDESCRIPTION: Specifies SmoothQuant as a quantization recipe using a diff.  It shows the change to quantization configuration, adding SmoothQuant with specified alpha and folding.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n- quantization_config = PostTrainingQuantConfig(approach=\"static\")\n+ recipes={\"smooth_quant\": True,  \"smooth_quant_args\": {\"alpha\": 0.5, \"folding\": True}}\n+ quantization_config = PostTrainingQuantConfig(approach=\"static\", backend=\"ipex\", recipes=recipes)\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel with Neural Compressor\nDESCRIPTION: This command installs Optimum Intel with the 'neural-compressor' extra, ensuring all necessary dependencies for using Neural Compressor are included. The `--upgrade` and `--upgrade-strategy eager` flags ensure that `optimum-intel` and its dependencies are upgraded to the latest versions.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip install --upgrade --upgrade-strategy eager \"optimum[neural-compressor]\"\n```\n\n----------------------------------------\n\nTITLE: Install dependencies\nDESCRIPTION: This code installs necessary dependencies including optimum with openvino and nncf support, as well as a specific version of torch.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# ! pip install optimum[openvino,nncf] torch==2.2.2\n```\n\n----------------------------------------\n\nTITLE: Creating and Invoking Chain without Prompt\nDESCRIPTION: This snippet creates a chain that skips the prompt by binding `skip_prompt=True` to the HuggingFacePipeline model (`hf`). It then invokes the chain with a question and prints the generated answer. This provides a way to get the model's direct output without pre-defined instructions.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/langchain_hf_pipelines.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nchain = prompt | hf.bind(skip_prompt=True)\n\nquestion = \"What is electroencephalography?\"\n\nprint(chain.invoke({\"question\": question}))\n```\n\n----------------------------------------\n\nTITLE: Inference with Transformers Pipeline using Quantized Model\nDESCRIPTION: This code demonstrates how to use a quantized model loaded with `INCModelForSequenceClassification` in a Hugging Face Transformers pipeline. It initializes a tokenizer and a text classification pipeline, then uses the pipeline to perform inference on a sample text.  The pipeline seamlessly integrates the quantized model for efficient inference.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipe_cls = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\ntext = \"He's a dreadful magician.\"\noutputs = pipe_cls(text)\n\n[{'label': 'NEGATIVE', 'score': 0.9880216121673584}]\n```\n\n----------------------------------------\n\nTITLE: Distributed Accuracy-Aware Tuning (Diff)\nDESCRIPTION: Configures distributed accuracy-aware tuning by setting `quant_level` to `1` in the quantization configuration, as shown by the diff.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n- quantization_config = PostTrainingQuantConfig(approach=\"static\")\n+ quantization_config = PostTrainingQuantConfig(approach=\"static\", quant_level=1)\n```\n\n----------------------------------------\n\nTITLE: Distributed Accuracy-Aware Tuning (Shell)\nDESCRIPTION: Runs the distributed tuning process using `mpirun` with a specified number of processes and the command to execute.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/optimization.mdx#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmpirun -np <number_of_processes> <RUN_CMD>\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Optimum Intel\nDESCRIPTION: This snippet demonstrates text generation using a pre-trained GPT-2 model and the Optimum Intel library. It involves loading a model, tokenizer, creating a pipeline, and generating text from a prompt. The code requires `optimum.intel` and `transformers` libraries. The `model_id` variable specifies the model to be used.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n\nmodel_id = \"helenai/gpt2-ov\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = OVModelForCausalLM.from_pretrained(model_id)\nov_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nov_pipe(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=3)\n```\n\n----------------------------------------\n\nTITLE: Loading Evaluation Dataset and Metric\nDESCRIPTION: Loads the GLUE STSB validation dataset and the GLUE STSB evaluation metric using `datasets.load_dataset` and `evaluate.load`, respectively.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/sentence_transformer_quantization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom evaluate import load\n\neval_dataset = load_dataset(\"glue\", \"stsb\", split=\"validation\")\nmetric = load(\"glue\", \"stsb\")\n```\n\n----------------------------------------\n\nTITLE: Upgrading pip using python\nDESCRIPTION: This command upgrades the pip package manager to the latest version using the python module. It is recommended to upgrade pip before installing other packages.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/installation.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Reshape Model for Static Shapes\nDESCRIPTION: This snippet demonstrates how to reshape a model to use static shapes for potentially faster inference. It loads an OpenVINO model and a tokenizer, reshapes the model to a specific batch size and sequence length, and then creates a question-answering pipeline.  Static shapes require all inputs to have the same dimensions, necessitating padding or truncation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom optimum.intel import OVModelForQuestionAnswering\nfrom transformers import AutoTokenizer, pipeline\n\nmodel = OVModelForQuestionAnswering.from_pretrained(\n    \"helenai/distilbert-base-uncased-distilled-squad-ov-fp32\", compile=False\n)\ntokenizer = AutoTokenizer.from_pretrained(\"helenai/distilbert-base-uncased-distilled-squad-ov-fp32\")\n\nmax_length = 128\nmodel.reshape(batch_size=1, sequence_length=max_length)\nmodel.compile()\n\nov_pipe = pipeline(\n    \"question-answering\",\n    model=model,\n    tokenizer=tokenizer,\n    max_seq_len=max_length,\n    padding=\"max_length\",\n    truncation=True,\n)\nov_pipe(\"What is OpenVINO?\", \"OpenVINO is a toolkit for deep learning inference optimization\")\n```\n\n----------------------------------------\n\nTITLE: Loading a quantized model with INCModelForSequenceClassification\nDESCRIPTION: This Python code snippet demonstrates how to load a quantized model using `INCModelForSequenceClassification` from the `optimum.intel` library. It specifies the model ID hosted on the Hugging Face Hub and loads it for sequence classification tasks.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom optimum.intel import INCModelForSequenceClassification\n\nmodel_id = \"Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic\"\nmodel = INCModelForSequenceClassification.from_pretrained(model_id)\n```\n\n----------------------------------------\n\nTITLE: Exporting with INT4 Weights - Optimum CLI\nDESCRIPTION: Exports a model with INT4 weight compression using the optimum-cli. This further reduces the model size, potentially at the cost of some accuracy. The output model is saved in the ov_model/ directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Meta-Llama-3-8B --weight-format int4 ov_model/\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting IAM Dataset (Bash)\nDESCRIPTION: This command downloads the IAM dataset archive and extracts its contents. This dataset is used for optical character recognition tasks. The dataset will be extracted to the current directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/optical-character-recognition/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://layoutlm.blob.core.windows.net/trocr/dataset/IAM.tar.gz\ntar xvf IAM.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Printing Langchain HuggingFace Version\nDESCRIPTION: This code snippet imports the `_langchain_hf_version` variable from `optimum.intel.utils.import_utils` and prints the version of langchain-huggingface. It helps verify that the installed langchain-huggingface version is compatible with the notebook's requirements.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/langchain_hf_pipelines.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel.utils.import_utils import _langchain_hf_version\n\nprint(\"langchain-huggingface version is\", _langchain_hf_version)\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: This command installs the necessary training dependencies for running the Textual Inversion scripts, as specified in the `requirements.txt` file.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/textual-inversion/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Loading and Optimizing a Model with IPEX in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-trained language model (GPT-2) using `IPEXModelForCausalLM` from the `optimum.intel` library, which applies IPEX optimizations. It replaces the standard `AutoModelForCausalLM` with the IPEX-optimized version. The code also shows how to create a text generation pipeline using the loaded model and tokenizer.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/ipex/inference.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom transformers import AutoTokenizer, pipeline\nfrom optimum.intel import IPEXModelForCausalLM\n\nmodel_id = \"gpt2\"\nmodel = IPEXModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nresults = pipe(\"He's a dreadful magician and\")\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel with OpenVINO\nDESCRIPTION: This command installs Optimum Intel with the 'openvino' extra, including the dependencies required to utilize OpenVINO. The `--upgrade` and `--upgrade-strategy eager` flags ensure that `optimum-intel` and its dependencies are upgraded to the latest versions.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npip install --upgrade --upgrade-strategy eager \"optimum[openvino]\"\n```\n\n----------------------------------------\n\nTITLE: Close Existing Gradio Demo\nDESCRIPTION: This snippet attempts to close any existing Gradio demo instances to avoid conflicts when running the script multiple times. The try-except block handles cases where no demo instance is currently running.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    demo.close()\nexcept:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for Examples - Shell\nDESCRIPTION: This shell command navigates to the example folder and installs the necessary Python packages listed in the `requirements.txt` file using pip. It ensures that all dependencies required by the example are installed, allowing the example to run correctly.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\ncd <example-folder>\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Close Gradio Demo (commented)\nDESCRIPTION: This commented-out snippet shows how to close the Gradio demo programmatically. This is useful for terminating the server and freeing up resources after the demo is no longer needed.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# demo.close()\n```\n\n----------------------------------------\n\nTITLE: Generate with PLD\nDESCRIPTION: This code demonstrates Prompt Lookahead Decoding (PLD) by using the stateless Phi-2 model to generate a code completion for a given function signature and docstring.  The `prompt_lookup_num_tokens` parameter is passed to the `generate` method to enable PLD. TextStreamer is used for printing the tokens during generation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import TextStreamer\n\n\n# Tokenize the sample\ninputs = tokenizer([sample], return_tensors='pt')    \n\nout = stateless_model.generate(\n    **inputs,\n    max_new_tokens=128,\n    streamer=TextStreamer(tokenizer=tokenizer, skip_special_tokens=True),\n    pad_token_id=tokenizer.eos_token_id,\n    prompt_lookup_num_tokens=3,\n)    \n```\n\n----------------------------------------\n\nTITLE: Installing Diffusers Package in Bash\nDESCRIPTION: This code snippet shows the command to install the `diffusers` package along with the necessary dependencies for `optimum`. It utilizes the `pip` package manager and specifies the `optimum[diffusers]` extra to install `optimum` with diffusers-related features. This installation enables working with diffusion models within the `optimum` framework.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install optimum[diffusers]\n```\n\n----------------------------------------\n\nTITLE: Configure Draft Model Generation\nDESCRIPTION: This snippet configures the draft model to predict 3 tokens at each forward step, using a constant schedule for the number of assistant tokens. This configuration is found to work well in the current setup.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nasst_model.generation_config.num_assistant_tokens = 3\nasst_model.generation_config.num_assistant_tokens_schedule = \"const\"\n```\n\n----------------------------------------\n\nTITLE: Filtering and Mapping the Dataset\nDESCRIPTION: This code snippet filters the validation dataset to only include examples related to \"Super_Bowl_50\", then selects a subset of these examples for both training and validation.  It then applies the `preprocess_fn` to tokenize both the training and validation datasets, preparing them for model calibration and evaluation.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nNUM_TRAIN_ITEMS = 600\nfiltered_examples = dataset[\"validation\"].filter(lambda x: x[\"title\"].startswith(\"Super_Bowl_50\"))\ntrain_examples = filtered_examples.select(range(0, NUM_TRAIN_ITEMS))\ntrain_dataset = train_examples.map(lambda x: preprocess_fn(x, tokenizer), batched=True)\n\nvalidation_examples = filtered_examples.select(range(NUM_TRAIN_ITEMS, len(filtered_examples)))\nvalidation_dataset = validation_examples.map(lambda x: preprocess_fn(x, tokenizer), batched=True)\n```\n\n----------------------------------------\n\nTITLE: Inspect Pipeline Pre/Post Processing\nDESCRIPTION: These lines appear to be commented out placeholders intended for inspecting the preprocess and postprocess methods of a pipeline object. This is not a runnable snippet, but an indication of debugging or further exploration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# ov_pipe.preprocess??\n# ov_pipe.postprocess??\n```\n\n----------------------------------------\n\nTITLE: Reshaping Diffusers Model with Image Dimensions in Python\nDESCRIPTION: Demonstrates reshaping a Diffusers pipeline for image generation with specific height and width. This allows for controlling the output image size. Requires `optimum-intel` and a `OVStableDiffusionPipeline` instance. The `reshape` method sets the image dimensions before inference.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/inference.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbatch_size, num_images, height, width = 1, 1, 512, 512\npipeline.reshape(batch_size=batch_size, height=height, width=width, num_images_per_prompt=num_images)\nimages = pipeline(prompt, height=height, width=width, num_images_per_prompt=num_images).images\n```\n\n----------------------------------------\n\nTITLE: Accuracy Comparison\nDESCRIPTION: This code evaluates both the quantized and original models using the SQuAD evaluation metric. It calculates the exact match and F1 scores for both models and presents the results in a Pandas DataFrame for comparison.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsquad_eval = evaluator(\"question-answering\")\n\nov_eval_results = squad_eval.compute(\n    model_or_pipeline=ov_qa_pipeline_ptq,\n    data=validation_examples,\n    metric=\"squad\",\n    squad_v2_format=VERSION_2_WITH_NEGATIVE,\n)\n\nhf_eval_results = squad_eval.compute(\n    model_or_pipeline=hf_qa_pipeline,\n    data=validation_examples,\n    metric=\"squad\",\n    squad_v2_format=VERSION_2_WITH_NEGATIVE,\n)\npd.DataFrame.from_records(\n    [hf_eval_results, ov_eval_results],\n    columns=[\"exact_match\", \"f1\"],\n    index=[\"FP32\", \"INT8 PTQ\"],\n).round(2)\n```\n\n----------------------------------------\n\nTITLE: Add User Text to Chat History\nDESCRIPTION: This function takes the user's message and appends it to the chat history. It initializes a blank assistant response in the history, which will be populated by the model's generated text. The function returns the updated history and clears the user input field.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef add_user_text(message, history):\n    \"\"\"\n    Add user's message to chatbot history\n\n    Params:\n      message: current user message\n      history: conversation history\n    Returns:\n      Updated history, clears user message and status\n    \"\"\"\n    # Append current user message to history with a blank assistant message which will be generated by the model\n    history.append([message, None])\n    return ('', history)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning GPT-Neo with Pruning and Quantization Aware Training (CLM)\nDESCRIPTION: This bash script fine-tunes a GPT-Neo model on the WikiText-2 dataset. It applies snip_momentum pruning with a target sparsity of 0.02 and then uses quantization aware training. The script trains for 4 epochs with a maximum of 100 training samples. It utilizes causal language modeling (CLM) and saves the output to /tmp/clm_output.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/language-modeling/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython run_clm.py \\\n    --model_name_or_path EleutherAI/gpt-neo-125M \\\n    --dataset_name wikitext \\\n    --dataset_config_name wikitext-2-raw-v1 \\\n    --apply_quantization \\\n    --quantization_approach aware_training \\\n    --apply_pruning \\\n    --target_sparsity 0.02 \\\n    --num_train_epochs 4 \\\n    --max_train_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/clm_output\n```\n\n----------------------------------------\n\nTITLE: Configure model\nDESCRIPTION: This code defines configuration parameters for loading and quantizing the Phi-2 model, including the model name, save name, precision, quantization configuration (bits, sym, group_size, ratio), and device. It also sets the stateful flag for KV cache optimization.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"microsoft/phi-2\"\nsave_name = model_name.split(\"/\")[-1] + \"_openvino\"\nprecision = \"f32\"\nquantization_config = OVWeightQuantizationConfig(\n    bits=4,\n    sym=False,\n    group_size=128,\n    ratio=0.8,\n)\ndevice = \"gpu\"\n```\n\n----------------------------------------\n\nTITLE: Define Chatbot Examples\nDESCRIPTION: This snippet defines a list of example questions or prompts that users can select to initiate a conversation with the chatbot. These examples provide initial guidance and demonstrate the chatbot's capabilities.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nEXAMPLES = [\n    [\"What is OpenVINO?\"]\n    ,[\"Can you explain to me briefly what is Python programming language?\"]\n    ,[\"Explain the plot of Cinderella in a sentence.\"]\n    ,[\"Write a Python function to perform binary search over a sorted list. Use markdown to write code\"]\n    ,[\"Lily has a rubber ball that she drops from the top of a wall. The wall is 2 meters tall. How long will it take for the ball to reach the ground?\"]\n]\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Model Inference Latency in Python\nDESCRIPTION: This code benchmarks the inference latency of a question answering pipeline using a given dataset. It iterates through a specified number of items in the dataset, measures the time taken for each inference, and calculates the median latency. The `time.perf_counter()` function is used for precise time measurements. It requires `numpy` for calculating the median and assumes `qa_pipeline` is a callable that takes a dictionary with 'question' and 'context' keys.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(qa_pipeline, dataset, num_items=100):\n    \"\"\"\n    Benchmark PyTorch or OpenVINO model. This function does inference on `num_items`\n    dataset items and returns the median latency in milliseconds\n    \"\"\"\n    latencies = []\n    for i, item in enumerate(dataset.select(range(num_items))):\n        start_time = time.perf_counter()\n        results = qa_pipeline({\"question\": item[\"question\"], \"context\": item[\"context\"]})\n        end_time = time.perf_counter()\n        latencies.append(end_time - start_time)\n\n    return np.median(latencies) * 1000\n\n\noriginal_latency = benchmark(hf_qa_pipeline, validation_dataset)\nquantized_latency = benchmark(ov_qa_pipeline_ptq, validation_dataset)\ncpu_device_name = Core().get_property(\"CPU\", \"FULL_DEVICE_NAME\")\n\nprint(cpu_device_name)\nprint(f\"Latency of original FP32 model: {original_latency:.2f} ms\")\nprint(f\"Latency of quantized model: {quantized_latency:.2f} ms\")\nprint(f\"Speedup: {(original_latency/quantized_latency):.2f}x\")\n```\n\n----------------------------------------\n\nTITLE: Save Tokenizer\nDESCRIPTION: This snippet shows how to load a tokenizer from the Hugging Face Hub using `AutoTokenizer` and save it to a local directory. Saving the tokenizer alongside the model facilitates easy loading and use in subsequent inference tasks.  The tokenizer is essential for preprocessing text data before feeding it to the model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\ntokenizer.save_pretrained(\"distilbert-base-uncased-distilled-squad-ov-fp32\")\n```\n\n----------------------------------------\n\nTITLE: Checking for Partial Stop Strings in Output\nDESCRIPTION: This function `is_partial_stop` checks if the output string contains a partial match with any of the specified stop strings. It is used to prevent premature stopping of the generation process in cases where a stop string is partially generated.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef is_partial_stop(output, stop_str):\n    \"\"\"Check whether the output contains a partial stop str.\"\"\"\n    for i in range(0, min(len(output), len(stop_str))):\n        if stop_str.startswith(output[-i:]):\n            return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Inference with INT8 Model\nDESCRIPTION: This command performs inference using a trained INT8 Textual Inversion model, generating images based on a provided caption. It takes the pretrained model path and caption as input, along with the desired number of images to generate. The prompt should include the `placeholder_token`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/textual-inversion/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython text2images.py \\\n  --pretrained_model_name_or_path=$INT8_MODEL_NAME \\\n  --caption \"a lovely <dicoo> in red dress and hat, in the snowly and brightly night, with many brighly buildings.\" \\\n  --images_num 4\n```\n\n----------------------------------------\n\nTITLE: On-the-fly Conversion to OpenVINO\nDESCRIPTION: Demonstrates how to load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly during model loading. It then saves the converted model and tokenizer to a specified directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/export.mdx#_snippet_9\n\nLANGUAGE: diff\nCODE:\n```\n- from transformers import AutoModelForCausalLM\n+ from optimum.intel import OVModelForCausalLM\n  from transformers import AutoTokenizer\n\n  model_id = \"meta-llama/Meta-Llama-3-8B\"\n- model = AutoModelForCausalLM.from_pretrained(model_id)\n+ model = OVModelForCausalLM.from_pretrained(model_id, export=True)\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n  save_directory = \"ov_model\"\n  model.save_pretrained(save_directory)\n  tokenizer.save_pretrained(save_directory)\n```\n\n----------------------------------------\n\nTITLE: Quantizing Speech-to-Text Whisper Model\nDESCRIPTION: This example demonstrates how to quantize the speech-to-text Whisper model.  It uses `OVModelForSpeechSeq2Seq.from_pretrained` with a `OVQuantizationConfig` specifying the quantization parameters such as the number of samples, dataset, processor, and `matmul_sq_alpha`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nmodel_id = \"openai/whisper-tiny\"\nov_model = OVModelForSpeechSeq2Seq.from_pretrained(\n    model_id,\n    quantization_config=OVQuantizationConfig(\n        num_samples=10,\n        dataset=\"librispeech\",\n        processor=model_id,\n        matmul_sq_alpha=0.95,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Code Completion Sample\nDESCRIPTION: This code defines a sample function signature and docstring taken from the HumanEval dataset, intended to be used as input for code completion by the Phi-2 model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsample = \"\"\"from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    \"\"\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Prepare History for Regeneration\nDESCRIPTION: This function prepares the chat history for regeneration by removing the last assistant message. This allows the user to regenerate the response to their previous query. It updates the history by setting the last assistant message to None.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_for_regenerate(history):\n    \"\"\"\n    Delete last assistant message to prepare for regeneration\n\n    Params:\n      history: conversation history\n    Returns:\n      updated history\n    \"\"\" \n    history[-1][1] = None\n    return history\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel from source with extras\nDESCRIPTION: This command installs Optimum Intel from source along with specified optional dependencies ('extras') like ipex, neural-compressor, openvino, or nncf.  This allows for end-to-end installation, including required dependencies for each Intel acceleration toolkit.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\npython -m pip install \"optimum-intel[extras]\"@git+https://github.com/huggingface/optimum-intel.git\n```\n\n----------------------------------------\n\nTITLE: Setting Chat Template for Tokenizer\nDESCRIPTION: This code sets the chat template for the tokenizer. It uses a simple template of \"User: content\\nAssistant: content\\n...\" to format the input for the model. The tokenizer requires a chat template to understand the structure of turns in the conversations.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntokenizer.chat_template = \"{% for message in messages %}{{message['role'] + ': ' + message['content'] + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\"\n```\n\n----------------------------------------\n\nTITLE: Quantizing a model with Neural Compressor CLI\nDESCRIPTION: This command utilizes the Optimum command-line interface (CLI) to quantize a DistilBERT model using Intel Neural Compressor. It specifies the input model (`distilbert-base-cased-distilled-squad`) and the output directory (`./quantized_distilbert`) for the quantized model.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\noptimum-cli inc quantize --model distilbert-base-cased-distilled-squad --output ./quantized_distilbert\n```\n\n----------------------------------------\n\nTITLE: Weight-only Quantization (Data-free) - Image-Text-to-Text - CLI\nDESCRIPTION: Exports an InternVL2 model to OpenVINO format with int4 weight-only quantization using the optimum-cli.  The `--task image-text-to-text` is specified to indicate the task. The `--trust-remote-code` flag is also included.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --task image-text-to-text -m OpenGVLab/InternVL2-1B --trust-remote-code --weight-format int4 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain and Optimum IPEX\nDESCRIPTION: This command installs the langchain-huggingface and optimum[ipex] packages using pip. It's intended to be run in a Colab environment to ensure the required dependencies are available. This is a prerequisite for using the Hugging Face pipelines with Optimum Intel.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/langchain_hf_pipelines.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#! pip install langchain-huggingface optimum[ipex]\n```\n\n----------------------------------------\n\nTITLE: Export CodeBERT Model to OpenVINO with INT4 Quantization and WikiText2 Dataset (CLI)\nDESCRIPTION: This command exports the microsoft/codebert-base model to OpenVINO using the optimum-cli with INT4 quantization, utilizing the wikitext2 dataset for calibration.  The resulting model is saved to the ./save_dir directory.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino -m microsoft/codebert-base --weight-format int4 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Export SentenceTransformer to OpenVINO with NF4/F8E4M3 Quantization and WikiText2 (CLI)\nDESCRIPTION: This command exports the sentence-transformers/all-mpnet-base-v2 model using optimum-cli with mixed precision quantization (nf4_f8e4m3) using wikitext2 dataset for calibration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --library sentence_transformers -m sentence-transformers/all-mpnet-base-v2 --quant-mode nf4_f8e4m3 --dataset wikitext2 ./save_dir\n```\n\n----------------------------------------\n\nTITLE: Listing task-specific parameters for seq2seq model\nDESCRIPTION: This simple code snippet retrieves and displays the available task-specific parameters associated with the loaded sequence-to-sequence model. This provides insights into the configuration options supported by the model for different tasks.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Show the pipeline tasks for this model\nlist(model.config.task_specific_params.keys())\n```\n\n----------------------------------------\n\nTITLE: Launch Gradio Demo\nDESCRIPTION: This snippet launches the Gradio demo, making it accessible to users through a web browser. This command starts the Gradio server and displays the interface, allowing users to interact with the chatbot.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndemo.launch()\n```\n\n----------------------------------------\n\nTITLE: Sequence-to-Sequence Translation with Pipeline\nDESCRIPTION: This snippet shows how to perform translation using a pipeline with the `t5-small` model. The code translates the sentence \"What is a sequence to sequence model?\" from English to French using the pre-configured `translation_en_to_fr` pipeline.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nov_pipe = pipeline(\"translation_en_to_fr\", model=model, tokenizer=tokenizer)\nov_pipe(\"What is a sequence to sequence model?\")\n```\n\n----------------------------------------\n\nTITLE: Quantize SentenceTransformer to INT4 with OpenVINO Weight Quantization and WikiText2 (Python)\nDESCRIPTION: This code quantizes sentence-transformers/all-mpnet-base-v2 to INT4 using OVWeightQuantizationConfig with the wikitext2 dataset for calibration.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/optimization.mdx#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nOVSentenceTransformer.from_pretrained('sentence-transformers/all-mpnet-base-v2', quantization_config=OVWeightQuantizationConfig(bits=4, dataset='wikitext2')).save_pretrained('save_dir')\n```\n\n----------------------------------------\n\nTITLE: Multi-Node Distributed Training with oneCCL\nDESCRIPTION: This command launches a distributed training job across multiple nodes using `mpirun` and a hostfile. It sets environment variables, including the master address, and then executes the PyTorch training script (`run_qa.py`) with relevant arguments. The `hostfile` specifies the IP addresses of the nodes involved in the training.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/distributed_training.mdx#_snippet_5\n\nLANGUAGE: shell script\nCODE:\n```\nexport CCL_WORKER_COUNT=1\nexport MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip\nmpirun -f hostfile -n 4 -ppn 2 \\\n-genv OMP_NUM_THREADS=23 \\\npython3 run_qa.py \\\n    --model_name_or_path distilbert-base-uncased-distilled-squad \\\n    --dataset_name squad \\\n    --apply_quantization \\\n    --quantization_approach static \\\n    --do_train \\\n    --do_eval \\\n    --verify_loading \\\n    --output_dir /tmp/squad_output \\\n    --no_cuda \\\n    --xpu_backend ccl\n```\n\n----------------------------------------\n\nTITLE: Setting up environment variables for oneCCL (< 1.12.0)\nDESCRIPTION: This snippet retrieves the path to `torch_ccl` and sources the `setvars.sh` script to set up the necessary environment variables for oneCCL.  It uses python to find the directory where the `torch_ccl` package is installed and executes the `setvars.sh` script.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/distributed_training.mdx#_snippet_2\n\nLANGUAGE: shell script\nCODE:\n```\ntorch_ccl_path=$(python -c \"import torch; import torch_ccl; import os;  print(os.path.abspath(os.path.dirname(torch_ccl.__file__)))\")\nsource $torch_ccl_path/env/setvars.sh\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries\nDESCRIPTION: This snippet imports the required Python libraries for model handling, data processing, and OpenVINO integration. It includes modules from `datasets`, `evaluate`, `numpy`, `pandas`, `transformers`, `openvino`, and `optimum.intel`.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport warnings\nfrom pathlib import Path\n\nimport datasets\nimport evaluate\nimport numpy as np\nimport pandas as pd\nimport transformers\nfrom evaluate import evaluator\nfrom openvino.runtime import Core\nfrom optimum.intel import OVModelForQuestionAnswering, OVQuantizer, OVQuantizationConfig, OVConfig\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\ntransformers.logging.set_verbosity_error()\ndatasets.logging.set_verbosity_error()\n```\n\n----------------------------------------\n\nTITLE: Import Gradio Library\nDESCRIPTION: This snippet imports the Gradio library, which is used to create the interactive web interface for the chatbot.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\n```\n\n----------------------------------------\n\nTITLE: Installing Optimum Intel from source\nDESCRIPTION: This command installs Optimum Intel directly from the GitHub repository. It's useful for accessing the latest, unreleased changes. This installs only the base `optimum-intel` package without any extras/dependencies.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/README.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\npython -m pip install git+https://github.com/huggingface/optimum-intel.git\n```\n\n----------------------------------------\n\nTITLE: Printing Optimum Intel Version\nDESCRIPTION: This code snippet imports the `__version__` attribute from the `optimum.intel.version` module and prints the version of the optimum-intel package. This is useful for verifying the installed version of the library.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/ipex/langchain_hf_pipelines.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel.version import __version__\n\nprint(\"optimum-intel version is\", __version__)\n```\n\n----------------------------------------\n\nTITLE: Install requirements for OpenVINO notebooks using pip\nDESCRIPTION: This command installs the necessary dependencies to run all OpenVINO integration notebooks in the  Optimum project. It uses pip, the Python package installer, to install the packages listed in the `requirements.txt` file.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing oneCCL Bindings for PyTorch\nDESCRIPTION: This command installs the `oneccl_bindings_for_pytorch` package using pip, specifying the PyTorch version and using a custom index URL.  The `pytorch_version` placeholder should be replaced with the specific PyTorch version you are using. This ensures compatibility between oneCCL bindings and PyTorch.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/neural_compressor/distributed_training.mdx#_snippet_0\n\nLANGUAGE: shell script\nCODE:\n```\npip install oneccl_bind_pt=={pytorch_version} -f https://software.intel.com/ipex-whl-stable\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with Stable Diffusion XL and OpenVINO\nDESCRIPTION: This code demonstrates how to load and use a Stable Diffusion XL (SDXL) model with OpenVINO for text-to-image generation. It initializes the `OVStableDiffusionXLPipeline` with a specified model ID, sets a prompt, generates an image, and saves it.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/docs/source/openvino/tutorials/diffusers.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nbase = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"train station by Caspar David Friedrich\"\nimage = base(prompt).images[0]\nimage.save(\"train_station.png\")\n```\n\n----------------------------------------\n\nTITLE: Measure Acceptance Rate\nDESCRIPTION: This snippet demonstrates how to use the `AcceptanceRateRecorder` to measure the acceptance rate on the HumanEval dataset. It loads the dataset, iterates through a subset of the samples, generates text with speculative decoding, and prints the acceptance rate.\nSOURCE: https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\nfrom datasets import load_dataset\n\ndataset_name = \"openai_humaneval\"\ndataset_subset_name = None\nfield_name = \"prompt\"\nprompt_template = \"\"\"{text}\"\"\"\ndataset = load_dataset(dataset_name, dataset_subset_name, split=\"test\")[field_name]\nsamples_number = 30\nwith AcceptanceRateRecorder(stateless_model) as ar_recorder:\n    for text in tqdm(dataset[:samples_number]):\n        tokenized_prompt = tokenizer([prompt_template.format(text=text)], return_tensors='pt')\n        stateless_model.generate(\n            **tokenized_prompt,\n            max_new_tokens=128,\n            pad_token_id=tokenizer.eos_token_id,\n            assistant_model=asst_model,\n        )\nprint(f\"Acceptance rate: {ar_recorder.acceptance_rate() * 100:.2f}%\"\n```"
  }
]
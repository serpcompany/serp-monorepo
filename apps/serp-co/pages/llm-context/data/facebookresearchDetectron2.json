[
  {
    "owner": "facebookresearch",
    "repo": "detectron2",
    "content": "TITLE: Registering a Custom Dataset in Detectron2\nDESCRIPTION: This snippet demonstrates how to register a custom dataset with Detectron2 by implementing a function that returns dataset items and registering it with DatasetCatalog. The function should return a list of dictionaries containing dataset information.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/datasets.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef my_dataset_function():\n  ...\n  return list[dict] in the following format\n\nfrom detectron2.data import DatasetCatalog\nDatasetCatalog.register(\"my_dataset\", my_dataset_function)\n# later, to access the data:\ndata: List[Dict] = DatasetCatalog.get(\"my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Metadata to a Dataset in Detectron2\nDESCRIPTION: Example showing how to add metadata to a custom dataset using MetadataCatalog. This snippet demonstrates adding 'thing_classes' which defines the names of object categories for detection tasks.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/datasets.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.data import MetadataCatalog\nMetadataCatalog.get(\"my_dataset\").thing_classes = [\"person\", \"dog\"]\n```\n\n----------------------------------------\n\nTITLE: Running Inference Demo with Pre-trained Model\nDESCRIPTION: Command to run inference using Detectron2's demo script with a pre-trained Mask R-CNN model. Supports input from images, webcam, or video with various configuration options.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/GETTING_STARTED.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd demo/\npython demo.py --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n  --input input1.jpg input2.jpg \\\n  [--other-options]\n  --opts MODEL.WEIGHTS detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Backbone in Detectron2\nDESCRIPTION: This snippet demonstrates how to create and register a custom backbone architecture with Detectron2. It implements a simple ToyBackbone class that follows the Backbone interface and registers it in the BACKBONE_REGISTRY for use in configuration files.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/write-models.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec\n\n@BACKBONE_REGISTRY.register()\nclass ToyBackbone(Backbone):\n  def __init__(self, cfg, input_shape):\n    super().__init__()\n    # create your own backbone\n    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=16, padding=3)\n\n  def forward(self, image):\n    return {\"conv1\": self.conv1(image)}\n\n  def output_shape(self):\n    return {\"conv1\": ShapeSpec(channels=64, stride=16)}\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Backbone in Detectron2 Configuration\nDESCRIPTION: This snippet shows how to reference and use a custom backbone component in a Detectron2 configuration. It sets the MODEL.BACKBONE.NAME to the custom ToyBackbone class name, allowing the build_model function to find and use the previously registered implementation.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/write-models.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncfg = ...   # read a config\ncfg.MODEL.BACKBONE.NAME = 'ToyBackbone'   # or set it in the config file\nmodel = build_model(cfg)  # it will find `ToyBackbone` defined above\n```\n\n----------------------------------------\n\nTITLE: Installing Detectron2 from GitHub Source\nDESCRIPTION: Commands to install Detectron2 directly from the GitHub repository or from a local clone. Includes special instructions for macOS users who may need to specify environment variables.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n# (add --user if you don't have permission)\n\n# Or, to install it from a local clone:\ngit clone https://github.com/facebookresearch/detectron2.git\npython -m pip install -e detectron2\n\n# On macOS, you may need to prepend the above commands with a few environment variables:\nCC=clang CXX=clang++ ARCHFLAGS=\"-arch x86_64\" python -m pip install ...\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Pre-trained Mask R-CNN Model in Detectron2\nDESCRIPTION: Command for running inference using demo.py with a pre-trained Mask R-CNN model on input images. The command specifies the config file, input images, and model weights from Detectron2's model zoo.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/getting_started.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd demo/\npython demo.py --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n  --input input1.jpg input2.jpg \\\n  [--other-options]\n  --opts MODEL.WEIGHTS detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n```\n\n----------------------------------------\n\nTITLE: Initializing Mask R-CNN Model with Config\nDESCRIPTION: Simple initialization of a Mask R-CNN model using a configuration file loaded from YAML.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/extend.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# load proper yaml config file, then\nmodel = build_model(cfg)\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Detectron2's Augmentation System\nDESCRIPTION: Demonstrates how to define a sequence of augmentations, prepare input data, apply augmentations, and transform additional data types. Shows the core workflow with AugmentationList, AugInput, and Transform classes.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/augmentation.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.data import transforms as T\n# Define a sequence of augmentations:\naugs = T.AugmentationList([\n    T.RandomBrightness(0.9, 1.1),\n    T.RandomFlip(prob=0.5),\n    T.RandomCrop(\"absolute\", (640, 640))\n])  # type: T.Augmentation\n\n# Define the augmentation input (\"image\" required, others optional):\ninput = T.AugInput(image, boxes=boxes, sem_seg=sem_seg)\n# Apply the augmentation:\ntransform = augs(input)  # type: T.Transform\nimage_transformed = input.image  # new image\nsem_seg_transformed = input.sem_seg  # new semantic segmentation\n\n# For any extra data that needs to be augmented together, use transform, e.g.:\nimage2_transformed = transform.apply_image(image2)\npolygons_transformed = transform.apply_polygons(polygons)\n```\n\n----------------------------------------\n\nTITLE: Training Detectron2 Model with Multiple GPUs\nDESCRIPTION: Command for training a Mask R-CNN model using train_net.py with 8 GPUs. The script uses the default configuration file for Mask R-CNN with ResNet-50 FPN backbone trained for 1x schedule.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/getting_started.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd tools/\n./train_net.py --num-gpus 8 \\\n  --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\n```\n\n----------------------------------------\n\nTITLE: Training Detectron2 Model with Single GPU\nDESCRIPTION: Command for training a Mask R-CNN model on a single GPU with adjusted hyperparameters. The batch size and learning rate are modified to accommodate single-GPU training based on research recommendations.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/getting_started.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./train_net.py \\\n  --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\\n  --num-gpus 1 SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Counter Evaluator in Python\nDESCRIPTION: Example implementation of a custom DatasetEvaluator that counts detected instances in the validation set. Shows the basic structure with reset(), process(), and evaluate() methods.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/evaluation.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Counter(DatasetEvaluator):\n  def reset(self):\n    self.count = 0\n  def process(self, inputs, outputs):\n    for output in outputs:\n      self.count += len(output[\"instances\"])\n  def evaluate(self):\n    # save self.count somewhere, or print it, or return it.\n    return {\"count\": self.count}\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Trained Detectron2 Model\nDESCRIPTION: Command for evaluating a previously trained Detectron2 model using train_net.py in evaluation-only mode. The command specifies the configuration file and path to the model checkpoint.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/getting_started.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./train_net.py \\\n  --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\\n  --eval-only MODEL.WEIGHTS /path/to/checkpoint_file\n```\n\n----------------------------------------\n\nTITLE: Training Detectron2 Model on Multiple GPUs\nDESCRIPTION: Command to train a Detectron2 model using 8 GPUs with the default configuration for Mask R-CNN.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/GETTING_STARTED.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd tools/\n./train_net.py --num-gpus 8 \\\n  --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\n```\n\n----------------------------------------\n\nTITLE: Evaluating Detectron2 Model Performance\nDESCRIPTION: Command to evaluate a trained model's performance using a specific checkpoint file.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/GETTING_STARTED.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./train_net.py \\\n  --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\\n  --eval-only MODEL.WEIGHTS /path/to/checkpoint_file\n```\n\n----------------------------------------\n\nTITLE: Loading a Python Config File with LazyConfig in Detectron2\nDESCRIPTION: Demonstrates how to define a config file in Python and load it using LazyConfig.load(). The resulting cfg object is an omegaconf dictionary that can be accessed using dot notation.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/lazyconfigs.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# config.py:\na = dict(x=1, y=2, z=dict(xx=1))\nb = dict(x=3, y=4)\n\n# my_code.py:\nfrom detectron2.config import LazyConfig\ncfg = LazyConfig.load(\"path/to/config.py\")  # an omegaconf dictionary\nassert cfg.a.z.xx == 1\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Mapper Function for Detectron2 Dataloader\nDESCRIPTION: This snippet demonstrates how to implement a custom mapper function when the default DatasetMapper is not sufficient. It implements a minimal mapper that reads images, resizes them to 800x800, and transforms annotations accordingly.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/data_loading.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.data import detection_utils as utils\n # Show how to implement a minimal mapper, similar to the default DatasetMapper\ndef mapper(dataset_dict):\n    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n    # can use other ways to read image\n    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n    # See \"Data Augmentation\" tutorial for details usage\n    auginput = T.AugInput(image)\n    transform = T.Resize((800, 800))(auginput)\n    image = torch.from_numpy(auginput.image.transpose(2, 0, 1))\n    annos = [\n        utils.transform_instance_annotations(annotation, [transform], image.shape[1:])\n        for annotation in dataset_dict.pop(\"annotations\")\n    ]\n    return {\n       # create the format that the model expects\n       \"image\": image,\n       \"instances\": utils.annotations_to_instances(annos, image.shape[1:])\n    }\ndataloader = build_detection_train_loader(cfg, mapper=mapper)\n```\n\n----------------------------------------\n\nTITLE: Registering New Data Types for Transforms\nDESCRIPTION: Shows how to extend the transform system to support new data types beyond the built-in ones. The example demonstrates registering a custom handler for rotated bounding boxes with horizontal flipping.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/augmentation.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@T.HFlipTransform.register_type(\"rotated_boxes\")\ndef func(flip_transform: T.HFlipTransform, rotated_boxes: Any):\n    # do the work\n    return flipped_rotated_boxes\n\nt = HFlipTransform(width=800)\ntransformed_rotated_boxes = t.apply_rotated_boxes(rotated_boxes)  # func will be called\n```\n\n----------------------------------------\n\nTITLE: Partial Model Execution in Detectron2\nDESCRIPTION: Demonstrates how to partially execute a model to obtain intermediate features, specifically mask features before mask head.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/models.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimages = ImageList.from_tensors(...)  # preprocessed input tensor\nmodel = build_model(cfg)\nmodel.eval()\nfeatures = model.backbone(images.tensor)\nproposals, _ = model.proposal_generator(images, features)\ninstances, _ = model.roi_heads(images, features, proposals)\nmask_features = [features[f] for f in model.roi_heads.in_features]\nmask_features = model.roi_heads.mask_pooler(mask_features, [x.pred_boxes for x in instances])\n```\n\n----------------------------------------\n\nTITLE: Inference with Detectron2 Models\nDESCRIPTION: Shows how to perform inference using a model in evaluation mode with gradient computation disabled.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/models.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel.eval()\nwith torch.no_grad():\n  outputs = model(inputs)\n```\n\n----------------------------------------\n\nTITLE: Accessing Detectron2 Model Registries\nDESCRIPTION: Lists various model registries provided by Detectron2 for customizing components. These registries allow users to replace default components with custom implementations without modifying the core code.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/modeling.rst#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. autodata:: detectron2.modeling.META_ARCH_REGISTRY\n.. autodata:: detectron2.modeling.BACKBONE_REGISTRY\n.. autodata:: detectron2.modeling.PROPOSAL_GENERATOR_REGISTRY\n.. autodata:: detectron2.modeling.RPN_HEAD_REGISTRY\n.. autodata:: detectron2.modeling.ANCHOR_GENERATOR_REGISTRY\n.. autodata:: detectron2.modeling.ROI_HEADS_REGISTRY\n.. autodata:: detectron2.modeling.ROI_BOX_HEAD_REGISTRY\n.. autodata:: detectron2.modeling.ROI_MASK_HEAD_REGISTRY\n.. autodata:: detectron2.modeling.ROI_KEYPOINT_HEAD_REGISTRY\n```\n\n----------------------------------------\n\nTITLE: Building Model from Config in Detectron2\nDESCRIPTION: Demonstrates how to build a model structure from a yacs config object using the build_model function.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/models.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.modeling import build_model\nmodel = build_model(cfg)  # returns a torch.nn.Module\n```\n\n----------------------------------------\n\nTITLE: Custom Transform Strategy for Keypoints\nDESCRIPTION: Demonstrates an advanced usage where transform operations are inspected to apply domain-specific logic for keypoint annotations, such as swapping left/right keypoints after horizontal flipping.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/augmentation.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# augs, input are defined as in previous examples\ntransform = augs(input)  # type: T.Transform\nkeypoints_xy = transform.apply_coords(keypoints_xy)   # transform the coordinates\n\n# get a list of all transforms that were applied\ntransforms = T.TransformList([transform]).transforms\n# check if it is flipped for odd number of times\ndo_hflip = sum(isinstance(t, T.HFlipTransform) for t in transforms) % 2 == 1\nif do_hflip:\n    keypoints_xy = keypoints_xy[flip_indices_mapping]\n```\n\n----------------------------------------\n\nTITLE: HTML Table for Faster R-CNN Model Performance Metrics\nDESCRIPTION: HTML table displaying performance metrics for various Faster R-CNN models with different backbones (R50, R101, X101) and feature extractors (C4, DC5, FPN). Includes training schedules, speed metrics, memory usage, and AP scores with model download links.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table><tbody>\n<!-- START TABLE -->\n<!-- TABLE HEADER -->\n<th valign=\"bottom\">Name</th>\n<th valign=\"bottom\">lr<br/>sched</th>\n<th valign=\"bottom\">train<br/>time<br/>(s/iter)</th>\n<th valign=\"bottom\">inference<br/>time<br/>(s/im)</th>\n<th valign=\"bottom\">train<br/>mem<br/>(GB)</th>\n<th valign=\"bottom\">box<br/>AP</th>\n<th valign=\"bottom\">model id</th>\n<th valign=\"bottom\">download</th>\n<!-- TABLE BODY -->\n<!-- ROW: faster_rcnn_R_50_C4_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml\">R50-C4</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.551</td>\n<td align=\"center\">0.102</td>\n<td align=\"center\">4.8</td>\n<td align=\"center\">35.7</td>\n<td align=\"center\">137257644</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/137257644/model_final_721ade.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/137257644/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_R_50_DC5_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_50_DC5_1x.yaml\">R50-DC5</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.380</td>\n<td align=\"center\">0.068</td>\n<td align=\"center\">5.0</td>\n<td align=\"center\">37.3</td>\n<td align=\"center\">137847829</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_DC5_1x/137847829/model_final_51d356.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_DC5_1x/137847829/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_R_50_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\">R50-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.210</td>\n<td align=\"center\">0.038</td>\n<td align=\"center\">3.0</td>\n<td align=\"center\">37.9</td>\n<td align=\"center\">137257794</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_1x/137257794/model_final_b275ba.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_1x/137257794/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_R_50_C4_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_50_C4_3x.yaml\">R50-C4</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.543</td>\n<td align=\"center\">0.104</td>\n<td align=\"center\">4.8</td>\n<td align=\"center\">38.4</td>\n<td align=\"center\">137849393</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_C4_3x/137849393/model_final_f97cb7.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_C4_3x/137849393/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_R_50_DC5_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml\">R50-DC5</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.378</td>\n<td align=\"center\">0.070</td>\n<td align=\"center\">5.0</td>\n<td align=\"center\">39.0</td>\n<td align=\"center\">137849425</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_DC5_3x/137849425/model_final_68d202.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_DC5_3x/137849425/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_R_50_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\">R50-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.209</td>\n<td align=\"center\">0.038</td>\n<td align=\"center\">3.0</td>\n<td align=\"center\">40.2</td>\n<td align=\"center\">137849458</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_R_101_C4_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_101_C4_3x.yaml\">R101-C4</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.619</td>\n<td align=\"center\">0.139</td>\n<td align=\"center\">5.9</td>\n<td align=\"center\">41.1</td>\n<td align=\"center\">138204752</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_C4_3x/138204752/model_final_298dad.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_C4_3x/138204752/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_R_101_DC5_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml\">R101-DC5</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.452</td>\n<td align=\"center\">0.086</td>\n<td align=\"center\">6.1</td>\n<td align=\"center\">40.6</td>\n<td align=\"center\">138204841</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_DC5_3x/138204841/model_final_3e0943.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_DC5_3x/138204841/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_R_101_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\">R101-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.286</td>\n<td align=\"center\">0.051</td>\n<td align=\"center\">4.1</td>\n<td align=\"center\">42.0</td>\n<td align=\"center\">137851257</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: faster_rcnn_X_101_32x8d_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\">X101-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.638</td>\n<td align=\"center\">0.098</td>\n<td align=\"center\">6.7</td>\n<td align=\"center\">43.0</td>\n<td align=\"center\">139173657</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/model_final_68b088.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/metrics.json\">metrics</a></td>\n</tr>\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: Creating Custom ROI Heads with Explicit Arguments in Detectron2\nDESCRIPTION: This snippet demonstrates how to modify a Faster R-CNN model by creating custom ROI heads with explicit arguments. It shows how to instantiate StandardROIHeads with a custom box predictor implementation to use custom loss functions while keeping other components unchanged.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/write-models.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nroi_heads = StandardROIHeads(\n  cfg, backbone.output_shape(),\n  box_predictor=MyRCNNOutput(...)\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving Model Checkpoints in Detectron2\nDESCRIPTION: Shows how to load existing checkpoints and save new ones using DetectionCheckpointer class.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/models.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.checkpoint import DetectionCheckpointer\nDetectionCheckpointer(model).load(file_path_or_url)  # load a file, usually from cfg.MODEL.WEIGHTS\n\ncheckpointer = DetectionCheckpointer(model, save_dir=\"output\")\ncheckpointer.save(\"model_999\")  # save to output/model_999.pth\n```\n\n----------------------------------------\n\nTITLE: Viewing LazyConfig Structure in Detectron2\nDESCRIPTION: Shows how to view the structure of a LazyConfig object by converting it to a Python dictionary, which is useful for understanding and modifying complex configurations.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/lazyconfigs.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.model_zoo import get_config\nfrom detectron2.config import LazyConfig\nprint(LazyConfig.to_py(get_config(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.py\")))\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Color and Resize Augmentations\nDESCRIPTION: Shows how to implement new augmentations by subclassing T.Augmentation and implementing the get_transform method. Includes examples for color transformation and custom resizing.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/augmentation.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyColorAugmentation(T.Augmentation):\n    def get_transform(self, image):\n        r = np.random.rand(2)\n        return T.ColorTransform(lambda x: x * r[0] + r[1] * 10)\n\nclass MyCustomResize(T.Augmentation):\n    def get_transform(self, image):\n        old_h, old_w = image.shape[:2]\n        new_h, new_w = int(old_h * np.random.rand()), int(old_w * 1.5)\n        return T.ResizeTransform(old_h, old_w, new_h, new_w)\n\naugs = MyCustomResize()\ntransform = augs(input)\n```\n\n----------------------------------------\n\nTITLE: Configuring DensePose R-101-FPN Model with Legacy Schedule in YAML\nDESCRIPTION: YAML configuration for a DensePose R-101-FPN model using the legacy training schedule. This defines the model architecture, training parameters, and evaluation metrics for a deeper backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndensepose_rcnn_R_101_FPN_s1x_legacy\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Structure for Instance/Keypoint Detection\nDESCRIPTION: Defines the expected directory structure for COCO instance and keypoint detection datasets including annotation files and image directories.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ncoco/\n  annotations/\n    instances_{train,val}2017.json\n    person_keypoints_{train,val}2017.json\n  {train,val}2017/\n    # image files that are mentioned in the corresponding json\n```\n\n----------------------------------------\n\nTITLE: Command Line Config Override for Detectron2 Demo\nDESCRIPTION: Shows how to override config values via command line arguments in Detectron2's demo script. Demonstrates setting model weights and input size test parameters.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/configs.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./demo.py --config-file config.yaml [--other-options] \\\n  --opts MODEL.WEIGHTS /path/to/weights INPUT.MIN_SIZE_TEST 1000\n```\n\n----------------------------------------\n\nTITLE: Customizing Detectron2 Dataloader with Fixed Image Size\nDESCRIPTION: This snippet shows how to customize the Detectron2 dataloader by using the default DatasetMapper with a custom transformation that resizes all images to a fixed size (800x800) during training.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/data_loading.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport detectron2.data.transforms as T\nfrom detectron2.data import DatasetMapper   # the default mapper\ndataloader = build_detection_train_loader(cfg,\n   mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n      T.Resize((800, 800))\n   ]))\n# use this dataloader instead of the default\n```\n\n----------------------------------------\n\nTITLE: Training Cascade Mask R-CNN Models in Detectron2\nDESCRIPTION: Command to train Cascade Mask R-CNN models using the lazyconfig_train_net.py tool in Detectron2. This executes training with the specified configuration file using 64 GPUs by default with a batch size of 64.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/ViTDet/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n../../tools/lazyconfig_train_net.py --config-file configs/path/to/config.py\n```\n\n----------------------------------------\n\nTITLE: Setting Detectron2 Dataset Directory\nDESCRIPTION: Sets the environment variable for the location of Detectron2 datasets. This determines where Detectron2 will look for dataset files.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport DETECTRON2_DATASETS=/path/to/datasets\n```\n\n----------------------------------------\n\nTITLE: Importing Detectron2 Modeling Module\nDESCRIPTION: Imports the detectron2.modeling module and its members. This module is the main entry point for Detectron2's modeling functionality.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/modeling.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: detectron2.modeling\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Inverting Transforms for Inference Results\nDESCRIPTION: Demonstrates how to invert applied augmentations to map prediction results back to the original image space, particularly useful for segmentation masks or other dense predictions.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/augmentation.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntransform = augs(input)\npred_mask = make_prediction(input.image)\ninv_transform = transform.inverse()\npred_mask_orig = inv_transform.apply_segmentation(pred_mask)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Panoptic-DeepLab Model on Cityscapes Dataset\nDESCRIPTION: This command evaluates a trained Panoptic-DeepLab model on the Cityscapes dataset. It specifies the configuration file and the path to the model checkpoint.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/Panoptic-DeepLab/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/detectron2/projects/Panoptic-DeepLab\npython train_net.py --config-file configs/Cityscapes-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_90k_bs32_crop_512_1024_dsconv.yaml --eval-only MODEL.WEIGHTS /path/to/model_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Initializing Mask R-CNN with Full Explicit Arguments\nDESCRIPTION: Detailed initialization of a Mask R-CNN model using fully explicit arguments, including backbone, proposal generator, ROI heads, and pixel normalization parameters.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/extend.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = GeneralizedRCNN(\n    backbone=FPN(\n        ResNet(\n            BasicStem(3, 64, norm=\"FrozenBN\"),\n            ResNet.make_default_stages(50, stride_in_1x1=True, norm=\"FrozenBN\"),\n            out_features=[\"res2\", \"res3\", \"res4\", \"res5\"],\n        ).freeze(2),\n        [\"res2\", \"res3\", \"res4\", \"res5\"],\n        256,\n        top_block=LastLevelMaxPool(),\n    ),\n    proposal_generator=RPN(\n        in_features=[\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"],\n        head=StandardRPNHead(in_channels=256, num_anchors=3),\n        anchor_generator=DefaultAnchorGenerator(\n            sizes=[[32], [64], [128], [256], [512]],\n            aspect_ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64],\n            offset=0.0,\n        ),\n        anchor_matcher=Matcher([0.3, 0.7], [0, -1, 1], allow_low_quality_matches=True),\n        box2box_transform=Box2BoxTransform([1.0, 1.0, 1.0, 1.0]),\n        batch_size_per_image=256,\n        positive_fraction=0.5,\n        pre_nms_topk=(2000, 1000),\n        post_nms_topk=(1000, 1000),\n        nms_thresh=0.7,\n    ),\n    roi_heads=StandardROIHeads(\n        num_classes=80,\n        batch_size_per_image=512,\n        positive_fraction=0.25,\n        proposal_matcher=Matcher([0.5], [0, 1], allow_low_quality_matches=False),\n        box_in_features=[\"p2\", \"p3\", \"p4\", \"p5\"],\n        box_pooler=ROIPooler(7, (1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32), 0, \"ROIAlignV2\"),\n        box_head=FastRCNNConvFCHead(\n            ShapeSpec(channels=256, height=7, width=7), conv_dims=[], fc_dims=[1024, 1024]\n        ),\n        box_predictor=FastRCNNOutputLayers(\n            ShapeSpec(channels=1024),\n            test_score_thresh=0.05,\n            box2box_transform=Box2BoxTransform((10, 10, 5, 5)),\n            num_classes=80,\n        ),\n        mask_in_features=[\"p2\", \"p3\", \"p4\", \"p5\"],\n        mask_pooler=ROIPooler(14, (1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32), 0, \"ROIAlignV2\"),\n        mask_head=MaskRCNNConvUpsampleHead(\n            ShapeSpec(channels=256, width=14, height=14),\n            num_classes=80,\n            conv_dims=[256, 256, 256, 256, 256],\n        ),\n    ),\n    pixel_mean=[103.530, 116.280, 123.675],\n    pixel_std=[1.0, 1.0, 1.0],\n    input_format=\"BGR\",\n)\n```\n\n----------------------------------------\n\nTITLE: Training TensorMask BiPyramid with ResNet-50 on Multiple GPUs\nDESCRIPTION: Specific command to train a TensorMask model with BiPyramid and ResNet-50 backbone using 1x schedule on 8 GPUs. This is an example of a complete training command with specific configuration.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/TensorMask/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython /path/to/detectron2/projects/TensorMask/train_net.py --config-file configs/tensormask_R_50_FPN_1x.yaml --num-gpus 8\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Crop Augmentation with Multiple Inputs\nDESCRIPTION: Demonstrates how to implement an augmentation that uses multiple input types from AugInput to make decisions. Shows how to use both image and semantic segmentation to determine crop parameters.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/augmentation.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyCustomCrop(T.Augmentation):\n    def get_transform(self, image, sem_seg):\n        # decide where to crop using both image and sem_seg\n        return T.CropTransform(...)\n\naugs = MyCustomCrop()\nassert hasattr(input, \"image\") and hasattr(input, \"sem_seg\")\ntransform = augs(input)\n```\n\n----------------------------------------\n\nTITLE: Evaluating TensorMask Model Performance\nDESCRIPTION: Command to evaluate a trained TensorMask model. This example uses a 6x schedule with scale augmentation and requires specifying the path to the model checkpoint.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/TensorMask/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython /path/to/detectron2/projects/TensorMask/train_net.py --config-file configs/tensormask_R_50_FPN_6x.yaml --eval-only MODEL.WEIGHTS /path/to/model_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Registering a COCO Format Dataset in Detectron2\nDESCRIPTION: Example of registering a dataset in COCO format using the built-in register_coco_instances function. This automatically handles both dataset registration and setting up basic metadata.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/datasets.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"my_dataset\", {}, \"json_annotation.json\", \"path/to/image/dir\")\n```\n\n----------------------------------------\n\nTITLE: Registering Custom ROI Heads for Configuration in Detectron2\nDESCRIPTION: This snippet shows how to register custom ROI heads for use in configuration files. It creates a subclass of StandardROIHeads that uses a custom output layer implementation, then registers it in the ROI_HEADS_REGISTRY to make it accessible from configuration.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/write-models.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@ROI_HEADS_REGISTRY.register()\nclass MyStandardROIHeads(StandardROIHeads):\n  def __init__(self, cfg, input_shape):\n    super().__init__(cfg, input_shape,\n                     box_predictor=MyRCNNOutput(...))\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Detectron2 Dataset Structure\nDESCRIPTION: Shows the basic directory structure required for Detectron2's builtin datasets under the DETECTRON2_DATASETS environment variable.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n$DETECTRON2_DATASETS/\n  coco/\n  lvis/\n  cityscapes/\n  VOC20{07,12}/\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Hello Hook in Python for Detectron2 Training\nDESCRIPTION: This snippet demonstrates how to create a custom hook in Detectron2 that prints a hello message every 100 iterations during training. It subclasses the HookBase class and implements the after_step method.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/training.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass HelloHook(HookBase):\n  def after_step(self):\n    if self.trainer.iter % 100 == 0:\n      print(f\"Hello at iteration {self.trainer.iter}!\")\n```\n\n----------------------------------------\n\nTITLE: Executing Apply Net Tool in Dump Mode (Bash)\nDESCRIPTION: General command structure for using the apply_net tool in dump mode. It requires a configuration file, model file, and input image file or folder. An optional output file can be specified.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py dump [-h] [-v] [--output <dump_file>] <config> <model> <input>\n```\n\n----------------------------------------\n\nTITLE: Basic Config Operations in Python with Detectron2\nDESCRIPTION: Demonstrates fundamental operations with Detectron2's config system including initialization, modification, file loading/saving, and config merging. Shows how to obtain default config, add custom components, load from file, merge from list, and save to file.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/configs.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.config import get_cfg\ncfg = get_cfg()    # obtain detectron2's default config\ncfg.xxx = yyy      # add new configs for your own custom components\ncfg.merge_from_file(\"my_cfg.yaml\")   # load values from a file\n\ncfg.merge_from_list([\"MODEL.WEIGHTS\", \"weights.pth\"])   # can also load values from a list of str\nprint(cfg.dump())  # print formatted configs\nwith open(\"output.yaml\", \"w\") as f:\n  f.write(cfg.dump())   # save config to file\n```\n\n----------------------------------------\n\nTITLE: Training Models with EventStorage in Detectron2\nDESCRIPTION: Example of using EventStorage context manager for model training to capture training statistics.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/models.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.utils.events import EventStorage\nwith EventStorage() as storage:\n  losses = model(inputs)\n```\n\n----------------------------------------\n\nTITLE: Training TensorMask with a Configuration File\nDESCRIPTION: Generic training command for TensorMask models, which requires specifying a configuration file that defines the model architecture and training parameters.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/TensorMask/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython /path/to/detectron2/projects/TensorMask/train_net.py --config-file <config.yaml>\n```\n\n----------------------------------------\n\nTITLE: Training Detectron2 Model on Single GPU\nDESCRIPTION: Modified training command for single GPU setup with adjusted learning parameters including batch size and base learning rate.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/GETTING_STARTED.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./train_net.py \\\n  --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\\n  --num-gpus 1 SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025\n```\n\n----------------------------------------\n\nTITLE: Evaluating DensePose Model\nDESCRIPTION: This command shows how to evaluate a trained DensePose model using the same script as training, with additional flags for evaluation mode and model weights.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/GETTING_STARTED.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython train_net.py --config-file configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\n    --eval-only MODEL.WEIGHTS model.pth\n```\n\n----------------------------------------\n\nTITLE: Adding Project-Specific Configs in Detectron2\nDESCRIPTION: Example of how to add project-specific configurations to Detectron2's config system, using PointRend project as an example.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/configs.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.projects.point_rend import add_pointrend_config\ncfg = get_cfg()    # obtain detectron2's default config\nadd_pointrend_config(cfg)  # add pointrend's default config\n# ... ...\n```\n\n----------------------------------------\n\nTITLE: Training PointRend Model with Detectron2\nDESCRIPTION: Command to train a PointRend model using 8 GPUs. It uses a specific configuration file for instance segmentation on the COCO dataset with a ResNet-50 FPN backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/PointRend/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/detectron2/projects/PointRend\npython train_net.py --config-file configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_1x_coco.yaml --num-gpus 8\n```\n\n----------------------------------------\n\nTITLE: Using Legacy ROIAlign Behavior in Detectron2\nDESCRIPTION: Code example showing how to enable the old ROIAlign behavior in Detectron2 to maintain compatibility with Detectron. This allows users to use aligned=False parameter or choose ROIAlign instead of the default ROIAlignV2.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/notes/compatibility.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nROIAlign(aligned=False)\n```\n\n----------------------------------------\n\nTITLE: Visualizing CSE Rainbow with DensePose\nDESCRIPTION: Command to display bounding boxes and CSE rainbow visualization (closest vertices prediction) for detected persons using a DensePose CSE model with ResNet-50 FPN backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show configs/cse/densepose_rcnn_R_50_FPN_s1x.yaml  \\\nhttps://dl.fbaipublicfiles.com/densepose/cse/densepose_rcnn_R_50_FPN_s1x/251155172/model_final_c4ea5f.pkl \\\nimage.jpg dp_vertex,bbox -v\n```\n\n----------------------------------------\n\nTITLE: Manual Evaluation Loop Implementation\nDESCRIPTION: Shows how to manually implement an evaluation loop using evaluators, including data processing and result collection.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/evaluation.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_all_inputs_outputs():\n  for data in data_loader:\n    yield data, model(data)\n\nevaluator.reset()\nfor inputs, outputs in get_all_inputs_outputs():\n  evaluator.process(inputs, outputs)\neval_results = evaluator.evaluate()\n```\n\n----------------------------------------\n\nTITLE: Logging Custom Metrics in Detectron2 Models during Training\nDESCRIPTION: This code snippet shows how to log custom metrics during training in Detectron2. It demonstrates accessing the EventStorage, computing a custom value, and storing it as a scalar metric.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/training.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.utils.events import get_event_storage\n\n# inside the model:\nif self.training:\n  value = # compute the value from inputs\n  storage = get_event_storage()\n  storage.put_scalar(\"some_accuracy\", value)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorMask for Detectron2\nDESCRIPTION: Command to install TensorMask as a Detectron2 extension. This compiles the TensorMask-specific operation called 'swap_align2nat' after Detectron2 is installed and datasets are set up.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/TensorMask/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e /path/to/detectron2/projects/TensorMask\n```\n\n----------------------------------------\n\nTITLE: Creating a Recursive Instantiation Dictionary with LazyCall in Detectron2\nDESCRIPTION: Shows how to use LazyCall to create a dictionary representing a recursive instantiation of objects. This approach allows for flexible object creation and configuration.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/lazyconfigs.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.config import LazyCall as L\nfrom my_app import Trainer, Optimizer\ncfg = L(Trainer)(\n  optimizer=L(Optimizer)(\n    lr=0.01,\n    algo=\"SGD\"\n  )\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing V Coordinates for Body Parts with DensePose\nDESCRIPTION: Command to display bounding boxes and V coordinate values for body parts using a DensePose model with ResNet-50 FPN backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml  \\\nhttps://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\nimage.jpg bbox,dp_v -v\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CUDA 11.3 and PyTorch 1.10\nDESCRIPTION: Command to install pre-built Detectron2 package for CUDA 11.3 with PyTorch 1.10 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n```\n\n----------------------------------------\n\nTITLE: Configuring Improved DensePose R-50-FPN Model in YAML\nDESCRIPTION: YAML configuration for an improved DensePose R-50-FPN model with updated training schedule and Panoptic FPN head. This defines the enhanced model architecture and training parameters.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndensepose_rcnn_R_50_FPN_s1x\n```\n\n----------------------------------------\n\nTITLE: Using inference_on_dataset with Multiple Evaluators\nDESCRIPTION: Demonstrates how to use the inference_on_dataset function with multiple evaluators combined using DatasetEvaluators.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/evaluation.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\neval_results = inference_on_dataset(\n    model,\n    data_loader,\n    DatasetEvaluators([COCOEvaluator(...), Counter()]))\n```\n\n----------------------------------------\n\nTITLE: Training Mask R-CNN with point supervision on COCO\nDESCRIPTION: Command to train the Mask R-CNN R50-FPN model using point supervision with point augmentation on COCO dataset. The model is trained for 3x schedule using 8 GPUs.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/PointSup/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train_net.py --config-file configs/mask_rcnn_R_50_FPN_3x_point_sup_point_aug_coco.yaml --num-gpus 8\n```\n\n----------------------------------------\n\nTITLE: Instantiating Objects from Recursive Instantiation Dictionary in Detectron2\nDESCRIPTION: Demonstrates how to use the instantiate function to create actual objects from the recursive instantiation dictionary created with LazyCall.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/lazyconfigs.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom detectron2.config import instantiate\ntrainer = instantiate(cfg)\n# equivalent to:\n# from my_app import Trainer, Optimizer\n# trainer = Trainer(optimizer=Optimizer(lr=0.01, algo=\"SGD\"))\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CUDA 11.1 and PyTorch 1.9\nDESCRIPTION: Command to install a pre-built Detectron2 package for systems with CUDA 11.1 and PyTorch 1.9 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.9/index.html\n```\n\n----------------------------------------\n\nTITLE: Installing Detectron2 from Source\nDESCRIPTION: Commands to install Detectron2 directly from GitHub repository or from a local clone. Includes special instructions for macOS users requiring environment variables.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n# (add --user if you don't have permission)\n\n# Or, to install it from a local clone:\ngit clone https://github.com/facebookresearch/detectron2.git\npython -m pip install -e detectron2\n\n# On macOS, you may need to prepend the above commands with a few environment variables:\nCC=clang CXX=clang++ ARCHFLAGS=\"-arch x86_64\" python -m pip install ...\n```\n\n----------------------------------------\n\nTITLE: Parsing DensePose IUV Model Output (Python)\nDESCRIPTION: Python code snippet demonstrating how to parse the outputs of the first detected instance on the first image for an IUV model. It extracts the bounding box and UV coordinates.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimg_id, instance_id = 0, 0  # Look at the first image and the first detected instance\nbbox_xyxy = data[img_id]['pred_boxes_XYXY'][instance_id]\nresult = data[img_id]['pred_densepose'][instance_id]\nuv = result.uv\n```\n\n----------------------------------------\n\nTITLE: Generating 10-points annotations for COCO dataset\nDESCRIPTION: Command to generate 10-point annotations for the COCO dataset without using mask information. This prepares the dataset for point-supervised training.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/PointSup/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython tools/prepare_coco_point_annotations_without_masks.py 10\n```\n\n----------------------------------------\n\nTITLE: Training TridentNet with Detectron2\nDESCRIPTION: Command for training a TridentNet model using Detectron2 framework. Requires path to detectron2 project and a configuration file.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/TridentNet/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython /path/to/detectron2/projects/TridentNet/train_net.py --config-file <config.yaml>\n```\n\n----------------------------------------\n\nTITLE: Visualizing UV Coordinates as Contour Plots with DensePose\nDESCRIPTION: Command to display bounding boxes and UV coordinate contour plots for detected persons using a DensePose model with ResNet-50 FPN backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml  \\\nhttps://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\nimage.jpg dp_contour,bbox -v\n```\n\n----------------------------------------\n\nTITLE: Loading Dumped DensePose Results (Python)\nDESCRIPTION: Python code snippet to load the pickle file generated by the apply_net tool. It demonstrates how to add the DensePose project to the Python path and load the data using the pickle module.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# make sure DensePose is in your PYTHONPATH, or use the following line to add it:\nsys.path.append(\"/your_detectron2_path/detectron2_repo/projects/DensePose/\")\n\nf = open('/your_result_path/results.pkl', 'rb')\ndata = pickle.load(f)\n```\n\n----------------------------------------\n\nTITLE: Building and Launching Detectron2 Docker Container\nDESCRIPTION: Commands to build and run the Detectron2 Docker container with GPU support. Includes setting up proper shared memory and X11 forwarding for displaying images.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docker/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docker/\n# Build:\ndocker build --build-arg USER_ID=$UID -t detectron2:v0 .\n# Launch (require GPUs):\ndocker run --gpus all -it \\\n  --shm-size=8gb --env=\"DISPLAY\" --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n  --name=detectron2 detectron2:v0\n\n# Grant docker access to host X server to show images\nxhost +local:`docker inspect --format='{{ .Config.Hostname }}' detectron2`\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CUDA 11.3 and PyTorch 1.10\nDESCRIPTION: Command to install a pre-built Detectron2 package for systems with CUDA 11.3 and PyTorch 1.10 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n```\n\n----------------------------------------\n\nTITLE: Basic DensePose Visualization Command Structure\nDESCRIPTION: The general command structure for running apply_net.py in visualization mode, showing the main arguments and options that can be specified.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show [-h] [-v] [--min_score <score>] [--nms_thresh <threshold>] [--output <image_file>] <config> <model> <input> <visualizations>\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Detectron2 Models with benchmark.py\nDESCRIPTION: Command for benchmarking the training speed, inference speed, or data loading speed of a given Detectron2 configuration.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/tools/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark.py --config-file config.yaml --task train/eval/data [optional DDP flags]\n```\n\n----------------------------------------\n\nTITLE: Initializing Mask R-CNN with Mixed Config and Arguments\nDESCRIPTION: Creates a Mask R-CNN model using a combination of config file and explicit argument overrides for ROI heads and pixel standardization.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/extend.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = GeneralizedRCNN(\n  cfg,\n  roi_heads=StandardROIHeads(cfg, batch_size_per_image=666),\n  pixel_std=[57.0, 57.0, 57.0])\n```\n\n----------------------------------------\n\nTITLE: Training TridentNet-Fast with ResNet-50 Backbone\nDESCRIPTION: Example command for end-to-end TridentNet training using ResNet-50 backbone on 8 GPUs, specifying the configuration file for TridentNet-Fast.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/TridentNet/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython /path/to/detectron2/projects/TridentNet/train_net.py --config-file configs/tridentnet_fast_R_50_C4_1x.yaml --num-gpus 8\n```\n\n----------------------------------------\n\nTITLE: Exporting Detectron2 Model with Scripting Method\nDESCRIPTION: Command to export a Mask R-CNN model using scripting method to TorchScript format and run inference using C++.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/tools/deploy/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./export_model.py --config-file ../../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n    --output ./output --export-method scripting --format torchscript \\\n    MODEL.WEIGHTS detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl \\\n\n./build/torchscript_mask_rcnn output/model.ts input.jpg scripting\n```\n\n----------------------------------------\n\nTITLE: Training DensePose-RCNN on Multiple GPUs\nDESCRIPTION: This command launches end-to-end DensePose-RCNN training with ResNet-50 FPN backbone on 8 GPUs following the s1x schedule.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/GETTING_STARTED.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train_net.py --config-file configs/densepose_rcnn_R_50_FPN_s1x.yaml --num-gpus 8\n```\n\n----------------------------------------\n\nTITLE: HTML Table for COCO Panoptic Segmentation Baselines\nDESCRIPTION: A table displaying benchmark results for Panoptic FPN models on COCO Panoptic Segmentation task, including training schedules, speed metrics, memory usage, and accuracy metrics (box AP, mask AP, and PQ).\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<table><tbody>\n<!-- START TABLE -->\n<!-- TABLE HEADER -->\n<th valign=\"bottom\">Name</th>\n<th valign=\"bottom\">lr<br/>sched</th>\n<th valign=\"bottom\">train<br/>time<br/>(s/iter)</th>\n<th valign=\"bottom\">inference<br/>time<br/>(s/im)</th>\n<th valign=\"bottom\">train<br/>mem<br/>(GB)</th>\n<th valign=\"bottom\">box<br/>AP</th>\n<th valign=\"bottom\">mask<br/>AP</th>\n<th valign=\"bottom\">PQ</th>\n<th valign=\"bottom\">model id</th>\n<th valign=\"bottom\">download</th>\n<!-- TABLE BODY -->\n<!-- ROW: panoptic_fpn_R_50_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x.yaml\">R50-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.304</td>\n<td align=\"center\">0.053</td>\n<td align=\"center\">4.8</td>\n<td align=\"center\">37.6</td>\n<td align=\"center\">34.7</td>\n<td align=\"center\">39.4</td>\n<td align=\"center\">139514544</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x/139514544/model_final_dbfeb4.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x/139514544/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: panoptic_fpn_R_50_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\">R50-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.302</td>\n<td align=\"center\">0.053</td>\n<td align=\"center\">4.8</td>\n<td align=\"center\">40.0</td>\n<td align=\"center\">36.5</td>\n<td align=\"center\">41.5</td>\n<td align=\"center\">139514569</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x/139514569/model_final_c10459.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x/139514569/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: panoptic_fpn_R_101_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\">R101-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.392</td>\n<td align=\"center\">0.066</td>\n<td align=\"center\">6.0</td>\n<td align=\"center\">42.4</td>\n<td align=\"center\">38.5</td>\n<td align=\"center\">43.0</td>\n<td align=\"center\">139514519</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/metrics.json\">metrics</a></td>\n</tr>\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: Training DeepLab Model with Detectron2\nDESCRIPTION: This command trains a DeepLab model using Detectron2 on 8 GPUs. It specifies the configuration file for a DeepLabV3+ model trained on the Cityscapes dataset.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DeepLab/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/detectron2/projects/DeepLab\npython train_net.py --config-file configs/Cityscapes-SemanticSegmentation/deeplab_v3_plus_R_103_os16_mg124_poly_90k_bs16.yaml --num-gpus 8\n```\n\n----------------------------------------\n\nTITLE: Training Detectron2 Models with LazyConfig\nDESCRIPTION: Command to train Detectron2 models using the LazyConfig system. It specifies the config file and number of GPUs to use.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/Rethinking-BatchNorm/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n../../tools/lazyconfig_train_net.py --config-file configs/X.py --num-gpus 8\n```\n\n----------------------------------------\n\nTITLE: Training MViTv2 Detection Models with Detectron2\nDESCRIPTION: Command to train MViTv2 detection models using Detectron2's lazyconfig_train_net.py script. It specifies the config file path and uses 64 GPUs with a batch size of 64 by default.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/MViTv2/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n../../tools/lazyconfig_train_net.py --config-file configs/path/to/config.py\n```\n\n----------------------------------------\n\nTITLE: HTML Table for Detectron2 RPN & Fast R-CNN Model Comparison\nDESCRIPTION: HTML table that compares the performance and specifications of three Detectron2 object detection models: RPN R50-C4, RPN R50-FPN, and Fast R-CNN R50-FPN. The table includes metrics like learning rate schedule, training and inference speed, memory usage, and performance metrics.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<table><tbody>\n<!-- START TABLE -->\n<!-- TABLE HEADER -->\n<th valign=\"bottom\">Name</th>\n<th valign=\"bottom\">lr<br/>sched</th>\n<th valign=\"bottom\">train<br/>time<br/>(s/iter)</th>\n<th valign=\"bottom\">inference<br/>time<br/>(s/im)</th>\n<th valign=\"bottom\">train<br/>mem<br/>(GB)</th>\n<th valign=\"bottom\">box<br/>AP</th>\n<th valign=\"bottom\">prop.<br/>AR</th>\n<th valign=\"bottom\">model id</th>\n<th valign=\"bottom\">download</th>\n<!-- TABLE BODY -->\n<!-- ROW: rpn_R_50_C4_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/rpn_R_50_C4_1x.yaml\">RPN R50-C4</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.130</td>\n<td align=\"center\">0.034</td>\n<td align=\"center\">1.5</td>\n<td align=\"center\"></td>\n<td align=\"center\">51.6</td>\n<td align=\"center\">137258005</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/rpn_R_50_C4_1x/137258005/model_final_450694.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/rpn_R_50_C4_1x/137258005/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: rpn_R_50_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/rpn_R_50_FPN_1x.yaml\">RPN R50-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.186</td>\n<td align=\"center\">0.032</td>\n<td align=\"center\">2.7</td>\n<td align=\"center\"></td>\n<td align=\"center\">58.0</td>\n<td align=\"center\">137258492</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/rpn_R_50_FPN_1x/137258492/model_final_02ce48.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/rpn_R_50_FPN_1x/137258492/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: fast_rcnn_R_50_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/fast_rcnn_R_50_FPN_1x.yaml\">Fast R-CNN R50-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.140</td>\n<td align=\"center\">0.029</td>\n<td align=\"center\">2.6</td>\n<td align=\"center\">37.8</td>\n<td align=\"center\"></td>\n<td align=\"center\">137635226</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/fast_rcnn_R_50_FPN_1x/137635226/model_final_e5f7ce.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/fast_rcnn_R_50_FPN_1x/137635226/metrics.json\">metrics</a></td>\n</tr>\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: Evaluating Mask R-CNN point-supervised model\nDESCRIPTION: Command to evaluate a pre-trained Mask R-CNN model that was trained with point supervision. The command loads model weights from a specified checkpoint path.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/PointSup/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython train_net.py --config-file configs/mask_rcnn_R_50_FPN_3x_point_sup_point_aug_coco.yaml --eval-only MODEL.WEIGHTS /path/to/model_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Executing Print Mode Query for DensePose Dataset in Bash\nDESCRIPTION: Command structure for using the print mode of query_db.py to output dataset entries. It includes optional arguments for verbosity and limiting entries, as well as mandatory arguments for dataset specification and entry selection.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py print [-h] [-v] [--max-entries N] <dataset> <selector>\n```\n\n----------------------------------------\n\nTITLE: Evaluating MViTv2 Detection Models with Detectron2\nDESCRIPTION: Command to evaluate trained MViTv2 detection models using Detectron2's lazyconfig_train_net.py script. It specifies the config file path, sets the eval-only flag, and provides the path to the model checkpoint.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/MViTv2/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n../../tools/lazyconfig_train_net.py --config-file configs/path/to/config.py --eval-only train.init_checkpoint=/path/to/model_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Building Detectron2 Wheels for Multiple CUDA and Python Versions\nDESCRIPTION: Commands to build Detectron2 wheels for various combinations of CUDA and Python versions, and generate a wheel index file.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/dev/packaging/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./dev/packaging/build_all_wheels.sh\n./dev/packaging/gen_wheel_index.sh /path/to/wheels\n```\n\n----------------------------------------\n\nTITLE: Exporting Detectron2 Model with Tracing Method\nDESCRIPTION: Command to export a Mask R-CNN model using tracing method to TorchScript format and run inference using C++. Requires pre-trained weights and COCO dataset.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/tools/deploy/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./export_model.py --config-file ../../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n    --output ./output --export-method tracing --format torchscript \\\n    MODEL.WEIGHTS detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl \\\n    MODEL.DEVICE cuda\n\n./build/torchscript_mask_rcnn output/model.ts input.jpg tracing\n```\n\n----------------------------------------\n\nTITLE: Exporting Detectron2 Model with Caffe2 Tracing Method\nDESCRIPTION: Command to export a Mask R-CNN model using caffe2 tracing method to TorchScript format and run inference using C++. May have slight accuracy differences due to numerical precision.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/tools/deploy/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./export_model.py --config-file ../../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n    --output ./output --export-method caffe2_tracing --format torchscript \\\n    MODEL.WEIGHTS detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl \\\n\n./build/torchscript_mask_rcnn output/model.ts input.jpg caffe2_tracing\n```\n\n----------------------------------------\n\nTITLE: Installing DensePose as a Python Package\nDESCRIPTION: This command demonstrates how to install DensePose as a Python package using pip, directly from the GitHub repository.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/GETTING_STARTED.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/facebookresearch/detectron2@main#subdirectory=projects/DensePose\n```\n\n----------------------------------------\n\nTITLE: Evaluating PointRend Model with Detectron2\nDESCRIPTION: Command to evaluate a trained PointRend model. It uses the same configuration file as training and requires specifying the path to the model checkpoint.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/PointRend/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/detectron2/projects/PointRend\npython train_net.py --config-file configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_1x_coco.yaml --eval-only MODEL.WEIGHTS /path/to/model_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Building Detectron2 Wheel for CUDA 10.1 Release\nDESCRIPTION: Commands to build a Detectron2 wheel for CUDA 10.1 using a Docker container. It sets up the environment variables and runs the build script.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/dev/packaging/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ nvidia-docker run -it --storage-opt \"size=20GB\" --name pt  pytorch/manylinux-cuda101\n# inside the container:\n# git clone https://github.com/facebookresearch/detectron2/\n# cd detectron2\n# export CU_VERSION=cu101 D2_VERSION_SUFFIX= PYTHON_VERSION=3.7 PYTORCH_VERSION=1.8\n# ./dev/packaging/build_wheel.sh\n```\n\n----------------------------------------\n\nTITLE: Handling Keypoint Visibility After Transforms\nDESCRIPTION: Shows how to track keypoint visibility through a sequence of transformations by checking if keypoints remain within image boundaries after each transform step.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/augmentation.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntransform = augs(input)  # type: T.TransformList\nassert isinstance(transform, T.TransformList)\nfor t in transform.transforms:\n    keypoints_xy = t.apply_coords(keypoints_xy)\n    visibility &= (keypoints_xy >= [0, 0] & keypoints_xy <= [W, H]).all(axis=1)\n\n# btw, detectron2's `transform_keypoint_annotations` function chooses to label such keypoints \"visible\":\n# keypoints_xy = transform.apply_coords(keypoints_xy)\n# visibility &= (keypoints_xy >= [0, 0] & keypoints_xy <= [W, H]).all(axis=1)\n```\n\n----------------------------------------\n\nTITLE: DensePose CSE Feature List\nDESCRIPTION: Markdown list detailing the key features and updates in the DensePose CSE implementation, including file paths to cycle consistency losses, evaluator, dataset links and model zoo references.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/RELEASE_2021_06.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* [Pixel to shape](../densepose/modeling/losses/cycle_pix2shape.py) and [shape to shape](../densepose/modeling/losses/cycle_shape2shape.py) cycle consistency losses;\n* Mesh alignment [evaluator](../densepose/evaluation/mesh_alignment_evaluator.py);\n* Existing CSE datasets renamed to [ds1_train](https://dl.fbaipublicfiles.com/densepose/annotations/lvis/densepose_lvis_v1_ds1_train_v1.json) and [ds1_val](https://dl.fbaipublicfiles.com/densepose/annotations/lvis/densepose_lvis_v1_ds1_val_v1.json);\n* New CSE datasets [ds2_train](https://dl.fbaipublicfiles.com/densepose/annotations/lvis/densepose_lvis_v1_ds2_train_v1.json) and [ds2_val](https://dl.fbaipublicfiles.com/densepose/annotations/lvis/densepose_lvis_v1_ds2_val_v1.json) added;\n* Better CSE animal models trained with the 16k schedule added to the [model zoo](DENSEPOSE_CSE.md#animal-cse-models).\n```\n\n----------------------------------------\n\nTITLE: Training Panoptic-DeepLab on Cityscapes Dataset\nDESCRIPTION: This command trains a Panoptic-DeepLab model on the Cityscapes dataset using 8 GPUs. It specifies the configuration file and the number of GPUs to use.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/Panoptic-DeepLab/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/detectron2/projects/Panoptic-DeepLab\npython train_net.py --config-file configs/Cityscapes-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_90k_bs32_crop_512_1024_dsconv.yaml --num-gpus 8\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CUDA 10.2 and PyTorch 1.9\nDESCRIPTION: Command to install pre-built Detectron2 package for CUDA 10.2 with PyTorch 1.9 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n```\n\n----------------------------------------\n\nTITLE: Visualizing Bounding Box and V-Coordinate Colored Points in DensePose Dataset in Bash\nDESCRIPTION: Example command to show bounding box and points colored by V coordinate in part parameterization for a specific image (id=322) from the densepose_coco_2014_train dataset. It demonstrates the use of the dp_v visualization specifier.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py show densepose_coco_2014_train image_id:int=322 bbox,dp_v -v\n```\n\n----------------------------------------\n\nTITLE: Evaluating DeepLab Model with Detectron2\nDESCRIPTION: This command evaluates a trained DeepLab model using Detectron2. It specifies the configuration file and the path to the model checkpoint for evaluation.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DeepLab/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/detectron2/projects/DeepLab\npython train_net.py --config-file configs/Cityscapes-SemanticSegmentation/deeplab_v3_plus_R_103_os16_mg124_poly_90k_bs16.yaml --eval-only MODEL.WEIGHTS /path/to/model_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CUDA 11.1 and PyTorch 1.9\nDESCRIPTION: Command to install pre-built Detectron2 package for CUDA 11.1 with PyTorch 1.9 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.9/index.html\n```\n\n----------------------------------------\n\nTITLE: Explaining Detectron2 Utility Functions Purpose\nDESCRIPTION: This markdown snippet explains the purpose of the utility functions folder in the Detectron2 project. It clarifies that these functions are not part of the core library but are useful for model building and training using the configuration system.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/detectron2/utils/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Utility functions\n\nThis folder contain utility functions that are not used in the\ncore library, but are useful for building models or training\ncode using the config system.\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CUDA 10.2 and PyTorch 1.10\nDESCRIPTION: Command to install a pre-built Detectron2 package for systems with CUDA 10.2 and PyTorch 1.10 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.10/index.html\n```\n\n----------------------------------------\n\nTITLE: Evaluating RetinaNet with Domain-Specific Statistics\nDESCRIPTION: Command to evaluate a RetinaNet checkpoint after recomputing domain-specific statistics. This is used to reproduce specific results from Table 5 in the paper.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/Rethinking-BatchNorm/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./retinanet-eval-domain-specific.py checkpoint.pth\n```\n\n----------------------------------------\n\nTITLE: Evaluating TridentNet Model\nDESCRIPTION: Command for evaluating a pre-trained TridentNet model. Requires the configuration file and path to the model weights.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/TridentNet/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython /path/to/detectron2/projects/TridentNet/train_net.py --config-file configs/tridentnet_fast_R_50_C4_1x.yaml --eval-only MODEL.WEIGHTS model.pth\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CUDA 11.1 and PyTorch 1.8\nDESCRIPTION: Command to install pre-built Detectron2 package for CUDA 11.1 with PyTorch 1.8 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.8/index.html\n```\n\n----------------------------------------\n\nTITLE: HTML Table for RetinaNet Model Performance Metrics\nDESCRIPTION: HTML table displaying performance metrics for RetinaNet models with R50 and R101 backbones. Includes training schedules, speed metrics, memory usage, and AP scores with model download links for each configuration.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<table><tbody>\n<!-- START TABLE -->\n<!-- TABLE HEADER -->\n<th valign=\"bottom\">Name</th>\n<th valign=\"bottom\">lr<br/>sched</th>\n<th valign=\"bottom\">train<br/>time<br/>(s/iter)</th>\n<th valign=\"bottom\">inference<br/>time<br/>(s/im)</th>\n<th valign=\"bottom\">train<br/>mem<br/>(GB)</th>\n<th valign=\"bottom\">box<br/>AP</th>\n<th valign=\"bottom\">model id</th>\n<th valign=\"bottom\">download</th>\n<!-- TABLE BODY -->\n<!-- ROW: retinanet_R_50_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/retinanet_R_50_FPN_1x.yaml\">R50</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.205</td>\n<td align=\"center\">0.041</td>\n<td align=\"center\">4.1</td>\n<td align=\"center\">37.4</td>\n<td align=\"center\">190397773</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/retinanet_R_50_FPN_1x/190397773/model_final_bfca0b.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/retinanet_R_50_FPN_1x/190397773/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: retinanet_R_50_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/retinanet_R_50_FPN_3x.yaml\">R50</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.205</td>\n<td align=\"center\">0.041</td>\n<td align=\"center\">4.1</td>\n<td align=\"center\">38.7</td>\n<td align=\"center\">190397829</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/retinanet_R_50_FPN_3x/190397829/model_final_5bd44e.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/retinanet_R_50_FPN_3x/190397829/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: retinanet_R_101_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Detection/retinanet_R_101_FPN_3x.yaml\">R101</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.291</td>\n<td align=\"center\">0.054</td>\n<td align=\"center\">5.2</td>\n<td align=\"center\">40.4</td>\n<td align=\"center\">190397697</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/retinanet_R_101_FPN_3x/190397697/model_final_971ab9.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/retinanet_R_101_FPN_3x/190397697/metrics.json\">metrics</a></td>\n</tr>\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: Visualizing JSON Results from COCO/LVIS Evaluators\nDESCRIPTION: Command to visualize the JSON instance detection or segmentation results produced by Detectron2's COCO or LVIS evaluators.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/tools/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython visualize_json_results.py --input x.json --output dir/ --dataset coco_2017_val\n```\n\n----------------------------------------\n\nTITLE: Configuring ROIAlign Pool Type in Detectron2\nDESCRIPTION: Configuration parameter to use the legacy ROIAlign implementation rather than the default ROIAlignV2 in Detectron2. This is an alternative way to maintain compatibility with the older Detectron behavior.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/notes/compatibility.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nPOOLER_TYPE=ROIAlign\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CUDA 11.1 and PyTorch 1.10\nDESCRIPTION: Command to install a pre-built Detectron2 package for systems with CUDA 11.1 and PyTorch 1.10 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n```\n\n----------------------------------------\n\nTITLE: HTML Table for COCO Person Keypoint Detection Baselines\nDESCRIPTION: A table displaying benchmark results for Keypoint R-CNN models on COCO Person Keypoint Detection task, including training schedules, speed metrics, memory usage, and accuracy metrics (box AP and keypoint AP).\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<table><tbody>\n<!-- START TABLE -->\n<!-- TABLE HEADER -->\n<th valign=\"bottom\">Name</th>\n<th valign=\"bottom\">lr<br/>sched</th>\n<th valign=\"bottom\">train<br/>time<br/>(s/iter)</th>\n<th valign=\"bottom\">inference<br/>time<br/>(s/im)</th>\n<th valign=\"bottom\">train<br/>mem<br/>(GB)</th>\n<th valign=\"bottom\">box<br/>AP</th>\n<th valign=\"bottom\">kp.<br/>AP</th>\n<th valign=\"bottom\">model id</th>\n<th valign=\"bottom\">download</th>\n<!-- TABLE BODY -->\n<!-- ROW: keypoint_rcnn_R_50_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml\">R50-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.315</td>\n<td align=\"center\">0.072</td>\n<td align=\"center\">5.0</td>\n<td align=\"center\">53.6</td>\n<td align=\"center\">64.0</td>\n<td align=\"center\">137261548</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x/137261548/model_final_04e291.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x/137261548/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: keypoint_rcnn_R_50_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\">R50-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.316</td>\n<td align=\"center\">0.066</td>\n<td align=\"center\">5.0</td>\n<td align=\"center\">55.4</td>\n<td align=\"center\">65.5</td>\n<td align=\"center\">137849621</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x/137849621/model_final_a6e10b.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x/137849621/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: keypoint_rcnn_R_101_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\">R101-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.390</td>\n<td align=\"center\">0.076</td>\n<td align=\"center\">6.1</td>\n<td align=\"center\">56.4</td>\n<td align=\"center\">66.1</td>\n<td align=\"center\">138363331</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x/138363331/model_final_997cc7.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x/138363331/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: keypoint_rcnn_X_101_32x8d_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\">X101-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.738</td>\n<td align=\"center\">0.121</td>\n<td align=\"center\">8.7</td>\n<td align=\"center\">57.3</td>\n<td align=\"center\">66.0</td>\n<td align=\"center\">139686956</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x/139686956/model_final_5ad38f.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x/139686956/metrics.json\">metrics</a></td>\n</tr>\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: Running Developer Shell Scripts for Detectron2\nDESCRIPTION: This code snippet lists several shell scripts available for Detectron2 developers including linter.sh for code linting, run_{inference,instant}_tests.sh for testing model inference and training, and parse_results.sh for log file analysis. The testing scripts require 2 GPUs to execute.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/dev/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n- `linter.sh`: lint the codebase before commit.\n- `run_{inference,instant}_tests.sh`: run inference/training for a few iterations.\n   Note that these tests require 2 GPUs.\n- `parse_results.sh`: parse results from a log file.\n```\n\n----------------------------------------\n\nTITLE: Visualizing U Coordinates for Body Parts with DensePose\nDESCRIPTION: Command to display bounding boxes and U coordinate values for body parts using a DensePose model with ResNet-50 FPN backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml  \\\nhttps://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\nimage.jpg bbox,dp_u -v\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CPU-only and PyTorch 1.9\nDESCRIPTION: Command to install a pre-built CPU-only Detectron2 package with PyTorch 1.9 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.9/index.html\n```\n\n----------------------------------------\n\nTITLE: Preparing COCO-fied LVIS Annotations\nDESCRIPTION: Runs a Python script to prepare COCO-style annotations for the LVIS dataset, enabling evaluation of COCO-trained models on LVIS.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython datasets/prepare_cocofied_lvis.py\n```\n\n----------------------------------------\n\nTITLE: Model Performance Table - LVIS Dataset\nDESCRIPTION: Markdown table showing performance metrics for Mask R-CNN models with ViTDet backbones on LVIS dataset, including training time, inference time, memory usage, and AP scores.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/ViTDet/README.md#2025-04-21_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | pre-train | train time (s/im) | inference time (s/im) | train mem (GB) | box AP | mask AP | model id | download |\n|------|-----------|-----------------|-------------------|---------------|---------|----------|-----------|----------|\n| ViTDet, ViT-B | IN1K, MAE | 0.317 | 0.085 | 14.4 | 40.2 | 38.2 | 329225748 | model |\n| ViTDet, ViT-L | IN1K, MAE | 0.576 | 0.137 | 24.7 | 46.1 | 43.6 | 329211570 | model |\n| ViTDet, ViT-H | IN1K, MAE | 1.059 | 0.186 | 35.3 | 49.1 | 46.0 | 332434656 | model |\n```\n\n----------------------------------------\n\nTITLE: PanopticFPN Dataset Structure\nDESCRIPTION: Shows the required directory structure for PanopticFPN including JSON annotations and PNG files for panoptic segmentation.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ncoco/\n  annotations/\n    panoptic_{train,val}2017.json\n  panoptic_{train,val}2017/  # png annotations\n  panoptic_stuff_{train,val}2017/  # generated by the script mentioned below\n```\n\n----------------------------------------\n\nTITLE: Visualizing Texture Transfer with DensePose IUV\nDESCRIPTION: Command to display bounding boxes and texture transfer to detected persons using IUV mode with a DensePose model with ResNet-50 FPN backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml  \\\nhttps://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\nimage.jpg dp_iuv_texture,bbox --texture_atlas texture_from_SURREAL.jpg -v\n```\n\n----------------------------------------\n\nTITLE: Querying DensePose Dataset by Image ID Range in Bash\nDESCRIPTION: Example command to output all entries with image_id between 36 and 156 from the densepose_coco_2014_train dataset. It shows how to use a range selector for integer fields.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py print densepose_coco_2014_train image_id:int=36-156 -v\n```\n\n----------------------------------------\n\nTITLE: Export Method Comparison Table in reStructuredText\nDESCRIPTION: A table comparing different export methods (tracing, scripting, caffe2_tracing) and their supported features, formats, runtimes, and model compatibilities.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/deployment.md#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n```eval_rst\n+----------------------------+-------------+-------------+-----------------------------+\n|       Export Method        |   tracing   |  scripting  |       caffe2_tracing        |\n+============================+=============+=============+=============================+\n| **Formats**                | TorchScript | TorchScript | Caffe2, TorchScript, ONNX   |\n+----------------------------+-------------+-------------+-----------------------------+\n| **Runtime**                | PyTorch     | PyTorch     | Caffe2, PyTorch             |\n+----------------------------+-------------+-------------+-----------------------------+\n| C++/Python inference       | ✅          | ✅          | ✅                          |\n+----------------------------+-------------+-------------+-----------------------------+\n| Dynamic resolution         | ✅          | ✅          | ✅                          |\n+----------------------------+-------------+-------------+-----------------------------+\n| Batch size requirement     | Constant    | Dynamic     | Batch inference unsupported |\n+----------------------------+-------------+-------------+-----------------------------+\n| Extra runtime deps         | torchvision | torchvision | Caffe2 ops (usually already |\n|                            |             |             |                             |\n|                            |             |             | included in PyTorch)        |\n+----------------------------+-------------+-------------+-----------------------------+\n| Faster/Mask/Keypoint R-CNN | ✅          | ✅          | ✅                          |\n+----------------------------+-------------+-------------+-----------------------------+\n| RetinaNet                  | ✅          | ✅          | ✅                          |\n+----------------------------+-------------+-------------+-----------------------------+\n| PointRend R-CNN            | ✅          | ❌          | ❌                          |\n+----------------------------+-------------+-------------+-----------------------------+\n| Cascade R-CNN              | ✅          | ❌          | ❌                          |\n+----------------------------+-------------+-------------+-----------------------------+\n```\n```\n\n----------------------------------------\n\nTITLE: Preparing Panoptic FPN Semantic Annotations\nDESCRIPTION: Runs a Python script to extract semantic annotations from panoptic annotations for the PanopticFPN dataset.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython datasets/prepare_panoptic_fpn.py\n```\n\n----------------------------------------\n\nTITLE: Visualizing CSE Texture Transfer with DensePose\nDESCRIPTION: Command to display bounding boxes and CSE texture transfer to detected persons using a DensePose CSE model with ResNet-50 FPN backbone, specifying a texture atlas map for the SMPL mesh.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show configs/cse/densepose_rcnn_R_50_FPN_s1x.yaml  \\\nhttps://dl.fbaipublicfiles.com/densepose/cse/densepose_rcnn_R_50_FPN_s1x/251155172/model_final_c4ea5f.pkl \\\nimage.jpg dp_cse_texture,bbox  --texture_atlases_map '{\"smpl_27554\": \"smpl_uvSnapshot_colors.jpg\"}' -v\n```\n\n----------------------------------------\n\nTITLE: Querying DensePose Dataset by Filename in Bash\nDESCRIPTION: Example command to output all entries with a specific file_name from the densepose_coco_2014_train dataset. It demonstrates how to use an exact match selector for the file_name field.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py print densepose_coco_2014_train file_name=COCO_train2014_000000000036.jpg -v\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CUDA 10.2 and PyTorch 1.8\nDESCRIPTION: Command to install pre-built Detectron2 package for CUDA 10.2 with PyTorch 1.8 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.8/index.html\n```\n\n----------------------------------------\n\nTITLE: Creating Cityscapes Label Training IDs\nDESCRIPTION: Command to generate labelTrainIds.png files for Cityscapes dataset using cityscapesscripts.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nCITYSCAPES_DATASET=/path/to/abovementioned/cityscapes python cityscapesscripts/preparation/createTrainIdLabelImgs.py\n```\n\n----------------------------------------\n\nTITLE: Dumping DensePose Results for Specific Image Pattern (Bash)\nDESCRIPTION: Example command to dump results of the R_50_FPN_s1x DensePose model for images matching a specific filename pattern to a pickle file.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py dump configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\nhttps://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\n\"image*.jpg\" --output results.pkl -v\n```\n\n----------------------------------------\n\nTITLE: Visualizing Bounding Box and Segmentation in DensePose Dataset in Bash\nDESCRIPTION: Example command to show bounding box and segmentation visualization for a specific image (id=322) from the densepose_coco_2014_train dataset. It demonstrates how to use multiple visualization specifiers.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py show densepose_coco_2014_train image_id:int=322 bbox,dp_segm -v\n```\n\n----------------------------------------\n\nTITLE: Evaluating Cascade Mask R-CNN Models in Detectron2\nDESCRIPTION: Command to evaluate trained Cascade Mask R-CNN models using the lazyconfig_train_net.py tool in Detectron2. This runs evaluation only on a trained model checkpoint without performing training.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/ViTDet/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n../../tools/lazyconfig_train_net.py --config-file configs/path/to/config.py --eval-only train.init_checkpoint=/path/to/model_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Running DensePose Inference with Apply Net Tool\nDESCRIPTION: This command demonstrates how to use the Apply Net tool to visualize DensePose results on an image using contour visualization and bounding boxes.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/GETTING_STARTED.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml densepose_rcnn_R_50_FPN_s1x.pkl image.jpg dp_contour,bbox --output image_densepose_contour.png\n```\n\n----------------------------------------\n\nTITLE: Table Generation Command for Deformable Conv and Cascade R-CNN Ablations\nDESCRIPTION: HTML table generation command for comparing baseline R50-FPN models with Deformable Convolution and Cascade R-CNN variants using different training schedules (1x and 3x).\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_8\n\nLANGUAGE: html\nCODE:\n```\n<!--\n./gen_html_table.py --config 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml' 'Misc/*R_50_FPN_1x_dconv*' 'Misc/cascade*1x.yaml' 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml' 'Misc/*R_50_FPN_3x_dconv*' 'Misc/cascade*3x.yaml' --name \"Baseline R50-FPN\" \"Deformable Conv\" \"Cascade R-CNN\" \"Baseline R50-FPN\" \"Deformable Conv\" \"Cascade R-CNN\"  --fields lr_sched train_speed inference_speed mem box_AP mask_AP\n-->\n```\n\n----------------------------------------\n\nTITLE: Visualizing Bounding Box and U-Coordinate Colored Points in DensePose Dataset in Bash\nDESCRIPTION: Example command to show bounding box and points colored by U coordinate in part parameterization for a specific image (id=322) from the densepose_coco_2014_train dataset. It illustrates the use of the dp_u visualization specifier.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py show densepose_coco_2014_train image_id:int=322 bbox,dp_u -v\n```\n\n----------------------------------------\n\nTITLE: Visualizing Bounding Box and Green Points in DensePose Dataset in Bash\nDESCRIPTION: Example command to show bounding box and annotated points in green color for a specific image (id=322) from the densepose_coco_2014_train dataset. It demonstrates the use of the dp_pts visualization specifier.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py show densepose_coco_2014_train image_id:int=322 bbox,dp_segm -v\n```\n\n----------------------------------------\n\nTITLE: Training DensePose-RCNN on Single GPU\nDESCRIPTION: This command demonstrates how to train DensePose-RCNN on a single GPU by applying the linear learning rate scaling rule.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/GETTING_STARTED.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython train_net.py --config-file configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\n    SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Detectron2 Model Performance Metrics\nDESCRIPTION: An HTML table displaying performance metrics for large Detectron2 models. The table includes columns for model name, inference time, memory usage, accuracy measurements (box AP, mask AP, PQ), model ID, and download links for models and metrics.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_10\n\nLANGUAGE: html\nCODE:\n```\n<table><tbody>\n<!-- START TABLE -->\n<!-- TABLE HEADER -->\n<th valign=\"bottom\">Name</th>\n<th valign=\"bottom\">inference<br/>time<br/>(s/im)</th>\n<th valign=\"bottom\">train<br/>mem<br/>(GB)</th>\n<th valign=\"bottom\">box<br/>AP</th>\n<th valign=\"bottom\">mask<br/>AP</th>\n<th valign=\"bottom\">PQ</th>\n<th valign=\"bottom\">model id</th>\n<th valign=\"bottom\">download</th>\n<!-- TABLE BODY -->\n<!-- ROW: panoptic_fpn_R_101_dconv_cascade_gn_3x -->\n <tr><td align=\"left\"><a href=\"configs/Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x.yaml\">Panoptic FPN R101</a></td>\n<td align=\"center\">0.098</td>\n<td align=\"center\">11.4</td>\n<td align=\"center\">47.4</td>\n<td align=\"center\">41.3</td>\n<td align=\"center\">46.1</td>\n<td align=\"center\">139797668</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x/139797668/model_final_be35db.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x/139797668/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv -->\n <tr><td align=\"left\"><a href=\"configs/Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\">Mask R-CNN X152</a></td>\n<td align=\"center\">0.234</td>\n<td align=\"center\">15.1</td>\n<td align=\"center\">50.2</td>\n<td align=\"center\">44.0</td>\n<td align=\"center\"></td>\n<td align=\"center\">18131413</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv/18131413/model_0039999_e76410.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv/18131413/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: TTA cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv -->\n <tr><td align=\"left\">above + test-time aug.</td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\">51.9</td>\n<td align=\"center\">45.9</td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: Configuring DensePose R-101-FPN Model with DeepLabV3 Head in YAML\nDESCRIPTION: YAML configuration for a DensePose R-101-FPN model using the improved schedule, Panoptic FPN, and DeepLabV3 head. This defines the advanced model architecture with a deeper backbone and enhanced segmentation.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndensepose_rcnn_R_101_FPN_DL_s1x\n```\n\n----------------------------------------\n\nTITLE: LVIS Dataset Structure\nDESCRIPTION: Defines the directory structure for LVIS instance segmentation dataset including JSON annotations and image directories.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\ncoco/\n  {train,val,test}2017/\nlvis/\n  lvis_v0.5_{train,val}.json\n  lvis_v0.5_image_info_test.json\n  lvis_v1_{train,val}.json\n  lvis_v1_image_info_test{,_challenge}.json\n```\n\n----------------------------------------\n\nTITLE: Visualizing Ground Truth Annotations or Training Data\nDESCRIPTION: Command to visualize ground truth raw annotations or training data after preprocessing or augmentations in Detectron2.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/tools/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython visualize_data.py --config-file config.yaml --source annotation/dataloader --output-dir dir/ [--show]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Bounding Boxes and Segmentation with DensePose\nDESCRIPTION: Command to display bounding boxes and segmentation masks for detected persons using a DensePose model with ResNet-50 FPN backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\nhttps://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\nimage.jpg bbox,dp_segm -v\n```\n\n----------------------------------------\n\nTITLE: Documenting the detectron2.model_zoo module using reStructuredText\nDESCRIPTION: This reStructuredText code configures the documentation for the detectron2.model_zoo module. It uses the automodule directive to automatically generate documentation from the module's docstrings, including all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/model_zoo.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: detectron2.model_zoo\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Configuring DensePose R-50-FPN Model with Legacy Schedule in YAML\nDESCRIPTION: YAML configuration for a DensePose R-50-FPN model using the legacy training schedule. This defines the model architecture, training parameters, and evaluation metrics.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndensepose_rcnn_R_50_FPN_s1x_legacy\n```\n\n----------------------------------------\n\nTITLE: Pascal VOC Dataset Structure\nDESCRIPTION: Defines the expected directory structure for Pascal VOC dataset including annotations, image sets, and JPEG images.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nVOC20{07,12}/\n  Annotations/\n  ImageSets/\n    Main/\n      trainval.txt\n      test.txt\n      # train.txt or val.txt, if you use these splits\n  JPEGImages/\n```\n\n----------------------------------------\n\nTITLE: Visualizing Bounding Box and Colored Points in DensePose Dataset in Bash\nDESCRIPTION: Example command to show bounding box and points colored by containing part for a specific image (id=322) from the densepose_coco_2014_train dataset. It illustrates the use of the dp_i visualization specifier.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py show densepose_coco_2014_train image_id:int=322 bbox,dp_i -v\n```\n\n----------------------------------------\n\nTITLE: Including Detectron2 Default Configuration in Python\nDESCRIPTION: This code snippet includes the default configuration options from the defaults.py file in the Detectron2 project. It uses a literalinclude directive to include the file contents.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/config.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../detectron2/config/defaults.py\n  :language: python\n  :linenos:\n  :lines: 7-\n```\n\n----------------------------------------\n\nTITLE: Configuring Improved DensePose R-101-FPN Model in YAML\nDESCRIPTION: YAML configuration for an improved DensePose R-101-FPN model with updated training schedule and Panoptic FPN head. This defines the enhanced model architecture with a deeper backbone.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndensepose_rcnn_R_101_FPN_s1x\n```\n\n----------------------------------------\n\nTITLE: Installing PanopticAPI for COCO PanopticFPN\nDESCRIPTION: Installs the PanopticAPI library required for working with COCO panoptic segmentation annotations.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/cocodataset/panopticapi.git\n```\n\n----------------------------------------\n\nTITLE: Preparing ADE20k Semantic Segmentation Annotations\nDESCRIPTION: Runs a Python script to generate Detectron2-compatible annotations for the ADE20k Scene Parsing dataset.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython datasets/prepare_ade20k_sem_seg.py\n```\n\n----------------------------------------\n\nTITLE: Sample Model Configuration Path YAML Reference\nDESCRIPTION: YAML configuration file paths for different DensePose CSE model architectures, referenced in the model zoo tables.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_CSE.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n../configs/cse/densepose_rcnn_R_50_FPN_s1x.yaml\n../configs/cse/densepose_rcnn_R_101_FPN_s1x.yaml\n../configs/cse/densepose_rcnn_R_50_FPN_DL_s1x.yaml\n../configs/cse/densepose_rcnn_R_101_FPN_DL_s1x.yaml\n../configs/cse/densepose_rcnn_R_50_FPN_soft_s1x.yaml\n../configs/cse/densepose_rcnn_R_101_FPN_soft_s1x.yaml\n../configs/cse/densepose_rcnn_R_50_FPN_DL_soft_s1x.yaml\n../configs/cse/densepose_rcnn_R_101_FPN_DL_soft_s1x.yaml\n```\n\n----------------------------------------\n\nTITLE: Generating Detectron2 Solver Module Documentation using Sphinx\nDESCRIPTION: This code snippet uses Sphinx's automodule directive to automatically generate documentation for the detectron2.solver module. It includes all members, undocumented members, and shows inheritance relationships.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/solver.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: detectron2.solver\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: DensePose JSON Annotation Structure\nDESCRIPTION: Basic JSON structure for DensePose annotations following COCO data format. Includes definitions for info, images, annotations, and licenses.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_DATASETS.md#2025-04-21_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"info\": info,\n    \"images\": [image],\n    \"annotations\": [annotation],\n    \"licenses\": [license],\n}\n\ninfo{\n    \"year\": int,\n    \"version\": str,\n    \"description\": str,\n    \"contributor\": str,\n    \"url\": str,\n    \"date_created\": datetime,\n}\n\nimage{\n    \"id\": int,\n    \"width\": int,\n    \"height\": int,\n    \"file_name\": str,\n    \"license\": int,\n    \"flickr_url\": str,\n    \"coco_url\": str,\n    \"date_captured\": datetime,\n}\n\nlicense{\n    \"id\": int, \"name\": str, \"url\": str,\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Cityscapes Panoptic Dataset\nDESCRIPTION: Runs a Cityscapes script to generate the panoptic dataset, which is required for panoptic segmentation tasks.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nCITYSCAPES_DATASET=/path/to/abovementioned/cityscapes python cityscapesscripts/preparation/createPanopticImgs.py\n```\n\n----------------------------------------\n\nTITLE: Configuring DensePose R-50-FPN Model with DeepLabV3 Head in YAML\nDESCRIPTION: YAML configuration for a DensePose R-50-FPN model using the improved schedule, Panoptic FPN, and DeepLabV3 head. This defines the advanced model architecture with enhanced segmentation capabilities.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndensepose_rcnn_R_50_FPN_DL_s1x\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CUDA 11.1 and PyTorch 1.8\nDESCRIPTION: Command to install a pre-built Detectron2 package for systems with CUDA 11.1 and PyTorch 1.8 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.8/index.html\n```\n\n----------------------------------------\n\nTITLE: Verifying CUDA Installation for Detectron2\nDESCRIPTION: Python command to check CUDA availability and CUDA_HOME path to verify proper CUDA setup for Detectron2 installation. This helps diagnose GPU support issues.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npython -c 'import torch; from torch.utils.cpp_extension import CUDA_HOME; print(torch.cuda.is_available(), CUDA_HOME)'\n```\n\n----------------------------------------\n\nTITLE: Installing Cityscapes Scripts\nDESCRIPTION: Command to install Cityscapes scripts package for data preparation and processing.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/mcordts/cityscapesScripts.git\n```\n\n----------------------------------------\n\nTITLE: DensePose R-50 FPN Configuration References\nDESCRIPTION: YAML configuration file references for DensePose R-50 FPN models with different confidence estimation approaches (WC1M, WC2M) and learning schedules.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n../configs/densepose_rcnn_R_50_FPN_WC1M_s1x.yaml\n../configs/densepose_rcnn_R_50_FPN_WC2M_s1x.yaml\n../configs/densepose_rcnn_R_50_FPN_DL_WC1M_s1x.yaml\n../configs/densepose_rcnn_R_50_FPN_DL_WC2M_s1x.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CUDA 11.1 and PyTorch 1.10\nDESCRIPTION: Command to install pre-built Detectron2 package for CUDA 11.1 with PyTorch 1.10 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CUDA 10.2 and PyTorch 1.9\nDESCRIPTION: Command to install a pre-built Detectron2 package for systems with CUDA 10.2 and PyTorch 1.9 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n```\n\n----------------------------------------\n\nTITLE: DensePose Features List in Markdown\nDESCRIPTION: A markdown list outlining the key improvements and features added to DensePose models including confidence estimation, new architectures, and evaluation metrics.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/RELEASE_2020_04.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# DensePose Confidence Estimation and Model Zoo Improvements\n\n* [DensePose models with confidence estimation](doc/DENSEPOSE_IUV.md#ModelZooConfidence)\n* [Panoptic FPN and DeepLabV3 head implementation](doc/DENSEPOSE_IUV.md#ModelZooDeepLabV3)\n* Test time augmentations for DensePose\n* New evaluation metric (GPSm) that yields more reliable scores\n```\n\n----------------------------------------\n\nTITLE: Configuring Category Whitelisting in DensePose Master Model Training\nDESCRIPTION: YAML configuration for whitelisting specific categories during master model training. This filters the training data to include only humans and selected animal classes.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/BOOTSTRAPPING_PIPELINE.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  WHITELISTED_CATEGORIES:\n    \"base_coco_2017_train\":\n      - 1  # person\n      - 16 # bird\n      - 17 # cat\n      - 18 # dog\n      - 19 # horse\n      - 20 # sheep\n      - 21 # cow\n      - 22 # elephant\n      - 23 # bear\n      - 24 # zebra\n      - 25 # girafe\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CUDA 10.2 and PyTorch 1.8\nDESCRIPTION: Command to install a pre-built Detectron2 package for systems with CUDA 10.2 and PyTorch 1.8 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.8/index.html\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CUDA 10.1 and PyTorch 1.8\nDESCRIPTION: Command to install pre-built Detectron2 package for CUDA 10.1 with PyTorch 1.8 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n```\n\n----------------------------------------\n\nTITLE: Citation - DensePose Confidence Estimation\nDESCRIPTION: BibTeX citation for the DensePose confidence estimation paper by Neverova et al., published in NeurIPS 2019.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_9\n\nLANGUAGE: bibtex\nCODE:\n```\n@InProceedings{Neverova2019DensePoseConfidences,\n    title = {Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels},\n    author = {Neverova, Natalia and Novotny, David and Vedaldi, Andrea},\n    journal = {Advances in Neural Information Processing Systems},\n    year = {2019},\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Class-Agnostic Training with Category Maps\nDESCRIPTION: YAML configuration that maps all animal categories to the person class, enabling class-agnostic training where all instances are treated as the same class.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/BOOTSTRAPPING_PIPELINE.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  CATEGORY_MAPS:\n    \"base_coco_2017_train\":\n      \"16\": 1 # bird -> person\n      \"17\": 1 # cat -> person\n      \"18\": 1 # dog -> person\n      \"19\": 1 # horse -> person\n      \"20\": 1 # sheep -> person\n      \"21\": 1 # cow -> person\n      \"22\": 1 # elephant -> person\n      \"23\": 1 # bear -> person\n      \"24\": 1 # zebra -> person\n      \"25\": 1 # girafe -> person\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CUDA 10.1 and PyTorch 1.8\nDESCRIPTION: Command to install a pre-built Detectron2 package for systems with CUDA 10.1 and PyTorch 1.8 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Architecture List for Compilation\nDESCRIPTION: Environment variable setting to specify CUDA architecture flags when building Detectron2 for specific GPU architectures like P100s and V100s.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport TORCH_CUDA_ARCH_LIST=\"6.0;7.0\"\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for ViTDet\nDESCRIPTION: BibTeX entry for citing the ViTDet paper when using ViTDet models. This references the paper 'Exploring plain vision transformer backbones for object detection' by Li et al.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/ViTDet/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{li2022exploring,\n  title={Exploring plain vision transformer backbones for object detection},\n  author={Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},\n  journal={arXiv preprint arXiv:2203.16527},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Dumping DensePose Results for Image Folder (Bash)\nDESCRIPTION: Example command to dump results of the R_50_FPN_s1x DensePose model for images in a folder to a pickle file. It uses a specific configuration and model file from a URL.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_APPLY_NET.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython apply_net.py dump configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\nhttps://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\nimages --output dump.pkl -v\n```\n\n----------------------------------------\n\nTITLE: Configuring Bootstrap Datasets for Student Model Training\nDESCRIPTION: YAML configuration for bootstrapping datasets in student model training. This setup specifies how to load images from the target domain, apply the master model, filter outputs, and sample annotations.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/BOOTSTRAPPING_PIPELINE.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nBOOTSTRAP_DATASETS:\n  - DATASET: \"chimpnsee\"\n    RATIO: 1.0\n    IMAGE_LOADER:\n      TYPE: \"video_keyframe\"\n      SELECT:\n        STRATEGY: \"random_k\"\n        NUM_IMAGES: 4\n      TRANSFORM:\n        TYPE: \"resize\"\n        MIN_SIZE: 800\n        MAX_SIZE: 1333\n      BATCH_SIZE: 8\n      NUM_WORKERS: 1\n    INFERENCE:\n      INPUT_BATCH_SIZE: 1\n      OUTPUT_BATCH_SIZE: 1\n    DATA_SAMPLER:\n      # supported types:\n      #   densepose_uniform\n      #   densepose_UV_confidence\n      #   densepose_fine_segm_confidence\n      #   densepose_coarse_segm_confidence\n      TYPE: \"densepose_uniform\"\n      COUNT_PER_CLASS: 8\n    FILTER:\n      TYPE: \"detection_score\"\n      MIN_VALUE: 0.8\nBOOTSTRAP_MODEL:\n  WEIGHTS: https://dl.fbaipublicfiles.com/densepose/evolution/densepose_R_50_FPN_DL_WC1M_3x_Atop10P_CA/217578784/model_final_9fe1cc.pkl\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CPU-only and PyTorch 1.8\nDESCRIPTION: Command to install a pre-built CPU-only Detectron2 package with PyTorch 1.8 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.8/index.html\n```\n\n----------------------------------------\n\nTITLE: Running Detectron2 with Docker Compose\nDESCRIPTION: Command to run Detectron2 using docker-compose, which simplifies container management. Requires docker-compose version 1.28.0 or newer and nvidia-docker-toolkit.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docker/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docker && USER_ID=$UID docker-compose run detectron2\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Panoptic-DeepLab Network Speed\nDESCRIPTION: This command benchmarks the network speed of a Panoptic-DeepLab model without post-processing. It uses a specific configuration flag to enable speed benchmarking.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/Panoptic-DeepLab/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/detectron2/projects/Panoptic-DeepLab\npython train_net.py --config-file configs/Cityscapes-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_90k_bs32_crop_512_1024_dsconv.yaml --eval-only MODEL.WEIGHTS /path/to/model_checkpoint MODEL.PANOPTIC_DEEPLAB.BENCHMARK_NETWORK_SPEED True\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Table for Detectron2 Model Configurations\nDESCRIPTION: This YAML-like command generates an HTML table comparing different Detectron2 model configurations for Cityscapes and PASCAL VOC datasets. It specifies the configs to use, names for each row, and which fields to include in the table.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n./gen_html_table.py --config 'Cityscapes/*' 'PascalVOC-Detection/*' --name \"R50-FPN, Cityscapes\" \"R50-C4, VOC\" --fields train_speed inference_speed mem box_AP box_AP50 mask_AP\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CPU and PyTorch 1.9\nDESCRIPTION: Command to install pre-built Detectron2 package for CPU-only with PyTorch 1.9 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.9/index.html\n```\n\n----------------------------------------\n\nTITLE: Installing Cityscapes Scripts\nDESCRIPTION: Installs the Cityscapes scripts library required for working with the Cityscapes dataset.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/mcordts/cityscapesScripts.git\n```\n\n----------------------------------------\n\nTITLE: Adding New Dependencies to Detectron2 Docker Container\nDESCRIPTION: Example Dockerfile snippet showing how to install additional dependencies (vim in this example) in the Detectron2 container for persistent changes.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docker/README.md#2025-04-21_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nRUN sudo apt-get update && sudo apt-get install -y vim\n```\n\n----------------------------------------\n\nTITLE: Table Generation Command for Normalization Methods and Scratch Training\nDESCRIPTION: HTML table generation command for comparing different normalization methods (GN, SyncBN) and models trained from scratch following the 'Rethinking ImageNet Pre-training' methodology with various training schedules.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_9\n\nLANGUAGE: html\nCODE:\n```\n<!--\n./gen_html_table.py --config 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml' 'Misc/mask*50_FPN_3x_gn.yaml' 'Misc/mask*50_FPN_3x_syncbn.yaml' 'Misc/scratch*' --name \"Baseline R50-FPN\" \"GN\" \"SyncBN\" \"GN (from scratch)\" \"GN (from scratch)\" \"SyncBN (from scratch)\" --fields lr_sched train_speed inference_speed mem box_AP mask_AP\n   -->\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CPU and PyTorch 1.8\nDESCRIPTION: Command to install pre-built Detectron2 package for CPU-only with PyTorch 1.8 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.8/index.html\n```\n\n----------------------------------------\n\nTITLE: Model Performance Table - Mask R-CNN COCO\nDESCRIPTION: Markdown table showing performance metrics for Mask R-CNN models with different Vision Transformer backbones (ViT-B, ViT-L, ViT-H) on COCO dataset, including training time, inference time, memory usage, and AP scores.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/ViTDet/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | pre-train | train time (s/im) | inference time (s/im) | train mem (GB) | box AP | mask AP | model id | download |\n|------|-----------|-----------------|-------------------|---------------|---------|----------|-----------|----------|\n| ViTDet, ViT-B | IN1K, MAE | 0.314 | 0.079 | 10.9 | 51.6 | 45.9 | 325346929 | model |\n| ViTDet, ViT-L | IN1K, MAE | 0.603 | 0.125 | 20.9 | 55.5 | 49.2 | 325599698 | model |\n| ViTDet, ViT-H | IN1K, MAE | 1.098 | 0.178 | 31.5 | 56.7 | 50.2 | 329145471 | model |\n```\n\n----------------------------------------\n\nTITLE: Building Detectron2 Documentation with Make\nDESCRIPTION: Command to build the Detectron2 HTML documentation after installing the required dependencies. This should be run from the documentation directory.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Executing Visualization Mode Query for DensePose Dataset in Bash\nDESCRIPTION: Command structure for using the show mode of query_db.py to visualize dataset entries. It includes optional arguments for verbosity, limiting entries, and specifying output file, as well as mandatory arguments for dataset, selector, and visualizations.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py show [-h] [-v] [--max-entries N] [--output <image_file>] <dataset> <selector> <visualizations>\n```\n\n----------------------------------------\n\nTITLE: Installing LVIS API\nDESCRIPTION: Command to install the LVIS API package required for working with LVIS annotations.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/lvis-dataset/lvis-api.git\n```\n\n----------------------------------------\n\nTITLE: Checking CUDA Availability in Python\nDESCRIPTION: Python command to verify CUDA availability and CUDA_HOME path to diagnose GPU support issues during Detectron2 installation.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npython -c 'import torch; from torch.utils.cpp_extension import CUDA_HOME; print(torch.cuda.is_available(), CUDA_HOME)'\n```\n\n----------------------------------------\n\nTITLE: Model Performance Table - Cascade Mask R-CNN COCO\nDESCRIPTION: Markdown table showing performance metrics for Cascade Mask R-CNN models with Swin, MViTv2, and ViTDet backbones on COCO dataset, including training time, inference time, memory usage, and AP scores.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/ViTDet/README.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | pre-train | train time (s/im) | inference time (s/im) | train mem (GB) | box AP | mask AP | model id | download |\n|------|-----------|-----------------|-------------------|---------------|---------|----------|-----------|----------|\n| Swin-B | IN21K, sup | 0.389 | 0.077 | 8.7 | 53.9 | 46.2 | 342979038 | model |\n| Swin-L | IN21K, sup | 0.508 | 0.097 | 12.6 | 55.0 | 47.2 | 342979186 | model |\n| MViTv2-B | IN21K, sup | 0.475 | 0.090 | 8.9 | 55.6 | 48.1 | 325820315 | model |\n```\n\n----------------------------------------\n\nTITLE: Running Detectron2 Unit Tests via Python unittest\nDESCRIPTION: Command sequence to execute all unit tests in the Detectron2 project using Python's unittest framework. This runs tests from the ./tests directory in verbose mode.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/tests/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd detectron2\npython -m unittest discover -v -s ./tests\n```\n\n----------------------------------------\n\nTITLE: Building and Running Detectron2 Deployment Container for C++ Examples\nDESCRIPTION: Commands for building and running a specialized deployment container for testing C++ examples in Detectron2. This container is built after the base Detectron2 container.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docker/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Build:\ndocker build -t detectron2-deploy:v0 -f deploy.Dockerfile .\n# Launch:\ndocker run --gpus all -it detectron2-deploy:v0\n```\n\n----------------------------------------\n\nTITLE: Generating Cityscapes Panoptic Dataset\nDESCRIPTION: Command to create panoptic segmentation files for Cityscapes dataset.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nCITYSCAPES_DATASET=/path/to/abovementioned/cityscapes python cityscapesscripts/preparation/createPanopticImgs.py\n```\n\n----------------------------------------\n\nTITLE: Printing Limited Entries from DensePose Dataset in Bash\nDESCRIPTION: Example command to output at most 10 first entries from the densepose_coco_2014_train dataset. It uses the wildcard selector to match all entries and limits the output to 10 entries with verbose mode.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/TOOL_QUERY_DB.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython query_db.py print densepose_coco_2014_train \\* --max-entries 10 -v\n```\n\n----------------------------------------\n\nTITLE: HTML Table for Mask R-CNN Model Performance Comparison\nDESCRIPTION: A detailed HTML table that presents performance metrics for different Mask R-CNN configurations on the COCO dataset. The table includes training schedules, speeds, memory usage, and accuracy metrics (box AP and mask AP) for various model configurations.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<table><tbody>\n<!-- START TABLE -->\n<!-- TABLE HEADER -->\n<th valign=\"bottom\">Name</th>\n<th valign=\"bottom\">lr<br/>sched</th>\n<th valign=\"bottom\">train<br/>time<br/>(s/iter)</th>\n<th valign=\"bottom\">inference<br/>time<br/>(s/im)</th>\n<th valign=\"bottom\">train<br/>mem<br/>(GB)</th>\n<th valign=\"bottom\">box<br/>AP</th>\n<th valign=\"bottom\">mask<br/>AP</th>\n<th valign=\"bottom\">model id</th>\n<th valign=\"bottom\">download</th>\n<!-- TABLE BODY -->\n<!-- ROW: mask_rcnn_R_50_C4_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml\">R50-C4</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.584</td>\n<td align=\"center\">0.110</td>\n<td align=\"center\">5.2</td>\n<td align=\"center\">36.8</td>\n<td align=\"center\">32.2</td>\n<td align=\"center\">137259246</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x/137259246/model_final_9243eb.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x/137259246/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_50_DC5_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x.yaml\">R50-DC5</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.471</td>\n<td align=\"center\">0.076</td>\n<td align=\"center\">6.5</td>\n<td align=\"center\">38.3</td>\n<td align=\"center\">34.2</td>\n<td align=\"center\">137260150</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x/137260150/model_final_4f86c3.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x/137260150/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_50_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\">R50-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.261</td>\n<td align=\"center\">0.043</td>\n<td align=\"center\">3.4</td>\n<td align=\"center\">38.6</td>\n<td align=\"center\">35.2</td>\n<td align=\"center\">137260431</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x/137260431/model_final_a54504.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x/137260431/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_50_C4_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml\">R50-C4</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.575</td>\n<td align=\"center\">0.111</td>\n<td align=\"center\">5.2</td>\n<td align=\"center\">39.8</td>\n<td align=\"center\">34.4</td>\n<td align=\"center\">137849525</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x/137849525/model_final_4ce675.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x/137849525/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_50_DC5_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x.yaml\">R50-DC5</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.470</td>\n<td align=\"center\">0.076</td>\n<td align=\"center\">6.5</td>\n<td align=\"center\">40.0</td>\n<td align=\"center\">35.9</td>\n<td align=\"center\">137849551</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x/137849551/model_final_84107b.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x/137849551/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_50_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\">R50-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.261</td>\n<td align=\"center\">0.043</td>\n<td align=\"center\">3.4</td>\n<td align=\"center\">41.0</td>\n<td align=\"center\">37.2</td>\n<td align=\"center\">137849600</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_101_C4_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\">R101-C4</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.652</td>\n<td align=\"center\">0.145</td>\n<td align=\"center\">6.3</td>\n<td align=\"center\">42.6</td>\n<td align=\"center\">36.7</td>\n<td align=\"center\">138363239</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x/138363239/model_final_a2914c.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x/138363239/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_101_DC5_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x.yaml\">R101-DC5</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.545</td>\n<td align=\"center\">0.092</td>\n<td align=\"center\">7.6</td>\n<td align=\"center\">41.9</td>\n<td align=\"center\">37.3</td>\n<td align=\"center\">138363294</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x/138363294/model_final_0464b7.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x/138363294/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_101_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\">R101-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.340</td>\n<td align=\"center\">0.056</td>\n<td align=\"center\">4.6</td>\n<td align=\"center\">42.9</td>\n<td align=\"center\">38.6</td>\n<td align=\"center\">138205316</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_X_101_32x8d_FPN_3x -->\n <tr><td align=\"left\"><a href=\"configs/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\">X101-FPN</a></td>\n<td align=\"center\">3x</td>\n<td align=\"center\">0.690</td>\n<td align=\"center\">0.103</td>\n<td align=\"center\">7.2</td>\n<td align=\"center\">44.3</td>\n<td align=\"center\">39.5</td>\n<td align=\"center\">139653917</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x/139653917/model_final_2d9806.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x/139653917/metrics.json\">metrics</a></td>\n</tr>\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: HTML Comment for Table Generation Command\nDESCRIPTION: An HTML comment containing a Python script command used to generate the HTML table. The script selects specific configuration files and defines column names for the performance metrics table.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_11\n\nLANGUAGE: html\nCODE:\n```\n<!--\n./gen_html_table.py --config 'Misc/panoptic_*dconv*' 'Misc/cascade_*152*' --name \"Panoptic FPN R101\" \"Mask R-CNN X152\" --fields inference_speed mem box_AP mask_AP PQ\n# manually add TTA results\n-->\n```\n\n----------------------------------------\n\nTITLE: Creating Cityscapes Train ID Label Images\nDESCRIPTION: Runs a Cityscapes script to create labelTrainIds.png files, which are required for certain Cityscapes tasks.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nCITYSCAPES_DATASET=/path/to/abovementioned/cityscapes python cityscapesscripts/preparation/createTrainIdLabelImgs.py\n```\n\n----------------------------------------\n\nTITLE: Installing LVIS API\nDESCRIPTION: Installs the LVIS API library required for working with the LVIS dataset.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/builtin_datasets.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/lvis-dataset/lvis-api.git\n```\n\n----------------------------------------\n\nTITLE: Cityscapes Dataset Structure\nDESCRIPTION: Shows the complete directory structure for Cityscapes dataset including fine annotations, images, and panoptic segmentation files.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\ncityscapes/\n  gtFine/\n    train/\n      aachen/\n        color.png, instanceIds.png, labelIds.png, polygons.json,\n        labelTrainIds.png\n      ...\n    val/\n    test/\n    # below are generated Cityscapes panoptic annotation\n    cityscapes_panoptic_train.json\n    cityscapes_panoptic_train/\n    cityscapes_panoptic_val.json\n    cityscapes_panoptic_val/\n    cityscapes_panoptic_test.json\n    cityscapes_panoptic_test/\n  leftImg8bit/\n    train/\n    val/\n    test/\n```\n\n----------------------------------------\n\nTITLE: Generating Detectron2 Export Module Documentation\nDESCRIPTION: This snippet uses Sphinx's automodule directive to automatically generate documentation for the detectron2.export module. It includes all members, undocumented members, and shows inheritance information.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/export.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: detectron2.export\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for TorchScript Mask R-CNN with PyTorch/OpenCV Dependencies\nDESCRIPTION: This CMake configuration defines a C++ executable project for Mask R-CNN using TorchScript. It sets up the required dependencies including PyTorch, OpenCV, and TorchVision (needed for tracing/scripting export methods), and configures linking options and C++ standard (C++14) for the build.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/tools/deploy/CMakeLists.txt#2025-04-21_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# Copyright (c) Facebook, Inc. and its affiliates.\n# See https://pytorch.org/tutorials/advanced/cpp_frontend.html\ncmake_minimum_required(VERSION 3.12 FATAL_ERROR)\nproject(torchscript_mask_rcnn)\n\nfind_package(Torch REQUIRED)\nfind_package(OpenCV REQUIRED)\nfind_package(TorchVision REQUIRED)   # needed by export-method=tracing/scripting\n\nadd_executable(torchscript_mask_rcnn torchscript_mask_rcnn.cpp)\ntarget_link_libraries(\n  torchscript_mask_rcnn\n  -Wl,--no-as-needed TorchVision::TorchVision -Wl,--as-needed\n  \"${TORCH_LIBRARIES}\" ${OpenCV_LIBS})\nset_property(TARGET torchscript_mask_rcnn PROPERTY CXX_STANDARD 14)\n```\n\n----------------------------------------\n\nTITLE: Installing PanopticAPI\nDESCRIPTION: Command to install the PanopticAPI package required for processing panoptic annotations.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/cocodataset/panopticapi.git\n```\n\n----------------------------------------\n\nTITLE: ADE20k Scene Parsing Dataset Structure\nDESCRIPTION: Shows the directory structure for ADE20k Scene Parsing dataset including annotations and images.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/datasets/README.md#2025-04-21_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nADEChallengeData2016/\n  annotations/\n  annotations_detectron2/\n  images/\n  objectInfo150.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring autodoc for detectron2.structures Module in reStructuredText\nDESCRIPTION: This code snippet configures the autodoc extension to generate documentation for the detectron2.structures module. It specifies that all members, undocumented members, and inheritance relationships should be included in the generated documentation.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/structures.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: detectron2.structures\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Running Linter for Detectron2 in Bash\nDESCRIPTION: This command runs the linter script for Detectron2 to ensure code quality and adherence to style guidelines.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/notes/contributing.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dev/linter.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents Structure in reStructuredText for Detectron2 Documentation\nDESCRIPTION: This code snippet defines a table of contents (toctree) directive in reStructuredText format. It sets a maximum depth of 2 for the navigation hierarchy and includes four documentation pages: benchmarks, compatibility, contributing, and changelog.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/notes/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   benchmarks\n   compatibility\n   contributing\n   changelog\n```\n\n----------------------------------------\n\nTITLE: Importing Detectron2 Box Regression Module\nDESCRIPTION: Imports the detectron2.modeling.box_regression module and its members. This module likely contains functions and classes for performing bounding box regression in object detection tasks.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/modeling.rst#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: detectron2.modeling.box_regression\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: Sphinx documentation configuration showing the structure and organization of Detectron2's utility modules. Each module is documented with members, undocumented members, and inheritance information.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/utils.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: detectron2.utils.colormap\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: detectron2.utils.comm\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: detectron2.utils.events\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: detectron2.utils.logger\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: detectron2.utils.registry\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: detectron2.utils.memory\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: detectron2.utils.analysis\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: detectron2.utils.visualizer\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: detectron2.utils.video_visualizer\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Importing Detectron2 Poolers Module\nDESCRIPTION: Imports the detectron2.modeling.poolers module and its members. This module likely contains implementations of various pooling operations used in object detection models.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/modeling.rst#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: detectron2.modeling.poolers\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Importing Detectron2 Config Module in Python\nDESCRIPTION: This code snippet demonstrates how to import and use the detectron2.config module. It includes automodule directives for generating documentation.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/config.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. automodule:: detectron2.config\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements for Detectron2\nDESCRIPTION: Comprehensive list of Python package dependencies required for Detectron2, including documentation generators (Sphinx, docutils), deep learning frameworks (PyTorch, torchvision), and various utility packages. Some packages have specific version requirements, while others are installed from direct URLs or Git repositories.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\ndocutils==0.16\n# https://github.com/sphinx-doc/sphinx/commit/7acd3ada3f38076af7b2b5c9f3b60bb9c2587a3d\nsphinx==3.2.0\nrecommonmark==0.6.0\nsphinx_rtd_theme\n# Dependencies here are only those required by import\ntermcolor\nnumpy\ntqdm\nmatplotlib\ntermcolor\nyacs\ntabulate\ncloudpickle\nPillow\nfuture\ngit+https://github.com/facebookresearch/fvcore.git\nhttps://download.pytorch.org/whl/cpu/torch-1.8.1%2Bcpu-cp37-cp37m-linux_x86_64.whl\nhttps://download.pytorch.org/whl/cpu/torchvision-0.9.1%2Bcpu-cp37-cp37m-linux_x86_64.whl\nomegaconf>=2.1.0.dev24\nhydra-core>=1.1.0.dev5\nscipy\ntimm\n```\n\n----------------------------------------\n\nTITLE: HTML Table for LVIS Instance Segmentation Baselines\nDESCRIPTION: A table displaying benchmark results for Mask R-CNN models on LVIS Instance Segmentation task, including training schedules, speed metrics, memory usage, and accuracy metrics (box AP and mask AP).\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#2025-04-21_snippet_6\n\nLANGUAGE: html\nCODE:\n```\n<table><tbody>\n<!-- START TABLE -->\n<!-- TABLE HEADER -->\n<th valign=\"bottom\">Name</th>\n<th valign=\"bottom\">lr<br/>sched</th>\n<th valign=\"bottom\">train<br/>time<br/>(s/iter)</th>\n<th valign=\"bottom\">inference<br/>time<br/>(s/im)</th>\n<th valign=\"bottom\">train<br/>mem<br/>(GB)</th>\n<th valign=\"bottom\">box<br/>AP</th>\n<th valign=\"bottom\">mask<br/>AP</th>\n<th valign=\"bottom\">model id</th>\n<th valign=\"bottom\">download</th>\n<!-- TABLE BODY -->\n<!-- ROW: mask_rcnn_R_50_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\">R50-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.292</td>\n<td align=\"center\">0.107</td>\n<td align=\"center\">7.1</td>\n<td align=\"center\">23.6</td>\n<td align=\"center\">24.4</td>\n<td align=\"center\">144219072</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x/144219072/model_final_571f7c.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/LVISv0.5-InstanceSegmentation/mask_rcnn_R_50_FPN_1x/144219072/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_R_101_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml\">R101-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.371</td>\n<td align=\"center\">0.114</td>\n<td align=\"center\">7.8</td>\n<td align=\"center\">25.6</td>\n<td align=\"center\">25.9</td>\n<td align=\"center\">144219035</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x/144219035/model_final_824ab5.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/LVISv0.5-InstanceSegmentation/mask_rcnn_R_101_FPN_1x/144219035/metrics.json\">metrics</a></td>\n</tr>\n<!-- ROW: mask_rcnn_X_101_32x8d_FPN_1x -->\n <tr><td align=\"left\"><a href=\"configs/LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml\">X101-FPN</a></td>\n<td align=\"center\">1x</td>\n<td align=\"center\">0.712</td>\n<td align=\"center\">0.151</td>\n<td align=\"center\">10.2</td>\n<td align=\"center\">26.7</td>\n<td align=\"center\">27.1</td>\n<td align=\"center\">144219108</td>\n<td align=\"center\"><a href=\"https://dl.fbaipublicfiles.com/detectron2/LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x/144219108/model_final_5e3439.pkl\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/LVISv0.5-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x/144219108/metrics.json\">metrics</a></td>\n</tr>\n</tbody></table>\n```\n\n----------------------------------------\n\nTITLE: File Path References for DensePose Components\nDESCRIPTION: Collection of file path references to core DensePose components including data loaders, datasets, models, and visualization tools.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/RELEASE_2021_03.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* [`InferenceBasedLoader`](../densepose/data/inference_based_loader.py)\n* [`VideoKeyframeDataset`](../densepose/data/video/video_keyframe_dataset.py)\n* [Hard embedding](../densepose/modeling/losses/embed.py)\n* [Soft embedding](../densepose/modeling/losses/soft_embed.py)\n* [Embedder](../(densepose/modeling/cse/embedder.py)\n* [Storage](../densepose/evaluation/tensor_storage.py)\n* [Chart outputs](../densepose/structures/chart.py)\n* [Chart outputs with confidences](../densepose/structures/chart_confidence.py)\n* [Chart results](../densepose/structures/chart_result.py)\n* [CSE outputs](../densepose/structures/cse.py)\n* [Chart-based estimation](../densepose/modeling/predictors/chart.py)\n* [Confidence estimation](../densepose/modeling/predictors/chart_confidence.py)\n* [CSE estimation](../densepose/modeling/predictors/cse.py)\n* [Conversions](../densepose/converters)\n* [Losses](../densepose/modeling/losses)\n* [IUV setting](../densepose/modeling/losses/utils.py)\n* [CSE setting](../densepose/modeling/losses/embed_utils.py)\n* [HRNet](../densepose/modeling/hrnet.py)\n* [HRFPN](../densepose/modeling/hrfpn.py)\n* [IUV texture visualizer](../densepose/vis/densepose_results_textures.py)\n```\n\n----------------------------------------\n\nTITLE: Importing and Documenting detectron2.evaluation Module with Sphinx\nDESCRIPTION: A Sphinx documentation directive that automatically generates API documentation for the detectron2.evaluation module. The directive includes all members, undocumented members, and shows inheritance relationships.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/evaluation.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: detectron2.evaluation\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure with Sphinx toctree in reStructuredText\nDESCRIPTION: This snippet creates a table of contents structure in Sphinx documentation using the toctree directive. It lists all the main API components of Detectron2 that are documented separately.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n\n    checkpoint\n    config\n    data\n    data_transforms\n    engine\n    evaluation\n    layers\n    model_zoo\n    modeling\n    solver\n    structures\n    utils\n    export\n    fvcore\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Structure for Detectron2\nDESCRIPTION: This snippet defines the main table of contents (toctree) for Detectron2's documentation. It specifies a maximum depth of 2 and includes three main sections: tutorials, notes, and modules.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   tutorials/index\n   notes/index\n   modules/index\n```\n\n----------------------------------------\n\nTITLE: Linking to Detectron2 Documentation in Markdown\nDESCRIPTION: This snippet demonstrates how to create a Markdown link to the latest Detectron2 documentation hosted on ReadTheDocs. It uses the standard Markdown link syntax to provide a clickable link with descriptive text.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/README.md#2025-04-21_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n[detectron2.readthedocs.io](https://detectron2.readthedocs.io/)\n```\n\n----------------------------------------\n\nTITLE: Citing Detectron2 using BibTeX\nDESCRIPTION: BibTeX entry for citing Detectron2 in research papers or when referring to baseline results published in the Model Zoo. Includes author information, title, URL, and publication year.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{wu2019detectron2,\n  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and\n                  Wan-Yen Lo and Ross Girshick},\n  title =        {Detectron2},\n  howpublished = {\\url{https://github.com/facebookresearch/detectron2}},\n  year =         {2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation - DensePose Bootstrapping Pipeline\nDESCRIPTION: BibTeX citation for the DensePose bootstrapping pipeline paper by Sanakoyeu et al., published in CVPR 2020.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_8\n\nLANGUAGE: bibtex\nCODE:\n```\n@InProceedings{Sanakoyeu2020TransferringDensePose,\n    title = {Transferring Dense Pose to Proximal Animal Classes},\n    author = {Artsiom Sanakoyeu and Vasil Khalidov and Maureen S. McCarthy and Andrea Vedaldi and Natalia Neverova},\n    journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year = {2020},\n}\n```\n\n----------------------------------------\n\nTITLE: Citation - Original DensePose\nDESCRIPTION: BibTeX citation for the original DensePose paper by Guler et al., published in CVPR 2018.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_10\n\nLANGUAGE: bibtex\nCODE:\n```\n@InProceedings{Guler2018DensePose,\n  title={DensePose: Dense Human Pose Estimation In The Wild},\n  author={R\\{i}za Alp G\\\"uler, Natalia Neverova, Iasonas Kokkinos},\n  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Detectron2 Sampling Module\nDESCRIPTION: Imports the detectron2.modeling.sampling module and its members. This module probably contains functions for sampling operations used in training object detection models.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/modules/modeling.rst#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: detectron2.modeling.sampling\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: DensePose R-101 FPN Configuration References\nDESCRIPTION: YAML configuration file references for DensePose R-101 FPN models with different confidence estimation approaches (WC1M, WC2M) and learning schedules.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_IUV.md#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n../configs/densepose_rcnn_R_101_FPN_WC1M_s1x.yaml\n../configs/densepose_rcnn_R_101_FPN_WC2M_s1x.yaml\n../configs/densepose_rcnn_R_101_FPN_DL_WC1M_s1x.yaml\n../configs/densepose_rcnn_R_101_FPN_DL_WC2M_s1x.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CUDA 10.2 and PyTorch 1.10\nDESCRIPTION: Command to install pre-built Detectron2 package for CUDA 10.2 with PyTorch 1.10 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.10/index.html\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Built Detectron2 with CPU and PyTorch 1.10\nDESCRIPTION: Command to install pre-built Detectron2 package for CPU-only with PyTorch 1.10 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html\n```\n\n----------------------------------------\n\nTITLE: BibTeX citation for Pointly-Supervised Instance Segmentation\nDESCRIPTION: BibTeX entry for citing the original paper on Pointly-Supervised Instance Segmentation by Cheng, Parkhi, and Kirillov.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/PointSup/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{cheng2021pointly,\n  title={Pointly-Supervised Instance Segmentation},\n  author={Bowen Cheng and Omkar Parkhi and Alexander Kirillov},\n  journal={arXiv},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation for Continuous Surface Embeddings Paper\nDESCRIPTION: BibTeX citation entry for the original CSE paper by Neverova et al. (2020) describing the continuous surface embeddings methodology.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_CSE.md#2025-04-21_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@InProceedings{Neverova2020ContinuousSurfaceEmbeddings,\n    title = {Continuous Surface Embeddings},\n    author = {Neverova, Natalia and Novotny, David and Khalidov, Vasil and Szafraniec, Marc and Labatut, Patrick and Vedaldi, Andrea},\n    journal = {Advances in Neural Information Processing Systems},\n    year = {2020},\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-built Detectron2 with CPU-only and PyTorch 1.10\nDESCRIPTION: Command to install a pre-built CPU-only Detectron2 package with PyTorch 1.10 using pip.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/install.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install detectron2 -f \\\n  https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html\n```\n\n----------------------------------------\n\nTITLE: Citation for Cycle Losses Paper\nDESCRIPTION: BibTeX citation entry for the paper by Neverova et al. (2021) describing cycle losses and universal canonical maps between object categories.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/doc/DENSEPOSE_CSE.md#2025-04-21_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@InProceedings{Neverova2021UniversalCanonicalMaps,\n    title = {Discovering Relationships between Object Categories via Universal Canonical Maps},\n    author = {Neverova, Natalia and Sanakoyeu, Artsiom and Novotny, David and Labatut, Patrick and Vedaldi, Andrea},\n    journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year = {2021},\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for Detectron2 Documentation\nDESCRIPTION: This code snippet defines a table of contents using reStructuredText syntax for the Detectron2 project documentation. It specifies the maximum depth of the table and lists various tutorial topics.\nSOURCE: https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   install\n   getting_started\n   builtin_datasets\n   extend\n   datasets\n   data_loading\n   augmentation\n   models\n   write-models\n   training\n   evaluation\n   configs\n   lazyconfigs\n   deployment\n```"
  }
]
[
  {
    "owner": "thu-ml",
    "repo": "sageattention",
    "content": "TITLE: Basic Usage of SageAttention in Python\nDESCRIPTION: Example of using the SageAttention function for efficient attention computation in PyTorch models. Supports different tensor layouts and causal masking options.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sageattention import sageattn\nattn_output = sageattn(q, k, v, tensor_layout=\"HND\", is_causal=False)\n```\n\n----------------------------------------\n\nTITLE: Plug-and-Play Integration with PyTorch\nDESCRIPTION: Code snippet demonstrating how to replace PyTorch's built-in scaled_dot_product_attention with SageAttention for easy integration into existing models.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn.functional as F\nfrom sageattention import sageattn\nF.scaled_dot_product_attention = sageattn\n```\n\n----------------------------------------\n\nTITLE: Implementing SageAttention as Drop-in Replacement\nDESCRIPTION: Simple code to replace the standard scaled_dot_product_attention with SageAttention implementation. This modification allows for seamless integration with existing models.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/example/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sageattention import sageattn\nimport torch.nn.functional as F\n\nF.scaled_dot_product_attention = sageattn\n```\n\n----------------------------------------\n\nTITLE: Installing SageAttention From Source\nDESCRIPTION: Commands for cloning and installing SageAttention from source code. This is recommended for accessing the latest SageAttention 2.1.1 version with all features.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/thu-ml/SageAttention.git\ncd sageattention \npython setup.py install  # or pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Stable Release via Pip\nDESCRIPTION: Command for installing the stable Triton-only version of SageAttention using pip package manager.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install sageattention==1.0.6\n```\n\n----------------------------------------\n\nTITLE: Running CogVideoX Example with SageAttention\nDESCRIPTION: Command for running the CogVideoX-2b example with SageAttention integration, using compile mode for best performance.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd example\npython cogvideox-2b.py --compile --attention_type sage\n```\n\n----------------------------------------\n\nTITLE: Running CogvideoX with SageAttention\nDESCRIPTION: Command line instruction to run CogvideoX-2b with SageAttention compilation enabled. The --attention_type flag specifies the use of sage attention.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/example/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd example\npython cogvideox-2b.py --compile --attention_type sage\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Parallel SageAttention\nDESCRIPTION: Shell commands for installing required dependencies (xDiT and diffusers) and running parallel SageAttention inference. Requires specific versions of xfuser (>=0.3.5) and diffusers (>=0.32.0.dev0).\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/example/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# install latest xDiT(xfuser).\npip install \"xfuser[flash_attn]\"\n# install latest diffusers (>=0.32.0.dev0), need by latest xDiT.\ngit clone https://github.com/huggingface/diffusers.git\ncd diffusers && python3 setup.py bdist_wheel && cd dist && python3 -m pip install *.whl\n# then run parallel sage attention inference.\n./run_parallel.sh\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Against FlashAttention3\nDESCRIPTION: Instructions for compiling FlashAttention3 from source to benchmark performance against SageAttention.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Dao-AILab/flash-attention.git --recursive\ngit checkout b7d29fb3b79f0b78b1c369a52aaa6628dabfb0d7 # 2.7.2 release\ncd hopper\npython setup.py install\n```\n\n----------------------------------------\n\nTITLE: Running SageAttention Benchmarks on RTX 4090 with Custom Parameters\nDESCRIPTION: Example command for benchmarking SageAttention kernels on an RTX 4090 GPU with specific accumulation data type and quantization granularity settings.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/bench/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython bench_qk_int8_pv_fp8_cuda.py --pv_accum_dtype fp32+fp32 --quant_gran per_warp\n```\n\n----------------------------------------\n\nTITLE: Running SageAttention Benchmarks on H100 with Custom Parameters\nDESCRIPTION: Example command for benchmarking SageAttention kernels on an H100 GPU with specific accumulation data type and thread-level quantization granularity settings.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/bench/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython bench_qk_int8_pv_fp8_cuda_sm90.py --pv_accum_dtype fp32+fp32 --quant_gran per_thread\n```\n\n----------------------------------------\n\nTITLE: Installing FlashAttention3 for Benchmarking with FP8 Support\nDESCRIPTION: Commands to clone the FlashAttention repository, checkout a specific release version (2.7.2), and install the package for benchmarking FlashAttention3 and its FP8 variant.\nSOURCE: https://github.com/thu-ml/sageattention/blob/main/bench/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Dao-AILab/flash-attention.git --recursive\ngit checkout b7d29fb3b79f0b78b1c369a52aaa6628dabfb0d7 # 2.7.2 release\ncd hopper\npython setup.py install\n```"
  }
]
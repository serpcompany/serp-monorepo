[
  {
    "owner": "spider-rs",
    "repo": "spider",
    "content": "TITLE: Basic Web Crawl Example Rust\nDESCRIPTION: This example showcases a basic asynchronous web crawl using the `spider` crate. It initializes a `Website` struct with a URL, crawls the website, and prints the discovered links. It depends on the `spider` and `tokio` crates.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::website::Website;\nuse spider::tokio;\n\n#[tokio::main]\nasync fn main() {\n    let url = \"https://choosealicense.com\";\n    let mut website = Website::new(&url);\n    website.crawl().await;\n\n    for link in website.get_links() {\n        println!(\"- {:?}\", link.as_ref());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Launching and Using a Browser Instance in Rust\nDESCRIPTION: This Rust snippet demonstrates how to launch a Chromium browser instance, navigate to a webpage, interact with elements, and extract content. It utilizes the `chromiumoxide` crate to create a browser, open a new page, find and interact with a search input, and retrieve the resulting HTML. The code is asynchronous and leverages `tokio` for concurrent execution.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_chrome/README.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse futures::StreamExt;\n\nuse chromiumoxide::browser::{Browser, BrowserConfig};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    \n   // create a `Browser` that spawns a `chromium` process running with UI (`with_head()`, headless is default) \n   // and the handler that drives the websocket etc.\n    let (mut browser, mut handler) =\n        Browser::launch(BrowserConfig::builder().with_head().build()?).await?;\n    \n   // spawn a new task that continuously polls the handler\n    let handle = tokio::task::spawn(async move {\n        while let Some(h) = handler.next().await {\n            if h.is_err() {\n                break;\n            }\n        }\n    });\n    \n   // create a new browser page and navigate to the url\n    let page = browser.new_page(\"https://en.wikipedia.org\").await?;\n    \n   // find the search bar type into the search field and hit `Enter`,\n   // this triggers a new navigation to the search result page\n   page.find_element(\"input#searchInput\")\n           .await?\n           .click()\n           .await?\n           .type_str(\"Rust programming language\")\n           .await?\n           .press_key(\"Enter\")\n           .await?;\n\n   let html = page.wait_for_navigation().await?.content().await?;\n   \n    browser.close().await?;\n    let _ = handle.await;\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawler with Builder Rust\nDESCRIPTION: This code shows how to configure the crawler using the builder pattern.  It demonstrates setting configurations such as `respect_robots_txt`, `subdomains`, `tld`, `delay`, `request_timeout`, `user_agent`, `budget`, `limit`, `caching`, `external_domains`, `headers`, `blacklist_url`, `cron`, and `proxies`. This uses the `Website` struct from the `spider` crate, and shows chained method calls.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet mut website = Website::new(\"https://choosealicense.com\");\n\nwebsite\n   .with_respect_robots_txt(true)\n   .with_subdomains(true)\n   .with_tld(false)\n   .with_delay(0)\n   .with_request_timeout(None)\n   .with_http2_prior_knowledge(false)\n   .with_user_agent(Some(\"myapp/version\".into()))\n   .with_budget(Some(spider::hashbrown::HashMap::from([(\" * \", 300), (\"/licenses\", 10)])))\n   .with_limit(300)\n   .with_caching(false)\n   .with_external_domains(Some(Vec::from([\"https://creativecommons.org/licenses/by/3.0/\"].map( |d| d.to_string())).into_iter()))\n   .with_headers(None)\n   .with_blacklist_url(Some(Vec::from([\"https://choosealicense.com/licenses/\".into()])))\n   .with_cron(\"1/5 * * * * *\", Default::Default())\n   .with_proxies(None);\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawler Rust\nDESCRIPTION: This code snippet demonstrates how to configure the crawler using the `Configuration` object.  It shows how to set options such as `respect_robots_txt`, `subdomains`, `tld`, `delay`, `request_timeout`, `user_agent`, `blacklist_url`, `proxies`, `budget`, `cron_str`, `cron_type`, `limit` and `cache`. This example uses the `Website` struct from the `spider` crate.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n// ..\nlet mut website = Website::new(\"https://choosealicense.com\");\n\nwebsite.configuration.respect_robots_txt = true;\nwebsite.configuration.subdomains = true;\nwebsite.configuration.tld = false;\nwebsite.configuration.delay = 0; // Defaults to 0 ms due to concurrency handling\nwebsite.configuration.request_timeout = None; // Defaults to 15000 ms\nwebsite.configuration.http2_prior_knowledge = false; // Enable if you know the webserver supports http2\nwebsite.configuration.user_agent = Some(\"myapp/version\".into()); // Defaults to using a random agent\nwebsite.on_link_find_callback = Some(|s, html| { println!(\"link target: {}\", s); (s, html)});// Callback to run on each link find - useful for mutating the url, ex: convert the top level domain from `.fr` to `.es`.\nwebsite.configuration.blacklist_url.get_or_insert(Default::default()).push(\"https://choosealicense.com/licenses/\".into());\nwebsite.configuration.proxies.get_or_insert(Default::default()).push(\"socks5://10.1.1.1:12345\".into()); // Defaults to None - proxy list.\nwebsite.configuration.budget = Some(spider::hashbrown::HashMap::from([(spider::CaseInsensitiveString::new(\"*\"), 300), (spider::CaseInsensitiveString::new(\"/licenses\"), 10)])); // Defaults to None.\nwebsite.configuration.cron_str = \"1/5 * * * * *\".into(); // Defaults to empty string - Requires the `cron` feature flag\nwebsite.configuration.cron_type = spider::website::CronType::Crawl; // Defaults to CronType::Crawl - Requires the `cron` feature flag\nwebsite.configuration.limit = 300; // The limit of pages crawled. By default there is no limit.\nwebsite.configuration.cache = false; // HTTP caching. Requires the `cache` or `chrome` feature flag.\n\nwebsite.crawl().await;\n```\n\n----------------------------------------\n\nTITLE: Crawling a Website and Outputting Links\nDESCRIPTION: Crawls the specified URL and outputs all visited links. The `--output-links` option specifies that the links should be extracted and displayed or saved.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_cli/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nspider --url https://choosealicense.com crawl --output-links\n```\n\n----------------------------------------\n\nTITLE: Crawling with Depth Limit - Rust\nDESCRIPTION: This Rust code demonstrates how to set a depth limit for a web crawl using the `spider` crate. It configures a `Website` instance with a specific depth limit and then performs the crawl, preventing the crawler from going too deep into the website's link structure.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_25\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::{tokio, website::Website};\n\n#[tokio::main]\nasync fn main() {\n    let mut website = Website::new(\"https://choosealicense.com\").with_depth(3).build().unwrap();\n    website.crawl().await;\n\n    for link in website.get_links() {\n        println!(\"- {:?}\", link.as_ref());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Scraping/Gathering HTML - Rust\nDESCRIPTION: This Rust code shows how to scrape HTML content from a website using the `spider` crate. It initializes a `Website` instance, scrapes the content, and then prints the URL and HTML of each page to the standard output.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_13\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::tokio;\nuse spider::website::Website;\n\n#[tokio::main]\nasync fn main() {\n    use std::io::{Write, stdout};\n\n    let url = \"https://choosealicense.com/\";\n    let mut website = Website::new(&url);\n\n    website.scrape().await;\n\n    let mut lock = stdout().lock();\n\n    let separator = \"-\".repeat(url.len());\n\n    for page in website.get_pages().unwrap().iter() {\n        writeln!(\n            lock,\n            \"{}\n{}\n\n{}\n\n{}\",\n            separator,\n            page.get_url_final(),\n            page.get_html(),\n            separator\n        )\n            .unwrap();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: CSS Scraping with spider_utils in Rust\nDESCRIPTION: This code snippet demonstrates how to use the spider_utils crate to scrape data from HTML using CSS selectors. It defines a QueryCSSMap to associate a key (\"list\") with a set of CSS selectors (\".list\", \".sub-list\"). The css_query_select_map_streamed function is then used to extract data from the provided HTML string based on these selectors, and prints the extracted data as a HashMap.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_utils/README.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse spider::{\n    hashbrown::HashMap,\n    packages::scraper::Selector,\n};\nuse spider_utils::{QueryCSSMap, QueryCSSSelectSet, build_selectors, css_query_select_map_streamed};\n\nasync fn css_query_selector_extract() {\n    let map = QueryCSSMap::from([\n        (\"list\", QueryCSSSelectSet::from([\".list\", \".sub-list\"])),\n    ]);\n    let data = css_query_select_map_streamed(\n        r#\"<html>\n            <body>\n                <ul class=\"list\"><li>First</li></ul>\n                <ul class=\"sub-list\"><li>Second</li></ul>\n            </body>\n        </html>\"#,\n        &build_selectors(map),\n    )\n    .await;\n\n    println!(\"{:?}\", data);\n    // {\"list\": [\"First\", \"Second\"]}\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling and Outputting to JSON\nDESCRIPTION: Crawls the specified URL and redirects the output to a JSON file. The `-o` option is used to redirect the standard output to a file.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_cli/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nspider --url https://choosealicense.com crawl -o > spider_choosealicense.json\n```\n\n----------------------------------------\n\nTITLE: Executing Custom PDF Command in Rust\nDESCRIPTION: This Rust snippet shows how to execute a custom PrintToPdf command using the `chromiumoxide` crate.  It defines an asynchronous `pdf` function that takes `PrintToPdfParams` as input and returns a vector of bytes representing the PDF content. It relies on the `Page::execute` function to send the command and then decodes the base64 encoded result.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_chrome/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\npub async fn pdf(&self, params: PrintToPdfParams) -> Result<Vec<u8>>> {\n     let res = self.execute(params).await?;\n     Ok(base64::decode(&res.data)?)\n }\n```\n\n----------------------------------------\n\nTITLE: Transforming Content using Spider Transformations\nDESCRIPTION: This Rust code snippet demonstrates how to use the `spider_transformations` crate to transform content. It initializes a `TransformConfig` with a `Markdown` return format, then calls `transform_content` to perform the transformation. It requires the `spider_transformations` crate.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_transformations/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse spider_transformations::transformation::content;\n\nfn main() {\n    // page comes from the spider object when streaming.\n    let mut conf = content::TransformConfig::default();\n    conf.return_format = content::ReturnFormat::Markdown;\n    let content = content::transform_content(&page, &conf, &None, &None);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting a Crawl Budget with Path and Page Limits\nDESCRIPTION: Sets a crawl budget with specific limits for different paths.  This example limits all pages to 100 total, and only allows 10 pages matching the /blog/ path.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_cli/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nspider --url https://choosealicense.com --budget \"*,100,/blog/,10\" crawl -o\n```\n\n----------------------------------------\n\nTITLE: Crawling with Caching - Rust\nDESCRIPTION: This Rust code demonstrates how to enable HTTP caching for a web crawl using the `spider` crate. It configures a `Website` instance to use caching and then performs the crawl. Subsequent crawls will be faster due to the cached content.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_19\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::tokio;\nuse spider::website::Website;\n\n#[tokio::main]\nasync fn main() {\n    let mut website: Website = Website::new(\"https://spider.cloud\")\n        .with_caching(true)\n        .build()\n        .unwrap();\n\n    website.crawl().await;\n\n    println!(\"Links found {:?}\", website.get_links().len());\n    /// next run to website.crawl().await; will be faster since content is stored on disk.\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling with Smart Mode - Rust\nDESCRIPTION: This Rust code demonstrates how to use the smart mode crawling feature of the `spider` crate. It initializes a `Website` instance and uses `crawl_smart()` to crawl the website, which automatically determines whether to use HTTP or JavaScript rendering for each page.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_21\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::website::Website;\nuse spider::tokio;\n\n#[tokio::main]\nasync fn main() {\n    let mut website = Website::new(\"https://choosealicense.com\");\n    website.crawl_smart().await;\n\n    for link in website.get_links() {\n        println!(\"- {:?}\", link.as_ref());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Regex Blacklisting Example Rust\nDESCRIPTION: This example shows how to use regex for blacklisting routes. It initializes a `Website` struct, adds a regex pattern to the blacklist, crawls the website, and prints the discovered links. Requires the `regex` feature.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_10\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::website::Website;\nuse spider::tokio;\n\n#[tokio::main]\nasync fn main() {\n    let mut website = Website::new(\"https://choosealicense.com\");\n    website.configuration.blacklist_url.push(\"/licenses/\".into());\n    website.crawl().await;\n\n    for link in website.get_links() {\n        println!(\"- {:?}\", link.as_ref());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Scheduling Cron Jobs - Rust\nDESCRIPTION: This Rust code demonstrates how to schedule a web crawl using cron jobs with the `spider` crate. It sets a cron string to run the crawl every 5 seconds, subscribes to the crawl results, and then starts the cron runner.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_15\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::website::{Website, run_cron};\nuse spider::tokio;\n\n#[tokio::main]\nasync fn main() {\n    let mut website = Website::new(\"https://choosealicense.com\");\n    // set the cron to run or use the builder pattern `website.with_cron`.\n    website.cron_str = \"1/5 * * * * *\".into();\n\n    let mut rx2 = website.subscribe(16).unwrap();\n\n    let join_handle = tokio::spawn(async move {\n        while let Ok(res) = rx2.recv().await {\n            println!(\"{:?}\", res.get_url());\n        }\n    });\n\n    // take ownership of the website. You can also use website.run_cron, except you need to perform abort manually on handles created.\n    let mut runner = run_cron(website).await;\n\n    println!(\"Starting the Runner for 10 seconds\");\n    tokio::time::sleep(tokio::time::Duration::from_secs(10)).await;\n    let _ = tokio::join!(runner.stop(), join_handle);\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching and Configuring a Browser Instance in Rust\nDESCRIPTION: This Rust snippet illustrates how to fetch and configure a Chromium browser instance for automation. It uses the `chromiumoxide` crate to download a compatible Chromium version to a specified directory, then configures the `BrowserConfig` to use the downloaded executable. The code sets up a browser environment that can run without a pre-installed Chromium instance.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_chrome/README.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse std::path::Path;\n\nuse futures::StreamExt;\n\nuse chromiumoxide::browser::{BrowserConfig};\nuse chromiumoxide::fetcher::{BrowserFetcher, BrowserFetcherOptions};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    let download_path = Path::new(\"./download\");\n    tokio::fs::create_dir_all(&download_path).await?;\n    let fetcher = BrowserFetcher::new(\n        BrowserFetcherOptions::builder()\n            .with_path(&download_path)\n            .build()?,\n    );\n    let info = fetcher.fetch().await?;\n\n    let config = BrowserConfig::builder()\n        .chrome_executable(info.executable_path)\n        .build()?,\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling with Chrome Interception - Rust\nDESCRIPTION: This Rust code demonstrates how to use Chrome for crawling with request interception using the `spider` crate.  It configures a `Website` instance to use Chrome with request interception and then performs the crawl.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_17\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::tokio;\nuse spider::website::Website;\n\n#[tokio::main]\nasync fn main() {\n    let mut website: Website = Website::new(\"https://spider.cloud\")\n        .with_chrome_intercept(spider::features::chrome_common::RequestInterceptConfiguration::new(true))\n        .build()\n        .unwrap();\n\n    website.crawl().await;\n\n    println!(\"Links found {:?}\", website.get_links().len());\n}\n```\n\n----------------------------------------\n\nTITLE: Crawling with OpenAI - Rust\nDESCRIPTION: This Rust code demonstrates how to integrate OpenAI for generating scripts to drive the browser during a web crawl using the `spider` crate. It configures a `Website` instance to use OpenAI with specific settings and then performs the crawl.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_23\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::{tokio, website::Website, configuration::GPTConfigs};\n\n#[tokio::main]\nasync fn main() {\n    let mut website: Website = Website::new(\"https://google.com\")\n        .with_openai(Some(GPTConfigs::new(\"gpt-4-1106-preview\", \"Search for Movies\", 256)))\n        .with_limit(1)\n        .build()\n        .unwrap();\n\n    website.crawl().await;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Spider CLI\nDESCRIPTION: Installs the `spider_cli` crate using `cargo`.  Different features can be enabled during installation, such as `chrome` for headless browsing, `smart` for intelligent crawling, and `full_resources` to download all resources.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_cli/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncargo install spider_cli\n# with headless\ncargo install -F chrome spider_cli\n# with smart mode defaults to HTTP and Headless when needed\ncargo install -F smart spider_cli\n# with full resources not just web pages\ncargo install -F full_resources spider_cli\n```\n\n----------------------------------------\n\nTITLE: Downloading HTML to a Local Destination\nDESCRIPTION: Downloads the HTML content of the specified URL to a local directory. The `-t` option specifies the target destination folder.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_cli/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nspider --url https://choosealicense.com download -t _temp_spider_downloads\n```\n\n----------------------------------------\n\nTITLE: Subscribe to changes Rust\nDESCRIPTION: This example shows how to subscribe to changes from the `spider` crate. It creates a `Website` struct, subscribes to its changes with a specified buffer size, spawns a Tokio task to receive and print URLs, and then crawls the website. The `sync` feature is required.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::website::Website;\nuse spider::tokio;\n\n#[tokio::main]\nasync fn main() {\n    let mut website = Website::new(\"https://choosealicense.com\");\n    let mut rx2 = website.subscribe(16).unwrap();\n\n    tokio::spawn(async move {\n        while let Ok(res) = rx2.recv().await {\n            println!(\"{:?}\", res.get_url());\n        }\n    });\n\n    website.crawl().await;\n    website.unsubscribe();\n}\n```\n\n----------------------------------------\n\nTITLE: Pausing, Resuming, and Shutting Down Crawls - Rust\nDESCRIPTION: This Rust code demonstrates how to pause, resume, and shutdown a web crawl using the `control` feature of the `spider` crate. It spawns a Tokio task to control the crawl after a specified duration.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_12\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::tokio;\nuse spider::website::Website;\n\n#[tokio::main]\nasync fn main() {\n    use spider::utils::{pause, resume, shutdown};\n    let url = \"https://choosealicense.com/\";\n    let mut website: Website = Website::new(&url);\n\n    tokio::spawn(async move {\n        pause(url).await;\n        sleep(tokio::time::Duration::from_millis(5000)).await;\n        resume(url).await;\n        // perform shutdown if crawl takes longer than 15s\n        sleep(tokio::time::Duration::from_millis(15000)).await;\n        // you could also abort the task to shutdown crawls if using website.crawl in another thread.\n        shutdown(url).await;\n    });\n\n    website.crawl().await;\n}\n```\n\n----------------------------------------\n\nTITLE: Decentralized Worker Shell\nDESCRIPTION: These commands install and then start the `spider_worker` executable. The RUST_LOG environment variable sets the logging level, and SPIDER_WORKER_PORT sets the port the worker listens on. The next command launches the main rust project with the decentralised features.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n# install the worker\ncargo install spider_worker\n# start the worker [set the worker on another machine in prod]\nRUST_LOG=info SPIDER_WORKER_PORT=3030 spider_worker\n# start rust project as normal with SPIDER_WORKER env variable\nSPIDER_WORKER=http://127.0.0.1:3030 cargo run --example example --features decentralized\n```\n\n----------------------------------------\n\nTITLE: Decentralized Feature Dependency TOML\nDESCRIPTION: This snippet shows how to include the `decentralized` feature for use with workers.  Workers will handle the IO more efficiently and allow for scaling the processing.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2\", features = [\"decentralized\"] }\n```\n\n----------------------------------------\n\nTITLE: Setting a Crawl Budget with Domain Restriction\nDESCRIPTION: Sets a crawl budget to limit the number of pages crawled per domain.  The `*,1` budget allows crawling all pages on the domain, but limits it to 1 page total.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_cli/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nspider --url https://choosealicense.com --budget \"*,1\" crawl -o\n```\n\n----------------------------------------\n\nTITLE: Adding Spider dependency TOML\nDESCRIPTION: This snippet demonstrates how to add the `spider` crate as a dependency to your `Cargo.toml` file. This is the first step to using the Spider crawler in your Rust project.  The version is set to \"2\".\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = \"2\"\n```\n\n----------------------------------------\n\nTITLE: Adding Spider Transformations Dependency\nDESCRIPTION: This snippet shows how to add the `spider_transformations` crate as a dependency in a Rust project using Cargo.  It specifies version 2 of the crate.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_transformations/README.md#_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider_transformations = \"2\"\n```\n\n----------------------------------------\n\nTITLE: Adding chromiumoxide to a Project with Tokio Support\nDESCRIPTION: This TOML snippet demonstrates how to add the `chromiumoxide` crate to a Rust project using Cargo.  It configures the dependency to include version 2 and disables default features. It leverages `tokio-tungstenite` for websocket support. This configuration enables asynchronous communication with the Chrome/Chromium browser.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_chrome/README.md#_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\nspider_chrome = { version = \"2\", default-features = false }\n```\n\n----------------------------------------\n\nTITLE: Enabling Features TOML\nDESCRIPTION: This snippet demonstrates how to enable optional features for the `spider` crate in your `Cargo.toml` file. It shows how to specify the `regex` and `ua_generator` features. These features add extra functionalities to the crawler.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2\", features = [\"regex\", \"ua_generator\"] }\n```\n\n----------------------------------------\n\nTITLE: Enabling Caching Feature\nDESCRIPTION: This TOML snippet enables the `cache` feature for the `spider` crate, allowing for caching HTTP responses to improve performance.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_18\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2.0.12\", features = [\"cache\"] }\n```\n\n----------------------------------------\n\nTITLE: Enabling Control Feature\nDESCRIPTION: This TOML snippet enables the `control` feature for the `spider` crate, allowing for pausing, resuming, and shutting down crawls. This is useful for managing large workloads.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2\", features = [\"control\"] }\n```\n\n----------------------------------------\n\nTITLE: Sync Feature Dependency TOML\nDESCRIPTION: This snippet shows how to include the `sync` feature for subscribing to changes using broadcast channels. By subscribing you are able to handle the Page data processing asynchronously.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2\", features = [\"sync\"] }\n```\n\n----------------------------------------\n\nTITLE: Enabling Cron Feature\nDESCRIPTION: This TOML snippet enables the `sync` and `cron` features for the `spider` crate, allowing scheduling crawls using cron jobs.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2\", features = [\"sync\", \"cron\"] }\n```\n\n----------------------------------------\n\nTITLE: Enabling Smart Mode Feature\nDESCRIPTION: This TOML snippet enables the `smart` feature for the `spider` crate, allowing for intelligently crawling websites using a combination of HTTP requests and JavaScript rendering when needed.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_20\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2.0.12\", features = [\"smart\"] }\n```\n\n----------------------------------------\n\nTITLE: Enabling Chrome and Chrome Intercept Features\nDESCRIPTION: This TOML snippet enables the `chrome` and `chrome_intercept` features for the `spider` crate, allowing for crawling websites with JavaScript rendering and network interception.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_16\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2\", features = [\"chrome\", \"chrome_intercept\"] }\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenAI Feature\nDESCRIPTION: This TOML snippet enables the `openai` feature for the `spider` crate, allowing for generating dynamic scripts to drive the browser using OpenAI.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_22\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2.0.12\", features = [\"openai\"] }\n```\n\n----------------------------------------\n\nTITLE: Regex Feature Dependency TOML\nDESCRIPTION: This snippet shows how to include the `regex` feature for using regex based blacklisting.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2\", features = [\"regex\"] }\n```\n\n----------------------------------------\n\nTITLE: Benchmark Execution on macOS (sh)\nDESCRIPTION: This shell snippet represents the environment used for the benchmark execution on macOS. It includes system information such as the CPU, RAM, and disk space. The benchmark target URL is also specified.\nSOURCE: https://github.com/spider-rs/spider/blob/main/benches/BENCHMARKS.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n----------------------\nmac Apple M1 Max\n10-core CPU\n64 GB of RAM memory\n1 TB of SSD disk space\n-----------------------\n\nTest url: `https://rsseau.fr/`\n\n185 pages\n```\n\n----------------------------------------\n\nTITLE: Benchmark Execution on Linux (sh)\nDESCRIPTION: This shell snippet represents the environment used for the benchmark execution on Linux (Ubuntu). It includes system information such as the CPU, RAM, and disk space. The benchmark target URL is also specified.\nSOURCE: https://github.com/spider-rs/spider/blob/main/benches/BENCHMARKS.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n----------------------\nlinux ubuntu-latest\n2-core CPU\n7 GB of RAM memory\n14 GB of SSD disk space\n-----------------------\n\nTest url: `https://rsseau.fr/`\n\n185 pages\n```\n\n----------------------------------------\n\nTITLE: Setting Depth Limit - TOML\nDESCRIPTION: This TOML snippet shows the dependency configuration for setting a depth limit.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_24\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nspider = { version = \"2.0.12\" }\n```\n\n----------------------------------------\n\nTITLE: Reusing Configuration - Rust\nDESCRIPTION: This Rust code demonstrates how to reuse a configuration for multiple web crawls using the `spider` crate. It creates a `Configuration` instance with specific settings and then uses it to build multiple `Website` instances for different URLs, allowing for consistent crawling behavior across multiple websites.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider/README.md#_snippet_26\n\nLANGUAGE: rust\nCODE:\n```\nextern crate spider;\n\nuse spider::configuration::Configuration;\nuse spider::{tokio, website::Website};\nuse std::io::Error;\nuse std::time::Instant;\n\nconst CAPACITY: usize = 5;\nconst CRAWL_LIST: [&str; CAPACITY] = [\n    \"https://spider.cloud\",\n    \"https://choosealicense.com\",\n    \"https://jeffmendez.com\",\n    \"https://spider-rs.github.io/spider-nodejs/\",\n    \"https://spider-rs.github.io/spider-py/\",\n];\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    let config = Configuration::new()\n        .with_user_agent(Some(\"SpiderBot\"))\n        .with_blacklist_url(Some(Vec::from([\"https://spider.cloud/resume\".into()])))\n        .with_subdomains(false)\n        .with_tld(false)\n        .with_redirect_limit(3)\n        .with_retry(1)\n        .with_respect_robots_txt(true)\n        .with_external_domains(Some(\n            Vec::from([\"http://loto.rsseau.fr/\"].map(|d| d.to_string())).into_iter(),\n        ))\n        .build();\n\n    let mut handles = Vec::with_capacity(CAPACITY);\n\n    for website_url in CRAWL_LIST {\n        match Website::new(website_url)\n            .with_config(config.to_owned())\n            .build()\n        {\n            Ok(mut website) => {\n                let handle = tokio::spawn(async move {\n                    println!(\"Starting Crawl - {:?}\", website.get_url().inner());\n\n                    let start = Instant::now();\n                    website.crawl().await;\n                    let duration = start.elapsed();\n\n                    let links = website.get_links();\n\n                    for link in links {\n                        println!(\"- {:?}\", link.as_ref());\n                    }\n\n                    println!(\n                        \"{:?} - Time elapsed in website.crawl() is: {:?} for total pages: {:?}\",\n                        website.get_url().inner(),\n                        duration,\n                        links.len()\n                    );\n                });\n\n                handles.push(handle);\n            }\n            Err(e) => println!(\"{:?}\", e)\n        }\n    }\n\n    for handle in handles {\n        let _ = handle.await;\n    }\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Ubuntu\nDESCRIPTION: Installs the `pkg-config` package on Ubuntu. This is needed so that `openssl` can be recognized by Cargo during the build process.\nSOURCE: https://github.com/spider-rs/spider/blob/main/spider_cli/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\napt install pkg-config\n```"
  }
]
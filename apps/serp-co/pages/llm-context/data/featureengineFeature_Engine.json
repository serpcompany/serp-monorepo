[
  {
    "owner": "feature-engine",
    "repo": "feature_engine",
    "content": "TITLE: Making Predictions with the Fitted Pipeline - Python\nDESCRIPTION: This code snippet uses the fitted pipeline (`pipe`) to generate forecasts for the test data (`X_test`).  The `predict` method of the pipeline returns the predicted values. The code then creates a pandas DataFrame named `forecasts` from these predictions, using the index from the transformed test data and column names indicating the prediction steps (e.g., 'step_1', 'step_2'). Finally, it prints the first few rows of the `forecasts` DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nforecast = pipe.predict(X_test)\n\n    forecasts = pd.DataFrame(\n        pipe.predict(X_test),\n        index=Xt.loc[end_train:].index,\n        columns=[f\"step_{i+1}\" for i in range(6)]\n\n    )\n\n    print(forecasts.head())\n```\n\n----------------------------------------\n\nTITLE: Fit and Transform with SmartCorrelatedSelection in Python\nDESCRIPTION: This code snippet demonstrates how to use the fit_transform method of the SmartCorrelatedSelection class to find correlated variables, select the ones to keep, and drop the remaining features from the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nXt = tr.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameter Grid for GridSearchCV - Python\nDESCRIPTION: This code defines a hyperparameter grid to be used in a `GridSearchCV` search.  The grid specifies the values to be explored for the `C` parameter of the `LogisticRegression` model ('logit__C'), the `top_categories` parameter of the `OneHotEncoder` ('enc__top_categories'), and the `capping_method` parameter of the `OutlierTrimmer` ('outliers__capping_method').  This allows for tuning these parameters to optimize the pipeline's performance.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nparam_grid= {\n        'logit__C': [0.1, 10.],\n        'enc__top_categories': [None, 5],\n        'outliers__capping_method': [\"mad\", 'iqr']\n    }\n```\n\n----------------------------------------\n\nTITLE: Feature-engine in Scikit-learn Pipeline\nDESCRIPTION: This code demonstrates how to integrate Feature-engine transformers within a Scikit-learn pipeline for a complete feature engineering and modeling workflow. It includes data loading, preprocessing steps (imputation, discretisation, encoding, scaling), and a Lasso regression model. The pipeline automates the sequence of transformations and model fitting, improving code organization and reproducibility. It also shows evaluation metrics after training and prediction.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/index.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    from math import sqrt\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    from sklearn.linear_model import Lasso\n    from sklearn.metrics import mean_squared_error\n    from sklearn.model_selection import train_test_split\n    from sklearn.pipeline import Pipeline as pipe\n    from sklearn.preprocessing import MinMaxScaler\n    \n    from feature_engine.encoding import RareLabelEncoder, MeanEncoder\n    from feature_engine.discretisation import DecisionTreeDiscretiser\n    from feature_engine.imputation import (\n        AddMissingIndicator,\n        MeanMedianImputer,\n        CategoricalImputer,\n    )\n\n    # load dataset\n    data = pd.read_csv('houseprice.csv')\n\n    # drop some variables\n    data.drop(\n        labels=['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'Id'],\n        axis=1,\n        inplace=True\n    )\n\n    # make a list of categorical variables\n    categorical = [var for var in data.columns if data[var].dtype == 'O']\n\n    # make a list of numerical variables\n    numerical = [var for var in data.columns if data[var].dtype != 'O']\n\n    # make a list of discrete variables\n    discrete = [ var for var in numerical if len(data[var].unique()) < 20]\n\n    # categorical encoders work only with object type variables\n    # to treat numerical variables as categorical, we need to re-cast them\n    data[discrete]= data[discrete].astype('O')\n\n    # continuous variables\n    numerical = [\n        var for var in numerical if var not in discrete\n        and var not in ['Id', 'SalePrice']\n        ]\n\n    # separate into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n                                            data.drop(labels=['SalePrice'], axis=1),\n                                            data.SalePrice,\n                                            test_size=0.1,\n                                            random_state=0\n                                            )\n\n    # set up the pipeline\n    price_pipe = pipe([\n        # add a binary variable to indicate missing information for the 2 variables below\n        ('continuous_var_imputer', AddMissingIndicator(variables=['LotFrontage'])),\n\n        # replace NA by the median in the 2 variables below, they are numerical\n        ('continuous_var_median_imputer', MeanMedianImputer(\n            imputation_method='median', variables=['LotFrontage', 'MasVnrArea']\n        )),\n\n        # replace NA by adding the label \"Missing\" in categorical variables\n        ('categorical_imputer', CategoricalImputer(variables=categorical)),\n\n        # disretise continuous variables using trees\n        ('numerical_tree_discretiser', DecisionTreeDiscretiser(\n            cv=3,\n            scoring='neg_mean_squared_error',\n            variables=numerical,\n            regression=True)),\n\n        # remove rare labels in categorical and discrete variables\n        ('rare_label_encoder', RareLabelEncoder(\n            tol=0.03, n_categories=1, variables=categorical+discrete\n        )),\n\n        # encode categorical and discrete variables using the target mean\n        ('categorical_encoder', MeanEncoder(variables=categorical+discrete)),\n\n        # scale features\n        ('scaler', MinMaxScaler()),\n\n        # Lasso\n        ('lasso', Lasso(random_state=2909, alpha=0.005))\n\n    ])\n\n    # train feature engineering transformers and Lasso\n    price_pipe.fit(X_train, np.log(y_train))\n\n    # predict\n    pred_train = price_pipe.predict(X_train)\n    pred_test = price_pipe.predict(X_test)\n\n    # Evaluate\n    print('Lasso Linear Model train mse: {}'.format(\n        mean_squared_error(y_train, np.exp(pred_train))))\n    print('Lasso Linear Model train rmse: {}'.format(\n        sqrt(mean_squared_error(y_train, np.exp(pred_train)))))\n    print()\n    print('Lasso Linear Model test mse: {}'.format(\n        mean_squared_error(y_test, np.exp(pred_test))))\n    print('Lasso Linear Model test rmse: {}'.format(\n        sqrt(mean_squared_error(y_test, np.exp(pred_test)))))\n\n```\n\n----------------------------------------\n\nTITLE: Pipeline Setup with make_pipeline (Python)\nDESCRIPTION: This code snippet demonstrates how to create a `Pipeline` using `make_pipeline` in feature_engine. It includes steps for dropping missing data, encoding categorical variables using ordinal encoding, and applying a Lasso regression model. The pipeline automatically assigns names to each step.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nfrom feature_engine.imputation import DropMissingData\nfrom feature_engine.encoding import OrdinalEncoder\nfrom feature_engine.pipeline import make_pipeline\n\nfrom sklearn.linear_model import Lasso\n\nX = pd.DataFrame(\n    dict(\n        x1=[2, 1, 1, 0, np.nan],\n        x2=[\"a\", np.nan, \"b\", np.nan, \"a\"],\n    )\n)\ny = pd.Series([1, 2, 3, 4, 5])\n\npipe = make_pipeline(\n    DropMissingData(),\n    OrdinalEncoder(encoding_method=\"arbitrary\"),\n    Lasso(random_state=10),\n)\n# predict\npipe.fit(X, y)\npreds_pipe = pipe.predict(X)\npreds_pipe\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with DecisionTreeEncoder (Feature-engine)\nDESCRIPTION: This code transforms the training and testing datasets using the fitted `DecisionTreeEncoder`. The `encoder.transform()` method replaces the original categorical values in the specified variables ('MSZoning' and 'LotShape') with their corresponding encoded values based on the decision tree mappings. It then prints the head of the transformed training data for the encoded variables to show the effect of the transformation. Dependencies include `feature_engine.encoding.DecisionTreeEncoder`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n\nprint(train_t[[\"MSZoning\", 'LotShape']].head(10))\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting OneHotEncoder in Python\nDESCRIPTION: This code initializes the `OneHotEncoder` from Feature-engine, specifying the variables to be encoded ('cabin', 'embarked') and setting `drop_last=True` to create k-1 dummy variables. It then fits the encoder to the training data, learning the unique categories for each specified variable. The `fit()` method prepares the encoder for transforming new data based on the learned categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nencoder = OneHotEncoder(\n    variables=['cabin', 'embarked'],\n    drop_last=True,\n    )\n\nencoder.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Creating a Feature Engineering Pipeline for WoE Encoding\nDESCRIPTION: This code snippet creates a pipeline using Feature-engine to discretize numerical variables, group rare labels, and then encode all variables using WoE encoding.  The pipeline consists of an EqualFrequencyDiscretiser, a RareLabelEncoder, and a WoEEncoder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(\n    [\n        (\"disc\", EqualFrequencyDiscretiser(variables=numerical_features)),\n        (\"rare_label\", RareLabelEncoder(tol=0.1, n_categories=2, variables=all, ignore_format=True)),\n        (\"woe\", WoEEncoder(variables=all)),\n    ])\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names After One-Hot Encoding in Python\nDESCRIPTION: This snippet shows how to retrieve the names of the features generated after one-hot encoding using the `get_feature_names_out()` method of the fitted OneHotEncoder.  This is useful for understanding the final feature space after the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nencoder.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Inverse Transforming Data with OrdinalEncoder in Python\nDESCRIPTION: This code snippet demonstrates how to revert encoded values back to the original categories using the `inverse_transform()` method of the OrdinalEncoder. The method takes the transformed data `train_t` as input and returns a dataframe `train_inv` with the original categorical values. The `head()` method is then used to display the first few rows of the reverted dataframe.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_inv = encoder.inverse_transform(train_t)\n\nprint(train_inv.head())\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Data for SelectByInformationValue\nDESCRIPTION: Loads the credit approval dataset using pandas, renames columns, preprocesses data by replacing '?' with NaN, converts columns to appropriate types, maps target variable, and removes rows with missing values.  The `data.head()` call is included to preview the processed data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# load data\ndata = pd.read_csv('crx.data', header=None)\n\n# name variables\nvar_names = ['A' + str(s) for s in range(1,17)]\ndata.columns = var_names\ndata.rename(columns={'A16': 'target'}, inplace=True)\n\n# preprocess data\ndata = data.replace('?', np.nan)\ndata['A2'] = data['A2'].astype('float')\ndata['A14'] = data['A14'].astype('float')\ndata['target'] = data['target'].map({'+':1, '-':0})\n\n# drop rows with missing data\ndata.dropna(axis=0, inplace=True)\n\ndata.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing Pipeline with DropMissingData, OrdinalEncoder and Lasso\nDESCRIPTION: This code initializes a feature-engine Pipeline with three steps: dropping missing data using DropMissingData, encoding categorical variables using OrdinalEncoder, and fitting a Lasso regression model.  The pipeline is then fit to the data and used to make predictions. The dependencies are pandas, numpy, scikit-learn, and feature-engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nfrom feature_engine.imputation import DropMissingData\nfrom feature_engine.encoding import OrdinalEncoder\nfrom feature_engine.pipeline import Pipeline\n\nfrom sklearn.linear_model import Lasso\n\nX = pd.DataFrame(\n    dict(\n        x1=[2, 1, 1, 0, np.nan],\n        x2=[\"a\", np.nan, \"b\", np.nan, \"a\"],\n    )\n)\ny = pd.Series([1, 2, 3, 4, 5])\n\npipe = Pipeline(\n    [\n        (\"drop\", DropMissingData()),\n        (\"enc\", OrdinalEncoder(encoding_method=\"arbitrary\")),\n        (\"lasso\", Lasso(random_state=10)),\n    ]\n)\n# predict\npipe.fit(X, y)\npreds_pipe = pipe.predict(X)\npreds_pipe\n```\n\n----------------------------------------\n\nTITLE: Applying PowerTransformer with custom exponent\nDESCRIPTION: This snippet demonstrates how to apply a power transformation with a custom exponent (0.001) to the 'LotArea' and 'GrLivArea' variables using Feature-engine's PowerTransformer. It fits the transformer to the training data and then visualizes the transformed 'LotArea' using seaborn's histplot. The key parameter is `exp`, which allows for specifying the lambda value of the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\t# Set up the variable transformer (tf)\n\ttf_custom = PowerTransformer(variables = ['LotArea', 'GrLivArea'], exp=0.001)\n\n\t# Fit the transformer\n\tX_train_transformed_custom = tf_custom.fit_transform(X_train)\n\n\t# Plot histogram\n\tsns.histplot(X_train_transformed_custom['LotArea'], kde=True, bins=50)\n```\n\n----------------------------------------\n\nTITLE: Fitting and Transforming Data with Ordered OrdinalEncoder in Python\nDESCRIPTION: This code snippet fits the OrdinalEncoder to the training data `X_train` and target variable `y_train` to learn the ordered mappings. It then transforms both the training and testing sets using the learned mappings. Note that the `fit_transform()` method requires both X_train and y_train for ordered encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nX_train_t = ordered_encoder.fit_transform(X_train, y_train)\nX_test_t = ordered_encoder.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Checking Number of Unique Values After Transformation in Python\nDESCRIPTION: This snippet calculates and prints the number of unique values in the 'LotArea' and 'GrLivArea' columns of the transformed training set (`train_t`). This demonstrates the effect of the discretization, showing the reduced number of distinct values after the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[['LotArea', 'GrLivArea']].nunique()\n```\n\n----------------------------------------\n\nTITLE: Cyclical Encoding with Feature-engine\nDESCRIPTION: This code snippet demonstrates cyclical feature encoding using the `CyclicalFeatures` transformer from Feature-engine. It initializes the transformer with `drop_original=True` to remove the original columns. It then fits and transforms the specified columns ('month', 'weekday', 'hour') of the DataFrame `df`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntr = CyclicalFeatures(drop_original=True)\n    Xt = tr.fit_transform(df[[\"month\", \"weekday\", \"hour\"]])\n\n    print(Xt)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with OrdinalEncoder in Python\nDESCRIPTION: This code snippet demonstrates how to transform categorical features using a fitted OrdinalEncoder. The `transform()` method applies the learned mappings to the categorical features in the input dataframes, `X_train` and `X_test`, resulting in ordinal variables. The `head()` method is used to display the first few rows of the transformed training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Discretizing Data with EqualFrequencyDiscretiser\nDESCRIPTION: This snippet instantiates and uses the `EqualFrequencyDiscretiser` from `feature_engine.discretisation`. It discretizes the sample data into 5 equal frequency bins.  The transformed data is stored in `X_transformed`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate discretizer\ndisc = EqualFrequencyDiscretiser(q=5)\n\n# Transform simulated data\nX_transformed = disc.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using LogCpTransformer with non-strictly positive data\nDESCRIPTION: This code snippet demonstrates how to initialize and use the LogCpTransformer on non-strictly positive variables from the diabetes dataset. The transformer automatically detects the optimal constant C to shift the distribution to positive values before applying the logarithm.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogCpTransformer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom feature_engine.transformation import LogCpTransformer\n\n# Load dataset\nX, y = load_diabetes( return_X_y=True, as_frame=True)\n\n# Separate into train and test sets\nX_train, X_test, y_train, y_test =  train_test_split(\n    X, y, test_size=0.3, random_state=0)\n\nprint(X_train[[\"bmi\", \"s3\"]].describe())\n\ntf = LogCpTransformer(variables = [\"bmi\", \"s3\"], C=\"auto\")\ntf.fit(X_train)\n\ntf.C_\n\ntrain_t= tf.transform(X_train)\ntest_t= tf.transform(X_test)\n\nX_train[\"bmi\"].hist(bins=20)\nplt.title(\"bmi - original distribution\")\nplt.ylabel(\"Number of observations\")\n\n# transformed variable\ntrain_t[\"bmi\"].hist(bins=20)\nplt.title(\"bmi - transformed distribution\")\nplt.ylabel(\"Number of observations\")\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Winsorizer - Python\nDESCRIPTION: This code snippet applies the fitted `Winsorizer` to transform both the training and testing datasets. The `transform` method caps the outlier values in the specified variables ('age' and 'fare') based on the capping values learned during the `fit` stage. The transformed datasets are assigned to `train_t` and `test_t` respectively.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/Winsorizer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# transform the data\ntrain_t = capper.transform(X_train)\ntest_t = capper.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Loading Australia Electricity Demand Dataset\nDESCRIPTION: This code loads the Australia electricity demand dataset from a remote URL using pandas. It performs preprocessing steps such as dropping a column, converting the date, creating a datetime column, handling missing values, renaming columns, and resampling to hourly data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://raw.githubusercontent.com/tidyverts/tsibbledata/master/data-raw/vic_elec/VIC2015/demand.csv\"\ndf = pd.read_csv(url)\n\ndf.drop(columns=[\"Industrial\"], inplace=True)\n\n# Convert the integer Date to an actual date with datetime type\ndf[\"date\"] = df[\"Date\"].apply(\n    lambda x: pd.Timestamp(\"1899-12-30\") + pd.Timedelta(x, unit=\"days\")\n)\n\n# Create a timestamp from the integer Period representing 30 minute intervals\ndf[\"date_time\"] = df[\"date\"] + \\\n    pd.to_timedelta((df[\"Period\"] - 1) * 30, unit=\"m\")\n\ndf.dropna(inplace=True)\n\n# Rename columns\ndf = df[[\"date_time\", \"OperationalLessIndustrial\"]]\n\ndf.columns = [\"date_time\", \"demand\"]\n\n# Resample to hourly\ndf = (\n    df.set_index(\"date_time\")\n    .resample(\"h\")\n    .agg({\"demand\": \"sum\"})\n)\n\nprint(df.head())\n```\n\nLANGUAGE: python\nCODE:\n```\n                              demand\n    date_time\n    2002-01-01 00:00:00  6919.366092\n    2002-01-01 01:00:00  7165.974188\n    2002-01-01 02:00:00  6406.542994\n    2002-01-01 03:00:00  5815.537828\n    2002-01-01 04:00:00  5497.732922\n```\n\n----------------------------------------\n\nTITLE: Lag Features with Datetime and Drop NA in Python\nDESCRIPTION: This snippet shows how to create lag features based on datetime frequency and remove rows containing NaN values in the resulting features, using the LagFeatures transformer in Feature Engine. It transform both X and y.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(\n    variables=[\"module_temp\", \"irradiation\"], freq=\"30min\", drop_na=True)\n\nlag_f.fit(X)\n\nX_tr, y_tr = lag_f.transform_x_y(X, y)\n\nX_tr.shape, y_tr.shape, X.shape, y.shape\n```\n\n----------------------------------------\n\nTITLE: Initializing Pipeline with DropMissingData, OneHotEncoder and Lasso\nDESCRIPTION: This code initializes a pipeline with DropMissingData, OneHotEncoder, and Lasso. It then fits the pipeline to sample data X and y.  The dependencies are pandas, numpy, scikit-learn, and feature-engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nfrom feature_engine.imputation import DropMissingData\nfrom feature_engine.encoding import OneHotEncoder\nfrom feature_engine.pipeline import Pipeline\n\nfrom sklearn.linear_model import Lasso\n\nX = pd.DataFrame(\n    dict(\n        x1=[2, 1, 1, 0, np.nan],\n        x2=[\"a\", np.nan, \"b\", np.nan, \"a\"],\n    )\n)\ny = pd.Series([1, 2, 3, 4, 5])\n\npipe = Pipeline(\n    [\n        (\"drop\", DropMissingData()),\n        (\"enc\", OneHotEncoder()),\n        (\"lasso\", Lasso(random_state=10)),\n    ]\n)\npipe.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Transform X and y Together\nDESCRIPTION: This code demonstrates how to transform both the input DataFrame `X` and the target variable `y` simultaneously, dropping rows with missing values in the lag features and ensuring that the shapes of the transformed `X_tr` and `y_tr` are consistent. It utilizes the `transform_x_y` method of the `LagFeatures` transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nX_tr, y_tr = lag_f.transform_x_y(X, y)\n\nX_tr.shape, y_tr.shape, X.shape, y.shape\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Bin Numbers\nDESCRIPTION: Transforms the training and testing data using the DecisionTreeDiscretiser, which replaces the original values with the bin numbers. Prints the first few rows of the transformed data to display the bin numbers into which the original values have been categorized.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = disc.transform(X_train)\n    test_t = disc.transform(X_test)\n\n    print(train_t[['LotArea', 'GrLivArea']].head())\n```\n\n----------------------------------------\n\nTITLE: Transforming train and test data with AddMissingIndicator - feature-engine\nDESCRIPTION: This code demonstrates how to use the fitted AddMissingIndicator to transform both the training and testing datasets. It calls the `transform` method of the `addBinary_imputer` object, applying the missing indicator transformation to the X_train and X_test DataFrames. The resulting transformed data is stored in train_t and test_t, respectively, and displays the first few rows of the created indicator columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/AddMissingIndicator.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\t# transform the data\n\ttrain_t = addBinary_imputer.transform(X_train)\n\ttest_t = addBinary_imputer.transform(X_test)\n\n\ttrain_t[['Alley_na', 'MasVnrType_na', 'LotFrontage_na', 'MasVnrArea_na']].head()\n```\n\n----------------------------------------\n\nTITLE: PowerTransformer Fit and Transform\nDESCRIPTION: This code snippet shows how to fit and transform data using the PowerTransformer in Feature-engine. It initializes a PowerTransformer object (tf) with the variables 'left_skewed' and 'right_skewed'. The `fit_transform` method is then called on the transformer, which fits the transformer to the data (df_sim) and returns the transformed data, storing it in the variable `df_sim_transformed`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\t# Set up the variable transformer (tf)\n\ttf = PowerTransformer(variables = ['left_skewed', 'right_skewed'])\n\n\t# Fit the transformer\n\tdf_sim_transformed = tf.fit_transform(df_sim)\n```\n\n----------------------------------------\n\nTITLE: Fit OrdinalEncoder to Training Data\nDESCRIPTION: Fits the `OrdinalEncoder` instance to the training data (`X_train`). This step is necessary for the encoder to learn the unique categories within each specified variable and create the mappings for ordinal encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nencoder.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with MeanEncoder - Python\nDESCRIPTION: This code snippet transforms the training (X_train) and testing (X_test) datasets using the fitted MeanEncoder. The transform method replaces the categorical values with their corresponding mean target values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Transform DataFrame with Scaler\nDESCRIPTION: This code snippet applies the transformation learned by the MeanNormalizationScaler to the DataFrame. It scales the specified variables using the calculated mean and range.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/scaling/MeanNormalizationScaler.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# scale the data\ndf = scaler.transform(df)\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Plotting Performance Drifts\nDESCRIPTION: This code snippet creates a bar plot visualizing the performance drifts of each feature along with their standard deviations, helping to identify important features based on the performance improvement they contribute.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n        pd.Series(tr.performance_drifts_),\n        pd.Series(tr.performance_drifts_std_)\n    ], axis=1\n    )\nr.columns = ['mean', 'std']\n\nr['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)\n\nplt.title(\"Performance drift elicited by adding features\")\nplt.ylabel('Mean performance drift')\nplt.xlabel('Features')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Data for WoE Encoding\nDESCRIPTION: This code snippet loads the Titanic dataset using Feature-engine, handles missing values, and splits the data into training and testing sets. It also prints the head of the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import WoEEncoder, RareLabelEncoder\nfrom feature_engine.pipeline import Pipeline\nfrom feature_engine.discretisation import EqualFrequencyDiscretiser\n\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    handle_missing=True,\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for MeanMedianImputer\nDESCRIPTION: Imports necessary libraries including seaborn, matplotlib, scikit-learn datasets, pipeline, model selection, and Feature-engine's MeanMedianImputer and AddMissingIndicator for data analysis and imputation workflows. These libraries are essential for data manipulation, visualization, model building, and missing value imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\timport seaborn as sns\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.datasets import fetch_openml\n\tfrom sklearn.pipeline import make_pipeline\n\tfrom sklearn.model_selection import train_test_split\n\tfrom feature_engine.imputation import MeanMedianImputer\n\tfrom feature_engine.imputation import AddMissingIndicator\n```\n\n----------------------------------------\n\nTITLE: Check Imputer Dictionary (Frequent)\nDESCRIPTION: Accesses the `imputer_dict_` attribute of the fitted CategoricalImputer to view the dictionary containing variables and their corresponding most frequent categories. This shows the mapping of each specified variable to its most frequent value.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimputer.imputer_dict_\n```\n\n----------------------------------------\n\nTITLE: Building a Classification Pipeline - Python\nDESCRIPTION: This code defines a scikit-learn `Pipeline` for classification, incorporating Feature-engine's `OutlierTrimmer` and `OneHotEncoder`, along with scikit-learn's `StandardScaler` and `LogisticRegression`. The `OutlierTrimmer` handles outliers in 'age' and 'fare' features, `OneHotEncoder` encodes categorical variables, `StandardScaler` scales the data, and `LogisticRegression` performs the classification.  The pipeline is instantiated but not yet fitted.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(\n        [\n            (\"outliers\", OutlierTrimmer(variables=[\"age\", \"fare\"])),\n            (\"enc\", OneHotEncoder()),\n            (\"scaler\", StandardScaler()),\n            (\"logit\", LogisticRegression(random_state=10)),\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Transforming Test Data and Targets using the Pipeline - Python\nDESCRIPTION: This code snippet transforms the test data (`X_test`) and target variable (`y_test`) using the fitted pipeline, up to but not including the final estimator (`pipe[:-1]`). This is achieved using the `transform_x_y` method, which applies the feature engineering steps of the pipeline. It also displays the shape of the original and transformed data to illustrate how the pipeline modifies the data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nXt, yt = pipe[:-1].transform_x_y(X_test, y_test)\n\n    X_test.shape, y_test.shape, Xt.shape, yt.shape\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting DecisionTreeDiscretiser in Python\nDESCRIPTION: This code snippet initializes a `DecisionTreeDiscretiser` object from `feature_engine.discretisation`. The `bin_output` is set to \"prediction\" to replace continuous values with decision tree predictions. Cross-validation (`cv`) is set to 3, `scoring` is set to 'neg_mean_squared_error' for regression, and `variables` specifies the columns to be discretized ('LotArea' and 'GrLivArea'). The discretizer is then fit to the training data (X_train, y_train).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.discretisation import DecisionTreeDiscretiser\n\ndisc = DecisionTreeDiscretiser(bin_output=\"prediction\",\n                                 cv=3,\n                                 scoring='neg_mean_squared_error',\n                                 variables=['LotArea', 'GrLivArea'],\n                                 regression=True)\n\ndisc.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed DataFrame with Additions and Removals\nDESCRIPTION: This code displays the head of the transformed DataFrame, showing that the missing columns ('sex', 'age') have been added with NaN values and the extra columns ('var_a', 'var_b') have been removed. This output demonstrates the complete functionality of `MatchVariables` to synchronize the test and train datasets.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nThe following variables are added to the DataFrame: ['age', 'sex']\nThe following variables are dropped from the DataFrame: ['var_b', 'var_a']\n     pclass  survived  sex  age  sibsp  parch     fare cabin embarked\n1000      3         1  NaN  NaN      0      0   7.7500     n        Q\n1001      3         1  NaN  NaN      2      0  23.2500     n        Q\n1002      3         1  NaN  NaN      2      0  23.2500     n        Q\n1003      3         1  NaN  NaN      2      0  23.2500     n        Q\n1004      3         1  NaN  NaN      0      0   7.7875     n        Q\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Load Dataset\nDESCRIPTION: This snippet imports necessary libraries, including matplotlib for plotting, scikit-learn for dataset loading and train-test splitting, and the EqualFrequencyDiscretiser from feature_engine. It loads the Ames House Prices dataset, sets the index, and splits the data into training and testing sets. Dependencies: matplotlib, scikit-learn, feature-engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\nfrom feature_engine.discretisation import EqualFrequencyDiscretiser\n\n# Load dataset\nX, y = fetch_openml(name='house_prices', version=1, return_X_y=True, as_frame=True)\nX.set_index('Id', inplace=True)\n\n# Separate into train and test sets\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=42)\n```\n\n----------------------------------------\n\nTITLE: Combining Datetime Extraction and Subtraction in Python Pipeline\nDESCRIPTION: This code snippet shows how to combine the DatetimeFeatures and DatetimeSubtraction transformers within a Scikit-learn Pipeline. The DatetimeFeatures transformer extracts features (month, year, day of week, etc.) from datetime columns, and then DatetimeSubtraction calculates the difference between specified datetime columns.  The pipeline allows for streamlined and modular feature engineering.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.datetime import DatetimeFeatures, DatetimeSubtraction\n\ndata = pd.DataFrame({\n    \"date1\" : [\"2022-09-01\", \"2022-10-01\", \"2022-12-01\"],\n    \"date2\" : [\"2022-09-15\", \"2022-10-15\", \"2022-12-15\"],\n    \"date3\" : [\"2022-08-01\", \"2022-09-01\", \"2022-11-01\"],\n    \"date4\" : [\"2022-08-15\", \"2022-09-15\", \"2022-11-15\"],\n})\n\ndtf = DatetimeFeatures(variables=[\"date1\", \"date2\"], drop_original=False)\ndts = DatetimeSubtraction(\n    variables=[\"date1\", \"date2\"],\n    reference=[\"date3\", \"date4\"],\n    drop_original=True,\n)\n\npipe = Pipeline([\n    (\"features\", dtf),(\"subtraction\", dts)\n])\n\ndata = pipe.fit_transform(data)\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Splitting Data with Feature-engine\nDESCRIPTION: This code snippet demonstrates how to load the Titanic dataset using Feature-engine's `load_titanic` function, convert the 'pclass' column to an object type, and split the data into training and testing sets for use with the `MatchVariables` transformer. The `predictors_only=True` argument ensures that only predictor variables are loaded, and `cabin=\"letter_only\"` extracts the first letter of cabin.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.preprocessing import MatchVariables\nfrom feature_engine.datasets import load_titanic\n\n# Load dataset\ndata = load_titanic(\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\n\ndata['pclass'] = data['pclass'].astype('O')\n\n# Split test and train\ntrain = data.iloc[0:1000, :]\ntest = data.iloc[1000:, :]\n```\n\n----------------------------------------\n\nTITLE: Adding a Logistic Regression Model to the Pipeline\nDESCRIPTION: This code snippet demonstrates how to add a logistic regression model to the Feature-engine pipeline for prediction after WoE encoding. It trains the pipeline on the training data, makes predictions on the test data, and calculates the accuracy score.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\npipe = Pipeline(\n    [\n        (\"disc\", EqualFrequencyDiscretiser(variables=numerical_features)),\n        (\"rare_label\", RareLabelEncoder(tol=0.1, n_categories=2, variables=all, ignore_format=True)),\n        (\"woe\", WoEEncoder(variables=all)),\n        ('model', LogisticRegression(random_state=0)),\n    ])\n\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Applying MathFeatures with Custom Names\nDESCRIPTION: This code demonstrates assigning custom names to the newly created features using the new_variables_names parameter. It calculates the sum, minimum, and maximum of 'Age' and 'Marks', assigning the names 'sum_vars', 'min_vars', and 'max_vars' to the resulting columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/MathFeatures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntransformer = MathFeatures(\n        variables=[\"Age\", \"Marks\"],\n        func = [\"sum\", \"min\", \"max\"],\n        new_variables_names = [\"sum_vars\", \"min_vars\", \"max_vars\"]\n    )\n\n    df_t = transformer.fit_transform(df)\n\n    print(df_t)\n```\n\n----------------------------------------\n\nTITLE: Create Lag Features and Impute Missing Values\nDESCRIPTION: This code demonstrates how to create lag features and simultaneously impute the missing values (NaNs) introduced by the lagging process. It initializes the `LagFeatures` transformer with `periods=1` and `fill_value=0`, replacing NaN values with 0. It then fits and transforms the input DataFrame `X` to produce `X_tr` with imputed lag features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(periods=1, fill_value=0)\n\nX_tr = lag_f.fit_transform(X)\n\nprint(X_tr.head())\n```\n\n----------------------------------------\n\nTITLE: Forecasting with Pipeline (Python)\nDESCRIPTION: This code makes predictions using the fitted pipeline on the test set (X_test). The pipeline applies the learned transformations and predicts the future energy demand for the specified horizon. The predictions are converted to a DataFrame for easy analysis.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nforecast = pipe.predict(X_test)\n\nforecasts = pd.DataFrame(\n    pipe.predict(X_test),\n    columns=[f\"step_{i+1}\" for i in range(3)]\n\n)\n\nprint(forecasts.head())\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Fitting MeanMedianImputer\nDESCRIPTION: Initializes `MeanMedianImputer` with the 'mean' imputation strategy and specifies the 'LotFrontage' and 'MasVnrArea' variables for imputation. The imputer is then fitted to the training data (`X_train`) to learn the mean values for these variables. This step is crucial for preparing the imputer to transform the data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\t# Set up the imputer\n\tmmi = MeanMedianImputer(\n\t\timputation_method='mean',\n\t\tvariables=['LotFrontage', 'MasVnrArea']\n\t)\n\n\t# Fit transformer with training data\n\tmmi.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Checking Shape of Transformed Data in Python\nDESCRIPTION: This snippet prints the shape of the original training data (X_train) and the transformed training data (train_t). This allows for comparison to determine the number of rows removed due to outlier trimming.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nX_train.shape, train_t.shape\n```\n\nLANGUAGE: python\nCODE:\n```\n((916, 8), (764, 8))\n```\n\n----------------------------------------\n\nTITLE: Fitting and Transforming with DropCorrelatedFeatures - Python\nDESCRIPTION: This snippet demonstrates fitting and transforming a DataFrame using the DropCorrelatedFeatures transformer.  The `fit_transform()` method finds and removes correlated features based on the specified threshold. The input is a pandas DataFrame, and the output is a transformed DataFrame with the correlated features removed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nXt = tr.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Fitting SelectBySingleFeaturePerformance - Python\nDESCRIPTION: This snippet demonstrates fitting the SelectBySingleFeaturePerformance transformer to the input features (X) and target variable (y). The fit method trains a separate model for each feature and determines the performance based on the specified scoring metric. After fitting, the transformer identifies and stores the features to drop.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# fit transformer\nsel.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Displaying Correlated Feature Sets - Python\nDESCRIPTION: This snippet shows an example of the output from `tr.correlated_feature_sets_`. It displays a list of sets, where each set represents a group of correlated features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[{'var_0', 'var_8'}, {'var_4', 'var_6', 'var_7', 'var_9'}]\n```\n\n----------------------------------------\n\nTITLE: Transforming Data\nDESCRIPTION: This code transforms both the training (X_train) and testing (X_test) datasets using the fitted RareLabelEncoder.  The transformation replaces infrequent categories with the specified 'Rare' category.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# transform the data\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Example of Unique Categories Identified by OneHotEncoder\nDESCRIPTION: This is an example output showing the unique categories for each identified categorical variable. It shows the values that each column can take, which will become the one-hot encoded columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n{'sex': ['female'],\n 'cabin': ['M', 'E', 'C', 'D', 'B', 'A', 'F', 'T'],\n 'embarked': ['S', 'C', 'Q']}\n```\n\n----------------------------------------\n\nTITLE: Accessing Duplicated Features\nDESCRIPTION: This snippet shows how to access the `features_to_drop_` attribute of the fitted DropDuplicateFeatures transformer. This attribute contains the set of feature names that will be dropped during the transform step because they are duplicates.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropDuplicateFeatures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntransformer.features_to_drop_\n```\n\nLANGUAGE: python\nCODE:\n```\n{'age_dup', 'sex_dup', 'sibsp_dup'}\n```\n\n----------------------------------------\n\nTITLE: Lag Features with Multiple Freq and Drop Original in Python\nDESCRIPTION: This snippet demonstrates how to create multiple lag features with different datetime frequencies and drop the original feature after lagging, using the LagFeatures transformer in Feature Engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(variables=\"irradiation\",\n                    freq=[\"30min\", \"45min\"],\n                    drop_original=True,\n                    )\n\nX_tr = lag_f.fit_transform(X)\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Create Pandas DataFrame for Lag Features Example\nDESCRIPTION: This code snippet creates a Pandas DataFrame with numerical, categorical variables, and a datetime index to demonstrate the LagFeatures transformer.  It also creates a Pandas Series as a target variable.  The dataframe and series will be used in the subsequent examples to show how to add lag features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nX = {\"ambient_temp\": [31.31, 31.51, 32.15, 32.39, 32.62, 32.5, 32.52, 32.68],\n     \"module_temp\": [49.18, 49.84, 52.35, 50.63, 49.61, 47.01, 46.67, 47.52],\n     \"irradiation\": [0.51, 0.79, 0.65, 0.76, 0.42, 0.49, 0.57, 0.56],\n     \"color\": [\"green\"] * 4 + [\"blue\"] * 4,\n     }\n\nX = pd.DataFrame(X)\nX.index = pd.date_range(\"2020-05-15 12:00:00\", periods=8, freq=\"15min\")\ny = pd.Series([1, 2, 3, 4, 5, 6, 7, 8])\ny.index = X.index\n\nX.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing and using RecursiveFeatureElimination with Linear Regression in Python\nDESCRIPTION: This code snippet demonstrates how to use the RecursiveFeatureElimination class from the feature_engine library to perform feature selection using a Linear Regression model. It loads the diabetes dataset from scikit-learn, initializes a LinearRegression estimator and a RecursiveFeatureElimination transformer, fits the transformer to the data, and transforms the data to keep only the selected features. It requires the scikit-learn and feature-engine libraries.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureElimination.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom feature_engine.selection import RecursiveFeatureElimination\n\n# load dataset\nX, y = load_diabetes(return_X_y=True, as_frame=True)\n\nprint(X.head())\n```\n\nLANGUAGE: python\nCODE:\n```\n# initialize linear regresion estimator\nlinear_model = LinearRegression()\n\n# initialize feature selector\ntr = RecursiveFeatureElimination(estimator=linear_model, scoring=\"r2\", cv=3)\n```\n\nLANGUAGE: python\nCODE:\n```\nXt = tr.fit_transform(X, y)\nprint(Xt.head())\n```\n\nLANGUAGE: python\nCODE:\n```\n# get the initial linear model performance, using all features\ntr.initial_model_performance_\n```\n\nLANGUAGE: python\nCODE:\n```\ntr.feature_importances_\n```\n\nLANGUAGE: python\nCODE:\n```\ntr.feature_importances_std_\n```\n\nLANGUAGE: python\nCODE:\n```\n# Get the performance drift of each feature\ntr.performance_drifts_\n```\n\nLANGUAGE: python\nCODE:\n```\n# Get the performance drift of each feature\ntr.performance_drifts_std_\n```\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n    pd.Series(tr.performance_drifts_),\n    pd.Series(tr.performance_drifts_std_)\n], axis=1\n)\nr.columns = ['mean', 'std']\n\nr['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)\n\nplt.title(\"Performance drift elicited by adding features\")\nplt.ylabel('Mean performance drift')\nplt.xlabel('Features')\nplt.show()\n```\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n    tr.feature_importances_,\n    tr.feature_importances_std_,\n], axis=1\n)\nr.columns = ['mean', 'std']\n\nr['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)\n\nplt.title(\"Feature importance derived from the linear regression\")\nplt.ylabel('Coefficients value')\nplt.xlabel('Features')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Access Encoding Mappings\nDESCRIPTION: Accesses and displays the `encoder_dict_` attribute of the fitted `OrdinalEncoder` instance. This dictionary contains the mapping of each category in the specified variables to its corresponding ordinal integer value, which the encoder has learned from the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Shifting Rolling Window Features with Pandas\nDESCRIPTION: This snippet extends the previous one by shifting the calculated window features forward in time. Shifting is crucial in time series forecasting to use only past values when predicting future values.  The shift(period=1) moves the calculated values one step forward.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX[[\"var_1\", \"var_2\"].rolling(window=3).agg([\"max\", \"mean\"]).shift(period=1)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Selected Features - Python\nDESCRIPTION: This snippet demonstrates how to transform the original feature set (X) by dropping the features that were identified as low-performing by the SelectBySingleFeaturePerformance transformer. The transform method returns a new DataFrame (Xt) containing only the selected features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# drop variables\nXt = sel.transform(X)\n```\n\n----------------------------------------\n\nTITLE: Datetime Subtraction with Multiple Variables\nDESCRIPTION: This code snippet demonstrates how to subtract multiple datetime variables simultaneously using Feature-engine's `DatetimeSubtraction`. It subtracts 'date3' and 'date4' from 'date1' and 'date2', respectively. The input dates are strings which are automatically converted to datetimes.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.datetime import DatetimeSubtraction\n\ndata = pd.DataFrame({\n    \"date1\" : [\"2022-09-01\", \"2022-10-01\", \"2022-12-01\"],\n    \"date2\" : [\"2022-09-15\", \"2022-10-15\", \"2022-12-15\"],\n    \"date3\" : [\"2022-08-01\", \"2022-09-01\", \"2022-11-01\"],\n    \"date4\" : [\"2022-08-15\", \"2022-09-15\", \"2022-11-15\"],\n})\n\ndtf = DatetimeSubtraction(variables=[\"date1\", \"date2\"], reference=[\"date3\", \"date4\"])\n\ndata = dtf.fit_transform(data)\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data\nDESCRIPTION: This snippet demonstrates how to transform the training and testing data using the fitted EqualFrequencyDiscretiser. The transform() method applies the learned bin boundaries to convert continuous values into discrete bins. Dependencies: Fitted EqualFrequencyDiscretiser.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Transform the data\ntrain_t = disc.transform(X_train)\ntest_t = disc.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Load Dataset\nDESCRIPTION: Imports necessary libraries, including matplotlib, scikit-learn, and Feature-engine's CategoricalImputer. Loads the Ames house prices dataset and splits it into training and testing sets. Required libraries are matplotlib, scikit-learn, and feature-engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\nfrom feature_engine.imputation import CategoricalImputer\n\ndata = fetch_openml(name='house_prices', as_frame=True)\ndata = data.frame\n\nX = data.drop(['SalePrice', 'Id'], axis=1)\ny = data['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Access encoder dictionary\nDESCRIPTION: This code snippet accesses the `encoder_dict_` attribute of the fitted `MeanEncoder` object. This dictionary stores the calculated mean target value for each category of each encoded variable. This dictionary shows the learned mappings from categories to target means.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Creating Lag and Window Features for Time Series - Python\nDESCRIPTION: This snippet demonstrates how to create lag and window features for time series data using Feature-engine's `LagFeatures` and `WindowFeatures` transformers. `LagFeatures` creates lagged versions of the 'demand' variable, while `WindowFeatures` calculates the mean of the 'demand' variable over a 3-hour window with a 1-hour frequency. The `missing_values` parameter is set to 'ignore' and `drop_na` is set to True to handle missing values and drop rows with NaN values resulting from the lags and window calculations.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlagf = LagFeatures(\n        variables=[\"demand\"],\n        periods=[1, 2, 3, 4, 5, 6],\n        missing_values=\"ignore\",\n        drop_na=True,\n    )\n\n\n    winf = WindowFeatures(\n        variables=[\"demand\"],\n        window=[\"3h\"],\n        freq=\"1h\",\n        functions=[\"mean\"],\n        missing_values=\"ignore\",\n        drop_original=True,\n        drop_na=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed DataFrame after Column Addition\nDESCRIPTION: This code displays the head of the transformed DataFrame (`test_tt`). The output confirms that the columns 'age' and 'sex' were added back to the DataFrame, with missing values filled with NaN. This ensures that the transformed DataFrame has the same columns as the original training set.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nThe following variables are added to the DataFrame: ['age', 'sex']\n     pclass  survived  sex  age  sibsp  parch     fare cabin embarked\n1000      3         1  NaN  NaN      0      0   7.7500     n        Q\n1001      3         1  NaN  NaN      2      0  23.2500     n        Q\n1002      3         1  NaN  NaN      2      0  23.2500     n        Q\n1003      3         1  NaN  NaN      2      0  23.2500     n        Q\n1004      3         1  NaN  NaN      0      0   7.7875     n        Q\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Train/Test Split\nDESCRIPTION: This code snippet loads the Titanic dataset, preprocesses it, and splits it into training and testing sets using scikit-learn's train_test_split function. It also imports the necessary modules from feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import RareLabelEncoder\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    handle_missing=True,\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\nX[\"pclass\"] = X[\"pclass\"].astype(\"O\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Initializing MRMR with Random Forests (RFCQ) - Python\nDESCRIPTION: This code initializes the MRMR feature selection using random forests for relevance and Pearson correlation for redundancy (RFCQ method). It defines a hyperparameter grid for random forest optimization and specifies cross-validation, a scoring metric (`roc_auc`), and a random state for reproducibility. It then fits the selector to the data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsel = MRMR(\n    method=\"RFCQ\",\n    scoring=\"roc_auc\",\n    param_grid = {\"n_estimators\": [5, 50, 500], \"max_depth\":[1,2,3]},\n    cv=3,\n    regression=False,\n    random_state=42,\n)\n\nsel.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Initializing OneHotEncoder for Frequent Category Encoding in Python\nDESCRIPTION: This snippet initializes a OneHotEncoder instance to encode only the most frequent categories (top 2 in this case) in specified categorical columns. The `top_categories` parameter is set to 2, the `variables` parameter specifies the columns to encode, and `ignore_format` is set to True. The purpose is to reduce the number of dummy variables created, which can be useful for high cardinality features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nohe = OneHotEncoder(\n    top_categories=2,\n    variables=['pclass', 'cabin', 'embarked'],\n    ignore_format=True,\n    )\n\ntrain_t = ohe.fit_transform(X_train)\ntest_t = ohe.transform(X_test)\n\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Performing GridSearchCV for Hyperparameter Optimization - Python\nDESCRIPTION: This code performs a `GridSearchCV` to find the best hyperparameters for the pipeline. It uses the defined `param_grid` to search through different combinations of hyperparameter values, evaluating the pipeline's performance using 2-fold cross-validation (`cv=2`).  The `refit=False` argument prevents the grid search from refitting the pipeline with the best parameters on the entire training set after the search is complete. It fits the grid search to the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ngrid = GridSearchCV(\n        pipe,\n        param_grid=param_grid,\n        cv=2,\n        refit=False,\n    )\n\n    grid.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Fitting the SelectByTargetMeanPerformance Transformer - Python\nDESCRIPTION: This code snippet fits the SelectByTargetMeanPerformance transformer to the training data (X_train and y_train). This step involves replacing categories and bins with target means, calculating the ROC-AUC for each transformed variable, and selecting features based on the specified criteria.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsel.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Inverse transform to revert data\nDESCRIPTION: This code performs inverse transformation on the test set (`test_t`) using the fitted DecisionTreeEncoder. It converts the encoded numerical values back to their original categorical representations.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrevert = encoder.inverse_transform(test_t)\nrevert[['cabin', 'pclass', 'embarked']].head(10)\n```\n\n----------------------------------------\n\nTITLE: Scatter Plot of Hour vs Sine and Cosine of Hour\nDESCRIPTION: This code creates a scatter plot of the 'hour' variable against both its sine ('hour_sin') and cosine ('hour_cos') transformations. This demonstrates how using both sine and cosine functions together breaks the symmetry and assigns a unique codification to each hour.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nplt.scatter(df[\"hour\"], df[\"hour_sin\"])\nplt.scatter(df[\"hour\"], df[\"hour_cos\"])\n```\n\n----------------------------------------\n\nTITLE: Imputing NaN Values using Pipeline with ExpandingWindowFeatures in Python\nDESCRIPTION: This code snippet demonstrates how to impute NaN values in the expanding window features using a pipeline with Feature-engine's `MeanMedianImputer`. It initializes `ExpandingWindowFeatures` and `MeanMedianImputer`, combines them in a `Pipeline`, and then applies the pipeline to transform the data. The `MeanMedianImputer` replaces NaN values with the median of each feature. The resulting dataframe is then printed to show the imputed values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.imputation import MeanMedianImputer\nfrom feature_engine.pipeline import Pipeline\n\nwin_f = ExpandingWindowFeatures(functions=[\"mean\", \"std\"])\n\npipe = Pipeline([\n    (\"windows\", win_f),\n    (\"imputer\", MeanMedianImputer(imputation_method=\"median\"))\n])\n\nX_tr = pipe.fit_transform(X, y)\n\nprint(X_tr.head())\n```\n\n----------------------------------------\n\nTITLE: Getting Support (Selected Features) - Python\nDESCRIPTION: This snippet demonstrates how to retrieve a boolean mask indicating which features were selected by MRMR. `sel.get_support()` returns an array of booleans, where `True` corresponds to a selected feature and `False` to a dropped feature.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsel.get_support()\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Importances\nDESCRIPTION: This code snippet generates a bar plot of the feature importances derived from the Linear Regression model, including the standard deviations, for comparison with the performance drift plot.  This helps in understanding which features have high coefficients and how those relate to model performance changes.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n        tr.feature_importances_,\n        tr.feature_importances_std_,\n    ], axis=1\n    )\nr.columns = ['mean', 'std']\n\nr['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)\n\nplt.title(\"Feature importance derived from the linear regression\")\nplt.ylabel('Coefficients value')\nplt.xlabel('Features')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing SelectBySingleFeaturePerformance - Python\nDESCRIPTION: This snippet initializes the SelectBySingleFeaturePerformance class from the feature_engine.selection module. It sets up the feature selector to use LinearRegression as the estimator, 'r2' as the scoring metric, 3-fold cross-validation, and a threshold of 0.01 for feature selection. Features with an R2 score below this threshold will be dropped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# initialize feature selector\nsel = SelectBySingleFeaturePerformance(\n        estimator=LinearRegression(), scoring=\"r2\", cv=3, threshold=0.01)\n```\n\n----------------------------------------\n\nTITLE: Dropping NA values using Feature-engine DropMissingData\nDESCRIPTION: This code snippet demonstrates how to drop rows containing missing values (NaN) from a Pandas DataFrame using the `DropMissingData` transformer from Feature-engine. It imports the necessary libraries, creates a DataFrame with NaN values, initializes `DropMissingData`, and then transforms the DataFrame to remove rows with NaN values.  The transformer adheres to scikit-learn's `fit` and `transform` API.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom feature_engine.imputation import DropMissingData\n\nX = pd.DataFrame(dict(\n       x1 = [np.nan,1,1,0,np.nan],\n       x2 = [\"a\", np.nan, \"b\", np.nan, \"a\"],\n       ))\n\ndmd = DropMissingData()\ndmd.fit(X)\ndmd.transform(X)\n```\n\n----------------------------------------\n\nTITLE: RareLabelEncoder with max_n_categories\nDESCRIPTION: This code initializes and applies the RareLabelEncoder with the `max_n_categories` parameter set to 2. It groups the categories, retaining only the 2 most frequent ones (A and B) and grouping the rest into the 'Rare' group. The value counts after the transformation are then printed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrare_encoder = RareLabelEncoder(tol=0.05, n_categories=3, max_n_categories=2)\nXt = rare_encoder.fit_transform(data)\nXt['var_A'].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Inverse Transforming Data\nDESCRIPTION: This code snippet performs the inverse Box-Cox transformation on the transformed training and testing datasets (train_t and test_t) using the fitted BoxCoxTransformer. This reverts the variables back to their original scale, creating train_unt and test_unt.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_unt = boxcox.inverse_transform(train_t)\ntest_unt = boxcox.inverse_transform(test_t)\n```\n\n----------------------------------------\n\nTITLE: Accessing WoE Mapping - Python\nDESCRIPTION: This code snippet shows how to access the learned WoE mappings for each category of the encoded variables. The `encoder_dict_` attribute of the `WoEEncoder` stores a dictionary where keys are the variable names and values are dictionaries mapping categories to their corresponding WoE values. This allows for inspecting the WoE values assigned to each category after fitting the encoder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwoe_encoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Initializing ProbeFeatureSelection with Multiple Probes - Python\nDESCRIPTION: This code initializes and fits a ProbeFeatureSelection transformer with multiple probe features (`n_probes=1`) and different distributions. It configures the transformer with a RandomForestClassifier estimator, precision scoring, and cross-validation to select features by comparing their importance to that of random probes.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nsel = ProbeFeatureSelection(\n    estimator=RandomForestClassifier(),\n    variables=None,\n    scoring=\"precision\",\n    n_probes=1,\n    distribution=\"all\",\n    cv=5,\n    random_state=150,\n    confirm_variables=False\n)\n\nsel.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Adding Missing Indicators with AddMissingIndicator\nDESCRIPTION: Adding missing indicators involves creating binary variables to denote missing values (0 for observed, 1 for missing). The :class:`AddMissingIndicator()` class in Feature-engine performs this task. This method doesn't replace the missing data itself, it provides information about its presence. It is typically used in conjunction with other imputation techniques like mean-median or frequent category imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/api_doc/imputation/index.rst#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Transforming Pandas Series: WindowFeatures\nDESCRIPTION: This example demonstrates how to use WindowFeatures with a pandas Series.  The Series is first converted to a DataFrame using the `.to_frame()` method. The WindowFeatures class is then used to generate windowed features from this DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwin_f = WindowFeatures(\n    window=[\"45min\"],\n    functions=[\"mean\", \"max\"],\n    freq=\"30min\",\n)\n\nX_tr = win_f.fit_transform(X['ambient_temp'].to_frame())\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Transforming DataFrame with Column Addition and Removal\nDESCRIPTION: This code applies the `MatchVariables` transformer to the `test_t` DataFrame, which now has both missing ('sex', 'age') and extra columns ('var_a', 'var_b'). The transformer adds the missing columns with NaN values and removes the extra columns, aligning the DataFrame with the structure of the original training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntest_tt = match_cols.transform(test_t)\n\ntest_tt.head()\n```\n\n----------------------------------------\n\nTITLE: Accessing Encoder Dictionary in Python\nDESCRIPTION: This code snippet demonstrates how to access the learned encodings from the OrdinalEncoder. The `encoder_dict_` attribute stores the mappings between the original categories and the encoded ordinal values. This dictionary can be inspected to understand how each category was encoded.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Visualize transformed data distribution\nDESCRIPTION: This code generates histograms of the log-transformed 'LotArea' and 'GrLivArea' columns from the transformed training dataset (train_t) to visualize the effect of the transformation.  It aims to show that the log transformation makes the variables approximate a normal distribution.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogTransformer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[['LotArea', 'GrLivArea']].hist(figsize=(10,5))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using RecursiveFeatureAddition\nDESCRIPTION: This code snippet initializes a Linear Regression model and uses RecursiveFeatureAddition to select features based on the r2 score. It fits and transforms the data, then prints the head of the transformed DataFrame showing only the selected features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# initialize linear regression estimator\nlinear_model = LinearRegression()\n\n# initialize feature selector\ntr = RecursiveFeatureAddition(estimator=linear_model, scoring=\"r2\", cv=3)\n\nXt = tr.fit_transform(X, y)\nprint(Xt.head())\n```\n\n----------------------------------------\n\nTITLE: Making Predictions\nDESCRIPTION: This snippet shows how to make predictions using the trained Feature-engine Pipeline with a logistic regression model. It demonstrates predicting on the training data and displaying the first 10 predictions.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\npreds = pipe.predict(X_train)\n\n    preds[0:10]\n```\n\n----------------------------------------\n\nTITLE: Pipeline with Estimator\nDESCRIPTION: Creates a Feature-engine pipeline including an estimator (Lasso regression). The pipeline consists of dropping missing values with `DropMissingData`, encoding categorical variables with `OrdinalEncoder`, and finally applying `Lasso` regression. This demonstrates how to incorporate a machine learning model into a Feature-engine pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import Lasso\n\nfrom feature_engine.imputation import DropMissingData\nfrom feature_engine.encoding import OrdinalEncoder\nfrom feature_engine.pipeline import Pipeline\n\ndf = pd.DataFrame(\n    dict(\n        x1=[2, 1, 1, 0, np.nan],\n        x2=[\"a\", np.nan, \"b\", np.nan, \"a\"],\n        x3=[2, 3, 4, 5, 5],\n    )\n)\ny = pd.Series([1, 2, 3, 4, 5])\n\npipe = Pipeline(\n    [\n        (\"drop\", DropMissingData()),\n        (\"enc\", OrdinalEncoder(encoding_method=\"arbitrary\")),\n        (\"lasso\", Lasso(random_state=2))\n    ]\n)\n\npipe.fit(df, y)\npipe.predict(df)\n```\n\n----------------------------------------\n\nTITLE: Transforming data with ArcsinTransformer - Python\nDESCRIPTION: This code snippet applies the fitted `ArcsinTransformer` to transform both the training and testing datasets.  It uses the transform method of the ArcsinTransformer object.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/ArcsinTransformer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# transform the data\ntrain_t = tf.transform(X_train)\ntest_t = tf.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Get Feature Names After OneHotEncoding\nDESCRIPTION: This code retrieves the feature names after the second step of the pipeline, which is OneHotEncoder. It uses the get_feature_names_out() method on the pipeline sliced to include the first two steps.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe[:2].get_feature_names_out()\n```\n\nLANGUAGE: python\nCODE:\n```\n['x1', 'x2_a', 'x2_b']\n```\n\n----------------------------------------\n\nTITLE: Getting Performance Drifts of Each Feature\nDESCRIPTION: This code snippet retrieves the performance drifts associated with each feature, representing the change in model performance when each feature is added to the model during the recursive addition process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Get the performance drift of each feature\ntr.performance_drifts_\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Splitting into Train/Test Sets in Python\nDESCRIPTION: This code snippet loads the Titanic dataset using feature_engine, splits it into training and testing sets using scikit-learn, and prints the head of the training set. It imports necessary modules and configures the dataset appropriately.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.outliers import OutlierTrimmer\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    predictors_only=True,\n    handle_missing=True,\n)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Loading data and splitting into train/test sets with pandas and sklearn\nDESCRIPTION: This code snippet loads the Ames house prices dataset using scikit-learn's `fetch_openml` function, then splits the data into training and testing sets using `train_test_split`. It imports necessary libraries like numpy, pandas, and sklearn.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/YeoJohnsonTransformer.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.transformation import YeoJohnsonTransformer\n\n# Load dataset\ndata = fetch_openml(name='house_prices', as_frame=True)\ndata = data.frame\n\n# Separate into train and test sets\nX_train, X_test, y_train, y_test =  train_test_split(\n            data.drop(['Id', 'SalePrice'], axis=1),\n            data['SalePrice'], test_size=0.3, random_state=0)\n\nX_train.head()\n```\n\n----------------------------------------\n\nTITLE: Pipeline with Logistic Regression\nDESCRIPTION: This snippet demonstrates creating a Feature-engine Pipeline that includes outlier trimming, one-hot encoding, and a logistic regression model. This shows how to integrate outlier removal as a preprocessing step within a complete machine learning pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import LogisticRegression\n\n    pipe = Pipeline(\n        [\n            (\"outliers\", ot),\n            (\"enc\", OneHotEncoder()),\n            (\"logit\", LogisticRegression(random_state=10)),\n        ]\n    )\n\n    pipe.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed Dataframe after Ordered Encoding in Python\nDESCRIPTION: This code snippet prints the first few rows of the transformed training dataframe `X_train_t` after applying the ordered ordinal encoding. This allows verification that the 'HouseAgeCategorical' variable has been replaced with numerical encodings based on the order derived from the target variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(X_train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Correlated Feature Sets after Model Performance Selection in Python\nDESCRIPTION: This code snippet accesses the correlated_feature_sets_ attribute of the SmartCorrelatedSelection object after using 'model_performance' as the selection method.  It shows the correlated groups of features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntr.correlated_feature_sets_\n```\n\n----------------------------------------\n\nTITLE: Example WoE Mapping Dictionary\nDESCRIPTION: This is an example dictionary showcasing the structure of the `encoder_dict_` attribute in Feature-engine's `WoEEncoder`. It demonstrates how WoE values are stored for each category within the specified variables. This dictionary provides a clear view of the mapping between original categories and their corresponding WoE values, aiding in understanding the impact of each category on the target variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n{'cabin': {'M': -0.35752781962490193, 'Rare': 1.083797390800775},\n 'pclass': {'1': 0.9453018143294478,\n  '2': 0.21009172435857942,\n  '3': -0.5841726684724614},\n 'embarked': {'C': 0.679904786667102,\n  'Rare': 0.012075414091446468,\n  'S': -0.20113381737960143}}\n```\n\n----------------------------------------\n\nTITLE: Dropping NA Values From All Columns (missing_only=False)\nDESCRIPTION: This code snippet demonstrates how to force `DropMissingData` to remove rows with NA values across all columns, regardless of whether they had NA values during the fit stage. This is achieved by setting the `missing_only` parameter to `False` during initialization. The `fit_transform` method is used to fit and transform the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndmd =  DropMissingData(missing_only=False)\nXt = dmd.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Fitting and Transforming data with the pipeline\nDESCRIPTION: This code snippet fits the feature engineering pipeline to the training data and then transforms both training and testing sets using the fitted pipeline. The first few rows of the transformed training set are then printed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nX_trans_t = pipe.fit_transform(X_train, y_train)\n\nprint(X_trans_t.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Correlated Feature Dictionary - Python\nDESCRIPTION: Accesses the `correlated_feature_dict_` attribute of the `SmartCorrelatedSelection` transformer to identify the features retained as keys after correlation analysis. The dictionary maps retained features to sets of correlated features that were dropped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntr.correlated_feature_dict_\n```\n\n----------------------------------------\n\nTITLE: Checking Data Types of Transformed Variables\nDESCRIPTION: This snippet displays the data types of the transformed variables.  By default, the EqualFrequencyDiscretiser returns integer-encoded bins. Dependencies: pandas DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[TARGET_NUMERIC_FEATURES].dtypes\n```\n\n----------------------------------------\n\nTITLE: Pipeline Fitting (Python)\nDESCRIPTION: This snippet fits the constructed pipeline to the training data (X_train, y_train).  This step trains the `LagFeatures`, `WindowFeatures` and the `MultiOutputRegressor` to learn the relationships between lagged features, window features, and future demand.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npipe.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop - Python\nDESCRIPTION: This snippet shows how to access the list of features that the ProbeFeatureSelection transformer has identified as non-informative and will drop from the dataset. It relies on the `.features_to_drop_` attribute of a fitted ProbeFeatureSelection object.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsel.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Load and split the Titanic dataset\nDESCRIPTION: This code snippet loads the Titanic dataset using Feature-engine's `load_titanic` function and splits it into training and test sets using `train_test_split` from scikit-learn. Missing values are handled during the dataset loading process, and only predictor variables are considered.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX, y = load_titanic(\n    return_X_y_frame=True,\n    handle_missing=True,\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: StringSimilarityEncoder Fit and Transform\nDESCRIPTION: This snippet showcases the application of StringSimilarityEncoder to transform a DataFrame.  It initializes the encoder, then fits and transforms a DataFrame, converting the categorical feature 'words' into numerical features based on string similarity. It shows how to apply the encoder to create new features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/StringSimilarityEncoder.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nencoder =  StringSimilarityEncoder()\ndft = encoder.fit_transform(df)\ndft\n```\n\n----------------------------------------\n\nTITLE: Transforming Predictors and Target Variables with OutlierTrimmer in Python\nDESCRIPTION: This code snippet applies the fitted `OutlierTrimmer` to both the predictor (X_train, X_test) and target (y_train, y_test) variables, ensuring that the target variable is aligned with the transformed predictor variables after outlier removal. It returns transformed predictor and target variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrain_t, y_train_t = ot.transform_x_y(X_train, y_train)\ntest_t, y_test_t = ot.transform_x_y(X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Discretization Returning Bin Boundaries\nDESCRIPTION: This snippet demonstrates how to configure the `EqualFrequencyDiscretiser` to return the bin boundaries instead of integer labels by setting the `return_boundaries` parameter to `True`.  It then transforms the test set and prints the head.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Set up the discretization transformer\ndisc = EqualFrequencyDiscretiser(\n    bins=10,\n    variables=TARGET_NUMERIC_FEATURES,\n    return_boundaries=True)\n\n# Fit the transformer\ndisc.fit(X_train)\n\n# Transform test set & visualize limit\ntest_t = disc.transform(X_test)\n\n# Visualize output (boundaries)\nprint(test_t[TARGET_NUMERIC_FEATURES].head())\n```\n\nLANGUAGE: Python\nCODE:\n```\n\t\t     LotArea         GrLivArea\n\tId                                     \n\t893   (-inf, 22694.5]   (864.8, 1395.6]\n\t1106  (-inf, 22694.5]  (2457.2, 2988.0]\n\t414   (-inf, 22694.5]   (864.8, 1395.6]\n\t523   (-inf, 22694.5]  (1395.6, 1926.4]\n\t1037  (-inf, 22694.5]  (1395.6, 1926.4]\n```\n\n----------------------------------------\n\nTITLE: Datetime Subtraction with Feature-engine\nDESCRIPTION: This code snippet demonstrates how to use the `DatetimeSubtraction` transformer from Feature-engine to subtract 'date2' from 'date1' and express the difference in years. It initializes the transformer with the desired variables and output unit, then applies it to the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.datetime import DatetimeSubtraction\n\ndata = pd.DataFrame({\n    \"date1\": pd.date_range(\"2019-03-05\", periods=5, freq=\"D\"),\n    \"date2\": pd.date_range(\"2018-03-05\", periods=5, freq=\"W\")})\n\ndtf = DatetimeSubtraction(\n    variables=\"date1\",\n    reference=\"date2\",\n    output_unit=\"Y\")\n\ndata = dtf.fit_transform(data)\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Automatically Impute All Categorical Variables\nDESCRIPTION: Initializes and fits a CategoricalImputer to automatically find and impute all categorical features in the training dataset. The `fit_transform` method fits the imputer and transforms the training data in one step. The `transform` method then transforms the test dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimputer = CategoricalImputer(\n    variables=None,\n)\n\ntrain_t = imputer.fit_transform(X_train)\ntest_t = imputer.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Checking maximum values after transformation\nDESCRIPTION: This code snippet checks the maximum values of the 'fare' and 'age' columns in the transformed training data. It verifies that the maximum values are capped at the specified values (200 for 'fare' and 50 for 'age').\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/ArbitraryOutlierCapper.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[['fare', 'age']].max()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting DecisionTreeEncoder (Feature-engine)\nDESCRIPTION: This code initializes a `DecisionTreeEncoder` from Feature-engine to encode the 'MSZoning' and 'LotShape' features based on a decision tree trained on the target variable.  The `regression=True` parameter specifies that this is a regression task.  The `cv=3` parameter specifies 3-fold cross-validation. `ignore_format=True` disables format validation. `precision=0` sets the precision to 0 decimal places. Then the encoder is fitted to the training data using `encoder.fit(X_train, y_train)`. Dependencies include `feature_engine.encoding.DecisionTreeEncoder`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nencoder = DecisionTreeEncoder(\n    variables=[\"MSZoning\", 'LotShape'],\n    regression=True,\n    cv=3,\n    random_state=0,\n    ignore_format=True,\n    precision=0,\n)\n\nencoder.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names After Transformation\nDESCRIPTION: This code snippet demonstrates how to retrieve the names of the features in the transformed data using the `get_feature_names_out()` method from a Feature-engine feature selection object. It returns a list of the feature names present in the transformed DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsel.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Fitting and Transforming with OrdinalEncoder in Python\nDESCRIPTION: This code snippet initializes and fits an OrdinalEncoder to encode the 'pclass' variable using arbitrary mapping. The `ignore_format=True` parameter allows encoding of numerical variables. The `fit_transform()` method learns the mappings from the training data `X_train` and then transforms it. The resulting encodings are stored within the encoder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nencoder = OrdinalEncoder(\n    encoding_method='arbitrary',\n    variables=['pclass'],\n    ignore_format=True)\n\ntrain_t = encoder.fit_transform(X_train)\n```\n\n----------------------------------------\n\nTITLE: Displaying First Rows of Transformed DataFrame\nDESCRIPTION: This code snippet displays the first 3 rows of the transformed DataFrame `df_sim_transformed` using the `head()` method.  It shows the effect of the PowerTransformer on the original data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\tdf_sim_transformed.head(3)\n```\n\n----------------------------------------\n\nTITLE: Impute with Arbitrary String\nDESCRIPTION: Initializes and fits a CategoricalImputer to impute missing values in 'Alley' and 'MasVnrType' with the string 'missing'. The `fit` method learns the replacement value. Requires the data and CategoricalImputer to be initialized.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimputer = CategoricalImputer(\n    variables=['Alley', 'MasVnrType'],\n    fill_value=\"missing\",\n)\n\nimputer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Initialize and fit MeanEncoder\nDESCRIPTION: This code snippet initializes the `MeanEncoder` class from the `feature_engine.encoding` module, specifying the categorical variables to be encoded. The encoder is then fit to the training data (`X_train`, `y_train`) to learn the target mean for each category within the specified variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nencoder = MeanEncoder(\n    variables=['cabin', 'sex', 'embarked'],\n)\n\nencoder.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Applying MathFeatures with Numpy Functions\nDESCRIPTION: This snippet demonstrates using numpy functions (np.sum, np.prod, np.min, np.max, np.std) directly within the MathFeatures transformer. It performs the same calculations as the previous example but uses numpy functions instead of string representations.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/MathFeatures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntransformer = MathFeatures(\n        variables=[\"Age\", \"Marks\"],\n        func = [np.sum, np.prod, np.min, np.max, np.std],\n    )\n\n    df_t = transformer.fit_transform(df)\n\n    print(df_t)\n```\n\n----------------------------------------\n\nTITLE: Initializing DecisionTreeDiscretiser with Bin Numbers\nDESCRIPTION: Initializes the DecisionTreeDiscretiser to output bin numbers (ordinal values).  It uses cross-validation with 3 folds and negative mean squared error as the scoring metric. The specified variables 'LotArea' and 'GrLivArea' will be discretized using regression.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndisc = DecisionTreeDiscretiser(\n        bin_output=\"bin_number\",\n        cv=3,\n        scoring='neg_mean_squared_error',\n        variables=['LotArea', 'GrLivArea'],\n        regression=True,\n    )\n\n    # fit the transformer\n    disc.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Cyclical Encoding with NumPy\nDESCRIPTION: This code snippet demonstrates how to manually encode cyclical features using NumPy's sine and cosine functions. It calculates the sine and cosine components of a variable by normalizing it to 2*pi radians. This approach requires specifying the variable and its maximum value.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nX[f\"{variable}_sin\"] = np.sin(X[\"variable\"] * (2.0 * np.pi / X[\"variable\"]).max())\nX[f\"{variable}_cos\"] = np.cos(X[\"variable\"] * (2.0 * np.pi / X[\"variable\"]).max())\n```\n\n----------------------------------------\n\nTITLE: Rolling Window Aggregation with Pandas\nDESCRIPTION: This code snippet demonstrates how to create rolling window features using pandas. It calculates the maximum and mean values over a window of the current and 2 previous rows for the specified variables (var_1 and var_2).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nX[[\"var_1\", \"var_2\"].rolling(window=3).agg([\"max\", \"mean\"])\n```\n\n----------------------------------------\n\nTITLE: Loading data and splitting into train and test sets - pandas, scikit-learn\nDESCRIPTION: This code snippet demonstrates how to load data using pandas and split it into training and testing sets using scikit-learn's train_test_split function.  It imports necessary libraries, reads a CSV file named 'houseprice.csv', separates features (X) and target (y), and then splits them into training and testing sets with a 70/30 split and a fixed random state for reproducibility.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/AddMissingIndicator.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\timport numpy as np\n\timport pandas as pd\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.model_selection import train_test_split\n\n\tfrom feature_engine.imputation import AddMissingIndicator\n\n\t# Load dataset\n\tdata = pd.read_csv('houseprice.csv')\n\n\n\t# Separate into train and test sets\n\tX_train, X_test, y_train, y_test = train_test_split(\n    \tdata.drop(['Id', 'SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)\n```\n\n----------------------------------------\n\nTITLE: Wrapping Lasso Regression with MultiOutputRegressor - Python\nDESCRIPTION: This code creates a `MultiOutputRegressor` using a `Lasso` regression model.  The `MultiOutputRegressor` allows for predicting multiple target variables simultaneously, and the `Lasso` model is used as the base regressor.  The `random_state` is set for reproducibility, and `max_iter` is set to 10 to control the maximum number of iterations.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlasso = MultiOutputRegressor(Lasso(random_state=0, max_iter=10))\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Feature-engine\nDESCRIPTION: This code snippet shows how to transform a dataset using the `transform()` method from a Feature-engine feature selection object. It takes a Pandas DataFrame `X_test` as input and returns a transformed DataFrame `Xtr` with the selected features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nXtr = sel.transform(X_test)\n\nXtr.head()\n```\n\n----------------------------------------\n\nTITLE: Checking Numerical Variables with Feature-Engine (Error Case)\nDESCRIPTION: This code snippet demonstrates the error handling of the `check_categorical_variables` function. When a numerical variable (`num_var_1`) is included in the list of variables to check, the function raises a `TypeError` because it expects only categorical (object or categorical dtype) variables. This shows the function's validation of input variable types.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_categorical_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncheck_categorical_variables(X, [\"cat_var1\", \"num_var_1\"])\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with EndTailImputer\nDESCRIPTION: This code snippet applies the fitted EndTailImputer to transform both the training and testing sets. It uses the learned imputation values from the fit step to replace missing values in the specified columns of the datasets.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/EndTailImputer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\t# transform the data\n\ttrain_t= tail_imputer.transform(X_train)\n\ttest_t= tail_imputer.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Equal Frequency Discretization\nDESCRIPTION: This snippet demonstrates how to use the EqualFrequencyDiscretiser to discretize specified numerical variables (LotArea and GrLivArea) into 10 equal-frequency intervals.  It initializes the discretizer with the desired number of quantiles and the target variables, then fits it to the training data to learn the bin boundaries. Dependencies: EqualFrequencyDiscretiser.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# List the target numeric variables to be transformed\nTARGET_NUMERIC_FEATURES= ['LotArea','GrLivArea']\n\n# Set up the discretization transformer\ndisc = EqualFrequencyDiscretiser(q=10, variables=TARGET_NUMERIC_FEATURES)\n\n# Fit the transformer\ndisc.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Classes for MRMR\nDESCRIPTION: Imports necessary libraries including pandas, matplotlib, sklearn datasets, train_test_split, and the MRMR class from feature_engine.selection. These are required to load data, split it into training and testing sets and use the MRMR class.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.selection import MRMR\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting RareLabelEncoder\nDESCRIPTION: This code initializes and fits the RareLabelEncoder to the training data (X_train). The encoder groups categories that appear less than 3% of the time into a 'Rare' category for the 'cabin', 'pclass', and 'embarked' variables, provided those variables have more than 2 unique categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nencoder = RareLabelEncoder(\n    tol=0.03,\n    n_categories=2,\n    variables=['cabin', 'pclass', 'embarked'],\n    replace_with='Rare',\n)\n\n# fit the encoder\nencoder.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Initializing SmartCorrelatedSelection with Variance in Python\nDESCRIPTION: This code snippet initializes the SmartCorrelatedSelection class to find correlated features with a correlation coefficient > 0.8 and retain the feature with the highest variance. It specifies the selection method as 'variance' and uses the Pearson correlation method.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# set up the selector\ntr = SmartCorrelatedSelection(\n    variables=None,\n    method=\"pearson\",\n    threshold=0.8,\n    missing_values=\"raise\",\n    selection_method=\"variance\",\n    estimator=None,\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Encoder Dictionary - Python\nDESCRIPTION: This code shows how to access the calculated categorical mappings, which are stored in the `encoder_dict_` attribute. This dictionary contains the mean target value for each category in each of the encoded variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Initializing and Splitting Data for One-Hot Encoding in Python\nDESCRIPTION: This code snippet imports necessary libraries (sklearn, feature_engine) and loads the Titanic dataset. It then splits the dataset into training and testing sets, preparing the data for one-hot encoding.  It utilizes `train_test_split` for splitting the dataset and `load_titanic` from feature_engine for loading the data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import OneHotEncoder\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    handle_missing=True,\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Initializing SmartCorrelatedSelection with Model Performance in Python\nDESCRIPTION: This code snippet initializes the SmartCorrelatedSelection class to find correlated features with a correlation coefficient > 0.8 and retain the feature that results in the best model performance. It uses a DecisionTreeClassifier as the estimator and ROC AUC as the scoring metric.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntr = SmartCorrelatedSelection(\n    variables=None,\n    method=\"pearson\",\n    threshold=0.8,\n    missing_values=\"raise\",\n    selection_method=\"model_performance\",\n    estimator=DecisionTreeClassifier(random_state=1),\n    scoring='roc_auc',\n    cv=3,\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Values after Outlier Trimming in Python\nDESCRIPTION: This snippet calculates and prints the maximum values of the 'fare' and 'age' columns in the transformed training dataset (train_t). This verifies the effect of the outlier trimming and checks if the maximum values are below the capping thresholds.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[['fare', 'age']].max()\n```\n\nLANGUAGE: python\nCODE:\n```\nfare    65.0\nage     53.0\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Transforming Data Using DecisionTreeDiscretiser in Python\nDESCRIPTION: This code snippet applies the fitted `DecisionTreeDiscretiser` to transform both the training (X_train) and testing (X_test) sets. The transformed data is stored in `train_t` and `test_t`, respectively.  It then prints the head of the transformed 'LotArea' and 'GrLivArea' columns of the training set to display the discretized values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = disc.transform(X_train)\ntest_t = disc.transform(X_test)\n\nprint(train_t[['LotArea', 'GrLivArea']].head())\n```\n\n----------------------------------------\n\nTITLE: Fitting MatchCategories transformer in Python\nDESCRIPTION: This snippet sets up the MatchCategories transformer with the 'ignore' option for missing values and then fits it to the training data.  The transformer learns the mapping of categories to integers from the training set.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# set up the transformer\nmatch_categories = MatchCategories(missing_values=\"ignore\")\n\n# learn the mapping of categories to integers in the train set\nmatch_categories.fit(train)\n```\n\n----------------------------------------\n\nTITLE: Return Boundaries Setup\nDESCRIPTION: This snippet demonstrates how to configure the `EqualFrequencyDiscretiser` to return the bin boundaries instead of integer indices. Setting `return_boundaries=True` will cause the transformer to output the interval limits for each bin. Dependencies: EqualFrequencyDiscretiser.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Set up the discretization transformer\ndisc = EqualFrequencyDiscretiser(\n    q=10,\n    variables=TARGET_NUMERIC_FEATURES,\n    return_boundaries=True)\n\n# Fit the transformer\ndisc.fit(X_train)\n\n# Transform test set & visualize limit\ntest_t = disc.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Initialize and Fit MeanNormalizationScaler\nDESCRIPTION: This code snippet initializes the MeanNormalizationScaler from Feature Engine, specifying the variables to be scaled. It then fits the scaler to the DataFrame, learning the mean and range for each specified variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/scaling/MeanNormalizationScaler.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# set up the scaler\nscaler = MeanNormalizationScaler(variables = vars)\n\n# fit the scaler\nscaler.fit(df)\n```\n\n----------------------------------------\n\nTITLE: Displaying Feature Names from Training Set\nDESCRIPTION: This shows the output of accessing the `feature_names_in_` attribute. It displays a list of feature names extracted from the training dataset. This list is used by the transformer to match the columns in subsequent datasets.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n['pclass',\n 'survived',\n 'sex',\n 'age',\n 'sibsp',\n 'parch',\n 'fare',\n 'cabin',\n 'embarked']\n```\n\n----------------------------------------\n\nTITLE: Data Type Confirmation\nDESCRIPTION: This displays the data type, confirming that the 'sex' column now has the object data type (`dtype('O')`), matching the original training data due to setting the `match_dtypes` parameter to True when initializing the MatchVariables object.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndtype('O')\n```\n\n----------------------------------------\n\nTITLE: Transform training and test sets\nDESCRIPTION: This code snippet transforms the training and test sets using the fitted `MeanEncoder`. The `transform` method replaces the original categorical values in the specified variables with their corresponding mean target values learned during the `fit` step.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Initializing MRMR with Mutual Information (MIQ) - Python\nDESCRIPTION: This snippet initializes the MRMR feature selection using mutual information for relevance and redundancy (MIQ method).  It sets the `discrete_features` parameter to indicate which variables are categorical. It also sets `max_features` to limit the number of features to be selected.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsel = MRMR(\n    variables = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup'],\n    method=\"MIQ\",\n    max_features=4,\n    discrete_features=[False, False, True, True, False, True],\n    regression=True,\n    random_state=42,\n)\n\nsel.fit(X,y)\n```\n\n----------------------------------------\n\nTITLE: Inverse transform data\nDESCRIPTION: This code applies the inverse transformation (exponential function) to the transformed training and testing datasets (train_t and test_t) using the fitted LogTransformer.  This recovers the original scale of the variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogTransformer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_unt = logt.inverse_transform(train_t)\ntest_unt = logt.inverse_transform(test_t)\n```\n\n----------------------------------------\n\nTITLE: Displaying PSI Values with DropHighPSIFeatures\nDESCRIPTION: This snippet shows how to access the calculated PSI (Population Stability Index) values after fitting the `DropHighPSIFeatures` transformer. The `psi_values_` attribute contains a dictionary where keys are feature names and values are the corresponding PSI scores. High PSI values indicate significant distribution shifts between the two datasets formed by the split.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ntransformer.psi_values_\n```\n\n----------------------------------------\n\nTITLE: Fitting OutlierTrimmer to Training Data in Python\nDESCRIPTION: This snippet fits the initialized OutlierTrimmer (ot) to the training data (X_train).  This step calculates the capping values based on the specified method and parameters.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\not.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names After Transformation\nDESCRIPTION: Retrieves the names of the features remaining after the transformation using the `get_feature_names_out()` method of the fitted `SelectByInformationValue` transformer. This provides a list of the selected feature names.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsel.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Removing High PSI Features with Split Fraction - Python\nDESCRIPTION: This code snippet utilizes the `DropHighPSIFeatures` class to remove features exhibiting high PSI values after splitting the dataset using a specified split fraction. It initializes the transformer with `split_frac=0.6`, fits it to the data `X`, and then accesses the calculated PSI values. Dependencies include feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Remove the features with high PSI values using a 60-40 split.\n\ntransformer = DropHighPSIFeatures(split_frac=0.6)\ntransformer.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Access Interval Limits\nDESCRIPTION: Shows how to access the interval limits identified for each variable using the `binner_dict_` attribute of the fitted GeometricWidthDiscretiser.  The output is a dictionary where keys are the variable names and values are lists of bin boundaries.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/GeometricWidthDiscretiser.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\tdisc.binner_dict_\n```\n\n----------------------------------------\n\nTITLE: Displaying First Rows of Processed Data (Python)\nDESCRIPTION: This snippet prints the first few rows of the processed DataFrame `df` to showcase the data structure after loading, cleaning, and resampling. This allows for verification of the transformations applied.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(df.head())\n```\n\n----------------------------------------\n\nTITLE: Applying PowerTransformer with default exponent\nDESCRIPTION: This code initializes a PowerTransformer from feature_engine, specifying 'LotArea' and 'GrLivArea' as variables to be transformed. It then fits the transformer to the training data (X_train) and transforms it. Finally, it plots the transformed 'LotArea' variable using seaborn's histplot to visualize the effect of the power transformation with the default exponent (0.5).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\t# Set up the variable transformer (tf)\n\ttf = PowerTransformer(variables = ['LotArea', 'GrLivArea'])\n\n\t# Fit the transformer\n\tX_train_transformed = tf.fit_transform(X_train)\n\n\t# Plot histogram\n\tsns.histplot(X_train_transformed['LotArea'], kde=True, bins=50)\n```\n\n----------------------------------------\n\nTITLE: Creating an Imputation Pipeline\nDESCRIPTION: Creates a scikit-learn pipeline that first adds missing indicators using `AddMissingIndicator` and then imputes missing values using `MeanMedianImputer`. The pipeline is then fitted to the training data (`X_train`). This showcases a common workflow for handling missing data by first flagging the missingness and then imputing the values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\t# Create imputer pipeline\n\timputer = make_pipeline(\n\t\tAddMissingIndicator(),\n\t\tMeanMedianImputer()\n\t)\n\n\t# Fit the pipeline\n\timputer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Data Distribution\nDESCRIPTION: This snippet visualizes the distribution of a variable before and after equal frequency discretization. It creates a figure with two subplots: a histogram of the raw data and a bar plot of the transformed data. Dependencies: matplotlib, pandas DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate a figure with two axes\nfig, axes = plt.subplots(ncols=2, figsize=(10,5))\n\n# Plot raw distribution\nX_train['GrLivArea'].plot.hist(bins=disc.q, ax=axes[0])\naxes[0].set_title('Raw data with equal width binning')\naxes[0].set_xlabel('GrLivArea')\n\n# Plot transformed distribution\ntrain_t['GrLivArea'].value_counts().sort_index().plot.bar(ax=axes[1])\naxes[1].set_title('Transformed data with equal frequency binning')\n\nplt.tight_layout(w_pad=2)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Visualize original data distribution\nDESCRIPTION: This code generates histograms of 'LotArea' and 'GrLivArea' columns from the training dataset (X_train) to visualize their distribution. The purpose is to identify right-skewed variables suitable for log transformation.  matplotlib.pyplot is required for plotting.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogTransformer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX_train[['LotArea', 'GrLivArea']].hist(figsize=(10,5))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Setting up Arbitrary Number Imputer - Python\nDESCRIPTION: This snippet sets up and fits the ArbitraryNumberImputer to impute 'LotFrontage' and 'MasVnrArea' with -999 in the training set. It demonstrates how to initialize the imputer with specific variables and an arbitrary value using feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/ArbitraryNumberImputer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# set up the imputer\narbitrary_imputer = ArbitraryNumberImputer(\n        arbitrary_number=-999,\n        variables=['LotFrontage', 'MasVnrArea'],\n        )\n\n# fit the imputer\narbitrary_imputer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Create Lag Features and Drop Missing Values\nDESCRIPTION: This snippet shows how to create lag features and drop rows containing missing values resulting from the lagging operation. The `LagFeatures` transformer is initialized with `periods=1` and `drop_na=True`. The code then fits and transforms the input DataFrame `X`, removing rows with NaN values in the lag features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(periods=1, drop_na=True)\n\nX_tr = lag_f.fit_transform(X)\n\nprint(X_tr.head())\n```\n\n----------------------------------------\n\nTITLE: Computing Coefficient of Variation\nDESCRIPTION: This code defines a function `compute_cv` to calculate the coefficient of variation (CV) for a given dataset.  It then calculates and prints the CV for the original 'LotArea' data, the transformed data using the default exponent, and the transformed data using the custom exponent.  The CV is used to assess the impact of the power transformations on the variance of the data. It handles potential division by zero by returning infinity if the mean is zero.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\t# Compute coefficient of variation (CV)\n\n\tdef compute_cv(data):\n\t\t\"\"\"Compute the coefficient of variation (CV) for a given dataset.\"\"\"\n\t\treturn np.std(data, ddof=1) / np.mean(data) if np.mean(data) != 0 else np.inf\n\n\tcv_raw_data = compute_cv(X_train['LotArea'])\n\tcv_transformed_data = compute_cv(X_train_transformed['LotArea'])\n\tcv_transformed_data_custom = compute_cv(X_train_transformed_custom['LotArea'])\n\n\tprint(f\"\"\"\n\tRaw data CV: {cv_raw_data:.2%}\n\tTransformed data exp:0.5 CV: {cv_transformed_data:.2%}\n\tTransformed data exp:0.001 CV (custom): {cv_transformed_data_custom:.2%}\n\t\"\"\")\n```\n\n----------------------------------------\n\nTITLE: PowerTransformer Inverse Transformation\nDESCRIPTION: This code snippet demonstrates how to perform an inverse transformation using the PowerTransformer. The `inverse_transform()` method is called on the previously fitted transformer (tf), passing the transformed DataFrame (`df_sim_transformed`) as input. The result of the inverse transformation, which should be the original data, is then displayed using the `head(3)` method to show the first 3 rows.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\ttf.inverse_transform(df_sim_transformed).head(3)\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Train and Test Sets - Python\nDESCRIPTION: This code snippet uses the train_test_split function from scikit-learn to split the Titanic dataset into training and testing sets. It drops the 'survived' column from the features (X) and assigns it to the target variable (y). The test_size is set to 0.1, and a random_state is used for reproducibility.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(['survived'], axis=1),\n    data['survived'],\n    test_size=0.1,\n    random_state=0)\n\nX_train.shape, X_test.shape\n```\n\n----------------------------------------\n\nTITLE: Transforming the data based on selected features\nDESCRIPTION: Transforms the test set (X_test) using the fitted `SelectByInformationValue` transformer (`sel`).  It drops features that have an IV score below the specified threshold. The resulting transformed data (Xtr) is assigned to the new variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nXtr = sel.transform(X_test)\n\nXtr.head()\n```\n\n----------------------------------------\n\nTITLE: Pipeline Construction for Forecasting (Python)\nDESCRIPTION: This code constructs a `Pipeline` using `make_pipeline` for time series forecasting. It includes `LagFeatures`, `WindowFeatures`, and a `MultiOutputRegressor` with Lasso regression. `make_pipeline` automatically names the steps.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipe = make_pipeline(lagf, winf, lasso)\n\nprint(pipe)\n```\n\n----------------------------------------\n\nTITLE: Plotting Distributions Before and After Discretization (Skewed)\nDESCRIPTION: This snippet plots the original distribution and the transformed distribution of the skewed feature using matplotlib and pandas. This allows for visual comparison of the data before and after applying `EqualFrequencyDiscretiser`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].hist(X.feature2, bins=disc.q)\naxes[0].set(xlabel='feature2', ylabel='count', title='Raw data')\n\nX_transformed.feature2.value_counts().sort_index().plot.bar(ax=axes[1])\naxes[1].set_title('Transformed data')\n\nplt.suptitle('Skewed distributed data', weight='bold', size='large', y=1.05)\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initialize and fit LogTransformer\nDESCRIPTION: This code snippet initializes the LogTransformer, specifying 'LotArea' and 'GrLivArea' as the variables to transform using the natural logarithm (base e). It then fits the transformer to the training data (X_train).  Fitting checks for numerical variables; no parameters are learned.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogTransformer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlogt = LogTransformer(variables = ['LotArea', 'GrLivArea'])\n\nlogt.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Initializing SelectByInformationValue\nDESCRIPTION: Initializes the `SelectByInformationValue` transformer, specifying the `variables` parameter with a list of categorical feature names and setting the `threshold` to 0.2. It uses the fit method to calculate WoE, IV and identify features to drop based on the threshold.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsel = SelectByInformationValue(\n    variables=['A1', 'A6', 'A9', 'A10', 'A12', 'A13'],\n    threshold=0.2,\n)\n\nsel.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Initializing DecisionTreeDiscretiser with Boundaries\nDESCRIPTION: Initializes the DecisionTreeDiscretiser to output interval boundaries with a precision of 3 decimal places. It uses cross-validation with 3 folds and negative mean squared error as the scoring metric.  The specified variables 'LotArea' and 'GrLivArea' will be discretized using regression.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndisc = DecisionTreeDiscretiser(\n        bin_output=\"boundaries\",\n        precision=3,\n        cv=3,\n        scoring='neg_mean_squared_error',\n        variables=['LotArea', 'GrLivArea'],\n        regression=True)\n\n    # fit the transformer\n    disc.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Accessing Encoder Dictionary after Ordered Encoding in Python\nDESCRIPTION: This code accesses the encoder dictionary of the fitted OrdinalEncoder to inspect the mapping between the original categorical values of 'HouseAgeCategorical' and their corresponding numerical encodings after the ordered encoding process. This shows the assigned numerical value for each category.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nordered_encoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting the Winsorizer - Python\nDESCRIPTION: This code initializes the `Winsorizer` class with specified parameters for capping outliers. It sets the `capping_method` to 'gaussian', the `tail` to 'right', the `fold` to 3, and specifies the variables 'age' and 'fare' to be capped.  It then fits the `Winsorizer` to the training data `X_train` to learn the capping values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/Winsorizer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncapper = Winsorizer(capping_method='gaussian',\n                    tail='right', \n                    fold=3, \n                    variables=['age', 'fare'])\n\ncapper.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Applying the Yeo-Johnson transformation\nDESCRIPTION: This code snippet applies the Yeo-Johnson transformation to both the training and testing sets using the fitted `YeoJohnsonTransformer` instance (`tf`). The transformed data is stored in `train_t` and `test_t` respectively.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/YeoJohnsonTransformer.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntrain_t = tf.transform(X_train)\ntest_t = tf.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: DecisionTreeDiscretiser with Precision Parameter in Python\nDESCRIPTION: This snippet shows how to use the `precision` parameter to round the predictions of the decision tree after the transformation. This can make the output easier to visualize and interpret.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndisc = DecisionTreeDiscretiser(\n    bin_output=\"prediction\",\n    precision=1,\n    cv=3,\n    scoring='neg_mean_squared_error',\n    variables=['LotArea', 'GrLivArea'],\n    regression=True)\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names After Pipeline Transformation\nDESCRIPTION: This snippet shows how to retrieve the names of the features after transformation by the Feature-engine Pipeline, excluding the final step (e.g., the Logistic Regression model).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\npipe[:-1].get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Accessing Correlated Feature Dictionary in Python\nDESCRIPTION: This code snippet accesses the correlated_feature_dict_ attribute of the SmartCorrelatedSelection object, which is a dictionary that maps the retained feature to the correlated features that will be dropped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntr.correlated_feature_dict_\n```\n\n----------------------------------------\n\nTITLE: Accessing Learned Bin Boundaries\nDESCRIPTION: This snippet displays the learned bin boundaries stored in the `binner_dict_` attribute of the fitted `EqualWidthDiscretiser`. The dictionary contains the boundaries for each discretized variable, which are used to transform the data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Learned limits for each variable\ndisc.binner_dict_\n```\n\nLANGUAGE: Python\nCODE:\n```\n{'LotArea': [-inf,\n  22694.5,\n  44089.0,\n  65483.5,\n  86878.0,\n  108272.5,\n  129667.0,\n  151061.5,\n  172456.0,\n  193850.5,\n  inf],\n 'GrLivArea': [-inf,\n  864.8,\n  1395.6,\n  1926.3999999999999,\n  2457.2,\n  2988.0,\n  3518.7999999999997,\n  4049.5999999999995,\n  4580.4,\n  5111.2,\n  inf]}\n```\n\n----------------------------------------\n\nTITLE: Creating Correlated Data - Python\nDESCRIPTION: Generates a synthetic dataset with correlated features using `sklearn.datasets.make_classification` and transforms it into a pandas DataFrame. This provides data suitable for demonstrating the `SmartCorrelatedSelection` class.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom feature_engine.selection import SmartCorrelatedSelection\n\n# make dataframe with some correlated variables\ndef make_data():\n    X, y = make_classification(n_samples=1000,\n                               n_features=12,\n                               n_redundant=4,\n                               n_clusters_per_class=1,\n                               weights=[0.50],\n                               class_sep=2,\n                               random_state=1)\n\n    # transform arrays into pandas df and series\n    colnames = ['var_'+str(i) for i in range(12)]\n    X = pd.DataFrame(X, columns=colnames)\n    y = pd.Series(y)\n    return X, y\n\nX, y = make_data()\n```\n\n----------------------------------------\n\nTITLE: Accessing Best Hyperparameters from GridSearchCV - Python\nDESCRIPTION: This code snippet accesses and prints the best hyperparameter combination found by the `GridSearchCV`. The `best_params_` attribute returns a dictionary containing the optimal values for each hyperparameter specified in the `param_grid`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ngrid.best_params_\n```\n\n----------------------------------------\n\nTITLE: Transforming train and test sets\nDESCRIPTION: This code transforms the training and testing sets using the fitted DecisionTreeEncoder. It replaces the categorical values with the predictions from the decision trees.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Drop Original Variables with CyclicalFeatures\nDESCRIPTION: This code initializes the CyclicalFeatures transformer with `drop_original=True`, fits it to the DataFrame `df`, and transforms it. Setting `drop_original=True` ensures that the original 'day' and 'months' columns are removed from the resulting DataFrame, leaving only the encoded cyclical features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncyclical = CyclicalFeatures(variables=None, drop_original=True)\nX = cyclical.fit_transform(df)\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Transforming Training and Testing Data Using OneHotEncoder in Python\nDESCRIPTION: This snippet applies the fitted `OneHotEncoder` to transform both the training (`X_train`) and testing (`X_test`) datasets. The `transform()` method converts the specified categorical features into one-hot encoded numerical features. The original categorical columns are dropped by default.  The transformed datasets are then assigned to new variables (`train_t`, `test_t`).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: WindowFeatures Transformation with drop_original=True (Python)\nDESCRIPTION: This code snippet demonstrates how to use the WindowFeatures transformer to calculate windowed mean and max features for a time series while dropping the original time series column. It initializes the WindowFeatures transformer with specified window sizes, functions, frequency, and the drop_original parameter set to True. After fitting and transforming the input DataFrame, the head of the transformed DataFrame is displayed, showing only the newly generated features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwin_f = WindowFeatures(\n    window=[\"45min\"],\n    functions=[\"mean\", \"max\"],\n    freq=\"30min\",\n    drop_original=True,\n)\n\nX_tr = win_f.fit_transform(X['ambient_temp'].to_frame())\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Applying PowerTransformer to toy datasets\nDESCRIPTION: This code snippet applies the PowerTransformer from feature_engine to the generated left-skewed and right-skewed datasets. It initializes the transformer with the columns to be transformed ('left_skewed', 'right_skewed'), fits the transformer to the DataFrame `df_sim`, and then transforms the data, storing the result in `df_sim_transformed`. Finally, it generates histograms of the transformed data using seaborn to visualize the effect of the power transformation on the distributions.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\t# Set up the variable transformer (tf)\n\ttf = PowerTransformer(variables = ['left_skewed', 'right_skewed'])\n\n\t# Fit the transformer\n\tdf_sim_transformed = tf.fit_transform(df_sim)\n\n\t# Plot histograms\n\tfig,axes = plt.subplots(ncols=2, figsize=(12,4))\n\n\tsns.histplot(\n\t\tdf_sim_transformed['left_skewed'], ax=axes[0], color='blue', **hist_params\n\t)\n\tsns.histplot(\n```\n\n----------------------------------------\n\nTITLE: WoE Encoding - Python\nDESCRIPTION: This code snippet utilizes Feature-engine's `WoEEncoder` to encode the specified categorical variables ('cabin', 'pclass', 'embarked') using the Weight of Evidence (WoE) method. The encoder is initialized with `ignore_format=True` to allow encoding of numerical variables. The `fit` method learns the WoE for each category based on the training data. The learned WoE values are stored in the `encoder_dict_` attribute.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# set up a weight of evidence encoder\nwoe_encoder = WoEEncoder(\n    variables=['cabin', 'pclass', 'embarked'],\n    ignore_format=True,\n)\n\n# fit the encoder\nwoe_encoder.fit(train_t, y_train)\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop\nDESCRIPTION: This code snippet shows how to retrieve the list of features that were determined to be unimportant and will be dropped from the dataset based on the specified threshold during the RecursiveFeatureAddition process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# the features to drop\ntr.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Create Multiple Lag Features with List of Periods\nDESCRIPTION: This snippet demonstrates how to create multiple lag features with different lag periods in a single step. The `LagFeatures` transformer is initialized with `periods=[1, 2]`, specifying that lag features for both one and two periods should be created. The code then fits and transforms the input DataFrame `X`, resulting in `X_tr` containing lag features for both periods.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(periods=[1, 2])\n\nX_tr = lag_f.fit_transform(X)\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Plotting Mean House Price by MSZoning (Before Encoding)\nDESCRIPTION: This code snippet groups the training data by the 'MSZoning' feature and plots the mean house price for each category using a bar chart. It demonstrates the relationship between the original categorical values and the target variable before encoding.  The `plt.ylabel` sets the y-axis label, and `plt.show()` displays the plot. Dependencies include `pandas` for data manipulation and `matplotlib` for plotting.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ny_train.groupby(X_train[\"MSZoning\"]).mean().plot.bar()\nplt.ylabel(\"mean house price\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Arbitrary Number Imputation with Dictionary - Python\nDESCRIPTION: This code snippet shows how to use the ArbitraryNumberImputer to impute different variables with different arbitrary numbers using a dictionary. It uses the feature_engine library. The `imputer_dict` parameter maps variable names to their corresponding imputation values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/ArbitraryNumberImputer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntransformer = ArbitraryNumberImputer(\n        imputer_dict = {'varA' : 1, 'varB': 99}\n        )\n\nXt = transformer.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Performance - Python\nDESCRIPTION: This code snippet accesses the `feature_performance_` attribute of the fitted SelectByTargetMeanPerformance object (sel). This attribute is a dictionary containing the ROC-AUC score for each feature that was evaluated.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_performance_\n```\n\n----------------------------------------\n\nTITLE: Boolean Feature Selection Output\nDESCRIPTION: Demonstrates the boolean array output from a feature selection process. The array contains True for selected features and False for features that will be dropped.  This output is commonly used to filter the original dataset to keep only the selected features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n[False, False, True, True, True, False, False, False, True, False]\n```\n\n----------------------------------------\n\nTITLE: Initializing SelectByShuffling for Feature Selection\nDESCRIPTION: This code initializes the SelectByShuffling class, configuring it to use the LinearRegression model, 'r2' scoring, and 3-fold cross-validation. The random_state ensures reproducibility of the shuffling process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntr = SelectByShuffling(\n    estimator=linear_model,\n    scoring=\"r2\",\n    cv=3,\n    random_state=0,\n)\n```\n\n----------------------------------------\n\nTITLE: Pandas Expanding Window with Shift for Forecasting\nDESCRIPTION: This snippet builds upon the previous one by adding a `shift` operation. This shifts the window features forward by one period, ensuring that only information known at predict time is used for forecasting.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX[[\"var_1\", \"var_2\"].expanding(min_periods=3).agg([\"max\", \"mean\"]).shift(period=1)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Box-Cox\nDESCRIPTION: This code applies the fitted Box-Cox transformation to both the training (X_train) and testing (X_test) datasets, creating transformed datasets train_t and test_t.  It uses the optimal lambda values learned during the fit step to perform the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = boxcox.transform(X_train)\ntest_t = boxcox.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Initializing AddMissingIndicator for specific variables - feature-engine\nDESCRIPTION: This snippet shows how to initialize the AddMissingIndicator class from the feature_engine library. It creates an instance of the class, specifying a list of variables ('Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea') for which missing indicators will be created. The imputer is then fit to the training data (X_train).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/AddMissingIndicator.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\t# set up the imputer\n\taddBinary_imputer = AddMissingIndicator(\n        variables=['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea'],\n        )\n\n\t# fit the imputer\n\taddBinary_imputer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Accessing Dropped Features - Python\nDESCRIPTION: Accesses the `features_to_drop_` attribute of the `SmartCorrelatedSelection` transformer to retrieve a list of feature names that have been dropped due to correlation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntr.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Displaying the Head of Transformed Training Data in Python\nDESCRIPTION: This code snippet prints the first 5 rows of the transformed training dataset (`train_t`), showcasing the result of the one-hot encoding process. It allows for a quick inspection of the newly created dummy variables and confirms that the original categorical features have been replaced. No prerequisites are needed; it merely prints the head of the `train_t` dataframe.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with DropHighPSIFeatures - Python\nDESCRIPTION: Applies the `transform` method of the fitted `DropHighPSIFeatures` transformer to remove features with high PSI values from the input dataframe `X`. The transformed dataframe `X_transformed` contains only the remaining features. Dependencies include feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nX_transformed = transformer.transform(X)\n\nX_transformed.columns\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop - Python\nDESCRIPTION: This code snippet accesses the `features_to_drop_` attribute of the fitted DropCorrelatedFeatures transformer. This attribute stores a list of the names of the features that will be removed from the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntr.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed Data (Interval Boundaries)\nDESCRIPTION: Presents the output of transforming the data using interval boundaries. It shows the first few rows of the 'LotArea' and 'GrLivArea' columns, displaying the interval to which each original value was assigned.  This demonstrates the result of the discretization process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n                    LotArea         GrLivArea\n    254      (-inf, 8637.5]  (1199.0, 1413.0]\n    1066     (-inf, 8637.5]  (1483.0, 1651.5]\n    638   (8637.5, 10924.0]    (749.5, 808.0]\n    799      (-inf, 8637.5]  (1651.5, 1825.0]\n    380      (-inf, 8637.5]  (1651.5, 1825.0]\n```\n\n----------------------------------------\n\nTITLE: Accessing Information Values after Binning\nDESCRIPTION: Accesses the `information_values_` attribute of the fitted `SelectByInformationValue` transformer to retrieve a dictionary containing the calculated information value for each variable, after binning the numerical features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsel.information_values_\n```\n\n----------------------------------------\n\nTITLE: Import Statements for Time Series Forecasting\nDESCRIPTION: This code imports the necessary libraries for time series forecasting using Feature-engine's Pipeline. It includes pandas, numpy, matplotlib, scikit-learn, and Feature-engine modules.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom feature_engine.timeseries.forecasting import (\n    LagFeatures,\n    WindowFeatures,\n)\nfrom feature_engine.pipeline import Pipeline\n```\n\n----------------------------------------\n\nTITLE: Missing Data Imputation with Feature-engine\nDESCRIPTION: This code demonstrates how to use Feature-engine's MeanMedianImputer to impute missing values in a dataset. It involves loading data, splitting it into training and testing sets, setting up the imputer with the desired imputation method ('median'), fitting the imputer to the training data, and then transforming both the training and testing data. Finally, it plots the distribution of 'LotFrontage' before and after imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/index.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\timport numpy as np\n\timport pandas as pd\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.model_selection import train_test_split\n\n\tfrom feature_engine.imputation import MeanMedianImputer\n\n\t# Load dataset\n\tdata = pd.read_csv('houseprice.csv')\n\n\t# Separate into train and test sets\n\tX_train, X_test, y_train, y_test = train_test_split(\n    \t    data.drop(['Id', 'SalePrice'], axis=1),\n            data['SalePrice'],\n            test_size=0.3,\n            random_state=0\n        )\n\n\t# set up the imputer\n\tmedian_imputer = MeanMedianImputer(\n            imputation_method='median', variables=['LotFrontage', 'MasVnrArea']\n            )\n\n\t# fit the imputer\n\tmedian_imputer.fit(X_train)\n\n\t# transform the data\n\ttrain_t = median_imputer.transform(X_train)\n\ttest_t = median_imputer.transform(X_test)\n\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tX_train['LotFrontage'].plot(kind='kde', ax=ax)\n\ttrain_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')\n\tlines, labels = ax.get_legend_handles_labels()\n\tax.legend(lines, labels, loc='best')\n```\n\n----------------------------------------\n\nTITLE: Transforming Data and Target with Partial Pipeline\nDESCRIPTION: This snippet shows how to transform both the predictor dataset and the target variable using a partial Feature-engine Pipeline (excluding the final model). This is useful for obtaining the preprocessed data for further analysis or modeling.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nX_test_t, y_test_t = pipe[:-1].transform_x_y(X_test, y_test)\n\n    X_test.shape, X_test_t.shape\n```\n\n----------------------------------------\n\nTITLE: Dropping NA From Subset of Variables\nDESCRIPTION: This code snippet demonstrates how to remove missing data only from a specific set of columns by passing a list of column names to the `variables` parameter of `DropMissingData`. In the example, `missing_only` is set to `False` to make sure the transformer removes rows with NA in the specified variables regardless of whether those variables had NA during fitting.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndmd = DropMissingData(variables=[\"x1\", \"x3\"], missing_only=False)\nXt = dmd.fit_transform(X)\nXt.head()\n```\n\n----------------------------------------\n\nTITLE: Arbitrary Number Imputation with Single Value - Python\nDESCRIPTION: This code snippet demonstrates how to use the ArbitraryNumberImputer to impute multiple variables with the same arbitrary number. It requires the feature_engine library. The `variables` parameter specifies the columns to impute, and `arbitrary_number` sets the imputation value.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/ArbitraryNumberImputer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntransformer = ArbitraryNumberImputer(\n        variables = ['varA', 'varB'],\n        arbitrary_number = 99\n        )\n\nXt = transformer.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data after RF-based Selection - Python\nDESCRIPTION: This code transforms a dataset using the fitted MRMR selector (configured with Random Forests) to keep only the selected features, generating a new reduced dataset. The selected features are those deemed relevant by the Random Forest feature importance and not redundant based on the redundancy measure.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nXtr = sel.transform(X_test)\nprint(Xtr.head())\n```\n\n----------------------------------------\n\nTITLE: Matching Columns and Data Types with MatchVariables\nDESCRIPTION: This code snippet initializes the `MatchVariables` transformer with `missing_values=\"ignore\"` and `match_dtypes=True`.  Then it fits the transformer to the train set and transforms the test set.  `match_dtypes=True` forces the transformer to change the dtypes of the transformed columns to match those of the fitted (training) set.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmatch_cols_and_dtypes = MatchVariables(missing_values=\"ignore\", match_dtypes=True)\nmatch_cols_and_dtypes.fit(train)\n\ntest_ttt = match_cols_and_dtypes.transform(test_t)\n```\n\n----------------------------------------\n\nTITLE: Equal-Width Discretization with EqualWidthDiscretiser\nDESCRIPTION: This code snippet demonstrates how to use the `EqualWidthDiscretiser` to discretize two numerical variables (`LotArea` and `GrLivArea`) into 10 equal-width intervals. The discretizer is initialized with the specified number of bins and target variables, then fitted to the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# List the target numeric variables for equal-width discretization\nTARGET_NUMERIC_FEATURES= ['LotArea','GrLivArea']\n\n# Set up the discretization transformer\ndisc = EqualWidthDiscretiser(bins=10, variables=TARGET_NUMERIC_FEATURES)\n\n# Fit the transformer\ndisc.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Transforming 'embarked' in train dataset using MatchCategories in Python\nDESCRIPTION: This snippet applies the fitted MatchCategories transformer to the 'embarked' column of the training dataset and displays the categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# with 'match_categories', the encoding remains the same\nmatch_categories.transform(train).embarked.cat.categories\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop\nDESCRIPTION: This snippet shows how to access the list of features that SelectByShuffling has identified as non-important and will be dropped from the dataset based on the performance drift threshold.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntr.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Define Variables for Mean Normalization\nDESCRIPTION: This code defines a list of variable names to be used with the MeanNormalizationScaler. These are the numerical columns from the DataFrame that will be scaled.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/scaling/MeanNormalizationScaler.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nvars = [\n  'Age',\n  'Marks',\n  'Height',\n]\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Splitting into Train/Test Sets\nDESCRIPTION: This code snippet demonstrates how to load the Titanic dataset, handle missing values, and split the data into training and testing sets using scikit-learn's `train_test_split` function.  It imports necessary modules from `sklearn.model_selection`, `feature_engine.datasets`, and `feature_engine.selection`. The `load_titanic` function returns the features (X) and target (y) as Pandas DataFrames.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropFeatures.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.selection import DropFeatures\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    handle_missing=True,\n)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Selected Features - Python\nDESCRIPTION: This code snippet demonstrates how to transform a dataset using the fitted MRMR selector, retaining only the selected features. The `sel.transform()` method takes the input data (e.g., `X_test`) and returns a new dataset (`Xtr`) with reduced features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nXtr = sel.transform(X_test)\nprint(Xtr.head())\n```\n\n----------------------------------------\n\nTITLE: Dropping NA Values Across All Columns\nDESCRIPTION: This code snippet demonstrates how to use `DropMissingData` to remove rows with NA values from all columns in a DataFrame. It initializes the transformer without specifying specific columns, so it will learn the columns with missing data during fit.  The `fit_transform` method is used to both fit the transformer to the data and transform the data simultaneously. It imports necessary libraries like pandas, numpy, and DropMissingData from feature_engine.imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nfrom feature_engine.imputation import DropMissingData\n\nX = pd.DataFrame(\n    dict(\n        x1=[2, 1, 1, 0, np.nan],\n        x2=[\"a\", np.nan, \"b\", np.nan, \"a\"],\n        x3=[2, 3, 4, 5, 5],\n    )\n)\ny = pd.Series([1, 2, 3, 4, 5])\n\nprint(X.head())\n```\n\nLANGUAGE: python\nCODE:\n```\ndmd =  DropMissingData()\nXt = dmd.fit_transform(X)\nXt.head()\n```\n\n----------------------------------------\n\nTITLE: LagFeatures Transformation with Drop Original, Python\nDESCRIPTION: Demonstrates how to use the `LagFeatures` transformer to create lagged features from a time series, dropping the original series from the output. The `periods` parameter specifies the lag values, and `drop_original` is set to `True` to remove the original column.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(periods=[1, 2, 3], drop_original=True)\n\nX_tr = lag_f.fit_transform(X['ambient_temp'].to_frame())\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Cyclical Encoding with Scikit-learn ColumnTransformer\nDESCRIPTION: This code uses Scikit-learn's `ColumnTransformer` to apply cyclical sine and cosine transformations to multiple columns ('month', 'weekday', 'hour') of a DataFrame. It utilizes the `sin_transformer` and `cos_transformer` functions defined earlier with different periods. The transformer outputs a Pandas DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ncyclic_cossin_transformer = ColumnTransformer(\n        transformers=[\n            (\"month_sin\", sin_transformer(12), [\"month\"]),\n            (\"month_cos\", cos_transformer(12), [\"month\"]),\n            (\"weekday_sin\", sin_transformer(7), [\"weekday\"]),\n            (\"weekday_cos\", cos_transformer(7), [\"weekday\"]),\n            (\"hour_sin\", sin_transformer(24), [\"hour\"]),\n            (\"hour_cos\", cos_transformer(24), [\"hour\"]),\n        ],\n    ).set_output(transform=\"pandas\")\n```\n\n----------------------------------------\n\nTITLE: Splitting Data by Proportion with DropHighPSIFeatures - Python\nDESCRIPTION: This snippet demonstrates how to split a dataset into reference and test sets based on a specified proportion using `DropHighPSIFeatures`. It creates a sample dataframe, adds a column with a distribution shift, and then uses `DropHighPSIFeatures` to identify and remove features with high PSI values. The `split_frac` parameter controls the proportion of data in the reference set. Dependencies include pandas, seaborn, scikit-learn, and feature-engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.datasets import make_classification\nfrom feature_engine.selection import DropHighPSIFeatures\n\n# Create a dataframe with 500 observations and 6 random variables\nX, y = make_classification(\n    n_samples=500,\n    n_features=6,\n    random_state=0\n)\n\ncolnames = [\"var_\" + str(i) for i in range(6)]\nX = pd.DataFrame(X, columns=colnames)\n\n# Add a column with a shift.\nX['var_3'][250:] = X['var_3'][250:] + 1\n```\n\n----------------------------------------\n\nTITLE: Example of Learned Bin Boundaries Dictionary\nDESCRIPTION: This snippet provides an example of the dictionary structure returned by the `binner_dict_` attribute of the `EqualFrequencyDiscretiser`. It showcases the bin boundaries for 'LotArea' and 'GrLivArea', including -inf and inf as the lower and upper bounds, respectively.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n{'LotArea': [-inf,\n  5000.0,\n  7105.6,\n  8099.200000000003,\n  8874.0,\n  9600.0,\n  10318.400000000001,\n  11173.5,\n  12208.2,\n  14570.699999999999,\n  inf],\n 'GrLivArea': [-inf,\n  918.5,\n  1080.4,\n  1218.0,\n  1348.4,\n  1476.5,\n  1601.6000000000001,\n  1717.6999999999998,\n  1893.0000000000005,\n  2166.3999999999996,\n  inf]}\n```\n\n----------------------------------------\n\nTITLE: Calculating Cardinality of Categorical Features in Python\nDESCRIPTION: This snippet calculates the number of unique values (cardinality) for specified categorical features ('sex', 'pclass', 'cabin', 'embarked') in the training dataset. It uses the `nunique()` method to determine the cardinality of each feature, providing insights into the number of categories present in each variable. This is important for deciding how to apply one-hot encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nX_train[['sex', 'pclass', 'cabin', 'embarked']].nunique()\n```\n\n----------------------------------------\n\nTITLE: Lag Features with Datetime Frequency in Python\nDESCRIPTION: This snippet demonstrates how to create lag features based on datetime frequency using the LagFeatures transformer in Feature Engine. It shows how to specify the variables to lag and the frequency of the lag.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(variables = [\"module_temp\", \"irradiation\"], freq=\"30min\")\n\nX_tr = lag_f.fit_transform(X)\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing OneHotEncoder to Encode Numeric Variables in Python\nDESCRIPTION: This snippet initializes a OneHotEncoder instance to encode a numeric variable ('pclass') by explicitly specifying it in the `variables` parameter and setting `ignore_format` to True.  `drop_last` is set to True to avoid multicollinearity. The `.fit()` method learns the unique values of the specified numeric column from the training data (X_train).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nenc = OneHotEncoder(\n    variables=['pclass'],\n    drop_last=True,\n    ignore_format=True,\n    )\n\nenc.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Display Feature Importances\nDESCRIPTION: This code displays the feature importances calculated by the `ProbeFeatureSelection` transformer using the `head()` method to show the importance of the first few features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_importances_.head()\n```\n\n----------------------------------------\n\nTITLE: Accessing First Pipeline Step\nDESCRIPTION: This code shows how to access the first step of the pipeline using slicing.  The pipeline is sliced to extract the first step, which is DropMissingData.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipe[:1]\n```\n\nLANGUAGE: python\nCODE:\n```\nPipeline(steps=[('drop', DropMissingData())])\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop - Python\nDESCRIPTION: This snippet shows how to access the features that the SelectBySingleFeaturePerformance transformer has determined should be dropped based on their individual performance. The features_to_drop_ attribute contains a list of these features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsel.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Define Sine and Cosine Transformer Functions with Scikit-learn\nDESCRIPTION: This code defines two functions, `sin_transformer` and `cos_transformer`, which return `FunctionTransformer` objects. These transformers apply sine and cosine transformations respectively to input data, scaled by a given period. The period parameter defines the cycle length for the transformation.  It relies on numpy and sklearn.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef sin_transformer(period):\n        return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n\n    def cos_transformer(period):\n        return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n```\n\n----------------------------------------\n\nTITLE: Fitting and Transforming with Ignore Format - Python\nDESCRIPTION: This code fits and transforms the training data and transforms the testing data with the configured MeanEncoder, including numerical features.  The fit_transform combines the fitting and transforming steps for the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nt_train = encoder.fit_transform(X_train, y_train)\nt_test = encoder.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting EndTailImputer in Python\nDESCRIPTION: This snippet initializes the EndTailImputer with the 'gaussian' imputation method, imputing the right tail with a fold of 3 standard deviations.  The imputer is then fit to the training data to learn the imputation values for the specified variables 'LotFrontage' and 'MasVnrArea'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/EndTailImputer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\t# set up the imputer\n\ttail_imputer = EndTailImputer(imputation_method='gaussian',\n                                  tail='right',\n                                  fold=3,\n                                  variables=['LotFrontage', 'MasVnrArea'])\n\t# fit the imputer\n\ttail_imputer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Get Feature Names After Datetime Subtraction in Python\nDESCRIPTION: This code demonstrates how to use the get_feature_names_out() method after applying the DatetimeSubtraction transformer. This method is useful for retrieving the names of all features in the transformed DataFrame, including the original and newly created features. It is useful for compatibility with Scikit-learn Pipelines.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.datetime import DatetimeSubtraction\n\ndata = pd.DataFrame({\n    \"date1\" : [\"2022-09-01\", \"2022-10-01\", \"2022-12-01\"],\n    \"date2\" : [\"2022-09-15\", \"2022-10-15\", \"2022-12-15\"],\n    \"date3\" : [\"2022-08-01\", \"2022-09-01\", \"2022-11-01\"],\n    \"date4\" : [\"2022-08-15\", \"2022-09-15\", \"2022-11-15\"],\n})\n\ndtf = DatetimeSubtraction(variables=[\"date1\", \"date2\"], reference=[\"date3\", \"date4\"])\ndtf.fit(data)\n\ndtf.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Accessing Correlated Feature Sets - Python\nDESCRIPTION: This code snippet accesses the `correlated_feature_sets_` attribute of the fitted DropCorrelatedFeatures transformer. This attribute stores a list of sets, where each set contains the names of correlated features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntr.correlated_feature_sets_\n```\n\n----------------------------------------\n\nTITLE: Transforming Data and Target with Feature-engine Pipeline\nDESCRIPTION: This snippet shows how to transform both the predictor dataset and the target variable using the Feature-engine Pipeline. It highlights Feature-engine's ability to adjust the target variable along with the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ntrain_t, y_train_t = pipe.transform_x_y(X_train, y_train)\n\n    y_train.shape, y_train_t.shape\n```\n\n----------------------------------------\n\nTITLE: Fit and Transform with Target Variable in Python\nDESCRIPTION: This code snippet demonstrates how to use the fit_transform method of the SmartCorrelatedSelection class with a target variable to find correlated variables, train single feature models, and drop the remaining features from the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nXt = tr.fit_transform(X, y)\n```\n\n----------------------------------------\n\nTITLE: Accessing Correlated Feature Sets in Python\nDESCRIPTION: This code snippet accesses the correlated_feature_sets_ attribute of the SmartCorrelatedSelection object, which stores the groups of correlated features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntr.correlated_feature_sets_\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame with Added Columns\nDESCRIPTION: This code displays the first few rows of the DataFrame after adding extra columns ('var_a' and 'var_b'). It shows that these columns have been added to the DataFrame and initialized with the value 0.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n     pclass  survived  sibsp  parch     fare cabin embarked  var_a  var_b\n1000      3         1      0      0   7.7500     n        Q      0      0\n1001      3         1      2      0  23.2500     n        Q      0      0\n1002      3         1      2      0  23.2500     n        Q      0      0\n1003      3         1      2      0  23.2500     n        Q      0      0\n1004      3         1      0      0   7.7875     n        Q      0      0\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Performance Standard Deviation - Python\nDESCRIPTION: This snippet retrieves the standard deviation of the performance scores for each feature. The feature_performance_std_ attribute contains the standard deviations, providing insight into the variability of the performance estimates.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_performance_std_\n```\n\n----------------------------------------\n\nTITLE: Initializing MathFeatures Transformer\nDESCRIPTION: This code snippet demonstrates how to initialize the MathFeatures transformer to calculate the sum and mean of specified variables. It defines the variables to be used in the calculation, the functions to apply (sum and mean), and the names for the new variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/MathFeatures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntransformer = MathFeatures(\n        variables=[\n            'number_payments_first_quarter',\n            'number_payments_second_quarter',\n            'number_payments_third_quarter',\n            'number_payments_fourth_quarter'\n        ],\n        func=['sum','mean'],\n        new_variables_name=[\n            'total_number_payments',\n            'mean_number_payments'\n        ]\n    )\n\n    Xt = transformer.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Lag Features with Fill Value in Python\nDESCRIPTION: This snippet demonstrates how to create lag features and fill the resulting NaN values with a specified value (in this case, the string 'None') using the LagFeatures transformer in Feature Engine. It shows how to initialize the transformer, fit and transform the data, and print the head of the transformed DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(periods=[1, 2], fill_value='None')\n\nX_tr = lag_f.fit_transform(X)\n\nprint(X_tr.head())\n```\n\n----------------------------------------\n\nTITLE: Features to remove example - Python\nDESCRIPTION: This code snippet shows an example output of the features_to_drop_ attribute, demonstrating a list of feature names ('age', 's3', 's4', 's6') that were identified for removal by the RecursiveFeatureElimination process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureElimination.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n['age', 's3', 's4', 's6']\n```\n\n----------------------------------------\n\nTITLE: Fitting the Box-Cox Transformer\nDESCRIPTION: This snippet initializes a BoxCoxTransformer to transform 'LotArea' and 'GrLivArea' variables. It fits the transformer to the training data (X_train), allowing it to learn the optimal lambda values for each specified variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nboxcox = BoxCoxTransformer(variables = ['LotArea', 'GrLivArea'])\nboxcox.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Boxplot Visualization of Outliers in Python\nDESCRIPTION: This snippet generates boxplots for the 'age', 'fare', and 'sibsp' columns of the training dataset to visually identify potential outliers. It utilizes matplotlib for plotting and sets titles and labels for clarity.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX_train.boxplot(column=['age', 'fare', 'sibsp'])\nplt.title(\"Box plot - outliers\")\nplt.ylabel(\"variable values\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Discretizing MedInc with Arbitrary Boundaries\nDESCRIPTION: This code snippet shows how to discretize the 'MedInc' variable using the ArbitraryDiscretiser with user-defined boundaries.  It creates a dictionary specifying the bin boundaries for 'MedInc', instantiates the ArbitraryDiscretiser, and transforms the data.  The `return_object` and `return_boundaries` parameters are set to `False`, indicating that the transformed variable should be numerical and represent the bin number. It relies on previously loading the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/ArbitraryDiscretiser.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nuser_dict = {'MedInc': [0, 2, 4, 6, np.inf]}\n\ntransformer = ArbitraryDiscretiser(\n    binning_dict=user_dict, return_object=False, return_boundaries=False)\n\nX = transformer.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Load and Split Dataset for Discretization\nDESCRIPTION: This code snippet loads the Ames House Prices dataset using scikit-learn's `fetch_openml` function and splits it into training and testing sets. It imports necessary libraries such as `matplotlib.pyplot`, `sklearn.datasets`, and `sklearn.model_selection`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\nfrom feature_engine.discretisation import EqualFrequencyDiscretiser\n\n# Load dataset\nX, y = fetch_openml(name='house_prices', version=1, return_X_y=True, as_frame=True)\nX.set_index('Id', inplace=True)\n\n# Separate into train and test sets\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=42)\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data into Train and Test Sets - Python\nDESCRIPTION: This code snippet splits time series data into training and testing sets based on specific date ranges. It defines the end of the training period and the beginning of the test period, then uses these dates to slice the input DataFrame `df` and target variable `y` into `X_train`, `y_train`, `X_test`, and `y_test`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nend_train = '2014-12-31 23:59:59'\nX_train = df.loc[:end_train]\ny_train = y.loc[:end_train]\n\nbegin_test = '2014-12-31 17:59:59'\nX_test  = df.loc[begin_test:]\ny_test = y.loc[begin_test:]\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names after Transformation\nDESCRIPTION: This snippet shows how to obtain the names of the features in the transformed dataset after applying the MathFeatures transformer. The get_feature_names_out method retrieves the names of all original and newly created features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/MathFeatures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntransformer.get_feature_names_out(input_features=None)\n```\n\n----------------------------------------\n\nTITLE: Dropping Original Series after Transformation in Python\nDESCRIPTION: This code snippet demonstrates how to drop the original time series after transformation using the ExpandingWindowFeatures transformer by setting the `drop_original` parameter to `True`. It converts a pandas Series to a DataFrame, applies the transformer with the specified parameter, and then prints the head of the transformed dataframe, which contains only the new window features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwin_f = ExpandingWindowFeatures(\n    functions=[\"mean\", \"max\"],\n    drop_original=True,\n)\n\nX_tr = win_f.fit_transform(X['ambient_temp'].to_frame())\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Transform Data and Plot\nDESCRIPTION: Transforms the training and testing datasets using the fitted CategoricalImputer, then plots the value counts of 'MasVnrType' in the transformed test set. Depends on the CategoricalImputer having been fitted. Requires matplotlib.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = imputer.transform(X_train)\ntest_t = imputer.transform(X_test)\n\ntest_t['MasVnrType'].value_counts().plot.bar()\nplt.ylabel(\"Number of observations\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Printing Transformed Dataframe in Python\nDESCRIPTION: This code snippet prints the head of the transformed dataframe (Xt) after removing correlated features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(Xt.head())\n```\n\n----------------------------------------\n\nTITLE: Plotting Mean House Price by LotShape (Before Encoding)\nDESCRIPTION: This code snippet groups the training data by the 'LotShape' feature and plots the mean house price for each category using a bar chart.  It illustrates the relationship between the original 'LotShape' categories and the target variable before any encoding is applied. The plot is displayed using `plt.show()`, after setting the y-axis label with `plt.ylabel`. Dependencies include `pandas` and `matplotlib`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ny_train.groupby(X_train[\"LotShape\"]).mean().plot.bar()\nplt.ylabel(\"mean house price\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Relevance (F-Statistic) - Python\nDESCRIPTION: This code snippet generates a bar plot visualizing feature relevance scores. It uses `pandas` to create a Series from `sel.relevance_`, indexes it with feature names (`sel.variables_`), sorts the values in descending order, and then plots it as a bar chart using `matplotlib`.  Dependencies: pandas, matplotlib.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.Series(sel.relevance_, index=sel.variables_).sort_values(\n    ascending=False).plot.bar(figsize=(15, 4))\nplt.title(\"Relevance\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Transforming Train and Test Data with OutlierTrimmer in Python\nDESCRIPTION: This code applies the fitted OutlierTrimmer to both the training (X_train) and testing (X_test) datasets, creating transformed datasets (train_t and test_t) where outliers are capped or removed based on the fitted parameters.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = ot.transform(X_train)\ntest_t = ot.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Initializing SmartCorrelatedSelection - Python\nDESCRIPTION: Initializes an instance of the `SmartCorrelatedSelection` class, configuring it to use Pearson correlation, a threshold of 0.8, and select features based on correlation with the target variable. It raises an error if missing values are present.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# set up the selector\ntr = SmartCorrelatedSelection(\n    variables=None,\n    method=\"pearson\",\n    threshold=0.8,\n    missing_values=\"raise\",\n    selection_method=\"corr_with_target\",\n    estimator=None,\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying the accuracy of the model\nDESCRIPTION: This snippet displays the accuracy of the trained model after applying feature engineering steps and the logistic regression model in the pipeline. This gives a measure of the model's performance on the test dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nAccuracy: 0.76\n```\n\n----------------------------------------\n\nTITLE: Displaying DecisionTreeEncoder Mappings (Feature-engine)\nDESCRIPTION: This snippet displays the mappings created by the `DecisionTreeEncoder` after fitting it to the data. The `encoder.encoder_dict_` attribute contains a dictionary where keys are the variable names and values are dictionaries mapping original categories to their encoded values (mean target values from the decision tree leafs). There are no direct dependencies, it shows the fitted data from the previous step.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Lag Features with Drop NA in Python\nDESCRIPTION: This snippet demonstrates how to create lag features and drop rows containing NaN values in the resulting features using the LagFeatures transformer in Feature Engine. It also shows how to transform both the feature matrix (X) and the target variable (y).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(periods=[1, 2], drop_na=True)\n\nlag_f.fit(X)\n\nX_tr, y_tr = lag_f.transform_x_y(X, y)\n\nX_tr.shape, y_tr.shape, X.shape, y.shape\n```\n\n----------------------------------------\n\nTITLE: Get Feature Names After Cyclical Encoding\nDESCRIPTION: This code retrieves the names of the features generated by the CyclicalFeatures transformer using the `get_feature_names_out()` method. This is useful for knowing the names of the new sine and cosine features after the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncyclical.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Transform train and test data\nDESCRIPTION: This code applies the fitted LogTransformer to both the training and testing datasets (X_train and X_test), creating transformed datasets train_t and test_t respectively. The LogTransformer transforms the specified variables using the natural logarithm.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogTransformer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = logt.transform(X_train)\ntest_t = logt.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Adjusting Target Variable After Dropping NA\nDESCRIPTION: This code snippet demonstrates how to use the `transform_x_y` method of `DropMissingData` to remove rows with NA values from both the training set (X) and the target variable (y), ensuring that the target variable is aligned with the transformed DataFrame. The function returns the transformed X and y.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nXt, yt = dmd.transform_x_y(X, y)\nXt\n```\n\nLANGUAGE: python\nCODE:\n```\nyt\n```\n\nLANGUAGE: python\nCODE:\n```\nXt.shape, yt.shape\n```\n\n----------------------------------------\n\nTITLE: Feature-engine Pipeline Setup\nDESCRIPTION: Defines a Feature-engine pipeline that first drops rows with missing values using `DropMissingData` and then encodes categorical features using `OrdinalEncoder`. The `encoding_method` is set to 'arbitrary'. The pipeline is instantiated but not yet fitted.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(\n    [\n        (\"drop\", DropMissingData()),\n        (\"enc\", OrdinalEncoder(encoding_method=\"arbitrary\")),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing the Binner Dictionary from DecisionTreeDiscretiser in Python\nDESCRIPTION: This code snippet accesses and prints the `binner_dict_` attribute of the fitted `DecisionTreeDiscretiser`. The `binner_dict_` stores the fitted GridSearchCV objects for each feature, containing information about the decision trees used for discretization.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndisc.binner_dict_\n```\n\n----------------------------------------\n\nTITLE: Getting Initial Model Performance\nDESCRIPTION: This code snippet retrieves the initial model performance (r2 score) of the Linear Regression model trained on all features using the RecursiveFeatureAddition class.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# get the initial linear model performance, using all features\ntr.initial_model_performance_\n```\n\n----------------------------------------\n\nTITLE: Transforming data using a fitted WoE encoder\nDESCRIPTION: This code snippet demonstrates how to transform training and testing datasets using a previously fitted WoE encoder. It prints the head of the transformed training data to show the result of the encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = woe_encoder.transform(train_t)\ntest_t = woe_encoder.transform(test_t)\n\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed Dataframe Head - Python\nDESCRIPTION: This snippet shows the head of the transformed dataframe after dropping the correlated features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n          var_0     var_1     var_2     var_3     var_4     var_5    var_10  \\\n0  1.471061 -2.376400 -0.247208  1.210290 -3.247521  0.091527  2.070526\n1  1.819196  1.969326 -0.126894  0.034598 -2.910112 -0.186802  1.184820\n2  1.625024  1.499174  0.334123 -2.233844 -3.399345 -0.313881 -0.066448\n3  1.939212  0.075341  1.627132  0.943132 -4.783124 -0.468041  0.713558\n4  1.579307  0.372213  0.338141  0.951526 -3.199285  0.729005  0.398790\n\n       var_11\n0 -1.989335\n1 -1.309524\n2 -0.852703\n3  0.484649\n4 -0.186530\n```\n\n----------------------------------------\n\nTITLE: Accessing Encoding Dictionary\nDESCRIPTION: This code accesses and prints the encoding dictionary (`encoder.encoder_dict_`) which stores the mappings from each category to the decision tree's prediction for that category.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Recovering the Original Data using Inverse Transform\nDESCRIPTION: This code snippet demonstrates how to recover the original data from the transformed data using the `inverse_transform` method of the fitted `YeoJohnsonTransformer` instance. This allows for the reversal of the transformation applied to the training and testing datasets.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/YeoJohnsonTransformer.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntrain_unt = tf.inverse_transform(train_t)\ntest_unt = tf.inverse_transform(test_t)\n```\n\n----------------------------------------\n\nTITLE: Output of max() after transformation\nDESCRIPTION: This code snippet shows the expected output of the .max() call, demonstrating that the 'fare' and 'age' columns are capped to 200.0 and 50.0 respectively.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/ArbitraryOutlierCapper.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfare    200.0\nage      50.0\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting RandomSampleImputer with seed\nDESCRIPTION: This code snippet initializes the RandomSampleImputer with a random state derived from 'MSSubClass' and 'YrSold', sets the seed to 'observation', and the seeding method to 'add'. It then fits the imputer to the training data (X_train).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/RandomSampleImputer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\n\t# set up the imputer\n\timputer = RandomSampleImputer(\n                random_state=['MSSubClass', 'YrSold'],\n                seed='observation',\n                seeding_method='add'\n            )\n\n\t# fit the imputer\n\timputer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: LagFeatures Instantiation (Python)\nDESCRIPTION: This snippet initializes the `LagFeatures` transformer to create lag features from the 'demand' variable. It specifies the lag periods ([1, 3, 6]), handles missing values by ignoring them, and drops rows with NaN values introduced by the lag creation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlagf = LagFeatures(\n    variables=[\"demand\"],\n    periods=[1, 3, 6],\n    missing_values=\"ignore\",\n    drop_na=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Subtracting Datetime Features in Pandas (Days)\nDESCRIPTION: This code snippet subtracts the 'date2' column from the 'date1' column in a pandas DataFrame using the `.sub()` method, calculating the difference in days. The result is stored in a new column named 'diff'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata[\"diff\"] = data[\"date1\"].sub(data[\"date2\"])\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names with WindowFeatures (Python)\nDESCRIPTION: This snippet illustrates how to obtain the names of the generated features after applying the WindowFeatures transformer. It initializes the WindowFeatures transformer with default parameters, fits it to the input DataFrame X, and then calls the get_feature_names_out method to retrieve a list containing the names of all features present in the output DataFrame, including the original features and newly generated window features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwin_f = WindowFeatures()\n\nwin_f.fit(X)\n\nwin_f.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Initializing OrdinalEncoder for Ordered Encoding in Python\nDESCRIPTION: This code initializes an OrdinalEncoder from the feature-engine library to perform ordered encoding on the 'HouseAgeCategorical' variable. The `encoding_method` is set to 'ordered' to indicate that the categories should be encoded based on the mean of the target variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nordered_encoder = OrdinalEncoder(\n    encoding_method='ordered',\n    variables=['HouseAgeCategorical']\n )\n```\n\n----------------------------------------\n\nTITLE: Printing Time Series Target Variable\nDESCRIPTION: This snippet simply prints the time series target variable 'y', showing its values and corresponding datetime index.  It's used to display the target series created earlier.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny\n```\n\n----------------------------------------\n\nTITLE: Displaying categories after transforming 'embarked' in test dataset in Python\nDESCRIPTION: This code snippet displays the categories of the 'embarked' column in the test data after applying the `MatchCategories` transformation, showcasing the consistent encoding as the training set.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nIndex(['C', 'Missing', 'Q', 'S'], dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Load Breast Cancer Dataset\nDESCRIPTION: This code snippet loads the Breast Cancer Wisconsin (Diagnostic) dataset using scikit-learn's `load_breast_cancer` function. The dataset is loaded into pandas DataFrames for features (`cancer_X`) and target variable (`cancer_y`).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncancer_X, cancer_y = load_breast_cancer(return_X_y=True, as_frame=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Target Variables for Direct Forecasting\nDESCRIPTION: This code creates the target variables for direct multi-step forecasting. It shifts the demand column by different horizon values (1 to 6 hours) to create 6 target variables, one for each forecast step.  It then handles missing values introduced by the shifting.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 6\ny = pd.DataFrame(index=df.index)\nfor h in range(horizon):\n    y[f\"h_{h}\"] = df.shift(periods=-h, freq=\"h\")\ny.dropna(inplace=True)\ndf = df.loc[y.index]\nprint(y.head())\n```\n\nLANGUAGE: python\nCODE:\n```\n                                 h_0          h_1          h_2          h_3  \\\n    date_time\n    2002-01-01 00:00:00  6919.366092  7165.974188  6406.542994  5815.537828\n    2002-01-01 01:00:00  7165.974188  6406.542994  5815.537828  5497.732922\n    2002-01-01 02:00:00  6406.542994  5815.537828  5497.732922  5385.851060\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Interval Boundaries\nDESCRIPTION: Transforms the training and testing data using the DecisionTreeDiscretiser, which replaces the original values with the interval boundaries. Prints the first few rows of the transformed data to display the interval boundaries into which the original values have been categorized.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = disc.transform(X_train)\n    test_t = disc.transform(X_test)\n\n    print(train_t[['LotArea', 'GrLivArea']].head())\n```\n\n----------------------------------------\n\nTITLE: Displaying Dataframe Head\nDESCRIPTION: This code displays the first few rows of the training dataframe X_train, showcasing the data format and the included features. It's used for quick inspection.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n          pclass     sex        age  sibsp  parch     fare cabin embarked\n    501        2  female  13.000000      0      1  19.5000     M        S\n    588        2  female   4.000000      1      1  23.0000     M        S\n    402        2  female  30.000000      1      0  13.8583     M        C\n    1193       3    male  29.881135      0      0   7.7250     M        Q\n    686        3  female  22.000000      0      0   7.7250     M        Q\n```\n\n----------------------------------------\n\nTITLE: Transform 'hour' to Sine and Cosine\nDESCRIPTION: This code initializes the CyclicalFeatures transformer, fits it to the DataFrame containing the 'hour' column, and transforms the data. The resulting DataFrame will contain the original 'hour' column along with the 'hour_sin' and 'hour_cos' columns representing the cyclical encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncyclical = CyclicalFeatures(variables=None)\n\ndf = cyclical.fit_transform(df)\n\nprint(df.head())\n```\n\n----------------------------------------\n\nTITLE: Plotting Histograms of Transformed Data with Matplotlib\nDESCRIPTION: This code snippet plots histograms of the transformed 'LotArea' and 'GrLivArea' variables from the training set using pandas and matplotlib. It visualizes the effect of the Yeo-Johnson transformation on the variable distributions.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/YeoJohnsonTransformer.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ntrain_t[['LotArea', 'GrLivArea']].hist(bins=50, figsize=(10,4))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Fitting and Transforming Data with SelectByShuffling\nDESCRIPTION: This snippet demonstrates how to fit the SelectByShuffling transformer to the data (X, y) and then transform the data to keep only the selected features. The fit_transform method performs both fitting and transformation in one step.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nXt = tr.fit_transform(X, y)\n```\n\n----------------------------------------\n\nTITLE: Subtracting Datetime Features in Pandas (Years)\nDESCRIPTION: This code snippet subtracts 'date2' from 'date1' in a pandas DataFrame and converts the result to years using `np.timedelta64`. The `.div()` method is used to divide the time difference by the timedelta representing one year.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata[\"diff\"] = data[\"date1\"].sub(data[\"date2\"], axis=0).div(\n    np.timedelta64(1, \"Y\").astype(\"timedelta64[ns]\"))\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Applying ExpandingWindowFeatures Transformer\nDESCRIPTION: This snippet demonstrates the usage of the `ExpandingWindowFeatures` transformer from feature-engine. It initializes the transformer with a list of aggregation functions (mean, max, std), then fits and transforms the input DataFrame to create new expanding window features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.timeseries.forecasting import ExpandingWindowFeatures\n\nwin_f = ExpandingWindowFeatures(functions=[\"mean\", \"max\", \"std\"])\n\nX_tr = win_f.fit_transform(X)\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using LogCpTransformer with strictly positive data\nDESCRIPTION: This code snippet demonstrates how to initialize and use the LogCpTransformer on strictly positive variables from the California housing dataset. The transformer automatically detects the optimal constant C.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogCpTransformer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_california_housing\n\nfrom feature_engine.transformation import LogCpTransformer\n\n# Load dataset\nX, y = fetch_california_housing( return_X_y=True, as_frame=True)\n\n# Separate into train and test sets\nX_train, X_test, y_train, y_test =  train_test_split(\n    X, y, test_size=0.3, random_state=0)\n\n# set up the variable transformer\ntf = LogCpTransformer(variables = [\"MedInc\", \"HouseAge\"], C=\"auto\")\n\n# fit the transformer\ntf.fit(X_train)\n\n# learned constant C\ntf.C_\n\n# transform the data\ntrain_t= tf.transform(X_train)\ntest_t= tf.transform(X_test)\n\n# un-transformed variable\nX_train[\"MedInc\"].hist(bins=20)\nplt.title(\"MedInc - original distribution\")\nplt.ylabel(\"Number of observations\")\n\n# transformed variable\ntrain_t[\"MedInc\"].hist(bins=20)\nplt.title(\"MedInc - transformed distribution\")\nplt.ylabel(\"Number of observations\")\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Test Sets\nDESCRIPTION: This snippet splits the prepared dataset into training and testing sets using scikit-learn's train_test_split function. The 'survived' column is separated as the target variable. The resulting training data is then printed to the console.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropDuplicateFeatures.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Separate into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(['survived'], axis=1),\n    data['survived'],\n    test_size=0.3,\n    random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Performance Standard Deviation - Python\nDESCRIPTION: This code snippet accesses the `feature_performance_std_` attribute of the fitted SelectByTargetMeanPerformance object (sel). This attribute is a dictionary containing the standard deviation of the ROC-AUC score for each feature across the cross-validation folds.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_performance_std_\n```\n\n----------------------------------------\n\nTITLE: Categorical Imputation with CategoricalImputer\nDESCRIPTION: Categorical imputation replaces missing values with a new, specific category label (e.g., 'Missing', 'NaN'). The :class:`CategoricalImputer()` class in Feature-engine implements this. This is particularly useful for MNAR data as it flags missing values and avoids introducing bias from statistical estimates. While simple and assumption-free, it can introduce noise if the proportion of missing values is low.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/api_doc/imputation/index.rst#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Get Feature Names After DropMissingData Step\nDESCRIPTION: This code retrieves the feature names after the first step of the pipeline, which is DropMissingData. It uses the get_feature_names_out() method on the sliced pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipe[:1].get_feature_names_out()\n```\n\nLANGUAGE: python\nCODE:\n```\n['x1', 'x2']\n```\n\n----------------------------------------\n\nTITLE: Initializing MRMR with F-Statistic (FCQ) - Python\nDESCRIPTION: This code snippet initializes and fits the MRMR feature selection class with the F-statistic (FCQ) method for feature relevance.  It sets `regression=False` to indicate a classification task.  The fitted MRMR object can then be used to determine feature relevance, redundancy, and importance.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsel = MRMR(method=\"FCQ\", regression=False)\nsel.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: RareLabelEncoder with n_categories\nDESCRIPTION: This code initializes and applies the RareLabelEncoder to the toy dataframe. It groups rare labels only for variables with more than 3 unique categories, setting tol=0.05 and n_categories=3. The value counts after transformation are displayed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrare_encoder = RareLabelEncoder(tol=0.05, n_categories=3)\nrare_encoder.fit_transform(data)['var_A'].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Plotting Inverse Transformed Distributions\nDESCRIPTION: This snippet generates histograms for the inverse transformed 'LotArea' and 'GrLivArea' variables in the training dataset (train_unt).  This shows the distribution of the data after being transformed and then inverse transformed, demonstrating the reversibility of the Box-Cox transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrain_unt[['LotArea', 'GrLivArea']].hist(figsize=(10,5))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Datetime Subtraction with Custom Variable Names in Python\nDESCRIPTION: This code snippet shows how to use the DatetimeSubtraction transformer and assign a specific name to the new variable created by the transformation. It creates a Pandas DataFrame with two datetime columns and then applies DatetimeSubtraction, naming the output column 'my_new_var'. The number of variables in `variables` parameter multiplied by the variables in `reference` must match the number of names provided in the `new_variables_names` parameter.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.datetime import DatetimeSubtraction\n\ndata = pd.DataFrame({\n    \"date1\": pd.date_range(\"2019-03-05\", periods=5, freq=\"D\"),\n    \"date2\": pd.date_range(\"2018-03-05\", periods=5, freq=\"W\")})\n\ndtf = DatetimeSubtraction(\n    variables=\"date1\",\n    reference=\"date2\",\n    new_variables_names=[\"my_new_var\"]\n    )\n\ndata = dtf.fit_transform(data)\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Dropping NA based on percentage of non-NA values\nDESCRIPTION: This code snippet demonstrates how to use the `threshold` parameter of `DropMissingData` to specify the minimum percentage of non-NA values that a row must have to be kept.  This is equivalent to pandas.dropna's `thresh` parameter.  If a row has fewer non-NA values than specified by the threshold, it will be dropped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nX = pd.DataFrame(\n    dict(\n        x1=[2, 1, 1, np.nan, np.nan],\n        x2=[\"a\", np.nan, \"b\", np.nan, np.nan],\n        x3=[2, 3, 4, 5, np.nan],\n    )\n)\nX\n```\n\nLANGUAGE: python\nCODE:\n```\ndmd = DropMissingData(threshold=.5)\ndmd.fit(X)\ndmd.transform(X)\n```\n\nLANGUAGE: python\nCODE:\n```\ndmd = DropMissingData(threshold=.3)\ndmd.fit(X)\ndmd.transform(X)\n```\n\n----------------------------------------\n\nTITLE: StringSimilarityEncoder Transform Data\nDESCRIPTION: This code applies the fitted StringSimilarityEncoder to transform the training and testing datasets (X_train and X_test).  The transform method replaces the original categorical variables with new numerical features representing the string similarity to the most frequent categories as defined during the fit stage. The transformed data is then stored in train_t and test_t.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/StringSimilarityEncoder.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# transform the data\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n\ntest_t.head()\n```\n\n----------------------------------------\n\nTITLE: Finding Variables, Excluding Datetime\nDESCRIPTION: This code snippet demonstrates using `find_all_variables` to extract variable names from a DataFrame while excluding datetime variables. The `exclude_datetime=True` parameter ensures that datetime columns are not included in the resulting list.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_all_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nvars_all = find_all_variables(X, exclude_datetime=True)\n\nvars_all\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample DataFrame with Pandas\nDESCRIPTION: This code snippet initializes a Pandas DataFrame with sample data including numerical, categorical, and datetime variables. It demonstrates creating a DataFrame with columns 'Name', 'City', 'Age', 'Marks', and 'dob' using a dictionary and `pd.date_range`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/retain_variables_if_in_df.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndf = pd.DataFrame({\n    \"Name\": [\"tom\", \"nick\", \"krish\", \"jack\"],\n    \"City\": [\"London\", \"Manchester\", \"Liverpool\", \"Bristol\"],\n    \"Age\": [20, 21, 19, 18],\n    \"Marks\": [0.9, 0.8, 0.7, 0.6],\n    \"dob\": pd.date_range(\"2020-02-24\", periods=4, freq=\"T\"),\n})\n\nprint(df.head())\n```\n\n----------------------------------------\n\nTITLE: Checking Datetime Variables in DataFrame - Python\nDESCRIPTION: This code snippet demonstrates how to use the `check_datetime_variables` function from the `feature_engine.variable_handling` module. It takes a Pandas DataFrame `X` and a list of variable names (`[\"date2\", \"date3\"]`) as input. The function checks if the specified variables are or can be parsed as datetime objects. The resulting list `var_date` contains the names of the variables that can be parsed as datetime.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_datetime_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import check_datetime_variables\n\nvar_date = check_datetime_variables(X, [\"date2\", \"date3\"])\n\nvar_date\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names with ExpandingWindowFeatures in Python\nDESCRIPTION: This code snippet shows how to obtain the names of the original and new features created by the ExpandingWindowFeatures transformer using the `get_feature_names_out()` method. It initializes the transformer, fits it to the data, and then calls the method to retrieve a list of feature names.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwin_f = ExpandingWindowFeatures()\n\nwin_f.fit(X)\n\nwin_f.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Accessing category mappings in Python\nDESCRIPTION: This snippet demonstrates how to access the category mappings stored within the fitted MatchCategories transformer. The `category_dict_` attribute stores the mappings for each categorical variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# the transformer stores the mappings for categorical variables\nmatch_categories.category_dict_\n```\n\n----------------------------------------\n\nTITLE: Displaying unique 'cabin' categories from test set in Python\nDESCRIPTION: This code snippet outputs the unique values found in the 'cabin' column of the testing dataset before any transformation is applied, noting the presence of a new category 'G'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\narray(['M', 'F', 'E', 'G'], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Transforming data using RandomSampleImputer\nDESCRIPTION: This code snippet shows how to use the fitted RandomSampleImputer to transform both the training (X_train) and testing (X_test) datasets. It applies the imputation strategy learned during the fit stage to replace missing values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/RandomSampleImputer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\n\t# transform the data\n\ttrain_t = imputer.transform(X_train)\n\ttest_t = imputer.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Get Feature Names with LagFeatures in Python\nDESCRIPTION: This snippet demonstrates how to retrieve the names of the features generated by the LagFeatures transformer in Feature Engine. It uses the get_feature_names_out() method to obtain the feature names after applying lag transformations.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlag_f.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed Data (Bin Numbers)\nDESCRIPTION: Presents the output of transforming the data using bin numbers. It shows the first few rows of the 'LotArea' and 'GrLivArea' columns, displaying the bin number to which each original value was assigned.  This demonstrates the result of the discretization process when using ordinal bin numbers.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n          LotArea  GrLivArea\n    254         0          5\n    1066        0          8\n    638         1          1\n    799         0          9\n    380         0          9\n```\n\n----------------------------------------\n\nTITLE: Create Pandas DataFrame for Scaling\nDESCRIPTION: This code snippet creates a Pandas DataFrame for demonstrating the MeanNormalizationScaler. It initializes the DataFrame with sample data including numerical and categorical features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/scaling/MeanNormalizationScaler.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.scaling import MeanNormalizationScaler\n\ndf = pd.DataFrame.from_dict(\n    {\n        \"Name\": [\"tom\", \"nick\", \"krish\", \"jack\"],\n        \"City\": [\"London\", \"Manchester\", \"Liverpool\", \"Bristol\"],\n        \"Age\": [20, 21, 19, 18],\n        \"Height\": [1.80, 1.77, 1.90, 2.00],\n        \"Marks\": [0.9, 0.8, 0.7, 0.6],\n        \"dob\": pd.date_range(\"2020-02-24\", periods=4, freq=\"min\"),\n    })\n\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Fitting and Transforming Data - Python\nDESCRIPTION: Fits the `SmartCorrelatedSelection` transformer to the input data (`X` and `y`) and transforms the data, dropping the correlated features. The `fit_transform` method combines the fitting and transformation steps.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nXt = tr.fit_transform(X, y)\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Load Data\nDESCRIPTION: Imports necessary libraries like numpy, pandas, matplotlib, and sklearn's train_test_split. Loads the house price dataset into a pandas DataFrame and splits it into training and testing sets, dropping the 'Id' and 'SalePrice' columns from the features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/GeometricWidthDiscretiser.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\timport numpy as np\n\timport pandas as pd\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.model_selection import train_test_split\n\n\tfrom feature_engine.discretisation import GeometricWidthDiscretiser\n\n\t# Load dataset\n\tdata = pd.read_csv('houseprice.csv')\n\n\t# Separate into train and test sets\n\tX_train, X_test, y_train, y_test =  train_test_split(\n\t\t    data.drop(['Id', 'SalePrice'], axis=1),\n\t\t    data['SalePrice'], test_size=0.3, random_state=0)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Maximum Values after Transformation - Python\nDESCRIPTION: This code snippet calculates and prints the maximum values of the 'fare' and 'age' columns in the transformed training dataset (`train_t`). It demonstrates that the maximum values have been capped at the values specified in the `right_tail_caps_` attribute of the `Winsorizer`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/Winsorizer.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[['fare', 'age']].max()\n```\n\n----------------------------------------\n\nTITLE: StringSimilarityEncoder Dictionary\nDESCRIPTION: This code accesses and displays the encoder_dict_ attribute of a fitted StringSimilarityEncoder.  This dictionary contains the most popular categories that will be used to derive similarity variables for each specified categorical feature. Examining this dictionary allows for verification of the categories the encoder will consider during the transformation process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/StringSimilarityEncoder.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Initialize OrdinalEncoder with Arbitrary Encoding\nDESCRIPTION: Initializes an `OrdinalEncoder` from the `feature_engine.encoding` module to perform arbitrary ordinal encoding on specified categorical features ('cabin', 'embarked', 'sex') of the Titanic dataset. The `encoding_method` parameter is set to 'arbitrary' to ensure categories are encoded based on their order of appearance.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nencoder = OrdinalEncoder(\n         encoding_method='arbitrary',\n         variables=['cabin', 'embarked', 'sex'])\n```\n\n----------------------------------------\n\nTITLE: Check Categorical Variables\nDESCRIPTION: Accesses the `variables_` attribute of the fitted CategoricalImputer to view the list of categorical variables that were automatically detected and imputed. Shows the variables that were identified as categorical based on their data type.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimputer.variables_\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed Data Head\nDESCRIPTION: This snippet prints the head of the transformed training data for the specified target numeric features ('LotArea' and 'GrLivArea').  It shows the discrete bin indices assigned by the EqualFrequencyDiscretiser. Dependencies: pandas DataFrame, transformed by EqualFrequencyDiscretiser.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Transformed data\nprint(train_t[TARGET_NUMERIC_FEATURES].head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop - Python\nDESCRIPTION: This snippet demonstrates how to access the list of features that will be dropped by the `transform` method, stored in the `features_to_drop_` attribute of the fitted `DropHighPSIFeatures` transformer. This list is determined based on the PSI values exceeding the defined threshold. Dependencies include feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntransformer.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Predict with Pipeline\nDESCRIPTION: Shows the prediction made by the final estimator (`Lasso`) of the Feature-engine pipeline using the transformed data.  The pipeline automatically applies the `DropMissingData` and `OrdinalEncoder` steps before passing the transformed data to the Lasso model for prediction.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\narray([2., 2.])\n```\n\n----------------------------------------\n\nTITLE: Initializing YeoJohnsonTransformer in Feature-engine\nDESCRIPTION: This code snippet initializes the `YeoJohnsonTransformer` from Feature-engine, specifying the variables 'LotArea' and 'GrLivArea' to be transformed. It then fits the transformer to the training data `X_train` to learn the optimal lambda values for each variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/YeoJohnsonTransformer.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ntf = YeoJohnsonTransformer(variables = ['LotArea', 'GrLivArea'])\n\ntf.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Printing Transformed Data - Python\nDESCRIPTION: This snippet prints the head of the transformed DataFrame (Xt) to the console. This allows you to inspect the selected features after the transformation and verify that the low-performing features have been successfully removed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(Xt.head())\n```\n\n----------------------------------------\n\nTITLE: Categorizing a Numerical Variable with Pandas qcut in Python\nDESCRIPTION: This code snippet uses the `pd.qcut` function to convert the continuous 'HouseAge' variable into a categorical variable with four classes: 'new', 'newish', 'old', and 'very_old'. The `q=4` argument specifies that the data should be divided into four quantiles. The resulting categorical variable 'HouseAgeCategorical' is then added to the dataframe, and the first few rows of 'HouseAge' and 'HouseAgeCategorical' are printed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndata['HouseAgeCategorical'] = pd.qcut(data['HouseAge'], q=4, labels=['new',   'newish', 'old', 'very_old'])\n\nprint(data[['HouseAge', 'HouseAgeCategorical']].head())\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with OneHotEncoder in Python\nDESCRIPTION: This snippet demonstrates how to transform training and test data using the fitted OneHotEncoder. The `encoder.transform()` method applies the one-hot encoding to the specified categorical features in the input DataFrames (`X_train` and `X_test`). The transformed DataFrames are then printed to display the resulting one-hot encoded representation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = encoder.transform(X_train)\ntest_t = encoder.transform(X_test)\n\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Initializing RandomSampleImputer with observation seed\nDESCRIPTION: This code snippet demonstrates how to initialize the RandomSampleImputer with a specified random state based on observation and seeding method set to 'add'. The imputer will impute missing values using the sum of specified variables as the seed for each observation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/RandomSampleImputer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nRandomSampleImputer(random_state=['height', 'weight'],\n                                  seed='observation',\n                                  seeding_method='add')\n```\n\n----------------------------------------\n\nTITLE: Applying MathFeatures with String Functions\nDESCRIPTION: This code shows how to apply the MathFeatures transformer using string representations of mathematical functions like 'sum', 'prod', 'min', 'max', and 'std'. The transformer calculates these functions for the 'Age' and 'Marks' columns of the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/MathFeatures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransformer = MathFeatures(\n        variables=[\"Age\", \"Marks\"],\n        func = [\"sum\", \"prod\", \"min\", \"max\", \"std\"],\n    )\n\n    df_t = transformer.fit_transform(df)\n\n    print(df_t)\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Relevance (F-Statistic) - Python\nDESCRIPTION: This code snippet demonstrates how to access the computed feature relevance scores (F-statistic from ANOVA) after fitting the MRMR model. The `sel.relevance_` attribute stores an array of relevance scores, where each score corresponds to a feature.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsel.relevance_\n```\n\n----------------------------------------\n\nTITLE: Initializing ProbeFeatureSelection with Individual Strategy - Python\nDESCRIPTION: This code initializes the ProbeFeatureSelection with the `collective=False` parameter and uses the roc-auc score.  This sets up the transformer to train a model for each individual feature and uses that model's performance to estimate feature importance.  It configures cross-validation and a RandomForestClassifier with a limited number of estimators to keep runtime reasonable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nsel = ProbeFeatureSelection(\n    estimator=RandomForestClassifier(n_estimators=5, random_state=1),\n    variables=None,\n    collective=False,\n    scoring=\"roc_auc\",\n    n_probes=1,\n    distribution=\"all\",\n    cv=5,\n    random_state=150,\n    confirm_variables=False\n)\n\nsel.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Initializing DropHighPSIFeatures with split_distinct\nDESCRIPTION: This code snippet initializes the `DropHighPSIFeatures` transformer with the `split_col` and `split_distinct` parameters. The `split_col` parameter specifies the column to split the data on, and the `split_distinct` parameter indicates that the split should ensure that each sub-dataframe contains distinct values in the split column. It then fits the transformer to the input dataframe X.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ntransformer = DropHighPSIFeatures(split_col='group', split_distinct=True)\n    transformer.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Transforming with Categorical List Cutoff\nDESCRIPTION: This code initializes and transforms the data using a list of categories as the cutoff for splitting.  It computes PSI by comparing a dataframe filtered by the list of categories to the rest of the data. The result is stored in `X_no_drift`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntrans = DropHighPSIFeatures(split_col='group', cut_off=['A', 'C', 'E'])\nX_no_drift = trans.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Get Support from Feature Selection\nDESCRIPTION: Illustrates how to retrieve a boolean mask indicating selected features from a feature selection object, similar to Scikit-learn's feature selection classes. The `get_support()` method returns an array where True signifies a selected feature and False indicates a dropped feature. This method is typically used after a feature selection process has been completed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntr.get_support()\n```\n\n----------------------------------------\n\nTITLE: Creating Feature-engine Pipeline\nDESCRIPTION: This snippet demonstrates creating a Feature-engine Pipeline that includes outlier trimming and one-hot encoding. This custom pipeline is used to handle the target variable adjustments during transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.encoding import OneHotEncoder\n    from feature_engine.pipeline import Pipeline\n\n    pipe = Pipeline(\n        [\n            (\"outliers\", ot),\n            (\"enc\", OneHotEncoder()),\n        ]\n    )\n\n    pipe.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Getting Support - Python\nDESCRIPTION: Retrieves a boolean mask indicating which features were retained after the transformation.  This utilizes the `get_support()` method of the `SmartCorrelatedSelection` transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntr.get_support()\n```\n\n----------------------------------------\n\nTITLE: Showing Learned Mean Values\nDESCRIPTION: Accesses and displays the `imputer_dict_` attribute of the fitted `MeanMedianImputer` instance. This dictionary contains the learned mean values for each specified variable, which will be used to impute missing values during the transform step.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\t# Show mean values learned with the training data\n\tmmi.imputer_dict_\n```\n\n----------------------------------------\n\nTITLE: Initialize GeometricWidthDiscretiser\nDESCRIPTION: Initializes the GeometricWidthDiscretiser with a specified number of bins and a list of variables to discretize. The transformer is then fit to the training data to learn the interval boundaries. Requires X_train to fit the transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/GeometricWidthDiscretiser.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\t# set up the discretisation transformer\n\tdisc = GeometricWidthDiscretiser(bins=10, variables=['LotArea', 'GrLivArea'])\n\n\t# fit the transformer\n\tdisc.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Creating Window Features with Feature-engine\nDESCRIPTION: This code demonstrates how to create window features using Feature-engine's WindowFeatures class. It defines window sizes ('30min', '60min'), aggregation functions (mean, max, std), and the frequency to shift the calculations ('15min'). The fit_transform method adds the calculated window features to the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.timeseries.forecasting import WindowFeatures\n\nwin_f = WindowFeatures(\n    window=[\"30min\", \"60min\"], functions=[\"mean\", \"max\", \"std\"], freq=\"15min\",\n)\n\nX_tr = win_f.fit_transform(X)\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop - Python\nDESCRIPTION: This code snippet accesses the `features_to_drop_` attribute of the fitted SelectByTargetMeanPerformance object (sel). This attribute contains a list of the features that were not selected by the transformer based on the target mean performance.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsel.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop in Python\nDESCRIPTION: This code snippet accesses the features_to_drop_ attribute of the SmartCorrelatedSelection object, which stores the list of features that will be removed from the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntr.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Cyclical Encoding with Feature-engine and Custom Max Values\nDESCRIPTION: This code snippet demonstrates cyclical feature encoding with Feature-engine, explicitly setting the `max_values` parameter. This allows controlling the divisor used for scaling the original features before applying the sine and cosine transformations. It initializes the transformer with a dictionary specifying the maximum values for 'month', 'weekday', and 'hour', and sets `drop_original=True`.  Then fits and transforms the specified columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntr = CyclicalFeatures(\n        max_values={\"month\": 12, \"weekday\": 7, \"hour\": 24},\n        drop_original=True,\n    )\n    Xt = tr.fit_transform(df[[\"month\", \"weekday\", \"hour\"]])\n\n    print(Xt)\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Performance Drifts Standard Deviation\nDESCRIPTION: This snippet retrieves the standard deviation of the performance drifts for each feature. The tr.performance_drifts_std_ attribute provides insight into the variability of the performance change caused by shuffling each feature.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntr.performance_drifts_std_\n```\n\n----------------------------------------\n\nTITLE: Accessing First Two Pipeline Steps\nDESCRIPTION: This code shows how to extract the first two steps from the pipeline using slicing.  The pipeline is sliced to obtain the DropMissingData and OrdinalEncoder steps.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe[:2]\n```\n\nLANGUAGE: python\nCODE:\n```\nPipeline(steps=[('drop', DropMissingData()),\n             ('enc', OrdinalEncoder(encoding_method='arbitrary'))])\n```\n\n----------------------------------------\n\nTITLE: Plotting Transformed Variable Distributions\nDESCRIPTION: This snippet generates histograms for the transformed 'LotArea' and 'GrLivArea' variables in the training dataset (train_t). This visualization allows assessment of the effect of the Box-Cox transformation on the distribution of the variables, aiming to approximate a normal distribution.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[['LotArea', 'GrLivArea']].hist(figsize=(10,5))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying Feature Importances Tail - Python\nDESCRIPTION: This code retrieves and displays the last few feature importances from the ProbeFeatureSelection transformer, likely showcasing the importance scores of the probe features themselves.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_importances_.tail()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Loading Data\nDESCRIPTION: This code snippet imports necessary libraries like matplotlib, scikit-learn, and feature-engine's BoxCoxTransformer. It then loads the house prices dataset, splits it into training and testing sets, and prints the head of the training data to show the predictor variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\nfrom feature_engine.transformation import BoxCoxTransformer\n\ndata = fetch_openml(name='house_prices', as_frame=True)\ndata = data.frame\n\nX = data.drop(['SalePrice', 'Id'], axis=1)\ny = data['SalePrice']\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Initializing and splitting data\nDESCRIPTION: This code snippet loads the Titanic dataset, splits it into training and testing sets, and prints the head of the selected categorical columns. It sets up the data for use with the DecisionTreeEncoder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import DecisionTreeEncoder\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    handle_missing=True,\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train[['cabin', 'pclass', 'embarked']].head(10))\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with DropFeatures\nDESCRIPTION: This code snippet demonstrates how to transform the training and testing datasets using the fitted `DropFeatures` transformer.  The `transform` method removes the specified features from the input DataFrames.  This prepares the data for subsequent steps in a machine learning pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropFeatures.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntrain_t = transformer.transform(X_train)\ntest_t = transformer.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Printing Feature Correlations - Python\nDESCRIPTION: Calculates and prints the absolute correlation of each feature in the DataFrame `X` with the target variable `y`. This provides insights into the relationship between features and the target.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nprint(X.corrwith(y).abs())\n```\n\n----------------------------------------\n\nTITLE: Inverse Transform Scaled Data\nDESCRIPTION: This code snippet performs an inverse transformation on the scaled DataFrame, restoring the original values of the scaled variables. It uses the inverse_transform method of the fitted MeanNormalizationScaler.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/scaling/MeanNormalizationScaler.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# inverse transform the dataframe\ndf = scaler.inverse_transform(df)\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Transforming 'cabin' in test dataset using MatchCategories in Python\nDESCRIPTION: This snippet applies the fitted MatchCategories transformer to the 'cabin' column of the testing dataset and displays the unique values after transformation, demonstrating how unseen values are handled.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# unseen category 'G' will not get mapped to any integer\nmatch_categories.transform(test).cabin.unique()\n```\n\n----------------------------------------\n\nTITLE: Fit the DecisionTreeEncoder\nDESCRIPTION: This snippet fits the DecisionTreeEncoder to the training data (X_train, y_train). This step trains the decision trees for each specified variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nencoder.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Checking numerical variables\nDESCRIPTION: This code snippet demonstrates how to use the `check_numerical_variables` function from the `feature_engine.variable_handling` module to check if the 'Age' and 'Marks' columns of the DataFrame are numerical. It imports the function, calls it with the DataFrame and column names, and prints the returned list of variable names if they are numerical.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_numerical_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import check_numerical_variables\n\nvar_num = check_numerical_variables(df, ['Age', 'Marks'])\n\nvar_num\n```\n\n----------------------------------------\n\nTITLE: Checking Data Types After Discretization\nDESCRIPTION: This snippet displays the data types of the transformed numerical features. By default, the `EqualWidthDiscretiser` returns integer variables representing the bin indices.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ntrain_t[TARGET_NUMERIC_FEATURES].dtypes\n```\n\nLANGUAGE: Python\nCODE:\n```\nLotArea      int64\nGrLivArea    int64\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Displaying unique 'embarked' categories from test set in Python\nDESCRIPTION: This code snippet outputs the unique values found in the 'embarked' column of the testing dataset before any transformation is applied.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\narray(['Q', 'S', 'C'], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Displaying Value Counts after RareLabelEncoding\nDESCRIPTION: This displays the value counts for each category in the 'var_A' column after applying RareLabelEncoding.  It shows the distribution after rare categories have been grouped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nA       10\nB       10\nC        2\nRare     1\nName: var_A, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Imputer - Python\nDESCRIPTION: This code applies the fitted ArbitraryNumberImputer to transform both the training and testing datasets. It demonstrates how to replace missing values with the specified arbitrary number using the `.transform()` method.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/ArbitraryNumberImputer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# transform the data\ntrain_t= arbitrary_imputer.transform(X_train)\ntest_t= arbitrary_imputer.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Accessing Pandas Series Data\nDESCRIPTION: This code snippet retrieves a pandas Series representing the 'ambient_temp' column from a DataFrame named X. The Series is then printed to display its contents and properties (index, data type, name).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nX['ambient_temp']\n```\n\n----------------------------------------\n\nTITLE: Import libraries and load dataset\nDESCRIPTION: This code snippet imports the necessary libraries including matplotlib for plotting, sklearn for dataset loading and train-test split, and the LogTransformer from feature_engine. It then loads the Ames house prices dataset, separates features (X) and target (y), and splits the data into training and testing sets.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogTransformer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\nfrom feature_engine.transformation import LogTransformer\n\ndata = fetch_openml(name='house_prices', as_frame=True)\ndata = data.frame\n\nX = data.drop(['SalePrice', 'Id'], axis=1)\ny = data['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Initializing DropCorrelatedFeatures - Python\nDESCRIPTION: This snippet initializes the DropCorrelatedFeatures transformer with specified parameters. It sets the variables to None (meaning all numerical variables will be considered), the correlation method to 'pearson', and the correlation threshold to 0.8. Any feature with absolute correlation greater than 0.8 will be dropped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntr = DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.8)\n```\n\n----------------------------------------\n\nTITLE: Accessing Right Tail Caps - Python\nDESCRIPTION: This code snippet accesses and prints the `right_tail_caps_` attribute of the fitted `Winsorizer` object. This attribute contains a dictionary of the capping values calculated for the right tail of each specified variable ('age' and 'fare' in this example).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/Winsorizer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncapper.right_tail_caps_\n```\n\n----------------------------------------\n\nTITLE: Initializing OneHotEncoder with Automatic Categorical Feature Detection in Python\nDESCRIPTION: This snippet initializes a OneHotEncoder instance to automatically find and encode all categorical features in a Pandas DataFrame. It sets `variables` to None to enable automatic detection and `drop_last` to True to avoid multicollinearity. The `.fit()` method learns the categorical features and their unique categories from the training data (X_train).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nencoder = OneHotEncoder(\n    variables=None,\n    drop_last=True,\n    )\n\nencoder.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Accessing Dropped Features\nDESCRIPTION: Accesses the `features_to_drop_` attribute of the fitted `SelectByInformationValue` transformer to retrieve the list of features that were identified for removal due to their information value being below the specified threshold.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsel.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Check for Missing Values\nDESCRIPTION: Checks for missing values in the 'Alley' and 'MasVnrType' columns of the training dataset. This snippet uses the `.isnull().sum()` method to count the number of null values in each specified column, which is a preliminary step before imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX_train[['Alley', 'MasVnrType']].isnull().sum()\n```\n\n----------------------------------------\n\nTITLE: Initializing SelectByTargetMeanPerformance - Python\nDESCRIPTION: This code initializes the SelectByTargetMeanPerformance class with specific parameters. It sets the scoring metric to 'roc_auc', the number of bins to 3, the strategy to 'equal_frequency', the number of cross-validation folds to 3, and specifies that it's a regression problem.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsel = SelectByTargetMeanPerformance(\n    variables=None,\n    scoring=\"roc_auc\",\n    threshold=None,\n    bins=3,\n    strategy=\"equal_frequency\",\n    cv=3,\n    regression=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Display Feature Importance Standard Deviation\nDESCRIPTION: This code displays the standard deviation of the feature importances using the `head()` method to show the standard deviation for the first few features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_importances_std_.head()\n```\n\n----------------------------------------\n\nTITLE: Identifying numerical variables in DataFrame\nDESCRIPTION: This code snippet demonstrates how to use the `find_numerical_variables` function from `feature_engine.variable_handling` to identify numerical columns within a Pandas DataFrame. It imports the function, calls it with the DataFrame, and stores the result in the `var_num` variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_numerical_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import find_numerical_variables\n\nvar_num = find_numerical_variables(df)\n\nvar_num\n```\n\n----------------------------------------\n\nTITLE: Printing encoding dictionary with precision\nDESCRIPTION: This snippet displays the encoding dictionary after fitting the DecisionTreeEncoder with precision = 2. The predictions in the dictionary are now rounded to two decimal places.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    {'cabin': {'M': 0.3,\n      'E': 0.61,\n      'C': 0.61,\n      'D': 0.7,\n      'B': 0.7,\n      'A': 0.7,\n      'F': 0.7,\n      'T': 0.0,\n      'G': 0.5},\n     'pclass': {2: 0.44, 3: 0.26, 1: 0.62},\n     'embarked': {'S': 0.34, 'C': 0.55, 'Q': 0.37, 'Missing': 1.0}}\n```\n\n----------------------------------------\n\nTITLE: Transforming Data - Python\nDESCRIPTION: This snippet applies the ProbeFeatureSelection transformer to a dataset (`X_test`) to remove the identified non-informative features. The `.transform()` method modifies the data, reducing its dimensionality.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nXtr = sel.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Creating a sample Pandas DataFrame\nDESCRIPTION: This code snippet creates a sample Pandas DataFrame with numerical, categorical, and datetime variables. It imports the pandas library, defines a dictionary containing data for different columns, and then constructs the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_numerical_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndf = pd.DataFrame({\n    \"Name\": [\"tom\", \"nick\", \"krish\", \"jack\"],\n    \"City\": [\"London\", \"Manchester\", \"Liverpool\", \"Bristol\"],\n    \"Age\": [20, 21, 19, 18],\n    \"Marks\": [0.9, 0.8, 0.7, 0.6],\n    \"dob\": pd.date_range(\"2020-02-24\", periods=4, freq=\"T\"),\n})\n\nprint(df.head())\n```\n\n----------------------------------------\n\nTITLE: Train-Test Split\nDESCRIPTION: This code snippet splits the loaded dataset into training and testing sets using `train_test_split` from scikit-learn.  The `test_size` is set to 0.2, meaning 20% of the data will be used for testing, and `random_state` is set for reproducibility.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer_X,\n    cancer_y,\n    test_size=0.2,\n    random_state=3\n)\n\nX_train.shape, X_test.shape\n```\n\n----------------------------------------\n\nTITLE: Loading California Housing Dataset - Python\nDESCRIPTION: This snippet loads the California housing dataset using `sklearn.datasets.fetch_california_housing` and converts selected columns ('AveRooms', 'AveBedrms', 'AveOccup') to integer type. Dependencies: scikit-learn, pandas.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import fetch_california_housing\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nX[['AveRooms', 'AveBedrms', 'AveOccup']] = X[['AveRooms', 'AveBedrms', 'AveOccup']].astype(int)\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Displaying the first 5 rows of data\nDESCRIPTION: Shows the first 5 rows of the dataframe, including the column names and data. It uses the head() method to achieve that.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame with Relative Features\nDESCRIPTION: This code shows the result of applying the `RelativeFeatures` transformer, displaying the newly created feature 'total_phenols_sub_nonflavanoid_phenols' alongside the original features in the DataFrame. It gives an example of the data points within the created column.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/index.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71  2.43               15.6      127.0           2.80\n1    13.20        1.78  2.14               11.2      100.0           2.65\n2    13.16        2.36  2.67               18.6      101.0           2.80\n3    14.37        1.95  2.50               16.8      113.0           3.85\n4    13.24        2.59  2.87               21.0      118.0           2.80\n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0        3.06                  0.28             2.29             5.64  1.04\n1        2.76                  0.26             1.28             4.38  1.05\n2        3.24                  0.30             2.81             5.68  1.03\n3        3.49                  0.24             2.18             7.80  0.86\n4        2.69                  0.39             1.82             4.32  1.04\n\n   od280/od315_of_diluted_wines  proline  \\\n0                          3.92   1065.0\n1                          3.40   1050.0\n2                          3.17   1185.0\n3                          3.45   1480.0\n4                          2.93    735.0\n\n   total_phenols_sub_nonflavanoid_phenols\n0                                    2.52\n1                                    2.39\n2                                    2.50\n3                                    3.61\n4                                    2.41\n```\n\n----------------------------------------\n\nTITLE: Accessing Last Pipeline Step\nDESCRIPTION: This code shows how to extract the last step from the pipeline, which is Lasso, using slicing.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipe[-1:]\n```\n\nLANGUAGE: python\nCODE:\n```\nPipeline(steps=[('lasso', Lasso(random_state=10))])\n```\n\n----------------------------------------\n\nTITLE: Showing unique values of 'cabin' in train dataset in Python\nDESCRIPTION: This snippet demonstrates how the unique categories are represented in the 'cabin' column of the training dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# categories present in the train data\ntrain.cabin.unique()\n```\n\n----------------------------------------\n\nTITLE: Getting Standard Deviation of Performance Drifts\nDESCRIPTION: This code snippet retrieves the standard deviation of the performance drifts for each feature, providing insight into the variability of the performance changes observed during feature addition.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Get the performance drift of each feature\ntr.performance_drifts_std_\n```\n\n----------------------------------------\n\nTITLE: Dropping NaN Rows: WindowFeatures\nDESCRIPTION: This example illustrates how to drop rows containing NaN values introduced by the WindowFeatures transformation. Setting `drop_na=True` during WindowFeatures initialization ensures that rows with insufficient data to create windows are automatically removed. The code demonstrates instantiating WindowFeatures, fitting it to data, and transforming both X and y.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwin_f = WindowFeatures(\n    window=[\"30min\", \"60min\"],\n    functions=[\"mean\", ],\n    freq=\"15min\",\n    drop_na=True,\n)\n\nwin_f.fit(X)\n\nX_tr, y_tr = win_f.transform_x_y(X, y)\n\nX.shape, y.shape, X_tr.shape, y_tr.shape\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Performance with Standard Deviation - Python\nDESCRIPTION: This snippet shows how to create a bar plot of the feature performance along with its standard deviation using pandas and matplotlib. It concatenates the mean performance and standard deviation into a DataFrame, then creates a bar plot with error bars representing the standard deviation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n        pd.Series(sel.feature_performance_),\n        pd.Series(sel.feature_performance_std_)\n    ], axis=1\n    )\nr.columns = ['mean', 'std']\n\nr['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)\n\nplt.title(\"Single feature model Performance\")\nplt.ylabel('R2')\nplt.xlabel('Features')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Create Lag Features with `periods` parameter\nDESCRIPTION: This code creates lag features using the `LagFeatures` transformer from feature-engine, shifting numerical variables by one period (row) forward. It initializes the transformer with `periods=1`, then fits and transforms the input DataFrame `X`. The resulting DataFrame `X_tr` includes the original features and the newly created lag features. NaN values are introduced for the first row of the lag features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.timeseries.forecasting import LagFeatures\n\nlag_f = LagFeatures(periods=1)\n\nX_tr = lag_f.fit_transform(X)\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Splitting into Train/Test Sets - Python\nDESCRIPTION: This code snippet loads the Titanic dataset, splits it into training and testing sets, and displays the head of the training data. It uses `sklearn.model_selection` for splitting and `feature_engine.datasets` and `feature_engine.outliers` for loading the data and the `Winsorizer` class respectively. The `load_titanic` function handles missing values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/Winsorizer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.outliers import Winsorizer\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    predictors_only=True,\n    handle_missing=True,\n)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: StringSimilarityEncoder Initialization\nDESCRIPTION: This snippet initializes the StringSimilarityEncoder, specifying 'top_categories' as 2, indicating that only the two most frequent categories for each of the specified variables ('name', 'home.dest', 'ticket') will be encoded. The 'ignore_format=True' ensures that the encoder handles numeric values correctly.  This configuration limits feature space expansion and focuses on the most relevant categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/StringSimilarityEncoder.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# set up the encoder\nencoder = StringSimilarityEncoder(\n    top_categories=2,\n    variables=['name', 'home.dest', 'ticket'],\n    ignore_format=True\n    )\n\n# fit the encoder\nencoder.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Getting Lag Feature Names with get_feature_names_out, Python\nDESCRIPTION: Illustrates how to retrieve the names of the lagged features created by the `LagFeatures` transformer using the `get_feature_names_out` method. This example shows how to obtain a list of all features in the output dataframe, including the original features and newly created lag features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(periods=[1, 2])\n\nlag_f.fit(X)\n\nlag_f.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Create Toy DataFrame for Visualization\nDESCRIPTION: This code creates a pandas DataFrame with a single 'hour' column containing values from 0 to 23. This DataFrame is used to visualize the effect of cyclical encoding on a time-based variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame([i for i in range(24)], columns=['hour'])\n```\n\n----------------------------------------\n\nTITLE: Transforming Data after MI-based Selection - Python\nDESCRIPTION: This code snippet transforms a dataset using the fitted MRMR selector based on Mutual Information(MIQ), keeping only the features selected for relevance and non-redundancy. In this specific example, also latitude and longitude are kept as part of the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nXtr = sel.transform(X)\nprint(Xtr.head())\n```\n\n----------------------------------------\n\nTITLE: Splitting Data by Numerical Cutoff with DropHighPSIFeatures - Python\nDESCRIPTION: This snippet shows how to split data based on a numerical cutoff value using the `cut_off` parameter in `DropHighPSIFeatures`. It creates a sample dataframe with customer IDs and seniority, and then uses `DropHighPSIFeatures` to split the data based on the `customer_id` column and a cutoff value of 250. Dependencies include pandas, scikit-learn, and feature-engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom feature_engine.selection import DropHighPSIFeatures\n\nX, y = make_classification(\n        n_samples=500,\n        n_features=6,\n        random_state=0\n    )\n\ncolnames = [\"var_\" + str(i) for i in range(6)]\nX = pd.DataFrame(X, columns=colnames)\n\n# Let's add a variable for the customer ID\nX['customer_id'] = [customer_id for customer_id in range(1, 501)]\n\n# Add a column with the seniority... that is related to the customer ID\nX['seniority'] = 100 - X['customer_id'] // 10\n\ntransformer = DropHighPSIFeatures(split_col='customer_id', cut_off=250)\ntransformer.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Boxplot Visualization for 'fare' in Python\nDESCRIPTION: This snippet creates a boxplot specifically for the 'fare' column of the training dataset. The purpose is to visualize the distribution and potential outliers in this variable. Uses matplotlib.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nX_train.boxplot(column=['fare'])\nplt.title(\"Box plot - outliers\")\nplt.ylabel(\"variable values\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing SelectByInformationValue with Bins\nDESCRIPTION: Initializes the `SelectByInformationValue` transformer to handle both categorical and numerical variables by binning the numerical variables. It sets the `bins` parameter to 5, the `strategy` to \"equal_frequency\", and the `threshold` to 0.2. The fit method is called on the training data, after dropping specified categorical columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsel = SelectByInformationValue(\n    bins=5,\n    strategy=\"equal_frequency\",\n    threshold=0.2,\n)\n\nsel.fit(X_train.drop([\"A4\", \"A5\", \"A7\"], axis=1), y_train)\n```\n\n----------------------------------------\n\nTITLE: Initializing OneHotEncoder for Binary Variable Encoding in Python\nDESCRIPTION: This snippet initializes a OneHotEncoder instance to encode binary variables into k-1 dummies (one dummy variable) and other categorical variables into k dummies by setting `drop_last_binary` to True and `drop_last` to False. The `variables` parameter specifies the categorical columns to be encoded.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nohe = OneHotEncoder(\n    variables=['sex', 'cabin','embarked'],\n    drop_last=False,\n    drop_last_binary=True,\n    )\n\ntrain_t = ohe.fit_transform(X_train)\ntest_t = ohe.transform(X_test)\n\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Calculating Standard Deviation of Correlated Features in Python\nDESCRIPTION: This code snippet calculates the standard deviation of the features within a specific correlated feature set.  This is to corroborate that the retained feature had the highest variability.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nX[list(tr.correlated_feature_sets_[0])].std()\n```\n\n----------------------------------------\n\nTITLE: Inspect Left Tail Caps (Gaussian)\nDESCRIPTION: This snippet inspects the lower limit for the 'age' feature beyond which values are considered outliers when using the Gaussian (Z-score) capping method.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\not_age.left_tail_caps_\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names\nDESCRIPTION: This snippet retrieves the names of the features present in the transformed dataset after outlier trimming. This is useful to understand the structure of the data after the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\not.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Checking for non-numerical variables\nDESCRIPTION: This code snippet demonstrates how the `check_numerical_variables` function handles non-numerical variables. It calls the function with the DataFrame and a list of column names including 'Name', which is a categorical variable. This will raise a TypeError because 'Name' is not numerical.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_numerical_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncheck_numerical_variables(df, ['Age', 'Name'])\n```\n\n----------------------------------------\n\nTITLE: Displaying Cardinality Results of Categorical Features in Python\nDESCRIPTION: This code snippet prints the result of applying the `.nunique()` method to the specified columns of the training data. The output is a pandas series where the index contains column names and the values contain the number of unique elements in each column.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX_train[['sex', 'pclass', 'cabin', 'embarked']].nunique()\n```\n\n----------------------------------------\n\nTITLE: Print head of transformed data\nDESCRIPTION: This snippet prints the first 10 rows of the transformed 'cabin', 'pclass', and 'embarked' columns in the training dataset, demonstrating the effect of the encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[['cabin', 'pclass', 'embarked']].head(10)\n```\n\n----------------------------------------\n\nTITLE: Displaying First Rows of DataFrame\nDESCRIPTION: This code snippet displays the first 3 rows of the DataFrame `df_sim` using the `head()` method.  It's used to show the original data before transformation, enabling comparison with transformed and inverse-transformed data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\tdf_sim.head(3)\n```\n\n----------------------------------------\n\nTITLE: Create Cyclical Features DataFrame\nDESCRIPTION: This code creates a pandas DataFrame with 'day' and 'months' columns to be used in subsequent examples. It is a toy dataset with integer values representing days of the week and months of the year, showcasing data used to demonstrate cyclical feature encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.creation import CyclicalFeatures\n\ndf = pd.DataFrame({\n    'day': [6, 7, 5, 3, 1, 2, 4],\n    'months': [3, 7, 9, 12, 4, 6, 12],\n    })\n```\n\n----------------------------------------\n\nTITLE: Boxplot of Transformed 'sibsp' and 'fare' in Python\nDESCRIPTION: This snippet creates boxplots for the 'sibsp' and 'fare' columns in the transformed training dataset (train_t). This visualization helps assess the impact of outlier trimming on the data distribution.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrain_t.boxplot(column=['sibsp', \"fare\"])\nplt.title(\"Box plot - outliers\")\nplt.ylabel(\"variable values\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop after Split by Cutoff - Python\nDESCRIPTION: Demonstrates retrieving the list of features identified for removal by `DropHighPSIFeatures` when using a specific cutoff for splitting. Dependencies include feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntransformer.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Installing Feature-engine from PyPI\nDESCRIPTION: This code snippet demonstrates how to install the feature-engine library using pip, the Python package installer. It allows users to easily integrate feature-engine into their Python environment for feature engineering tasks.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install feature_engine\n```\n\n----------------------------------------\n\nTITLE: Creating a sample Pandas DataFrame\nDESCRIPTION: This code snippet creates a Pandas DataFrame with mixed data types (string, integer, float, and datetime) for demonstration purposes.  It uses `pd.DataFrame` to construct the DataFrame and `pd.date_range` to create a sequence of datetime objects. The resulting DataFrame is then printed to the console using `df.head()`. \nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_numerical_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndf = pd.DataFrame({\n    \"Name\": [\"tom\", \"nick\", \"krish\", \"jack\"],\n    \"City\": [\"London\", \"Manchester\", \"Liverpool\", \"Bristol\"],\n    \"Age\": [20, 21, 19, 18],\n    \"Marks\": [0.9, 0.8, 0.7, 0.6],\n    \"dob\": pd.date_range(\"2020-02-24\", periods=4, freq=\"T\"),\n})\n\nprint(df.head())\n```\n\n----------------------------------------\n\nTITLE: Decision Tree Discretizer Transformation\nDESCRIPTION: Applies the fitted DecisionTreeDiscretiser to transform the training and testing data. The transformed data, train_t and test_t, will contain the discretized values of the features specified during the Discretizer instantiation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndisc.fit(X_train, y_train)\n\n    train_t= disc.transform(X_train)\n    test_t= disc.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Access Lagged Variables with `variables_` attribute\nDESCRIPTION: This snippet demonstrates how to access the names of the variables for which lag features were created using the `variables_` attribute of the fitted `LagFeatures` transformer. It shows the numerical columns that were automatically selected for lag feature generation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlag_f.variables_\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame with Math Features\nDESCRIPTION: This code shows the transformed DataFrame after applying the `MathFeatures` transformer, displaying the newly created features 'sum_flavanoids_proanthocyanins_proline' and 'mean_flavanoids_proanthocyanins_proline' alongside the original features. It shows the output including the newly created columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/index.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71  2.43               15.6      127.0           2.80\n1    13.20        1.78  2.14               11.2      100.0           2.65\n2    13.16        2.36  2.67               18.6      101.0           2.80\n3    14.37        1.95  2.50               16.8      113.0           3.85\n4    13.24        2.59  2.87               21.0      118.0           2.80\n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0        3.06                  0.28             2.29             5.64  1.04\n1        2.76                  0.26             1.28             4.38  1.05\n2        3.24                  0.30             2.81             5.68  1.03\n3        3.49                  0.24             2.18             7.80  0.86\n4        2.69                  0.39             1.82             4.32  1.04\n\n   od280/od315_of_diluted_wines  proline  \\\n0                          3.92   1065.0\n1                          3.40   1050.0\n2                          3.17   1185.0\n3                          3.45   1480.0\n4                          2.93    735.0\n\n   total_phenols_sub_nonflavanoid_phenols  \\\n0                                    2.52\n1                                    2.39\n2                                    2.50\n3                                    3.61\n4                                    2.41\n\n   sum_flavanoids_proanthocyanins_proline\n0                                 1070.35\n1                                 1054.04\n2                                 1191.05\n3                                 1485.67\n```\n\n----------------------------------------\n\nTITLE: Plotting Shifted Variable Distribution (income)\nDESCRIPTION: This code uses Seaborn's `ecdfplot` to visualize the cumulative distribution function (CDF) of the 'income' feature, grouped by the 'group' column.  The purpose is to illustrate the distribution shift between different groups for a variable identified as having a high PSI value and being a target for removal. Requires Seaborn and Matplotlib.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nsns.ecdfplot(data=X, x='income', hue='group')\n```\n\n----------------------------------------\n\nTITLE: Transform Data Using GeometricWidthDiscretiser\nDESCRIPTION: Transforms the training and testing data using the fitted GeometricWidthDiscretiser, sorting the values into the learned intervals. Requires the transformer to be fitted first using the `fit()` method. Returns transformed pandas DataFrames.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/GeometricWidthDiscretiser.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\t# transform the data\n\ttrain_t= disc.transform(X_train)\n\ttest_t= disc.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Displaying the shape of train and test sets\nDESCRIPTION: Displays the shape (number of rows and columns) of both the training and testing feature sets (X_train and X_test).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nX_train.shape, X_test.shape\n```\n\n----------------------------------------\n\nTITLE: Defining categorical and numerical features\nDESCRIPTION: This code defines lists containing the names of categorical and numerical features to be used in the data transformation pipeline. These lists will be used by the feature engineering steps.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncategorical_features = ['cabin', 'pclass', 'embarked', 'sex', 'sibsp', 'parch']\nnumerical_features = ['fare', 'age']\nall = categorical_features + numerical_features\n```\n\n----------------------------------------\n\nTITLE: Accessing Evaluated Variables\nDESCRIPTION: Accesses the `variables_` attribute of the fitted `SelectByInformationValue` transformer to retrieve the list of variables that were evaluated by the transformer for information value.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsel.variables_\n```\n\n----------------------------------------\n\nTITLE: Displaying categories after transforming 'embarked' in train dataset in Python\nDESCRIPTION: This code snippet displays the categories of the 'embarked' column in the training data after applying the `MatchCategories` transformation, demonstrating that it maintains the original categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nIndex(['C', 'Missing', 'Q', 'S'], dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Dataframe Creation with Correlated Variables - Python\nDESCRIPTION: This function creates a pandas DataFrame with correlated variables using the make_classification function from sklearn.datasets. It returns a DataFrame with 12 features, where 4 are redundant, simulating correlated data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom feature_engine.selection import DropCorrelatedFeatures\n\n# make dataframe with some correlated variables\ndef make_data():\n    X, y = make_classification(n_samples=1000,\n                           n_features=12,\n                           n_redundant=4,\n                           n_clusters_per_class=1,\n                           weights=[0.50],\n                           class_sep=2,\n                           random_state=1)\n\n    # trasform arrays into pandas df and series\n    colnames = ['var_'+str(i) for i in range(12)]\n    X = pd.DataFrame(X, columns =colnames)\n    return X\n\nX = make_data()\n```\n\n----------------------------------------\n\nTITLE: Install Test Dependencies\nDESCRIPTION: This snippet installs the dependencies required for testing the Feature-engine package.  It uses pip and the `test_requirements.txt` file, which lists all the test-related packages.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install -r test_requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing Data and DropHighPSIFeatures for Categorical Split\nDESCRIPTION: This code snippet shows how to create a sample dataset with a categorical feature and related features. It then initializes the `DropHighPSIFeatures` transformer to split the data based on a categorical column, using a single category as the cutoff.  Dependencies include pandas and sklearn.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.datasets import make_classification\nfrom feature_engine.selection import DropHighPSIFeatures\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=6,\n    random_state=0\n)\n\ncolnames = [\"var_\" + str(i) for i in range(6)]\nX = pd.DataFrame(X, columns=colnames)\n\n# Add a categorical column\nX['group'] = [\"A\", \"B\", \"C\", \"D\", \"E\"] * 200\n\n# And two category related features\nX['group_means'] = X.group.map({\"A\": 1, \"B\": 2, \"C\": 0, \"D\": 1.5, \"E\": 2.5})\nX['shifted_feature'] = X['group_means'] + X['var_2']\n\ntransformer = DropHighPSIFeatures(split_col='group', cut_off='C')\nX_transformed = transformer.fit_transform(X)\n```\n\n----------------------------------------\n\nTITLE: Output of find_categorical_variables\nDESCRIPTION: This code snippet shows the expected output of the `find_categorical_variables` function, which is a list of categorical variable names found in the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_categorical_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n['cat_var1', 'cat_var2']\n```\n\n----------------------------------------\n\nTITLE: Reverted categories example\nDESCRIPTION: This snippet shows the data after the inverse transformation. Here we see the original categorical variables as they were before encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n         cabin  pclass embarked\n1139     M       3        S\n533      M       2        S\n459      M       2        S\n1150     M       3        S\n393      M       2        S\n1189     G       3        S\n5        C       1        S\n231      C       1        S\n330      M       2        S\n887      M       3        S\n```\n\n----------------------------------------\n\nTITLE: Visualize Output Boundaries\nDESCRIPTION: This snippet prints the head of the transformed data when `return_boundaries=True`. The output shows the interval limits for each bin instead of integer indices. Dependencies: pandas DataFrame, transformed by EqualFrequencyDiscretiser with return_boundaries=True.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Visualize output (boundaries)\nprint(test_t[TARGET_NUMERIC_FEATURES].head())\n```\n\n----------------------------------------\n\nTITLE: Displaying Predictors Training Set - Python\nDESCRIPTION: This snippet prints the head of the transformed test set `Xt`, so that the input features can be examined. This helps to ensure that the regression model is receiving the intended variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprint(Xt.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Learned Lambda Values\nDESCRIPTION: This snippet displays the learned lambda values for the specified variables ('LotArea' and 'GrLivArea') after fitting the BoxCoxTransformer. These lambda values are the optimal parameters found during the fitting process for performing the Box-Cox transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nboxcox.lambda_dict_\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample DataFrame with Pandas and Scikit-learn\nDESCRIPTION: This code snippet generates a sample pandas DataFrame with numerical, categorical, and datetime variables. It uses `make_classification` from `sklearn.datasets` to create numerical features and then adds categorical and datetime columns. This DataFrame is used to demonstrate the `find_categorical_variables` function.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_categorical_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_redundant=1,\n    n_clusters_per_class=1,\n    weights=[0.50],\n    class_sep=2,\n    random_state=1,\n)\n\n# transform arrays into pandas df and series\ncolnames = [f\"num_var_{i+1}\" for i in range(4)]\nX = pd.DataFrame(X, columns=colnames)\n\nX[\"cat_var1\"] = [\"Hello\"] * 1000\nX[\"cat_var2\"] = [\"Bye\"] * 1000\n\nX[\"date1\"] = pd.date_range(\"2020-02-24\", periods=1000, freq=\"T\")\nX[\"date2\"] = pd.date_range(\"2021-09-29\", periods=1000, freq=\"H\")\nX[\"date3\"] = [\"2020-02-24\"] * 1000\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Finding Unique Categories\nDESCRIPTION: This code snippet determines and prints the unique categories present in the 'cabin' column of the training dataset X_train. This helps understand the cardinality of the feature before encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nX_train[\"cabin\"].unique()\n```\n\n----------------------------------------\n\nTITLE: Output Right Tail Caps (Gaussian)\nDESCRIPTION: This snippet displays the output of the upper limit for outlier capping using the Gaussian (Z-score) method for the 'age' feature.  It provides the value beyond which data points are considered outliers.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n{'age': 67.73951212364803}\n```\n\n----------------------------------------\n\nTITLE: Initialize CyclicalFeatures Transformer\nDESCRIPTION: This code initializes the CyclicalFeatures transformer with default parameters.  `variables=None` indicates that the transformer should automatically detect numerical variables. `drop_original=False` specifies that the original variables should be kept alongside the encoded features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncyclical = CyclicalFeatures(variables=None, drop_original=False)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Wine Quality Dataset\nDESCRIPTION: This code snippet displays the first few rows of the wine quality dataset.  It shows the structure of the data and the available features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/index.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71  2.43               15.6      127.0           2.80\n1    13.20        1.78  2.14               11.2      100.0           2.65\n2    13.16        2.36  2.67               18.6      101.0           2.80\n3    14.37        1.95  2.50               16.8      113.0           3.85\n4    13.24        2.59  2.87               21.0      118.0           2.80\n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0        3.06                  0.28             2.29             5.64  1.04\n1        2.76                  0.26             1.28             4.38  1.05\n2        3.24                  0.30             2.81             5.68  1.03\n3        3.49                  0.24             2.18             7.80  0.86\n4        2.69                  0.39             1.82             4.32  1.04\n\n   od280/od315_of_diluted_wines  proline\n0                          3.92   1065.0\n1                          3.40   1050.0\n2                          3.17   1185.0\n3                          3.45   1480.0\n4                          2.93    735.0\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed Data After Discretization\nDESCRIPTION: This snippet displays the first few rows of the transformed data for the selected numerical features after applying equal-width discretization. The original continuous values are replaced with the corresponding bin indices.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Transformed data\nprint(train_t[TARGET_NUMERIC_FEATURES].head())\n```\n\nLANGUAGE: Python\nCODE:\n```\n\t\t  LotArea  GrLivArea\n\tId                      \n\t136         0          2\n\t1453        0          1\n\t763         0          2\n\t933         0          2\n\t436         0          2\n```\n\n----------------------------------------\n\nTITLE: Initializing OutlierTrimmer with IQR in Python\nDESCRIPTION: This code initializes the OutlierTrimmer class from the feature_engine library, setting the capping method to IQR, tail to 'right', fold to 1.5, and specifying the variables 'sibsp' and 'fare' to be trimmed.  The `tail='right'` argument means that it only considers upper bounds.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\not = OutlierTrimmer(capping_method='iqr',\n                    tail='right',\n                    fold=1.5,\n                    variables=['sibsp', 'fare'],\n                    )\n```\n\n----------------------------------------\n\nTITLE: Fit and Transform Pipeline\nDESCRIPTION: Fits the Feature-engine pipeline to the DataFrame X and target y and then transforms X.  This returns only the transformed feature matrix (X) with rows containing NaNs removed and categorical variables encoded.  The target variable is not realigned in this step.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npipe.fit_transform(X, y)\n```\n\n----------------------------------------\n\nTITLE: Transforming 'cabin' in train dataset using MatchCategories in Python\nDESCRIPTION: This snippet applies the fitted MatchCategories transformer to the 'cabin' column of the training dataset and displays the unique values after the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmatch_categories.transform(train).cabin.unique()\n```\n\n----------------------------------------\n\nTITLE: Import necessary libraries\nDESCRIPTION: This code snippet imports the required libraries and modules for data splitting, dataset loading, and mean encoding using scikit-learn and feature-engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import MeanEncoder\n```\n\n----------------------------------------\n\nTITLE: Dataframe Creation with Correlated Variables in Python\nDESCRIPTION: This code snippet generates a Pandas DataFrame with correlated variables using the make_classification function from scikit-learn. It creates a classification dataset, transforms it into a DataFrame, and returns the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom feature_engine.selection import SmartCorrelatedSelection\n\n# make dataframe with some correlated variables\ndef make_data():\n    X, y = make_classification(n_samples=1000,\n                               n_features=12,\n                               n_redundant=4,\n                               n_clusters_per_class=1,\n                               weights=[0.50],\n                               class_sep=2,\n                               random_state=1)\n\n    # transform arrays into pandas df and series\n    colnames = ['var_'+str(i) for i in range(12)]\n    X = pd.DataFrame(X, columns=colnames)\n    return X\n\nX = make_data()\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with MatchVariables in Python\nDESCRIPTION: This code snippet applies the `MatchVariables` transformer to a modified DataFrame (`test_t`). The transformer adds the missing columns back to the DataFrame with default missing values (NaN), aligning the columns with the training set used during the `fit` stage.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# the transformer adds the columns back\ntest_tt = match_cols.transform(test_t)\n\ntest_tt.head()\n```\n\n----------------------------------------\n\nTITLE: Accessing Right Tail Caps in OutlierTrimmer in Python\nDESCRIPTION: This snippet displays the `right_tail_caps_` attribute of the fitted OutlierTrimmer object. This attribute stores the calculated capping values for the right tail (upper limits) of the specified variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\not.right_tail_caps_\n```\n\nLANGUAGE: python\nCODE:\n```\n{'sibsp': 2.5, 'fare': 66.34379999999999}\n```\n\n----------------------------------------\n\nTITLE: Transforming Data after Split by Cutoff - Python\nDESCRIPTION: Executes the transformation, removing features with high PSI, after splitting based on a defined cutoff. The `customer_id` is not dropped as it's the split column. Dependencies include feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nX_transformed = transformer.transform(X)\n\nX_transformed.columns\n```\n\n----------------------------------------\n\nTITLE: Accessing Interval Boundaries\nDESCRIPTION: Accesses the `binner_dict_` attribute of the fitted DecisionTreeDiscretiser to retrieve the calculated interval boundaries for each specified feature. The output shows the lower and upper limits of each bin, which will be used to discretize the continuous variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndisc.binner_dict_\n```\n\n----------------------------------------\n\nTITLE: Initializing MeanEncoder with Smoothing - Python\nDESCRIPTION: This code initializes the MeanEncoder with the 'auto' smoothing parameter. Smoothing is used to control the cardinality of the variable and prevent overfitting. The variables parameter is set to None, which means the encoder will automatically detect categorical variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nencoder = MeanEncoder(\n    variables=None,\n    smoothing=\"auto\"\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Initial Model Performance\nDESCRIPTION: This code retrieves the initial model performance (r2 score) from the SelectByShuffling object after fitting. This value represents the model performance before any feature shuffling occurs, providing a baseline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntr.initial_model_performance_\n```\n\n----------------------------------------\n\nTITLE: Installing Feature-engine in Editable Mode\nDESCRIPTION: This command shows how to install the feature-engine repository in editable mode. This allows changes to source files to be reflected directly.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Pipeline Printing (Python)\nDESCRIPTION: This snippet shows how to print the pipeline to display the automatically assigned names and the steps included within the pipeline. Useful for debugging and understanding the structure of the pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(pipe)\n```\n\n----------------------------------------\n\nTITLE: Displaying Unique Categories\nDESCRIPTION: This displays the array of unique values present in the 'cabin' feature.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\narray(['M', 'E', 'C', 'D', 'B', 'A', 'F', 'T', 'G'], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Check Imputer Dictionary\nDESCRIPTION: Accesses the `imputer_dict_` attribute of the fitted CategoricalImputer to view the dictionary containing variables and their corresponding fill values. This shows the mapping of each specified variable to the arbitrary string used for imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimputer.imputer_dict_\n```\n\n----------------------------------------\n\nTITLE: Check Modes\nDESCRIPTION: Checks the modes of the 'PoolQC' column in the training dataset. This verifies that the variable has multiple categories with the same maximum frequency, leading to the ValueError in the previous example.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nX_train['PoolQC'].mode()\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Percentiles\nDESCRIPTION: This snippet applies the outlier trimming transformation (using percentiles) to both the training and test datasets, including the target variable. This ensures consistency in data preprocessing between the training and test sets.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntrain_t, y_train_t = ot_age.transform_x_y(X_train, y_train)\n    test_t, y_test_t = ot_age.transform_x_y(X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Finding Categorical/Numerical Variables with a Variable Subset\nDESCRIPTION: This snippet shows how to use the find_categorical_and_numerical_variables function with a specified list of variables. This allows you to identify the types of only a subset of columns within the DataFrame. It takes the DataFrame and a list of variable names as input.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_categorical_and_numerical_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nvar_cat, var_num = find_categorical_and_numerical_variables(X, [\"num_var_1\", \"cat_var1\"])\n\nvar_cat, var_num\n```\n\n----------------------------------------\n\nTITLE: Checking Data Type of a Pandas Series\nDESCRIPTION: This code checks the data type of the 'sex' column in the `train` DataFrame. This is used to illustrate a potential issue where data types in the train and transformed test sets may not match, which can be addressed using the `match_dtypes` parameter of `MatchVariables`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrain.sex.dtype\n```\n\n----------------------------------------\n\nTITLE: Lag Features with Datetime and Fill Value in Python\nDESCRIPTION: This snippet demonstrates how to create lag features based on datetime frequency and fill the resulting NaN values with a specified numerical value (100) using the LagFeatures transformer in Feature Engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(\n    variables=[\"module_temp\", \"irradiation\"], freq=\"30min\", fill_value=100)\n\nX_tr = lag_f.fit_transform(X)\n\nprint(X_tr.head())\n```\n\n----------------------------------------\n\nTITLE: Install Feature-engine in Developer Mode\nDESCRIPTION: This snippet installs the Feature-engine package in developer mode using pip.  The `-e` flag creates a symbolic link, so code changes are automatically picked up without reinstalling.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Column Names\nDESCRIPTION: This snippet shows how to access and print the column names of a Pandas DataFrame. This is useful for inspecting the available features in the dataset.  It is typically used after loading and preprocessing the data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropFeatures.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nX_train.columns\n```\n\n----------------------------------------\n\nTITLE: Install documentation dependencies\nDESCRIPTION: Installs the dependencies required to build the documentation using pip. Requires being in the feature_engine directory.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Plotting Performance Drifts with Standard Deviation\nDESCRIPTION: This code creates a bar plot visualizing the mean performance drift for each feature, along with error bars representing the standard deviation. It uses pandas and matplotlib to generate the plot, which helps in understanding feature importance.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n    pd.Series(tr.performance_drifts_),\n    pd.Series(tr.performance_drifts_std_)\n], axis=1\n)\nr.columns = ['mean', 'std']\n\nr['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)\n\nplt.title(\"Performance drift elicited by shuffling a feature\")\nplt.ylabel('Mean performance drift')\nplt.xlabel('Features')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Transform Features and Target\nDESCRIPTION: Demonstrates the `transform_x_y` method of the Feature-engine pipeline.  First, the pipeline is fit to the DataFrame X and target y. Then, `transform_x_y` is called to obtain both the transformed features (Xt) and the re-aligned target variable (yt). This ensures that the feature matrix and target have the same number of rows after dropping rows with missing values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npipe.fit(X,y)\nXt, yt = pipe.transform_x_y(X, y)\nXt\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with DropHighPSIFeatures\nDESCRIPTION: This snippet applies the `transform` method of the fitted `DropHighPSIFeatures` transformer to the input dataframe `X`. The `transform` method removes the features identified in `features_to_drop_` from the dataframe, resulting in a transformed dataframe `X_transformed` containing only the remaining features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nX_transformed = transformer.transform(X)\n\n    X_transformed.columns\n```\n\n----------------------------------------\n\nTITLE: Pandas Expanding Window Aggregation\nDESCRIPTION: This snippet demonstrates how to create expanding window features using the `expanding` and `agg` methods in pandas. It calculates the maximum and mean values for 'var_1' and 'var_2' over an expanding window with a minimum period of 3.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nX[[\"var_1\", \"var_2\"].expanding(min_periods=3).agg([\"max\", \"mean\"])\n```\n\n----------------------------------------\n\nTITLE: Output Left Tail Caps (Percentiles)\nDESCRIPTION: This snippet displays the output of the lower limit for outlier capping using percentiles for the 'age' feature.  It provides the value beyond which data points are considered outliers.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n{'age': 9.0}\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Testing Sets\nDESCRIPTION: Splits the prepared data into training and testing sets using `train_test_split` from scikit-learn.  It separates the target variable ('target') from the features and specifies a test size of 20% with a random state for reproducibility. The shapes of train and test sets are printed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(['target'], axis=1),\n    data['target'],\n    test_size=0.2,\n    random_state=0)\n\nX_train.shape, X_test.shape\n```\n\n----------------------------------------\n\nTITLE: Loading Libraries and Dataset with Pandas and Scikit-learn\nDESCRIPTION: This code snippet loads necessary libraries (NumPy, Pandas, Scikit-learn, Feature-engine) and the Bike Sharing Demand dataset. It utilizes pandas for data manipulation, Scikit-learn for data fetching and preprocessing, and Feature-engine for cyclical feature creation.  It prints the head of the dataframe for inspection.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n    import pandas as pd\n\n    from sklearn.compose import ColumnTransformer\n    from sklearn.datasets import fetch_openml\n    from sklearn.preprocessing import FunctionTransformer\n\n    from feature_engine.creation import CyclicalFeatures\n\n    df = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True).frame\n\n    print(df.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Performance - Python\nDESCRIPTION: This snippet retrieves the performance of each feature as determined by the individual models trained on them. The feature_performance_ attribute stores the performance scores (e.g., R2) for each feature, allowing for analysis of individual feature importance.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_performance_\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Importance with Standard Deviation - Python\nDESCRIPTION: This code snippet creates a bar plot of the feature importance (ROC-AUC) along with the standard deviation as error bars. It concatenates the mean and standard deviation of the feature performance into a pandas DataFrame and then uses the `plot.bar` method to visualize the results, including titles and labels for better understanding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n    pd.Series(sel.feature_performance_),\n    pd.Series(sel.feature_performance_std_)\n], axis=1\n)\nr.columns = ['mean', 'std']\n\nr['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)\n\nplt.title(\"Feature importance\")\nplt.ylabel('ROC-AUC')\nplt.xlabel('Features')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading the House Prices Dataset\nDESCRIPTION: Loads the 'house_prices' dataset from OpenML using `fetch_openml`. The `return_X_y=True` argument ensures that features (X) and target variable (y) are returned separately. The `as_frame=True` argument specifies that the data should be returned as a pandas DataFrame. Parser is set to 'auto'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\t# Load dataset\n\tX, y = fetch_openml(name='house_prices', version=1, return_X_y=True, as_frame=True, parser='auto')\n```\n\n----------------------------------------\n\nTITLE: Loading Diabetes Dataset with Pandas and Scikit-learn\nDESCRIPTION: This code snippet demonstrates how to load the diabetes dataset using scikit-learn and convert it into a pandas DataFrame for easier manipulation. It imports necessary libraries like pandas, matplotlib, and datasets from sklearn.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom feature_engine.selection import SelectByShuffling\n\nX, y = load_diabetes(return_X_y=True, as_frame=True)\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Navigating to Project Directory\nDESCRIPTION: This snippet changes the current directory to the cloned Feature-engine project directory using the `cd` command.  This is required to install dependencies.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n$ cd feature_engine\n```\n\n----------------------------------------\n\nTITLE: Initializing LogCpTransformer with same constant for all variables\nDESCRIPTION: This code shows how to initialize LogCpTransformer to add the same constant to all numerical variables. The `C` parameter is set to a single numerical value.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogCpTransformer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntf = LogCpTransformer(C=5)\ntf.fit(X_train)\n\ntf.variables_\n```\n\n----------------------------------------\n\nTITLE: Example of Categorical Variables Identified by OneHotEncoder\nDESCRIPTION: This is an example output showing the categorical variables identified by the OneHotEncoder. It lists the columns 'sex', 'cabin', and 'embarked' as categorical features in the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n['sex', 'cabin', 'embarked']\n```\n\n----------------------------------------\n\nTITLE: Identifying Features to Drop (Categorical, List)\nDESCRIPTION: This snippet retrieves the list of features identified for removal based on their high PSI values after splitting the data based on a list of categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntrans.features_to_drop_\n\n['group_means', 'shifted_features']\n```\n\n----------------------------------------\n\nTITLE: Showing unique values of 'cabin' in test dataset in Python\nDESCRIPTION: This snippet demonstrates how the unique categories are represented in the 'cabin' column of the testing dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# categories present in the test data - 'G' is new\ntest.cabin.unique()\n```\n\n----------------------------------------\n\nTITLE: Transforming the data\nDESCRIPTION: This code snippet demonstrates how to transform the training and testing datasets using the fitted ArbitraryOutlierCapper. The transform method applies the capping to the specified features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/ArbitraryOutlierCapper.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\ttrain_t = capper.transform(X_train)\n\ttest_t = capper.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Using find_categorical_variables from Feature Engine\nDESCRIPTION: This code snippet demonstrates how to use the `find_categorical_variables` function from the `feature_engine.variable_handling` module. It takes the DataFrame created in the previous snippet as input and returns a list containing the names of the categorical variables (`object` or `categorical` type).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_categorical_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import find_categorical_variables\n\nvar_cat = find_categorical_variables(X)\n\nvar_cat\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Load Titanic Dataset - Python\nDESCRIPTION: This code snippet imports necessary libraries such as numpy, pandas, matplotlib, and scikit-learn's train_test_split. It then loads the Titanic dataset using feature_engine's load_titanic function, handles missing values, and performs some preprocessing steps like replacing infrequent cabins, capping maximum values for 'parch' and 'sibsp', and casting variables to object type.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import RareLabelEncoder\nfrom feature_engine.selection import SelectByTargetMeanPerformance\n\ndata = load_titanic(\n    handle_missing=True,\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\n\n# replace infrequent cabins by N\ndata['cabin'] = np.where(data['cabin'].isin(['T', 'G']), 'N', data['cabin'])\n\n# cap maximum values\ndata['parch'] = np.where(data['parch']>3,3,data['parch'])\ndata['sibsp'] = np.where(data['sibsp']>3,3,data['sibsp'])\n\n# cast variables as object to treat as categorical\ndata[['pclass','sibsp','parch']] = data[['pclass','sibsp','parch']].astype('O')\n\nprint(data.head())\n```\n\n----------------------------------------\n\nTITLE: Printing Specific Feature Correlations - Python\nDESCRIPTION: Prints the absolute correlation with the target for specific features (`var_0` and `var_8`) to compare their relevance for feature selection.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprint(X.corrwith(y).abs().loc[['var_0', 'var_8']])\n```\n\n----------------------------------------\n\nTITLE: Plotting Data Distribution Before and After Imputation\nDESCRIPTION: This code snippet uses matplotlib to plot the kernel density estimation (KDE) of a variable before and after imputation. The original distribution is plotted in blue, and the imputed distribution is plotted in red, allowing for visual comparison of the impact of imputation on the variable's distribution.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/EndTailImputer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tX_train['LotFrontage'].plot(kind='kde', ax=ax)\n\ttrain_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')\n\tlines, labels = ax.get_legend_handles_labels()\n\tax.legend(lines, labels, loc='best')\n```\n\n----------------------------------------\n\nTITLE: Displaying Correlated Feature Dictionary - Python\nDESCRIPTION: This snippet shows an example of the output from `tr.correlated_feature_dict_`. It displays a dictionary that maps retained features to the set of correlated features that will be removed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n{'var_0': {'var_8'}, 'var_4': {'var_6', 'var_7', 'var_9'}}\n```\n\n----------------------------------------\n\nTITLE: Pipeline Output (Python)\nDESCRIPTION: This snippet represents the output of the `print(pipe)` statement, which illustrates the structure of the pipeline, including the automatically assigned names to each step (dropmissingdata, ordinalencoder, lasso) and the corresponding transformers/estimators with their configurations.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nPipeline(steps=[('dropmissingdata', DropMissingData()),\n                    ('ordinalencoder', OrdinalEncoder(encoding_method='arbitrary')),\n                    ('lasso', Lasso(random_state=10))])\n```\n\n----------------------------------------\n\nTITLE: get_support() output example - Python\nDESCRIPTION: This code snippet showcases an example output of the get_support() method, representing a boolean array. True indicates that the corresponding feature is selected, while False indicates that the feature is marked for removal.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureElimination.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[False, True, True, True, True, True, False, False, True, False]\n```\n\n----------------------------------------\n\nTITLE: Applying Rare Label Encoding and Showing Value Counts\nDESCRIPTION: This Python code continues the previous example by applying the RareLabelEncoder to transform the DataFrame. After the transformation, it prints the value counts of the encoded 'var_A' column, showing how rare labels are grouped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n>>> rare_encoder = RareLabelEncoder(tol=0.10, n_categories=3)\n>>> data_encoded = rare_encoder.fit_transform(data)\n>>> data_encoded['var_A'].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Check Imputation Value\nDESCRIPTION: Accesses the `fill_value` attribute of the fitted CategoricalImputer to verify the arbitrary fill value that will be used. The output is a string, representing the value with which missing data will be replaced.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimputer.fill_value\n```\n\n----------------------------------------\n\nTITLE: Visualize inverse transformed data distribution\nDESCRIPTION: This code generates histograms of 'LotArea' and 'GrLivArea' columns from the inverse transformed training dataset (train_unt) to visualize the effect of inverse transformation.  The purpose is to show the variables are back in the original scale.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogTransformer.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_unt[['LotArea', 'GrLivArea']].hist(figsize=(10,5))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing DropDuplicateFeatures and Loading Data\nDESCRIPTION: This code snippet demonstrates how to import necessary libraries, load the Titanic dataset using Feature-engine, and create duplicate columns for demonstration purposes. It imports pandas for data manipulation, scikit-learn for splitting the data, and the DropDuplicateFeatures class from Feature-engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropDuplicateFeatures.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.selection import DropDuplicateFeatures\n\ndata = load_titanic(\n    handle_missing=True,\n    predictors_only=True,\n)\n\n# Lets duplicate some columns\ndata = pd.concat([data, data[['sex', 'age', 'sibsp']]], axis=1)\ndata.columns = ['pclass', 'survived', 'sex', 'age',\n                'sibsp', 'parch', 'fare','cabin', 'embarked',\n                'sex_dup', 'age_dup', 'sibsp_dup']\n```\n\n----------------------------------------\n\nTITLE: Visualizing Distribution with Categorical Hue\nDESCRIPTION: These two lines of code demonstrate how to visualize feature distributions using seaborn's `ecdfplot` function, with the 'group' column used as a hue. This allows comparison of the distribution of `shifted_feature` and `var_0` across different categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nsns.ecdfplot(data=X, x='shifted_feature', hue='group')\n\nsns.ecdfplot(data=X, x='var_0', hue='group')\n```\n\n----------------------------------------\n\nTITLE: Datetime Subtraction with Pandas and Feature-engine in Python\nDESCRIPTION: This code snippet demonstrates how to use the DatetimeSubtraction transformer to calculate the difference between two datetime columns ('date1' and 'date2') in a Pandas DataFrame. It sets the output unit to milliseconds, specifies that the timezones must be converted to UTC and uses a mixed format. The fit_transform method calculates the time difference and adds a new column, date1_sub_date2, to the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.datetime import DatetimeSubtraction\n\ndata = pd.DataFrame({\n    \"date1\": ['12:34:45+3', '23:01:02-6', '11:59:21-8', '08:44:23Z'],\n    \"date2\": ['09:34:45+1', '23:01:02-6+1', '11:59:21-8-2', '08:44:23+3']\n})\n\ndfts = DatetimeSubtraction(\n    variables=\"date1\",\n    reference=\"date2\",\n    utc=True,\n    output_unit=\"ms\",\n    format=\"mixed\"\n)\n\nnew = dfts.fit_transform(data)\n\nprint(new)\n```\n\n----------------------------------------\n\nTITLE: Accessing Unique Categories in OneHotEncoder in Python\nDESCRIPTION: This snippet demonstrates how to access the unique categories for each categorical variable learned by the OneHotEncoder during the fitting process. The `encoder.encoder_dict_` attribute stores a dictionary where keys are the categorical column names, and values are lists of their corresponding unique categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Testing Sets in Python\nDESCRIPTION: This code snippet splits the California housing dataset into training and testing sets using scikit-learn's `train_test_split` function. The features (X) are separated from the target variable (y), and then the data is split with a test size of 0.3 and a random state of 0 for reproducibility. The `head()` method is then used to display the first few rows of the training set.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nX = data.drop('MedHouseVal', axis=1)\ny = data['MedHouseVal']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Install black code formatter\nDESCRIPTION: Installs the black code formatter using pip. Black is used to automatically format code to comply with PEP8 style guidelines.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install black\n```\n\n----------------------------------------\n\nTITLE: Accessing Features to Drop - Python\nDESCRIPTION: This code snippet shows how to access the list of features that will be dropped by the MRMR feature selection process. The `sel.features_to_drop_` attribute contains a list of the names of the features that were not selected.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsel.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Transform Data and Plot (Frequent)\nDESCRIPTION: Transforms the training and testing datasets using the fitted CategoricalImputer (using the 'frequent' method), then plots the value counts of 'MasVnrType' in the transformed test set. Depends on the CategoricalImputer having been fitted. Requires matplotlib.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = imputer.transform(X_train)\ntest_t = imputer.transform(X_test)\n\ntest_t['MasVnrType'].value_counts().plot.bar()\nplt.ylabel(\"Number of observations\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Data and DropHighPSIFeatures for Time-Based Split\nDESCRIPTION: This code snippet demonstrates how to create a sample dataset, add time-based features, and shuffle the data. It then initializes the `DropHighPSIFeatures` transformer to split the data based on a date column and a cutoff date, effectively comparing PSI values before and after the specified date. Dependencies include pandas and sklearn.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom datetime import date\nfrom sklearn.datasets import make_classification\nfrom feature_engine.selection import DropHighPSIFeatures\n\nX, y = make_classification(\n        n_samples=1000,\n        n_features=6,\n        random_state=0\n    )\n\ncolnames = [\"var_\" + str(i) for i in range(6)]\nX = pd.DataFrame(X, columns=colnames)\n\n# Add two time variables to the dataframe\nX['time'] = [date(year, 1, 1) for year in range(1000, 2000)]\nX['century'] = X['time'].apply(lambda x: ((x.year - 1) // 100) + 1)\n\n# Let's shuffle the dataframe and reset the index to remove the correlation\n# between the index and the time variables.\n\nX = X.sample(frac=1).reset_index(drop=True)\n\ntransformer = DropHighPSIFeatures(split_col='time', cut_off=date(1789, 7, 14))\ntransformer.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Creating sample DataFrame with pandas\nDESCRIPTION: This code snippet generates a Pandas DataFrame named 'X' that includes numerical, categorical, and datetime variables. It uses sklearn to create numerical data and populates the DataFrame with it, then adds string and datetime columns.  It prints the first few rows of the dataframe to show the result.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_all_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_redundant=1,\n    n_clusters_per_class=1,\n    weights=[0.50],\n    class_sep=2,\n    random_state=1,\n)\n\n# transform arrays into pandas df and series\ncolnames = [f\"num_var_{i+1}\" for i in range(4)]\nX = pd.DataFrame(X, columns=colnames)\n\nX[\"cat_var1\"] = [\"Hello\"] * 1000\nX[\"cat_var2\"] = [\"Bye\"] * 1000\n\nX[\"date1\"] = pd.date_range(\"2020-02-24\", periods=1000, freq=\"T\")\nX[\"date2\"] = pd.date_range(\"2021-09-29\", periods=1000, freq=\"H\")\nX[\"date3\"] = [\"2020-02-24\"] * 1000\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Rare Label Encoding - Python\nDESCRIPTION: This code snippet demonstrates the use of Feature-engine's `RareLabelEncoder` to group infrequent categories in specified categorical variables ('cabin', 'pclass', 'embarked') into a single 'Rare' category. The encoder is initialized with a tolerance level (`tol`) of 0.1, a minimum category count (`n_categories`) of 2, and `ignore_format=True` to handle numeric variables like 'pclass'. This step helps to reduce the impact of rare categories on the WoE encoding process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# set up a rare label encoder\nrare_encoder = RareLabelEncoder(\n    tol=0.1,\n    n_categories=2,\n    variables=['cabin', 'pclass', 'embarked'],\n    ignore_format=True,\n)\n\n# fit and transform data\ntrain_t = rare_encoder.fit_transform(X_train)\ntest_t = rare_encoder.transform(X_train)\n```\n\n----------------------------------------\n\nTITLE: Rare Label Encoding Result (encoded value_counts)\nDESCRIPTION: This is the expected output after transforming the data using the RareLabelEncoder and displaying the value counts. It shows the result of grouping rare labels into a 'Rare' category.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nOut[2]:\nA       10\nB       10\nRare     3\nName: var_A, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Print Shape of the Dataframe\nDESCRIPTION: This code prints the shape (number of rows and columns) of the `cancer_X` DataFrame, which contains the features of the breast cancer dataset. This helps to understand the dimensions of the dataset being used.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(cancer_X.shape)\n```\n\n----------------------------------------\n\nTITLE: Discretizing MedInc returning interval boundaries\nDESCRIPTION: This code snippet demonstrates discretizing the 'MedInc' variable and returns the interval limits after transformation. The `return_boundaries` parameter is set to `True`. The result is a plot of the number of observations per interval. It starts by loading the dataset again to avoid carrying over previous transformations. It requires pandas, numpy, matplotlib, and sklearn.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/ArbitraryDiscretiser.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX, y = fetch_california_housing( return_X_y=True, as_frame=True)\n\nuser_dict = {'MedInc': [0, 2, 4, 6, np.inf]}\n\ntransformer = ArbitraryDiscretiser(\n    binning_dict=user_dict, return_object=False, return_boundaries=True)\nX = transformer.fit_transform(X)\n\nX['MedInc'].value_counts().plot.bar(rot=0)\nplt.xlabel('MedInc - bins')\nplt.ylabel('Number of observations')\nplt.title('Discretised MedInc')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Finding Unique Categories After Transformation\nDESCRIPTION: This code snippet retrieves and displays the unique categories present in the 'cabin' column of the transformed training dataset (train_t). This demonstrates the effect of the RareLabelEncoder in grouping infrequent categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrain_t[\"cabin\"].unique()\n```\n\n----------------------------------------\n\nTITLE: Titanic Dataset Split\nDESCRIPTION: This code loads and cleans the Titanic dataset, then splits it into training and testing sets using train_test_split.  It drops the 'survived', 'sex', 'cabin', and 'embarked' columns from the feature set (X) and uses 'survived' as the target variable (y). This creates the data subsets necessary for training and evaluating the StringSimilarityEncoder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/StringSimilarityEncoder.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = clean_titanic()\n# Separate into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(['survived', 'sex', 'cabin', 'embarked'], axis=1),\n    data['survived'],\n    test_size=0.3,\n    random_state=0\n)\n\nX_train.head()\n```\n\n----------------------------------------\n\nTITLE: Get Feature Names with `get_feature_names_out()`\nDESCRIPTION: This snippet shows how to retrieve the names of all the features in the transformed DataFrame, including the original features and the newly created lag features, using the `get_feature_names_out()` method of the `LagFeatures` transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlag_f.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Initializing Linear Regression Model\nDESCRIPTION: This snippet initializes a LinearRegression model from scikit-learn, which will be used as the estimator in the SelectByShuffling feature selection process. It serves as a basic model for evaluating feature importance.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlinear_model = LinearRegression()\n```\n\n----------------------------------------\n\nTITLE: Install isort import sorter\nDESCRIPTION: Installs the isort import sorter using pip. Isort is used to automatically sort imports alphabetically.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install isort\n```\n\n----------------------------------------\n\nTITLE: Boxplot Visualization for 'age' in Python\nDESCRIPTION: This snippet generates a boxplot specifically for the 'age' column of the training dataset. The code uses matplotlib and includes labels and a title for better interpretation of the boxplot.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX_train.boxplot(column=['age'])\nplt.title(\"Box plot - outliers\")\nplt.ylabel(\"variable values\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Using get_support() method - RecursiveFeatureElimination - Python\nDESCRIPTION: This snippet demonstrates how to use the get_support() method of the RecursiveFeatureElimination class, which functions similarly to Scikit-learn's feature selection classes. It returns a boolean array indicating whether each feature is selected (True) or will be dropped (False).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureElimination.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntr.get_support()\n```\n\n----------------------------------------\n\nTITLE: Accessing Dropped Features (Empty List) - Python\nDESCRIPTION: This snippet demonstrates accessing the `features_to_drop_` attribute, which returns an empty list. This happens when the feature importances are such that no features are considered less important than the probe features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nsel.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Displaying numerical variables found\nDESCRIPTION: This snippet displays the list of numerical variables returned by `find_numerical_variables`. In this case, it will output a list containing the strings 'Age' and 'Marks'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_numerical_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n['Age', 'Marks']\n```\n\n----------------------------------------\n\nTITLE: Checking Categorical Variables with Feature-Engine\nDESCRIPTION: This code snippet uses the `check_categorical_variables` function to identify categorical variables within the DataFrame `X`. It checks if the specified variables (`cat_var1` and `date3`) are of type object or categorical. The function returns a list containing the names of the variables that satisfy this condition.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_categorical_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import check_categorical_variables\n\nvar_cat = check_categorical_variables(X, [\"cat_var1\", \"date3\"])\n\nvar_cat\n```\n\n----------------------------------------\n\nTITLE: Finding Categorical and Numerical Variables using feature-engine\nDESCRIPTION: This code snippet demonstrates how to use the find_categorical_and_numerical_variables function from the feature_engine.variable_handling module. It identifies and separates categorical and numerical variables within the created DataFrame. It requires the feature_engine library to be installed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_categorical_and_numerical_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import find_categorical_and_numerical_variables\n\nvar_cat, var_num = find_categorical_and_numerical_variables(X)\n\nvar_cat, var_num\n```\n\n----------------------------------------\n\nTITLE: Creating a Toy Dataset with Pandas\nDESCRIPTION: This code snippet creates a Pandas DataFrame with numerical, categorical, and datetime variables using `sklearn.datasets.make_classification` and `pd.date_range`. The resulting DataFrame `X` contains columns with different data types to demonstrate the functionality of the `check_categorical_variables` function.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_categorical_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_redundant=1,\n    n_clusters_per_class=1,\n    weights=[0.50],\n    class_sep=2,\n    random_state=1,\n)\n\n# transform arrays into pandas df and series\ncolnames = [f\"num_var_{i+1}\" for i in range(4)]\nX = pd.DataFrame(X, columns=colnames)\n\nX[\"cat_var1\"] = [\"Hello\"] * 1000\nX[\"cat_var2\"] = [\"Bye\"] * 1000\n\nX[\"date1\"] = pd.date_range(\"2020-02-24\", periods=1000, freq=\"T\")\nX[\"date2\"] = pd.date_range(\"2021-09-29\", periods=1000, freq=\"H\")\nX[\"date3\"] = [\"2020-02-24\"] * 1000\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Displaying Unique Categories After Transformation\nDESCRIPTION: This code displays the unique categories in the 'cabin' feature after the rare label encoding has been applied.  The 'Rare' category should be present indicating the successful grouping of infrequent categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\narray(['M', 'E', 'C', 'D', 'B', 'Rare'], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names: WindowFeatures\nDESCRIPTION: This snippet demonstrates how to retrieve the names of the generated features after applying the WindowFeatures transformation. The `get_feature_names_out()` method returns a list of strings, each representing a new feature created by the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwin_f.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Rebase git feature branch\nDESCRIPTION: Rebases a specified feature branch onto the 'main' branch, incorporating the latest changes.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_42\n\nLANGUAGE: bash\nCODE:\n```\n$ git rebase main\n```\n\n----------------------------------------\n\nTITLE: Run pytest with code coverage\nDESCRIPTION: Runs pytest with code coverage enabled.  Requires the coverage package to be installed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ coverage run -m pytest\n```\n\n----------------------------------------\n\nTITLE: Displaying Raw Data Before Discretization\nDESCRIPTION: This snippet displays the first few rows of the original, untransformed data for the selected numerical features (`LotArea` and `GrLivArea`) from the training set, allowing for comparison with the transformed data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Raw data\nprint(X_train[TARGET_NUMERIC_FEATURES].head())\n```\n\nLANGUAGE: Python\nCODE:\n```\n\t\t  LotArea  GrLivArea\n\tId                      \n\t136     10400       1682\n\t1453     3675       1072\n\t763      8640       1547\n\t933     11670       1905\n\t436     10667       1661\n```\n\n----------------------------------------\n\nTITLE: Install mypy type checker\nDESCRIPTION: Installs the mypy type checker using pip. Mypy is used to check for type hinting errors in Python code.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install mypy\n```\n\n----------------------------------------\n\nTITLE: Accessing Automatically Detected Variables - Python\nDESCRIPTION: This code shows how to access the categorical variables that the MeanEncoder automatically detected during the fit process. The detected variables are stored in the `variables_` attribute.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nencoder.variables_\n```\n\n----------------------------------------\n\nTITLE: Prepare Credit Approval Dataset with Pandas\nDESCRIPTION: This Python script loads the Credit Approval dataset, preprocesses it by replacing missing values, recasting variable types, encoding the target variable to binary, and saving the cleaned data to a CSV file.  It uses the pandas and numpy libraries for data manipulation. The script requires the 'crx.data' file to be in the same directory.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/datasets.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport pandas as pd\nimport numpy as np\n\n# load data\ndata = pd.read_csv('crx.data', header=None)\n\n# create variable names according to UCI Machine Learning information\nvarnames = ['A'+str(s) for s in range(1,17)]\ndata.columns = varnames\n\n# replace ? by np.nan\ndata = data.replace('?', np.nan)\n\n# re-cast some variables to the correct types\ndata['A2'] = data['A2'].astype('float')\ndata['A14'] = data['A14'].astype('float')\n\n# encode target to binary\ndata['A16'] = data['A16'].map({'+':1, '-':0})\n\n# save the data\ndata.to_csv('creditApprovalUCI.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Accuracy Output\nDESCRIPTION: This snippet shows the accuracy output of the model when tested against the test set.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n0.7823343848580442\n```\n\n----------------------------------------\n\nTITLE: Defining variables for Arcsin Transformation - Python\nDESCRIPTION: This code snippet defines a list of variable names that will be used in the subsequent Arcsin transformation. These variables represent columns in the dataframe that will be transformed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/ArcsinTransformer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nvars_ = [\n  'mean compactness',\n  'mean concavity',\n  'mean concave points',\n  'mean fractal dimension',\n  'smoothness error',\n  'compactness error',\n  'concavity error',\n  'concave points error',\n  'symmetry error',\n  'fractal dimension error',\n  'worst symmetry',\n  'worst fractal dimension']\n```\n\n----------------------------------------\n\nTITLE: Accessing OrdinalEncoder Mappings\nDESCRIPTION: This code retrieves the mappings used by the OrdinalEncoder within the pipeline. It accesses the named_steps attribute to get the encoder and then accesses the encoder_dict_ attribute.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe.named_steps[\"enc\"].encoder_dict_\n```\n\nLANGUAGE: python\nCODE:\n```\n{'x2': {'a': 0, 'b': 1}}\n```\n\n----------------------------------------\n\nTITLE: Initializing DecisionTreeEncoder\nDESCRIPTION: This code initializes the DecisionTreeEncoder to encode 'cabin', 'pclass', and 'embarked' using 3-fold cross-validation, optimizing the tree depth with the roc_auc metric. It also sets random_state for reproducibility and ignores the format of the input data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nencoder = DecisionTreeEncoder(\n    variables=['cabin', 'pclass', 'embarked'],\n    regression=False,\n    scoring='roc_auc',\n    cv=3,\n    random_state=0,\n    ignore_format=True)\n```\n\n----------------------------------------\n\nTITLE: Scatter Plot of Cyclical Features with Matplotlib\nDESCRIPTION: This code snippet generates a scatter plot visualizing cyclical features (hour_sin and hour_cos) created by cyclical encoding. The plot uses Matplotlib to display the relationship between the sine and cosine components, with color indicating the original hour value.  It requires the 'df' DataFrame to be pre-calculated, containing columns 'hour_sin', 'hour_cos' and 'hour'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(figsize=(7, 5))\n    sp = ax.scatter(df[\"hour_sin\"], df[\"hour_cos\"], c=df[\"hour\"])\n    ax.set(\n        xlabel=\"sin(hour)\",\n        ylabel=\"cos(hour)\",\n    )\n    _ = fig.colorbar(sp)\n```\n\n----------------------------------------\n\nTITLE: Titanic Dataset Cleaning Function\nDESCRIPTION: This function, clean_titanic(), preprocesses the Titanic dataset by removing punctuation, stripping whitespace, and converting strings to lowercase in the 'home.dest', 'name', and 'ticket' columns. It uses str.maketrans() and string.punctuation for efficient string cleaning. This prepares the data for use with the StringSimilarityEncoder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/StringSimilarityEncoder.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import StringSimilarityEncoder\n\ndef clean_titanic():\n    translate_table = str.maketrans('' , '', string.punctuation)\n    data = load_titanic()\n    data['home.dest'] = (\n    data['home.dest']\n    .str.strip()\n    .str.translate(translate_table)\n    .str.replace('  ', ' ')\n    .str.lower()\n    )\n    data['name'] = (\n    data['name']\n    .str.strip()\n    .str.translate(translate_table)\n    .str.replace('  ', ' ')\n    .str.lower()\n    )\n    data['ticket'] = (\n    data['ticket']\n    .str.strip()\n    .str.translate(translate_table)\n    .str.replace('  ', ' ')\n    .str.lower()\n    )\n    return data\n```\n\n----------------------------------------\n\nTITLE: Sort imports using isort\nDESCRIPTION: Sorts the imports in a specified Python script alphabetically using isort.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n$ isort my_new_script.py\n```\n\n----------------------------------------\n\nTITLE: Install pytest package\nDESCRIPTION: Installs the pytest package using pip. Pytest is used for running tests in the Feature-engine project.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install pytest\n```\n\n----------------------------------------\n\nTITLE: Cloning a Git Repository\nDESCRIPTION: This command clones a remote Git repository to your local machine. Replace `<YOURUSERNAME>` with your GitHub username. This allows you to work on the repository locally.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_jup.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<YOURUSERNAME>/feature-engine-examples.git\n```\n\n----------------------------------------\n\nTITLE: Print Transformed DataFrame Head\nDESCRIPTION: This code prints the head of the transformed DataFrame `X` after applying cyclical feature encoding. It displays the original 'day' and 'months' columns along with the newly created 'day_sin', 'day_cos', 'months_sin', and 'months_cos' columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Importances - Python\nDESCRIPTION: This code retrieves the feature importances for the selected features and a probe feature ('gaussian_probe_0') from the ProbeFeatureSelection transformer. This allows comparison between original features and the probe to determine feature relevance.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_importances_.loc[sel.features_to_drop_+[\"gaussian_probe_0\"]]\n```\n\n----------------------------------------\n\nTITLE: Accessing Evaluated Variables - Python\nDESCRIPTION: This code snippet accesses the `variables_` attribute of the fitted SelectByTargetMeanPerformance object (sel). This attribute contains a list of the variables that were evaluated by the transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsel.variables_\n```\n\n----------------------------------------\n\nTITLE: Dropping NaN Values with ExpandingWindowFeatures in Python\nDESCRIPTION: This code snippet demonstrates how to use the ExpandingWindowFeatures transformer to drop rows containing NaN values within the calculated window features. It showcases the initialization of the transformer with the `drop_na=True` parameter, fitting it to the data (X), and then transforming both the features (X) and target variable (y) to remove NaN rows. The output shows the shapes of the original and transformed dataframes, indicating the number of rows dropped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwin_f = ExpandingWindowFeatures(\n    functions=[\"mean\", \"max\", \"std\"],\n    drop_na=True,\n)\n\nwin_f.fit(X)\n\nX_tr, y_tr = win_f.transform_x_y(X, y)\n\nX.shape, y.shape, X_tr.shape, y_tr.shape\n```\n\n----------------------------------------\n\nTITLE: Loading Titanic Dataset and Splitting Data - Python\nDESCRIPTION: This code loads the Titanic dataset using Feature-engine's `load_titanic` function. It then splits the data into training and testing sets using `train_test_split` from scikit-learn.  The `return_X_y_frame`, `predictors_only`, and `handle_missing` parameters configure the dataset loading process. The training data's head is printed for inspection.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import OneHotEncoder\nfrom feature_engine.outliers import OutlierTrimmer\nfrom feature_engine.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = load_titanic(\n        return_X_y_frame=True,\n        predictors_only=True,\n        handle_missing=True,\n    )\n\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0,\n    )\n\n    print(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Displaying Encoder Dictionary Content\nDESCRIPTION: This code shows the content of the encoder_dict_ attribute, revealing the frequent categories for cabin, pclass, and embarked variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n{'cabin': ['M', 'C', 'B', 'E', 'D'],\n    'pclass': [3, 1, 2],\n    'embarked': ['S', 'C', 'Q']}\n```\n\n----------------------------------------\n\nTITLE: Commit changes to git\nDESCRIPTION: Commits the staged changes in Git with a descriptive message.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n$ git commit -m \"fix code style\"\n```\n\n----------------------------------------\n\nTITLE: Transforming Test Data Subset\nDESCRIPTION: Applies the fitted `MeanMedianImputer` to the selected subset of the test data (`X_test_subset`) to impute missing values. The transformed data is stored in `X_test_subset_t`. This demonstrates how the imputer replaces missing values with the learned mean values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\t# Transform the subset of the test data\n\tX_test_subset_t = mmi.transform(X_test_subset)\n```\n\n----------------------------------------\n\nTITLE: Creating a Git Branch\nDESCRIPTION: This command creates and switches to a new branch named `mynotebookbranch`. Creating a branch allows you to work on new features or bug fixes in isolation, preventing conflicts with the main codebase.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_jup.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b mynotebookbranch\n```\n\n----------------------------------------\n\nTITLE: PowerTransformer with Custom Exponents\nDESCRIPTION: This code snippet demonstrates how to use the PowerTransformer to transform skewed data using custom exponent values. It initializes two PowerTransformer objects, one for right-skewed data with an exponent of 0.246 and another for left-skewed data with an exponent of 4.404. The transformers are fitted to the input DataFrame (df_sim), applied to transform the data, and the transformed data is visualized using histograms. The `hist_params` variable is assumed to be defined elsewhere with histogram styling parameters. The `plt.subplots` function is used to create a figure with two subplots, one for each transformed variable. The `sns.histplot` function from the seaborn library is used to plot the histograms of the transformed data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\t# Set up the variable transformer (tf)\n\ttf_right = PowerTransformer(variables = ['right_skewed'], exp=0.246)\n\ttf_left = PowerTransformer(variables = ['left_skewed'], exp=4.404)\n\n\t# Fit the transformers\n\ttf_right.fit(df_sim)\n\ttf_left.fit(df_sim)\n\n\t# Plot histograms\n\tfig,axes = plt.subplots(ncols=2, figsize=(12,4))\n\n\tsns.histplot(\n\t\ttf_left.transform(df_sim)['left_skewed'], ax=axes[0],\n\t\tcolor='blue', **hist_params\n\t)\n\tsns.histplot(\n\t\ttf_right.transform(df_sim)['right_skewed'], ax=axes[1],\n\t\tcolor='red', **hist_params\n\t)\n\n\taxes[0].set_title('Transformed left-skewed data')\n\taxes[1].set_title('Transformed right-skewed data')\n\n\tplt.show()\n```\n\n----------------------------------------\n\nTITLE: Transforming Data after Encoding Numeric Variables in Python\nDESCRIPTION: This snippet transforms training and test data using the fitted OneHotEncoder, after instructing the encoder to ignore the data type of pclass and treat it as a categorical variable. The transformed dataframes are then printed to display the resulting one-hot encoded representation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = enc.transform(X_train)\ntest_t = enc.transform(X_test)\n\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Input Variables: WindowFeatures\nDESCRIPTION: This code snippet shows how to access the input variables used by the WindowFeatures class after it has been instantiated. The `variables_` attribute stores the names of the original variables from which the window features were created.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwin_f.variables_\n```\n\n----------------------------------------\n\nTITLE: Accessing Input Variables\nDESCRIPTION: This snippet shows how to access the variables used as input for creating the window features using the `variables_` attribute of the `ExpandingWindowFeatures` transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwin_f.variables_\n```\n\n----------------------------------------\n\nTITLE: Loading Wine Dataset using Scikit-learn\nDESCRIPTION: This code snippet demonstrates how to load the wine dataset from Scikit-learn and display the first few rows. It imports the necessary libraries (pandas, sklearn.datasets, and feature_engine.creation) and loads the dataset into a pandas DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/index.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import load_wine\nfrom feature_engine.creation import RelativeFeatures, MathFeatures\n\nX, y = load_wine(return_X_y=True, as_frame=True)\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: HTML Author Table Generation\nDESCRIPTION: This HTML snippet generates a table-like structure to display project authors, using avatar images and links to their GitHub profiles. It includes basic styling to format the avatars.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/about/former_authors.rst#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"authors-container\">\n    <style>\n      img.avatar {border-radius: 15px; padding: 10px;}\n      .author {text-align: center;}\n    </style>\n    <div class=\"author\">\n        <a href='https://github.com/christophergs'><img src='https://avatars.githubusercontent.com/christophergs?v=4' width=\"120\" height=\"120\"class='avatar' /></a> <br />\n        <p>Chris Samiullah</p>\n   </div>\n    <div class=\"author\">\n        <a href='https://github.com/nicogalli'><img src='https://avatars.githubusercontent.com/nicogalli?v=4' class='avatar'width=\"120\" height=\"120\"/></a> <br />\n        <p>Nicolas Galli</p>\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Visualizing Bin Counts\nDESCRIPTION: This snippet generates a bar plot showing the number of observations falling into each bin for the `GrLivArea` variable after discretization. This visualization helps understand the distribution of data across the created bins.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntrain_t['GrLivArea'].value_counts().sort_index().plot.bar()\nplt.ylabel('Number of houses')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Dropping NA Values using Pandas\nDESCRIPTION: This code snippet demonstrates how to drop rows containing missing values (NaN) from a Pandas DataFrame using the `dropna()` method.  It creates a DataFrame with NaN values and then removes rows containing any NaN in place. The resulting DataFrame without NaN values is then printed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nX = pd.DataFrame(dict(\n       x1 = [np.nan,1,1,0,np.nan],\n       x2 = [\"a\", np.nan, \"b\", np.nan, \"a\"],\n       ))\n\nX.dropna(inplace=True)\nprint(X)\n```\n\n----------------------------------------\n\nTITLE: Inspecting optimal lambda values after fitting\nDESCRIPTION: This code snippet shows how to access the optimal lambda values learned by the `YeoJohnsonTransformer` after fitting it to the training data.  The lambda values are stored in the `lambda_dict_` attribute.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/YeoJohnsonTransformer.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntf.lambda_dict_\n```\n\n----------------------------------------\n\nTITLE: Transforming Data (Time-Based)\nDESCRIPTION: This code applies the `transform` method to remove the identified features from the input DataFrame. The output `X_transformed` contains only the remaining features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nX_transformed = transformer.transform(X)\n\nX_transformed.columns\n```\n\n----------------------------------------\n\nTITLE: Scatter Plot of Predicted vs True Price\nDESCRIPTION: This code generates a scatter plot of predicted prices versus true prices to visualize the model's prediction accuracy.  It uses matplotlib to create the plot, labeling the axes as \"True Price\" and \"Predicted Price\" and then displays the plot.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/index.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    plt.scatter(y_test, np.exp(pred_test))\n    plt.xlabel('True Price')\n    plt.ylabel('Predicted Price')\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Features to Drop with DropHighPSIFeatures\nDESCRIPTION: This snippet displays the features identified for removal by the `DropHighPSIFeatures` transformer after the `fit` method has been called. The `features_to_drop_` attribute is a list containing the names of the features exceeding the defined PSI threshold, indicating distribution shifts between the base and reference datasets.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ntransformer.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Plotting the distribution of 'LotArea' using seaborn.\nDESCRIPTION: This snippet visualizes the distribution of the 'LotArea' variable from the training dataset (X_train) using seaborn's histplot function. It includes a kernel density estimate (kde) and specifies the number of bins for the histogram. The purpose is to observe the original distribution of the feature before any transformations are applied.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\tsns.histplot(X_train['LotArea'], kde=True, bins=50)\n```\n\n----------------------------------------\n\nTITLE: Cloning a Forked Repository\nDESCRIPTION: This command shows how to clone a forked version of the feature-engine repository from GitHub to local machine for contributing.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<YOURUSERNAME>/feature_engine.git\n```\n\n----------------------------------------\n\nTITLE: Retaining Variables Present in DataFrame\nDESCRIPTION: This code snippet demonstrates how to use `retain_variables_if_in_df` from the feature_engine library to filter a list of variables, keeping only those that exist as columns in a given Pandas DataFrame. The function returns a list containing only the variable names present in the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/retain_variables_if_in_df.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import retain_variables_if_in_df\n\nvars_in_df = retain_variables_if_in_df(df, variables = [\"Name\", \"City\", \"Dogs\"])\n\nvar_in_df\n```\n\n----------------------------------------\n\nTITLE: Showing unique values of 'embarked' in train dataset in Python\nDESCRIPTION: This snippet demonstrates how the unique categories are represented in the 'embarked' column of the training dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# encoding that would be gotten from the train set\ntrain.embarked.unique()\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Titanic Data - Python\nDESCRIPTION: This code snippet loads the Titanic dataset using Feature-engine and splits it into training and testing sets. It also preprocesses the data to handle missing values and extracts the first letter of the cabin feature. This setup prepares the data for subsequent feature engineering steps, such as rare label encoding and Weight of Evidence (WoE) encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import WoEEncoder, RareLabelEncoder\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    handle_missing=True,\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Creating Example Dataframe for ExpandingWindowFeatures\nDESCRIPTION: This snippet creates a pandas DataFrame with time series data, including numerical and categorical variables, and a datetime index. This DataFrame serves as input for demonstrating the ExpandingWindowFeatures transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nX = {\"ambient_temp\": [31.31, 31.51, 32.15, 32.39, 32.62, 32.5, 32.52, 32.68],\n     \"module_temp\": [49.18, 49.84, 52.35, 50.63, 49.61, 47.01, 46.67, 47.52],\n     \"irradiation\": [0.51, 0.79, 0.65, 0.76, 0.42, 0.49, 0.57, 0.56],\n     \"color\": [\"green\"] * 4 + [\"blue\"] * 4,\n     }\n\nX = pd.DataFrame(X)\nX.index = pd.date_range(\"2020-05-15 12:00:00\", periods=8, freq=\"15min\")\n\ny = pd.Series([1,2,3,4,5,6,7,8])\ny.index = X.index\n\nX.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying Value Counts After RareLabelEncoding with max_n_categories\nDESCRIPTION: This displays the frequency of the remaining values of the toy dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nA       10\nB       10\nRare     3\nName: var_A, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Accessing PSI Values for Time-Based Split\nDESCRIPTION: This snippet shows how to access the PSI values calculated by the `DropHighPSIFeatures` transformer after fitting it to the data. The `psi_values_` attribute provides a dictionary where keys are feature names and values are their corresponding PSI scores.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntransformer.psi_values_\n\n{'var_0': 0.0181623637463045,\n'var_1': 0.10595496570984747,\n'var_2': 0.05425659114295842,\n'var_3': 0.09720689210928271,\n'var_4': 0.07917647542638032,\n'var_5': 0.10122468631060424,\n'century': 8.272395772368412}\n```\n\n----------------------------------------\n\nTITLE: Plotting Variable Distributions\nDESCRIPTION: This code snippet generates histograms for the 'LotArea' and 'GrLivArea' variables in the training dataset.  The purpose is to visualize the initial distribution of these variables before applying the Box-Cox transformation, demonstrating their non-normal distribution.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nX_train[['LotArea', 'GrLivArea']].hist(figsize=(10,5))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Data Type Verification after Transformation with Matching Types\nDESCRIPTION: This code checks the data type of the 'sex' column in the `test_ttt` DataFrame after using `MatchVariables` with `match_dtypes=True`. This verifies that the data type has been correctly matched to that of the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntest_ttt.sex.dtype\n```\n\n----------------------------------------\n\nTITLE: Displaying Raw Data Head\nDESCRIPTION: This code snippet prints the head of the original, untransformed training data for the specified target numeric features ('LotArea' and 'GrLivArea'). It allows for comparison with the transformed data. Dependencies: pandas DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Raw data\nprint(X_train[TARGET_NUMERIC_FEATURES].head())\n```\n\n----------------------------------------\n\nTITLE: Displaying authors using HTML\nDESCRIPTION: This HTML snippet displays a container with information about the project authors, including their GitHub avatars and links to their profiles. The styling applies rounded borders and padding to the avatars.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/about/authors.rst#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n.. raw :: html\n\n    <!-- Generated by generate_authors_table.py -->\n    <div class=\"authors-container\">\n        <style>\n          img.avatar {border-radius: 15px; padding: 10px;}\n          .author {text-align: center;}\n        </style>\n        <div class=\"author\">\n            <a href='https://github.com/solegalli'><img src='https://avatars.githubusercontent.com/solegalli?v=4' class='avatar' width=\"120\" height=\"120\" /></a> <br />\n            <p>Soledad Galli</p>\n        </div>\n        <div class=\"author\">\n            <a href='https://github.com/morgan-sell'><img src='https://avatars.githubusercontent.com/morgan-sell?v=4' class='avatar' width=\"120\" height=\"120\" /></a> <br />\n            <p>Morgan Sell</p>\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Splitting Dataset and Selecting Subset\nDESCRIPTION: Splits the dataset into training and testing sets using `train_test_split`, retaining only 'Neighborhood', 'LotFrontage', and 'MasVnrArea' as features. A subset of the test set with specific indices (`target_idx`) is then selected for demonstration purposes. This simulates a scenario where specific observations with missing data are targeted for imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\ttarget_features = ['Neighborhood','LotFrontage','MasVnrArea']\n\tX = X[target_features]\n\n\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\t# Select specific houses with missing data from the test set\n\ttarget_idx = [113,292,650, 1018]\n\tX_test_subset = X_test.loc[target_idx]\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting DropFeatures Transformer\nDESCRIPTION: This code snippet demonstrates how to initialize the `DropFeatures` transformer with a list of features to drop and fit the transformer to the training data. The `features_to_drop` parameter specifies the columns that will be removed from the DataFrame. Fitting the transformer is a requirement but it does not learn any parameters.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropFeatures.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# set up the transformer\ntransformer = DropFeatures(\n    features_to_drop=['sibsp', 'parch', 'ticket', 'fare', 'body', 'home.dest']\n)\n\n# fit the transformer\ntransformer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Installing Feature-engine with conda\nDESCRIPTION: Installs the Feature-engine package using conda from the conda-forge channel. Conda is a package and environment management system.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/index.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ conda install -c conda-forge feature_engine\n```\n\n----------------------------------------\n\nTITLE: Fit and Transform Data with Scikit-learn Cyclical Encoder\nDESCRIPTION: This code snippet applies the cyclical encoding defined in the previous step to the input DataFrame `df`.  It fits the `cyclic_cossin_transformer` to the data and then transforms it. The transformed data, containing the sine and cosine features, is then printed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nXt = cyclic_cossin_transformer.fit_transform(df)\n\n    print(Xt)\n```\n\n----------------------------------------\n\nTITLE: Installing Feature-engine Package\nDESCRIPTION: This command installs the feature-engine package using pip. This is a prerequisite to run any Feature-engine code within the Jupyter notebook.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_jup.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install feature_engine\n```\n\n----------------------------------------\n\nTITLE: Fitting MatchVariables Transformer in Python\nDESCRIPTION: This snippet shows how to initialize and fit the `MatchVariables` transformer to the training data. The `missing_values=\"ignore\"` argument is set, indicating how missing values should be handled. The `fit` method learns the column names from the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# set up the transformer\nmatch_cols = MatchVariables(missing_values=\"ignore\")\n\n# learn the variables in the train set\nmatch_cols.fit(train)\n```\n\n----------------------------------------\n\nTITLE: Printing Transformed Dataframe - Python\nDESCRIPTION: This code prints the head of the transformed training dataframe (`train_t`). This allows you to inspect the encoded features and verify that the transformation was successful.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Outlier Trimming with MAD\nDESCRIPTION: This snippet demonstrates how to use the OutlierTrimmer class with the MAD (Median Absolute Deviation) capping method to remove outliers from the 'fare' variable. It includes fitting the transformer to the training data, transforming the data, and generating a box plot of the transformed variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\not = OutlierTrimmer(capping_method='mad',\n                        tail='right',\n                        fold=3,\n                        variables=['fare'],\n                        )\n\n    ot.fit(X_train)\n\n    train_t, y_train_t = ot.transform_x_y(X_train, y_train)\n    test_t, y_test_t = ot.transform_x_y(X_test, y_test)\n\n    train_t.boxplot(column=[\"fare\"])\n    plt.title(\"Box plot - outliers\")\n    plt.ylabel(\"variable values\")\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Plot Feature Importance with Standard Deviation\nDESCRIPTION: This code creates a bar plot of the feature importances along with their standard deviations. It concatenates the mean and standard deviation of feature importances into a single DataFrame, sorts by mean importance, and then plots the mean importance with error bars representing the standard deviation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n    sel.feature_importances_,\n    sel.feature_importances_std_\n], axis=1)\n\nr.columns = [\"mean\", \"std\"]\n\nr.sort_values(\"mean\", ascending=False)[\"mean\"].plot.bar(\n    yerr=[r['std'], r['std']], subplots=True, figsize=(15,6)\n)\nplt.title(\"Feature importance derived from the random forests\")\nplt.ylabel(\"Feature importance\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Load and Split Titanic Dataset\nDESCRIPTION: Loads the Titanic dataset, preprocesses it by handling missing values and selecting predictor variables, and then splits the data into training and testing sets using `train_test_split`. The resulting training set is printed to display the first few rows. This is a preliminary step to demonstrate ordinal encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.encoding import OrdinalEncoder\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    handle_missing=True,\n    predictors_only=True,\n    cabin=\"letter_only\",\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Lag Features from Pandas Series with to_frame() in Python\nDESCRIPTION: This snippet demonstrates how to create lag features from a pandas Series by first converting it to a pandas DataFrame using the to_frame() method.  It then uses the LagFeatures transformer in Feature Engine to create lag features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlag_f = LagFeatures(periods=[1, 2, 3])\n\nX_tr = lag_f.fit_transform(X['ambient_temp'].to_frame())\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Using check_all_variables to verify variable presence\nDESCRIPTION: This snippet demonstrates how to use the `check_all_variables` function from `feature_engine.variable_handling` to verify if a list of variables are present in the dataframe 'X'. It initializes `checked_vars` with the result, which should be the input list if all variables are present.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_all_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import check_all_variables\n\nchecked_vars = check_all_variables(X, [\"num_var_1\", \"cat_var1\", \"date1\"])\n\nchecked_vars\n```\n\n----------------------------------------\n\nTITLE: Initializing ArbitraryOutlierCapper\nDESCRIPTION: This code snippet initializes the ArbitraryOutlierCapper to cap the 'age' variable at 50 and the 'fare' variable at 200 on the upper side. The min_capping_dict is set to None, indicating no lower capping is applied. It fits the transformer to the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/ArbitraryOutlierCapper.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncapper = ArbitraryOutlierCapper(\n    max_capping_dict={'age': 50, 'fare': 200},\n    min_capping_dict=None,\n)\n\ncapper.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Setting up ArcsinTransformer - Python\nDESCRIPTION: This code snippet initializes and fits an `ArcsinTransformer` object from the `feature_engine` library. The `variables` parameter specifies the columns to be transformed. The `fit` method checks if the variables are numerical and within the [0,1] range, but does not learn any parameters.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/ArcsinTransformer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# set up the arcsin transformer\ntf = ArcsinTransformer(variables = vars_)\n\n# fit the transformer\ntf.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Build documentation with Sphinx\nDESCRIPTION: Builds the documentation using Sphinx, outputting the HTML files to the 'build' folder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\n$ sphinx-build -b html docs build\n```\n\n----------------------------------------\n\nTITLE: Calling check_all_variables returns variable list\nDESCRIPTION: This snippet shows the expected output of the `check_all_variables` function when all variables are present in the DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_all_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n['num_var_1', 'cat_var1', 'date1']\n```\n\n----------------------------------------\n\nTITLE: Identifying Features to Drop (Categorical, Single Value)\nDESCRIPTION: This snippet retrieves the list of features to drop based on high PSI values when splitting data using a single categorical value as a cutoff.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntransformer.features_to_drop_\n\n['group_means', 'shifted_feature']\n```\n\n----------------------------------------\n\nTITLE: Initializing Data for Unique Values Split\nDESCRIPTION: This code sets up data for split_distinct case, adding a group column with different size groups, and an income variable dependent on the group. This simulates a scenario where controlling distinct group values is important for PSI calculation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.datasets import make_classification\nfrom feature_engine.selection import DropHighPSIFeatures\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=6,\n    random_state=0\n)\n\ncolnames = [\"var_\" + str(i) for i in range(6)]\nX = pd.DataFrame(X, columns=colnames)\n\n# Add a categorical column\nX['group'] = [\"A\", \"B\", \"C\", \"D\", \"E\"] * 100 + [\"F\"] * 500\n\n# And an income variable that is category dependent.\nnp.random.seed(0)\nX['income'] = np.random.uniform(1000, 2000, 500).tolist() +\n```\n\n----------------------------------------\n\nTITLE: Plotting Transformed Data\nDESCRIPTION: This snippet generates a box plot of the transformed 'age' variable after applying outlier trimming with percentile capping.  It helps visualize the effect of the transformation on the variable's distribution.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntrain_t.boxplot(column=['age'])\n    plt.title(\"Box plot - outliers\")\n    plt.ylabel(\"variable values\")\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Relative Features with RelativeFeatures\nDESCRIPTION: This code snippet demonstrates how to create a new feature by subtracting 'nonflavanoid_phenols' from 'total_phenols' using the RelativeFeatures transformer from Feature-engine. It defines the variables and the subtraction function, fits the transformer to the data, transforms the data, and then prints the first few rows of the transformed DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/index.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrf = RelativeFeatures(\n    variables=[\"total_phenols\"],\n    reference=[\"nonflavanoid_phenols\"],\n    func=[\"sub\"],\n)\n\nrf.fit(X)\nX_tr = rf.transform(X)\n\nprint(X_tr.head())\n```\n\n----------------------------------------\n\nTITLE: Displaying Encoded Variable Dictionary in Python\nDESCRIPTION: This code displays the `encoder.encoder_dict_` to show what categories were retained for each specified feature.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Plot Variable Distribution Before and After Discretization\nDESCRIPTION: Creates a figure with two subplots to display the histogram of the 'LotArea' variable before and after discretization.  It demonstrates the impact of the transformation on the variable's distribution. Requires matplotlib to be installed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/GeometricWidthDiscretiser.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    fig, ax = plt.subplots(1, 2)\n    X_train['LotArea'].hist(ax=ax[0], bins=10);\n    train_t['LotArea'].hist(ax=ax[1], bins=10);\n```\n\n----------------------------------------\n\nTITLE: Navigating to a Directory\nDESCRIPTION: This command changes the current directory to `feature-engine-examples`. It's essential to navigate to the project directory to execute commands within the project's scope.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_jup.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd feature-engine-examples\n```\n\n----------------------------------------\n\nTITLE: Access Learned Parameters from Scaler\nDESCRIPTION: This code snippet demonstrates how to access the mean and range values learned by the MeanNormalizationScaler after it has been fitted to the data. It prints these values to the console.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/scaling/MeanNormalizationScaler.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# access the parameters learned by the scaler\nprint(f'Means: {scaler.mean_}')\nprint(f'Ranges: {scaler.range_}')\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed Data Head\nDESCRIPTION: This code snippet displays the head (first few rows) of the transformed Pandas DataFrame. It assumes that `Xtr` is a Pandas DataFrame that has been previously transformed using a feature selection technique.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n     pclass     sex     fare cabin\n1139      3    male   7.8958     M\n533       2  female  21.0000     M\n459       2    male  27.0000     M\n1150      3    male  14.5000     M\n393       2    male  31.5000     M\n```\n\n----------------------------------------\n\nTITLE: Upgrading Feature-engine with pip\nDESCRIPTION: Upgrades the Feature-engine package to the latest version using pip. The -U flag ensures that any dependencies are also upgraded if necessary.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/index.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U feature-engine\n```\n\n----------------------------------------\n\nTITLE: Push changes to remote git repository\nDESCRIPTION: Pushes the committed changes to the remote Git repository on the specified branch.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\n$ git push origin my_feature_branch\n```\n\n----------------------------------------\n\nTITLE: Creating Math Features with MathFeatures\nDESCRIPTION: This code snippet demonstrates how to create new features by calculating the sum and mean of 'flavanoids', 'proanthocyanins', and 'proline' using the MathFeatures transformer.  It defines the variables and the desired mathematical functions, fits the transformer to the data, transforms the data, and then prints the first few rows of the transformed DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/index.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmf = MathFeatures(\n    variables=[\"flavanoids\", \"proanthocyanins\", \"proline\"],\n    func=[\"sum\", \"mean\"],\n)\n\nmf.fit(X_tr)\nX_tr = mf.transform(X_tr)\n\nprint(X_tr.head())\n```\n\n----------------------------------------\n\nTITLE: Finding All Variables in DataFrame\nDESCRIPTION: This snippet utilizes the `find_all_variables` function from the `feature_engine.variable_handling` module to extract all variable names from the DataFrame `X`. The extracted names are then stored in the `vars_all` variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_all_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import find_all_variables\n\nvars_all = find_all_variables(X)\n\nvars_all\n```\n\n----------------------------------------\n\nTITLE: Adding and Committing Changes\nDESCRIPTION: These commands add all changes to the staging area and commit them with a meaningful message. The `git add .` command stages all modified and new files. The `git commit` command saves the staged changes to the local repository with a descriptive message.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_jup.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit add .\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -m \"a meaningful commit message\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit pull origin mynotebookbranch\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean ROC-AUC - Python\nDESCRIPTION: This code snippet calculates the mean ROC-AUC score across all features using the `feature_performance_` attribute of the SelectByTargetMeanPerformance object. It converts the dictionary to a pandas Series and then calculates the mean.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npd.Series(sel.feature_performance_).mean()\n```\n\n----------------------------------------\n\nTITLE: Accessing Categorical Variables Identified by OneHotEncoder in Python\nDESCRIPTION: This snippet demonstrates how to access the categorical variables identified by the OneHotEncoder after fitting it to the training data. The `encoder.variables_` attribute stores a list of the names of the categorical columns that were found and will be encoded.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nencoder.variables_\n```\n\n----------------------------------------\n\nTITLE: Display Transformed Target\nDESCRIPTION: Displays the re-aligned target variable (yt) after applying the `transform_x_y` method of the Feature-engine pipeline.  This target variable corresponds to the transformed feature matrix, ensuring consistency after dropping rows with missing values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nyt\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed DataFrame Column Names\nDESCRIPTION: This snippet shows how to access and print the column names of the transformed Pandas DataFrame.  It is used to verify that the specified features have been successfully removed by the `DropFeatures` transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropFeatures.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ntrain_t.columns\n```\n\n----------------------------------------\n\nTITLE: Accessing Information Values\nDESCRIPTION: Accesses the `information_values_` attribute of the fitted `SelectByInformationValue` transformer to retrieve a dictionary containing the calculated information value for each evaluated variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsel.information_values_\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Discretized Values\nDESCRIPTION: This snippet demonstrates how to transform the training and testing data using the fitted `EqualWidthDiscretiser`. The `transform` method converts the original continuous values into discrete values based on the learned bin boundaries.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualWidthDiscretiser.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Transform the data (data discretization)\ntrain_t = disc.transform(X_train)\ntest_t = disc.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Accessing Best Score from GridSearchCV - Python\nDESCRIPTION: This code snippet retrieves and displays the best score (accuracy in this case) achieved by the `GridSearchCV` during hyperparameter optimization. The `best_score_` attribute represents the mean cross-validated score of the best estimator.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ngrid.best_score_\n```\n\n----------------------------------------\n\nTITLE: WindowFeatures Instantiation (Python)\nDESCRIPTION: This snippet initializes the `WindowFeatures` transformer to create window features (mean) from the 'demand' variable. It calculates the mean over a 3-hour window, with a frequency of 1 hour, ignores missing values during calculation, drops the original variable and rows with NaN values created by the windowing.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwinf = WindowFeatures(\n    variables=[\"demand\"],\n    window=[\"3h\"],\n    freq=\"1h\",\n    functions=[\"mean\"],\n    missing_values=\"ignore\",\n    drop_original=True,\n    drop_na=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Target Value per Encoded Category in Python\nDESCRIPTION: This code snippet joins the transformed test data with the original target variable, groups the data by the encoded 'HouseAgeCategorical' variable, and calculates the mean of the target variable ('MedHouseVal') for each encoded category. This helps verify that the categories have been encoded in the order of their mean target values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntest_set = X_test_t.join(y_test)\nmean_target_per_encoded_category = test_set[['HouseAgeCategorical', 'MedHouseVal']].groupby('HouseAgeCategorical').mean().reset_index()\nprint(mean_target_per_encoded_category)\n```\n\n----------------------------------------\n\nTITLE: Output Right Tail Caps (Percentiles)\nDESCRIPTION: This snippet displays the output of the upper limit for outlier capping using percentiles for the 'age' feature.  It provides the value beyond which data points are considered outliers.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n{'age': 54.0}\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Feature-engine Pipeline\nDESCRIPTION: This snippet shows how to transform the predictor dataset using the Feature-engine Pipeline. It demonstrates the standard transform method available in scikit-learn's pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = pipe.transform(X_train)\n\n    X_train.shape, train_t.shape\n```\n\n----------------------------------------\n\nTITLE: Transforming Data\nDESCRIPTION: This code demonstrates how to transform the training and testing data using the fitted DropDuplicateFeatures transformer. The transform method removes the identified duplicate features from the datasets.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropDuplicateFeatures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_t = transformer.transform(X_train)\ntest_t = transformer.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Accuracy\nDESCRIPTION: This snippet shows how to evaluate the accuracy of the trained Feature-engine Pipeline on the test set. It uses the `score` method of the pipeline to calculate the accuracy.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\npipe.score(X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Viewing Interval Boundaries Dictionary\nDESCRIPTION: Shows an example of the `binner_dict_` attribute containing the boundaries.  The keys are the variable names, and the values are lists representing the boundaries of the intervals created by the decision tree. `-inf` and `inf` represent negative and positive infinity.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n{'LotArea': [-inf, 8637.5, 10924.0, 13848.5, inf],\n     'GrLivArea': [-inf,\n      749.5,\n      808.0,\n      1049.0,\n      1144.5,\n      1199.0,\n      1413.0,\n      1438.5,\n      1483.0,\n      1651.5,\n      1825.0,\n      1969.5,\n      2386.0,\n      2408.0,\n      2661.0,\n      4576.0,\n      inf]}\n```\n\n----------------------------------------\n\nTITLE: Imputing NaN Values: WindowFeatures\nDESCRIPTION: This code demonstrates imputing NaN values introduced by the WindowFeatures transformation using Feature-engine's ArbitraryNumberImputer within a pipeline.  It shows how to replace NaN values with a specific number (-99 in this case). The code showcases combining WindowFeatures and an imputer in a Pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.imputation import ArbitraryNumberImputer\nfrom feature_engine.pipeline import Pipeline\n\nwin_f = WindowFeatures(\n    window=[\"30min\", \"60min\"],\n    functions=[\"mean\", ],\n    freq=\"15min\",\n)\n\npipe = Pipeline([\n    (\"windows\", win_f),\n    (\"imputer\", ArbitraryNumberImputer(arbitrary_number=-99))\n])\n\nX_tr = pipe.fit_transform(X, y)\n\nprint(X_tr.head())\n```\n\n----------------------------------------\n\nTITLE: Data Loading and Preprocessing for Forecasting (Python)\nDESCRIPTION: This code snippet demonstrates loading a time series dataset, preprocessing it by dropping irrelevant columns, converting the date and period columns to a datetime format, handling missing values, renaming columns, and resampling the data to hourly intervals for forecasting purposes. It utilizes pandas for data manipulation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://raw.githubusercontent.com/tidyverts/tsibbledata/master/data-raw/vic_elec/VIC2015/demand.csv\"\ndf = pd.read_csv(url)\n\ndf.drop(columns=[\"Industrial\"], inplace=True)\n\n# Convert the integer Date to an actual date with datetime type\ndf[\"date\"] = df[\"Date\"].apply(\n    lambda x: pd.Timestamp(\"1899-12-30\") + pd.Timedelta(x, unit=\"days\")\n)\n\n# Create a timestamp from the integer Period representing 30 minute intervals\ndf[\"date_time\"] = df[\"date\"] + \\\n    pd.to_timedelta((df[\"Period\"] - 1) * 30, unit=\"m\")\n\ndf.dropna(inplace=True)\n\n# Rename columns\ndf = df[[\"date_time\", \"OperationalLessIndustrial\"]]\n\ndf.columns = [\"date_time\", \"demand\"]\n\n# Resample to hourly\ndf = (\n    df.set_index(\"date_time\")\n    .resample(\"h\")\n    .agg({\"demand\": \"sum\"})\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing WoE encoded values\nDESCRIPTION: This code snippet visualizes the WoE encoded values for the 'age' variable using a bar plot. It retrieves the WoE encoder from the pipeline, sorts the WoE values, and then generates a plot showing the WoE for each age category.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/WoEEncoder.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nage_woe = pipe.named_steps['woe'].encoder_dict_['age']\n\nsorted_age_woe = dict(sorted(age_woe.items(), key=lambda item: item[1]))\ncategories = [str(k) for k in sorted_age_woe.keys()]\nlog_odds = list(sorted_age_woe.values())\n\nplt.figure(figsize=(10, 6))\nplt.bar(categories, log_odds, color='skyblue')\nplt.xlabel('Age')\nplt.ylabel('WoE')\nplt.title('WoE for Age')\nplt.grid(axis='y')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Accessing PSI Values for Categorical Split (Single Value)\nDESCRIPTION: This snippet shows how to access the PSI values calculated by the `DropHighPSIFeatures` transformer after splitting on a single categorical value. The `psi_values_` attribute provides the PSI scores for each feature.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntransformer.psi_values_\n\n{'var_0': 0.06485778974895254,\n'var_1': 0.03605540598761757,\n'var_2': 0.040632784917352296,\n'var_3': 0.023845405645510645,\n'var_4': 0.028007185972248064,\n'var_5': 0.07009152672971862,\n'group_means': 6.601444547497699,\n'shifted_feature': 0.48428009522119164}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Cumulative Distribution (Time-Based)\nDESCRIPTION: This code demonstrates how to visualize the cumulative distribution function (CDF) of features to compare their distributions before and after a specific cutoff. It utilizes the `seaborn` library to plot CDFs based on whether the data is above or below the specified cutoff date.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nX['above_cut_off'] = X.time > pd.to_datetime(transformer.cut_off_)\nsns.ecdfplot(data=X, x='century', hue='above_cut_off')\n\nsns.ecdfplot(data=X, x='var_2', hue='above_cut_off')\n```\n\n----------------------------------------\n\nTITLE: Checking Feature Importances\nDESCRIPTION: This code snippet shows how to access the feature importances derived from the Linear Regression model used within RecursiveFeatureAddition.  These importances are used to rank the features for the recursive addition process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntr.feature_importances_\n```\n\n----------------------------------------\n\nTITLE: Building and Fitting a Pipeline for Time Series Forecasting - Python\nDESCRIPTION: This code defines a scikit-learn `Pipeline` that incorporates `LagFeatures`, `WindowFeatures`, and a `MultiOutputRegressor` with `Lasso`. The `set_output(transform=\"pandas\")` configures the pipeline to output pandas DataFrames. The pipeline is then fitted to the training data (`X_train`, `y_train`).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline(\n        [\n            (\"lagf\", lagf),\n            (\"winf\", winf),\n            (\"lasso\", lasso),\n        ]\n    ).set_output(transform=\"pandas\")\n\n    pipe.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Accessing PSI Values for Categorical Split (List)\nDESCRIPTION: This snippet shows how to access the PSI values calculated by the `DropHighPSIFeatures` transformer after splitting on a list of categorical values. The `psi_values_` attribute provides the PSI scores for each feature.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntrans.psi_values_\n\n'var_0': 0.04322345673014104,\n'var_1': 0.03534439253617049,\n'var_2': 0.05220272785661243,\n'var_3': 0.04550964862452317,\n'var_4': 0.04492720670343145,\n'var_5': 0.044886435640028144,\n'group_means': 6.601444547497699,\n'shifted_features': 0.3683642099948127\n```\n\n----------------------------------------\n\nTITLE: Displaying the transformed data\nDESCRIPTION: Shows the first 5 rows of the transformed dataframe, including the column names and data. It uses the head() method to achieve that.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nXtr.head()\n```\n\n----------------------------------------\n\nTITLE: Datetime Subtraction with Feature-engine, Drop Originals\nDESCRIPTION: This code snippet demonstrates how to use the `DatetimeSubtraction` transformer from Feature-engine to subtract 'date2' from 'date1', express the difference in months, and drop the original datetime columns. The `drop_original` parameter is set to True.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.datetime import DatetimeSubtraction\n\ndata = pd.DataFrame({\n    \"date1\": pd.date_range(\"2019-03-05\", periods=5, freq=\"D\"),\n    \"date2\": pd.date_range(\"2018-03-05\", periods=5, freq=\"W\")})\n\ndtf = DatetimeSubtraction(\n    variables=\"date1\",\n    reference=\"date2\",\n    output_unit=\"M\",\n    drop_original=True\n)\n\ndata = dtf.fit_transform(data)\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Dataframe Creation with Target Variable in Python\nDESCRIPTION: This code snippet generates a Pandas DataFrame with correlated variables and a target variable using the make_classification function from scikit-learn. It creates a classification dataset, transforms it into a DataFrame, and returns the DataFrame and the target variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom feature_engine.selection import SmartCorrelatedSelection\n\n# make dataframe with some correlated variables\ndef make_data():\n    X, y = make_classification(n_samples=1000,\n                               n_features=12,\n                               n_redundant=4,\n                               n_clusters_per_class=1,\n                               weights=[0.50],\n                               class_sep=2,\n                               random_state=1)\n\n    # transform arrays into pandas df and series\n    colnames = ['var_'+str(i) for i in range(12)]\n    X = pd.DataFrame(X, columns=colnames)\n    return X, y\n\nX, y = make_data()\n```\n\n----------------------------------------\n\nTITLE: Transforming 'embarked' in test dataset using MatchCategories in Python\nDESCRIPTION: This snippet applies the fitted MatchCategories transformer to the 'embarked' column of the testing dataset and displays the categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# this will have the same encoding as the train set\nmatch_categories.transform(test).embarked.cat.categories\n```\n\n----------------------------------------\n\nTITLE: Complete Case Analysis with DropMissingData\nDESCRIPTION: Complete case analysis (or listwise deletion) removes rows containing any missing values. The :class:`DropMissingData()` class in Feature-engine implements this. It's suitable for MCAR data and when the proportion of missing values is small. However, it reduces the sample size and can introduce bias, affecting data analysis.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/api_doc/imputation/index.rst#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Decision Tree Discretizer Visualization\nDESCRIPTION: Groups the transformed training data by 'GrLivArea', counts the occurrences in each group, and plots the result as a bar chart. This allows for visualizing the distribution of houses across different 'GrLivArea' categories after discretization. The matplotlib library is used for plotting.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_t.groupby('GrLivArea')['GrLivArea'].count().plot.bar()\n    plt.ylabel('Number of houses')\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Sample Probability Output\nDESCRIPTION: This snippet displays the output of the probability predictions generated by the model. It shows the probabilities for each class for the first 10 data points.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\narray([[0.13027536, 0.86972464],\n           [0.14982143, 0.85017857],\n           [0.2783799 , 0.7216201 ],\n           [0.86907159, 0.13092841],\n           [0.31794531, 0.68205469],\n           [0.86905145, 0.13094855],\n           [0.1396715 , 0.8603285 ],\n           [0.48403632, 0.51596368],\n           [0.6299007 , 0.3700993 ],\n           [0.49712853, 0.50287147]])\n```\n\n----------------------------------------\n\nTITLE: Output Left Tail Caps (Gaussian)\nDESCRIPTION: This snippet displays the output of the lower limit for outlier capping using the Gaussian (Z-score) method for the 'age' feature.  It provides the value beyond which data points are considered outliers.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n{'age': -7.410476010820627}\n```\n\n----------------------------------------\n\nTITLE: Frequent Category Imputation with CategoricalImputer\nDESCRIPTION: Frequent category imputation replaces missing values in categorical variables with the most frequent category.  The :class:`CategoricalImputer()` from Feature-engine implements this method. It's fast and easy but can distort correlations and over-represent the most frequent category.  Best suited when missing values are a small percentage of the observations and the data is MCAR.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/api_doc/imputation/index.rst#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Checking Standard Deviation of Feature Importances\nDESCRIPTION: This code snippet demonstrates accessing the standard deviation of the feature importances, which provides insight into the variability of the feature importance scores calculated during cross-validation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntr.feature_importances_std_\n```\n\n----------------------------------------\n\nTITLE: Conda Environment Creation\nDESCRIPTION: This snippet demonstrates how to create a Conda environment. Conda environments help isolate project dependencies.  The --name flag specifies the name of the new environment.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nconda create --name myenv\n```\n\n----------------------------------------\n\nTITLE: Displaying category dictionary in Python\nDESCRIPTION: This code snippet displays the content of the `category_dict_` attribute, showcasing how categorical variables are mapped to their corresponding integer indices.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n{'pclass': Int64Index([1, 2, 3], dtype='int64'),\n     'sex': Index(['female', 'male'], dtype='object'),\n     'cabin': Index(['A', 'B', 'C', 'D', 'E', 'F', 'M', 'T'], dtype='object'),\n     'embarked': Index(['C', 'Missing', 'Q', 'S'], dtype='object')}\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Names in MatchVariables\nDESCRIPTION: This code shows how to access the feature names learned during the `fit` stage, stored in the `feature_names_in_` attribute of the `MatchVariables` object. This attribute contains the list of column names present in the training dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# the transformer stores the input variables\nmatch_cols.feature_names_in_\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Splitting into Training and Testing Sets in Python\nDESCRIPTION: This code snippet loads the house prices dataset using `fetch_openml` from `sklearn.datasets`, separates the features (X) and target (y), and splits the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. It then prints the head of the training set to display the predictor variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/DecisionTreeDiscretiser.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\ndata = fetch_openml(name='house_prices', as_frame=True)\ndata = data.frame\n\nX = data.drop(['SalePrice', 'Id'], axis=1)\ny = data['SalePrice']\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: venv Activation\nDESCRIPTION: This snippet demonstrates how to activate a virtual environment created with venv, specifically for Windows.  It activates the virtual environment so the project uses isolated dependencies. May need to use \\ instead of /.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nDocuments/Repositories/envs/featureengine/Scripts/activate\n```\n\n----------------------------------------\n\nTITLE: Getting Output Feature Names\nDESCRIPTION: This snippet demonstrates how to retrieve the names of the generated expanding window features using the `get_feature_names_out()` method of the `ExpandingWindowFeatures` transformer. It returns a list of all feature names in the transformed dataframe, including the original features and the new window features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwin_f.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Loading data and splitting it into train and test sets.\nDESCRIPTION: This code snippet loads the breast cancer dataset from scikit-learn, converts it to a pandas DataFrame, and then splits the data into training and testing sets using train_test_split. It depends on numpy, pandas, matplotlib, scikit-learn, and feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/ArcsinTransformer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\n\nfrom feature_engine.transformation import ArcsinTransformer\n  \n#Load dataset\nbreast_cancer = load_breast_cancer()\nX = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\ny = breast_cancer.target\n\n# Separate data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n```\n\n----------------------------------------\n\nTITLE: Lasso Linear Model Performance\nDESCRIPTION: This code provides the mean squared error (MSE) and root mean squared error (RMSE) for both the training and test sets after applying a Lasso Linear Model. It displays the model's performance on unseen data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/index.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    Lasso Linear Model train mse: 949189263.8948538\n    Lasso Linear Model train rmse: 30808.9153313591\n\n    Lasso Linear Model test mse: 1344649485.0641894\n    Lasso Linear Model train rmse: 36669.46256852136\n```\n\n----------------------------------------\n\nTITLE: Conda Environment Activation\nDESCRIPTION: This snippet demonstrates how to activate a Conda environment. Activating the environment ensures that the project uses the correct dependencies.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nconda activate myenv\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Names - Python\nDESCRIPTION: Retrieves the names of the features in the transformed DataFrame using the `get_feature_names_out()` method of the `SmartCorrelatedSelection` transformer. This is a standard method for scikit-learn transformers.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntr.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Plotting Distributions Before and After Discretization (Normal)\nDESCRIPTION: This snippet plots the original distribution and the transformed distribution of the normally distributed feature. It utilizes `matplotlib` and `pandas` plotting functionalities to visualize the effect of the discretization. The bins parameter in the histogram is set to the number of quantiles used for discretization.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].hist(X.feature1, bins=disc.q)\naxes[0].set(xlabel='feature1', ylabel='count', title='Raw data')\n\nX_transformed.feature1.value_counts().sort_index().plot.bar(ax=axes[1])\naxes[1].set_title('Transformed data')\n\nplt.suptitle('Normal distributed data', weight='bold', size='large', y=1.05)\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Toy Dataframe\nDESCRIPTION: This code creates a simple Pandas DataFrame named 'data' with a single column 'var_A' containing strings representing different categories with varying frequencies. This dataframe is then used to demonstrate the functionality of the RareLabelEncoder with the `max_n_categories` parameter.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.encoding import RareLabelEncoder\nimport pandas as pd\ndata = {'var_A': ['A'] * 10 + ['B'] * 10 + ['C'] * 2 + ['D'] * 1}\ndata = pd.DataFrame(data)\ndata['var_A'].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Loading Diabetes Dataset with Pandas\nDESCRIPTION: This code snippet loads the diabetes dataset from scikit-learn using pandas DataFrames. It imports necessary libraries like matplotlib, pandas, and sklearn and prints the head of the DataFrame to display the data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureAddition.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom feature_engine.selection import RecursiveFeatureAddition\n\n# load dataset\nX, y = load_diabetes(return_X_y=True, as_frame=True)\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Learned Bin Boundaries\nDESCRIPTION: This snippet shows how to access the learned bin boundaries after fitting the EqualFrequencyDiscretiser. The binner_dict_ attribute stores the bin limits for each variable. Accessing the limits enables inspection of how the discretizer has divided the data.  It returns a dictionary with bin boundaries for each feature.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Learned limits for each variable\ndisc.binner_dict_\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample DataFrame with Pandas\nDESCRIPTION: This code snippet generates a sample Pandas DataFrame with numerical, categorical, and datetime variables using `sklearn.datasets.make_classification` and `pd.date_range`. It transforms the arrays into a pandas DataFrame and series, assigning column names and creating string and datetime columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_all_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_redundant=1,\n    n_clusters_per_class=1,\n    weights=[0.50],\n    class_sep=2,\n    random_state=1,\n)\n\n# transform arrays into pandas df and series\ncolnames = [f\"num_var_{i+1}\" for i in range(4)]\nX = pd.DataFrame(X, columns=colnames)\n\nX[\"cat_var1\"] = [\"Hello\"] * 1000\nX[\"cat_var2\"] = [\"Bye\"] * 1000\n\nX[\"date1\"] = pd.date_range(\"2020-02-24\", periods=1000, freq=\"T\")\nX[\"date2\"] = pd.date_range(\"2021-09-29\", periods=1000, freq=\"H\")\nX[\"date3\"] = [\"2020-02-24\"] * 1000\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Run tests for a new feature\nDESCRIPTION: Runs the tests specifically created for a new feature, assuming the test script and folder are named accordingly.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest tests/test_my_new_feature_folder/test_my_new_feature.py\n```\n\n----------------------------------------\n\nTITLE: Listing Transformed DataFrame Column Names Output\nDESCRIPTION: This is the expected output showing the column names of the transformed dataset. The output reflects the remaining features after the specified columns were dropped. This is useful for verifying the successful removal of the desired features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropFeatures.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nIndex(['pclass', 'name', 'sex', 'age', 'cabin', 'embarked', 'boat'], dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Accessing Dropped Features After Binning\nDESCRIPTION: Accesses the `features_to_drop_` attribute of the fitted `SelectByInformationValue` transformer to retrieve the list of features that were identified for removal due to their information value being below the specified threshold, after binning the numerical features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsel.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Creating Time Series DataFrame with Pandas\nDESCRIPTION: This code initializes a pandas DataFrame with time series data. It includes numerical features ('ambient_temp', 'module_temp', 'irradiation'), a categorical feature ('color'), and a datetime index. It also creates a target variable (y) aligned with the DataFrame's index.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/WindowFeatures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nX = {\"ambient_temp\": [31.31, 31.51, 32.15, 32.39, 32.62, 32.5, 32.52, 32.68],\n     \"module_temp\": [49.18, 49.84, 52.35, 50.63, 49.61, 47.01, 46.67, 47.52],\n     \"irradiation\": [0.51, 0.79, 0.65, 0.76, 0.42, 0.49, 0.57, 0.56],\n     \"color\": [\"green\"] * 4 + [\"blue\"] * 4,\n     }\n\nX = pd.DataFrame(X)\nX.index = pd.date_range(\"2020-05-15 12:00:00\", periods=8, freq=\"15min\")\n\ny = pd.Series([1,2,3,4,5,6,7,8])\ny.index = X.index\n\nX.head()\n```\n\n----------------------------------------\n\nTITLE: Adding Changes to Git\nDESCRIPTION: This snippet shows how to stage all changed files in the current directory for a commit using `git add .`. It prepares all modifications for the next commit.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n$ git add .\n```\n\n----------------------------------------\n\nTITLE: Handling TypeError with Non-Datetime Variable - Python\nDESCRIPTION: This code snippet showcases the error handling of the `check_datetime_variables` function when a non-datetime variable is included in the input list. It passes the DataFrame `X` and a list containing a datetime variable (`\"date2\"`) and a categorical variable (`\"cat_var1\"`). Because `cat_var1` cannot be parsed as datetime, the function raises a `TypeError` indicating that some variables are not or cannot be parsed as datetime. The user should handle this TypeError.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_datetime_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncheck_datetime_variables(X, [\"date2\", \"cat_var1\"])\n```\n\n----------------------------------------\n\nTITLE: Accessing PSI Values after Split by Cutoff - Python\nDESCRIPTION: Illustrates how to access the PSI values calculated by `DropHighPSIFeatures` when splitting the data using a specific `split_col` and `cut_off` value. Note that the `split_col` will not have a PSI value. Dependencies include feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntransformer.psi_values_\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: This command shows how to install the dependencies required to build the documentation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample DataFrame with Datetime Variables - Python\nDESCRIPTION: This code snippet creates a Pandas DataFrame named `X` with numerical, categorical, and datetime variables. It uses `sklearn.datasets.make_classification` to generate numerical features and then adds categorical and datetime columns. The `date1` and `date2` columns are created using `pd.date_range`, while `date3` is created as a string column with a date format. The snippet prints the head of the DataFrame to display the resulting data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_datetime_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_redundant=1,\n    n_clusters_per_class=1,\n    weights=[0.50],\n    class_sep=2,\n    random_state=1,\n)\n\n# transform arrays into pandas df and series\ncolnames = [f\"num_var_{i+1}\" for i in range(4)]\nX = pd.DataFrame(X, columns=colnames)\n\nX[\"cat_var1\"] = [\"Hello\"] * 1000\nX[\"cat_var2\"] = [\"Bye\"] * 1000\n\nX[\"date1\"] = pd.date_range(\"2020-02-24\", periods=1000, freq=\"T\")\nX[\"date2\"] = pd.date_range(\"2021-09-29\", periods=1000, freq=\"H\")\nX[\"date3\"] = [\"2020-02-24\"] * 1000\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Plotting Probe Feature Histograms - Python\nDESCRIPTION: This example shows how to plot histograms of the generated probe features to visualize their distributions. `plt.show()` displays the histogram plot after configuring bins and figure size.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nsel.probe_features_.hist(bins=30, figsize=(10,10))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Accessing Variables with Missing Values\nDESCRIPTION: This code snippet demonstrates how to access the `variables_` attribute of a fitted `DropMissingData` transformer to see which columns contained missing data during the fitting process. This attribute stores a list of the column names that had NaN values in the training set.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndmd.variables_\n```\n\n----------------------------------------\n\nTITLE: Checkout main branch in git\nDESCRIPTION: Switches the active branch in the local Git repository to the 'main' branch.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout main\n```\n\n----------------------------------------\n\nTITLE: Checking Data Type After Transformation\nDESCRIPTION: This code checks the data type of the 'sex' column in the transformed DataFrame `test_tt`. This is compared to the original data type in the training set to highlight the potential need for data type matching using `MatchVariables`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntest_tt.sex.dtype\n```\n\n----------------------------------------\n\nTITLE: Accessing Encoded Variable Dictionary in Python\nDESCRIPTION: This snippet accesses the `encoder_dict_` attribute of the fitted `OneHotEncoder` object, displaying the dictionary that maps each encoded variable to its unique categories. This dictionary reveals the categories that will be represented by dummy variables. No prerequisites are required, assuming `encoder.fit(X_train)` has been executed successfully.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Installing Feature-engine from Anaconda\nDESCRIPTION: This code snippet shows how to install the feature-engine library using conda, a package, dependency and environment management system. It provides an alternative installation method, especially useful for users within the Anaconda ecosystem.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge feature_engine\n```\n\n----------------------------------------\n\nTITLE: Plotting variable distribution after imputation\nDESCRIPTION: This code snippet demonstrates plotting the kernel density estimation (KDE) of the 'LotFrontage' variable before and after imputation using the RandomSampleImputer. It visualizes how the imputer preserves the original variable distribution, by plotting original and transformed distributions on the same graph.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/RandomSampleImputer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\n\tfig = plt.figure()\n\tax = fig.add_subplot(111)\n\tX_train['LotFrontage'].plot(kind='kde', ax=ax)\n\ttrain_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')\n\tlines, labels = ax.get_legend_handles_labels()\n\tax.legend(lines, labels, loc='best')\n```\n\n----------------------------------------\n\nTITLE: Commit Changes in Git\nDESCRIPTION: This snippet shows how to commit staged changes to the local Git repository using `git commit -m \"my commit message\"`. It records the changes with an informative message.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n$ git commit -m \"my commit message\"\n```\n\n----------------------------------------\n\nTITLE: Plotting Data Distributions - Python\nDESCRIPTION: This code snippet plots the kernel density estimation (KDE) of the 'LotFrontage' variable before and after imputation to visualize the distribution change.  It requires matplotlib and pandas and highlights the impact of imputation on variable distribution.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/ArbitraryNumberImputer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfig = plt.figure()\nax = fig.add_subplot(111)\nX_train['LotFrontage'].plot(kind='kde', ax=ax)\ntrain_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')\n```\n\n----------------------------------------\n\nTITLE: Getting Transformed Feature Names - Python\nDESCRIPTION: This code obtains the names of the features that remain in the dataset after the transformation by the ProbeFeatureSelection transformer. The `get_feature_names_out()` method provides a list of the selected feature names.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsel.get_feature_names_out()\n```\n\n----------------------------------------\n\nTITLE: Import Libraries for Pipeline\nDESCRIPTION: Imports necessary libraries for creating and using Feature-engine pipelines, including pandas for data manipulation, numpy for numerical operations, DropMissingData for handling missing values, OrdinalEncoder for categorical encoding, and Pipeline for creating the transformation pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nfrom feature_engine.imputation import DropMissingData\nfrom feature_engine.encoding import OrdinalEncoder\nfrom feature_engine.pipeline import Pipeline\n```\n\n----------------------------------------\n\nTITLE: MultiOutputRegressor with Lasso (Python)\nDESCRIPTION: This snippet wraps a Lasso regression model within a `MultiOutputRegressor`. This is necessary because we are predicting multiple target variables (3 hours ahead). The `MultiOutputRegressor` trains a separate Lasso model for each target variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlasso = MultiOutputRegressor(Lasso(random_state=0, max_iter=10))\n```\n\n----------------------------------------\n\nTITLE: Checkout a git feature branch\nDESCRIPTION: Switches the active branch in the local Git repository to a specified feature branch.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_41\n\nLANGUAGE: bash\nCODE:\n```\n$ git checkout myfeaturebranch\n```\n\n----------------------------------------\n\nTITLE: Checking Target Shape Adjustment\nDESCRIPTION: This snippet shows how to access the shape of a transformed target variable after outlier trimming using the OutlierTrimmer. It is used to confirm that the target variable has been adjusted correctly after removing rows corresponding to outlier values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ny_train.shape, y_train_t.shape,\n```\n\n----------------------------------------\n\nTITLE: Outlier Trimming with Percentiles\nDESCRIPTION: This snippet demonstrates how to use the OutlierTrimmer class with percentile capping to remove outliers from the 'fare' variable.  It caps the variable at the 5th and 95th percentiles, and includes fitting the transformer to the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\not = OutlierTrimmer(capping_method='mad',\n                        tail='right',\n                        fold=0.05,\n                        variables=['fare'],\n                        )\n\n    ot.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Listing DataFrame Column Names Output\nDESCRIPTION: This is the expected output when printing the columns of the training dataframe. It shows the list of features available in the dataframe. This allows the user to understand which columns can be dropped.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropFeatures.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nIndex(['pclass', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare',\n       'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n      dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Accessing Cut-off Value - Python\nDESCRIPTION: Demonstrates how to retrieve the cut-off value used for splitting the dataframe, stored in the `cut_off_` attribute of the `DropHighPSIFeatures` transformer. This value represents the index or value at which the data was split into reference and test sets. Dependencies include feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntransformer.cut_off_\n```\n\n----------------------------------------\n\nTITLE: Verify Git Remote Configuration\nDESCRIPTION: This snippet shows how to verify that the remote repositories (origin and upstream) are correctly configured in the local Git repository. This command lists the configured remote connections, verifying the fetch and push URLs.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ git remote -v\n```\n\n----------------------------------------\n\nTITLE: Returning Rows with NA Values\nDESCRIPTION: This code snippet demonstrates how to use the `return_na_data` method of `DropMissingData` to obtain the rows that contain NA values. This is the opposite of the `transform` method, which removes rows with NA values. It's useful for understanding which rows are being removed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndmd.return_na_data(X)\n```\n\n----------------------------------------\n\nTITLE: Loading data and splitting into train/test sets\nDESCRIPTION: This code snippet loads the Titanic dataset, splits it into training and testing sets using train_test_split from scikit-learn, and prints the head of the training set.  It uses feature_engine's load_titanic to retrieve the data and preprocess it. The random_state ensures reproducibility.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/ArbitraryOutlierCapper.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.datasets import load_titanic\nfrom feature_engine.outliers import ArbitraryOutlierCapper\n\nX, y = load_titanic(\n    return_X_y_frame=True,\n    predictors_only=True,\n    handle_missing=True,\n)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0,\n)\n\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Box-Cox Transformation Formula\nDESCRIPTION: This code snippet defines the mathematical formula for the Box-Cox transformation, highlighting its behavior for different values of lambda (λ). It shows how the transformation modifies the input variable 'x' to produce the transformed variable 'y'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/BoxCoxTransformer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ny = (x**λ - 1) / λ,      for λ != 0\ny = log(x),              for λ = 0\n```\n\n----------------------------------------\n\nTITLE: Create a New Git Branch\nDESCRIPTION: This snippet creates a new branch in the local Git repository using the command `git checkout -b <branch_name>`. The new branch is created from main.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n$ git checkout -b myfeaturebranch\n```\n\n----------------------------------------\n\nTITLE: Setting up the Dataset and Libraries in Python\nDESCRIPTION: This code snippet imports necessary libraries like pandas, matplotlib, scikit-learn and the feature-engine library's OrdinalEncoder. It also fetches the California Housing dataset using `fetch_california_housing` and creates a pandas DataFrame from it. Finally, it prints the first few rows of the dataframe to display the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.encoding import OrdinalEncoder\nfrom sklearn.datasets import fetch_california_housing\n\nhousing = fetch_california_housing(as_frame=True)\ndata = housing.frame\n\nprint(data.head())\n```\n\n----------------------------------------\n\nTITLE: Initializing MeanEncoder to Ignore Format - Python\nDESCRIPTION: This code initializes the MeanEncoder to encode numerical variables as well. The `ignore_format` parameter is set to True, and the `variables` parameter specifies the numerical variables to encode.  Requires the library feature-engine to be installed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nencoder = MeanEncoder(\n    variables=['cabin', 'pclass'],\n    ignore_format=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Missing Data Percentage\nDESCRIPTION: Calculates the proportion of missing values in the 'LotFrontage' column of the training data (`X_train`). The result is rounded to two decimal places. This provides insight into the amount of missing data present in the dataset before imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\tX_train.LotFrontage.isnull().mean().round(2)\n```\n\n----------------------------------------\n\nTITLE: Displaying unique transformed 'cabin' categories from training set in Python\nDESCRIPTION: This code snippet displays the unique values from the 'cabin' column in the training dataset after the `MatchCategories` transformation.  It shows all the original categories.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n['B', 'C', 'E', 'D', 'A', 'M', 'T', 'F']\nCategories (8, object): ['A', 'B', 'C', 'D', 'E', 'F', 'M', 'T']\n```\n\n----------------------------------------\n\nTITLE: Import libraries for Skewed Data Binning\nDESCRIPTION: This snippet imports the necessary libraries for demonstrating the benefits of equal frequency discretization on skewed variables.  It includes NumPy, Pandas, Matplotlib, and the EqualFrequencyDiscretiser. Dependencies: numpy, pandas, matplotlib, feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom feature_engine.discretisation import EqualFrequencyDiscretiser\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for SelectByInformationValue\nDESCRIPTION: Imports necessary libraries including pandas for data manipulation, numpy for numerical operations, scikit-learn for train/test splitting, and SelectByInformationValue from feature_engine for feature selection based on information value.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByInformationValue.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.selection import SelectByInformationValue\n```\n\n----------------------------------------\n\nTITLE: Identifying Features to Drop (Time-Based)\nDESCRIPTION: This snippet retrieves the list of features identified for removal based on their high PSI values. The `features_to_drop_` attribute of the `DropHighPSIFeatures` transformer stores this list after the fitting process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntransformer.features_to_drop_\n\n['century']\n```\n\n----------------------------------------\n\nTITLE: Accessing Feature Performance Drifts\nDESCRIPTION: This snippet shows how to access the performance drifts caused by shuffling each feature. The tr.performance_drifts_ attribute stores the change in model performance for each feature after shuffling its values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntr.performance_drifts_\n```\n\n----------------------------------------\n\nTITLE: Scatter Plot of Hour vs Sine of Hour\nDESCRIPTION: This code creates a scatter plot of the 'hour' variable against its sine transformation ('hour_sin'). It adds axis labels, a title, and vertical dashed lines at hours 0 and 22 to highlight the cyclical nature of the encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplt.scatter(df[\"hour\"], df[\"hour_sin\"])\n\n# Axis labels\nplt.ylabel('Sine of hour')\nplt.xlabel('Hour')\nplt.title('Sine transformation')\n\nplt.vlines(x=0, ymin=-1, ymax=0, color='g', linestyles='dashed')\nplt.vlines(x=22, ymin=-1, ymax=-0.25, color='g', linestyles='dashed')\n```\n\n----------------------------------------\n\nTITLE: Displaying Maximum Transformed Values - Python\nDESCRIPTION: This snippet displays the maximum values of the 'fare' and 'age' columns after applying the Winsorizer transformation. The values should match the `right_tail_caps_` values determined earlier.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/Winsorizer.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfare    174.703953\nage      67.739512\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Plotting Mean House Price by LotShape (After Encoding)\nDESCRIPTION: This code snippet groups the transformed test data by the 'LotShape' feature (after encoding) and plots the mean house price for each encoded category. It illustrates the relationship between the encoded 'LotShape' categories and the target variable after the decision tree encoding.  The plot is displayed using `plt.show()`, after setting the y-axis label with `plt.ylabel`. Dependencies include `pandas` and `matplotlib`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ny_test.groupby(test_t[\"LotShape\"]).mean().plot.bar()\nplt.ylabel(\"mean house price\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Dropping Columns from a Pandas DataFrame in Python\nDESCRIPTION: This code snippet demonstrates how to drop columns from a Pandas DataFrame using the `drop` method. Specifically, it drops the 'sex' and 'age' columns from the `test` DataFrame along the axis 1 (columns). The modified DataFrame is assigned to `test_t`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Let's drop some columns in the test set for the demo\ntest_t = test.drop([\"sex\", \"age\"], axis=1)\n\ntest_t.head()\n```\n\n----------------------------------------\n\nTITLE: Add file to git\nDESCRIPTION: Stages changes in 'my_new_script.py' for commit in Git.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\n$ git add my_new_script.py\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Testing Sets (Python)\nDESCRIPTION: This code divides the data into training and testing sets based on a specified date ('2014-12-31 23:59:59' for the end of training). This separation is crucial for evaluating the performance of the forecasting model on unseen data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nend_train = '2014-12-31 23:59:59'\nX_train = df.loc[:end_train]\ny_train = y.loc[:end_train]\n\nbegin_test = '2014-12-31 17:59:59'\nX_test  = df.loc[begin_test:]\ny_test = y.loc[begin_test:]\n```\n\n----------------------------------------\n\nTITLE: Plotting original variable distributions - Python\nDESCRIPTION: This code snippet generates histograms for the specified variables in the original training dataset.  It utilizes the hist method of the pandas DataFrame object along with matplotlib to create the visualization.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/ArcsinTransformer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# original variables\nX_train[vars_].hist(figsize=(20,20))\n```\n\n----------------------------------------\n\nTITLE: Displaying Target Variable\nDESCRIPTION: This snippet prints the target variable, a pandas Series with a datetime index, which corresponds to the time series data in the DataFrame.  The target variable is aligned with the dataframe's index.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny\n```\n\n----------------------------------------\n\nTITLE: Displaying Features to Drop - Python\nDESCRIPTION: This snippet shows an example of the output from `tr.features_to_drop_`. It displays a list of the features that have been identified as correlated and will be dropped during the transformation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n['var_8', 'var_6', 'var_7', 'var_9']\n```\n\n----------------------------------------\n\nTITLE: Print Target Variable\nDESCRIPTION: This snippet displays the target variable (Pandas Series) created in the previous example.  It's used to showcase the target variable before and after applying lag features and transformations.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ny\n```\n\n----------------------------------------\n\nTITLE: Accessing Duplicated Feature Sets\nDESCRIPTION: This code demonstrates how to access the `duplicated_feature_sets_` attribute of the fitted DropDuplicateFeatures transformer. This attribute contains a list of sets, where each set represents a group of duplicate features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropDuplicateFeatures.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntransformer.duplicated_feature_sets_\n```\n\nLANGUAGE: python\nCODE:\n```\n[{'sex', 'sex_dup'}, {'age', 'age_dup'}, {'sibsp', 'sibsp_dup'}]\n```\n\n----------------------------------------\n\nTITLE: Fitting MeanEncoder to Training Data - Python\nDESCRIPTION: This code snippet fits the MeanEncoder to the training data (X_train and y_train). The fit method calculates the mean target value for each category in the categorical variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/MeanEncoder.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nencoder.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Showing unique values of 'embarked' in test dataset in Python\nDESCRIPTION: This snippet demonstrates how the unique categories are represented in the 'embarked' column of the testing dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# encoding that would be gotten from the test set\ntest.embarked.unique()\n```\n\n----------------------------------------\n\nTITLE: Cloning the Feature-engine Repository\nDESCRIPTION: This command shows how to clone the feature-engine repository from GitHub. Cloning allows users to access the source code, contribute to the project, or install it manually.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/feature-engine/feature_engine.git\n```\n\n----------------------------------------\n\nTITLE: Plot Histogram of Probe Features\nDESCRIPTION: This code plots a histogram of the generated probe features to visualize their distribution. The number of bins is set to 30 for a detailed view.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsel.probe_features_.hist(bins=30)\n```\n\n----------------------------------------\n\nTITLE: Printing Transformed DataFrame - Python\nDESCRIPTION: Prints the first few rows of the transformed DataFrame (`Xt`) after applying the `SmartCorrelatedSelection` transformer. This demonstrates the effect of the feature selection process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(Xt.head())\n```\n\n----------------------------------------\n\nTITLE: Impute with Most Frequent\nDESCRIPTION: Initializes and fits a CategoricalImputer to impute missing values in 'Alley' and 'MasVnrType' with the most frequent category. The `fit` method calculates the most frequent value for each specified column. Requires the data and CategoricalImputer to be initialized.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimputer = CategoricalImputer(\n    variables=['Alley', 'MasVnrType'],\n    imputation_method=\"frequent\"\n)\n\nimputer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Transformed variables example\nDESCRIPTION: This snippet shows the transformed variables, specifically the 'cabin', 'pclass', and 'embarked' columns.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n            cabin    pclass  embarked\n501   0.304843  0.436170  0.338957\n588   0.304843  0.436170  0.338957\n402   0.304843  0.436170  0.553073\n1193  0.304843  0.259036  0.373494\n686   0.304843  0.259036  0.373494\n971   0.304843  0.259036  0.373494\n117   0.611650  0.617391  0.553073\n540   0.304843  0.436170  0.338957\n294   0.611650  0.617391  0.553073\n261   0.611650  0.617391  0.338957\n```\n\n----------------------------------------\n\nTITLE: Push changes to git fork\nDESCRIPTION: Pushes the updated local 'main' branch to the fork (remote) repository, ensuring they are in sync.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\n$ git push -f origin main\n```\n\n----------------------------------------\n\nTITLE: Plotting Histograms of Original Data with Matplotlib\nDESCRIPTION: This code snippet plots histograms of the original 'LotArea' and 'GrLivArea' variables from the training set using pandas and matplotlib. The `bins` parameter sets the number of bins, and `figsize` controls the figure size.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/YeoJohnsonTransformer.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nX_train[['LotArea', 'GrLivArea']].hist(bins=50, figsize=(10,4))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Dataset with Pandas and Scikit-learn (make_classification)\nDESCRIPTION: This code snippet initializes a sample dataset using pandas and scikit-learn's make_classification function. It creates a DataFrame with numerical and categorical variables, as well as datetime variables, for demonstration purposes. It requires pandas and scikit-learn to be installed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_categorical_and_numerical_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_redundant=1,\n    n_clusters_per_class=1,\n    weights=[0.50],\n    class_sep=2,\n    random_state=1,\n)\n\n# transform arrays into pandas df and series\ncolnames = [f\"num_var_{i+1}\" for i in range(4)]\nX = pd.DataFrame(X, columns=colnames)\n\nX[\"cat_var1\"] = [\"Hello\"] * 1000\nX[\"cat_var2\"] = [\"Bye\"] * 1000\n\nX[\"date1\"] = pd.date_range(\"2020-02-24\", periods=1000, freq=\"T\")\nX[\"date2\"] = pd.date_range(\"2021-09-29\", periods=1000, freq=\"H\")\nX[\"date3\"] = [\"2020-02-24\"] * 1000\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Plotting Cumulative Density Function with Seaborn - Python\nDESCRIPTION: This snippet demonstrates how to plot the cumulative density function (CDF) of a feature using Seaborn, highlighting the distribution difference between the reference and test datasets. It creates a new column 'above_cut_off' indicating whether an index is above the cut-off value and then uses `sns.ecdfplot` to visualize the distribution. Dependencies include pandas and seaborn.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nX['above_cut_off'] = X.index > transformer.cut_off_\nsns.ecdfplot(data=X, x='var_3', hue='above_cut_off')\n```\n\nLANGUAGE: python\nCODE:\n```\nsns.ecdfplot(data=X, x='var_1', hue='above_cut_off')\n```\n\n----------------------------------------\n\nTITLE: Import libraries for monotonic variable\nDESCRIPTION: This code imports necessary libraries, including matplotlib for plotting, sklearn datasets for loading datasets, sklearn model_selection for splitting data, and DecisionTreeEncoder from feature_engine for encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\nfrom feature_engine.encoding import DecisionTreeEncoder\n```\n\n----------------------------------------\n\nTITLE: Fitting DropDuplicateFeatures Transformer\nDESCRIPTION: This snippet fits the DropDuplicateFeatures transformer to the training data (X_train). During the fit process, the transformer identifies and stores the duplicate features based on the provided data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropDuplicateFeatures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntransformer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Initialize and Fit ProbeFeatureSelection\nDESCRIPTION: This code snippet initializes the `ProbeFeatureSelection` transformer with a `RandomForestClassifier` estimator, precision scoring, one normally distributed probe, 5-fold cross-validation, and a random state. It then fits the transformer to the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsel = ProbeFeatureSelection(\n    estimator=RandomForestClassifier(),\n    variables=None,\n    scoring=\"precision\",\n    n_probes=1,\n    distribution=\"normal\",\n    cv=5,\n    random_state=150,\n    confirm_variables=False\n)\n\nsel.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Transforming Pandas Series with ExpandingWindowFeatures in Python\nDESCRIPTION: This code snippet illustrates how to use the ExpandingWindowFeatures transformer with a pandas Series. It converts the Series to a DataFrame using `.to_frame()` before applying the transformer. The transformer calculates the mean and maximum expanding window features. The head of the transformed dataframe is then printed to show the new features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwin_f = ExpandingWindowFeatures(functions=[\"mean\", \"max\"])\n\nX_tr = win_f.fit_transform(X['ambient_temp'].to_frame())\n\nX_tr.head()\n```\n\n----------------------------------------\n\nTITLE: Create Example DataFrame with Datetime Variables - Python\nDESCRIPTION: This code snippet generates a Pandas DataFrame containing numerical, categorical, and datetime variables to demonstrate the functionality of `find_datetime_variables`. It utilizes `sklearn.datasets.make_classification` for initial data generation and `pandas.date_range` for creating datetime series. The `date3` column is intentionally created as a string representing a date to show that it can be detected as a datetime variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_datetime_variables.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_redundant=1,\n    n_clusters_per_class=1,\n    weights=[0.50],\n    class_sep=2,\n    random_state=1,\n)\n\n# transform arrays into pandas df and series\ncolnames = [f\"num_var_{i+1}\" for i in range(4)]\nX = pd.DataFrame(X, columns=colnames)\n\nX[\"cat_var1\"] = [\"Hello\"] * 1000\nX[\"cat_var2\"] = [\"Bye\"] * 1000\n\nX[\"date1\"] = pd.date_range(\"2020-02-24\", periods=1000, freq=\"T\")\nX[\"date2\"] = pd.date_range(\"2021-09-29\", periods=1000, freq=\"H\")\nX[\"date3\"] = [\"2020-02-24\"] * 1000\n\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Support - Python\nDESCRIPTION: This example demonstrates how to retrieve a boolean mask indicating which features are supported (selected) after applying the ProbeFeatureSelection transformer. It provides a mechanism consistent with Scikit-learn's selection transformers.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsel.get_support()\n```\n\n----------------------------------------\n\nTITLE: Generate code coverage report\nDESCRIPTION: Generates a report visualizing the code coverage from the previous pytest run.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n$ coverage report\n```\n\n----------------------------------------\n\nTITLE: Create DataFrame with Missing Values\nDESCRIPTION: Creates a pandas DataFrame with missing values (NaN) in both numerical and categorical columns.  Also creates a corresponding pandas Series as the target variable. This is a sample dataset to demonstrate the DropMissingData transformer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nX = pd.DataFrame(\n    dict(\n        x1=[2, 1, 1, 0, np.nan],\n        x2=[\"a\", np.nan, \"b\", np.nan, \"a\"],\n        x3=[2, 3, 4, 5, 5],\n    )\n)\ny = pd.Series([1, 2, 3, 4, 5])\n\nX.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying Right Tail Caps - Python\nDESCRIPTION: This snippet displays the right tail capping values calculated by the Winsorizer during the fit stage.  It shows the maximum values to which 'age' and 'fare' will be capped during the transform stage.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/Winsorizer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n{'age': 67.73951212364803, 'fare': 174.70395336846678}\n```\n\n----------------------------------------\n\nTITLE: Installing Feature-engine with pip\nDESCRIPTION: Installs the Feature-engine package using pip. This is the standard method for installing Python packages from the Python Package Index (PyPI).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/index.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install feature-engine\n```\n\n----------------------------------------\n\nTITLE: Displaying the First 5 Rows of Training Data in Python\nDESCRIPTION: This code snippet displays the first 5 rows of the training dataset, providing a preview of the data and its features before one-hot encoding. This allows for a quick inspection of the data's structure and content. No dependencies or prerequisites are required; it simply prints the head of the `X_train` dataframe.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(X_train.head())\n```\n\n----------------------------------------\n\nTITLE: Creating a Pandas DataFrame with Datetime Variables\nDESCRIPTION: This code snippet demonstrates how to create a pandas DataFrame with two datetime variables, 'date1' and 'date2', using `pd.date_range`. The 'date1' variable contains dates with daily frequency, and 'date2' contains dates with weekly frequency.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({\n    \"date1\": pd.date_range(\"2019-03-05\", periods=5, freq=\"D\"),\n    \"date2\": pd.date_range(\"2018-03-05\", periods=5, freq=\"W\")})\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Printing Transformed Dataframe - Python\nDESCRIPTION: This snippet prints the head of the transformed DataFrame after applying the DropCorrelatedFeatures transformer.  It demonstrates the removal of correlated features from the original dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(print(Xt.head()))\n```\n\n----------------------------------------\n\nTITLE: Accessing Right Tail Caps\nDESCRIPTION: This code snippet shows how to access the right_tail_caps_ attribute of the fitted ArbitraryOutlierCapper instance. This attribute stores the capping values defined in the max_capping_dict.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/ArbitraryOutlierCapper.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\tcapper.right_tail_caps_\n```\n\n----------------------------------------\n\nTITLE: Initializing DropDuplicateFeatures Transformer\nDESCRIPTION: This code initializes the DropDuplicateFeatures transformer. No parameters are passed, indicating that all columns in the dataset will be checked for duplicates.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropDuplicateFeatures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransformer = DropDuplicateFeatures()\n```\n\n----------------------------------------\n\nTITLE: Plotting Mean House Price by MSZoning (After Encoding)\nDESCRIPTION: This code snippet groups the transformed test data by the 'MSZoning' feature (after encoding) and plots the mean house price for each encoded category. It visualizes how the encoding affects the relationship between the categories and the target variable. The `plt.ylabel` sets the y-axis label, and `plt.show()` displays the plot. Dependencies include `pandas` and `matplotlib`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ny_test.groupby(test_t[\"MSZoning\"]).mean().plot.bar()\nplt.ylabel(\"mean house price\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Output of right_tail_caps_\nDESCRIPTION: This code shows the expected output of accessing the right_tail_caps_ attribute, demonstrating that the capping values are stored in a dictionary.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/ArbitraryOutlierCapper.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\t{'age': 50, 'fare': 200}\n```\n\n----------------------------------------\n\nTITLE: Creating a Feature Branch\nDESCRIPTION: This command shows how to create a new branch in the repository for a specific feature or bugfix to make changes.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b myfeaturebranch\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Data with NumPy and Pandas\nDESCRIPTION: This snippet uses NumPy to generate normally distributed and right-skewed data. It then combines these datasets into a Pandas DataFrame for subsequent discretization. A random seed is set for reproducibility of the generated data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate a normally distributed data\nnormal_data = np.random.normal(loc=0, scale=1, size=1000)\n\n# Generate a right-skewed data using exponential distribution\nskewed_data = np.random.exponential(scale=1, size=1000)\n\n# Create dataframe with simulated data\nX = pd.DataFrame({'feature1': normal_data, 'feature2': skewed_data})\n```\n\n----------------------------------------\n\nTITLE: Generating skewed toy datasets\nDESCRIPTION: This code generates synthetic left-skewed and right-skewed data using numpy's random number generation functions. It uses the exponential distribution for right-skewed data and a modified gamma distribution for left-skewed data. The data is then stored in a pandas DataFrame, and histograms of both distributions are plotted using seaborn and matplotlib.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\t# Set random seed for reproducibility\n\tnp.random.seed(42)\n\n\t# Generating right-skewed data using exponential distribution\n\tright_skewed_data = np.random.exponential(scale=2, size=1000)\n\n\t# Generating left-skewed data by flipping the right-skewed data\n\tleft_skewed_data = -np.random.gamma(shape=2, scale=2, size=1000) \\\n\t\t+ np.max(np.random.gamma(shape=2, scale=2, size=1000))\n\n\t# Create dataframe with simulated data\n\tdf_sim = pd.DataFrame({\n\t\t'left_skewed': left_skewed_data,\n\t\t'right_skewed': right_skewed_data}\n\t)\n\n\t# Plotting the distributions\n\tfig, axes = plt.subplots(ncols=2, figsize=(12, 4))\n\n\thist_params = dict(kde=True, bins=30, alpha=0.7)\n\tsns.histplot(df_sim.left_skewed, ax=axes[0], color='blue', **hist_params)\n\tsns.histplot(df_sim.right_skewed, ax=axes[1], color='red', **hist_params)\n\n\taxes[0].set_title('Left-skewed data')\n\taxes[1].set_title('Right-skewed data')\n\n\tplt.show()\n```\n\n----------------------------------------\n\nTITLE: Install Feature-engine with underscore\nDESCRIPTION: This snippet demonstrates installing Feature-engine using pip with an underscore in the package name. It's an alternative way to install the same package.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/index.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install feature_engine\n```\n\n----------------------------------------\n\nTITLE: Displaying Value Counts of Toy Dataframe\nDESCRIPTION: This displays the frequency of each unique value in the 'var_A' column of the DataFrame 'data'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nA    10\nB    10\nC     2\nD     1\nName: var_A, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Displaying Head of DataFrame after Column Removal\nDESCRIPTION: This code displays the first few rows of the DataFrame after columns have been dropped. The output shows that 'sex' and 'age' columns have been removed from the `test_t` DataFrame.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n     pclass  survived  sibsp  parch     fare cabin embarked\n1000      3         1      0      0   7.7500     n        Q\n1001      3         1      2      0  23.2500     n        Q\n1002      3         1      2      0  23.2500     n        Q\n1003      3         1      2      0  23.2500     n        Q\n1004      3         1      0      0   7.7875     n        Q\n```\n\n----------------------------------------\n\nTITLE: Visualizing Discretized MedInc values using bar plot\nDESCRIPTION: This snippet generates a bar plot to visualize the distribution of the discretized 'MedInc' variable.  It uses pandas' `value_counts()` to count the occurrences of each bin value and then plots these counts using a bar chart. The labels and title are set for clarity. Requires that the 'MedInc' column has already been transformed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/ArbitraryDiscretiser.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nX['MedInc'].value_counts().plot.bar(rot=0)\nplt.xlabel('MedInc - bins')\nplt.ylabel('Number of observations')\nplt.title('Discretised MedInc')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Handle Multiple Modes\nDESCRIPTION: Initializes and fits a CategoricalImputer to impute missing values in 'PoolQC' with the most frequent category. Demonstrates how the transformer raises a ValueError when a variable has multiple modes. Requires the data and CategoricalImputer to be initialized.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/CategoricalImputer.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimputer = CategoricalImputer(\n    variables=['PoolQC'],\n    imputation_method=\"frequent\"\n)\n\nimputer.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Printing Transformed Data\nDESCRIPTION: This code prints the head of the transformed dataset (Xt) after SelectByShuffling has removed the non-important features. This demonstrates the effect of the feature selection process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByShuffling.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(Xt.head())\n```\n\n----------------------------------------\n\nTITLE: Install Feature-engine with pip\nDESCRIPTION: This snippet shows how to install the Feature-engine library using pip. It installs the package from PyPI.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/index.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install feature-engine\n```\n\n----------------------------------------\n\nTITLE: Accessing Correlated Feature Sets - Python\nDESCRIPTION: Accesses the `correlated_feature_sets_` attribute of the fitted `SmartCorrelatedSelection` transformer. This attribute contains a list of sets, where each set represents a group of correlated features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SmartCorrelatedSelection.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntr.correlated_feature_sets_\n```\n\n----------------------------------------\n\nTITLE: Random Sample Imputation with RandomSampleImputer\nDESCRIPTION: Random sample imputation replaces missing values with a random sample from the observed values of the same variable. The :class:`RandomSampleImputer()` class from Feature-engine implements this technique. It preserves the variable's variance but can distort relationships with other variables.  It's computationally expensive compared to other methods and requires setting a proper seed for reproducibility.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/api_doc/imputation/index.rst#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Building the Documentation\nDESCRIPTION: This command shows how to build the documentation after having the dependencies installed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-build -b html docs build\n```\n\n----------------------------------------\n\nTITLE: Sample Feature Names Output\nDESCRIPTION: This snippet displays the sample feature names output after the preprocessing transformations applied by the pipeline, which include outlier trimming and one-hot encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n['pclass',\n     'age',\n     'sibsp',\n     'parch',\n     'fare',\n     'sex_female',\n     'sex_male',\n     'cabin_Missing',\n    ...\n```\n\n----------------------------------------\n\nTITLE: Plotting transformed variable distributions - Python\nDESCRIPTION: This code snippet generates histograms for the specified variables in the transformed training dataset. It showcases the distribution of the variables after applying the arcsin transformation using matplotlib.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/ArcsinTransformer.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# transformed variable\ntrain_t[vars_].hist(figsize=(20,20))\n```\n\n----------------------------------------\n\nTITLE: Example of transformed variables with precision\nDESCRIPTION: This is an example of the transformed variables with precision = 2. As we can observe, after the transformation, the values present only 2 decimals.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n          cabin  pclass  embarked\n501    0.30    0.44      0.34\n588    0.30    0.44      0.34\n402    0.30    0.44      0.55\n1193   0.30    0.26      0.37\n686    0.30    0.26      0.37\n971    0.30    0.26      0.37\n117    0.61    0.62      0.55\n540    0.30    0.44      0.34\n294    0.61    0.62      0.55\n261    0.61    0.62      0.34\n```\n\n----------------------------------------\n\nTITLE: Visualizing MedInc distribution with histogram\nDESCRIPTION: This code snippet demonstrates how to load the California housing dataset, plot a histogram of the 'MedInc' variable, and display the plot using matplotlib. It shows the initial distribution of the variable before discretization. It imports necessary libraries like numpy, pandas, matplotlib, sklearn, and the ArbitraryDiscretiser.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/discretisation/ArbitraryDiscretiser.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\nfrom feature_engine.discretisation import ArbitraryDiscretiser\n\nX, y = fetch_california_housing( return_X_y=True, as_frame=True)\n\nX['MedInc'].hist(bins=20)\nplt.xlabel('MedInc')\nplt.ylabel('Number of obs')\nplt.title('Histogram of MedInc')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying found Datetime Variables - Python\nDESCRIPTION: This snippet displays the list of identified datetime variables. It is the expected output of applying the function find_datetime_variables to the DataFrame X.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_datetime_variables.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n['date1', 'date2', 'date3']\n```\n\n----------------------------------------\n\nTITLE: Rare Label Encoding Example in Python\nDESCRIPTION: This Python code demonstrates how to use the RareLabelEncoder from the feature_engine.encoding module to encode rare labels in a pandas DataFrame.  It initializes a DataFrame, instantiates the RareLabelEncoder, and then transforms the data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n>>> import pandas as pd\n>>> from feature_engine.encoding import RareLabelEncoder\n\n>>> data = {'var_A': ['A'] * 10 + ['B'] * 10 + ['C'] * 2 + ['D'] * 1}\n>>> data = pd.DataFrame(data)\n>>> data['var_A'].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Initializing MatchCategories and loading data in Python\nDESCRIPTION: This snippet initializes the MatchCategories transformer and loads the Titanic dataset, splitting it into training and testing sets.  It converts the 'pclass' column to object type and handles missing values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.preprocessing import MatchCategories\nfrom feature_engine.datasets import load_titanic\n\n# Load dataset\ndata = load_titanic(\n    predictors_only=True,\n    handle_missing=True,\n    cabin=\"letter_only\",\n)\n\ndata['pclass'] = data['pclass'].astype('O')\n\n# Split test and train\ntrain = data.iloc[0:1000, :]\ntest = data.iloc[1000:, :]\n```\n\n----------------------------------------\n\nTITLE: Load and Display Breast Cancer Data\nDESCRIPTION: Loads the breast cancer dataset from sklearn.datasets, converts the target variable to a binary mapping (0:1, 1:0), and prints the first 5 rows of the features using pandas. This allows for inspecting the structure and values of the dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/MRMR.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\ny = y.map({0:1, 1:0})\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Printing first 10 rows of cabin, pclass, embarked columns\nDESCRIPTION: This code prints the first 10 rows of the 'cabin', 'pclass', and 'embarked' columns from the training dataset. This allows to check the categories before encoding.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    cabin  pclass embarked\n501      M       2        S\n588      M       2        S\n402      M       2        C\n1193     M       3        Q\n686      M       3        Q\n971      M       3        Q\n117      E       1        C\n540      M       2        S\n294      C       1        C\n261      E       1        S\n```\n\n----------------------------------------\n\nTITLE: Printing the Subset of the Test Set\nDESCRIPTION: Prints the selected subset of the test set (`X_test_subset`) to display the data containing missing values before imputation. This allows for visualizing the missing values and verifying the effect of the imputation.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\tprint(X_test_subset)\n```\n\n----------------------------------------\n\nTITLE: Displaying Data Type After Transformation\nDESCRIPTION: This code snippet displays the data type of the 'sex' column in the transformed test dataset. In this case it displays `float64`, indicating that the default behavior of adding missing values with NaN has changed the data type of the column, potentially leading to inconsistencies.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndtype('float64')\n```\n\n----------------------------------------\n\nTITLE: Printing encoding dictionary content\nDESCRIPTION: This snippet displays the dictionary holding the values used to replace each category in the variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    {'cabin': {'M': 0.30484330484330485,\n      'E': 0.6116504854368932,\n      'C': 0.6116504854368932,\n      'D': 0.6981132075471698,\n      'B': 0.6981132075471698,\n      'A': 0.6981132075471698,\n      'F': 0.6981132075471698,\n      'T': 0.0,\n      'G': 0.5},\n     'pclass': {2: 0.43617021276595747,\n      3: 0.25903614457831325,\n      1: 0.6173913043478261},\n     'embarked': {'S': 0.3389570552147239,\n      'C': 0.553072625698324,\n      'Q': 0.37349397590361444,\n      'Missing': 1.0}}\n```\n\n----------------------------------------\n\nTITLE: Sample Predictions\nDESCRIPTION: This snippet displays the first 10 predictions from the logistic regression model after preprocessing with the Feature-engine pipeline.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\narray([1, 1, 1, 0, 1, 0, 1, 1, 0, 1], dtype=int64)\n```\n\n----------------------------------------\n\nTITLE: Handling Errors when Variable is neither Numerical or Categorical\nDESCRIPTION: This snippet demonstrates the error handling of the find_categorical_and_numerical_variables function. If the input contains variables that are neither numerical nor categorical, a TypeError will be raised. This example includes a 'date1' column which causes the error.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_categorical_and_numerical_variables.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfind_categorical_and_numerical_variables(X, [\"num_var_1\", \"cat_var1\", \"date1\"])\n```\n\n----------------------------------------\n\nTITLE: Finding Datetime Variables with Feature Engine - Python\nDESCRIPTION: This snippet demonstrates how to use `find_datetime_variables` from the `feature_engine.variable_handling` module to identify datetime variables within a Pandas DataFrame. It imports the function, calls it with the DataFrame `X` created in the previous step, and then prints the resulting list of datetime variable names. The function automatically detects both datetime objects and strings that can be parsed as datetimes.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/find_datetime_variables.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom feature_engine.variable_handling import find_datetime_variables\n\nvar_date = find_datetime_variables(X)\n\nvar_date\n```\n\n----------------------------------------\n\nTITLE: Pandas Series to Lag Features in Python\nDESCRIPTION: This snippet demonstrates how to handle pandas Series data with LagFeatures. It includes the conversion of a pandas Series to a Dataframe with `to_frame()` and then using `LagFeatures` to generate new lagged features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/LagFeatures.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nX['ambient_temp']\n```\n\n----------------------------------------\n\nTITLE: Access Max Values in CyclicalFeatures\nDESCRIPTION: This code accesses the `max_values_` attribute of the fitted CyclicalFeatures transformer.  This attribute contains a dictionary mapping the name of each variable to its maximum value, which was used for scaling during the cyclical encoding process.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncyclical.max_values_\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Importances with Error Bars - Python\nDESCRIPTION: This snippet plots the feature importances derived from the individual feature models, including error bars representing the standard deviation of the importances. It concatenates the mean and standard deviation of the importances, sorts by the mean, and creates a bar plot with error bars.  It also sets the title and y label of the chart before showing it.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nr = pd.concat([\n    sel.feature_importances_,\n    sel.feature_importances_std_\n], axis=1)\n\nr.columns = [\"mean\", \"std\"]\n\nr.sort_values(\"mean\", ascending=False)[\"mean\"].plot.bar(\n    yerr=[r['std'], r['std']], subplots=True, figsize=(15,6)\n)\nplt.title(\"Feature importance derived from single feature models\")\nplt.ylabel(\"Feature importance - roc-auc\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Boxplot Visualization for 'sibsp' in Python\nDESCRIPTION: This snippet generates a boxplot specifically for the 'sibsp' column of the training dataset. It uses matplotlib for plotting and includes axis labels and a title.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nX_train.boxplot(column=['sibsp'])\nplt.title(\"Box plot - outliers\")\nplt.ylabel(\"variable values\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing DecisionTreeEncoder with Precision\nDESCRIPTION: This code initializes the DecisionTreeEncoder with the `precision` parameter set to 2, which rounds the encoded values to two decimal places. Other parameters are the same as before.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/DecisionTreeEncoder.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nencoder = DecisionTreeEncoder(\n    variables=['cabin', 'pclass', 'embarked'],\n    regression=False,\n    scoring='roc_auc',\n    cv=3,\n    random_state=0,\n    ignore_format=True,\n    precision=2,\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Transformed Columns (Categorical, Single Value)\nDESCRIPTION: This snippet shows the columns remaining in the transformed DataFrame after dropping features with high PSI when splitting data using a single categorical value.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nX_transformed.columns\n\nIndex(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'group'], dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Rare Label Encoding Result (value_counts)\nDESCRIPTION: This is the expected output after executing the first code snippet that creates and shows the value counts for the column 'var_A'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nOut[1]:\nA    10\nB    10\nC     2\nD     1\nName: var_A, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Check_all_variables output when variables are not present\nDESCRIPTION: This snippet shows the error message produced by the Keyerror when some variables are not in the dataframe.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_all_variables.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nKeyError: 'Some of the variables are not in the dataframe.'\n```\n\n----------------------------------------\n\nTITLE: StringSimilarityEncoder DataFrame Example\nDESCRIPTION: This example demonstrates how to create a Pandas DataFrame and then encode a string variable using the StringSimilarityEncoder. The encoder transforms the 'words' column into multiple columns representing the string similarity to each unique value in the original 'words' column.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/StringSimilarityEncoder.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom feature_engine.encoding import StringSimilarityEncoder\n\ndf = pd.DataFrame({\"words\": [\"dog\", \"dig\", \"cat\"]})\ndf\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Splitting into Train/Test Sets with pandas and sklearn\nDESCRIPTION: This code snippet demonstrates how to load a CSV file into a pandas DataFrame, and then split the data into training and testing sets using scikit-learn's train_test_split function.  It imports necessary libraries like pandas, numpy and sklearn.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/EndTailImputer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\timport numpy as np\n\timport pandas as pd\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.model_selection import train_test_split\n\n\tfrom feature_engine.imputation import EndTailImputer\n\n\t# Load dataset\n\tdata = pd.read_csv('houseprice.csv')\n\n\t# Separate into train and test sets\n\tX_train, X_test, y_train, y_test = train_test_split(\n                                            data.drop(['Id', 'SalePrice'], axis=1),\n                                            data['SalePrice'],\n                                            test_size=0.3,\n                                            random_state=0,\n                                            )\n```\n\n----------------------------------------\n\nTITLE: Datetime Subtraction Handling Missing Values\nDESCRIPTION: This code snippet demonstrates how to handle missing values (NaN) during datetime subtraction using Feature-engine's `DatetimeSubtraction`. The `missing_values` parameter is set to \"ignore\" to allow computations even with missing values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/datetime/DatetimeSubtraction.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nfrom feature_engine.datetime import DatetimeSubtraction\n\ndata = pd.DataFrame({\n    \"date1\" : [\"2022-09-01\", \"2022-10-01\", \"2022-12-01\"],\n    \"date2\" : [\"2022-09-15\", np.nan, \"2022-12-15\"],\n    \"date3\" : [\"2022-08-01\", \"2022-09-01\", \"2022-11-01\"],\n    \"date4\" : [\"2022-08-15\", \"2022-09-15\", np.nan],\n})\n\ndtf = DatetimeSubtraction(\n    variables=[\"date1\", \"date2\"],\n    reference=[\"date3\", \"date4\"],\n    missing_values=\"ignore\")\n\ndata = dtf.fit_transform(data)\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Accessing Transformed Columns (Categorical, List)\nDESCRIPTION: This snippet shows the columns remaining in the transformed DataFrame after dropping features with high PSI when splitting data using a list of categorical values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nX_transformed.columns\n\nIndex(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'group'], dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Display Feature Importances Tail\nDESCRIPTION: This code displays the last few feature importances, including the probe feature, to show the importance assigned to the generated random variable and how it compares to the other features. Uses `tail()` method.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsel.feature_importances_.tail()\n```\n\n----------------------------------------\n\nTITLE: Displaying the Resulting Dataframe - Python\nDESCRIPTION: This snippet shows the head of the training dataframe after loading and splitting. It is the output of the previous snippet and demonstrates the structure and content of the dataframe used in the subsequent steps.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/Winsorizer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npclass     sex        age  sibsp  parch     fare    cabin embarked\n501        2  female  13.000000      0      1  19.5000  Missing        S\n588        2  female   4.000000      1      1  23.0000  Missing        S\n402        2  female  30.000000      1      0  13.8583  Missing        C\n1193       3    male  29.881135      0      0   7.7250  Missing        Q\n686        3  female  22.000000      0      0   7.7250  Missing        Q\n```\n\n----------------------------------------\n\nTITLE: Display Probe Features\nDESCRIPTION: This code displays the first few rows of the generated probe features using the `head()` method. This allows for a quick inspection of the generated random variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsel.probe_features_.head()\n```\n\n----------------------------------------\n\nTITLE: Plotting Non-Shifted Variable Distribution (var_4)\nDESCRIPTION: This code snippet uses Seaborn's `ecdfplot` to visualize the cumulative distribution function (CDF) of the 'var_4' feature, grouped by the 'group' column. It serves as a comparison to the shifted variable ('income'), illustrating how a variable with a low PSI value (and therefore not targeted for removal) has a similar distribution across different groups. Requires Seaborn and Matplotlib.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nsns.ecdfplot(data=X, x=\"var_4\", hue=\"group\")\n```\n\n----------------------------------------\n\nTITLE: Displaying unique 'embarked' categories from training set in Python\nDESCRIPTION: This code snippet outputs the unique values found in the 'embarked' column of the training dataset before any transformation is applied.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\narray(['S', 'C', 'Missing', 'Q'], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Outlier Trimming with Gaussian (Z-score)\nDESCRIPTION: This snippet demonstrates how to use the OutlierTrimmer class with the Gaussian (Z-score) capping method to remove outliers from the 'age' variable. It caps both tails of the distribution and includes fitting the transformer to the training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\not_age = OutlierTrimmer(capping_method='gaussian',\n                        tail=\"both\",\n                        fold=3,\n                        variables=['age'],\n                        )\n\n\n    ot_age.fit(X_train)\n```\n\n----------------------------------------\n\nTITLE: Displaying Probe Features - Python\nDESCRIPTION: This snippet displays the first few rows of the probe features generated by the ProbeFeatureSelection transformer, allowing inspection of the generated random data used for comparison during feature selection.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nsel.probe_features_.head()\n```\n\n----------------------------------------\n\nTITLE: venv Environment Creation\nDESCRIPTION: This snippet demonstrates how to create a virtual environment using venv.  It isolates project dependencies.  The path specifies where the new environment will be created. Example given.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython -m venv /path/to/new/virtual/environment\n```\n\nLANGUAGE: shell\nCODE:\n```\npython -m venv Documents/Repositories/envs/featureengine\n```\n\n----------------------------------------\n\nTITLE: Loading Data with Pandas and Scikit-learn - Python\nDESCRIPTION: This snippet demonstrates how to load the diabetes dataset from scikit-learn into pandas DataFrames. It imports necessary libraries like pandas, matplotlib, and sklearn.datasets. The diabetes dataset is loaded, and the head of the feature DataFrame (X) is printed to the console.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectBySingleFeaturePerformance.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom feature_engine.selection import SelectBySingleFeaturePerformance\n\nX, y = load_diabetes(return_X_y=True, as_frame=True)\nprint(X.head())\n```\n\n----------------------------------------\n\nTITLE: Run flake8 code style checker\nDESCRIPTION: Runs the flake8 code style checker on a specified Python script to ensure compliance with coding conventions.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\n$ flake8 my_new_script.py\n```\n\n----------------------------------------\n\nTITLE: Displaying the Head of Transformed Training Data in Python\nDESCRIPTION: This prints the first 5 rows of the transformed training data.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OneHotEncoder.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(train_t.head())\n```\n\n----------------------------------------\n\nTITLE: Inspect Right Tail Caps (Gaussian)\nDESCRIPTION: This snippet inspects the upper limit for the 'age' feature beyond which values are considered outliers when using the Gaussian (Z-score) capping method.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\not_age.right_tail_caps_\n```\n\n----------------------------------------\n\nTITLE: Displaying unique 'cabin' categories from training set in Python\nDESCRIPTION: This code snippet outputs the unique values found in the 'cabin' column of the training dataset before any transformation is applied.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\narray(['B', 'C', 'E', 'D', 'A', 'M', 'T', 'F'], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Initializing LogCpTransformer with different constants for variables\nDESCRIPTION: This snippet demonstrates how to initialize the LogCpTransformer to add different constants to specific variables. The `C` parameter is set to a dictionary where keys are variable names and values are constants.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/LogCpTransformer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntf = LogCpTransformer(C={\"bmi\": 2, \"s3\": 3, \"s4\": 4})\ntf.fit(X_train)\n\ntf.variables_\n\ntf.C_\n```\n\n----------------------------------------\n\nTITLE: Accessing Correlated Feature Dictionary - Python\nDESCRIPTION: This code snippet accesses the `correlated_feature_dict_` attribute of the fitted DropCorrelatedFeatures transformer. This attribute stores a dictionary where keys are the retained features and values are sets of features correlated to the respective key feature. The features in the values will be removed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropCorrelatedFeatures.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntr.correlated_feature_dict_\n```\n\n----------------------------------------\n\nTITLE: Displaying unique transformed 'cabin' categories from test set in Python\nDESCRIPTION: This code snippet displays the unique values from the 'cabin' column in the test dataset after the `MatchCategories` transformation. Note that the unseen category 'G' becomes NaN.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchCategories.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n['M', 'F', 'E', NaN]\nCategories (8, object): ['A', 'B', 'C', 'D', 'E', 'F', 'M', 'T']\n```\n\n----------------------------------------\n\nTITLE: Sine Transformation Plot with Horizontal Line\nDESCRIPTION: This code creates a scatter plot of the 'hour' variable against its sine transformation ('hour_sin'). It adds axis labels, a title, a horizontal dashed line at y=0, and vertical dashed lines at hours 0 and 11.5 to highlight the symmetry issue where different observations can return similar values.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nplt.scatter(df[\"hour\"], df[\"hour_sin\"])\n\n# Axis labels\nplt.ylabel('Sine of hour')\nplt.xlabel('Hour')\nplt.title('Sine transformation')\n\nplt.hlines(y=0, xmin=0, xmax=11.5, color='r', linestyles='dashed')\n\nplt.vlines(x=0, ymin=-1, ymax=0, color='g', linestyles='dashed')\nplt.vlines(x=11.5, ymin=-1, ymax=0, color='g', linestyles='dashed')\n```\n\n----------------------------------------\n\nTITLE: Adding Extra Columns to Pandas DataFrame in Python\nDESCRIPTION: This code adds two new columns ('var_a' and 'var_b') to the `test_t` DataFrame and initializes all their values to 0. This demonstrates how extra, unrelated columns might be present in a test dataset.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# let's add some columns for the demo\ntest_t[['var_a', 'var_b']] = 0\n\ntest_t.head()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame for Transformation\nDESCRIPTION: This code initializes a pandas DataFrame with sample data for demonstrating the MathFeatures transformer. It imports necessary libraries like numpy and pandas, and then creates a DataFrame with columns 'Name', 'City', 'Age', 'Marks', and 'dob'.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/MathFeatures.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n    import pandas as pd\n    from feature_engine.creation import MathFeatures\n\n    df = pd.DataFrame.from_dict(\n        {\n            \"Name\": [\"tom\", \"nick\", \"krish\", \"jack\"],\n            \"City\": [\"London\", \"Manchester\", \"Liverpool\", \"Bristol\"],\n            \"Age\": [20, 21, 19, 18],\n            \"Marks\": [0.9, 0.8, 0.7, 0.6],\n            \"dob\": pd.date_range(\"2020-02-24\", periods=4, freq=\"T\"),\n        })\n```\n\n----------------------------------------\n\nTITLE: Importing and Using RandomSampleImputer with Pandas and Scikit-learn\nDESCRIPTION: This code snippet shows how to import necessary libraries such as numpy, pandas, matplotlib, and sklearn's train_test_split, and then demonstrates loading data, splitting it into training and testing sets, and utilizing the RandomSampleImputer from Feature Engine to impute missing values. The goal is to showcase a basic imputation workflow using the imputer.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/RandomSampleImputer.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\timport pandas as pd\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.model_selection import train_test_split\n\n\tfrom feature_engine.imputation import RandomSampleImputer\n\n\t# Load dataset\n\tdata = pd.read_csv('houseprice.csv')\n\n\t# Separate into train and test sets\n\tX_train, X_test, y_train, y_test = train_test_split(\n            data.drop(['Id', 'SalePrice'], axis=1),\n            data['SalePrice'],\n            test_size=0.3,\n            random_state=0\n        )\n```\n\n----------------------------------------\n\nTITLE: Pull and rebase upstream main branch\nDESCRIPTION: Pulls changes from the upstream 'main' branch and rebases the local 'main' branch onto it.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\n$ git pull --rebase upstream main\n```\n\n----------------------------------------\n\nTITLE: Data Loading and Splitting - Python\nDESCRIPTION: This snippet loads the House Prices dataset, separates it into training and testing sets, and imports necessary libraries. It uses pandas for data manipulation, scikit-learn for splitting data, and matplotlib for visualization. This serves as preparation for the subsequent imputation steps.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/ArbitraryNumberImputer.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom feature_engine.imputation import ArbitraryNumberImputer\n\n# Load dataset\ndata = pd.read_csv('houseprice.csv')\n\n# Separate into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n                                data.drop(['Id', 'SalePrice'], axis=1),\n                                data['SalePrice'],\n                                test_size=0.3,\n                                random_state=0,\n                                )\n```\n\n----------------------------------------\n\nTITLE: Impact of missing_only=True\nDESCRIPTION: This code snippet demonstrates the impact of setting the `missing_only` parameter to `True` when a list of variables is passed to `DropMissingData`. When `missing_only=True`, only variables that had missing values during the fit stage will be used to remove rows with NA values during the transform stage. If a variable in the list did not have missing values during fit, it will be ignored during transform.  This snippet highlights the importance of setting `missing_only` to `False` when specifying variables.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/DropMissingData.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndmd = DropMissingData(variables=[\"x1\", \"x3\"], missing_only=True)\nXt = dmd.fit_transform(X)\ndmd.variables_\n```\n\n----------------------------------------\n\nTITLE: Adding Specific Files to Git\nDESCRIPTION: This snippet demonstrates how to add specific files to the staging area in Git before committing. It uses `git add file1.py file2.py` to select certain files for the commit, excluding any other changes.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n$ git add file1.py file2.py\n```\n\n----------------------------------------\n\nTITLE: Accessing Encoder Dictionary\nDESCRIPTION: This code accesses and displays the `encoder_dict_` attribute of the fitted RareLabelEncoder.  This dictionary contains the frequent categories that will not be grouped for each specified variable.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/RareLabelEncoder.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nencoder.encoder_dict_\n```\n\n----------------------------------------\n\nTITLE: Format code using black\nDESCRIPTION: Formats the code in a specified Python script using black, automatically fixing code style issues.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\n$ black my_new_script.py\n```\n\n----------------------------------------\n\nTITLE: Plot Mean Target Value per Encoded Category (Python)\nDESCRIPTION: This code snippet generates a scatter plot visualizing the relationship between encoded categorical values and the mean target value. It expects a pandas DataFrame called `mean_target_per_encoded_category` with columns 'HouseAgeCategorical' (encoded category) and 'MedHouseVal' (mean target value). It uses matplotlib to create the plot, setting the title, axis labels, and displaying the plot. The 'HouseAgeCategorical' column is explicitly converted to string type before plotting to ensure correct plotting behavior.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/encoding/OrdinalEncoder.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmean_target_per_encoded_category['HouseAgeCategorical'] = mean_target_per_encoded_category['HouseAgeCategorical'].astype(str)\nplt.scatter(mean_target_per_encoded_category['HouseAgeCategorical'], mean_target_per_encoded_category['MedHouseVal'])\nplt.title('Mean target value per category')\nplt.xlabel('Encoded category')\nplt.ylabel('Mean target value')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Accessing features to drop - RecursiveFeatureElimination - Python\nDESCRIPTION: This snippet shows how to access the features that will be dropped by the RecursiveFeatureElimination transformer after fitting. The features_to_drop_ attribute stores the list of feature names identified for removal based on the specified threshold.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/RecursiveFeatureElimination.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# the features to remove\ntr.features_to_drop_\n```\n\n----------------------------------------\n\nTITLE: Displaying Feature Names\nDESCRIPTION: This code snippet shows the output of the `get_feature_names_out()` method, which is a list of feature names. The list represents the features present in the transformed data after feature selection.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/SelectByTargetMeanPerformance.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n['pclass', 'sex', 'fare', 'cabin']\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Imputation Pipeline\nDESCRIPTION: Transforms the subset of the test data (`X_test_subset`) using the fitted imputation pipeline. This results in a dataset where missing values are imputed, and additional indicator columns are created to flag the original missing values. The transformed data is stored in `X_test_subset_t`.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/imputation/MeanMedianImputer.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\tX_test_subset_t = imputer.transform(X_test_subset)\n```\n\n----------------------------------------\n\nTITLE: Loading data and splitting into train and test sets with pandas, sklearn.\nDESCRIPTION: This code snippet demonstrates how to load the Ames House Prices dataset using sklearn's fetch_openml function, set the index to 'Id', and then split the data into training and testing sets using train_test_split from sklearn.model_selection. It imports necessary libraries like numpy, pandas, seaborn, matplotlib, and specific modules from sklearn and feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/transformation/PowerTransformer.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\timport numpy as np\n\timport pandas as pd\n\timport seaborn as sns\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.preprocessing import scale\n\tfrom sklearn.datasets import fetch_openml\n\tfrom sklearn.model_selection import train_test_split\n\n\tfrom feature_engine.transformation import PowerTransformer\n\n\t# Load dataset\n\tX, y = fetch_openml(name='house_prices', version=1, return_X_y=True, as_frame=True)\n\tX.set_index('Id', inplace=True)\n\n\t# Separate into train and test sets\n\tX_train, X_test, y_train, y_test =  train_test_split(\n\t\tX, y, test_size=0.3, random_state=42\n\t)\n```\n\n----------------------------------------\n\nTITLE: Run all pytest tests\nDESCRIPTION: Runs all pytest test scripts within the 'test' folder. This command is executed from the feature-engine folder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest\n```\n\n----------------------------------------\n\nTITLE: Pandas Series Output Example\nDESCRIPTION: This is not a snippet of Python code that executes, but an example of what the output looks like if accessing a Pandas series by its name.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nX['ambient_temp']\n```\n\nLANGUAGE: python\nCODE:\n```\n2020-05-15 12:00:00    31.31\n2020-05-15 12:15:00    31.51\n2020-05-15 12:30:00    32.15\n2020-05-15 12:45:00    32.39\n2020-05-15 13:00:00    32.62\n2020-05-15 13:15:00    32.50\n2020-05-15 13:30:00    32.52\n2020-05-15 13:45:00    32.68\nFreq: 15T, Name: ambient_temp, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Target Variable Creation for Forecasting (Python)\nDESCRIPTION: This code creates the target variable `y` for direct forecasting. It shifts the 'demand' column by `horizon` (3 hours) to create future demand values, effectively preparing the dataset for predicting multiple time steps ahead.  It then removes rows with NaN values introduced by the shifting and trims the original dataframe.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/make_pipeline.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 3\ny = pd.DataFrame(index=df.index)\nfor h in range(horizon):\n    y[f\"h_{h}\"] = df.shift(periods=-h, freq=\"h\")\ny.dropna(inplace=True)\ndf = df.loc[y.index]\nprint(y.head())\n```\n\n----------------------------------------\n\nTITLE: Accessing Lasso Coefficients\nDESCRIPTION: This code shows how to access the coefficients of the Lasso regression model within the pipeline. It accesses the named_steps attribute and then the coef_ attribute of the Lasso model.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe.named_steps[\"lasso\"].coef_\n```\n\nLANGUAGE: python\nCODE:\n```\narray([-0.,  0.])\n```\n\n----------------------------------------\n\nTITLE: Install tox testing tool\nDESCRIPTION: Installs the tox testing tool using pip. Tox is used to automate the testing process in different Python environments.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install tox\n```\n\n----------------------------------------\n\nTITLE: Adding Upstream Remote to Git\nDESCRIPTION: This snippet illustrates how to add an 'upstream' remote to a local Git repository. The upstream remote points to the main Feature-engine repository, enabling the developer to pull the latest changes from the main repository into their fork.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ git remote add upstream https://github.com/feature-engine/feature_engine.git\n```\n\n----------------------------------------\n\nTITLE: Update Main Branch with Upstream\nDESCRIPTION: This snippet updates the local main branch with the latest changes from the upstream repository. It uses `git pull --rebase` to ensure a clean history.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n$ git pull --rebase upstream main\n```\n\n----------------------------------------\n\nTITLE: Navigate to Feature-engine module\nDESCRIPTION: Navigates to the main Feature-engine directory in the terminal.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\n$ cd feature_engine\n```\n\n----------------------------------------\n\nTITLE: Import libraries for ProbeFeatureSelection\nDESCRIPTION: This code snippet imports necessary libraries including matplotlib for plotting, pandas for data manipulation, scikit-learn for datasets, ensemble methods, and model selection, and feature_engine's ProbeFeatureSelection class.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/ProbeFeatureSelection.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.selection import ProbeFeatureSelection\n```\n\n----------------------------------------\n\nTITLE: Accessing PSI Values - Python\nDESCRIPTION: This snippet shows how to access the PSI values calculated by `DropHighPSIFeatures` after fitting the transformer to the data. The `psi_values_` attribute contains a dictionary of features and their corresponding PSI values. Dependencies include feature_engine.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropHighPSIFeatures.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransformer.psi_values_\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Sphinx\nDESCRIPTION: This snippet demonstrates how to build the Feature-engine documentation using Sphinx. It assumes you are in the root directory of the Feature-engine project.  It instructs Sphinx to read the source files from the `docs` folder and output the generated HTML files to the `build` folder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_docs.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ sphinx-build -b html docs build\n```\n\n----------------------------------------\n\nTITLE: Push Branch to Remote Repository\nDESCRIPTION: This snippet demonstrates how to push a local branch to a remote repository using `git push origin myfeaturebranch`. It uploads the branch and its commits to the origin repository, typically a fork.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n$ git push origin myfeaturebranch\n```\n\n----------------------------------------\n\nTITLE: Fit and Transform with CyclicalFeatures\nDESCRIPTION: This code fits the CyclicalFeatures transformer to the DataFrame `df` and then transforms it. The `fit_transform` method learns the maximum value of each variable, calculates the sine and cosine components, and returns a new DataFrame with the encoded features appended to the original ones.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/creation/CyclicalFeatures.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX = cyclical.fit_transform(df)\n```\n\n----------------------------------------\n\nTITLE: Installing Feature-engine with underscore\nDESCRIPTION: Installs the Feature-engine package using pip with an underscore. This is an alternative way to install the package.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/quickstart/index.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install feature_engine\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository via Git\nDESCRIPTION: This snippet demonstrates how to clone a forked repository to a local machine using git. It initializes a local copy of the repository, allowing developers to work on their contributions in a local environment.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ git clone https://github.com/<YOURUSERNAME>/feature_engine\n```\n\n----------------------------------------\n\nTITLE: Displaying Transformed Columns\nDESCRIPTION: This snippet prints the columns of the transformed training data (train_t), showing the features remaining after the duplicate features have been removed.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/selection/DropDuplicateFeatures.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_t.columns\n```\n\nLANGUAGE: python\nCODE:\n```\nIndex(['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked'], dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Install Feature-engine with conda\nDESCRIPTION: This snippet demonstrates installing Feature-engine using conda from the conda-forge channel. This is an alternative to using pip for users who prefer conda package management.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/index.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ conda install -c conda-forge feature_engine\n```\n\n----------------------------------------\n\nTITLE: Displaying Pandas Series Data Type\nDESCRIPTION: This code snippet outputs the data type of the 'sex' column. The expected output 'O' indicates that the 'sex' column in the original training data is of object type (typically strings or mixed data).\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/preprocessing/MatchVariables.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndtype('O')\n```\n\n----------------------------------------\n\nTITLE: Predicting Probabilities\nDESCRIPTION: This snippet shows how to predict probabilities using the trained Feature-engine Pipeline with a logistic regression model. It demonstrates predicting probabilities on the training data and displaying the first 10 probability predictions.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/outliers/OutlierTrimmer.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\npreds = pipe.predict_proba(X_train)\n\n    preds[0:10]\n```\n\n----------------------------------------\n\nTITLE: Setting Lasso Alpha Parameter\nDESCRIPTION: This code demonstrates how to set the alpha parameter of the Lasso regression model within the pipeline using the set_params() method. It sets the alpha value to 10.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/pipeline/Pipeline.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipe.set_params(lasso__alpha=10)\n```\n\n----------------------------------------\n\nTITLE: Using check_all_variables returns KeyError\nDESCRIPTION: This code snippet demonstrates the behavior of the `check_all_variables` function when a variable in the provided list is not found in the DataFrame 'X'. It calls the function with a list containing 'hola', which is not a column in X, and expects a KeyError.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/user_guide/variable_handling/check_all_variables.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncheck_all_variables(X, [\"hola\", \"cat_var1\", \"date1\"])\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Requirements using pip\nDESCRIPTION: This snippet shows how to install the necessary packages for building the Feature-engine documentation using pip.  It assumes you are in the root directory of the Feature-engine project and that you have already activated your virtual environment. It installs the dependencies listed in the `docs/requirements.txt` file.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_docs.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Run specific pytest test script\nDESCRIPTION: Runs a specific pytest test script, allowing for focused testing of individual components or features.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest tests/test_encoding/test_onehot_encoder.py\n```\n\n----------------------------------------\n\nTITLE: Run mypy type checker\nDESCRIPTION: Runs the mypy type checker on the entire feature_engine module to check for type hinting errors. Only runs on the code base, not tests.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\n$ mypy feature_engine\n```\n\n----------------------------------------\n\nTITLE: Upgrade Feature-engine with pip\nDESCRIPTION: This snippet shows how to upgrade Feature-engine to the latest version using pip. The -U flag ensures that pip upgrades the package to the newest available release.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/index.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U feature-engine\n```\n\n----------------------------------------\n\nTITLE: Install Documentation Dependencies\nDESCRIPTION: This snippet installs the dependencies required for building the documentation for the Feature-engine package. It uses pip and the `docs/requirements.txt` file.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Run tests using tox\nDESCRIPTION: Runs all tests (functionality, code style, typehint, and documentation) using tox. Runs for all Python versions configured in tox.ini.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/contribute/contribute_code.rst#_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\n$ tox\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Repository Folder\nDESCRIPTION: This command shows how to change the current directory to the feature-engine repository folder.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncd feature_engine\n```\n\n----------------------------------------\n\nTITLE: BibTeX entry for Feature-engine\nDESCRIPTION: This BibTeX entry can be used to cite Feature-engine in scientific publications. It includes the DOI, URL, year, publisher, volume, number, pages, author, title, and journal information for the software.\nSOURCE: https://github.com/feature-engine/feature_engine/blob/main/docs/about/about.rst#_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{Galli2021,\n    doi = {10.21105/joss.03642},\n    url = {https://doi.org/10.21105/joss.03642},\n    year = {2021},\n    publisher = {The Open Journal},\n    volume = {6},\n    number = {65},\n    pages = {3642},\n    author = {Soledad Galli},\n    title = {Feature-engine: A Python package for feature engineering for machine learning},\n    journal = {Journal of Open Source Software}\n    }\n```"
  }
]
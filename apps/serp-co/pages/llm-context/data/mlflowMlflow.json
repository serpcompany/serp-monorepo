[
  {
    "owner": "mlflow",
    "repo": "mlflow",
    "content": "TITLE: Implementing Custom ChatModel Subclass in Python for MLflow\nDESCRIPTION: This code snippet defines a BasicAgent class that subclasses MLflow's ChatModel. It implements methods for agent initialization, context loading, message handling, and prediction. The class showcases MLflow's tracing functionality, custom configuration management, and conversation flow between multiple agent roles.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.types.llm import ChatCompletionResponse, ChatMessage, ChatParams, ChatChoice\nfrom mlflow.pyfunc import ChatModel\nfrom mlflow import deployments\nfrom typing import List, Optional, Dict\n\n\nclass BasicAgent(ChatModel):\n    def __init__(self):\n        \"\"\"Initialize the BasicAgent with placeholder values.\"\"\"\n        self.deploy_client = None\n        self.models = {}\n        self.models_config = {}\n        self.conversation_history = []\n\n    def load_context(self, context):\n        \"\"\"Initialize the connectors and model configurations.\"\"\"\n        self.deploy_client = deployments.get_deploy_client(\"databricks\")\n        self.models = context.model_config.get(\"models\", {})\n        self.models_config = context.model_config.get(\"configuration\", {})\n\n    def _get_system_message(self, role: str) -> Dict:\n        \"\"\"\n        Get the system message configuration for the specified role.\n\n        Args:\n            role (str): The role of the agent (e.g., \"oracle\" or \"judge\").\n\n        Returns:\n            dict: The system message for the given role.\n        \"\"\"\n        if role not in self.models:\n            raise ValueError(f\"Unknown role: {role}\")\n\n        instruction = self.models[role][\"instruction\"]\n        return ChatMessage(role=\"system\", content=instruction).to_dict()\n\n    @mlflow.trace(name=\"Raw Agent Response\")\n    def _get_agent_response(\n        self, message_list: List[Dict], endpoint: str, params: Optional[dict] = None\n    ) -> Dict:\n        \"\"\"\n        Call the agent endpoint to get a response.\n\n        Args:\n            message_list (List[Dict]): List of messages for the agent.\n            endpoint (str): The agent's endpoint.\n            params (Optional[dict]): Additional parameters for the call.\n\n        Returns:\n            dict: The response from the agent.\n        \"\"\"\n        response = self.deploy_client.predict(\n            endpoint=endpoint, inputs={\"messages\": message_list, **(params or {})}\n        )\n        return response[\"choices\"][0][\"message\"]\n\n    @mlflow.trace(name=\"Agent Call\")\n    def _call_agent(\n        self, message: ChatMessage, role: str, params: Optional[dict] = None\n    ) -> Dict:\n        \"\"\"\n        Prepares and sends the request to a specific agent based on the role.\n\n        Args:\n            message (ChatMessage): The message to be processed.\n            role (str): The role of the agent (e.g., \"oracle\" or \"judge\").\n            params (Optional[dict]): Additional parameters for the call.\n\n        Returns:\n            dict: The response from the agent.\n        \"\"\"\n        system_message = self._get_system_message(role)\n        message_list = self._prepare_message_list(system_message, message)\n\n        # Fetch agent response\n        agent_config = self.models[role]\n        response = self._get_agent_response(\n            message_list, agent_config[\"endpoint\"], params\n        )\n\n        # Update conversation history\n        self.conversation_history.extend([message.to_dict(), response])\n        return response\n\n    @mlflow.trace(name=\"Assemble Conversation\")\n    def _prepare_message_list(\n        self, system_message: Dict, user_message: ChatMessage\n    ) -> List[Dict]:\n        \"\"\"\n        Prepare the list of messages to send to the agent.\n\n        Args:\n            system_message (dict): The system message dictionary.\n            user_message (ChatMessage): The user message.\n\n        Returns:\n            List[dict]: The complete list of messages to send.\n        \"\"\"\n        user_prompt = {\n            \"role\": \"user\",\n            \"content\": self.models_config.get(\n                \"user_response_instruction\", \"Can you make the answer better?\"\n            ),\n        }\n        if self.conversation_history:\n            return [system_message, *self.conversation_history, user_prompt]\n        else:\n            return [system_message, user_message.to_dict()]\n\n    def predict(\n        self, context, messages: List[ChatMessage], params: Optional[ChatParams] = None\n    ) -> ChatCompletionResponse:\n        \"\"\"\n        Predict method to handle agent conversation.\n\n        Args:\n            context: The MLflow context.\n            messages (List[ChatMessage]): List of messages to process.\n            params (Optional[ChatParams]): Additional parameters for the conversation.\n\n        Returns:\n            ChatCompletionResponse: The structured response object.\n        \"\"\"\n        # Use the fluent API context handler to have added control over what is included in the span\n        with mlflow.start_span(name=\"Audit Agent\") as root_span:\n            # Add the user input to the root span\n            root_span.set_inputs(messages)\n\n            # Add attributes to the root span\n            attributes = {**params.to_dict(), **self.models_config, **self.models}\n            root_span.set_attributes(attributes)\n\n            # Initiate the conversation with the oracle\n            oracle_params = self._get_model_params(\"oracle\")\n            oracle_response = self._call_agent(messages[0], \"oracle\", oracle_params)\n\n            # Process the response with the judge\n            judge_params = self._get_model_params(\"judge\")\n            judge_response = self._call_agent(\n                ChatMessage(**oracle_response), \"judge\", judge_params\n            )\n\n            # Reset the conversation history and return the final response\n            self.conversation_history = []\n\n            output = ChatCompletionResponse(\n                choices=[ChatChoice(index=0, message=ChatMessage(**judge_response))],\n                usage={},\n                model=judge_params.get(\"endpoint\", \"unknown\"),\n            )\n\n            root_span.set_outputs(output)\n\n        return output\n\n    def _get_model_params(self, role: str) -> dict:\n        \"\"\"\n        Retrieves model parameters for a given role.\n\n        Args:\n            role (str): The role of the agent (e.g., \"oracle\" or \"judge\").\n\n        Returns:\n            dict: A dictionary of parameters for the agent.\n        \"\"\"\n        role_config = self.models.get(role, {})\n\n        return {\n            \"temperature\": role_config.get(\"temperature\", 0.5),\n            \"max_tokens\": role_config.get(\"max_tokens\", 500),\n        }\n\n\n# IMPORTANT: specifies the Python ChatModel instance to use for inference requests when\n```\n\n----------------------------------------\n\nTITLE: Using Advanced LLM Task for Chat Inference in Python\nDESCRIPTION: This comprehensive example demonstrates logging a Transformers pipeline with the llm/v1/chat task, examining the model metadata, and using the model for chat-style inference with an OpenAI-compatible API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/task/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport transformers\n\npipeline = transformers.pipeline(\"text-generation\", \"gpt2\")\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        artifact_path=\"model\",\n        task=\"llm/v1/chat\",\n        input_example={\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"You are a bot.\"},\n                {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n            ]\n        },\n        save_pretrained=False,\n    )\n\n# Model metadata logs additional field \"inference_task\"\nprint(model_info.flavors[\"transformers\"][\"inference_task\"])\n# >> llm/v1/chat\n\n# The original native task type is also saved\nprint(model_info.flavors[\"transformers\"][\"task\"])\n# >> text-generation\n\n# Model signature is set to the chat API spec\nprint(model_info.signature)\n# >> inputs:\n# >>   ['messages': Array({content: string (required), name: string (optional), role: string (required)}) (required), 'temperature': double (optional), 'max_tokens': long (optional), 'stop': Array(string) (optional), 'n': long (optional), 'stream': boolean (optional)]\n# >> outputs:\n# >>   ['id': string (required), 'object': string (required), 'created': long (required), 'model': string (required), 'choices': Array({finish_reason: string (required), index: long (required), message: {content: string (required), name: string (optional), role: string (required)} (required)}) (required), 'usage': {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (required)]\n# >> params:\n# >>     None\n\n# The model can be served with the OpenAI-compatible inference API\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nprediction = pyfunc_model.predict(\n    {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a bot.\"},\n            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n        ],\n        \"temperature\": 0.5,\n        \"max_tokens\": 200,\n    }\n)\nprint(prediction)\n# >> [{'choices': [{'finish_reason': 'stop',\n# >>               'index': 0,\n# >>               'message': {'content': 'I'm doing well, thank you for asking.', 'role': 'assistant'}},\n# >>   'created': 1719875820,\n# >>   'id': '355c4e9e-040b-46b0-bf22-00e93486100c',\n# >>   'model': 'gpt2',\n# >>   'object': 'chat.completion',\n# >>   'usage': {'completion_tokens': 7, 'prompt_tokens': 13, 'total_tokens': 20}}]\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto-logging in MLflow\nDESCRIPTION: Shows how to use MLflow's auto-logging feature which automatically logs metrics, parameters, and models without explicit log statements. This works with popular ML libraries like Scikit-learn, PyTorch, and others.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.autolog()\n\n# Your training code...\n```\n\n----------------------------------------\n\nTITLE: Creating an MLflow PythonModel with Type Hints\nDESCRIPTION: Shows how to define a Pydantic model for input validation and create an MLflow PythonModel class that uses type hints. The model uses OpenAI to generate responses based on user messages.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# define your input schema\nclass Message(pydantic.BaseModel):\n    role: str\n    content: str\n\n\n# inherit mlflow PythonModel\nclass MyModel(mlflow.pyfunc.PythonModel):\n    # add type hint to model_input\n    def predict(self, model_input: list[Message]) -> str:\n        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=model_input)\n        return response.choices[0].message.content\n\n\nmodel = MyModel()\nmodel.predict([{\"role\": \"user\", \"content\": \"What is DSPy?\"}])\n```\n\n----------------------------------------\n\nTITLE: Logging Metrics and Parameters with MLflow in Python\nDESCRIPTION: A basic example showing how to start a run and log parameters and metrics using MLflow's tracking API. This pattern is used to record experiment data during model training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run():\n    mlflow.log_param(\"lr\", 0.001)\n    # Your ml code\n    ...\n    mlflow.log_metric(\"val_loss\", val_loss)\n```\n\n----------------------------------------\n\nTITLE: Extracting Span Fields into DataFrame Columns in Python\nDESCRIPTION: Shows how to extract specific fields from spans into separate columns in the returned DataFrame using the extract_fields parameter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/search.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntraces = mlflow.search_traces(\n    extract_fields=[\n        # Extract the \"name\" field in the \"morning_greeting\" span inputs\n        \"morning_greeting.inputs.name\",\n        # Extract all output fields in the \"morning_greeting\" span\n        \"morning_greeting.outputs\",\n    ],\n    experiment_ids=[morning_experiment.experiment_id],\n)\n\nprint(traces)\n\neval_data = traces.rename(\n    columns={\n        \"morning_greeting.inputs.name\": \"inputs\",\n        \"morning_greeting.outputs\": \"ground_truth\",\n    }\n)\nresults = mlflow.evaluate(\n    model,\n    eval_data,\n    targets=\"ground_truth\",\n    model_type=\"question-answering\",\n)\n```\n\n----------------------------------------\n\nTITLE: Comprehensive OpenAI Autologging Example in Python\nDESCRIPTION: A complete example of enabling OpenAI autologging with all features turned on, including input examples, model signatures, model logging, and trace logging. The example also includes model registration for easier management.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/autologging/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport mlflow\nimport openai\n\nAPI_KEY = os.environ.get(\"OPENAI_API_KEY\")\nEXPERIMENT_NAME = \"OpenAI Autologging Demonstration\"\nREGISTERED_MODEL_NAME = \"openai-auto\"\nMODEL_VERSION = 1\n\nmlflow.openai.autolog(\n    log_input_examples=True,\n    log_model_signatures=True,\n    log_models=True,\n    log_traces=True,\n    registered_model_name=REGISTERED_MODEL_NAME,\n)\n\nmlflow.set_experiment(EXPERIMENT_NAME)\n\nopenai_client = openai.OpenAI(api_key=API_KEY)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"State that you are responding to a test and that you are alive.\",\n    }\n]\n\nopenai_client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    temperature=0.95,\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Transformers Pipeline with Prompt Template in MLflow\nDESCRIPTION: This snippet initializes a text generation pipeline using the 'distilgpt2' model, defines a prompt template, and saves the model using MLflow's transformers flavor. The prompt template is specified when saving the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom transformers import pipeline\n\n# Initialize a pipeline. `distilgpt2` uses a \"text-generation\" pipeline\ngenerator = pipeline(model=\"distilgpt2\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with a Logged LangChain Model from MLflow\nDESCRIPTION: This code demonstrates how to load a previously logged LangChain model from MLflow and run inference with it. It loads the model using mlflow.langchain.load_model and invokes it with sample input data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load the model and run inference\nhomework_chain = mlflow.langchain.load_model(model_uri=info.model_uri)\n\nexam_question = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": {\n                \"question\": \"What is the primary function of control rods in a nuclear reactor?\",\n                \"answer\": \"To stir the primary coolant so that the neutrons are mixed well.\",\n            },\n        },\n    ]\n}\n\nresponse = homework_chain.invoke(exam_question)\n\npprint(response)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Evaluation on the RAG System\nDESCRIPTION: Executes the MLflow evaluation on the RAG system with the defined metrics for faithfulness, relevance, and latency, mapping the inputs and context appropriately for evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresults = mlflow.evaluate(\n    model,\n    eval_df,\n    model_type=\"question-answering\",\n    evaluators=\"default\",\n    predictions=\"result\",\n    extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n    evaluator_config={\n        \"col_mapping\": {\n            \"inputs\": \"questions\",\n            \"context\": \"source_documents\",\n        }\n    },\n)\nprint(results.metrics)\n```\n\n----------------------------------------\n\nTITLE: Using Inference Parameters with MLflow Python Model\nDESCRIPTION: Complete example demonstrating how to create, log, and use a model with inference parameters. Shows creating a model signature with params, logging the model, loading it, and making predictions with different parameter values.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params):\n        return list(params.values())\n\n\nparams = {\"temperature\": 0.5, \"suppress_tokens\": [101, 102]}\n# params' default values are saved with ModelSignature\nsignature = infer_signature([\"input\"], params=params)\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=MyModel(), artifact_path=\"my_model\", signature=signature\n    )\n\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Not passing params -- predict with default values\nloaded_predict = loaded_model.predict([\"input\"])\nassert loaded_predict == [0.5, [101, 102]]\n\n# Passing some params -- override passed-in params\nloaded_predict = loaded_model.predict([\"input\"], params={\"temperature\": 0.1})\nassert loaded_predict == [0.1, [101, 102]]\n\n# Passing all params -- override all params\nloaded_predict = loaded_model.predict(\n    [\"input\"], params={\"temperature\": 0.5, \"suppress_tokens\": [103]}\n)\nassert loaded_predict == [0.5, [103]]\n```\n\n----------------------------------------\n\nTITLE: Defining RAG System Evaluation Function in Python\nDESCRIPTION: Creates a simple function to run input questions through a RAG chain for evaluation. This function is used to assess the RAG system's performance in terms of relevance and latency using MLflow's evaluate function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef model(input_df):\n    return input_df[\"questions\"].map(qa).tolist()\n```\n\n----------------------------------------\n\nTITLE: Integrating OpenAI Model with MLflow\nDESCRIPTION: Configures and logs an OpenAI embedding model within MLflow, setting up the experiment context and model signature for document similarity analysis.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"Documenatation Similarity\")\n\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"text-embedding-ada-002\",\n        task=openai.embeddings,\n        artifact_path=\"model\",\n        signature=ModelSignature(\n            inputs=Schema([ColSpec(type=\"string\", name=None)]),\n            outputs=Schema([TensorSpec(type=np.dtype(\"float64\"), shape=(-1,))]),\n            params=ParamSchema([ParamSpec(name=\"batch_size\", dtype=\"long\", default=1024)]),\n        ),\n    )\n\n# Load the model in pyfunc format\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom PyFunc Class for LLM Deployment in MLflow\nDESCRIPTION: Definition of a custom PyFunc class (MPT) that encapsulates the model loading, prompt construction, and inference logic. The class includes methods for loading the model context, building prompts, and generating predictions with customizable parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MPT(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        \"\"\"\n        This method initializes the tokenizer and language model\n        using the specified model snapshot directory.\n        \"\"\"\n        # Initialize tokenizer and language model\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            context.artifacts[\"snapshot\"], padding_side=\"left\"\n        )\n\n        config = transformers.AutoConfig.from_pretrained(\n            context.artifacts[\"snapshot\"], trust_remote_code=True\n        )\n        # If you are running this in a system that has a sufficiently powerful GPU with available VRAM,\n        # uncomment the configuration setting below to leverage triton.\n        # Note that triton dramatically improves the inference speed performance\n\n        # config.attn_config[\"attn_impl\"] = \"triton\"\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            context.artifacts[\"snapshot\"],\n            config=config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n        # this setting will not function correctly. Setting device to 'cpu' is valid, but\n        # the performance will be very slow.\n        self.model.to(device=\"cpu\")\n        # If running on a GPU-compatible environment, uncomment the following line:\n        # self.model.to(device=\"cuda\")\n\n        self.model.eval()\n\n    def _build_prompt(self, instruction):\n        \"\"\"\n        This method generates the prompt for the model.\n        \"\"\"\n        INSTRUCTION_KEY = \"### Instruction:\"\n        RESPONSE_KEY = \"### Response:\"\n        INTRO_BLURB = (\n            \"Below is an instruction that describes a task. \"\n            \"Write a response that appropriately completes the request.\"\n        )\n\n        return f\"\"\"{INTRO_BLURB}\n        {INSTRUCTION_KEY}\n        {instruction}\n        {RESPONSE_KEY}\n        \"\"\"\n\n    def predict(self, context, model_input, params=None):\n        \"\"\"\n        This method generates prediction for the given input.\n        \"\"\"\n        prompt = model_input[\"prompt\"][0]\n\n        # Retrieve or use default values for temperature and max_tokens\n        temperature = params.get(\"temperature\", 0.1) if params else 0.1\n        max_tokens = params.get(\"max_tokens\", 1000) if params else 1000\n\n        # Build the prompt\n        prompt = self._build_prompt(prompt)\n\n        # Encode the input and generate prediction\n        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n        # If attempting to run this with GPU support, change 'cpu' to 'cuda' for maximum performance\n        encoded_input = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cpu\")\n        output = self.model.generate(\n            encoded_input,\n            do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n        )\n\n        # Removing the prompt from the generated text\n        prompt_length = len(self.tokenizer.encode(prompt, return_tensors=\"pt\")[0])\n        generated_response = self.tokenizer.decode(\n            output[0][prompt_length:], skip_special_tokens=True\n        )\n\n        return {\"candidates\": [generated_response]}\n```\n\n----------------------------------------\n\nTITLE: Viewing Evaluation Results\nDESCRIPTION: Accessing and displaying the evaluation metrics and results table\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresults.metrics\n\nresults.tables[\"eval_results_table\"]\n```\n\n----------------------------------------\n\nTITLE: Logging and Using a Simple MLflow Model From Code in Python\nDESCRIPTION: Demonstrates how to log the previously defined BasicModel using MLflow, and then load and use the model for predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/models-from-code/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_experiment(\"Basic Model From Code\")\n\nmodel_path = \"basic.py\"\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=model_path,  # Define the model as the path to the script that was just saved\n        artifact_path=\"arithmetic_model\",\n        input_example=[42.0, 24.0],\n    )\n```\n\nLANGUAGE: python\nCODE:\n```\nmy_model = mlflow.pyfunc.load_model(model_info.model_uri)\nmy_model.predict([2.2, 3.1, 4.7])\n\n# or, with a Pandas DataFrame input\nmy_model.predict(pd.DataFrame([5.0, 6.0, 7.0]))\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Dataset\nDESCRIPTION: Imports required libraries, loads the sms_spam dataset, and splits it into training and test sets with an 80/20 ratio.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    pipeline,\n)\n\nimport mlflow\n\n# Load the \"sms_spam\" dataset.\nsms_dataset = load_dataset(\"sms_spam\")\n\n# Split train/test by an 8/2 ratio.\nsms_train_test = sms_dataset[\"train\"].train_test_split(test_size=0.2)\ntrain_dataset = sms_train_test[\"train\"]\ntest_dataset = sms_train_test[\"test\"]\n```\n\n----------------------------------------\n\nTITLE: Logging Model and Metadata to MLflow\nDESCRIPTION: Python code to log the trained model, its parameters, metrics, and tags to MLflow, and register the model in the MLflow Model Registry.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Set our tracking server uri for logging\nmlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n\n# Create a new MLflow Experiment\nmlflow.set_experiment(\"MLflow Quickstart\")\n\n# Start an MLflow run\nwith mlflow.start_run():\n    # Log the hyperparameters\n    mlflow.log_params(params)\n\n    # Log the loss metric\n    mlflow.log_metric(\"accuracy\", accuracy)\n\n    # Set a tag that we can use to remind ourselves what this run was for\n    mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")\n\n    # Infer the model signature\n    signature = infer_signature(X_train, lr.predict(X_train))\n\n    # Log the model\n    model_info = mlflow.sklearn.log_model(\n        sk_model=lr,\n        artifact_path=\"iris_model\",\n        signature=signature,\n        input_example=X_train,\n        registered_model_name=\"tracking-quickstart\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging a Transformers Model with hf:/ Schema\nDESCRIPTION: Shows how to use the special 'hf:/' schema to log a transformers model from Hugging Face Hub directly, which is useful for large models when you want to serve the model without copying large files.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\nimport numpy as np\nimport transformers\n\n\n# Define a custom PythonModel\nclass QAModel(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        \"\"\"\n        This method initializes the tokenizer and language model\n        using the specified snapshot location from model context.\n        \"\"\"\n        snapshot_location = context.artifacts[\"bert-tiny-model\"]\n        # Initialize tokenizer and language model\n        tokenizer = transformers.AutoTokenizer.from_pretrained(snapshot_location)\n        model = transformers.BertForQuestionAnswering.from_pretrained(snapshot_location)\n        self.pipeline = transformers.pipeline(\n            task=\"question-answering\", model=model, tokenizer=tokenizer\n        )\n\n    def predict(self, context, model_input, params=None):\n        question = model_input[\"question\"][0]\n        if isinstance(question, np.ndarray):\n            question = question.item()\n        ctx = model_input[\"context\"][0]\n        if isinstance(ctx, np.ndarray):\n            ctx = ctx.item()\n        return self.pipeline(question=question, context=ctx)\n\n\n# Log the model\ndata = {\"question\": \"Who's house?\", \"context\": \"The house is owned by Run.\"}\npyfunc_artifact_path = \"question_answering_model\"\nwith mlflow.start_run() as run:\n    model_info = mlflow.pyfunc.log_model(\n        artifact_path=pyfunc_artifact_path,\n        python_model=QAModel(),\n        artifacts={\"bert-tiny-model\": \"hf:/prajjwal1/bert-tiny\"},\n        input_example=data,\n        signature=infer_signature(data, [\"Run\"]),\n        extra_pip_requirements=[\"torch\", \"accelerate\", \"transformers\", \"numpy\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging LangChain LLMChain in MLflow\nDESCRIPTION: Demonstrates how to create, log and use a basic LangChain LLMChain model with MLflow. The example shows initializing an OpenAI model with a prompt template, logging it to MLflow, and making predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/guide/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nimport mlflow\n\n# Ensure the OpenAI API key is set in the environment\nassert (\n    \"OPENAI_API_KEY\" in os.environ\n), \"Please set the OPENAI_API_KEY environment variable.\"\n\n# Initialize the OpenAI model and the prompt template\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\n\n# Create the LLMChain with the specified model and prompt\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Log the LangChain LLMChain in an MLflow run\nwith mlflow.start_run():\n    logged_model = mlflow.langchain.log_model(chain, \"langchain_model\")\n\n# Load the logged model using MLflow's Python function flavor\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n\n# Predict using the loaded model\nprint(loaded_model.predict([{\"product\": \"colorful socks\"}]))\n```\n\n----------------------------------------\n\nTITLE: Saving an XGBoost Model in MLflow Format\nDESCRIPTION: Creates a wrapper around an XGBoost model that conforms to MLflow's python_function inference API. The example shows how to train an XGBoost model, save it, create a wrapper, and use it for predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_57\n\nLANGUAGE: python\nCODE:\n```\n# Load training and test datasets\nfrom sys import version_info\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nPYTHON_VERSION = f\"{version_info.major}.{version_info.minor}.{version_info.micro}\"\niris = datasets.load_iris()\nx = iris.data[:, 2:]\ny = iris.target\nx_train, x_test, y_train, _ = train_test_split(x, y, test_size=0.2, random_state=42)\ndtrain = xgb.DMatrix(x_train, label=y_train)\n\n# Train and save an XGBoost model\nxgb_model = xgb.train(params={\"max_depth\": 10}, dtrain=dtrain, num_boost_round=10)\nxgb_model_path = \"xgb_model.pth\"\nxgb_model.save_model(xgb_model_path)\n\n# Create an `artifacts` dictionary that assigns a unique name to the saved XGBoost model file.\n# This dictionary will be passed to `mlflow.pyfunc.save_model`, which will copy the model file\n# into the new MLflow Model's directory.\nartifacts = {\"xgb_model\": xgb_model_path}\n\n# Define the model class\nimport mlflow.pyfunc\n\n\nclass XGBWrapper(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        import xgboost as xgb\n\n        self.xgb_model = xgb.Booster()\n        self.xgb_model.load_model(context.artifacts[\"xgb_model\"])\n\n    def predict(self, context, model_input, params=None):\n        input_matrix = xgb.DMatrix(model_input.values)\n        return self.xgb_model.predict(input_matrix)\n\n\n# Create a Conda environment for the new MLflow Model that contains all necessary dependencies.\nimport cloudpickle\n\nconda_env = {\n    \"channels\": [\"defaults\"],\n    \"dependencies\": [\n        f\"python={PYTHON_VERSION}\",\n        \"pip\",\n        {\n            \"pip\": [\n                f\"mlflow=={mlflow.__version__}\",\n                f\"xgboost=={xgb.__version__}\",\n                f\"cloudpickle=={cloudpickle.__version__}\",\n            ],\n        },\n    ],\n    \"name\": \"xgb_env\",\n}\n\n# Save the MLflow Model\nmlflow_pyfunc_model_path = \"xgb_mlflow_pyfunc\"\nmlflow.pyfunc.save_model(\n    path=mlflow_pyfunc_model_path,\n    python_model=XGBWrapper(),\n    artifacts=artifacts,\n    conda_env=conda_env,\n)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n\n# Evaluate the model\nimport pandas as pd\n\ntest_predictions = loaded_model.predict(pd.DataFrame(x_test))\nprint(test_predictions)\n```\n\n----------------------------------------\n\nTITLE: Logging Ollama ChatModel to MLflow\nDESCRIPTION: This snippet shows how to log the custom Ollama ChatModel to MLflow using the models-from-code approach. It sets up an experiment and logs the model with an input example for the chat interface.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-intro/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_experiment(\"chatmodel-quickstart\")\ncode_path = \"ollama_model.py\"\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        \"ollama_model\",\n        python_model=code_path,\n        input_example={\n            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Training and Logging TensorFlow Model with MLflow\nDESCRIPTION: Implements a simple linear regression using TensorFlow's low-level API. The training process is logged to MLflow, including learning rate, loss at each step, and final model parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport tensorflow as tf\n\nimport mlflow\n\nx = np.linspace(-4, 4, num=512)\ny = 3 * x + 10\n\n# estimate w and b where y = w * x + b\nlearning_rate = 0.1\nx_train = tf.Variable(x, trainable=False, dtype=tf.float32)\ny_train = tf.Variable(y, trainable=False, dtype=tf.float32)\n\n# initial values\nw = tf.Variable(1.0)\nb = tf.Variable(1.0)\n\nwith mlflow.start_run():\n    mlflow.log_param(\"learning_rate\", learning_rate)\n\n    for i in range(1000):\n        with tf.GradientTape(persistent=True) as tape:\n            # calculate MSE = 0.5 * (y_predict - y_train)^2\n            y_predict = w * x_train + b\n            loss = 0.5 * tf.reduce_mean(tf.square(y_predict - y_train))\n            mlflow.log_metric(\"loss\", value=loss.numpy(), step=i)\n\n        # Update the trainable variables\n        # w = w - learning_rate * gradient of loss function w.r.t. w\n        # b = b - learning_rate * gradient of loss function w.r.t. b\n        w.assign_sub(learning_rate * tape.gradient(loss, w))\n        b.assign_sub(learning_rate * tape.gradient(loss, b))\n\nprint(f\"W = {w.numpy():.2f}, b = {b.numpy():.2f}\")\n```\n\n----------------------------------------\n\nTITLE: OpenAI Function Calling with MLflow Tracing\nDESCRIPTION: Complete example implementing a simple function calling agent using OpenAI Function Calling and MLflow Tracing, with tool functions decorated by @mlflow.trace to create spans for tool execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/openai.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom openai import OpenAI\nimport mlflow\nfrom mlflow.entities import SpanType\n\nclient = OpenAI()\n\n\n# Define the tool function. Decorate it with `@mlflow.trace` to create a span for its execution.\n@mlflow.trace(span_type=SpanType.TOOL)\ndef get_weather(city: str) -> str:\n    if city == \"Tokyo\":\n        return \"sunny\"\n    elif city == \"Paris\":\n        return \"rainy\"\n    return \"unknown\"\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"city\": {\"type\": \"string\"}},\n            },\n        },\n    }\n]\n\n_tool_functions = {\"get_weather\": get_weather}\n\n\n# Define a simple tool calling agent\n@mlflow.trace(span_type=SpanType.AGENT)\ndef run_tool_agent(question: str):\n    messages = [{\"role\": \"user\", \"content\": question}]\n\n    # Invoke the model with the given question and available tools\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=messages,\n        tools=tools,\n    )\n    ai_msg = response.choices[0].message\n    messages.append(ai_msg)\n\n    # If the model request tool call(s), invoke the function with the specified arguments\n    if tool_calls := ai_msg.tool_calls:\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            if tool_func := _tool_functions.get(function_name):\n                args = json.loads(tool_call.function.arguments)\n                tool_result = tool_func(**args)\n            else:\n                raise RuntimeError(\"An invalid tool is returned from the assistant!\")\n\n            messages.append(\n                {\n                    \"role\": \"tool\",\n                    \"tool_call_id\": tool_call.id,\n                    \"content\": tool_result,\n                }\n            )\n\n        # Sent the tool results to the model and get a new response\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\", messages=messages\n        )\n\n    return response.choices[0].message.content\n\n\n# Run the tool calling agent\nquestion = \"What's the weather like in Paris today?\"\nanswer = run_tool_agent(question)\n```\n\n----------------------------------------\n\nTITLE: Logging and Using fastai Tabular Model with MLflow\nDESCRIPTION: This example demonstrates how to train a fastai tabular model on the Adult dataset, log it using MLflow, and then load it to make predictions using fastai's native interface. The code shows the complete workflow from data loading to model training and prediction generation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.data.external import URLs, untar_data\nfrom fastai.tabular.core import Categorify, FillMissing, Normalize, TabularPandas\nfrom fastai.tabular.data import TabularDataLoaders\nfrom fastai.tabular.learner import tabular_learner\nfrom fastai.data.transforms import RandomSplitter\nfrom fastai.metrics import accuracy\nfrom fastcore.basics import range_of\nimport pandas as pd\nimport mlflow\nimport mlflow.fastai\n\n\ndef print_auto_logged_info(r):\n    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n    artifacts = [\n        f.path for f in mlflow.MlflowClient().list_artifacts(r.info.run_id, \"model\")\n    ]\n    print(f\"run_id: {r.info.run_id}\")\n    print(f\"artifacts: {artifacts}\")\n    print(f\"params: {r.data.params}\")\n    print(f\"metrics: {r.data.metrics}\")\n    print(f\"tags: {tags}\")\n\n\ndef main(epochs=5, learning_rate=0.01):\n    path = untar_data(URLs.ADULT_SAMPLE)\n    path.ls()\n\n    df = pd.read_csv(path / \"adult.csv\")\n\n    dls = TabularDataLoaders.from_csv(\n        path / \"adult.csv\",\n        path=path,\n        y_names=\"salary\",\n        cat_names=[\n            \"workclass\",\n            \"education\",\n            \"marital-status\",\n            \"occupation\",\n            \"relationship\",\n            \"race\",\n        ],\n        cont_names=[\"age\", \"fnlwgt\", \"education-num\"],\n        procs=[Categorify, FillMissing, Normalize],\n    )\n\n    splits = RandomSplitter(valid_pct=0.2)(range_of(df))\n\n    to = TabularPandas(\n        df,\n        procs=[Categorify, FillMissing, Normalize],\n        cat_names=[\n            \"workclass\",\n            \"education\",\n            \"marital-status\",\n            \"occupation\",\n            \"relationship\",\n            \"race\",\n        ],\n        cont_names=[\"age\", \"fnlwgt\", \"education-num\"],\n        y_names=\"salary\",\n        splits=splits,\n    )\n\n    dls = to.dataloaders(bs=64)\n\n    model = tabular_learner(dls, metrics=accuracy)\n\n    mlflow.fastai.autolog()\n\n    with mlflow.start_run() as run:\n        model.fit(5, 0.01)\n        mlflow.fastai.log_model(model, \"model\")\n\n    print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))\n\n    model_uri = f\"runs:/{run.info.run_id}/model\"\n    loaded_model = mlflow.fastai.load_model(model_uri)\n\n    test_df = df.copy()\n    test_df.drop([\"salary\"], axis=1, inplace=True)\n    dl = learn.dls.test_dl(test_df)\n\n    predictions, _ = loaded_model.get_preds(dl=dl)\n    px = pd.DataFrame(predictions).astype(\"float\")\n    px.head(5)\n\n\nmain()\n```\n\n----------------------------------------\n\nTITLE: Basic OpenAI Tracing Example with MLflow\nDESCRIPTION: A complete example demonstrating how to set up OpenAI tracing with MLflow, including setting tracking URI and experiment name, and making a simple chat completion request.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/openai.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport mlflow\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"OpenAI\")\n\nopenai_client = openai.OpenAI()\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the capital of France?\",\n    }\n]\n\nresponse = openai_client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n    temperature=0.1,\n    max_tokens=100,\n)\n```\n\n----------------------------------------\n\nTITLE: Logging PyTorch Training Metrics with MLflow\nDESCRIPTION: A complete example demonstrating how to log PyTorch training experiments to MLflow, including model parameters, metrics, and artifacts. The code implements a neural network for FashionMNIST classification with proper MLflow integration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/guide/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport torch\n\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchmetrics import Accuracy\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=64)\n\n# Get cpu or gpu for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n# Define the model.\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\ndef train(dataloader, model, loss_fn, metrics_fn, optimizer):\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        accuracy = metrics_fn(pred, y)\n\n        # Backpropagation.\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch\n            mlflow.log_metric(\"loss\", f\"{loss:3f}\", step=(batch // 100))\n            mlflow.log_metric(\"accuracy\", f\"{accuracy:3f}\", step=(batch // 100))\n            print(\n                f\"loss: {loss:3f} accuracy: {accuracy:3f} [{current} / {len(dataloader)}]\"\n            )\n\n\nepochs = 3\nloss_fn = nn.CrossEntropyLoss()\nmetric_fn = Accuracy(task=\"multiclass\", num_classes=10).to(device)\nmodel = NeuralNetwork().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nwith mlflow.start_run():\n    params = {\n        \"epochs\": epochs,\n        \"learning_rate\": 1e-3,\n        \"batch_size\": 64,\n        \"loss_function\": loss_fn.__class__.__name__,\n        \"metric_function\": metric_fn.__class__.__name__,\n        \"optimizer\": \"SGD\",\n    }\n    # Log training parameters.\n    mlflow.log_params(params)\n\n    # Log model summary.\n    with open(\"model_summary.txt\", \"w\") as f:\n        f.write(str(summary(model)))\n    mlflow.log_artifact(\"model_summary.txt\")\n\n    for t in range(epochs):\n        print(f\"Epoch {t+1}\\n-------------------------------\")\n        train(train_dataloader, model, loss_fn, metric_fn, optimizer)\n\n    # Save the trained model to MLflow.\n    mlflow.pytorch.log_model(model, \"model\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search Model with MLflow and Sentence Transformers\nDESCRIPTION: This code snippet defines a SemanticSearchModel class that inherits from MLflow's PythonModel. It implements a custom semantic search functionality using Sentence Transformers for encoding and similarity computation. The model loads a pre-trained transformer and a corpus, then performs semantic search on input queries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\nimport mlflow\nfrom mlflow.models.signature import infer_signature\nfrom mlflow.pyfunc import PythonModel\n\n\nclass SemanticSearchModel(PythonModel):\n    def load_context(self, context):\n        \"\"\"Load the model context for inference, including the corpus from a file.\"\"\"\n        try:\n            # Load the pre-trained sentence transformer model\n            self.model = SentenceTransformer.load(context.artifacts[\"model_path\"])\n\n            # Load the corpus from the specified file\n            corpus_file = context.artifacts[\"corpus_file\"]\n            with open(corpus_file) as file:\n                self.corpus = file.read().splitlines()\n\n            # Encode the corpus and convert it to a tensor\n            self.corpus_embeddings = self.model.encode(self.corpus, convert_to_tensor=True)\n\n        except Exception as e:\n            raise ValueError(f\"Error loading model and corpus: {e}\")\n\n    def predict(self, context, model_input, params=None):\n        \"\"\"Predict method to perform semantic search over the corpus.\"\"\"\n\n        if isinstance(model_input, pd.DataFrame):\n            if model_input.shape[1] != 1:\n                raise ValueError(\"DataFrame input must have exactly one column.\")\n            model_input = model_input.iloc[0, 0]\n        elif isinstance(model_input, dict):\n            model_input = model_input.get(\"sentence\")\n            if model_input is None:\n                raise ValueError(\"The input dictionary must have a key named 'sentence'.\")\n        else:\n            raise TypeError(\n                f\"Unexpected type for model_input: {type(model_input)}. Must be either a Dict or a DataFrame.\"\n            )\n\n        # Encode the query\n        query_embedding = self.model.encode(model_input, convert_to_tensor=True)\n\n        # Compute cosine similarity scores\n        cos_scores = util.cos_sim(query_embedding, self.corpus_embeddings)[0]\n\n        # Determine the number of top results to return\n        top_k = params.get(\"top_k\", 3) if params else 3  # Default to 3 if not specified\n\n        minimum_relevancy = (\n            params.get(\"minimum_relevancy\", 0.2) if params else 0.2\n        )  # Default to 0.2 if not specified\n\n        # Get the top_k most similar sentences from the corpus\n        top_results = np.argsort(cos_scores, axis=0)[-top_k:]\n\n        # Prepare the initial results list\n        initial_results = [\n            (self.corpus[idx], cos_scores[idx].item()) for idx in reversed(top_results)\n        ]\n\n        # Filter the results based on the minimum relevancy threshold\n        filtered_results = [result for result in initial_results if result[1] >= minimum_relevancy]\n\n        # If all results are below the threshold, issue a warning and return the top result\n        if not filtered_results:\n            warnings.warn(\n                \"All top results are below the minimum relevancy threshold. \"\n                \"Returning the highest match instead.\",\n                RuntimeWarning,\n            )\n            return [initial_results[0]]\n        else:\n            return filtered_results\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Generation Pipeline with Transformers and MLflow in Python\nDESCRIPTION: Sets up a text generation pipeline using the Transformers library and prepares for MLflow integration. It defines the task, creates a generation pipeline, sets up an input example, and defines inference parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/text-generation/text-generation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\n\nimport mlflow\n\n# Define the task that we want to use (required for proper pipeline construction)\ntask = \"text2text-generation\"\n\n# Define the pipeline, using the task and a model instance that is applicable for our task.\ngeneration_pipeline = transformers.pipeline(\n    task=task,\n    model=\"declare-lab/flan-alpaca-large\",\n)\n\n# Define a simple input example that will be recorded with the model in MLflow, giving\n# users of the model an indication of the expected input format.\ninput_example = [\"prompt 1\", \"prompt 2\", \"prompt 3\"]\n\n# Define the parameters (and their defaults) for optional overrides at inference time.\nparameters = {\"max_length\": 512, \"do_sample\": True, \"temperature\": 0.4}\n```\n\n----------------------------------------\n\nTITLE: Creating a Prompt with MLflow Python API\nDESCRIPTION: Demonstrates how to create a new prompt using the mlflow.register_prompt API. The example includes setting a template, commit message, version metadata, and tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Use double curly braces for variables in the template\ninitial_template = \"\"\"\nSummarize content you are provided with in {{ num_sentences }} sentences.\n\nSentences: {{ sentences }}\n\"\"\"\n\n# Register a new prompt\nprompt = mlflow.register_prompt(\n    name=\"summarization-prompt\",\n    template=initial_template,\n    # Optional: Provide a commit message to describe the changes\n    commit_message=\"Initial commit\",\n    # Optional: Specify any additional metadata about the prompt version\n    version_metadata={\n        \"author\": \"author@example.com\",\n    },\n    # Optional: Set tags applies to the prompt (across versions)\n    tags={\n        \"task\": \"summarization\",\n        \"language\": \"en\",\n    },\n)\n\n# The prompt object contains information about the registered prompt\nprint(f\"Created prompt '{prompt.name}' (version {prompt.version})\")\n```\n\n----------------------------------------\n\nTITLE: Implementing DSPy Tracing with a Summarization Example\nDESCRIPTION: A complete example demonstrating how to set up MLflow tracing with DSPy, configure a model, define a summarizer module, and execute it while traces are automatically captured.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/dspy.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nimport mlflow\n\n# Enabling tracing for DSPy\nmlflow.dspy.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"DSPy\")\n\n# Define a simple ChainOfThought model and run it\nlm = dspy.LM(\"openai/gpt-4o-mini\")\ndspy.configure(lm=lm)\n\n\n# Define a simple summarizer model and run it\nclass SummarizeSignature(dspy.Signature):\n    \"\"\"Given a passage, generate a summary.\"\"\"\n\n    passage: str = dspy.InputField(desc=\"a passage to summarize\")\n    summary: str = dspy.OutputField(desc=\"a one-line summary of the passage\")\n\n\nclass Summarize(dspy.Module):\n    def __init__(self):\n        self.summarize = dspy.ChainOfThought(SummarizeSignature)\n\n    def forward(self, passage: str):\n        return self.summarize(passage=passage)\n\n\nsummarizer = Summarize()\nsummarizer(\n    passage=(\n        \"MLflow Tracing is a feature that enhances LLM observability in your Generative AI (GenAI) applications \"\n        \"by capturing detailed information about the execution of your application's services. Tracing provides \"\n        \"a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, \"\n        \"enabling you to easily pinpoint the source of bugs and unexpected behaviors.\"\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Training Keras Model with Custom MLflow Callback in Python\nDESCRIPTION: Demonstrates the use of the custom MLflow callback (MlflowCallbackLogPerBatch) during model training, allowing for more precise control over logging frequency and step counting.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/keras/quickstart/quickstart_keras.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = initialize_model()\n\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(),\n    metrics=[\"accuracy\"],\n)\n\nwith mlflow.start_run() as run:\n    model.fit(\n        x_train,\n        y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_split=0.1,\n        callbacks=[MlflowCallbackLogPerBatch(run, log_every_epoch=False, log_every_n_steps=5)],\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow via pip\nDESCRIPTION: Command to install MLflow using pip package manager. This installation includes the MLflow CLI tool needed to start a local tracking server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/tracking-server-overview/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Retriever Evaluation with MLflow (Python)\nDESCRIPTION: This snippet demonstrates a comprehensive evaluation of a retriever using MLflow, calculating multiple metrics including precision, recall, and NDCG at different k values. It logs the results to an MLflow run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    evaluate_results = mlflow.evaluate(\n        data=data,\n        targets=\"source\",\n        predictions=\"retrieved_doc_ids\",\n        evaluators=\"default\",\n        extra_metrics=[\n            mlflow.metrics.precision_at_k(1),\n            mlflow.metrics.precision_at_k(2),\n            mlflow.metrics.precision_at_k(3),\n            mlflow.metrics.recall_at_k(1),\n            mlflow.metrics.recall_at_k(2),\n            mlflow.metrics.recall_at_k(3),\n            mlflow.metrics.ndcg_at_k(1),\n            mlflow.metrics.ndcg_at_k(2),\n            mlflow.metrics.ndcg_at_k(3),\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Metric Evaluation Function\nDESCRIPTION: Implementation of a custom metric evaluation function that calculates absolute differences between predictions and targets.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.metrics import MetricValue\n\n\ndef my_metric_eval_fn(predictions, targets):\n    scores = np.abs(predictions - targets)\n    return MetricValue(\n        scores=list(scores),\n        aggregate_results={\n            \"mean\": np.mean(scores),\n            \"variance\": np.var(scores),\n            \"median\": np.median(scores),\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Training and Logging Prophet Model with MLflow in Python\nDESCRIPTION: This code snippet shows how to train a Prophet model on time series data, perform cross-validation, log metrics and parameters, and save the model using MLflow. It also demonstrates how to load the saved model and generate predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nfrom prophet import Prophet, serialize\nfrom prophet.diagnostics import cross_validation, performance_metrics\n\nimport mlflow\nfrom mlflow.models import infer_signature\n\n# URL to the dataset\nSOURCE_DATA = \"https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv\"\n\nnp.random.seed(12345)\n\n\ndef extract_params(pr_model):\n    params = {attr: getattr(pr_model, attr) for attr in serialize.SIMPLE_ATTRIBUTES}\n    return {k: v for k, v in params.items() if isinstance(v, (int, float, str, bool))}\n\n\n# Load the training data\ntrain_df = pd.read_csv(SOURCE_DATA)\n\n# Create a \"test\" DataFrame with the \"ds\" column containing 10 days after the end date in train_df\ntest_dates = pd.date_range(start=\"2016-01-21\", end=\"2016-01-31\", freq=\"D\")\ntest_df = pd.DataFrame({\"ds\": test_dates})\n\n# Initialize Prophet model with specific parameters\nprophet_model = Prophet(changepoint_prior_scale=0.5, uncertainty_samples=7)\n\nwith mlflow.start_run():\n    # Fit the model on the training data\n    prophet_model.fit(train_df)\n\n    # Extract and log model parameters\n    params = extract_params(prophet_model)\n    mlflow.log_params(params)\n\n    # Perform cross-validation\n    cv_results = cross_validation(\n        prophet_model,\n        initial=\"900 days\",\n        period=\"30 days\",\n        horizon=\"30 days\",\n        parallel=\"threads\",\n        disable_tqdm=True,\n    )\n\n    # Calculate and log performance metrics\n    cv_metrics = performance_metrics(cv_results, metrics=[\"mse\", \"rmse\", \"mape\"])\n    average_metrics = cv_metrics.drop(columns=[\"horizon\"]).mean(axis=0).to_dict()\n    mlflow.log_metrics(average_metrics)\n\n    # Generate predictions and infer model signature\n    train = prophet_model.history\n\n    # Log the Prophet model with MLflow\n    model_info = mlflow.prophet.log_model(\n        prophet_model,\n        artifact_path=\"prophet_model\",\n        input_example=train[[\"ds\"]].head(10),\n    )\n\n# Load the saved model as a pyfunc\nprophet_model_saved = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Generate predictions for the test set\npredictions = prophet_model_saved.predict(test_df)\n\n# Truncate and display the forecast if needed\nforecast = predictions[[\"ds\", \"yhat\"]]\n\nprint(f\"forecast:\\n{forecast.head(5)}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Ollama Chat Model with MLflow PythonModel\nDESCRIPTION: Implementation of a custom PythonModel class that wraps the Ollama LLM client. Handles chat completion requests, parameter configuration, and response formatting compatible with OpenAI-style chat interfaces.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-intro/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass OllamaPyfunc(PythonModel):\n    def __init__(self):\n        self.model_name = None\n        self.client = None\n\n    def load_context(self, context):\n        self.model_name = \"llama3.2:1b\"\n        self.client = ollama.Client()\n\n    def _prepare_options(self, params):\n        options = {}\n        if params:\n            if \"max_tokens\" in params:\n                options[\"num_predict\"] = params[\"max_tokens\"]\n            if \"temperature\" in params:\n                options[\"temperature\"] = params[\"temperature\"]\n            if \"top_p\" in params:\n                options[\"top_p\"] = params[\"top_p\"]\n            if \"stop\" in params:\n                options[\"stop\"] = params[\"stop\"]\n            if \"seed\" in params:\n                options[\"seed\"] = params[\"seed\"]\n\n        return Options(options)\n\n    def predict(self, context, model_input, params=None):\n        if isinstance(model_input, (pd.DataFrame, pd.Series)):\n            messages = model_input.to_dict(orient=\"records\")[0][\"messages\"]\n        else:\n            messages = model_input.get(\"messages\", [])\n\n        options = self._prepare_options(params)\n        ollama_messages = [\n            {\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in messages\n        ]\n\n        response = self.client.chat(\n            model=self.model_name, messages=ollama_messages, options=options\n        )\n\n        chat_response = ChatCompletionResponse(\n            choices=[\n                ChatChoice(\n                    index=0,\n                    message=ChatMessage(\n                        role=\"assistant\", content=response[\"message\"][\"content\"]\n                    ),\n                )\n            ],\n            model=self.model_name,\n        )\n\n        return chat_response.to_dict()\n```\n\n----------------------------------------\n\nTITLE: Creating MLflow Experiment for Transformers Model in Python\nDESCRIPTION: Sets up a new MLflow experiment for logging the Transformers model. This ensures that the model run is logged to a contextually relevant experiment instead of the default one.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/text-generation/text-generation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(\"Transformers Introduction\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Auto Logging in MLflow\nDESCRIPTION: A simple example showing how to enable MLflow's auto logging feature with a single function call at the beginning of ML code, which automatically logs metrics, parameters, and models without explicit log statements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.autolog()\n\n# Your training code...\n```\n\n----------------------------------------\n\nTITLE: Loading and Using MLflow PythonModel\nDESCRIPTION: Demonstration of loading a saved MLflow PythonModel and making predictions with custom parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-intro/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\nresult = loaded_model.predict(\n    data={\"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}]},\n    params={\"max_tokens\": 25, \"seed\": 42},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an Evaluation Dataset for RAG Testing\nDESCRIPTION: Prepares a pandas DataFrame with sample questions about MLflow functionality to evaluate the performance of the RAG system.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\neval_df = pd.DataFrame(\n    {\n        \"questions\": [\n            \"What is MLflow?\",\n            \"How to run mlflow.evaluate()?\",\n            \"How to log_table()?\",\n            \"How to load_table()?\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using an MLflow Model with Prompt Template\nDESCRIPTION: Demonstrates loading an MLflow model with a prompt template and using it for inference. The code loads the previously saved model using the PyFunc interface, which automatically applies the prompt template to the user's input before passing it to the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/prompt-templating/prompt-templating.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nloaded_generator = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\nloaded_generator.predict(\"Tell me the largest bird\")\n```\n\n----------------------------------------\n\nTITLE: Connecting to MLflow Tracking Server\nDESCRIPTION: Authenticates with the Databricks MLflow tracking server using the login method, which prompts for workspace host and personal access token.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmlflow.login()\n```\n\n----------------------------------------\n\nTITLE: Logging and Loading PEFT Models in MLflow\nDESCRIPTION: This example demonstrates the complete workflow for working with PEFT models in MLflow, including creating a model with LoRA configuration, logging it to MLflow, and loading it back. It shows how to properly structure the model dictionary with both the PEFT model and tokenizer.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"databricks/dolly-v2-7b\"\nbase_model = AutoModelForCausalLM.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\npeft_config = LoraConfig(...)\npeft_model = get_peft_model(base_model, peft_config)\n\nwith mlflow.start_run():\n    # Your training code here\n    ...\n\n    # Log the PEFT model\n    model_info = mlflow.transformers.log_model(\n        transformers_model={\n            \"model\": peft_model,\n            \"tokenizer\": tokenizer,\n        },\n        artifact_path=\"peft_model\",\n    )\n\n# Load the PEFT model\nloaded_model = mlflow.transformers.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Registering a Scikit-learn Model with MLflow\nDESCRIPTION: This Python script demonstrates training a RandomForestRegressor model, logging parameters and metrics, and registering the model with MLflow. It uses the scikit-learn model flavor and showcases MLflow's run management and model logging capabilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/step1-register-model/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\nimport mlflow.sklearn\n\nwith mlflow.start_run() as run:\n    X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    params = {\"max_depth\": 2, \"random_state\": 42}\n    model = RandomForestRegressor(**params)\n    model.fit(X_train, y_train)\n\n    # Log parameters and metrics using the MLflow APIs\n    mlflow.log_params(params)\n\n    y_pred = model.predict(X_test)\n    mlflow.log_metrics({\"mse\": mean_squared_error(y_test, y_pred)})\n\n    # Log the sklearn model and register as version 1\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        artifact_path=\"sklearn-model\",\n        input_example=X_train,\n        registered_model_name=\"sk-learn-random-forest-reg-model\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Tracking Server in Bash\nDESCRIPTION: Command to start the MLflow tracking server on localhost port 8080. This launches the server and makes it available for clients to connect to.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --host 127.0.0.1 --port 8080\n```\n\n----------------------------------------\n\nTITLE: Correct ChatModel Implementation Example\nDESCRIPTION: Example showing correct implementation of load_context in a custom ChatModel subclass with properly defined instance attributes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.pyfunc import ChatModel\n\n\nclass MyModel(ChatModel):\n    def __init__(self):\n        self.state = []\n        self.my_model_config = None  # Define the attribute here\n\n    def load_context(self, context):\n        self.my_model_config = context.get(\"my_model_config\")\n```\n\n----------------------------------------\n\nTITLE: Starting Training Run with MLflow for Experiment Tracking\nDESCRIPTION: Initiates an MLflow run to track the model training process. Within the MLflow run context, the trainer.train() method is called to start the fine-tuning process, which will automatically log training metrics and parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Logging Optimization and Evaluation in a Single MLflow Run\nDESCRIPTION: Example showing how to log both optimization and evaluation into a single MLflow run by manually creating a parent run. This code combines teleprompter compilation and evaluation within the same tracking context.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/optimizer.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run(run_name=\"My Optimization Run\") as run:\n    optimized_program = teleprompter.compile(\n        program,\n        trainset=trainset,\n    )\n    evaluation = dspy.Evaluate(devset=devset, metric=metric)\n    evaluation(optimized_program)\n```\n\n----------------------------------------\n\nTITLE: Defining Model Function for RAG Evaluation\nDESCRIPTION: Creates a function that runs each input through the RAG chain, to be used in MLflow evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef model(input_df):\n    answer = []\n    for index, row in input_df.iterrows():\n        answer.append(qa(row[\"questions\"]))\n\n    return answer\n```\n\n----------------------------------------\n\nTITLE: Defining and Logging a LangGraph Agent in MLflow\nDESCRIPTION: This code demonstrates how to define a LangGraph agent in a script file and log it to MLflow. It creates a simple weather-checking agent with React architecture using LangChain tools and OpenAI's chat model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nimport mlflow\n\n\n@tool\ndef get_weather(city: Literal[\"seattle\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"seattle\":\n        return \"It's probably raining. Again.\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n\n\nllm = ChatOpenAI()\ntools = [get_weather]\ngraph = create_react_agent(llm, tools)\n\n# specify the Agent as the model interface to be loaded when executing the script\nmlflow.models.set_model(graph)\n```\n\n----------------------------------------\n\nTITLE: Advanced Code Inspector Decorator for Automatic Reviews\nDESCRIPTION: Implements a Python decorator that automatically reviews functions using an MLflow model. This decorator extracts the source code, processes it with the model, and displays feedback while still executing the original function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport inspect\n\n\ndef code_inspector(model):\n    \"\"\"\n    Decorator for automatic code review using an MLflow pyfunc model.\n\n    Args:\n        model: The MLflow pyfunc model for code evaluation.\n    \"\"\"\n\n    def decorator_check_my_function(func):\n        # Decorator that wraps around the given function\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Extracting the source code of the decorated function\n                parsed_func = inspect.getsource(func)\n\n                # Using the MLflow model to evaluate the extracted source code\n                response = model.predict([parsed_func])\n\n                # Printing the response for code review feedback\n                print(response)\n\n            except Exception as e:\n                # Handling exceptions during model prediction or source code extraction\n                print(\"Error during model prediction or formatting:\", e)\n\n            # Executing and returning the original function's output\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator_check_my_function\n```\n\n----------------------------------------\n\nTITLE: Defining the LangChain RAG Pipeline\nDESCRIPTION: Creates a chain that combines the retriever, prompt, and language model. This chain will retrieve relevant context for a question and pass it to the LLM for generating a response.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# define our chain\nchain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n```\n\n----------------------------------------\n\nTITLE: Logging a LangChain Model with MLflow Models from Code\nDESCRIPTION: This code demonstrates how to log a LangChain model using MLflow's Models from Code feature. It sets up an experiment, specifies the path to the script containing the chain definition, provides an input example, and logs the model using MLflow's langchain integration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/models-from-code/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_experiment(\"Landscaping\")\n\nchain_path = \"./mfc.py\"\n\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": {\n                \"region\": \"Austin, TX, USA\",\n                \"area\": \"1750 square feet\",\n            },\n        }\n    ]\n}\n\nwith mlflow.start_run():\n    info = mlflow.langchain.log_model(\n        lc_model=chain_path,  # Defining the model as the script containing the chain definition and the set_model call\n        artifact_path=\"chain\",\n        input_example=input_example,\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining a LangChain Model in a Separate Python File for MLflow Logging\nDESCRIPTION: This code defines a LangChain model in a separate Python file that can be logged by MLflow. It creates a homework helper chain that evaluates homework assignments using OpenAI, with functions to extract questions and answers from input data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# %%writefile chain.py\n\nimport os\nfrom operator import itemgetter\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_openai import OpenAI\n\nimport mlflow\n\nmlflow.set_experiment(\"Homework Helper\")\n\nmlflow.langchain.autolog()\n\nprompt = PromptTemplate(\n    template=\"You are a helpful tutor that evaluates my homework assignments and provides suggestions on areas for me to study further.\"\n    \" Here is the question: {question} and my answer which I got wrong: {answer}\",\n    input_variables=[\"question\", \"answer\"],\n)\n\n\ndef get_question(input):\n    default = \"What is your name?\"\n    if isinstance(input_data[0], dict):\n        return input_data[0].get(\"content\").get(\"question\", default)\n    return default\n\n\ndef get_answer(input):\n    default = \"My name is Bobo\"\n    if isinstance(input_data[0], dict):\n        return input_data[0].get(\"content\").get(\"answer\", default)\n    return default\n\n\nmodel = OpenAI(temperature=0.95)\n\nchain = (\n    {\n        \"question\": itemgetter(\"messages\") | RunnableLambda(get_question),\n        \"answer\": itemgetter(\"messages\") | RunnableLambda(get_answer),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nmlflow.models.set_model(chain)\n```\n\n----------------------------------------\n\nTITLE: Processing and Displaying Text Message Analysis with MLflow and GPT-4 in Python\nDESCRIPTION: This code snippet demonstrates how to use MLflow and OpenAI's GPT-4 to analyze text messages. It creates a DataFrame with sample messages, predicts responses using a pre-trained model, and formats the output for display. The code showcases the integration of MLflow for model management and GPT-4 for advanced text analysis.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-chat-completions.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nvalidation_data = pd.DataFrame(\n    {\n        \"text\": [\n            \"Wow, what an interesting dinner last night! I had no idea that you could use canned \"\n            \"cat food to make a meatloaf.\",\n            \"I'd rather book a 14th century surgical operation than go to the movies with you on Thursday.\",\n            \"Can't wait for the roadtrip this weekend! Love the playlist mixes that you choose!\",\n            \"Thanks for helping out with the move this weekend. I really appreciate it.\",\n            \"You know what part I love most when you sing? The end. It means its over.\",\n        ]\n    }\n)\n\nchat_completions_response = model.predict(\n    validation_data, params={\"max_tokens\": 50, \"temperature\": 0.2}\n)\n\nformatted_output = \"<br>\".join(\n    [f\"<p><strong>{line.strip()}</strong></p>\" for line in chat_completions_response]\n)\ndisplay(HTML(formatted_output))\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Apple Sales Data with Promotional Adjustments in Python\nDESCRIPTION: This function generates a synthetic dataset for apple sales demand prediction with multiple influencing factors including temperature, promotions, pricing, and seasonality. It creates realistic data patterns by incorporating effects like competitor pricing, marketing intensity, and stock availability to simulate real-world sales dynamics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport pathlib\nfrom datetime import datetime, timedelta\n\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    mean_squared_error,\n    mean_squared_log_error,\n    median_absolute_error,\n    r2_score,\n)\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n\n\ndef generate_apple_sales_data_with_promo_adjustment(\n    base_demand: int = 1000,\n    n_rows: int = 5000,\n    competitor_price_effect: float = -50.0,\n):\n    \"\"\"\n    Generates a synthetic dataset for predicting apple sales demand with multiple\n    influencing factors.\n\n    This function creates a pandas DataFrame with features relevant to apple sales.\n    The features include date, average_temperature, rainfall, weekend flag, holiday flag,\n    promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,\n    and the previous day's demand. The target variable, 'demand', is generated based on a\n    combination of these features with some added noise.\n\n    Args:\n        base_demand (int, optional): Base demand for apples. Defaults to 1000.\n        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.\n        competitor_price_effect (float, optional): Effect of competitor's price being lower\n                                                   on our sales. Defaults to -50.\n\n    Returns:\n        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.\n\n    Example:\n        >>> df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)\n        >>> df.head()\n    \"\"\"\n\n    # Set seed for reproducibility\n    np.random.seed(9999)\n\n    # Create date range\n    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]\n    dates.reverse()\n\n    # Generate features\n    df = pd.DataFrame(\n        {\n            \"date\": dates,\n            \"average_temperature\": np.random.uniform(10, 35, n_rows),\n            \"rainfall\": np.random.exponential(5, n_rows),\n            \"weekend\": [(date.weekday() >= 5) * 1 for date in dates],\n            \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),\n            \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),\n            \"month\": [date.month for date in dates],\n        }\n    )\n\n    # Introduce inflation over time (years)\n    df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03\n\n    # Incorporate seasonality due to apple harvests\n    df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(\n        2 * np.pi * (df[\"month\"] - 9) / 12\n    )\n\n    # Modify the price_per_kg based on harvest effect\n    df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5\n\n    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month\n    peak_months = [4, 10]  # months following the peak availability\n    df[\"promo\"] = np.where(\n        df[\"month\"].isin(peak_months),\n        1,\n        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),\n    )\n\n    # Generate target variable based on features\n    base_price_effect = -df[\"price_per_kg\"] * 50\n    seasonality_effect = df[\"harvest_effect\"] * 50\n    promo_effect = df[\"promo\"] * 200\n\n    df[\"demand\"] = (\n        base_demand\n        + base_price_effect\n        + seasonality_effect\n        + promo_effect\n        + df[\"weekend\"] * 300\n        + np.random.normal(0, 50, n_rows)\n    ) * df[\"inflation_multiplier\"]  # adding random noise\n\n    # Add previous day's demand\n    df[\"previous_days_demand\"] = df[\"demand\"].shift(1)\n    df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n\n    # Introduce competitor pricing\n    df[\"competitor_price_per_kg\"] = np.random.uniform(0.5, 3, n_rows)\n    df[\"competitor_price_effect\"] = (\n        df[\"competitor_price_per_kg\"] < df[\"price_per_kg\"]\n    ) * competitor_price_effect\n\n    # Stock availability based on past sales price (3 days lag with logarithmic decay)\n    log_decay = -np.log(df[\"price_per_kg\"].shift(3) + 1) + 2\n    df[\"stock_available\"] = np.clip(log_decay, 0.7, 1)\n\n    # Marketing intensity based on stock availability\n    # Identify where stock is above threshold\n    high_stock_indices = df[df[\"stock_available\"] > 0.95].index\n\n    # For each high stock day, increase marketing intensity for the next week\n    for idx in high_stock_indices:\n        df.loc[idx : min(idx + 7, n_rows - 1), \"marketing_intensity\"] = np.random.uniform(0.7, 1)\n\n    # If the marketing_intensity column already has values, this will preserve them;\n    #  if not, it sets default values\n    fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)\n    df[\"marketing_intensity\"].fillna(fill_values, inplace=True)\n\n    # Adjust demand with new factors\n    df[\"demand\"] = df[\"demand\"] + df[\"competitor_price_effect\"] + df[\"marketing_intensity\"]\n\n    # Drop temporary columns\n    df.drop(\n        columns=[\n            \"inflation_multiplier\",\n            \"harvest_effect\",\n            \"month\",\n            \"competitor_price_effect\",\n            \"stock_available\",\n        ],\n        inplace=True,\n    )\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Training, Logging, and Using XGBoost Model with MLflow\nDESCRIPTION: Trains an XGBoost classifier on the Iris dataset, logs the model and parameters to MLflow, then loads the saved model to make predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nimport mlflow\nfrom mlflow.models import infer_signature\n\ndata = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    data[\"data\"], data[\"target\"], test_size=0.2\n)\n\nxgb_classifier = XGBClassifier(\n    n_estimators=10,\n    max_depth=3,\n    learning_rate=1,\n    objective=\"binary:logistic\",\n    random_state=123,\n)\n\n# log fitted model and XGBClassifier parameters\nwith mlflow.start_run():\n    xgb_classifier.fit(X_train, y_train)\n    clf_params = xgb_classifier.get_xgb_params()\n    mlflow.log_params(clf_params)\n    signature = infer_signature(X_train, xgb_classifier.predict(X_train))\n    model_info = mlflow.xgboost.log_model(\n        xgb_classifier, \"iris-classifier\", signature=signature\n    )\n\n# Load saved model and make predictions\nxgb_classifier_saved = mlflow.pyfunc.load_model(model_info.model_uri)\ny_pred = xgb_classifier_saved.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Formatting Predictions for Readability in Jupyter Notebook\nDESCRIPTION: A utility function for formatting model predictions to enhance readability within a Jupyter Notebook environment. It splits text into sentences and formats them with proper punctuation and line breaks.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/text-generation/text-generation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef format_predictions(predictions):\n    \"\"\"\n    Function for formatting the output for readability in a Jupyter Notebook\n    \"\"\"\n    formatted_predictions = []\n\n    for prediction in predictions:\n        # Split the output into sentences, ensuring we don't split on abbreviations or initials\n        sentences = [\n            sentence.strip() + (\".\" if not sentence.endswith(\".\") else \"\")\n            for sentence in prediction.split(\". \")\n            if sentence\n        ]\n\n        # Join the sentences with a newline character\n        formatted_text = \"\\n\".join(sentences)\n\n        # Add the formatted text to the list\n        formatted_predictions.append(formatted_text)\n\n    return formatted_predictions\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Outputs with LangChain in MLflow\nDESCRIPTION: Example of setting up and using the predict_stream function with a LangChain model in MLflow. This demonstrates how to create a chain with OpenAI, log it to MLflow, and then generate streaming responses for real-time output handling.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/guide/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\nimport mlflow\n\n\ntemplate_instructions = \"Provide brief answers to technical questions about {topic} and do not answer non-technical questions.\"\nprompt = PromptTemplate(\n    input_variables=[\"topic\"],\n    template=template_instructions,\n)\nchain = LLMChain(llm=OpenAI(temperature=0.05), prompt=prompt)\n\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(chain, \"tech_chain\")\n\n# Assuming the model is already logged in MLflow and loaded\nloaded_model = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# Simulate a single synchronous input\ninput_data = \"Hello, can you explain streaming outputs?\"\n\n# Generate responses in a streaming fashion\nresponse_stream = loaded_model.predict_stream(input_data)\nfor response_part in response_stream:\n    print(\"Streaming Response Part:\", response_part)\n    # Each part of the response is handled as soon as it is generated\n```\n\n----------------------------------------\n\nTITLE: Logging PyTorch Model with MLflow\nDESCRIPTION: This snippet demonstrates how to log a PyTorch model using MLflow. It defines input and output schemas for the model signature, starts an MLflow run, and logs the model with the defined signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/guide/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninput_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 28, 28))])\noutput_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 10))])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\nwith mlflow.start_run() as run:\n    mlflow.pytorch.log_model(model, \"model\", signature=signature)\n```\n\n----------------------------------------\n\nTITLE: Creating Input Example for MLflow ChatModel Agent in Python\nDESCRIPTION: This snippet defines an input_example dictionary that demonstrates the expected input format for the model. It includes a sample user message asking for a simple scone recipe, which helps validate the model during logging and provides a reference in the MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is a good recipe for baking scones that doesn't require a lot of skill?\",\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Predicting with PyTorch Models in MLflow\nDESCRIPTION: Example showing how to use PyTorch models with MLflow's pyfunc interface. The code creates a minimal PyTorch linear model, trains it, logs it to MLflow with a signature, and then loads and makes predictions with the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport mlflow\nfrom mlflow.models import infer_signature\nimport torch\nfrom torch import nn\n\n\nnet = nn.Linear(6, 1)\nloss_function = nn.L1Loss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n\nX = torch.randn(6)\ny = torch.randn(1)\n\nepochs = 5\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = net(X)\n\n    loss = loss_function(outputs, y)\n    loss.backward()\n\n    optimizer.step()\n\nwith mlflow.start_run() as run:\n    signature = infer_signature(X.numpy(), net(X).detach().numpy())\n    model_info = mlflow.pytorch.log_model(net, \"model\", signature=signature)\n\npytorch_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\npredictions = pytorch_pyfunc.predict(torch.randn(6).numpy())\nprint(predictions)\n```\n\n----------------------------------------\n\nTITLE: Validating Fine-Tuned Model with Test Query Before MLflow Logging\nDESCRIPTION: Tests the fine-tuned model with a complex sample query to validate its performance before logging to MLflow. This validation step ensures the model is performing as expected and is ready for potential deployment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Perform a validation of our assembled pipeline that contains our fine-tuned model.\nquick_check = (\n    \"I have a question regarding the project development timeline and allocated resources; \"\n    \"specifically, how certain are you that John and Ringo can work together on writing this next song? \"\n    \"Do we need to get Paul involved here, or do you truly believe, as you said, 'nah, they got this'?\"\n)\n\ntuned_pipeline(quick_check)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing MNIST Dataset in Python\nDESCRIPTION: Loads the MNIST dataset using Keras, expands the dimensions of the input data, and visualizes a sample of the dataset using matplotlib.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/keras/quickstart/quickstart_keras.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train = np.expand_dims(x_train, axis=3)\nx_test = np.expand_dims(x_test, axis=3)\nx_train[0].shape\n```\n\nLANGUAGE: python\nCODE:\n```\n# Visualize Dataset\nimport matplotlib.pyplot as plt\n\ngrid = 3\nfig, axes = plt.subplots(grid, grid, figsize=(6, 6))\nfor i in range(grid):\n    for j in range(grid):\n        axes[i][j].imshow(x_train[i * grid + j])\n        axes[i][j].set_title(f\"label={y_train[i * grid + j]}\")\nplt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Loading and Predicting with Scikit-learn Models in MLflow\nDESCRIPTION: Example demonstrating how to use Scikit-learn models with MLflow's pyfunc interface. The code creates a LogisticRegression model, fits it to data, logs it to MLflow with a signature, and then loads and makes predictions with the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nwith mlflow.start_run():\n    X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n    y = np.array([0, 0, 1, 1, 1, 0])\n    lr = LogisticRegression()\n    lr.fit(X, y)\n    signature = infer_signature(X, lr.predict(X))\n\n    model_info = mlflow.sklearn.log_model(\n        sk_model=lr, artifact_path=\"model\", signature=signature\n    )\n\nsklearn_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\ndata = np.array([-4, 1, 0, 10, -2, 1]).reshape(-1, 1)\n\npredictions = sklearn_pyfunc.predict(data)\n```\n\n----------------------------------------\n\nTITLE: Evaluating a LangGraph Agent with mlflow.evaluate\nDESCRIPTION: This code shows how to evaluate a LangGraph agent using mlflow.evaluate. It defines a custom wrapper function to extract the last message from a chat sequence and uses metrics like answer correctness and latency for evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport pandas as pd\nfrom typing import List\n\n# Note that we assume the `model_uri` variable is present\n# Also note that registering and loading the model is optional and you\n# can simply leverage your langgraph object in the custom function.\nloaded_model = mlflow.langchain.load_model(model_uri)\n\neval_data = pd.DataFrame(\n    {\n        \"inputs\": [\n            \"What is MLflow?\",\n            \"What is Spark?\",\n        ],\n        \"ground_truth\": [\n            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\",\n            \"Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks\",\n        ],\n    }\n)\n\n\ndef custom_langgraph_wrapper(inputs: pd.DataFrame) -> List[str]:\n    \"\"\"Extract the predictions from a chat message sequence.\"\"\"\n    answers = []\n    for content in inputs[\"inputs\"]:\n        prediction = loaded_model.invoke(\n            {\"messages\": [{\"role\": \"user\", \"content\": content}]}\n        )\n        last_message_content = prediction[\"messages\"][-1].content\n        answers.append(last_message_content)\n\n    return answers\n\n\nwith mlflow.start_run() as run:\n    results = mlflow.evaluate(\n        custom_langgraph_wrapper,  # Pass our function defined above\n        data=eval_data,\n        targets=\"ground_truth\",\n        model_type=\"question-answering\",\n        extra_metrics=[\n            mlflow.metrics.latency(),\n            mlflow.metrics.genai.answer_correctness(\"openai:/gpt-4o\"),\n        ],\n    )\nprint(results.metrics)\n```\n\n----------------------------------------\n\nTITLE: Training and Logging CatBoost Model with MLflow\nDESCRIPTION: Shows how to train a CatBoost classifier using wine dataset, create model signature, and log it using MLflow. Includes loading the saved model for predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom catboost import CatBoostClassifier\nfrom sklearn import datasets\n\nX, y = datasets.load_wine(as_frame=False, return_X_y=True)\n\nmodel = CatBoostClassifier(\n    iterations=5,\n    loss_function=\"MultiClass\",\n    allow_writing_files=False,\n)\nmodel.fit(X, y)\n\npredictions = model.predict(X)\nsignature = infer_signature(X, predictions)\n\nwith mlflow.start_run():\n    model_info = mlflow.catboost.log_model(model, \"model\", signature=signature)\n\ncatboost_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\nprint(catboost_pyfunc.predict(X[:5]))\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading PyTorch Models with MLflow\nDESCRIPTION: Example showing how to save a PyTorch model to MLflow and load it back for inference. Includes model definition, logging, and prediction using the loaded model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/guide/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport numpy as np\n\nfrom torch import nn\n\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nmodel = NeuralNetwork()\n\nwith mlflow.start_run() as run:\n    mlflow.pytorch.log_model(model, \"model\")\n\nlogged_model = f\"runs:/{run.info.run_id}/model\"\nloaded_model = mlflow.pyfunc.load_model(logged_model)\nloaded_model.predict(np.random.uniform(size=[1, 28, 28]).astype(np.float32))\n```\n\n----------------------------------------\n\nTITLE: Creating and Testing Text Generation Pipelines with Different Prompt Templates\nDESCRIPTION: Demonstrates setting up a text generation pipeline using TinyLlama-1.1B and testing different prompt templates. It creates a pipeline, defines various prompt templates, and generates responses for a sample user input using each template format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/prompt-templating/prompt-templating.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n\nuser_input = \"Tell me the largest bird\"\nprompt_templates = [\n    # no template\n    \"{prompt}\",\n    # question-answer style template\n    \"Q: {prompt}\\nA:\",\n    # dialogue style template with a system prompt\n    (\n        \"You are an assistant that is knowledgeable about birds. \"\n        \"If asked about the largest bird, you will reply 'Duck'.\\n\"\n        \"User: {prompt}\\n\"\n        \"Assistant:\"\n    ),\n]\n\nresponses = generator(\n    [template.format(prompt=user_input) for template in prompt_templates], max_new_tokens=15\n)\nfor idx, response in enumerate(responses):\n    print(f\"Response to Template #{idx}:\")\n    print(response[0][\"generated_text\"] + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Tracing txtai Agent for Astronomy Research\nDESCRIPTION: Example of tracing a txtai agent designed for astronomy research using MLflow autologging. It sets up an embedding database, defines a search function, and creates an agent to research habitable stars.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/txtai.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom txtai import Agent, Embeddings\n\n# Enable MLflow auto-tracing for txtai\nmlflow.txtai.autolog()\n\n\ndef search(query):\n    \"\"\"\n    Searches a database of astronomy data.\n\n    Make sure to call this tool only with a string input, never use JSON.\n\n    Args:\n        query: concepts to search for using similarity search\n\n    Returns:\n        list of search results with for each match\n    \"\"\"\n\n    return embeddings.search(\n        \"SELECT id, text, distance FROM txtai WHERE similar(:query)\",\n        10,\n        parameters={\"query\": query},\n    )\n\n\nembeddings = Embeddings()\nembeddings.load(provider=\"huggingface-hub\", container=\"neuml/txtai-astronomy\")\n\nagent = Agent(\n    tools=[search],\n    llm=\"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\",\n    max_iterations=10,\n)\n\nresearcher = \"\"\"\n{command}\n\nDo the following.\n - Search for results related to the topic.\n - Analyze the results\n - Continue querying until conclusive answers are found\n - Write a Markdown report\n\"\"\"\n\nagent(\n    researcher.format(\n        command=\"\"\"\nWrite a detailed list with explanations of 10 candidate stars that could potentially be habitable to life.\n\"\"\"\n    ),\n    maxlength=16000,\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Running Hybrid RAG Workflow in Python\nDESCRIPTION: This snippet demonstrates how to instantiate the HybridRAGWorkflow with vector search and BM25 retrieval, and run it with a sample query.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Workflow with VS + BM25 retrieval\nfrom workflow.workflow import HybridRAGWorkflow\n\nworkflow = HybridRAGWorkflow(retrievers=[\"vector_search\", \"bm25\"], timeout=60)\nresponse = await workflow.run(query=\"Why use MLflow with LlamaIndex?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: MLflow Custom Logger for spaCy Training\nDESCRIPTION: Implements a custom MLflow logger for spaCy training to track metrics and save models. Includes setup for logging steps and model finalization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.loggers(\"mlflow_logger.v1\")\ndef mlflow_logger():\n    def setup_logger(\n        nlp: Language,\n        stdout: IO = sys.stdout,\n        stderr: IO = sys.stderr,\n    ) -> Tuple[Callable, Callable]:\n        def log_step(info: Optional[Dict[str, Any]]):\n            if info:\n                step = info[\"step\"]\n                score = info[\"score\"]\n                metrics = {}\n\n                for pipe_name in nlp.pipe_names:\n                    loss = info[\"losses\"][pipe_name]\n                    metrics[f\"{pipe_name}_loss\"] = loss\n                    metrics[f\"{pipe_name}_score\"] = score\n                mlflow.log_metrics(metrics, step=step)\n\n        def finalize():\n            uri = mlflow.spacy.log_model(nlp, \"mlflow_textcat_example\")\n            mlflow.end_run()\n\n        return log_step, finalize\n\n    return setup_logger\n```\n\n----------------------------------------\n\nTITLE: Logging and Using Ollama ChatModel with MLflow in Python\nDESCRIPTION: This snippet demonstrates how to log the Ollama ChatModel to MLflow, load it, and use it for prediction. It includes an example of passing custom parameters like max_tokens and seed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-intro/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncode_path = \"ollama_model.py\"\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        \"ollama_model\",\n        python_model=code_path,\n        input_example={\n            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n        },\n    )\n\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\nresult = loaded_model.predict(\n    data={\n        \"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}],\n        \"max_tokens\": 25,\n    }\n)\nprint(result)\n\nresult = loaded_model.predict(\n    data={\n        \"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}],\n        \"max_tokens\": 25,\n        \"custom_inputs\": {\"seed\": \"321\"},\n    }\n)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Defining and Logging a Model from Code\nDESCRIPTION: Example of how to create a custom PythonModel in a separate file and use the set_model API to define a model from code. This allows models to be defined and logged directly from a Python script without using pickle/cloudpickle.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import set_model\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return model_input\n\n\n# Define the custom PythonModel instance that will be used for inference\nset_model(MyModel())\n```\n\n----------------------------------------\n\nTITLE: Extracting Default Experiment Details in Python\nDESCRIPTION: This code extracts the name and lifecycle stage of the default experiment from the list of all experiments. It uses list comprehension and dictionary creation for data manipulation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\n\ndefault_experiment = [\n    {\"name\": experiment.name, \"lifecycle_stage\": experiment.lifecycle_stage}\n    for experiment in all_experiments\n    if experiment.name == \"Default\"\n][0]\n\npprint(default_experiment)\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Tool with MLflow ChatModel\nDESCRIPTION: Creates a custom ChatModel class that implements a weather information tool. The class defines a tool for getting weather data, implements the tool function, and handles the OpenAI API interaction with tool calling.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/notebooks/chat-model-tool-calling.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass WeatherModel(mlflow.pyfunc.ChatModel):\n    def __init__(self):\n        weather_tool = FunctionToolDefinition(\n            name=\"get_weather\",\n            description=\"Get weather information\",\n            parameters=ToolParamsSchema(\n                {\n                    \"city\": ParamProperty(\n                        type=\"string\",\n                        description=\"City name to get weather information for\",\n                    ),\n                }\n            ),\n        ).to_tool_definition()\n\n        self.tools = [weather_tool.to_dict()]\n\n    @mlflow.trace(span_type=SpanType.TOOL)\n    def get_weather(self, city: str) -> str:\n        return \"It's sunny in {}, with a temperature of 20C\".format(city)\n\n    @mlflow.trace(span_type=SpanType.AGENT)\n    def predict(self, context, messages: list[ChatMessage], params: ChatParams):\n        client = OpenAI()\n\n        messages = [m.to_dict() for m in messages]\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=messages,\n            tools=self.tools,\n        )\n\n        tool_calls = response.choices[0].message.tool_calls\n        if tool_calls:\n            print(\"Received a tool call, calling the weather tool...\")\n\n            city = json.loads(tool_calls[0].function.arguments)[\"city\"]\n            tool_call_id = tool_calls[0].id\n\n            tool_response = ChatMessage(\n                role=\"tool\", content=self.get_weather(city), tool_call_id=tool_call_id\n            ).to_dict()\n\n            messages.append(response.choices[0].message)\n            messages.append(tool_response)\n            response = client.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=messages,\n                tools=self.tools,\n            )\n\n        return ChatResponse.from_dict(response.to_dict())\n```\n\n----------------------------------------\n\nTITLE: Logging and Loading SentenceTransformer Model with MLflow\nDESCRIPTION: This snippet demonstrates how to log a SentenceTransformer model using MLflow, and then load it using two different methods: mlflow.pyfunc.load_model and mlflow.sentence_transformers.load_model. It also shows how to use the loaded models to generate embeddings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n# Log the model using mlflow\nwith mlflow.start_run():\n    logged_model = mlflow.sentence_transformers.log_model(\n        model=model,\n        artifact_path=\"sbert_model\",\n        signature=signature,\n        input_example=example_sentences,\n    )\n\n# Load option 1: mlflow.pyfunc.load_model returns a PyFuncModel\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\nembeddings1 = loaded_model.predict([\"hello world\", \"i am mlflow\"])\n\n# Load option 2: mlflow.sentence_transformers.load_model returns a SentenceTransformer\nloaded_model = mlflow.sentence_transformers.load_model(logged_model.model_uri)\nembeddings2 = loaded_model.encode([\"hello world\", \"i am mlflow\"])\n\nprint(embeddings1)\n\n\"\"\"\n>> [[-3.44772562e-02  3.10232025e-02  6.73496164e-03  2.61089969e-02\n  ...\n  2.37922110e-02 -2.28897743e-02  3.89375277e-02  3.02067865e-02]\n [ 4.81191138e-03 -9.33756605e-02  6.95968643e-02  8.09735525e-03\n  ...\n   6.57437667e-02 -2.72239652e-02  4.02687863e-02 -1.05599344e-01]]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Comprehensive MLflow LangChain Autologging Example\nDESCRIPTION: Full example demonstrating MLflow LangChain autologging with a custom chain, including model registration and loading.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom operator import itemgetter\n\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda\n\nimport mlflow\n\nassert (\n    \"OPENAI_API_KEY\" in os.environ\n), \"Please set the OPENAI_API_KEY environment variable.\"\n\n# Enable mlflow langchain autologging\nmlflow.langchain.autolog(\n    log_input_examples=True,\n    log_model_signatures=True,\n    log_models=True,\n    registered_model_name=\"lc_model\",\n)\n\nprompt_with_history_str = \"\"\"\nHere is a history between you and a human: {chat_history}\nNow, please answer this question: {question}\n\"\"\"\nprompt_with_history = PromptTemplate(\n    input_variables=[\"chat_history\", \"question\"], template=prompt_with_history_str\n)\n\n\ndef extract_question(input):\n    return input[-1][\"content\"]\n\n\ndef extract_history(input):\n    return input[:-1]\n\n\nllm = OpenAI(temperature=0.9)\n\n# Build a chain with LCEL\nchain_with_history = (\n    {\n        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n    }\n    | prompt_with_history\n    | llm\n    | StrOutputParser()\n)\n\ninputs = {\"messages\": [{\"role\": \"user\", \"content\": \"Who owns MLflow?\"}]}\n\nprint(chain_with_history.invoke(inputs))\n\n# Load the registered model\nmodel_name = \"lc_model\"\nmodel_version = 1\nloaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\nprint(loaded_model.predict(inputs))\n```\n\n----------------------------------------\n\nTITLE: Logging Sentence Transformer Model with MLflow\nDESCRIPTION: Logs the Sentence Transformer model to MLflow including its signature and input examples. This facilitates model lifecycle management, enables version tracking, and simplifies future deployment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    logged_model = mlflow.sentence_transformers.log_model(\n        model=model,\n        artifact_path=\"sbert_model\",\n        signature=signature,\n        input_example=example_sentences,\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging Model with Input Example in Python\nDESCRIPTION: This snippet shows how to log a custom PythonModel with an input example using MLflow. It demonstrates the creation of input_example.json and serving_input_example.json files.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        return model_input\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        artifact_path=\"model\",\n        input_example={\"question\": \"What is MLflow?\"},\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging and Evaluating Model with Custom Metrics and Artifacts in MLflow\nDESCRIPTION: Logs a scikit-learn model to MLflow and evaluates it using custom metrics and artifacts. The code demonstrates how to use make_metric to create custom evaluation metrics and specify custom artifacts for visualization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    mlflow.sklearn.log_model(lin_reg, \"model\", signature=signature)\n    model_uri = mlflow.get_artifact_uri(\"model\")\n    result = mlflow.evaluate(\n        model=model_uri,\n        data=eval_data,\n        targets=\"target\",\n        model_type=\"regressor\",\n        evaluators=[\"default\"],\n        extra_metrics=[\n            make_metric(\n                eval_fn=squared_diff_plus_one,\n                greater_is_better=False,\n            ),\n            make_metric(\n                eval_fn=sum_on_target_divided_by_two,\n                greater_is_better=True,\n            ),\n        ],\n        custom_artifacts=[prediction_target_scatter],\n    )\n\nprint(f\"metrics:\\n{result.metrics}\")\nprint(f\"artifacts:\\n{result.artifacts}\")\n```\n\n----------------------------------------\n\nTITLE: Experiment Tracking with MLflow and scikit-learn\nDESCRIPTION: Example of using MLflow's autologging feature with scikit-learn to automatically track experiments. This snippet loads a dataset, splits it, and trains a RandomForestRegressor while logging parameters and metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow's automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Loading Mistral-7B with 4-bit Quantization\nDESCRIPTION: Initializes the Mistral-7B model with 4-bit quantization using BitsAndBytesConfig. Configures double quantization, NormalFloat type, and bfloat16 compute dtype to optimize memory usage during training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    # Load the model with 4-bit quantization\n    load_in_4bit=True,\n    # Use double quantization\n    bnb_4bit_use_double_quant=True,\n    # Use 4-bit Normal Float for storing the base model weights in GPU memory\n    bnb_4bit_quant_type=\"nf4\",\n    # De-quantize the weights to 16-bit (Brain) float before the forward/backward pass\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=quantization_config)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow and Dependencies\nDESCRIPTION: Installs MLflow and required packages for PostgreSQL and S3 access using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow psycopg2 boto3\n```\n\n----------------------------------------\n\nTITLE: Logging MLflow ChatModel with Example Input\nDESCRIPTION: Defines input examples for the ChatModel, instantiates the model, and logs it to MLflow for tracking and later use. This creates a model artifact that can be served or loaded for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/notebooks/chat-model-tool-calling.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# messages to use as input examples\nmessages = [\n    {\"role\": \"system\", \"content\": \"Please use the provided tools to answer user queries.\"},\n    {\"role\": \"user\", \"content\": \"What's the weather in Singapore?\"},\n]\n\ninput_example = {\n    \"messages\": messages,\n}\n\n# instantiate the model\nmodel = WeatherModel()\n\n# log the model\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        artifact_path=\"weather-model\",\n        python_model=model,\n        input_example=input_example,\n    )\n\n    print(\"Successfully logged the model at the following URI: \", model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using a LangGraph Agent from MLflow\nDESCRIPTION: This code demonstrates how to load a LangGraph agent from MLflow and use it for inference. It shows how to format queries for the agent and invoke it to get responses.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nagent = mlflow.langchain.load_model(model_info.model_uri)\nquery = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Should I bring an umbrella today when I go to work in San Francisco?\",\n        }\n    ]\n}\nagent.invoke(query)\n```\n\n----------------------------------------\n\nTITLE: Registering an Existing MLflow Model\nDESCRIPTION: Shows how to register an existing model from a completed run using mlflow.register_model(). This requires the run ID and artifact path.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult = mlflow.register_model(\n    \"runs:/d16076a3ec534311817565e6527539c0/sklearn-model\", \"sk-learn-random-forest-reg\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Workflow Events for ReAct Agent\nDESCRIPTION: Creates custom event classes for the workflow that model the different stages of a ReAct agent's execution, including handling messages, prompting the LLM, processing LLM output, making tool calls, and handling tool outputs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# [USE IN MODEL]\n\n# Event definitions\nfrom llama_index.core.llms import ChatMessage, ChatResponse\nfrom llama_index.core.tools import ToolOutput, ToolSelection\nfrom llama_index.core.workflow import Event\n\n\nclass PrepEvent(Event):\n    \"\"\"An event to handle new messages and prepare the chat history\"\"\"\n\n\nclass LLMInputEvent(Event):\n    \"\"\"An event to prmopt the LLM with the react prompt (chat history)\"\"\"\n\n    input: list[ChatMessage]\n\n\nclass LLMOutputEvent(Event):\n    \"\"\"An event represents LLM generation\"\"\"\n\n    response: ChatResponse\n\n\nclass ToolCallEvent(Event):\n    \"\"\"An event to trigger tool calls, if any\"\"\"\n\n    tool_calls: list[ToolSelection]\n\n\nclass ToolOutputEvent(Event):\n    \"\"\"An event to handle the results of tool calls, if any\"\"\"\n\n    output: ToolOutput\n```\n\n----------------------------------------\n\nTITLE: Defining MLflow Model Signature for Sentence Transformer\nDESCRIPTION: Creates an MLflow model signature by inferring input and output formats using example sentences. This signature ensures clarity in data formats, facilitates model deployment, and prevents errors during inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nexample_sentences = [\"A sentence to encode.\", \"Another sentence to encode.\"]\n\n# Infer the signature of the custom model by providing an input example and the resultant prediction output.\n# We're not including any custom inference parameters in this example, but you can include them as a third argument\n# to infer_signature(), as you will see in the advanced tutorials for Sentence Transformers.\nsignature = mlflow.models.infer_signature(\n    model_input=example_sentences,\n    model_output=model.encode(example_sentences),\n)\n\n# Visualize the signature\nsignature\n```\n\n----------------------------------------\n\nTITLE: Logging a Fine-Tuned Transformers Model to MLflow\nDESCRIPTION: This snippet demonstrates how to log a fine-tuned Transformers model to MLflow. It uses mlflow.transformers.log_model to store the model along with its signature, input examples, and configuration parameters in the same run that was used for training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Log the pipeline to the existing training run\nwith mlflow.start_run(run_id=run.info.run_id):\n    model_info = mlflow.transformers.log_model(\n        transformers_model=tuned_pipeline,\n        artifact_path=\"fine_tuned\",\n        signature=signature,\n        input_example=[\"Pass in a string\", \"And have it mark as spam or not.\"],\n        model_config=model_config,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment for MLflow and Transformers in Python\nDESCRIPTION: Sets up the environment by disabling tokenizers warnings and filtering out specific UserWarnings. This ensures a cleaner output when working with Transformers and MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/text-generation/text-generation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Keras Model with MLflow Logging in Python\nDESCRIPTION: Demonstrates how to use MLflow callback during model evaluation to log the test set performance metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/keras/quickstart/quickstart_keras.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    model.evaluate(x_test, y_test, callbacks=[mlflow.keras_core.MlflowCallback(run)])\n```\n\n----------------------------------------\n\nTITLE: Loading PEFT Model from MLflow Run\nDESCRIPTION: This snippet demonstrates how to load a saved PEFT model from a specific MLflow run using the mlflow.pyfunc.load_model() function. It requires the run ID of the MLflow experiment where the model was logged.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmlflow_model = mlflow.pyfunc.load_model(\"runs:/YOUR_RUN_ID/model\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Code Review Function with MLflow\nDESCRIPTION: Implements a straightforward review function that takes a function and an MLflow model as inputs. It extracts the source code of the function and uses the model to provide feedback, with error handling for exceptions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef review(func, model):\n    \"\"\"\n    Function to review the source code of a given function using a specified MLflow model.\n\n    Args:\n    func (function): The function to review.\n    model (MLflow pyfunc model): The MLflow pyfunc model used for evaluation.\n\n    Returns:\n    The model's prediction or an error message.\n    \"\"\"\n    try:\n        # Extracting the source code of the function\n        source_code = inspect.getsource(func)\n\n        # Using the model to predict/evaluate the source code\n        prediction = model.predict([source_code])\n        print(prediction)\n    except Exception as e:\n        # Handling any exceptions that occur and returning an error message\n        return f\"Error during model prediction or source code inspection: {e}\"\n```\n\n----------------------------------------\n\nTITLE: Defining Custom AddN MLflow Python Model Class\nDESCRIPTION: Creates a custom model class that inherits from mlflow.pyfunc.PythonModel to add a specified numeric value to all DataFrame columns. Implements initialization and prediction methods with full documentation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/introduction.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow.pyfunc\n\n\nclass AddN(mlflow.pyfunc.PythonModel):\n    \"\"\"\n    A custom model that adds a specified value `n` to all columns of the input DataFrame.\n\n    Attributes:\n    -----------\n    n : int\n        The value to add to input columns.\n    \"\"\"\n\n    def __init__(self, n):\n        \"\"\"\n        Constructor method. Initializes the model with the specified value `n`.\n\n        Parameters:\n        -----------\n        n : int\n            The value to add to input columns.\n        \"\"\"\n        self.n = n\n\n    def predict(self, context, model_input, params=None):\n        \"\"\"\n        Prediction method for the custom model.\n\n        Parameters:\n        -----------\n        context : Any\n            Ignored in this example. It's a placeholder for additional data or utility methods.\n\n        model_input : pd.DataFrame\n            The input DataFrame to which `n` should be added.\n\n        params : dict, optional\n            Additional prediction parameters. Ignored in this example.\n\n        Returns:\n        --------\n        pd.DataFrame\n            The input DataFrame with `n` added to all columns.\n        \"\"\"\n        return model_input.apply(lambda column: column + self.n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Ollama ChatModel in Python for MLflow\nDESCRIPTION: This code defines a custom MLflow ChatModel implementation that interfaces with Ollama. It handles model initialization, context loading, and message conversion between MLflow's ChatMessage format and Ollama's expected input format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-intro/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# %%writefile ollama_model.py\nfrom mlflow.pyfunc import ChatModel\nfrom mlflow.types.llm import ChatMessage, ChatCompletionResponse, ChatChoice\nfrom mlflow.models import set_model\nimport ollama\n\n\nclass SimpleOllamaModel(ChatModel):\n    def __init__(self):\n        self.model_name = \"llama3.2:1b\"\n        self.client = None\n\n    def load_context(self, context):\n        self.client = ollama.Client()\n\n    def predict(self, context, messages, params=None):\n        # Prepare the messages for Ollama\n        ollama_messages = [msg.to_dict() for msg in messages]\n\n        # Call Ollama\n        response = self.client.chat(model=self.model_name, messages=ollama_messages)\n\n        # Prepare and return the ChatCompletionResponse\n        return ChatCompletionResponse(\n            choices=[{\"index\": 0, \"message\": response[\"message\"]}],\n            model=self.model_name,\n        )\n\n\nset_model(SimpleOllamaModel())\n```\n\n----------------------------------------\n\nTITLE: Loading and Testing RAG Model in Python\nDESCRIPTION: This snippet demonstrates how to load the stored RAG model from MLflow and test it with a sample query. It includes a helper function for formatting and printing responses, showcasing how to interact with the loaded model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\ndef print_formatted_response(response_list, max_line_length=80):\n    \"\"\"\n    Formats and prints responses with a maximum line length for better readability.\n\n    Args:\n    response_list (list): A list of strings representing responses.\n    max_line_length (int): Maximum number of characters in a line. Defaults to 80.\n    \"\"\"\n    for response in response_list:\n        words = response.split()\n        line = \"\"\n        for word in words:\n            if len(line) + len(word) + 1 <= max_line_length:\n                line += word + \" \"\n            else:\n                print(line)\n                line = word + \" \"\n        print(line)\n\nanswer1 = loaded_model.predict([{\"query\": \"What does the document say about trespassers?\"}])\n\nprint_formatted_response(answer1)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Environment Variables for MLflow Retriever Evaluation\nDESCRIPTION: Imports required libraries and sets environment variables for retriever evaluation. It includes OpenAI API key setup and defines constants for file paths and chunk size.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ast\nimport os\nimport pprint\n\nimport pandas as pd\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\nimport mlflow\n\nos.environ[\"OPENAI_API_KEY\"] = \"<redacted>\"\n\nCHUNK_SIZE = 1000\n\n# Assume running from https://github.com/mlflow/mlflow/blob/master/examples/llms/rag\nOUTPUT_DF_PATH = \"question_answer_source.csv\"\nSCRAPPED_DOCS_PATH = \"mlflow_docs_scraped.csv\"\nEVALUATION_DATASET_PATH = \"static_evaluation_dataset.csv\"\nDB_PERSIST_DIR = \"faiss_index\"\n```\n\n----------------------------------------\n\nTITLE: Logging SentenceTransformer Model in MLflow\nDESCRIPTION: This snippet demonstrates how to log a SentenceTransformer model in MLflow. It includes loading a pre-trained model, defining example sentences, inferring the model signature, and logging the model with MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nimport mlflow\nimport mlflow.sentence_transformers\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\nexample_sentences = [\"This is a sentence.\", \"This is another sentence.\"]\n\n# Define the signature\nsignature = mlflow.models.infer_signature(\n    model_input=example_sentences,\n    model_output=model.encode(example_sentences),\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parent-Child Run Structure in MLflow for Hyperparameter Tuning\nDESCRIPTION: This snippet shows how to organize hyperparameter tuning experiments using MLflow's parent-child run structure. It includes enhanced functions for logging parameters, metrics, and tags, while nesting child runs under parent runs for better organization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define a function to log parameters and metrics and add tag\n# logging for search_runs functionality\ndef log_run(run_name, test_no, param1_choices, param2_choices, tag_ident):\n    with mlflow.start_run(run_name=run_name, nested=True):\n        mlflow.log_param(\"param1\", random.choice(param1_choices))\n        mlflow.log_param(\"param2\", random.choice(param2_choices))\n        mlflow.log_metric(\"metric1\", random.uniform(0, 1))\n        mlflow.log_metric(\"metric2\", abs(random.gauss(5, 2.5)))\n        mlflow.set_tag(\"test_identifier\", tag_ident)\n\n\n# Generate run names\ndef generate_run_names(test_no, num_runs=5):\n    return (f\"run_{i}_test_{test_no}\" for i in range(num_runs))\n\n\n# Execute tuning function, allowing for param overrides,\n# run_name disambiguation, and tagging support\ndef execute_tuning(\n    test_no, param1_choices=(\"a\", \"b\", \"c\"), param2_choices=(\"d\", \"e\", \"f\"), test_identifier=\"\"\n):\n    ident = \"default\" if not test_identifier else test_identifier\n    # Use a parent run to encapsulate the child runs\n    with mlflow.start_run(run_name=f\"parent_run_test_{ident}_{test_no}\"):\n        # Partial application of the log_run function\n        log_current_run = partial(\n            log_run,\n            test_no=test_no,\n            param1_choices=param1_choices,\n            param2_choices=param2_choices,\n            tag_ident=ident,\n        )\n        mlflow.set_tag(\"test_identifier\", ident)\n        # Generate run names and apply log_current_run function to each run name\n        runs = starmap(log_current_run, ((run_name,) for run_name in generate_run_names(test_no)))\n        # Consume the iterator to execute the runs\n        consume(runs)\n\n\n# Set the tracking uri and experiment\nmlflow.set_tracking_uri(\"http://localhost:8080\")\nmlflow.set_experiment(\"Nested Child Association\")\n\n# Define custom parameters\nparam_1_values = [\"x\", \"y\", \"z\"]\nparam_2_values = [\"u\", \"v\", \"w\"]\n\n# Execute hyperparameter tuning runs with custom parameter choices\nconsume(starmap(execute_tuning, ((x, param_1_values, param_2_values) for x in range(5))))\n```\n\n----------------------------------------\n\nTITLE: Autologging Tensorflow Experiments with MLflow\nDESCRIPTION: Demonstrates how to enable autologging for Tensorflow experiments using MLflow. It sets up a simple Keras model, compiles it, and fits it while automatically logging metrics and parameters to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/guide/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nmlflow.tensorflow.autolog()\n\n# Prepare data for a 2-class classification.\ndata = np.random.uniform(size=[20, 28, 28, 3])\nlabel = np.random.randint(2, size=20)\n\nmodel = keras.Sequential(\n    [\n        keras.Input([28, 28, 3]),\n        keras.layers.Conv2D(8, 2),\n        keras.layers.MaxPool2D(2),\n        keras.layers.Flatten(),\n        keras.layers.Dense(2),\n        keras.layers.Softmax(),\n    ]\n)\n\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(0.001),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nwith mlflow.start_run():\n    model.fit(data, label, batch_size=5, epochs=2)\n```\n\n----------------------------------------\n\nTITLE: Logging MLflow PythonModel\nDESCRIPTION: Example of logging a custom PythonModel to MLflow, including setting up input examples and parameters for model signature inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-intro/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncode_path = \"ollama_pyfunc_model.py\"\n\nparams = {\n    \"max_tokens\": 25,\n    \"temperature\": 0.5,\n    \"top_p\": 0.5,\n    \"stop\": [\"\\n\"],\n    \"seed\": 123,\n}\nrequest = {\"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}]}\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        \"ollama_pyfunc_model\",\n        python_model=code_path,\n        input_example=(request, params),\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating an MLflow Experiment for Sentence Transformers\nDESCRIPTION: Sets up a new MLflow Experiment specifically for Sentence Transformers to organize runs in a contextually relevant way rather than using the default experiment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(\"Introduction to Sentence Transformers\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Predicting with Keras Models in MLflow\nDESCRIPTION: Example configuration showing how to use Keras models with MLflow's pyfunc interface. The code demonstrates creating a minimal Sequential model, training it, saving it with MLflow, and then loading and making predictions with the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport numpy as np\nimport pathlib\nimport shutil\nfrom tensorflow import keras\n\nmlflow.tensorflow.autolog()\n\nX = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\ny = np.array([0, 0, 1, 1, 1, 0])\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=(1,)),\n        keras.layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(X, y, batch_size=3, epochs=5, validation_split=0.2)\n\nlocal_artifact_dir = \"/tmp/mlflow/keras_model\"\npathlib.Path(local_artifact_dir).mkdir(parents=True, exist_ok=True)\n\nmodel_uri = f\"runs:/{mlflow.last_active_run().info.run_id}/model\"\nkeras_pyfunc = mlflow.pyfunc.load_model(\n    model_uri=model_uri, dst_path=local_artifact_dir\n)\n\ndata = np.array([-4, 1, 0, 10, -2, 1]).reshape(-1, 1)\npredictions = keras_pyfunc.predict(data)\n\nshutil.rmtree(local_artifact_dir)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Importing Libraries\nDESCRIPTION: Imports required libraries, sets up the OpenAI API key, and verifies its presence in the environment variables.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nfrom llama_index.core import Document, VectorStoreIndex\nfrom llama_index.core.llms import ChatMessage\n\nimport mlflow\n\nos.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\nLANGUAGE: python\nCODE:\n```\nassert \"OPENAI_API_KEY\" in os.environ, \"Please set the OPENAI_API_KEY environment variable.\"\n```\n\n----------------------------------------\n\nTITLE: Advanced Example: Function Calling Agent with DeepSeek and MLflow Tracing\nDESCRIPTION: This advanced example implements a function calling agent using DeepSeek and MLflow Tracing, demonstrating how to capture function calling responses and create spans for tool executions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/deepseek.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom openai import OpenAI\nimport mlflow\nfrom mlflow.entities import SpanType\n\n# Initialize the OpenAI client with DeepSeek API endpoint\nclient = OpenAI(base_url=\"https://api.deepseek.com\", api_key=\"<your_deepseek_api_key>\")\n\n\n# Define the tool function. Decorate it with `@mlflow.trace` to create a span for its execution.\n@mlflow.trace(span_type=SpanType.TOOL)\ndef get_weather(city: str) -> str:\n    if city == \"Tokyo\":\n        return \"sunny\"\n    elif city == \"Paris\":\n        return \"rainy\"\n    return \"unknown\"\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"city\": {\"type\": \"string\"}},\n            },\n        },\n    }\n]\n\n_tool_functions = {\"get_weather\": get_weather}\n\n\n# Define a simple tool calling agent\n@mlflow.trace(span_type=SpanType.AGENT)\ndef run_tool_agent(question: str):\n    messages = [{\"role\": \"user\", \"content\": question}]\n\n    # Invoke the model with the given question and available tools\n    response = client.chat.completions.create(\n        model=\"deepseek-chat\",\n        messages=messages,\n        tools=tools,\n    )\n\n    ai_msg = response.choices[0].message\n    messages.append(ai_msg)\n\n    # If the model request tool call(s), invoke the function with the specified arguments\n    if tool_calls := ai_msg.tool_calls:\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            if tool_func := _tool_functions.get(function_name):\n                args = json.loads(tool_call.function.arguments)\n                tool_result = tool_func(**args)\n            else:\n                raise RuntimeError(\"An invalid tool is returned from the assistant!\")\n\n            messages.append(\n                {\n                    \"role\": \"tool\",\n                    \"tool_call_id\": tool_call.id,\n                    \"content\": tool_result,\n                }\n            )\n\n        # Sent the tool results to the model and get a new response\n        response = client.chat.completions.create(\n            model=\"deepseek-chat\", messages=messages\n        )\n\n    return response.choices[0].message.content\n\n\n# Run the tool calling agent\nquestion = \"What's the weather like in Paris today?\"\nanswer = run_tool_agent(question)\n```\n\n----------------------------------------\n\nTITLE: Environment Setup and Warning Configuration in Python\nDESCRIPTION: Sets up the environment by disabling tokenizer parallelism and filtering warnings. This ensures clean execution of the ML pipeline.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-similarity/semantic-similarity-sentence-transformers.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Training with Custom MLflow Callback\nDESCRIPTION: Trains the model using the custom MLflow callback to log custom metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    run_id = run.info.run_id\n    model.fit(\n        x=train_ds,\n        epochs=2,\n        callbacks=[MlflowCustomCallback(run)],\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced MLflow Logging for Hyperparameter Tuning in Python\nDESCRIPTION: This comprehensive snippet shows the implementation of advanced MLflow logging for hyperparameter tuning. It includes functions for logging individual runs, generating run names, and executing tuning with best metric and parameter logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Define a function to log parameters and metrics and add tag\n# logging for search_runs functionality\ndef log_run(run_name, test_no, param1_choices, param2_choices, tag_ident):\n    with mlflow.start_run(run_name=run_name, nested=True) as run:\n        param1 = random.choice(param1_choices)\n        param2 = random.choice(param2_choices)\n        metric1 = random.uniform(0, 1)\n        metric2 = abs(random.gauss(5, 2.5))\n\n        mlflow.log_param(\"param1\", param1)\n        mlflow.log_param(\"param2\", param2)\n        mlflow.log_metric(\"metric1\", metric1)\n        mlflow.log_metric(\"metric2\", metric2)\n        mlflow.set_tag(\"test_identifier\", tag_ident)\n\n        return run.info.run_id, metric1, param1, param2\n\n\n# Generate run names\ndef generate_run_names(test_no, num_runs=5):\n    return (f\"run_{i}_test_{test_no}\" for i in range(num_runs))\n\n\n# Execute tuning function, allowing for param overrides,\n# run_name disambiguation, and tagging support\ndef execute_tuning(\n    test_no,\n    param1_choices=(\"a\", \"b\", \"c\"),\n    param2_choices=(\"d\", \"e\", \"f\"),\n    test_identifier=\"\",\n    num_child_runs=5,\n):\n    ident = \"default\" if not test_identifier else test_identifier\n    best_metric1 = float(\"inf\")\n    best_params = None\n    # Use a parent run to encapsulate the child runs\n    with mlflow.start_run(run_name=f\"parent_run_test_{ident}_{test_no}\"):\n        # Partial application of the log_run function\n        log_current_run = partial(\n            log_run,\n            test_no=test_no,\n            param1_choices=param1_choices,\n            param2_choices=param2_choices,\n            tag_ident=ident,\n        )\n        mlflow.set_tag(\"test_identifier\", ident)\n        # Generate run names and apply log_current_run function to each run name\n        results = list(\n            starmap(\n                log_current_run,\n                ((run_name,) for run_name in generate_run_names(test_no, num_child_runs)),\n            )\n        )\n\n        for _, metric1, param1, param2 in results:\n            if metric1 < best_metric1:\n                best_metric1 = metric1\n                best_params = (param1, param2)\n\n        mlflow.log_metric(\"best_metric1\", best_metric1)\n        mlflow.log_param(\"best_param1\", best_params[0])\n        mlflow.log_param(\"best_param2\", best_params[1])\n        # Consume the iterator to execute the runs\n        consume(results)\n\n\n# Set the tracking uri and experiment\nmlflow.set_tracking_uri(\"http://localhost:8080\")\nmlflow.set_experiment(\"Parent Child Association Challenge\")\n\nparam_1_values = [\"a\", \"b\"]\nparam_2_values = [\"d\", \"f\"]\n\n# Execute hyperparameter tuning runs with custom parameter choices\nconsume(\n    starmap(\n        execute_tuning, ((x, param_1_values, param_2_values, \"subset_test\", 25) for x in range(5))\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Loading TensorFlow Model from MLflow Run and Making Predictions in Python\nDESCRIPTION: This code loads a TensorFlow model from a specific MLflow run using the run ID and a save path. It then demonstrates how to use the loaded model to make predictions on random input data. The snippet emphasizes that a model signature is not necessary for loading and inference, but it's a good practice for model understanding.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/guide/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Load back the model.\nloaded_model = mlflow.tensorflow.load_model(f\"runs:/{run.info.run_id}/{save_path}\")\n\nprint(loaded_model.predict(tf.random.uniform([1, 28, 28, 3])))\n```\n\n----------------------------------------\n\nTITLE: Logging and Loading Custom MLflow ChatModel Agent in Python\nDESCRIPTION: This code demonstrates how to log a custom ChatModel agent using MLflow, including the model configuration and input example. It then shows how to load the model and make a prediction using a sample input.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        \"model\",\n        # If needed, update `python_model` to the Python file containing your agent code\n        python_model=\"basic_agent.py\",\n        model_config=model_config,\n        input_example=input_example,\n    )\n\nloaded = mlflow.pyfunc.load_model(model_info.model_uri)\n\nresponse = loaded.predict(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What is the best material to make a baseball bat out of?\",\n            }\n        ]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model and Inferring Signature for MLflow\nDESCRIPTION: This code defines model configuration parameters and infers the model signature using MLflow. It specifies batch size as a configurable parameter and uses MLflow's infer_signature function to automatically determine the input and output schema of the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Define a set of parameters that we would like to be able to flexibly override at inference time, along with their default values\nmodel_config = {\"batch_size\": 8}\n\n# Infer the model signature, including a representative input, the expected output, and the parameters that we would like to be able to override at inference time.\nsignature = mlflow.models.infer_signature(\n    [\"This is a test!\", \"And this is also a test.\"],\n    mlflow.transformers.generate_signature_output(\n        tuned_pipeline, [\"This is a test response!\", \"So is this.\"]\n    ),\n    params=model_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Logging a Transformers Pipeline with Prompt Template in MLflow\nDESCRIPTION: Logs a Transformers pipeline model to MLflow with a prompt template. This code sets up the tracking URI, creates an experiment, and logs the model with a specified prompt template, signature, and input example. It also includes an optional commented-out parameter for reference-only model saving.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/prompt-templating/prompt-templating.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\nmlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n\n# Set a name for the experiment that is indicative of what the runs being created within it are in regards to\nmlflow.set_experiment(\"prompt-templating\")\n\nprompt_template = \"Q: {prompt}\\nA:\"\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=generator,\n        artifact_path=\"model\",\n        task=\"text-generation\",\n        signature=signature,\n        input_example=\"Tell me the largest bird\",\n        prompt_template=prompt_template,\n        # Since MLflow 2.11.0, you can save the model in 'reference-only' mode to reduce storage usage by not saving\n        # the base model weights but only the reference to the HuggingFace model hub. To enable this, uncomment the\n        # following line:\n        # save_pretrained=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Auto-tracing for Anthropic\nDESCRIPTION: This snippet demonstrates how to enable automatic tracing for Anthropic LLMs using MLflow. It sets up the MLflow environment and configures the Anthropic client.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/anthropic.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport anthropic\nimport mlflow\n\n# Enable auto-tracing for Anthropic\nmlflow.anthropic.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"Anthropic\")\n\n# Configure your API key.\nclient = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\n# Use the create method to create new message.\nmessage = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Creating a Translation Pipeline with Transformers\nDESCRIPTION: Imports necessary libraries and initializes a translation pipeline for English to French using the google/flan-t5-base model. Creates a complete translation pipeline combining the T5ForConditionalGeneration model and T5TokenizerFast tokenizer.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\n\nimport mlflow\n\nmodel_architecture = \"google/flan-t5-base\"\n\ntranslation_pipeline = transformers.pipeline(\n    task=\"translation_en_to_fr\",\n    model=transformers.T5ForConditionalGeneration.from_pretrained(\n        model_architecture, max_length=1000\n    ),\n    tokenizer=transformers.T5TokenizerFast.from_pretrained(model_architecture, return_tensors=\"pt\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Logging a Scikit-learn Model with Inferred Signature in MLflow\nDESCRIPTION: This example demonstrates how to automatically infer and store a model signature for a RandomForestClassifier trained on the Iris dataset. The signature is derived from an input example taken from the training dataset.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\nimport mlflow\n\niris = datasets.load_iris()\niris_train = pd.DataFrame(iris.data, columns=iris.feature_names)\nclf = RandomForestClassifier(max_depth=7, random_state=0)\n\nwith mlflow.start_run():\n    clf.fit(iris_train, iris.target)\n    # Take the first row of the training dataset as the model input example.\n    input_example = iris_train.iloc[[0]]\n    # The signature is automatically inferred from the input example and its predicted output.\n    mlflow.sklearn.log_model(clf, \"iris_rf\", input_example=input_example)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules for MLflow Tracking with scikit-learn\nDESCRIPTION: Imports the necessary Python modules for MLflow tracking and scikit-learn model training. This includes MLflow for tracking, scikit-learn's RandomForestRegressor for modeling, and evaluation metrics functions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step6-logging-a-run/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n```\n\n----------------------------------------\n\nTITLE: Implementing a VADER Sentiment Analysis PythonModel\nDESCRIPTION: Creates a custom MLflow PythonModel class that wraps the VADER sentiment analysis functionality. This class encapsulates the sentiment analyzer and defines methods for scoring text input.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nclass SocialMediaAnalyserModel(mlflow.pyfunc.PythonModel):\n    def __init__(self):\n        super().__init__()\n        # embed your vader model instance\n        self._analyser = SentimentIntensityAnalyzer()\n\n    # preprocess the input with prediction from the vader sentiment model\n    def _score(self, txt):\n        prediction_scores = self._analyser.polarity_scores(txt)\n        return prediction_scores\n\n    def predict(self, context, model_input, params=None):\n        # Apply the preprocess function from the vader model to score\n        model_output = model_input.apply(lambda col: self._score(col))\n        return model_output\n```\n\n----------------------------------------\n\nTITLE: Creating and Logging a PandasDataset in MLflow\nDESCRIPTION: This snippet demonstrates how to create a PandasDataset from a CSV file and log it to an MLflow run. It includes loading data from a URL, creating a dataset object, and logging it using mlflow.log_input.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/dataset/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow.data\nimport pandas as pd\nfrom mlflow.data.pandas_dataset import PandasDataset\n\n\ndataset_source_url = \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\"\nraw_data = pd.read_csv(dataset_source_url, delimiter=\";\")\n\n# Create an instance of a PandasDataset\ndataset = mlflow.data.from_pandas(\n    raw_data, source=dataset_source_url, name=\"wine quality - white\", targets=\"quality\"\n)\n\n# Log the Dataset to an MLflow run by using the `log_input` API\nwith mlflow.start_run() as run:\n    mlflow.log_input(dataset, context=\"training\")\n\n# Retrieve the run information\nlogged_run = mlflow.get_run(run.info.run_id)\n\n# Retrieve the Dataset object\nlogged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n\n# View some of the recorded Dataset information\nprint(f\"Dataset name: {logged_dataset.name}\")\nprint(f\"Dataset digest: {logged_dataset.digest}\")\nprint(f\"Dataset profile: {logged_dataset.profile}\")\nprint(f\"Dataset schema: {logged_dataset.schema}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Working Environment and FAISS Database in Python\nDESCRIPTION: This snippet demonstrates the setup process for a RAG application, including creating a temporary directory, defining paths, fetching documents from URLs, and creating a FAISS database. It uses the previously defined functions to streamline the process.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntemporary_directory = tempfile.mkdtemp()\n\ndoc_path = os.path.join(temporary_directory, \"docs.txt\")\npersist_dir = os.path.join(temporary_directory, \"faiss_index\")\n\nurl_listings = [\n    \"https://www.archives.gov/milestone-documents/act-establishing-yellowstone-national-park#transcript\",\n    \"https://www.archives.gov/milestone-documents/sherman-anti-trust-act#transcript\",\n]\n\nfetch_and_save_documents(url_listings, doc_path)\n\nvector_db = create_faiss_database(doc_path, persist_dir)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using a Logged LangChain Model for Inference\nDESCRIPTION: This code shows how to load a previously logged LangChain model from MLflow and use it for inference. It demonstrates loading the model using the model URI returned from the logging step and invoking it with new input data to generate landscape design recommendations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/models-from-code/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Load the model and run inference\nlandscape_chain = mlflow.langchain.load_model(model_uri=info.model_uri)\n\nquestion = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": {\n                \"region\": \"Raleigh, North Carolina USA\",\n                \"area\": \"3850 square feet\",\n            },\n        },\n    ]\n}\n\nresponse = landscape_chain.invoke(question)\n```\n\n----------------------------------------\n\nTITLE: Logging Custom LLM Model to MLflow with Artifacts and Dependencies\nDESCRIPTION: Code that logs the custom MPT model to MLflow with appropriate artifacts, dependencies, input example, and signature. It captures the current torch version and ensures all required dependencies are specified.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Get the current base version of torch that is installed, without specific version modifiers\ntorch_version = torch.__version__.split(\"+\")[0]\n\n# Start an MLflow run context and log the MPT-7B model wrapper along with the param-included signature to\n# allow for overriding parameters at inference time\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        \"mpt-7b-instruct\",\n        python_model=MPT(),\n        # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context() method in our MPT() class.\n        artifacts={\"snapshot\": snapshot_location},\n        pip_requirements=[\n            f\"torch=={torch_version}\",\n            f\"transformers=={transformers.__version__}\",\n            f\"accelerate=={accelerate.__version__}\",\n            \"einops\",\n            \"sentencepiece\",\n        ],\n        input_example=input_example,\n        signature=signature,\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating New MLflow Experiment with Tags in Python\nDESCRIPTION: This snippet demonstrates how to create a new MLflow experiment with metadata tags. It sets up an experiment for a grocery forecasting project, specifically for apple models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nexperiment_description = (\n    \"This is the grocery forecasting project. \"\n    \"This experiment contains the produce models for apples.\"\n)\n\nexperiment_tags = {\n    \"project_name\": \"grocery-forecasting\",\n    \"store_dept\": \"produce\",\n    \"team\": \"stores-ml\",\n    \"project_quarter\": \"Q3-2023\",\n    \"mlflow.note.content\": experiment_description,\n}\n\nproduce_apples_experiment = client.create_experiment(name=\"Apple_Models\", tags=experiment_tags)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Retrieval Models with MLflow by Specifying Model Type\nDESCRIPTION: This snippet demonstrates how to evaluate a retrieval model by specifying the model_type parameter. It sets up the evaluation with default evaluators and a custom retriever_k configuration parameter to control the number of top results to consider.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Case 1: Specifying the model type\nevaluate_results = mlflow.evaluate(\n    data=data,\n    model_type=\"retriever\",\n    targets=\"ground_truth_context\",\n    predictions=\"retrieved_context\",\n    evaluators=\"default\",\n    evaluator_config={\"retriever_k\": 5}\n  )\n```\n\n----------------------------------------\n\nTITLE: Saving Custom MLflow PyFunc Model with Artifacts and Requirements\nDESCRIPTION: Saves a custom MLflow PyFunc model with specified artifacts, signature, and pip requirements. This ensures the model can be loaded with all necessary dependencies and retains the ability to use dynamic prediction methods.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npyfunc_path = \"/tmp/dynamic_regressor\"\n\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.save_model(\n        path=pyfunc_path,\n        python_model=ModelWrapper(),\n        input_example=x_train,\n        signature=signature,\n        artifacts=artifacts,\n        pip_requirements=[\"joblib\", \"sklearn\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating a Faithfulness Metric with Llama2 as Judge\nDESCRIPTION: Defines a faithfulness metric using Databricks-hosted Llama2-70b-chat model as a judge, with example inputs, outputs, and scores to guide the evaluation process.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create a good and bad example for faithfulness in the context of this problem\nfaithfulness_examples = [\n    EvaluationExample(\n        input=\"How do I disable MLflow autologging?\",\n        output=\"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \",\n        score=2,\n        justification=\"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\",\n        grading_context={\n            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n        },\n    ),\n    EvaluationExample(\n        input=\"How do I disable MLflow autologging?\",\n        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n        score=5,\n        justification=\"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\",\n        grading_context={\n            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n        },\n    ),\n]\n\nfaithfulness_metric = faithfulness(\n    model=\"endpoints:/databricks-llama-2-70b-chat\", examples=faithfulness_examples\n)\nprint(faithfulness_metric)\n```\n\n----------------------------------------\n\nTITLE: Basic Multi-Agent Workflow with MLflow Tracing in Python\nDESCRIPTION: Example of a multi-agent workflow using OpenAI Agents SDK with MLflow tracing enabled. Three agents collaborate to handle language detection and response, with MLflow capturing all agent interactions and API calls.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/openai-agent.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport asyncio\nfrom agents import Agent, Runner\n\n# Enable auto tracing for OpenAI Agents SDK\nmlflow.openai.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"OpenAI Agent\")\n\n# Define a simple multi-agent workflow\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n)\n\n\nasync def main():\n    result = await Runner.run(triage_agent, input=\"Hola, ¿cómo estás?\")\n    print(result.final_output)\n\n\n# If you are running this code in a Jupyter notebook, replace this with `await main()`.\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Enabling Groq Auto-tracing and Creating Chat Completion in Python\nDESCRIPTION: This snippet demonstrates how to enable MLflow auto-tracing for Groq and use the Groq SDK to create a chat completion. It includes setting up the client, creating a message, and printing the response.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/groq.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport groq\n\nimport mlflow\n\n# Turn on auto tracing for Groq by calling mlflow.groq.autolog()\nmlflow.groq.autolog()\n\nclient = groq.Groq()\n\n# Use the create method to create new message\nmessage = client.chat.completions.create(\n    model=\"llama3-8b-8192\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of low latency LLMs.\",\n        }\n    ],\n)\n\nprint(message.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Loading FAISS Vector Database for Document Retrieval\nDESCRIPTION: Loads the previously saved FAISS vector database from disk and creates a retriever for document retrieval.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Load the db from local disk\ndb = FAISS.load_local(DB_PERSIST_DIR, embeddings)\nretriever = db.as_retriever()\n```\n\n----------------------------------------\n\nTITLE: Loading FAISS Vector Database for Document Retrieval\nDESCRIPTION: Loads the previously saved FAISS vector database from disk and creates a retriever for document retrieval.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Load the db from local disk\ndb = FAISS.load_local(DB_PERSIST_DIR, embeddings)\nretriever = db.as_retriever()\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Training Loop with MLflow Logging\nDESCRIPTION: Defines the training function that runs one epoch of training, calculates loss and accuracy metrics, and logs them to MLflow at regular intervals.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef train(dataloader, model, loss_fn, metrics_fn, optimizer, epoch):\n    \"\"\"Train the model on a single pass of the dataloader.\n\n    Args:\n        dataloader: an instance of `torch.utils.data.DataLoader`, containing the training data.\n        model: an instance of `torch.nn.Module`, the model to be trained.\n        loss_fn: a callable, the loss function.\n        metrics_fn: a callable, the metrics function.\n        optimizer: an instance of `torch.optim.Optimizer`, the optimizer used for training.\n        epoch: an integer, the current epoch number.\n    \"\"\"\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        accuracy = metrics_fn(pred, y)\n\n        # Backpropagation.\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch\n            step = batch // 100 * (epoch + 1)\n            mlflow.log_metric(\"loss\", f\"{loss:2f}\", step=step)\n            mlflow.log_metric(\"accuracy\", f\"{accuracy:2f}\", step=step)\n            print(f\"loss: {loss:2f} accuracy: {accuracy:2f} [{current} / {len(dataloader)}]\")\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Loaded MLflow Model\nDESCRIPTION: Shows how to perform inference with the loaded model. The snippet highlights a key consideration that the loaded MLflow model requires numpy array input rather than PyTorch tensors.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\noutputs = loaded_model.predict(training_data[0][0][None, :].numpy())\n```\n\n----------------------------------------\n\nTITLE: Loading and Predicting with MLflow-logged PyTorch Model\nDESCRIPTION: This snippet shows how to load a previously logged PyTorch model using MLflow's pyfunc interface and make predictions. It uses a random input tensor for demonstration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/guide/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlogged_model = f\"runs:/{run.info.run_id}/model\"\nloaded_model = mlflow.pyfunc.load_model(logged_model)\nloaded_model.predict(np.random.uniform(size=[1, 28, 28]).astype(np.float32))\n```\n\n----------------------------------------\n\nTITLE: Creating an MLflow Experiment for DialoGPT Model\nDESCRIPTION: Sets up a dedicated MLflow experiment named 'Conversational' to organize runs related to the conversational AI model, providing better organization of model artifacts and metadata.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/conversational-model.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\n# Set a name for the experiment that is indicative of what the runs being created within it are in regards to\nmlflow.set_experiment(\"Conversational\")\n```\n\n----------------------------------------\n\nTITLE: Training and Logging LightGBM Model with MLflow\nDESCRIPTION: Demonstrates training a LightGBM classifier, logging feature importances as metrics, and making predictions using the saved model. Includes data preprocessing and model signature creation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfeature_names = [\n    name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n    for name in data[\"feature_names\"]\n]\nX_train, X_test, y_train, y_test = train_test_split(\n    data[\"data\"], data[\"target\"], test_size=0.2\n)\nlgb_classifier = LGBMClassifier(\n    n_estimators=10,\n    max_depth=3,\n    learning_rate=1,\n    objective=\"binary:logistic\",\n    random_state=123,\n)\n\nwith mlflow.start_run():\n    lgb_classifier.fit(X_train, y_train)\n    feature_importances = dict(zip(feature_names, lgb_classifier.feature_importances_))\n    feature_importance_metrics = {\n        f\"feature_importance_{feature_name}\": imp_value\n        for feature_name, imp_value in feature_importances.items()\n    }\n    mlflow.log_metrics(feature_importance_metrics)\n    signature = infer_signature(X_train, lgb_classifier.predict(X_train))\n    model_info = mlflow.lightgbm.log_model(\n        lgb_classifier, \"iris-classifier\", signature=signature\n    )\n\nlgb_classifier_saved = mlflow.pyfunc.load_model(model_info.model_uri)\ny_pred = lgb_classifier_saved.predict(X_test)\nprint(y_pred)\n```\n\n----------------------------------------\n\nTITLE: Creating a PyFunc model that uses environment variables\nDESCRIPTION: Example of creating and logging an MLflow PyFunc model that uses environment variables for API key authentication. This demonstrates how MLflow automatically records environment variable references for deployment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport os\n\nos.environ[\"TEST_API_KEY\"] = \"test_api_key\"\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        if os.environ.get(\"TEST_API_KEY\"):\n            return model_input\n        raise Exception(\"API key not found\")\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        \"model\", python_model=MyModel(), input_example=\"data\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Complete Example of DSPy Optimizer Autologging\nDESCRIPTION: A comprehensive example demonstrating DSPy optimizer autologging with MLflow. The code sets up a Chain of Thought model for solving GSM8K math problems and uses the MIPROv2 teleprompter to optimize the program.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/optimizer.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nfrom dspy.datasets.gsm8k import GSM8K, gsm8k_metric\nimport mlflow\n\n# Enabling tracing for DSPy\nmlflow.dspy.autolog(log_compiles=True, log_evals=True, log_traces_from_compile=True)\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"DSPy\")\n\nlm = dspy.LM(model=\"openai/gpt-3.5-turbo\", max_tokens=250)\ndspy.configure(lm=lm)\n\ngsm8k = GSM8K()\n\ntrainset, devset = gsm8k.train, gsm8k.dev[:50]\n\n\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought(\"question -> answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\nprogram = CoT()\n\n# define a teleprompter\nteleprompter = dspy.teleprompt.MIPROv2(\n    metric=gsm8k_metric,\n    auto=\"light\",\n)\n# run the optimizer\noptimized_program = teleprompter.compile(\n    program,\n    trainset=trainset,\n    max_bootstrapped_demos=3,\n    max_labeled_demos=4,\n    requires_permission_to_run=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining the Template Instruction for Sous Chef Roleplay in Python\nDESCRIPTION: Creates a templated instruction prompt that guides an LLM to act as a sous chef, focusing on listing ingredients, preparation techniques, ingredient staging, and cooking implement preparation for a given recipe and customer count.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntemplate_instruction = (\n    \"Imagine you are a fine dining sous chef. Your task is to meticulously prepare for a dish, focusing on the mise-en-place process.\"\n    \"Given a recipe, your responsibilities are: \"\n    \"1. List the Ingredients: Carefully itemize all ingredients required for the dish, ensuring every element is accounted for. \"\n    \"2. Preparation Techniques: Describe the techniques and operations needed for preparing each ingredient. This includes cutting, \"\n    \"processing, or any other form of preparation. Focus on the art of mise-en-place, ensuring everything is perfectly set up before cooking begins.\"\n    \"3. Ingredient Staging: Provide detailed instructions on how to stage and arrange each ingredient. Explain where each item should be placed for \"\n    \"efficient access during the cooking process. Consider the timing and sequence of use for each ingredient. \"\n    \"4. Cooking Implements Preparation: Enumerate all the cooking tools and implements needed for each phase of the dish's preparation. \"\n    \"Detail any specific preparation these tools might need before the actual cooking starts and describe what pots, pans, dishes, and \"\n    \"other tools will be needed for the final preparation.\"\n    \"Remember, your guidance stops at the preparation stage. Do not delve into the actual cooking process of the dish. \"\n    \"Your goal is to set the stage flawlessly for the chef to execute the cooking seamlessly.\"\n    \"The recipe you are given is for: {recipe} for {customer_count} people. \"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Tool Calling Agent with Anthropic and MLflow Tracing\nDESCRIPTION: This advanced example implements a tool calling agent using Anthropic Tool Calling and MLflow Tracing. It uses the asynchronous Anthropic SDK and demonstrates how to annotate tool functions with the @mlflow.trace decorator.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/anthropic.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport anthropic\nimport mlflow\nimport asyncio\nfrom mlflow.entities import SpanType\n\nclient = anthropic.AsyncAnthropic()\nmodel_name = \"claude-3-5-sonnet-20241022\"\n\n\n# Define the tool function. Decorate it with `@mlflow.trace` to create a span for its execution.\n@mlflow.trace(span_type=SpanType.TOOL)\nasync def get_weather(city: str) -> str:\n    if city == \"Tokyo\":\n        return \"sunny\"\n    elif city == \"Paris\":\n        return \"rainy\"\n    return \"unknown\"\n\n\ntools = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Returns the weather condition of a given city.\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\"city\": {\"type\": \"string\"}},\n            \"required\": [\"city\"],\n        },\n    }\n]\n\n_tool_functions = {\"get_weather\": get_weather}\n\n\n# Define a simple tool calling agent\n@mlflow.trace(span_type=SpanType.AGENT)\nasync def run_tool_agent(question: str):\n    messages = [{\"role\": \"user\", \"content\": question}]\n\n    # Invoke the model with the given question and available tools\n    ai_msg = await client.messages.create(\n        model=model_name,\n        messages=messages,\n        tools=tools,\n        max_tokens=2048,\n    )\n    messages.append({\"role\": \"assistant\", \"content\": ai_msg.content})\n\n    # If the model requests tool call(s), invoke the function with the specified arguments\n    tool_calls = [c for c in ai_msg.content if c.type == \"tool_use\"]\n    for tool_call in tool_calls:\n        if tool_func := _tool_functions.get(tool_call.name):\n            tool_result = await tool_func(**tool_call.input)\n        else:\n            raise RuntimeError(\"An invalid tool is returned from the assistant!\")\n\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"tool_result\",\n                        \"tool_use_id\": tool_call.id,\n                        \"content\": tool_result,\n                    }\n                ],\n            }\n        )\n\n    # Send the tool results to the model and get a new response\n    response = await client.messages.create(\n        model=model_name,\n        messages=messages,\n        max_tokens=2048,\n    )\n\n    return response.content[-1].text\n\n\n# Run the tool calling agent\ncities = [\"Tokyo\", \"Paris\", \"Sydney\"]\nquestions = [f\"What's the weather like in {city} today?\" for city in cities]\nanswers = await asyncio.gather(*(run_tool_agent(q) for q in questions))\n\nfor city, answer in zip(cities, answers):\n    print(f\"{city}: {answer}\")\n```\n\n----------------------------------------\n\nTITLE: Defining XGBoost Objective Function for Optuna Optimization\nDESCRIPTION: This snippet defines the objective function for Optuna optimization that trains an XGBoost model with various hyperparameters, evaluates it, and logs both parameters and metrics to MLflow as nested runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef objective(trial):\n    with mlflow.start_run(nested=True):\n        # Define hyperparameters\n        params = {\n            \"objective\": \"reg:squarederror\",\n            \"eval_metric\": \"rmse\",\n            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        }\n\n        if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":\n            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n            params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n            params[\"grow_policy\"] = trial.suggest_categorical(\n                \"grow_policy\", [\"depthwise\", \"lossguide\"]\n            )\n\n        # Train XGBoost model\n        bst = xgb.train(params, dtrain)\n        preds = bst.predict(dvalid)\n        error = mean_squared_error(valid_y, preds)\n\n        # Log to MLflow\n        mlflow.log_params(params)\n        mlflow.log_metric(\"mse\", error)\n        mlflow.log_metric(\"rmse\", math.sqrt(error))\n\n    return error\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating Semantic Similarity with Contrasting Text Pairs\nDESCRIPTION: Shows how to evaluate the model's performance on pairs of texts with varying degrees of semantic similarity. Includes tests for both low and high similarity scenarios to validate the model's discrimination capabilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-similarity/semantic-similarity-sentence-transformers.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlow_similarity = {\n    \"sentence_1\": \"The explorer stood at the edge of the dense rainforest, \"\n    \"contemplating the journey ahead. The untamed wilderness was \"\n    \"a labyrinth of exotic plants and unknown dangers, a challenge \"\n    \"for even the most seasoned adventurer, brimming with the \"\n    \"prospect of new discoveries and uncharted territories.\",\n    \"sentence_2\": \"To install the software, begin by downloading the latest \"\n    \"version from the official website. Once downloaded, run the \"\n    \"installer and follow the on-screen instructions. Ensure that \"\n    \"your system meets the minimum requirements and agree to the \"\n    \"license terms to complete the installation process successfully.\",\n}\n\nhigh_similarity = {\n    \"sentence_1\": \"Standing in the shadow of the Great Pyramids of Giza, I felt a \"\n    \"profound sense of awe. The towering structures, a testament to \"\n    \"ancient ingenuity, rose majestically against the clear blue sky. \"\n    \"As I walked around the base of the pyramids, the intricate \"\n    \"stonework and sheer scale of these wonders of the ancient world \"\n    \"left me speechless, enveloped in a deep sense of history.\",\n    \"sentence_2\": \"My visit to the Great Pyramids of Giza was an unforgettable \"\n    \"experience. Gazing upon these monumental structures, I was \"\n    \"captivated by their grandeur and historical significance. Each \"\n    \"step around these ancient marvels filled me with a deep \"\n    \"appreciation for the architectural prowess of a civilization long \"\n    \"gone, yet still speaking through these timeless monuments.\",\n}\n\n# Validate that semantically unrelated texts return a low similarity score\nlow_similarity_score = loaded_dynamic.predict(low_similarity)\n\nprint(f\"The similarity score for the 'low_similarity' pair is: {low_similarity_score}\")\n\n# Validate that semantically similar texts return a high similarity score\nhigh_similarity_score = loaded_dynamic.predict(high_similarity)\n\nprint(f\"The similarity score for the 'high_similarity' pair is: {high_similarity_score}\")\n```\n\n----------------------------------------\n\nTITLE: Logging a Transformers Model with Parameters in MLflow\nDESCRIPTION: This example demonstrates how to create and store a model signature that includes parameters for a Hugging Face Transformers text generation model. The parameters include configurations like beam search parameters and temperature for controlling generation behavior.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\nimport transformers\n\narchitecture = \"mrm8488/t5-base-finetuned-common_gen\"\nmodel = transformers.pipeline(\n    task=\"text2text-generation\",\n    tokenizer=transformers.T5TokenizerFast.from_pretrained(architecture),\n    model=transformers.T5ForConditionalGeneration.from_pretrained(architecture),\n)\ndata = \"pencil draw paper\"\n\nparams = {\n    \"top_k\": 2,\n    \"num_beams\": 5,\n    \"max_length\": 30,\n    \"temperature\": 0.62,\n    \"top_p\": 0.85,\n    \"repetition_penalty\": 1.15,\n    \"begin_suppress_tokens\": [1, 2, 3],\n}\n\n# infer signature with params\nsignature = infer_signature(\n    data,\n    mlflow.transformers.generate_signature_output(model, data),\n    params,\n)\n\n# save model with signature\nmlflow.transformers.save_model(\n    model,\n    \"text2text\",\n    signature=signature,\n)\npyfunc_loaded = mlflow.pyfunc.load_model(\"text2text\")\n\n# predict with params\nresult = pyfunc_loaded.predict(data, params=params)\n```\n\n----------------------------------------\n\nTITLE: Logging PEFT Model to MLflow\nDESCRIPTION: Demonstrates how to log a PEFT model to MLflow, including saving the model without padding in tokenizer, applying prompt templates, and setting model signatures.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Get the ID of the MLflow Run that was automatically created above\nlast_run_id = mlflow.last_active_run().info.run_id\n\n# Save a tokenizer without padding because it is only needed for training\ntokenizer_no_pad = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True)\n\nwith mlflow.start_run(run_id=last_run_id):\n    mlflow.log_params(peft_config.to_dict())\n    mlflow.transformers.log_model(\n        transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer_no_pad},\n        prompt_template=prompt_template,\n        signature=signature,\n        artifact_path=\"model\",  # This is a relative path to save model files within MLflow run\n    )\n```\n\n----------------------------------------\n\nTITLE: Registering an Unsupported ML Model with MLflow using PyFunc\nDESCRIPTION: This code demonstrates how to register an unsupported ML model (vaderSentiment) with MLflow by wrapping it in a custom PyFunc model. It includes functions for model creation, saving, logging, and scoring.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom sys import version_info\nimport cloudpickle\nimport pandas as pd\n\nimport mlflow.pyfunc\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nINPUT_TEXTS = [\n    {\"text\": \"This is a bad movie. You don't want to see it! :-)\"},\n    {\"text\": \"Ricky Gervais is smart, witty, and creative!!!!!! :D\"},\n    {\"text\": \"LOL, this guy fell off a chair while sleeping and snoring in a meeting\"},\n    {\"text\": \"Men shoots himself while trying to steal a dog, OMG\"},\n    {\"text\": \"Yay!! Another good phone interview. I nailed it!!\"},\n    {\n        \"text\": \"This is INSANE! I can't believe it. How could you do such a horrible thing?\"\n    },\n]\n\nPYTHON_VERSION = f\"{version_info.major}.{version_info.minor}.{version_info.micro}\"\n\ndef score_model(model):\n    for i, text in enumerate(INPUT_TEXTS):\n        text = INPUT_TEXTS[i][\"text\"]\n        m_input = pd.DataFrame([text])\n        scores = loaded_model.predict(m_input)\n        print(f\"<{text}> -- {str(scores[0])}\")\n```\n\n----------------------------------------\n\nTITLE: Basic Example of LiteLLM Tracing with MLflow\nDESCRIPTION: Complete example showing how to enable MLflow tracing for LiteLLM, set tracking URI and experiment, and make a basic completion call to Anthropic's Claude model via LiteLLM.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/litellm.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport litellm\n\n# Enable auto-tracing for LiteLLM\nmlflow.litellm.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"LiteLLM\")\n\n# Call Anthropic API via LiteLLM\nresponse = litellm.completion(\n    model=\"claude-3-5-sonnet-20240620\",\n    messages=[{\"role\": \"user\", \"content\": \"Hey! how's it going?\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Logging and Using an MLflow Model From Code with Dependencies in Python\nDESCRIPTION: Demonstrates how to log the AddModel with its dependencies using MLflow, and then load and use the model for predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/models-from-code/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_experiment(\"Arithmetic Model From Code\")\n\nmodel_path = \"math_model.py\"\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=model_path,  # The model is defined as the path to the script containing the model definition\n        artifact_path=\"arithmetic_model\",\n        code_paths=[\n            \"calculator.py\"\n        ],  # dependency definition included for the model to successfully import the implementation\n    )\n```\n\nLANGUAGE: python\nCODE:\n```\nmy_model_from_code = mlflow.pyfunc.load_model(model_info.model_uri)\nmy_model_from_code.predict({\"x\": 42, \"y\": 9001})\n```\n\n----------------------------------------\n\nTITLE: Loading Generated Questions Dataset for Retriever Evaluation\nDESCRIPTION: Loads a previously generated dataset of questions for evaluation from a CSV file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngenerated_df = pd.read_csv(OUTPUT_DF_PATH)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Lissajous PyFunc Model in Python\nDESCRIPTION: This snippet defines a custom PyFunc model class for generating Lissajous curves. It includes methods for initializing the model, generating curve data, and plotting the results using matplotlib and seaborn.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport mlflow.pyfunc\nfrom mlflow.models import infer_signature\n\n\nclass Lissajous(mlflow.pyfunc.PythonModel):\n    def __init__(self, A=1, B=1, num_points=1000):\n        self.A = A\n        self.B = B\n        self.num_points = num_points\n        self.t_range = (0, 2 * np.pi)\n\n    def generate_lissajous(self, a, b, delta):\n        t = np.linspace(self.t_range[0], self.t_range[1], self.num_points)\n        x = self.A * np.sin(a * t + delta)\n        y = self.B * np.sin(b * t)\n        return pd.DataFrame({\"x\": x, \"y\": y})\n\n    def predict(self, context, model_input, params=None):  # noqa: D417\n        \"\"\"\n        Generate and plot the Lissajous curve with annotations for parameters.\n\n        Args:\n        - model_input (pd.DataFrame): DataFrame containing columns 'a' and 'b'.\n        - params (dict, optional): Dictionary containing optional parameter 'delta'.\n        \"\"\"\n        # Extract a and b values from the input DataFrame\n        a = model_input[\"a\"].iloc[0]\n        b = model_input[\"b\"].iloc[0]\n\n        # Extract delta from params or set it to 0 if not provided\n        delta = params.get(\"delta\", 0)\n\n        # Generate the Lissajous curve data\n        df = self.generate_lissajous(a, b, delta)\n\n        sns.set_theme()\n\n        # Create the plot components\n        fig, ax = plt.subplots(figsize=(10, 8))\n        ax.plot(df[\"x\"], df[\"y\"])\n        ax.set_title(\"Lissajous Curve\")\n\n        # Define the annotation string\n        annotation_text = f\"\"\"\n        A = {self.A}\n        B = {self.B}\n        a = {a}\n        b = {b}\n        delta = {np.round(delta, 2)} rad\n        \"\"\"\n\n        # Add the annotation with a bounding box outside the plot area\n        ax.annotate(\n            annotation_text,\n            xy=(1.05, 0.5),\n            xycoords=\"axes fraction\",\n            fontsize=12,\n            bbox={\"boxstyle\": \"round,pad=0.25\", \"facecolor\": \"aliceblue\", \"edgecolor\": \"black\"},\n        )\n\n        # Adjust plot borders to make space for the annotation\n        plt.subplots_adjust(right=0.65)\n        plt.close()\n\n        # Return the plot\n        return fig\n```\n\n----------------------------------------\n\nTITLE: Saving Transformers Model with Prompt Template\nDESCRIPTION: Demonstrates saving a transformers model with a custom prompt template and loading it for inference using MLflow pyfunc.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define a prompt template\nprompt_template = \"Answer the following question: {prompt}\"\n\n# Save the model\nmlflow.transformers.save_model(\n    transformers_model=generator,\n    path=\"path/to/model\",\n    prompt_template=prompt_template,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Answer Quality Metric\nDESCRIPTION: Sets up custom evaluation metric definitions and examples for measuring answer quality based on fluency, clarity, and conciseness.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# The definition explains what \"answer quality\" entails\nanswer_quality_definition = \"\"\"Please evaluate answer quality for the provided output on the following criteria:\nfluency, clarity, and conciseness. Each of the criteria is defined as follows:\n  - Fluency measures how naturally and smooth the output reads.\n  - Clarity measures how understandable the output is.\n  - Conciseness measures the brevity and efficiency of the output without compromising meaning.\nThe more fluent, clear, and concise a text, the higher the score it deserves.\n\"\"\"\n\n# The grading prompt explains what each possible score means\nanswer_quality_grading_prompt = \"\"\"Answer quality: Below are the details for different scores:\n  - Score 1: The output is entirely incomprehensible and cannot be read.\n  - Score 2: The output conveys some meaning, but needs lots of improvement in to improve fluency, clarity, and conciseness.\n  - Score 3: The output is understandable but still needs improvement.\n  - Score 4: The output performs well on two of fluency, clarity, and conciseness, but could be improved on one of these criteria.\n  - Score 5: The output reads smoothly, is easy to understand, and clear. There is no clear way to improve the output on these criteria.\n\"\"\"\n\n# We provide an example of a \"bad\" output\nexample1 = EvaluationExample(\n    input=\"What is MLflow?\",\n    output=\"MLflow is an open-source platform. For managing machine learning workflows, it \"\n    \"including experiment tracking model packaging versioning and deployment as well as a platform \"\n    \"simplifying for on the ML lifecycle.\",\n    score=2,\n    justification=\"The output is difficult to understand and demonstrates extremely low clarity. \"\n    \"However, it still conveys some meaning so this output deserves a score of 2.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Evaluating Basic OpenAI QA Model\nDESCRIPTION: Initializes a basic question-answering model using OpenAI's GPT-4 and evaluates it using MLflow's evaluate function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/qa-evaluation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    system_prompt = \"Answer the following question in two sentences\"\n    basic_qa_model = mlflow.openai.log_model(\n        model=\"gpt-4o-mini\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": \"{question}\"},\n        ],\n    )\n    results = mlflow.evaluate(\n        basic_qa_model.model_uri,\n        eval_df,\n        targets=\"ground_truth\",\n        model_type=\"question-answering\",\n        evaluators=\"default\",\n    )\nresults.metrics\n```\n\n----------------------------------------\n\nTITLE: Logging LlamaIndex Model with MLflow\nDESCRIPTION: Logs the LlamaIndex model to MLflow, enabling tracking, versioning, and reproducibility. Sets up autologging, logs the model with specific parameters, and registers it in the model registry.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_quickstart.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.llama_index.autolog()  # This is for enabling tracing\n\nwith mlflow.start_run() as run:\n    mlflow.llama_index.log_model(\n        index,\n        artifact_path=\"llama_index\",\n        engine_type=\"query\",  # Defines the pyfunc and spark_udf inference type\n        input_example=\"hi\",  # Infers signature\n        registered_model_name=\"my_llama_index_vector_store\",  # Stores an instance in the model registry\n    )\n\n    run_id = run.info.run_id\n    model_uri = f\"runs:/{run_id}/llama_index\"\n    print(f\"Unique identifier for the model location for loading: {model_uri}\")\n```\n\n----------------------------------------\n\nTITLE: Saving and Logging Sentence Transformers Models in Python\nDESCRIPTION: Demonstrates how to save and log sentence-transformers models using MLflow's API functions. Shows both direct saving to a path and logging within an MLflow run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/guide/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"model_name\")\n\n# Saving the model\nmlflow.sentence_transformers.save_model(model=model, path=\"path/to/save/directory\")\n\n# Logging the model\nwith mlflow.start_run():\n    mlflow.sentence_transformers.log_model(\n        sentence_transformers_model=model, artifact_path=\"model_artifact_path\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging a LangChain Retriever with MLflow\nDESCRIPTION: This code example demonstrates creating a FAISS vector database from text documents, defining a loader function for the retriever, and logging it with MLflow. It requires an OpenAI API key as it uses OpenAI embeddings. The example shows the complete workflow from document loading to querying the loaded model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/guide/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\nimport mlflow\n\nassert (\n    \"OPENAI_API_KEY\" in os.environ\n), \"Please set the OPENAI_API_KEY environment variable.\"\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    persist_dir = os.path.join(temp_dir, \"faiss_index\")\n\n    # Create the vector database and persist it to a local filesystem folder\n    loader = TextLoader(\"tests/langchain/state_of_the_union.txt\")\n    documents = loader.load()\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    docs = text_splitter.split_documents(documents)\n    embeddings = OpenAIEmbeddings()\n    db = FAISS.from_documents(docs, embeddings)\n    db.save_local(persist_dir)\n\n    # Define a loader function to recall the retriever from the persisted vectorstore\n    def load_retriever(persist_directory):\n        embeddings = OpenAIEmbeddings()\n        vectorstore = FAISS.load_local(persist_directory, embeddings)\n        return vectorstore.as_retriever()\n\n    # Log the retriever with the loader function\n    with mlflow.start_run() as run:\n        logged_model = mlflow.langchain.log_model(\n            db.as_retriever(),\n            artifact_path=\"retriever\",\n            loader_fn=load_retriever,\n            persist_dir=persist_dir,\n        )\n\n# Load the retriever chain\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\nprint(\n    loaded_model.predict(\n        [{\"query\": \"What did the president say about Ketanji Brown Jackson\"}]\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Searching and Displaying Multiple Traces\nDESCRIPTION: Shows how to use mlflow.search_traces() to find and display multiple traces from an experiment. This API renders traces in a paginated view with a configurable limit.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmlflow.search_traces(experiment_ids=[experiment.experiment_id])\n```\n\n----------------------------------------\n\nTITLE: Creating Corpus for Paraphrase Mining in Python\nDESCRIPTION: This snippet creates a diverse corpus of sentences and writes it to a file. The corpus serves as the key dataset for the model to find semantically similar sentences, ensuring adaptability across various use cases.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/paraphrase-mining/paraphrase-mining-sentence-transformers.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncorpus = [\n    \"Exploring ancient cities in Europe offers a glimpse into history.\",\n    \"Modern AI technologies are revolutionizing industries.\",\n    \"Healthy eating contributes significantly to overall well-being.\",\n    \"Advancements in renewable energy are combating climate change.\",\n    \"Learning a new language opens doors to different cultures.\",\n    \"Gardening is a relaxing hobby that connects you with nature.\",\n    \"Blockchain technology could redefine digital transactions.\",\n    \"Homemade Italian pasta is a delight to cook and eat.\",\n    \"Practicing yoga daily improves both physical and mental health.\",\n    \"The art of photography captures moments in time.\",\n    \"Baking bread at home has become a popular quarantine activity.\",\n    \"Virtual reality is creating new experiences in gaming.\",\n    \"Sustainable travel is becoming a priority for eco-conscious tourists.\",\n    \"Reading books is a great way to unwind and learn.\",\n    \"Jazz music provides a rich tapestry of sound and rhythm.\",\n    \"Marathon training requires discipline and perseverance.\",\n    \"Studying the stars helps us understand our universe.\",\n    \"The rise of electric cars is an important environmental development.\",\n    \"Documentary films offer deep insights into real-world issues.\",\n    \"Crafting DIY projects can be both fun and rewarding.\",\n    \"The history of ancient civilizations is fascinating to explore.\",\n    \"Exploring the depths of the ocean reveals a world of marine wonders.\",\n    \"Learning to play a musical instrument can be a rewarding challenge.\",\n    \"Artificial intelligence is shaping the future of personalized medicine.\",\n    \"Cycling is not only a great workout but also eco-friendly transportation.\",\n    \"Home automation with IoT devices is enhancing living experiences.\",\n    \"Understanding quantum computing requires a grasp of complex physics.\",\n    \"A well-brewed cup of coffee is the perfect start to the day.\",\n    \"Urban farming is gaining popularity as a sustainable food source.\",\n    \"Meditation and mindfulness can lead to a more balanced life.\",\n    \"The popularity of podcasts has revolutionized audio storytelling.\",\n    \"Space exploration continues to push the boundaries of human knowledge.\",\n    \"Wildlife conservation is essential for maintaining biodiversity.\",\n    \"The fusion of technology and fashion is creating new trends.\",\n    \"E-learning platforms have transformed the educational landscape.\",\n    \"Dark chocolate has surprising health benefits when enjoyed in moderation.\",\n    \"Robotics in manufacturing is leading to more efficient production.\",\n    \"Creating a personal budget is key to financial well-being.\",\n    \"Hiking in nature is a great way to connect with the outdoors.\",\n    \"3D printing is innovating the way we create and manufacture objects.\",\n    \"Sommeliers can identify a wine's characteristics with just a taste.\",\n    \"Mind-bending puzzles and riddles are great for cognitive exercise.\",\n    \"Social media has a profound impact on communication and culture.\",\n    \"Urban sketching captures the essence of city life on paper.\",\n    \"The ethics of AI is a growing field in tech philosophy.\",\n    \"Homemade skincare remedies are becoming more popular.\",\n    \"Virtual travel experiences can provide a sense of adventure at home.\",\n    \"Ancient mythology still influences modern storytelling and literature.\",\n    \"Building model kits is a hobby that requires patience and precision.\",\n    \"The study of languages opens windows into different worldviews.\",\n    \"Professional esports has become a major global phenomenon.\",\n    \"The mysteries of the universe are unveiled through space missions.\",\n    \"Astronauts' experiences in space stations offer unique insights into life beyond Earth.\",\n    \"Telescopic observations bring distant galaxies within our view.\",\n    \"The study of celestial bodies helps us understand the cosmos.\",\n    \"Space travel advancements could lead to interplanetary exploration.\",\n    \"Observing celestial events provides valuable data for astronomers.\",\n    \"The development of powerful rockets is key to deep space exploration.\",\n    \"Mars rover missions are crucial in searching for extraterrestrial life.\",\n    \"Satellites play a vital role in our understanding of Earth's atmosphere.\",\n    \"Astrophysics is central to unraveling the secrets of space.\",\n    \"Zero gravity environments in space pose unique challenges and opportunities.\",\n    \"Space tourism might soon become a reality for many.\",\n    \"Lunar missions have contributed significantly to our knowledge of the moon.\",\n    \"The International Space Station is a hub for groundbreaking space research.\",\n    \"Studying comets and asteroids reveals information about the early solar system.\",\n    \"Advancements in space technology have implications for many scientific fields.\",\n    \"The possibility of life on other planets continues to intrigue scientists.\",\n    \"Black holes are among the most mysterious phenomena in space.\",\n    \"The history of space exploration is filled with remarkable achievements.\",\n    \"Future space missions could unlock the mysteries of dark matter.\"\n]\n\n# Write out the corpus to a file\ncorpus_file = \"/tmp/feedback.txt\"\nwith open(corpus_file, \"w\") as file:\n    for sentence in corpus:\n        file.write(sentence + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Using TypeFromExample Type Hint in MLflow PythonModel\nDESCRIPTION: Demonstrates the TypeFromExample type hint, which converts input data to match the type of the input example during PyFunc prediction. This is useful when you don't want to explicitly define a type hint but need data conformity.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.types.type_hints import TypeFromExample\n\n\nclass Model(mlflow.pyfunc.PythonModel):\n    def predict(self, model_input: TypeFromExample):\n        return model_input\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        artifact_path=\"model\",\n        python_model=Model(),\n        input_example=[\"a\", \"b\", \"c\"],\n    )\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nassert pyfunc_model.predict([\"d\", \"e\", \"f\"]) == [\"d\", \"e\", \"f\"]\n```\n\n----------------------------------------\n\nTITLE: Statsmodels ARIMA Time Series Model with MLflow Integration\nDESCRIPTION: This example demonstrates creating and training an ARIMA time series model with Statsmodels, logging it to MLflow, and using the MLflow PyFunc interface to make predictions. It shows how to manually log parameters and metrics specific to time series models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# create a time series dataset with seasonality\nnp.random.seed(0)\n\n# generate a time index with a daily frequency\ndates = pd.date_range(start=\"2022-12-01\", end=\"2023-12-01\", freq=\"D\")\n\n# generate the seasonal component (weekly)\nseasonality = np.sin(np.arange(len(dates)) * (2 * np.pi / 365.25) * 7)\n\n# generate the trend component\ntrend = np.linspace(-5, 5, len(dates)) + 2 * np.sin(\n    np.arange(len(dates)) * (2 * np.pi / 365.25) * 0.1\n)\n\n# generate the residual component\nresiduals = np.random.normal(0, 1, len(dates))\n\n# generate the final time series by adding the components\ntime_series = seasonality + trend + residuals\n\n# create a dataframe from the time series\ndata = pd.DataFrame({\"date\": dates, \"value\": time_series})\ndata.set_index(\"date\", inplace=True)\n\norder = (1, 0, 0)\n# create the ARIMA model\nmodel = ARIMA(data, order=order)\n\nmlflow.statsmodels.autolog(\n    log_models=True,\n    disable=False,\n    exclusive=False,\n    disable_for_unsupported_versions=False,\n    silent=False,\n    registered_model_name=None,\n)\n\nwith mlflow.start_run():\n    res = model.fit()\n    mlflow.log_params(\n        {\n            \"order\": order,\n            \"trend\": model.trend,\n            \"seasonal_order\": model.seasonal_order,\n        }\n    )\n    mlflow.log_params(res.params)\n    mlflow.log_metric(\"aic\", res.aic)\n    mlflow.log_metric(\"bic\", res.bic)\n    model_info = mlflow.statsmodels.log_model(res, artifact_path=\"ARIMA_model\")\n\n# load the pyfunc model\nstatsmodels_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# prediction dataframes for a TimeSeriesModel must have exactly one row and include columns called start and end\nstart = pd.to_datetime(\"2024-01-01\")\nend = pd.to_datetime(\"2024-01-07\")\n```\n\n----------------------------------------\n\nTITLE: Defining CNN Image Classifier Model in PyTorch\nDESCRIPTION: Implements a simple convolutional neural network for image classification with two convolutional layers, ReLU activations, and a final linear layer for 10-class output.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass ImageClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=3),\n            nn.ReLU(),\n            nn.Conv2d(8, 16, kernel_size=3),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.LazyLinear(10),  # 10 classes in total.\n        )\n\n    def forward(self, x):\n        return self.model(x)\n```\n\n----------------------------------------\n\nTITLE: Creating LangChain Model with Registered Prompt\nDESCRIPTION: Example of loading a registered prompt from MLflow Prompt Registry and converting it to a LangChain prompt template. This shows how to load a prompt, convert its format, create a LangChain chain with the prompt, and invoke it.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/run-and-model.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Load registered prompt\nprompt = mlflow.load_prompt(\"prompts:/summarization-prompt/2\")\n\n# Create LangChain prompt object\nlangchain_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            # IMPORTANT: Convert prompt template from double to single curly braces format\n            \"system\",\n            prompt.to_single_brace_format(),\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n)\n\n# Define the LangChain chain\nllm = ChatOpenAI()\nchain = langchain_prompt | llm\n\n# Invoke the chain\nresponse = chain.invoke({\"num_sentences\": 1, \"sentences\": \"This is a test sentence.\"})\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Multi-turn Chat Interactions with Gemini\nDESCRIPTION: Example demonstrating how to trace multi-turn conversations using Gemini's chat functionality with MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/gemini.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.gemini.autolog()\n\nchat = client.chats.create(model='gemini-1.5-flash')\nresponse = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\nprint(response.text)\nresponse = chat.send_message(\"Okay, how about a more detailed explanation to a high schooler?\")\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Searching All MLflow Runs\nDESCRIPTION: Python code demonstrating how to fetch all MLflow runs across all experiments\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nall_runs = mlflow.search_runs(search_all_experiments=True)\nprint(all_runs)\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Plugin Entry Points in setup.py\nDESCRIPTION: Example setup.py configuration showing how to declare MLflow plugin entry points. This includes definitions for tracking store, artifact repository, run context provider, request handlers, model registry store, project backend, deployments, and model evaluator plugins.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsetup(\n    name=\"mflow-test-plugin\",\n    # Require MLflow as a dependency of the plugin, so that plugin users can simply install\n    # the plugin and then immediately use it with MLflow\n    install_requires=[\"mlflow\"],\n    ...,\n    entry_points={\n        # Define a Tracking Store plugin for tracking URIs with scheme 'file-plugin'\n        \"mlflow.tracking_store\": \"file-plugin=mlflow_test_plugin.file_store:PluginFileStore\",\n        # Define a ArtifactRepository plugin for artifact URIs with scheme 'file-plugin'\n        \"mlflow.artifact_repository\": \"file-plugin=mlflow_test_plugin.local_artifact:PluginLocalArtifactRepository\",\n        # Define a RunContextProvider plugin. The entry point name for run context providers\n        # is not used, and so is set to the string \"unused\" here\n        \"mlflow.run_context_provider\": \"unused=mlflow_test_plugin.run_context_provider:PluginRunContextProvider\",\n        # Define a RequestHeaderProvider plugin. The entry point name for request header providers\n        # is not used, and so is set to the string \"unused\" here\n        \"mlflow.request_header_provider\": \"unused=mlflow_test_plugin.request_header_provider:PluginRequestHeaderProvider\",\n        # Define a RequestAuthProvider plugin. The entry point name for request auth providers\n        # is not used, and so is set to the string \"unused\" here\n        \"mlflow.request_auth_provider\": \"unused=mlflow_test_plugin.request_auth_provider:PluginRequestAuthProvider\",\n        # Define a Model Registry Store plugin for tracking URIs with scheme 'file-plugin'\n        \"mlflow.model_registry_store\": \"file-plugin=mlflow_test_plugin.sqlalchemy_store:PluginRegistrySqlAlchemyStore\",\n        # Define a MLflow Project Backend plugin called 'dummy-backend'\n        \"mlflow.project_backend\": \"dummy-backend=mlflow_test_plugin.dummy_backend:PluginDummyProjectBackend\",\n        # Define a MLflow model deployment plugin for target 'faketarget'\n        \"mlflow.deployments\": \"faketarget=mlflow_test_plugin.fake_deployment_plugin\",\n        # Define a MLflow model evaluator with name \"dummy_evaluator\"\n        \"mlflow.model_evaluator\": \"dummy_evaluator=mlflow_test_plugin.dummy_evaluator:DummyEvaluator\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Document Similarity Implementation with Python\nDESCRIPTION: Implements a custom document similarity model using sentence-transformers and MLflow's PythonModel class. Includes complete example with model saving, loading, and prediction functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/guide/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.pyfunc import PythonModel\nimport pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n\nclass DocumentSimilarityModel(PythonModel):\n    def load_context(self, context):\n        \"\"\"Load the model context for inference.\"\"\"\n        self.model = SentenceTransformer.load(context.artifacts[\"model_path\"])\n\n    def predict(self, context, model_input):\n        \"\"\"Predict method for comparing similarity between documents.\"\"\"\n        if isinstance(model_input, pd.DataFrame) and model_input.shape[1] == 2:\n            documents = model_input.values\n        else:\n            raise ValueError(\"Input must be a DataFrame with exactly two columns.\")\n\n        # Compute embeddings for each document separately\n        embeddings1 = self.model.encode(documents[:, 0], convert_to_tensor=True)\n        embeddings2 = self.model.encode(documents[:, 1], convert_to_tensor=True)\n\n        # Calculate cosine similarity\n        similarity_scores = util.cos_sim(embeddings1, embeddings2)\n\n        return pd.DataFrame(similarity_scores.numpy(), columns=[\"similarity_score\"])\n\n\n# Example model saving and loading\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nmodel_path = \"/tmp/sentence_transformers_model\"\nmodel.save(model_path)\n\n# Example usage\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        artifact_path=\"document_similarity_model\",\n        python_model=DocumentSimilarityModel(),\n        artifacts={\"model_path\": model_path},\n    )\n\nloaded = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Test prediction\ndf = pd.DataFrame(\n    {\n        \"doc1\": [\"Sentence Transformers is a wonderful package!\"],\n        \"doc2\": [\"MLflow is pretty great too!\"],\n    }\n)\n\nresult = loaded.predict(df)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Logging Model with JSON Object Example in Python\nDESCRIPTION: This code demonstrates how to log a Langchain model with a JSON object input example.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ninput_example = {\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"assistant\", \"content\": \"What would you like to ask?\"},\n        {\"role\": \"user\", \"content\": \"Who owns MLflow?\"},\n    ]\n}\nmlflow.langchain.log_model(..., input_example=input_example)\n```\n\n----------------------------------------\n\nTITLE: Logging Workflow Models with Different Configurations in MLflow\nDESCRIPTION: This code demonstrates how to log different configurations of the HybridRAGWorkflow as models in MLflow using the mlflow.llama_index.log_model() API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Different configurations we will evaluate. We don't run evaluation for all permutation\n# for demonstration purpose, but you can add as many patterns as you want.\nrun_name_to_retrievers = {\n    # 1. No retrievers (prior knowledge in LLM).\n    \"none\": [],\n    # 2. Vector search retrieval only.\n    \"vs\": [\"vector_search\"],\n    # 3. Vector search and keyword search (BM25)\n    \"vs + bm25\": [\"vector_search\", \"bm25\"],\n    # 4. All retrieval methods including web search.\n    \"vs + bm25 + web\": [\"vector_search\", \"bm25\", \"web_search\"],\n}\n\n# Create an MLflow Run and log model with each configuration.\nmodels = []\nfor run_name, retrievers in run_name_to_retrievers.items():\n    with mlflow.start_run(run_name=run_name):\n        model_info = mlflow.llama_index.log_model(\n            # Specify the model Python script.\n            llama_index_model=\"workflow/model.py\",\n            # Specify retrievers to use.\n            model_config={\"retrievers\": retrievers},\n            # Define dependency files to save along with the model\n            code_paths=[\"workflow\"],\n            # Subdirectory to save artifacts (not important)\n            artifact_path=\"model\",\n        )\n        models.append(model_info)\n```\n\n----------------------------------------\n\nTITLE: Logging and Predicting with OpenAI Chat Completions Model\nDESCRIPTION: Demonstrates logging an OpenAI chat completion model with variable inputs and making predictions using different input formats including pandas DataFrame and list of dictionaries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/guide/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-4o-mini\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are {person}\"},\n            {\"role\": \"user\", \"content\": \"Let me hear your thoughts on {topic}\"},\n        ],\n    )\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        \"person\": [\"Elon Musk\", \"Jeff Bezos\"],\n        \"topic\": [\"AI\", \"ML\"],\n    }\n)\nprint(model.predict(df))\n\nlist_of_dicts = [\n    {\"person\": \"Elon Musk\", \"topic\": \"AI\"},\n    {\"person\": \"Jeff Bezos\", \"topic\": \"ML\"},\n]\nprint(model.predict(list_of_dicts))\n```\n\n----------------------------------------\n\nTITLE: Predicting Improved Lyrics with OpenAI Model and Displaying HTML Results in Python\nDESCRIPTION: This code uses an improved MLflow-tracked OpenAI model to correct bad lyrics, then formats the results as HTML for display. The prediction includes parameters for controlling token length and temperature, with the output being formatted with HTML strong tags and paragraph elements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-quickstart.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Submit our faulty lyrics to the model\nfix_my_lyrics_improved = improved_model.predict(\n    bad_lyrics, params={\"max_tokens\": 500, \"temperature\": 0.1}\n)\n\n# See what the response is\nformatted_output = \"<br>\".join(\n    [f\"<p><strong>{line.strip()}</strong></p>\" for line in fix_my_lyrics_improved]\n)\ndisplay(HTML(formatted_output))\n```\n\n----------------------------------------\n\nTITLE: Implementing Question Generation using OpenAI GPT\nDESCRIPTION: Core functions for generating questions and answers from text chunks using OpenAI's GPT model. Includes prompt engineering and response handling with error management.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_raw_response(content):\n    prompt = f\"\"\"Please generate a question asking for the key information in the given paragraph.\n    Also answer the questions using the information in the given paragraph.\n    Please ask the specific question instead of the general question, like\n    'What is the key information in the given paragraph?'.\n    Please generate the answer using as much information as possible.\n    If you are unable to answer it, please generate the answer as 'I don't know.'\n    The answer should be informative and should be more than 3 sentences.\n\n    Paragraph: {content}\n\n    Please call the submit_function function to submit the generated question and answer.\n    \"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    submit_function = {\n        \"name\": \"submit_function\",\n        \"description\": \"Call this function to submit the generated question and answer.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"question\": {\n                    \"type\": \"string\",\n                    \"description\": \"The question asking for the key information in the given paragraph.\",\n                },\n                \"answer\": {\n                    \"type\": \"string\",\n                    \"description\": \"The answer to the question using the information in the given paragraph.\",\n                },\n            },\n            \"required\": [\"question\", \"answer\"],\n        },\n    }\n\n    return cached_openai_ChatCompletion_create(\n        messages=messages,\n        model=\"gpt-4o-mini\",\n        functions=[submit_function],\n        function_call=\"auto\",\n        temperature=0.0,\n        seed=SEED,\n        cache=cache,\n    )\n\n\ndef generate_question_answer(content):\n    if content is None or len(content) == 0:\n        return \"\", \"N/A\"\n\n    response = get_raw_response(content)\n    try:\n        func_args = json.loads(response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])\n        question = func_args[\"question\"]\n        answer = func_args[\"answer\"]\n        return question, answer\n    except Exception as e:\n        return str(e), \"N/A\"\n```\n\n----------------------------------------\n\nTITLE: Logging Paraphrase Mining Model with MLflow in Python\nDESCRIPTION: This snippet demonstrates how to log the custom Paraphrase Mining Model with MLflow. It initiates an MLflow run, logs the model with necessary parameters, and ensures model reproducibility and version control.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/paraphrase-mining/paraphrase-mining-sentence-transformers.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    model_info = mlflow.pyfunc.log_model(\n        \"paraphrase_model\",\n        python_model=ParaphraseMiningModel(),\n        input_example=input_example,\n        signature=signature,\n        artifacts=artifacts,\n        pip_requirements=[\"sentence_transformers\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging MLflow Model with Updated Dependencies\nDESCRIPTION: This snippet demonstrates how to log a new MLflow model with updated dependencies. It includes 'opencv-python' as an extra pip requirement when logging the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.pyfunc.log_model(\n    artifact_path=\"model\",\n    python_model=python_model,\n    extra_pip_requirements=[\"opencv-python==4.8.0\"],\n    input_example=input_data,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Child Runs in MLflow Using Search\nDESCRIPTION: This snippet shows how to retrieve all child runs associated with a specific parent run using MLflow's search_runs function with tag filtering.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nchild_runs = mlflow.search_runs(\n    experiment_ids=[YOUR_EXPERIMENT_ID],\n    filter_string=f\"tags.mlflow.parentRunId = '{parent_run.info.run_id}'\",\n)\nprint(\"child runs:\")\nprint(results[[\"run_id\", \"params.p\", \"metrics.val_loss\"]])\n# child runs:\n#             run_id params.p  metrics.val_loss\n#   0        0c0b...     0.01            0.1234\n#   0        c2a0...     0.02            0.0890\n#   0        g2j1...     0.03            0.1567\n```\n\n----------------------------------------\n\nTITLE: Defining an MLflow Model From Code with Dependencies in Python\nDESCRIPTION: Creates an MLflow model that uses the previously defined add function as a dependency. The model is defined in a separate Python file and uses MLflow's PythonModel class.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/models-from-code/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# If running in a Jupyter or Databricks notebook cell, uncomment the following line:\n# %%writefile \"./math_model.py\"\n\nfrom mlflow.pyfunc import PythonModel\nfrom mlflow.models import set_model\n\nfrom calculator import add\n\n\nclass AddModel(PythonModel):\n    def predict(self, context, model_input, params=None):\n        return add(model_input[\"x\"], model_input[\"y\"])\n\n\nset_model(AddModel())\n```\n\n----------------------------------------\n\nTITLE: Statsmodels OLS Regression with MLflow Integration\nDESCRIPTION: This example demonstrates how to create, train, and log a Statsmodels OLS regression model with MLflow. It shows how to use the autolog feature to automatically log parameters and metrics, and then load the model using PyFunc interface to generate predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\nimport statsmodels.formula.api as smf\n\n# load the diabetes dataset from sklearn\ndiabetes = load_diabetes()\n\n# create X and y dataframes for the features and target\nX = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\ny = pd.DataFrame(data=diabetes.target, columns=[\"target\"])\n\n# concatenate X and y dataframes\ndf = pd.concat([X, y], axis=1)\n\n# create the linear regression model (ordinary least squares)\nmodel = smf.ols(\n    formula=\"target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6\", data=df\n)\n\nmlflow.statsmodels.autolog(\n    log_models=True,\n    disable=False,\n    exclusive=False,\n    disable_for_unsupported_versions=False,\n    silent=False,\n    registered_model_name=None,\n)\n\nwith mlflow.start_run():\n    res = model.fit(method=\"pinv\", use_t=True)\n    model_info = mlflow.statsmodels.log_model(res, artifact_path=\"OLS_model\")\n\n# load the pyfunc model\nstatsmodels_pyfunc = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# generate predictions\npredictions = statsmodels_pyfunc.predict(X)\nprint(predictions)\n```\n\n----------------------------------------\n\nTITLE: MLflow Custom Flavor Save Model Implementation\nDESCRIPTION: Implementation of save_model() function for persisting models with the custom flavor\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndef save_model(\n    sktime_model,\n    path,\n    conda_env=None,\n    code_paths=None,\n    mlflow_model=None,\n    signature=None,\n    input_example=None,\n    pip_requirements=None,\n    extra_pip_requirements=None,\n    serialization_format=SERIALIZATION_FORMAT_PICKLE,\n):\n    _validate_env_arguments(conda_env, pip_requirements, extra_pip_requirements)\n\n    if serialization_format not in SUPPORTED_SERIALIZATION_FORMATS:\n        raise MlflowException(\n            message=(\n                f\"Unrecognized serialization format: {serialization_format}. \"\n                \"Please specify one of the following supported formats: \"\n                \"{SUPPORTED_SERIALIZATION_FORMATS}.\"\n            ),\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    _validate_and_prepare_target_save_path(path)\n    code_dir_subpath = _validate_and_copy_code_paths(code_paths, path)\n\n    if mlflow_model is None:\n        mlflow_model = Model()\n    if signature is not None:\n        mlflow_model.signature = signature\n    if input_example is not None:\n        _save_example(mlflow_model, input_example, path)\n\n    model_data_subpath = \"model.pkl\"\n    model_data_path = os.path.join(path, model_data_subpath)\n    _save_model(\n        sktime_model, model_data_path, serialization_format=serialization_format\n    )\n\n    pyfunc.add_to_model(\n        mlflow_model,\n        loader_module=\"flavor\",\n        model_path=model_data_subpath,\n        conda_env=_CONDA_ENV_FILE_NAME,\n        python_env=_PYTHON_ENV_FILE_NAME,\n        code=code_dir_subpath,\n    )\n\n    mlflow_model.add_flavor(\n        FLAVOR_NAME,\n        pickled_model=model_data_subpath,\n        sktime_version=sktime.__version__,\n        serialization_format=serialization_format,\n        code=code_dir_subpath,\n    )\n    mlflow_model.save(os.path.join(path, MLMODEL_FILE_NAME))\n\n    if conda_env is None:\n        if pip_requirements is None:\n            include_cloudpickle = (\n                serialization_format == SERIALIZATION_FORMAT_CLOUDPICKLE\n            )\n            default_reqs = get_default_pip_requirements(include_cloudpickle)\n            inferred_reqs = mlflow.models.infer_pip_requirements(\n                path, FLAVOR_NAME, fallback=default_reqs\n            )\n            default_reqs = sorted(set(inferred_reqs).union(default_reqs))\n        else:\n            default_reqs = None\n        conda_env, pip_requirements, pip_constraints = _process_pip_requirements(\n            default_reqs, pip_requirements, extra_pip_requirements\n        )\n    else:\n        conda_env, pip_requirements, pip_constraints = _process_conda_env(conda_env)\n\n    with open(os.path.join(path, _CONDA_ENV_FILE_NAME), \"w\") as f:\n        yaml.safe_dump(conda_env, stream=f, default_flow_style=False)\n\n    if pip_constraints:\n        write_to(os.path.join(path, _CONSTRAINTS_FILE_NAME), \"\\n\".join(pip_constraints))\n\n    write_to(os.path.join(path, _REQUIREMENTS_FILE_NAME), \"\\n\".join(pip_requirements))\n\n    _PythonEnv.current().to_yaml(os.path.join(path, _PYTHON_ENV_FILE_NAME))\n```\n\n----------------------------------------\n\nTITLE: Testing Inference on Day-of-Week Models with MLflow in Python\nDESCRIPTION: This snippet demonstrates how to load a specific day's model (Tuesday) from MLflow and perform inference using the training data. It showcases model loading and prediction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load Tuesday's model\ntuesday_dow = 1\nmodel_name = f\"{DOW_MODEL_NAME_PREFIX}{tuesday_dow}\"\nmodel_uri = f\"models:/{model_name}/latest\"\nmodel = mlflow.sklearn.load_model(model_uri)\n\n# Perform inference using our training data for Tuesday\npredictor_columns = [column for column in df.columns if column not in {\"y\", \"dow\"}]\nhead_of_training_data = df.loc[df[\"dow\"] == tuesday_dow, predictor_columns].head()\ntuesday_fitted_values = model.predict(head_of_training_data)\nprint(tuesday_fitted_values)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Cosine Similarity Scores with Matplotlib in Python\nDESCRIPTION: This snippet creates a histogram of cosine similarity scores using matplotlib. It helps visualize the distribution of relevance scores across all question-chunk pairs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nscores = embedded_queries[\"cossim\"].to_list()\nplt.hist(scores, bins=5)\n```\n\n----------------------------------------\n\nTITLE: Logging and Saving the VADER Model to MLflow\nDESCRIPTION: Starts an MLflow run to log parameters and save the VADER sentiment analysis model. This uses MLflow's PythonModel functionality to save both the model logic and its dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Save the model\nwith mlflow.start_run(run_name=\"Vader Sentiment Analysis\") as run:\n    model_path = f\"{model_path}-{run.info.run_uuid}\"\n    mlflow.log_param(\"algorithm\", \"VADER\")\n    mlflow.log_param(\"total_sentiments\", len(INPUT_TEXTS))\n    mlflow.pyfunc.save_model(\n        path=model_path, python_model=vader_model, conda_env=conda_env\n    )\n```\n\n----------------------------------------\n\nTITLE: Managing Permissions in MLflow\nDESCRIPTION: Example demonstrating how to create users and manage experiment permissions. Shows the process of creating users, creating an experiment, and then granting specific permissions to a user for that experiment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_TRACKING_USERNAME=admin\nexport MLFLOW_TRACKING_PASSWORD=password\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\nfrom mlflow.server import get_app_client\n\ntracking_uri = \"http://localhost:5000/\"\n\nauth_client = get_app_client(\"basic-auth\", tracking_uri=tracking_uri)\nauth_client.create_user(username=\"user1\", password=\"pw1\")\nauth_client.create_user(username=\"user2\", password=\"pw2\")\n\nclient = MlflowClient(tracking_uri=tracking_uri)\nexperiment_id = client.create_experiment(name=\"experiment\")\n\nauth_client.create_experiment_permission(\n    experiment_id=experiment_id, username=\"user2\", permission=\"MANAGE\"\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Final Query Step in Python Workflow\nDESCRIPTION: This code defines the query_result step in the workflow, which uses the reranked context and user query to generate the final answer using an LLM.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    @step\n    async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n        \"\"\"Get result with relevant text.\"\"\"\n        query = await ctx.get(\"query\")\n\n        prompt = FINAL_QUERY_TEMPLATE.format(context=ev.context, query=query)\n        response = self.llm.complete(prompt).text\n        return StopEvent(result=response)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Metric Function for Model Evaluation in Python\nDESCRIPTION: Defines a custom metric function that calculates the sum of squared differences between predictions and targets, plus one. This demonstrates how to create metrics based on prediction and target columns.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ndef squared_diff_plus_one(eval_df, _builtin_metrics):\n    \"\"\"\n    This example custom metric function creates a metric based on the ``prediction`` and\n    ``target`` columns in ``eval_df`.\n    \"\"\"\n    return np.sum(np.abs(eval_df[\"prediction\"] - eval_df[\"target\"] + 1) ** 2)\n```\n\n----------------------------------------\n\nTITLE: Batch Inference with MLflow Python API\nDESCRIPTION: Python code to run batch inference using MLflow's Python API. Loads a model from a run, makes predictions on a CSV file, and writes the results to another CSV file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmodel = mlflow.pyfunc.load_model(\"runs:/<run_id>/model\")\npredictions = model.predict(pd.read_csv(\"input.csv\"))\npredictions.to_csv(\"output.csv\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Arguments and Initializing Trainer with Transformers\nDESCRIPTION: Sets up the training environment by configuring TrainingArguments with parameters like output directory, evaluation strategy, batch sizes, and training epochs. Then initializes a Trainer instance with the model, arguments, datasets, and evaluation metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Checkpoints will be output to this `training_output_dir`.\ntraining_output_dir = \"/tmp/sms_trainer\"\ntraining_args = TrainingArguments(\n    output_dir=training_output_dir,\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_steps=8,\n    num_train_epochs=3,\n)\n\n# Instantiate a `Trainer` instance that will be used to initiate a training run.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tokenized,\n    eval_dataset=test_tokenized,\n    compute_metrics=compute_metrics,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions with Statsmodels PyFunc in Python\nDESCRIPTION: This snippet demonstrates how to use a saved Statsmodels model to generate predictions using the PyFunc flavor in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# generate predictions\nprediction_data = pd.DataFrame({\"start\": start, \"end\": end}, index=[0])\npredictions = statsmodels_pyfunc.predict(prediction_data)\nprint(predictions)\n```\n\n----------------------------------------\n\nTITLE: Training Wine Quality Prediction Model with MLflow and ElasticNet\nDESCRIPTION: Function that trains an ElasticNet regression model to predict wine quality. It loads data, splits it into training and test sets, trains the model with specified hyperparameters, evaluates performance metrics (RMSE, MAE, R2), and logs everything to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sklearn_elasticnet_wine/train.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport warnings\n\n\n# Wine Quality Sample\ndef train(in_alpha, in_l1_ratio):\n    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model import ElasticNet\n    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n    from sklearn.model_selection import train_test_split\n\n    import mlflow\n    import mlflow.sklearn\n    from mlflow.models import infer_signature\n\n    logging.basicConfig(level=logging.WARN)\n    logger = logging.getLogger(__name__)\n\n    def eval_metrics(actual, pred):\n        rmse = np.sqrt(mean_squared_error(actual, pred))\n        mae = mean_absolute_error(actual, pred)\n        r2 = r2_score(actual, pred)\n        return rmse, mae, r2\n\n    warnings.filterwarnings(\"ignore\")\n    np.random.seed(40)\n\n    # Read the wine-quality csv file from the URL\n    csv_url = (\n        \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n    )\n    try:\n        data = pd.read_csv(csv_url, sep=\";\")\n    except Exception as e:\n        logger.exception(\n            f\"Unable to download training & test CSV, check your internet connection. Error: {e}\"\n        )\n\n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is \"quality\" which is a scalar from [3, 9]\n    train_x = train.drop([\"quality\"], axis=1)\n    test_x = test.drop([\"quality\"], axis=1)\n    train_y = train[[\"quality\"]]\n    test_y = test[[\"quality\"]]\n\n    # Set default values if no alpha is provided\n    alpha = 0.5 if float(in_alpha) is None else float(in_alpha)\n\n    # Set default values if no l1_ratio is provided\n    l1_ratio = 0.5 if float(in_l1_ratio) is None else float(in_l1_ratio)\n\n    # Useful for multiple runs (only doing one run in this sample notebook)\n    with mlflow.start_run():\n        # Execute ElasticNet\n        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n        lr.fit(train_x, train_y)\n\n        # Evaluate Metrics\n        predicted_qualities = lr.predict(test_x)\n        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n        # Print out metrics\n        print(f\"Elasticnet model (alpha={alpha:f}, l1_ratio={l1_ratio:f}):\")\n        print(f\"  RMSE: {rmse}\")\n        print(f\"  MAE: {mae}\")\n        print(f\"  R2: {r2}\")\n\n        # Infer model signature\n        predictions = lr.predict(train_x)\n        signature = infer_signature(train_x, predictions)\n\n        # Log parameter, metrics, and model to MLflow\n        mlflow.log_param(\"alpha\", alpha)\n        mlflow.log_param(\"l1_ratio\", l1_ratio)\n        mlflow.log_metric(\"rmse\", rmse)\n        mlflow.log_metric(\"r2\", r2)\n        mlflow.log_metric(\"mae\", mae)\n\n        mlflow.sklearn.log_model(lr, \"model\", signature=signature)\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Tracking Server with SQLite Backend in Bash\nDESCRIPTION: Command to start the MLflow tracking server with a SQLite database as the backend store. This configures the server to use a local SQLite database for storing run metadata.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --backend-store-uri sqlite:///my.db\n```\n\n----------------------------------------\n\nTITLE: Setting Up Azure OpenAI Environment Variables\nDESCRIPTION: Configuration of environment variables for Azure OpenAI integration, including API keys and endpoint settings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=SCOPE_NAME, key=KEY_NAME)  # noqa: F821\nos.environ[\"OPENAI_API_TYPE\"] = \"azure\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://<NAME_OF_YOUR_INSTANCE>.openai.azure.com/\"  # replace this!\nos.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o-mini\"\nos.environ[\"OPENAI_ENGINE\"] = \"gpt-4o-mini\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Logging Explicit Model Signatures in MLflow\nDESCRIPTION: This example shows two approaches to explicitly create a model signature for a Scikit-learn classifier: manually constructing a signature object with schema definitions or using the infer_signature function to derive it from training data and predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import ModelSignature, infer_signature\nfrom mlflow.types.schema import Schema, ColSpec\n\n# Option 1: Manually construct the signature object\ninput_schema = Schema(\n    [\n        ColSpec(\"double\", \"sepal length (cm)\"),\n        ColSpec(\"double\", \"sepal width (cm)\"),\n        ColSpec(\"double\", \"petal length (cm)\"),\n        ColSpec(\"double\", \"petal width (cm)\"),\n    ]\n)\noutput_schema = Schema([ColSpec(\"long\")])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\n# Option 2: Infer the signature\nsignature = infer_signature(iris_train, clf.predict(iris_train))\n\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(clf, \"iris_rf\", signature=signature)\n```\n\n----------------------------------------\n\nTITLE: Establishing RetrievalQA Chain and Logging with MLflow in Python\nDESCRIPTION: This snippet shows how to create a RetrievalQA chain using OpenAI and FAISS, define a loader function for retrieval, and log the model with MLflow. It demonstrates the integration of a RAG application with MLflow for tracking and easy retrieval.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"Legal RAG\")\n\nretrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=vector_db.as_retriever())\n\n\n# Log the retrievalQA chain\ndef load_retriever(persist_directory):\n    embeddings = OpenAIEmbeddings()\n    vectorstore = FAISS.load_local(\n        persist_directory,\n        embeddings,\n        allow_dangerous_deserialization=True,  # This is required to load the index from MLflow\n    )\n    return vectorstore.as_retriever()\n\n\nwith mlflow.start_run() as run:\n    model_info = mlflow.langchain.log_model(\n        retrievalQA,\n        artifact_path=\"retrieval_qa\",\n        loader_fn=load_retriever,\n        persist_dir=persist_dir,\n    )\n```\n\n----------------------------------------\n\nTITLE: Fetching Documents and Creating FAISS Database in Python\nDESCRIPTION: This snippet defines two functions: one for fetching and saving documents from URLs, and another for creating a FAISS database from the saved documents. These functions are essential for setting up a Retrieval-Augmented Generation (RAG) application.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef fetch_and_save_documents(url_list, doc_path):\n    \"\"\"\n    Fetches documents from given URLs and saves them to a specified file path.\n\n    Args:\n        url_list (list): List of URLs to fetch documents from.\n        doc_path (str): Path to the file where documents will be saved.\n    \"\"\"\n    for url in url_list:\n        document = fetch_federal_document(url, \"col-sm-9\")\n        with open(doc_path, \"a\") as file:\n            file.write(document)\n\n\ndef create_faiss_database(document_path, database_save_directory, chunk_size=500, chunk_overlap=10):\n    \"\"\"\n    Creates and saves a FAISS database using documents from the specified file.\n\n    Args:\n        document_path (str): Path to the file containing documents.\n        database_save_directory (str): Directory where the FAISS database will be saved.\n        chunk_size (int, optional): Size of each document chunk. Default is 500.\n        chunk_overlap (int, optional): Overlap between consecutive chunks. Default is 10.\n\n    Returns:\n        FAISS database instance.\n    \"\"\"\n    # Load documents from the specified file\n    document_loader = TextLoader(document_path)\n    raw_documents = document_loader.load()\n\n    # Split documents into smaller chunks with specified size and overlap\n    document_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    document_chunks = document_splitter.split_documents(raw_documents)\n\n    # Generate embeddings for each document chunk\n    embedding_generator = OpenAIEmbeddings()\n    faiss_database = FAISS.from_documents(document_chunks, embedding_generator)\n\n    # Save the FAISS database to the specified directory\n    faiss_database.save_local(database_save_directory)\n\n    return faiss_database\n```\n\n----------------------------------------\n\nTITLE: Implementing ML Model Training and Visualization Logging with MLflow in Python\nDESCRIPTION: A complete machine learning workflow that trains a Ridge regression model, calculates various error metrics, generates multiple visualization plots, and logs everything to MLflow. This includes model parameters, metrics, and multiple types of visualizations for model evaluation and data exploration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(\"Visualizations Demo\")\n\nX = my_data.drop(columns=[\"demand\", \"date\"])\ny = my_data[\"demand\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nfig1 = plot_time_series_demand(my_data, window_size=28)\nfig2 = plot_box_weekend(my_data)\nfig3 = plot_scatter_demand_price(my_data)\nfig4 = plot_density_weekday_weekend(my_data)\n\n# Execute the correlation plot, saving the plot to a local temporary directory\nplot_correlation_matrix_and_save(my_data)\n\n# Define our Ridge model\nmodel = Ridge(alpha=1.0)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Calculate error metrics\nmse = mean_squared_error(y_test, y_pred)\nrmse = math.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nmsle = mean_squared_log_error(y_test, y_pred)\nmedae = median_absolute_error(y_test, y_pred)\n\n# Generate prediction-dependent plots\nfig5 = plot_residuals(y_test, y_pred)\nfig6 = plot_coefficients(model, X_test.columns)\nfig7 = plot_prediction_error(y_test, y_pred)\nfig8 = plot_qq(y_test, y_pred)\n\n# Start an MLflow run for logging metrics, parameters, the model, and our figures\nwith mlflow.start_run() as run:\n    # Log the model\n    mlflow.sklearn.log_model(sk_model=model, input_example=X_test, artifact_path=\"model\")\n\n    # Log the metrics\n    mlflow.log_metrics(\n        {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"msle\": msle, \"medae\": medae}\n    )\n\n    # Log the hyperparameter\n    mlflow.log_param(\"alpha\", 1.0)\n\n    # Log plots\n    mlflow.log_figure(fig1, \"time_series_demand.png\")\n    mlflow.log_figure(fig2, \"box_weekend.png\")\n    mlflow.log_figure(fig3, \"scatter_demand_price.png\")\n    mlflow.log_figure(fig4, \"density_weekday_weekend.png\")\n    mlflow.log_figure(fig5, \"residuals_plot.png\")\n    mlflow.log_figure(fig6, \"coefficients_plot.png\")\n    mlflow.log_figure(fig7, \"prediction_errors.png\")\n    mlflow.log_figure(fig8, \"qq_plot.png\")\n\n    # Log the saved correlation matrix plot by referring to the local file system location\n    mlflow.log_artifact(\"/tmp/corr_plot.png\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Scoring a Registered MLflow Model\nDESCRIPTION: This snippet shows how to load a registered model from the MLflow Model Registry using its name and version, and then use it to make predictions on test data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nmodel_uri = f\"models:/{reg_model_name}/1\"\nloaded_model = mlflow.sklearn.load_model(model_uri)\n\ndiabetes_y_pred = loaded_model.predict(diabetes_X_test)\nprint_predictions(loaded_model, diabetes_y_pred)\n```\n\n----------------------------------------\n\nTITLE: Creating Explicit Tensor-based Model Signatures in MLflow\nDESCRIPTION: This example shows two approaches to explicitly create a model signature for a TensorFlow neural network: manually constructing a signature with TensorSpec schema definitions or using infer_signature to derive it from test data and predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom mlflow.models import ModelSignature, infer_signature\nfrom mlflow.types.schema import Schema, TensorSpec\n\n# Option 1: Manually construct the signature object\ninput_schema = Schema(\n    [\n        TensorSpec(np.dtype(np.float64), (-1, 28, 28, 1)),\n    ]\n)\noutput_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 10))])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\n# Option 2: Infer the signature\nsignature = infer_signature(testX, model.predict(testX))\n\nwith mlflow.start_run():\n    mlflow.tensorflow.log_model(model, \"mnist_cnn\", signature=signature)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Saved PyTorch Model from MLflow\nDESCRIPTION: Demonstrates how to load a previously saved PyTorch model from MLflow and use it for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/logging/pytorch_log_model.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Run id from the run above:\", run.info.run_id)\n\n# Later, or in a different script, you can load the model using the run ID\nloaded_model = mlflow.pytorch.load_model(f\"runs:/{run.info.run_id}/model\")\n\n# you can now use the loaded model as you would've used the original pytorch model!\nloaded_model.eval()\nwith torch.no_grad():\n    sample_input = X_test_tensor[:1]\n    loaded_output = loaded_model(sample_input)\n    og_output = model(sample_input)\n    print(\"Original model output:\", og_output)\n    print(\"Loaded model output:\", loaded_output)\n```\n\n----------------------------------------\n\nTITLE: Logging Model and Metrics to MLflow\nDESCRIPTION: Log the trained model, hyperparameters, metrics, and model signature to MLflow tracking server within a run context.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/notebooks/tracking_quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Start an MLflow run\nwith mlflow.start_run():\n    # Log the hyperparameters\n    mlflow.log_params(params)\n\n    # Log the loss metric\n    mlflow.log_metric(\"accuracy\", accuracy)\n\n    # Set a tag that we can use to remind ourselves what this run was for\n    mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")\n\n    # Infer the model signature\n    signature = infer_signature(X_train, lr.predict(X_train))\n\n    # Log the model\n    model_info = mlflow.sklearn.log_model(\n        sk_model=lr,\n        artifact_path=\"iris_model\",\n        signature=signature,\n        input_example=X_train,\n        registered_model_name=\"tracking-quickstart\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Search Retrieve Event in Python\nDESCRIPTION: This snippet defines a Pydantic model for the VectorSearchRetrieveEvent, which is used to trigger the vector search step in the workflow by passing the user's query.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass VectorSearchRetrieveEvent(Event):\n    \"\"\"Event for triggering VectorStore index retrieval step.\"\"\"\n    query: str\n```\n\n----------------------------------------\n\nTITLE: Orchestrating Hyperparameter Tuning with MLflow Tracking\nDESCRIPTION: This code orchestrates the entire workflow, starting an MLflow parent run that includes Optuna hyperparameter tuning, logging best parameters and metrics, training the final model with optimal parameters, creating and logging visualization artifacts, and storing the model in MLflow's model registry.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Initiate the parent run and call the hyperparameter tuning child run logic\nwith mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n    # Initialize the Optuna study\n    study = optuna.create_study(direction=\"minimize\")\n\n    # Execute the hyperparameter optimization trials.\n    # Note the addition of the `champion_callback` inclusion to control our logging\n    study.optimize(objective, n_trials=500, callbacks=[champion_callback])\n\n    mlflow.log_params(study.best_params)\n    mlflow.log_metric(\"best_mse\", study.best_value)\n    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n\n    # Log tags\n    mlflow.set_tags(\n        tags={\n            \"project\": \"Apple Demand Project\",\n            \"optimizer_engine\": \"optuna\",\n            \"model_family\": \"xgboost\",\n            \"feature_set_version\": 1,\n        }\n    )\n\n    # Log a fit model instance\n    model = xgb.train(study.best_params, dtrain)\n\n    # Log the correlation plot\n    mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")\n\n    # Log the feature importances plot\n    importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))\n    mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")\n\n    # Log the residuals plot\n    residuals = plot_residuals(model, dvalid, valid_y)\n    mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")\n\n    artifact_path = \"model\"\n\n    mlflow.xgboost.log_model(\n        xgb_model=model,\n        artifact_path=artifact_path,\n        input_example=train_x.iloc[[0]],\n        model_format=\"ubj\",\n        metadata={\"model_data_version\": 1},\n    )\n\n    # Get the logged model uri so that we can load it from the artifact store\n    model_uri = mlflow.get_artifact_uri(artifact_path)\n\n```\n\n----------------------------------------\n\nTITLE: Logging MLflow Model with Input Example and Type Hints\nDESCRIPTION: Shows how to log a PythonModel with an input example that matches the type hints. The input example is used to validate the type hints and check if the predict function works as expected.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.pyfunc.log_model(\n    artifact_path=\"model\",\n    python_model=CustomModel(),\n    input_example=[\"a\", \"b\", \"c\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Logging DSPy Model to MLflow\nDESCRIPTION: Demonstrates how to log a DSPy model to MLflow, including specifying a model signature and input example.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Start an MLflow run\nwith mlflow.start_run():\n    # Log the model\n    model_info = mlflow.dspy.log_model(\n        dspy_model,\n        artifact_path=\"model\",\n        input_example=\"what is 2 + 2?\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Automatic Tracing with OpenAI SDK\nDESCRIPTION: This code demonstrates the simplest way to enable MLflow automatic tracing with the OpenAI Python SDK. It shows how to set up autologging and make a standard chat completion request with OpenAI, with the trace data automatically being captured for viewing in the MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport openai\n\n# Enable MLflow automatic tracing for OpenAI with one line of code!\nmlflow.openai.autolog()\n\n# Use OpenAI Python SDK as usual\nopenai.OpenAI().chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a chatbot.\"},\n        {\"role\": \"user\", \"content\": \"What is the weather like today?\"},\n    ],\n)\n\n# Then go to MLflow UI (if not started, run `mlflow ui` in your terminal) to see the trace!\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Apple Sales Data in Python\nDESCRIPTION: This code snippet displays the generated apple sales data stored in the 'my_data' variable. It allows for quick inspection of the synthetic dataset.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_data\n```\n\n----------------------------------------\n\nTITLE: Training Day-of-Week Models with MLflow in Python\nDESCRIPTION: This code trains separate Random Forest models for each day of the week using the created dataset. It logs each model with MLflow, including model signature and tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor dow in df[\"dow\"].unique():\n    # Create dataset corresponding to a single day of the week\n    X = df.loc[df[\"dow\"] == dow]\n    X.pop(\"dow\")  # Remove DOW as a predictor column\n    y = X.pop(\"y\")\n\n    # Fit our DOW model\n    model = RandomForestRegressor().fit(X, y)\n\n    # Infer signature of the model\n    signature = infer_signature(X, model.predict(X))\n\n    with mlflow.start_run():\n        model_path = f\"model_{dow}\"\n\n        # Log and register our DOW model with signature\n        mlflow.sklearn.log_model(\n            model,\n            model_path,\n            signature=signature,\n            registered_model_name=f\"{DOW_MODEL_NAME_PREFIX}{dow}\",\n        )\n        mlflow.set_tag(\"dow\", dow)\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Autolog for LangChain in Python\nDESCRIPTION: This snippet demonstrates how to enable automatic tracing for LangChain using MLflow's autolog function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/langchain.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.langchain.autolog()\n```\n\n----------------------------------------\n\nTITLE: MLflow Java Client Usage Example\nDESCRIPTION: Complete example demonstrating usage of MLflow Java client API for creating experiments, runs, logging parameters and metrics, and retrieving experiment details.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/java/client/README.md#2025-04-07_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npackage org.mlflow.tracking.samples;\n\nimport java.util.List;\nimport java.util.Optional;\n\nimport org.apache.log4j.Level;\nimport org.apache.log4j.LogManager;\n\nimport org.mlflow.api.proto.Service.*;\nimport org.mlflow.tracking.MlflowClient;\n\n/**\n * This is an example application which uses the MLflow Tracking API to create and manage\n * experiments and runs.\n */\npublic class QuickStartDriver {\n  public static void main(String[] args) throws Exception {\n    (new QuickStartDriver()).process(args);\n  }\n\n  void process(String[] args) throws Exception {\n    MlflowClient client;\n    if (args.length < 1) {\n      client = new MlflowClient();\n    } else {\n      client = new MlflowClient(args[0]);\n    }\n\n    boolean verbose = args.length >= 2 && \"true\".equals(args[1]);\n    if (verbose) {\n      LogManager.getLogger(\"org.mlflow.client\").setLevel(Level.DEBUG);\n    }\n\n    System.out.println(\"====== createExperiment\");\n    String expName = \"Exp_\" + System.currentTimeMillis();\n    String expId = client.createExperiment(expName);\n    System.out.println(\"createExperiment: expId=\" + expId);\n\n    System.out.println(\"====== getExperiment\");\n    GetExperiment.Response exp = client.getExperiment(expId);\n    System.out.println(\"getExperiment: \" + exp);\n\n    System.out.println(\"====== searchExperiments\");\n    List<Experiment> exps = client.searchExperiments();\n    System.out.println(\"#experiments: \" + exps.size());\n    exps.forEach(e -> System.out.println(\"  Exp: \" + e));\n\n    createRun(client, expId);\n\n    System.out.println(\"====== getExperiment again\");\n    GetExperiment.Response exp2 = client.getExperiment(expId);\n    System.out.println(\"getExperiment: \" + exp2);\n\n    System.out.println(\"====== getExperiment by name\");\n    Optional<Experiment> exp3 = client.getExperimentByName(expName);\n    System.out.println(\"getExperimentByName: \" + exp3);\n  }\n\n  void createRun(MlflowClient client, String expId) {\n    System.out.println(\"====== createRun\");\n\n    // Create run\n    String sourceFile = \"MyFile.java\";\n    RunInfo runCreated = client.createRun(expId, sourceFile);\n    System.out.println(\"CreateRun: \" + runCreated);\n    String runId = runCreated.getRunUuid();\n\n    // Log parameters\n    client.logParam(runId, \"min_samples_leaf\", \"2\");\n    client.logParam(runId, \"max_depth\", \"3\");\n\n    // Log metrics\n    client.logMetric(runId, \"auc\", 2.12F);\n    client.logMetric(runId, \"accuracy_score\", 3.12F);\n    client.logMetric(runId, \"zero_one_loss\", 4.12F);\n\n    // Update finished run\n    client.setTerminated(runId, RunStatus.FINISHED);\n\n    // Get run details\n    Run run = client.getRun(runId);\n    System.out.println(\"GetRun: \" + run);\n    client.close();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Parallel MLflow Experiments with Multiprocessing in Python\nDESCRIPTION: Shows how to execute multiple MLflow runs in parallel using Python's multiprocessing module. Each process creates a new run, allowing for concurrent execution of experiments.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport multiprocessing as mp\n\n\ndef train_model(param):\n    with mlflow.start_run():\n        mlflow.log_param(\"p\", param)\n        ...\n\n\nif __name__ == \"__main__\":\n    mlflow.set_experiment(\"multi-process\")\n    params = [0.01, 0.02, ...]\n    with mp.Pool(processes=4) as pool:\n        pool.map(train_model, params)\n```\n\n----------------------------------------\n\nTITLE: Logging to MLflow Tracking Server in Python\nDESCRIPTION: Python code to connect to a remote MLflow tracking server, set the experiment, and log parameters and metrics. This demonstrates how to use the MLflow Python API to log data to a tracking server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nremote_server_uri = \"...\"  # set to your server URI, e.g. http://127.0.0.1:8080\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(\"/my-experiment\")\nwith mlflow.start_run():\n    mlflow.log_param(\"a\", 1)\n    mlflow.log_metric(\"b\", 2)\n```\n\n----------------------------------------\n\nTITLE: Reconstructing Translation Pipeline\nDESCRIPTION: Demonstrates how to manually reconstruct a translation pipeline from loaded components and test its functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nreconstructed_pipeline = transformers.pipeline(**translation_components)\n\nreconstructed_response = reconstructed_pipeline(\n    \"transformers makes using Deep Learning models easy and fun!\")\n\nprint(reconstructed_response)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Professionalism Metric\nDESCRIPTION: Creates a custom metric to evaluate the professionalism of model outputs using GPT-4 as the judge, with defined scoring criteria.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/qa-evaluation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.metrics.genai import EvaluationExample, make_genai_metric\n\nprofessionalism_metric = make_genai_metric(\n    name=\"professionalism\",\n    definition=(\n        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\"\n    ),\n    grading_prompt=(\n        \"Professionalism: If the answer is written using a professional tone, below \"\n        \"are the details for different scores: \"\n        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\"\n        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\"\n        \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n        \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. \"\n        \"- Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \"\n    ),\n    examples=[\n        EvaluationExample(\n            input=\"What is MLflow?\",\n            output=(\n                \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\"\n            ),\n            score=2,\n            justification=(\n                \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \"\n            ),\n        )\n    ],\n    version=\"v1\",\n    model=\"openai:/gpt-4\",\n    parameters={\"temperature\": 0.0},\n    grading_context_columns=[],\n    aggregations=[\"mean\", \"variance\", \"p90\"],\n    greater_is_better=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating a Trace with Manual Function Tracing\nDESCRIPTION: Demonstrates creating a simple trace by applying the MLflow trace decorator to a function. When the decorated function is called, it automatically generates a trace viewable in the UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Simple manual tracing example\nimport mlflow\n\n\n@mlflow.trace\ndef foo(input):\n    return input + 1\n\n\n# running foo() generates a trace\nfoo(1)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom MLflow Callback\nDESCRIPTION: Defines a custom MLflow callback to log the logarithm of the loss instead of the raw loss value.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport math\n\n\nclass MlflowCustomCallback(MlflowCallback):\n    def on_epoch_end(self, epoch, logs=None):\n        if not self.log_every_epoch:\n            return\n        loss = logs[\"loss\"]\n        logs[\"log_loss\"] = math.log(loss)\n        del logs[\"loss\"]\n        mlflow.log_metrics(logs, epoch)\n```\n\n----------------------------------------\n\nTITLE: Logging Semantic Search Model with MLflow\nDESCRIPTION: Logs the semantic search model using MLflow's Python Function flavor, including model artifacts, signature, and dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    model_info = mlflow.pyfunc.log_model(\n        \"semantic_search\",\n        python_model=SemanticSearchModel(),\n        input_example=input_example,\n        signature=signature,\n        artifacts=artifacts,\n        pip_requirements=[\"sentence_transformers\", \"numpy\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging and Loading Transformers Model Components in Python\nDESCRIPTION: This snippet shows how to log individual components of a Transformers model using MLflow and then load them as a pipeline. It demonstrates logging a model and tokenizer, then loading and using the pipeline for text classification.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define the components of the model in a dictionary\ntransformers_model = {\"model\": model, \"tokenizer\": tokenizer}\n\n# Log the model components\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=transformers_model,\n        artifact_path=\"text_classifier\",\n        task=task,\n    )\n\n# Load the components as a pipeline\nloaded_pipeline = mlflow.transformers.load_model(\n    model_info.model_uri, return_type=\"pipeline\"\n)\n\nprint(type(loaded_pipeline).__name__)\n# >> TextClassificationPipeline\n\nloaded_pipeline([\"MLflow is awesome!\", \"Transformers is a great library!\"])\n\n# >> [{'label': 'POSITIVE', 'score': 0.9998478889465332},\n# >>  {'label': 'POSITIVE', 'score': 0.9998030066490173}]\n```\n\n----------------------------------------\n\nTITLE: Deploying MLflow Model Server Using CLI\nDESCRIPTION: Command to start a local MLflow model server on port 5000 using the mlflow models serve CLI command.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve -m runs:/<run_id>/model -p 5000\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Job Template for MLflow Project Execution\nDESCRIPTION: A Kubernetes Job specification template that defines how MLflow Projects are executed as Kubernetes Jobs. The template includes resource limits, container specifications, and environment variables that MLflow will replace during execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: \"{replaced with MLflow Project name}\"\n  namespace: mlflow\nspec:\n  ttlSecondsAfterFinished: 100\n  backoffLimit: 0\n  template:\n    spec:\n      containers:\n        - name: \"{replaced with MLflow Project name}\"\n          image: \"{replaced with URI of Docker image created during Project execution}\"\n          command: [\"{replaced with MLflow Project entry point command}\"]\n          env: [\"{appended with MLFLOW_TRACKING_URI, MLFLOW_RUN_ID and MLFLOW_EXPERIMENT_ID}\"]\n          resources:\n            limits:\n              memory: 512Mi\n            requests:\n              memory: 256Mi\n      restartPolicy: Never\n```\n\n----------------------------------------\n\nTITLE: Using Async Anthropic API with MLflow Tracing\nDESCRIPTION: This example shows how to use the asynchronous API of the Anthropic SDK with MLflow Tracing. It demonstrates that the usage is the same as the synchronous API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/anthropic.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport anthropic\n\n# Enable trace logging\nmlflow.anthropic.autolog()\n\nclient = anthropic.AsyncAnthropic()\n\nresponse = await client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Queries and Chunks in Python\nDESCRIPTION: This snippet creates embeddings for both queries and text chunks using a cached language model. It applies the embedding function to each query and chunk in the dataset.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nembedded_queries = all_result_df.copy()\nembedded_queries[\"chunk_emb\"] = all_result_df[\"chunk\"].apply(\n    lambda x: np.squeeze(cached_langchain_openai_embeddings(chunk=x, cache=embeddings_cache))\n)\nembedded_queries[\"question_emb\"] = all_result_df[\"question\"].apply(\n    lambda x: np.squeeze(cached_langchain_openai_embeddings(chunk=x, cache=embeddings_cache))\n)\n```\n\n----------------------------------------\n\nTITLE: Managing Tags on Finished Traces with MLflow Python Client\nDESCRIPTION: Shows how to set and delete tags on traces that have already been completed and logged in the backend store using the MlflowClient API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/how-to.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Set a tag on a trace\nclient.set_trace_tag(request_id=request_id, key=\"tag_key\", value=\"tag_value\")\n\n# Delete a tag from a trace\nclient.delete_trace_tag(request_id=request_id, key=\"tag_key\")\n```\n\n----------------------------------------\n\nTITLE: Direct Model Evaluation Example\nDESCRIPTION: Example of evaluating a model directly using MLflow evaluate with custom metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndef model(x):\n    return x[\"inputs\"]\n\n\neval_dataset = pd.DataFrame(\n    {\n        \"targets\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        \"inputs\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nmlflow.evaluate(model, eval_dataset, targets=\"targets\", extra_metrics=[mymetric])\n```\n\n----------------------------------------\n\nTITLE: Implementing ParaphraseMiningModel with Sentence Transformers and MLflow\nDESCRIPTION: This snippet defines a custom PythonModel class for MLflow that performs paraphrase mining. It uses Sentence Transformers to generate embeddings and compare sentences. The model includes methods for loading context, predicting paraphrases, and sorting/filtering matches based on similarity scores.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/paraphrase-mining/paraphrase-mining-sentence-transformers.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\nimport mlflow\nfrom mlflow.models.signature import infer_signature\nfrom mlflow.pyfunc import PythonModel\n\n\nclass ParaphraseMiningModel(PythonModel):\n    def load_context(self, context):\n        \"\"\"Load the model context for inference, including the customer feedback corpus.\"\"\"\n        try:\n            # Load the pre-trained sentence transformer model\n            self.model = SentenceTransformer.load(context.artifacts[\"model_path\"])\n\n            # Load the customer feedback corpus from the specified file\n            corpus_file = context.artifacts[\"corpus_file\"]\n            with open(corpus_file) as file:\n                self.corpus = file.read().splitlines()\n\n        except Exception as e:\n            raise ValueError(f\"Error loading model and corpus: {e}\")\n\n    def _sort_and_filter_matches(\n        self, query: str, paraphrase_pairs: list[tuple], similarity_threshold: float\n    ):\n        \"\"\"Sort and filter the matches by similarity score.\"\"\"\n\n        # Convert to list of tuples and sort by score\n        sorted_matches = sorted(paraphrase_pairs, key=lambda x: x[1], reverse=True)\n\n        # Filter and collect paraphrases for the query, avoiding duplicates\n        query_paraphrases = {}\n        for score, i, j in sorted_matches:\n            if score < similarity_threshold:\n                continue\n\n            paraphrase = self.corpus[j] if self.corpus[i] == query else self.corpus[i]\n            if paraphrase == query:\n                continue\n\n            if paraphrase not in query_paraphrases or score > query_paraphrases[paraphrase]:\n                query_paraphrases[paraphrase] = score\n\n        return sorted(query_paraphrases.items(), key=lambda x: x[1], reverse=True)\n\n    def predict(self, context, model_input, params=None):\n        \"\"\"Predict method to perform paraphrase mining over the corpus.\"\"\"\n\n        # Validate and extract the query input\n        if isinstance(model_input, pd.DataFrame):\n            if model_input.shape[1] != 1:\n                raise ValueError(\"DataFrame input must have exactly one column.\")\n            query = model_input.iloc[0, 0]\n        elif isinstance(model_input, dict):\n            query = model_input.get(\"query\")\n            if query is None:\n                raise ValueError(\"The input dictionary must have a key named 'query'.\")\n        else:\n            raise TypeError(\n                f\"Unexpected type for model_input: {type(model_input)}. Must be either a Dict or a DataFrame.\"\n            )\n\n        # Determine the minimum similarity threshold\n        similarity_threshold = params.get(\"similarity_threshold\", 0.5) if params else 0.5\n\n        # Add the query to the corpus for paraphrase mining\n        extended_corpus = self.corpus + [query]\n\n        # Perform paraphrase mining\n        paraphrase_pairs = util.paraphrase_mining(\n            self.model, extended_corpus, show_progress_bar=False\n        )\n\n        # Convert to list of tuples and sort by score\n        sorted_paraphrases = self._sort_and_filter_matches(\n            query, paraphrase_pairs, similarity_threshold\n        )\n\n        # Warning if no paraphrases found\n        if not sorted_paraphrases:\n            warnings.warn(\"No paraphrases found above the similarity threshold.\", UserWarning)\n\n        return {sentence[0]: str(sentence[1]) for sentence in sorted_paraphrases}\n```\n\n----------------------------------------\n\nTITLE: Logging Langchain Model with Input Example\nDESCRIPTION: Shows how to log a langchain model with a properly structured input example that includes question and answer fields.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": {\n                \"question\": \"What is the primary function of control rods in a nuclear reactor?\",\n                \"answer\": \"To stir the primary coolant so that the neutrons are mixed well.\",\n            },\n        },\n    ]\n}\n\nchain_path = \"langchain_model.py\"\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(\n        lc_model=chain_path, artifact_path=\"model\", input_example=input_example\n    )\n```\n\n----------------------------------------\n\nTITLE: Complete MLflow Tracing Example with LlamaIndex Query\nDESCRIPTION: Full example showing how to enable MLflow tracing, set up a tracking URI and experiment, and query a LlamaIndex index.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/llama_index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Enabling tracing for LlamaIndex\nmlflow.llama_index.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"LlamaIndex\")\n\n# Query the index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What was the first program the author wrote?\")\n```\n\n----------------------------------------\n\nTITLE: Installing Databricks LLM Integration Package\nDESCRIPTION: Installs the necessary package for integrating LlamaIndex with Databricks-hosted LLMs as an alternative to OpenAI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%pip install llama-index-llms-databricks\n```\n\n----------------------------------------\n\nTITLE: Training and Logging with MLflow\nDESCRIPTION: Executes the training loop while logging parameters, model summary, and metrics to MLflow. After training completes, the model is saved to MLflow's model registry. This allows for tracking experiments and model versions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    params = {\n        \"epochs\": epochs,\n        \"learning_rate\": 1e-3,\n        \"batch_size\": 64,\n        \"loss_function\": loss_fn.__class__.__name__,\n        \"metric_function\": metric_fn.__class__.__name__,\n        \"optimizer\": \"SGD\",\n    }\n    # Log training parameters.\n    mlflow.log_params(params)\n\n    # Log model summary.\n    with open(\"model_summary.txt\", \"w\") as f:\n        f.write(str(summary(model)))\n    mlflow.log_artifact(\"model_summary.txt\")\n\n    for t in range(epochs):\n        print(f\"Epoch {t + 1}\\n-------------------------------\")\n        train(train_dataloader, model, loss_fn, metric_fn, optimizer, epoch=t)\n        evaluate(test_dataloader, model, loss_fn, metric_fn, epoch=0)\n\n    # Save the trained model to MLflow.\n    mlflow.pytorch.log_model(model, \"model\")\n```\n\n----------------------------------------\n\nTITLE: Logging Translation Model with MLflow\nDESCRIPTION: Demonstrates how to log a translation pipeline to MLflow with model metadata, signature, and parameters. Uses mlflow.transformers.log_model to save the model components.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=translation_pipeline,\n        artifact_path=\"french_translator\",\n        signature=signature,\n        model_params=model_params,\n    )\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Directory Structure\nDESCRIPTION: Example directory structure created by save_model() for a custom MLflow model flavor\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_61\n\nLANGUAGE: text\nCODE:\n```\nmy_model/\n├── MLmodel\n├── conda.yaml\n├── model.pkl\n├── python_env.yaml\n└── requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing Tracing Decorator in MLflow GenAI\nDESCRIPTION: Example of using the @mlflow.trace decorator to add tracing functionality to a system message generator method. The decorator captures function execution details and can be customized with span names and attributes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@mlflow.trace\ndef _get_system_message(self, role: str) -> Dict:\n    if role not in self.models:\n        raise ValueError(f\"Unknown role: {role}\")\n\n    instruction = self.models[role][\"instruction\"]\n    return ChatMessage(role=\"system\", content=instruction).to_dict()\n```\n\n----------------------------------------\n\nTITLE: Enabling Multiple Auto Tracing Integrations with LangChain and OpenAI in Python\nDESCRIPTION: This code snippet demonstrates how to enable MLflow auto tracing for both LangChain and OpenAI libraries. It sets up a LangChain chain using OpenAI as the LLM provider and invokes it with a sample prompt.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\n# Enable MLflow Tracing for both LangChain and OpenAI\nmlflow.langchain.autolog()\nmlflow.openai.autolog()\n\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_experiment(\"LangChain\")\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n\n# Define a chain that uses OpenAI as an LLM provider\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=1000)\n\nprompt_template = PromptTemplate.from_template(\n    \"Answer the question as if you are {person}, fully embodying their style, wit, personality, and habits of speech. \"\n    \"Emulate their quirks and mannerisms to the best of your ability, embracing their traits—even if they aren't entirely \"\n    \"constructive or inoffensive. The question is: {question}\"\n)\n\nchain = prompt_template | llm | StrOutputParser()\n\nchain.invoke(\n    {\n        \"person\": \"Linus Torvalds\",\n        \"question\": \"Can I just set everyone's access to sudo to make things easier?\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Updating MLflow Model Requirements with Python API\nDESCRIPTION: This snippet demonstrates how to use the MLflow Python API to update a model's dependencies. It adds a new requirement 'opencv-python' to an existing model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.models.update_model_requirements(\n    model_uri=\"runs:/<run_id>/<model_path>\",\n    operation=\"add\",\n    requirement_list=[\"opencv-python==4.8.0\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Paginating Through Trace Search Results in Python\nDESCRIPTION: Demonstrates how to use the MlflowClient.search_traces() API for pagination when dealing with large result sets.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/search.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\npage_token = None\n\nwhile True:\n    results = client.search_traces(\n        experiment_ids=experiment_ids,\n        page_token=page_token,\n        max_results=100,  # Number of results per page\n    )\n\n    # Process the current page of results\n    for trace in results:\n        # Do something with each trace\n        print(trace.request_id)\n\n    # Check if there are more pages\n    if not results.token:\n        break\n\n    page_token = results.token\n```\n\n----------------------------------------\n\nTITLE: OpenAI Chat Completions with Inference Parameters\nDESCRIPTION: Demonstrates logging an OpenAI model with custom model signature including inference parameters like temperature, and making predictions with these parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/guide/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-4o-mini\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about {animal}.\"}],\n        signature=ModelSignature(\n            inputs=Schema([ColSpec(type=\"string\", name=None)]),\n            outputs=Schema([ColSpec(type=\"string\", name=None)]),\n            params=ParamSchema(\n                [ParamSpec(name=\"temperature\", default=0, dtype=\"float\")]\n            ),\n        ),\n    )\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame({\"animal\": [\"cats\", \"dogs\"]})\nprint(model.predict(df, params={\"temperature\": 1}))\n```\n\n----------------------------------------\n\nTITLE: Defining Data Loader Function with cuDF in Python\nDESCRIPTION: This function loads data from a CSV file using cuDF, a GPU-accelerated DataFrame library. It splits the data into features and target, then performs a train-test split. It's designed to work with both CPU and GPU models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/notebooks/rapids_mlflow.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef load_data(fpath):\n    \"\"\"\n    Simple helper function for loading data to be used by CPU/GPU models.\n\n    Args:\n        fpath: Path to the data to be ingested\n\n    Returns:\n        DataFrame wrapping the data at [fpath]. Data will be in either a Pandas or RAPIDS (cuDF) DataFrame\n    \"\"\"\n    import cudf\n\n    df = cudf.read_csv(fpath)\n    X = df.drop([\"target\"], axis=1)\n    y = df[\"target\"].astype(\"int32\")\n\n    return train_test_split(X, y, test_size=0.2)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using MLflow ChatModel for Inference\nDESCRIPTION: Loads the previously logged ChatModel and uses it to generate responses to weather-related queries. The model handles tool calling automatically to provide weather information for different cities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/notebooks/chat-model-tool-calling.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Load the previously logged ChatModel\ntool_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\nsystem_prompt = {\n    \"role\": \"system\",\n    \"content\": \"Please use the provided tools to answer user queries.\",\n}\n\nmessages = [\n    system_prompt,\n    {\"role\": \"user\", \"content\": \"What's the weather in Singapore?\"},\n]\n\n# Call the model's predict method\nresponse = tool_model.predict({\"messages\": messages})\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n\nmessages = [\n    system_prompt,\n    {\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"},\n]\n\n# Generating another response\nresponse = tool_model.predict({\"messages\": messages})\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n```\n\n----------------------------------------\n\nTITLE: Using Chat Message and Tool Attributes in MLflow Python\nDESCRIPTION: Illustrates how to use utility functions to set and retrieve chat messages and tools attributes for chat completion spans. It uses the @mlflow.trace decorator and demonstrates attribute setting and retrieval.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tracing-schema.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.entities.span import SpanType\nfrom mlflow.tracing.constant import SpanAttributeKey\nfrom mlflow.tracing import set_span_chat_messages, set_span_chat_tools\n\n# example messages and tools\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"please use the provided tool to answer the user's questions\",\n    },\n    {\"role\": \"user\", \"content\": \"what is 1 + 1?\"},\n]\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"add\",\n            \"description\": \"Add two numbers\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"a\": {\"type\": \"number\"},\n                    \"b\": {\"type\": \"number\"},\n                },\n                \"required\": [\"a\", \"b\"],\n            },\n        },\n    }\n]\n\n\n@mlflow.trace(span_type=SpanType.CHAT_MODEL)\ndef call_chat_model(messages, tools):\n    # mocking a response\n    response = {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"id\": \"123\",\n                \"function\": {\"arguments\": '{\"a\": 1,\"b\": 2}', \"name\": \"add\"},\n                \"type\": \"function\",\n            }\n        ],\n    }\n\n    combined_messages = messages + [response]\n\n    span = mlflow.get_current_active_span()\n    set_span_chat_messages(span, combined_messages)\n    set_span_chat_tools(span, tools)\n\n    return response\n\n\ncall_chat_model(messages, tools)\n\ntrace = mlflow.get_last_active_trace()\nspan = trace.data.spans[0]\n\nprint(\"Messages: \", span.get_attribute(SpanAttributeKey.CHAT_MESSAGES))\nprint(\"Tools: \", span.get_attribute(SpanAttributeKey.CHAT_TOOLS))\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Custom PythonModel with Type Hints in MLflow\nDESCRIPTION: This code snippet demonstrates how to create a custom PythonModel with type hints for input validation. It defines a pydantic model for input data, implements a CustomModel class extending PythonModel, and shows how to use the model with different input formats.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nimport pydantic\nimport mlflow\nfrom mlflow.pyfunc import PythonModel\n\n\n# Define the pydantic model input\nclass Message(pydantic.BaseModel):\n    role: str\n    content: str\n\n\nclass CustomModel(PythonModel):\n    # Define the model_input type hint\n    # NB: it must be list[...], check the python model type hints guide for more information\n    def predict(self, model_input: list[Message], params=None) -> list[str]:\n        return [m.content for m in model_input]\n\n\n# Construct the model and test\nmodel = CustomModel()\n\n# The input_example can be a list of Message objects as defined in the type hint\ninput_example = [\n    Message(role=\"system\", content=\"Hello\"),\n    Message(role=\"user\", content=\"Hi\"),\n]\nassert model.predict(input_example) == [\"Hello\", \"Hi\"]\n\n# The input example can also be a list of dictionaries that match the Message schema\ninput_example = [\n    {\"role\": \"system\", \"content\": \"Hello\"},\n    {\"role\": \"user\", \"content\": \"Hi\"},\n]\nassert model.predict(input_example) == [\"Hello\", \"Hi\"]\n\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow LangChain Autologging\nDESCRIPTION: Example of configuring MLflow LangChain autologging to disable trace logging and enable model logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.langchain.autolog(\n    log_traces=False,\n    log_models=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Evaluation with Custom Metrics\nDESCRIPTION: Executes MLflow evaluation on a text model using custom metrics for answer correctness and quality. Processes the first 10 rows of evaluation data with specified configuration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    results = mlflow.evaluate(\n        model_info.model_uri,\n        eval_df.head(10),\n        evaluators=\"default\",\n        model_type=\"text\",\n        targets=\"output\",\n        extra_metrics=[answer_correctness_metric, answer_quality_metric],\n        evaluator_config={\"col_mapping\": {\"inputs\": \"instruction\"}},\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining MLflow Package Dependencies\nDESCRIPTION: Specifies the exact versions and hashes of Python packages required for MLflow. This ensures reproducibility and security by pinning dependencies to specific versions and verifying package integrity.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/supply_chain_security/requirements.txt#2025-04-07_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nalembic==1.4.1 \\\n    --hash=sha256:791a5686953c4b366d3228c5377196db2f534475bb38d26f70eb69668efd9028\n    # via mlflow\ncertifi==2021.10.8 \\\n    --hash=sha256:78884e7c1d4b00ce3cea67b44566851c4343c120abd683433ce934a68ea58872 \\\n    --hash=sha256:d62a0163eb4c2344ac042ab2bdf75399a71a2d8c7d47eac2e2ee91b9d6339569\n    # via requests\ncharset-normalizer==2.0.7 \\\n    --hash=sha256:e019de665e2bcf9c2b64e2e5aa025fa991da8720daa3c1138cadd2fd1856aed0 \\\n    --hash=sha256:f7af805c321bfa1ce6714c51f254e0d5bb5e5834039bc17db7ebe3a4cec9492b\n    # via requests\nclick==8.0.3 \\\n    --hash=sha256:353f466495adaeb40b6b5f592f9f91cb22372351c84caeb068132442a4518ef3 \\\n    --hash=sha256:410e932b050f5eed773c4cda94de75971c89cdb3155a72a0831139a79e5ecb5b\n    # via\n    #   databricks-cli\n    #   flask\n    #   mlflow\ncloudpickle==2.0.0 \\\n    --hash=sha256:5cd02f3b417a783ba84a4ec3e290ff7929009fe51f6405423cfccfadd43ba4a4 \\\n    --hash=sha256:6b2df9741d06f43839a3275c4e6632f7df6487a1f181f5f46a052d3c917c3d11\n    # via mlflow\ndatabricks-cli==0.16.2 \\\n    --hash=sha256:3e9a65a19a589b795ebbd9b3b16a8e470d612d57d6216ae44a9c7a735e4080e6 \\\n    --hash=sha256:74c35b38fe686881af4b4797c4f3ab2e1c9e9ff4986f470486f152fb45d19b24\n    # via mlflow\ndocker==5.0.3 \\\n    --hash=sha256:7a79bb439e3df59d0a72621775d600bc8bc8b422d285824cb37103eab91d1ce0 \\\n    --hash=sha256:d916a26b62970e7c2f554110ed6af04c7ccff8e9f81ad17d0d40c75637e227fb\n    # via mlflow\nentrypoints==0.3 \\\n    --hash=sha256:589f874b313739ad35be6e0cd7efde2a4e9b6fea91edcc34e58ecbb8dbe56d19 \\\n    --hash=sha256:c70dd71abe5a8c85e55e12c19bd91ccfeec11a6e99044204511f9ed547d48451\n    # via mlflow\nflask==2.0.2 \\\n    --hash=sha256:7b2fb8e934ddd50731893bdcdb00fc8c0315916f9fcd50d22c7cc1a95ab634e2 \\\n    --hash=sha256:cb90f62f1d8e4dc4621f52106613488b5ba826b2e1e10a33eac92f723093ab6a\n    # via\n    #   mlflow\n    #   prometheus-flask-exporter\ngitdb==4.0.9 \\\n    --hash=sha256:8033ad4e853066ba6ca92050b9df2f89301b8fc8bf7e9324d412a63f8bf1a8fd \\\n    --hash=sha256:bac2fd45c0a1c9cf619e63a90d62bdc63892ef92387424b855792a6cabe789aa\n    # via gitpython\ngitpython==3.1.24 \\\n    --hash=sha256:dc0a7f2f697657acc8d7f89033e8b1ea94dd90356b2983bca89dc8d2ab3cc647 \\\n    --hash=sha256:df83fdf5e684fef7c6ee2c02fc68a5ceb7e7e759d08b694088d0cacb4eba59e5\n    # via mlflow\ngreenlet==1.1.2 \\\n    --hash=sha256:00e44c8afdbe5467e4f7b5851be223be68adb4272f44696ee71fe46b7036a711 \\\n    --hash=sha256:013d61294b6cd8fe3242932c1c5e36e5d1db2c8afb58606c5a67efce62c1f5fd \\\n    --hash=sha256:049fe7579230e44daef03a259faa24511d10ebfa44f69411d99e6a184fe68073 \\\n    --hash=sha256:14d4f3cd4e8b524ae9b8aa567858beed70c392fdec26dbdb0a8a418392e71708 \\\n    --hash=sha256:166eac03e48784a6a6e0e5f041cfebb1ab400b394db188c48b3a84737f505b67 \\\n    --hash=sha256:17ff94e7a83aa8671a25bf5b59326ec26da379ace2ebc4411d690d80a7fbcf23 \\\n    --hash=sha256:1e12bdc622676ce47ae9abbf455c189e442afdde8818d9da983085df6312e7a1 \\\n    --hash=sha256:21915eb821a6b3d9d8eefdaf57d6c345b970ad722f856cd71739493ce003ad08 \\\n    --hash=sha256:288c6a76705dc54fba69fbcb59904ae4ad768b4c768839b8ca5fdadec6dd8cfd \\\n    --hash=sha256:32ca72bbc673adbcfecb935bb3fb1b74e663d10a4b241aaa2f5a75fe1d1f90aa \\\n    --hash=sha256:356b3576ad078c89a6107caa9c50cc14e98e3a6c4874a37c3e0273e4baf33de8 \\\n    --hash=sha256:40b951f601af999a8bf2ce8c71e8aaa4e8c6f78ff8afae7b808aae2dc50d4c40 \\\n    --hash=sha256:572e1787d1460da79590bf44304abbc0a2da944ea64ec549188fa84d89bba7ab \\\n    --hash=sha256:58df5c2a0e293bf665a51f8a100d3e9956febfbf1d9aaf8c0677cf70218910c6 \\\n    --hash=sha256:64e6175c2e53195278d7388c454e0b30997573f3f4bd63697f88d855f7a6a1fc \\\n    --hash=sha256:7227b47e73dedaa513cdebb98469705ef0d66eb5a1250144468e9c3097d6b59b \\\n    --hash=sha256:7418b6bfc7fe3331541b84bb2141c9baf1ec7132a7ecd9f375912eca810e714e \\\n    --hash=sha256:7cbd7574ce8e138bda9df4efc6bf2ab8572c9aff640d8ecfece1b006b68da963 \\\n    --hash=sha256:7ff61ff178250f9bb3cd89752df0f1dd0e27316a8bd1465351652b1b4a4cdfd3 \\\n    --hash=sha256:833e1551925ed51e6b44c800e71e77dacd7e49181fdc9ac9a0bf3714d515785d \\\n    --hash=sha256:8639cadfda96737427330a094476d4c7a56ac03de7265622fcf4cfe57c8ae18d \\\n    --hash=sha256:8c790abda465726cfb8bb08bd4ca9a5d0a7bd77c7ac1ca1b839ad823b948ea28 \\\n    --hash=sha256:8d2f1fb53a421b410751887eb4ff21386d119ef9cde3797bf5e7ed49fb51a3b3 \\\n    --hash=sha256:903bbd302a2378f984aef528f76d4c9b1748f318fe1294961c072bdc7f2ffa3e \\\n    --hash=sha256:93f81b134a165cc17123626ab8da2e30c0455441d4ab5576eed73a64c025b25c \\\n    --hash=sha256:95e69877983ea39b7303570fa6760f81a3eec23d0e3ab2021b7144b94d06202d \\\n    --hash=sha256:9633b3034d3d901f0a46b7939f8c4d64427dfba6bbc5a36b1a67364cf148a1b0 \\\n    --hash=sha256:97e5306482182170ade15c4b0d8386ded995a07d7cc2ca8f27958d34d6736497 \\\n    --hash=sha256:9f3cba480d3deb69f6ee2c1825060177a22c7826431458c697df88e6aeb3caee \\\n    --hash=sha256:aa5b467f15e78b82257319aebc78dd2915e4c1436c3c0d1ad6f53e47ba6e2713 \\\n    --hash=sha256:abb7a75ed8b968f3061327c433a0fbd17b729947b400747c334a9c29a9af6c58 \\\n    --hash=sha256:aec52725173bd3a7b56fe91bc56eccb26fbdff1386ef123abb63c84c5b43b63a \\\n    --hash=sha256:b11548073a2213d950c3f671aa88e6f83cda6e2fb97a8b6317b1b5b33d850e06 \\\n    --hash=sha256:b1692f7d6bc45e3200844be0dba153612103db241691088626a33ff1f24a0d88 \\\n    --hash=sha256:b92e29e58bef6d9cfd340c72b04d74c4b4e9f70c9fa7c78b674d1fec18896dc4 \\\n    --hash=sha256:be5f425ff1f5f4b3c1e33ad64ab994eed12fc284a6ea71c5243fd564502ecbe5 \\\n    --hash=sha256:dd0b1e9e891f69e7675ba5c92e28b90eaa045f6ab134ffe70b52e948aa175b3c \\\n    --hash=sha256:e30f5ea4ae2346e62cedde8794a56858a67b878dd79f7df76a0767e356b1744a \\\n    --hash=sha256:e6a36bb9474218c7a5b27ae476035497a6990e21d04c279884eb10d9b290f1b1 \\\n    --hash=sha256:e859fcb4cbe93504ea18008d1df98dee4f7766db66c435e4882ab35cf70cac43 \\\n    --hash=sha256:eb6ea6da4c787111adf40f697b4e58732ee0942b5d3bd8f435277643329ba627 \\\n    --hash=sha256:ec8c433b3ab0419100bd45b47c9c8551248a5aee30ca5e9d399a0b57ac04651b \\\n    --hash=sha256:eff9d20417ff9dcb0d25e2defc2574d10b491bf2e693b4e491914738b7908168 \\\n    --hash=sha256:f0214eb2a23b85528310dad848ad2ac58e735612929c8072f6093f3585fd342d \\\n    --hash=sha256:f276df9830dba7a333544bd41070e8175762a7ac20350786b322b714b0e654f5 \\\n    --hash=sha256:f3acda1924472472ddd60c29e5b9db0cec629fbe3c5c5accb74d6d6d14773478 \\\n    --hash=sha256:f70a9e237bb792c7cc7e44c531fd48f5897961701cdaa06cf22fc14965c496cf \\\n    --hash=sha256:f9d29ca8a77117315101425ec7ec2a47a22ccf59f5593378fc4077ac5b754fce \\\n    --hash=sha256:fa877ca7f6b48054f847b61d6fa7bed5cebb663ebc55e018fda12db09dcc664c \\\n    --hash=sha256:fdcec0b8399108577ec290f55551d926d9a1fa6cad45882093a7a07ac5ec147b\n    # via sqlalchemy\ngunicorn==20.1.0 \\\n    --hash=sha256:9dcc4547dbb1cb284accfb15ab5667a0e5d1881cc443e0677b4882a4067a807e \\\n    --hash=sha256:e0a968b5ba15f8a328fdfd7ab1fcb5af4470c28aaf7e55df02a99bc13138e6e8\n    # via mlflow\nidna==3.3 \\\n    --hash=sha256:84d9dd047ffa80596e0f246e2eab0b391788b0503584e8945f2368256d2735ff \\\n    --hash=sha256:9d643ff0a55b762d5cdb124b8eaa99c66322e2157b69160bc32796e824360e6d\n    # via requests\nimportlib-metadata==4.8.2 \\\n    --hash=sha256:53ccfd5c134223e497627b9815d5030edf77d2ed573922f7a0b8f8bb81a1c100 \\\n    --hash=sha256:75bdec14c397f528724c1bfd9709d660b33a4d2e77387a3358f20b848bb5e5fb\n    # via mlflow\nitsdangerous==2.0.1 \\\n    --hash=sha256:5174094b9637652bdb841a3029700391451bd092ba3db90600dea710ba28e97c \\\n    --hash=sha256:9e724d68fc22902a1435351f84c3fb8623f303fffcc566a4cb952df8c572cff0\n    # via flask\njinja2==3.0.3 \\\n    --hash=sha256:077ce6014f7b40d03b47d1f1ca4b0fc8328a692bd284016f806ed0eaca390ad8 \\\n    --hash=sha256:611bb273cd68f3b993fabdc4064fc858c5b47a973cb5aa7999ec1ba405c87cd7\n    # via flask\njoblib==1.1.0 \\\n    --hash=sha256:4158fcecd13733f8be669be0683b96ebdbbd38d23559f54dca7205aea1bf1e35 \\\n    --hash=sha256:f21f109b3c7ff9d95f8387f752d0d9c34a02aa2f7060c2135f465da0e5160ff6\n    # via scikit-learn\nmako==1.1.5 \\\n    --hash=sha256:169fa52af22a91900d852e937400e79f535496191c63712e3b9fda5a9bed6fc3 \\\n    --hash=sha256:6804ee66a7f6a6416910463b00d76a7b25194cd27f1918500c5bd7be2a088a23\n    # via alembic\nmarkupsafe==2.0.1 \\\n    --hash=sha256:01a9b8ea66f1658938f65b93a85ebe8bc016e6769611be228d797c9d998dd298 \\\n    --hash=sha256:023cb26ec21ece8dc3907c0e8320058b2e0cb3c55cf9564da612bc325bed5e64 \\\n    --hash=sha256:0446679737af14f45767963a1a9ef7620189912317d095f2d9ffa183a4d25d2b \\\n    --hash=sha256:04635854b943835a6ea959e948d19dcd311762c5c0c6e1f0e16ee57022669194 \\\n    --hash=sha256:0717a7390a68be14b8c793ba258e075c6f4ca819f15edfc2a3a027c823718567 \\\n    --hash=sha256:0955295dd5eec6cb6cc2fe1698f4c6d84af2e92de33fbcac4111913cd100a6ff \\\n    --hash=sha256:0d4b31cc67ab36e3392bbf3862cfbadac3db12bdd8b02a2731f509ed5b829724 \\\n    --hash=sha256:10f82115e21dc0dfec9ab5c0223652f7197feb168c940f3ef61563fc2d6beb74 \\\n    --hash=sha256:168cd0a3642de83558a5153c8bd34f175a9a6e7f6dc6384b9655d2697312a646 \\\n    --hash=sha256:1d609f577dc6e1aa17d746f8bd3c31aa4d258f4070d61b2aa5c4166c1539de35 \\\n    --hash=sha256:1f2ade76b9903f39aa442b4aadd2177decb66525062db244b35d71d0ee8599b6 \\\n    --hash=sha256:20dca64a3ef2d6e4d5d615a3fd418ad3bde77a47ec8a23d984a12b5b4c74491a \\\n    --hash=sha256:2a7d351cbd8cfeb19ca00de495e224dea7e7d919659c2841bbb7f420ad03e2d6 \\\n    --hash=sha256:2d7d807855b419fc2ed3e631034685db6079889a1f01d5d9dac950f764da3dad \\\n    --hash=sha256:2ef54abee730b502252bcdf31b10dacb0a416229b72c18b19e24a4509f273d26 \\\n    --hash=sha256:36bc903cbb393720fad60fc28c10de6acf10dc6cc883f3e24ee4012371399a38 \\\n    --hash=sha256:37205cac2a79194e3750b0af2a5720d95f786a55ce7df90c3af697bfa100eaac \\\n    --hash=sha256:3c112550557578c26af18a1ccc9e090bfe03832ae994343cfdacd287db6a6ae7 \\\n    --hash=sha256:3dd007d54ee88b46be476e293f48c85048603f5f516008bee124ddd891398ed6 \\\n    --hash=sha256:4296f2b1ce8c86a6aea78613c34bb1a672ea0e3de9c6ba08a960efe0b0a09047 \\\n    --hash=sha256:47ab1e7b91c098ab893b828deafa1203de86d0bc6ab587b160f78fe6c4011f75 \\\n    --hash=sha256:49e3ceeabbfb9d66c3aef5af3a60cc43b85c33df25ce03d0031a608b0a8b2e3f \\\n    --hash=sha256:4dc8f9fb58f7364b63fd9f85013b780ef83c11857ae79f2feda41e270468dd9b \\\n\n```\n\n----------------------------------------\n\nTITLE: Basic Model Logging with Prompts in MLflow\nDESCRIPTION: The basic syntax for logging a model with associated prompts using the 'prompts' parameter in MLflow's log_model method. This allows you to track which prompts are used with specific model versions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/run-and-model.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run():\n    mlflow.<flavor>.log_model(\n        model,\n        ...\n        # Specify a list of prompt URLs or prompt objects.\n        prompts=[\"prompts:/summarization-prompt/2\"]\n    )\n```\n\n----------------------------------------\n\nTITLE: Using Dataclass for Model Signature\nDESCRIPTION: Demonstrates how to use Python dataclasses to define model signature structure with proper typing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom typing import List\nfrom mlflow.models import infer_signature\n\n\n@dataclass\nclass Message:\n    role: str\n    content: str\n\n\n@dataclass\nclass ChatCompletionRequest:\n    messages: List[Message]\n\n\nchat_request = ChatCompletionRequest(\n    messages=[\n        Message(\n            role=\"user\",\n            content=\"What is the primary function of control rods in a nuclear reactor?\",\n        ),\n        Message(role=\"user\", content=\"What is MLflow?\"),\n    ]\n)\n\nmodel_signature = infer_signature(\n    chat_request,\n    \"Sample output as a string\",\n)\nprint(model_signature)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Retrieval Models with MLflow using Custom Metrics\nDESCRIPTION: This snippet shows how to evaluate a retrieval model by directly specifying custom metrics without setting a model type. It uses the extra_metrics parameter to define precision at different k values for evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Case 2: Specifying the extra_metrics\nevaluate_results = mlflow.evaluate(\n    data=data,\n    targets=\"ground_truth_context\",\n    predictions=\"retrieved_context\",\n    extra_matrics=[\n      mlflow.metrics.precision_at_k(4),\n      mlflow.metrics.precision_at_k(5)\n    ],\n  )\n```\n\n----------------------------------------\n\nTITLE: Inferring Signatures with Composite Data Types\nDESCRIPTION: Example showing signature inference for complex data types including lists, numpy arrays, and nested dictionaries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import infer_signature\n\ninfer_signature(\n    model_input={\n        # Python list\n        \"list_col\": [\"a\", \"b\", \"c\"],\n        # Numpy array\n        \"numpy_col\": np.array([[1, 2], [3, 4]]),\n        # Dictionary\n        \"obj_col\": {\"long_prop\": 1, \"array_prop\": [\"a\", \"b\", \"c\"]},\n    }\n)\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsignature:\n    input: '[\n        {\"list_col\": Array(string) (required)},\n        {\"numpy_col\": Array(Array(long)) (required)},\n        {\"obj_col\": {array_prop: Array(string) (required), long_prop: long (required)} (required)}\n    ]'\n    output: null\n    params: null\n```\n\n----------------------------------------\n\nTITLE: Logging Transformers Model with Advanced LLM Task in Python\nDESCRIPTION: This example shows how to log a Transformers pipeline with an advanced MLflow-specific task type (llm/v1/chat). It specifies the task explicitly and includes an input example for the chat interface.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/task/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        artifact_path=\"model\",\n        task=\"llm/v1/chat\",  # <= Specify the llm/v1 task type\n        # Optional, recommended for large models to avoid creating a local copy of the model weights\n        save_pretrained=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for LLM Model\nDESCRIPTION: Command to install the necessary dependencies for optimal model performance, including special packages for efficient model loading, initialization, attention computations, and inference processing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install xformers==0.0.20 einops==0.6.1 flash-attn==v1.0.3.post0 triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python\n```\n\n----------------------------------------\n\nTITLE: OpenAI API Input Schema Definition\nDESCRIPTION: Example of a complex input schema definition for OpenAI API compatibility, demonstrating why ChatModel is preferred for simplifying GenAI implementations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[\n    {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"content\": {\"type\": \"string\", \"required\": True},\n                \"name\": {\"type\": \"string\", \"required\": False},\n                \"role\": {\"type\": \"string\", \"required\": True},\n            },\n        },\n        \"name\": \"messages\",\n        \"required\": True,\n    },\n    {\"type\": \"double\", \"name\": \"temperature\", \"required\": False},\n    {\"type\": \"long\", \"name\": \"max_tokens\", \"required\": False},\n    {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"name\": \"stop\", \"required\": False},\n    {\"type\": \"long\", \"name\": \"n\", \"required\": False},\n    {\"type\": \"boolean\", \"name\": \"stream\", \"required\": False},\n    {\"type\": \"double\", \"name\": \"top_p\", \"required\": False},\n    {\"type\": \"long\", \"name\": \"top_k\", \"required\": False},\n    {\"type\": \"double\", \"name\": \"frequency_penalty\", \"required\": False},\n    {\"type\": \"double\", \"name\": \"presence_penalty\", \"required\": False},\n]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for LlamaIndex and MLflow\nDESCRIPTION: Installs the necessary Python libraries (mlflow and llama-index) using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_quickstart.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install mlflow>=2.15 llama-index>=0.10.44 -q\n```\n\n----------------------------------------\n\nTITLE: Customizing MLflow Auto Logging Behavior\nDESCRIPTION: Example showing how to customize MLflow autologging by disabling specific features (model signatures) and adding custom tags to runs. This allows for fine-grained control over what gets logged.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/autolog/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.autolog(\n    log_model_signatures=False,\n    extra_tags={\"YOUR_TAG\": \"VALUE\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Running the RAG Pipeline with MLflow Tracing\nDESCRIPTION: Enables automatic tracing for LangChain and invokes the RAG chain to answer a question. This generates a complex trace with information about the retrieval process and LLM usage.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# call the langchain autolog function so that traces will be generated\nmlflow.langchain.autolog()\n\nresponse = chain.invoke(\"What is MLflow Tracing?\")\n```\n\n----------------------------------------\n\nTITLE: Training a Random Forest Regressor with MLflow Tracking\nDESCRIPTION: Demonstrates the complete workflow of training a RandomForestRegressor model while tracking parameters, metrics, and artifacts with MLflow. This includes data preparation, model training, evaluation, and logging all relevant information to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step6-logging-a-run/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Split the data into features and target and drop irrelevant date field and target field\nX = data.drop(columns=[\"date\", \"demand\"])\ny = data[\"demand\"]\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nparams = {\n    \"n_estimators\": 100,\n    \"max_depth\": 6,\n    \"min_samples_split\": 10,\n    \"min_samples_leaf\": 4,\n    \"bootstrap\": True,\n    \"oob_score\": False,\n    \"random_state\": 888,\n}\n\n# Train the RandomForestRegressor\nrf = RandomForestRegressor(**params)\n\n# Fit the model on the training data\nrf.fit(X_train, y_train)\n\n# Predict on the validation set\ny_pred = rf.predict(X_val)\n\n# Calculate error metrics\nmae = mean_absolute_error(y_val, y_pred)\nmse = mean_squared_error(y_val, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_val, y_pred)\n\n# Assemble the metrics we're going to write into a collection\nmetrics = {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n\n# Initiate the MLflow run context\nwith mlflow.start_run(run_name=run_name) as run:\n    # Log the parameters used for the model fit\n    mlflow.log_params(params)\n\n    # Log the error metrics that were calculated during validation\n    mlflow.log_metrics(metrics)\n\n    # Log an instance of the trained model for later use\n    mlflow.sklearn.log_model(\n        sk_model=rf, input_example=X_val, artifact_path=artifact_path\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging LlamaIndex Model with MLflow in Python\nDESCRIPTION: This snippet shows how to log a LlamaIndex model using MLflow. It starts a run, logs the model, and includes an input example to help MLflow record dependency and signature information accurately.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run(run_name=\"react-agent-workflow\"):\n    model_info = mlflow.llama_index.log_model(\n        \"react_agent.py\",\n        artifact_path=\"model\",\n        # Logging with an input example help MLflow to record dependency and signature information accurately.\n        input_example={\"input\": \"What is (123 + 456) * 789?\"},\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating a New Model Version\nDESCRIPTION: Shows how to create a new version for an existing registered model using the MlflowClient API. This adds a specific model artifact as a new version.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclient = MlflowClient()\nresult = client.create_model_version(\n    name=\"sk-learn-random-forest-reg-model\",\n    source=\"mlruns/0/d16076a3ec534311817565e6527539c0/artifacts/sklearn-model\",\n    run_id=\"d16076a3ec534311817565e6527539c0\",\n)\n```\n\n----------------------------------------\n\nTITLE: Custom MLflow Model Querying Gateway Server\nDESCRIPTION: Example of a custom MLflow PyFunc model that uses the gateway server for LLM completions. The model connects to a gateway endpoint, sends prompts, and formats the results. Includes model registration and example prediction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pandas as pd\nimport mlflow\n\n\ndef predict(data):\n    from mlflow.deployments import get_deploy_client\n\n    client = get_deploy_client(os.environ[\"MLFLOW_DEPLOYMENTS_TARGET\"])\n\n    payload = data.to_dict(orient=\"records\")\n    return [\n        client.predict(endpoint=\"completions\", inputs=query)[\"choices\"][0][\"text\"]\n        for query in payload\n    ]\n\n\ninput_example = pd.DataFrame.from_dict(\n    {\"prompt\": [\"Where is the moon?\", \"What is a comet made of?\"]}\n)\nsignature = mlflow.models.infer_signature(\n    input_example, [\"Above our heads.\", \"It's mostly ice and rocks.\"]\n)\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=predict,\n        registered_model_name=\"anthropic_completions\",\n        artifact_path=\"anthropic_completions\",\n        input_example=input_example,\n        signature=signature,\n    )\n\ndf = pd.DataFrame.from_dict(\n    {\n        \"prompt\": [\"Tell me about Jupiter\", \"Tell me about Saturn\"],\n        \"temperature\": 0.6,\n        \"max_records\": 500,\n    }\n)\n\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\nprint(loaded_model.predict(df))\n```\n\n----------------------------------------\n\nTITLE: Setting up RAG Components with LangChain and OpenAI\nDESCRIPTION: Initializes the basic components needed for a Retrieval Augmented Generation (RAG) system using LangChain and OpenAI. Creates a language model, embeddings generator, and vector store.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# define necessary RAG entities\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\nvector_store = InMemoryVectorStore(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sktime Model Wrapper for MLflow in Python\nDESCRIPTION: This class wraps Sktime models for use with MLflow. It handles various prediction methods and input configurations, supporting single-row DataFrame inputs for prediction parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nclass _SktimeModelWrapper:\n    def __init__(self, sktime_model):\n        self.sktime_model = sktime_model\n\n    def predict(self, dataframe, params=None) -> pd.DataFrame:\n        df_schema = dataframe.columns.values.tolist()\n\n        if len(dataframe) > 1:\n            raise MlflowException(\n                f\"The provided prediction pd.DataFrame contains {len(dataframe)} rows. \"\n                \"Only 1 row should be supplied.\",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n        # Convert the configuration dataframe into a dictionary to simplify the\n        # extraction of parameters passed to the sktime predcition methods.\n        attrs = dataframe.to_dict(orient=\"index\").get(0)\n        predict_method = attrs.get(\"predict_method\")\n\n        if not predict_method:\n            raise MlflowException(\n                f\"The provided prediction configuration pd.DataFrame columns ({df_schema}) do not \"\n                \"contain the required column `predict_method` for specifying the prediction method.\",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n        if predict_method not in SUPPORTED_SKTIME_PREDICT_METHODS:\n            raise MlflowException(\n                \"Invalid `predict_method` value.\"\n                f\"The supported prediction methods are {SUPPORTED_SKTIME_PREDICT_METHODS}\",\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n        # For inference parameters 'fh', 'X', 'coverage', 'alpha', and 'cov'\n        # the respective sktime default value is used if the value was not\n        # provided in the configuration dataframe.\n        fh = attrs.get(\"fh\", None)\n\n        # Any model that is trained with exogenous regressor elements will need\n        # to provide `X` entries as a numpy ndarray to the predict method.\n        X = attrs.get(\"X\", None)\n\n        # When the model is served via REST API the exogenous regressor must be\n        # provided as a list to the configuration DataFrame to be JSON serializable.\n        # Below we convert the list back to ndarray type as required by sktime\n        # predict methods.\n        if isinstance(X, list):\n            X = np.array(X)\n\n        # For illustration purposes only a subset of the available sktime prediction\n        # methods is exposed. Additional methods (e.g. predict_proba) could be added\n        # in a similar fashion.\n        if predict_method == SKTIME_PREDICT:\n            predictions = self.sktime_model.predict(fh=fh, X=X)\n\n        if predict_method == SKTIME_PREDICT_INTERVAL:\n            coverage = attrs.get(\"coverage\", 0.9)\n            predictions = self.sktime_model.predict_interval(\n                fh=fh, X=X, coverage=coverage\n            )\n\n        if predict_method == SKTIME_PREDICT_QUANTILES:\n            alpha = attrs.get(\"alpha\", None)\n            predictions = self.sktime_model.predict_quantiles(fh=fh, X=X, alpha=alpha)\n\n        if predict_method == SKTIME_PREDICT_VAR:\n            cov = attrs.get(\"cov\", False)\n            predictions = self.sktime_model.predict_var(fh=fh, X=X, cov=cov)\n\n        # Methods predict_interval() and predict_quantiles() return a pandas\n        # MultiIndex column structure. As MLflow signature inference does not\n        # support MultiIndex column structure the columns must be flattened.\n        if predict_method in [SKTIME_PREDICT_INTERVAL, SKTIME_PREDICT_QUANTILES]:\n            predictions.columns = flatten_multiindex(predictions)\n\n        return predictions\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom LLM Provider Plugin in Python\nDESCRIPTION: Demonstrates how to create a custom language model provider by implementing the necessary classes and methods. This includes defining a config class and a provider class with methods for completions, chat, and embeddings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import AsyncIterable\n\nfrom mlflow.utils.pydantic_utils import field_validator\nfrom mlflow.gateway.base_models import ConfigModel\nfrom mlflow.gateway.config import RouteConfig\nfrom mlflow.gateway.providers import BaseProvider\nfrom mlflow.gateway.schemas import chat, completions, embeddings\n\n\nclass MyLLMConfig(ConfigModel):\n    # This model defines the configuration for the provider such as API keys\n    my_llm_api_key: str\n\n    @field_validator(\"my_llm_api_key\", mode=\"before\")\n    def validate_my_llm_api_key(cls, value):\n        return os.environ[value.lstrip(\"$\")]\n\n\nclass MyLLMProvider(BaseProvider):\n    # Define the provider name. This will be displayed in log and error messages.\n    NAME = \"my_llm\"\n    # Define the config model for the provider.\n    # This must be a subclass of ConfigModel.\n    CONFIG_TYPE = MyLLMConfig\n\n    def __init__(self, config: RouteConfig) -> None:\n        super().__init__(config)\n        if config.model.config is None or not isinstance(\n            config.model.config, MyLLMConfig\n        ):\n            raise TypeError(f\"Unexpected config type {config.model.config}\")\n        self.my_llm_config: MyLLMConfig = config.model.config\n\n    # You can implement one or more of the following methods\n    # depending on the capabilities of your provider.\n    # Implementing `completions`, `chat` and `embeddings` will enable the respective endpoints.\n    # Implementing `completions_stream` and `chat_stream` will enable the `stream=True`\n    # option for the respective endpoints.\n    # Unimplemented methods will return a 501 Not Implemented HTTP response upon invocation.\n    async def completions_stream(\n        self, payload: completions.RequestPayload\n    ) -> AsyncIterable[completions.StreamResponsePayload]:\n        ...\n\n    async def completions(\n        self, payload: completions.RequestPayload\n    ) -> completions.ResponsePayload:\n        ...\n\n    async def chat_stream(\n        self, payload: chat.RequestPayload\n    ) -> AsyncIterable[chat.StreamResponsePayload]:\n        ...\n\n    async def chat(self, payload: chat.RequestPayload) -> chat.ResponsePayload:\n        ...\n\n    async def embeddings(\n        self, payload: embeddings.RequestPayload\n    ) -> embeddings.ResponsePayload:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Keras Backend in Python\nDESCRIPTION: Sets the Keras backend environment variable to 'tensorflow' before importing the necessary packages for the tutorial.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/keras/quickstart/quickstart_keras.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# You can use 'tensorflow', 'torch' or 'jax' as backend. Make sure to set the environment variable before importing.\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n```\n\nLANGUAGE: python\nCODE:\n```\nimport keras\nimport numpy as np\n\nimport mlflow\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Champion Callback for Optuna\nDESCRIPTION: This snippet defines a custom callback function for Optuna that overrides default logging behavior to report only when a new trial improves upon the best metric value recorded so far, enhancing readability and providing progress indicators during hyperparameter tuning.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# override Optuna's default logging to ERROR only\noptuna.logging.set_verbosity(optuna.logging.ERROR)\n\n# define a logging callback that will report on only new challenger parameter configurations if a\n# trial has usurped the state of 'best conditions'\n\n\ndef champion_callback(study, frozen_trial):\n    \"\"\"\n    Logging callback that will report when a new trial iteration improves upon existing\n    best trial values.\n\n    Note: This callback is not intended for use in distributed computing systems such as Spark\n    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n    workers or agents.\n    The race conditions with file system state management for distributed trials will render\n    inconsistent values with this callback.\n    \"\"\"\n\n    winner = study.user_attrs.get(\"winner\", None)\n\n    if study.best_value and winner != study.best_value:\n        study.set_user_attr(\"winner\", study.best_value)\n        if winner:\n            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n            print(\n                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n                f\"{improvement_percent: .4f}% improvement\"\n            )\n        else:\n            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n\n```\n\n----------------------------------------\n\nTITLE: Starting a Trace with MLflow Client\nDESCRIPTION: Demonstrates how to initialize a new trace using the MlflowClient. Creates a root span and obtains the request_id for creating associated child spans.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/client.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\n\n# Start a new trace\nroot_span = client.start_trace(\"my_trace\")\n\n# The request_id is used for creating additional spans that have a hierarchical association to this root span\nrequest_id = root_span.request_id\n```\n\n----------------------------------------\n\nTITLE: Topic Analysis and Visualization for Successful Retrievals\nDESCRIPTION: This snippet performs topic analysis on successfully retrieved questions and prepares the results for interactive visualization with pyLDAvis. This helps identify common topics or patterns in questions that the retrieval model handles well.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlda_model, corpus, dictionary = topical_analysis(hit_questions)\nvis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n\n# Uncomment the following line to render the interactive widget\n# pyLDAvis.display(vis_data)\n```\n\n----------------------------------------\n\nTITLE: Using Embedding Models with MLflow Deployment Client\nDESCRIPTION: This snippet demonstrates how to generate embeddings for multiple text inputs using the MLflow AI Gateway. It initializes a client, prepares an input list of text descriptions for Harry Potter houses, and sends the request to the embeddings endpoint. The response contains numerical vector representations of each text input.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/guides/step2-query-deployments/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client(\"http://localhost:5000\")\nname = \"embeddings\"\ndata = dict(\n    input=[\n        \"Gryffindor: Values bravery, courage, and leadership.\",\n        \"Hufflepuff: Known for loyalty, a strong work ethic, and a grounded nature.\",\n        \"Ravenclaw: A house for individuals who value wisdom, intellect, and curiosity.\",\n        \"Slytherin: Appreciates ambition, cunning, and resourcefulness.\",\n    ],\n)\n\nresponse = client.predict(endpoint=name, inputs=data)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Updating an MLflow Model Version Description\nDESCRIPTION: Shows how to update a model version's description using the MLflow Client API. This allows adding detailed information about the model's characteristics and intended use.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclient = MlflowClient()\nclient.update_model_version(\n    name=\"sk-learn-random-forest-reg-model\",\n    version=1,\n    description=\"This model version is a scikit-learn random forest containing 100 decision trees\",\n)\n```\n\n----------------------------------------\n\nTITLE: Function Wrapping for MLflow Tracing\nDESCRIPTION: Illustrates how to use function wrapping to add tracing to existing functions without modifying their definitions, particularly useful for tracing third-party or external functions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport mlflow\n\ndef invocation(x, y, exp=2):\n    # Wrap an external function from the math library\n    traced_pow = mlflow.trace(math.pow)\n    raised = traced_pow(x, exp)\n\n    traced_factorial = mlflow.trace(math.factorial)\n    factorial = traced_factorial(int(raised))\n    return response\n\ninvocation(4)\n```\n\n----------------------------------------\n\nTITLE: Building FAISS Vector Database for MLflow Documentation\nDESCRIPTION: Loads scraped MLflow documentation, converts it to documents, splits them into chunks, and creates a FAISS vector database for efficient retrieval.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nscrapped_df = pd.read_csv(SCRAPPED_DOCS_PATH)\nlist_of_documents = [\n    Document(page_content=row[\"text\"], metadata={\"source\": row[\"source\"]})\n    for i, row in scrapped_df.iterrows()\n]\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\ndocs = text_splitter.split_documents(list_of_documents)\ndb = FAISS.from_documents(docs, embeddings)\n\n# Save the db to local disk\ndb.save_local(DB_PERSIST_DIR)\n```\n\n----------------------------------------\n\nTITLE: Loading MLflow Model for Inference\nDESCRIPTION: Demonstrates how to load the trained model from MLflow for inference. This snippet shows the model loading process using the run ID from the previous training session.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlogged_model = f\"runs:/{run.info.run_id}/model\"\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI to View Experiment Results\nDESCRIPTION: This command starts the MLflow UI server, allowing you to view experiment metrics, parameters, and other details through a web interface at localhost:5000.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/IrisClassification/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Automatic Tracing with OpenAI Integration\nDESCRIPTION: Demonstrates automatic tracing with OpenAI by enabling autolog and creating a chat completion. MLflow automatically captures the API call and displays the trace.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nimport mlflow\n\nmlflow.openai.autolog()\n\nclient = OpenAI()\n\n# creating a chat completion will generate a trace\nclient.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"hello!\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Log Probability Method in MLflow PyFunc Model\nDESCRIPTION: Shows how to use the loaded MLflow PyFunc model to make predictions using the predict_log_proba method, demonstrating the flexibility of the dynamic prediction method selection.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nloaded_dynamic.predict(x_test, params={\"predict_method\": \"predict_log_proba\"})\n```\n\n----------------------------------------\n\nTITLE: Loading a LlamaIndex Model for Inference with MLflow PyFunc\nDESCRIPTION: Loads a previously saved LlamaIndex model and performs inference. This code demonstrates how to use the loaded model for question answering, showing that the chat engine maintains conversation context.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\n\nresponse = model.predict(\"What was the first program the author wrote?\")\nprint(response)\n# >> The first program the author wrote was on the IBM 1401 ...\n\n# The chat engine keeps track of the conversation history\nresponse = model.predict(\"How did the author feel about it?\")\nprint(response)\n# >> The author felt puzzled by the first program ...\n```\n\n----------------------------------------\n\nTITLE: Logging Diviner Metrics and Parameters as Artifacts in Python\nDESCRIPTION: This snippet demonstrates how to log Diviner model metrics and parameters as CSV artifacts using mlflow.log_artifacts(). It extracts model parameters and cross-validation metrics, saves them as CSV files in a temporary directory, then logs the directory as artifacts.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport mlflow\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    params = model.extract_model_params()\n    metrics = model.cross_validate_and_score(\n        horizon=\"72 hours\",\n        period=\"240 hours\",\n        initial=\"480 hours\",\n        parallel=\"threads\",\n        rolling_window=0.1,\n        monthly=False,\n    )\n    params.to_csv(f\"{tmpdir}/params.csv\", index=False, header=True)\n    metrics.to_csv(f\"{tmpdir}/metrics.csv\", index=False, header=True)\n\n    mlflow.log_artifacts(tmpdir, artifact_path=\"data\")\n```\n\n----------------------------------------\n\nTITLE: Function Calling with OpenAI Agents SDK and MLflow Tracing in Python\nDESCRIPTION: Implementation of function calling with OpenAI Agents SDK and MLflow tracing. Shows how to define functions that can be called by the agent, with MLflow capturing function calls, inputs, and outputs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/openai-agent.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom agents import Agent, Runner, function_tool\n\n# Enable auto tracing for OpenAI Agents SDK\nmlflow.openai.autolog()\n\n\n@function_tool\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is sunny.\"\n\n\nagent = Agent(\n    name=\"Hello world\",\n    instructions=\"You are a helpful agent.\",\n    tools=[get_weather],\n)\n\n\nasync def main():\n    result = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n    print(result.final_output)\n    # The weather in Tokyo is sunny.\n\n\n# If you are running this code in a Jupyter notebook, replace this with `await main()`.\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing Model and Label Mappings\nDESCRIPTION: Sets up label mappings and initializes the DistilBERT model for sequence classification with appropriate label configurations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Set the mapping between int label and its meaning.\nid2label = {0: \"ham\", 1: \"spam\"}\nlabel2id = {\"ham\": 0, \"spam\": 1}\n\n# Acquire the model from the Hugging Face Hub, providing label and id mappings so that both we and the model can 'speak' the same language.\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2,\n    label2id=label2id,\n    id2label=id2label,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic OpenAI Autologging Setup in Python\nDESCRIPTION: A simple example demonstrating how to enable OpenAI autologging in MLflow at the beginning of a script, which records trace information by default when interacting with the OpenAI API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/autologging/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport openai\nimport mlflow\n\n# Enables trace logging by default\nmlflow.openai.autolog()\n\nopenai_client = openai.OpenAI()\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What does turning something up to 11 refer to?\",\n    }\n]\n\n# The input messages and the response will be logged as a trace to the active experiment\nanswer = openai_client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    temperature=0.99,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Predicting with MLflow PyFunc Model\nDESCRIPTION: Demonstrates how to load the logged model as a PyFunc and make predictions using both mlflow.pyfunc and mlflow.models interfaces.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\npyfunc_model.predict(input_example)\n```\n\nLANGUAGE: python\nCODE:\n```\nmlflow.models.predict(\n    model_uri=model_info.model_uri,\n    input_data=[{\"messages\": [{\"role\": \"user\", \"content\": \"What's Spanish for hello?\"}]}],\n    env_manager=\"uv\",\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting MLflow Python Function Model as Apache Spark UDF\nDESCRIPTION: Demonstrates how to export an MLflow Python function model as an Apache Spark UDF, which can be used to score the model on a Spark cluster.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_75\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, \"<path-to-model>\")\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf(struct([...])))\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-threaded MLflow Runs with Child Runs in Python\nDESCRIPTION: Demonstrates how to use multi-threading with MLflow by creating child runs. This approach avoids data corruption issues associated with multiple active runs in the same process.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport threading\n\n\ndef train_model(param):\n    # Create a child run by passing nested=True\n    with mlflow.start_run(nested=True):\n        mlflow.log_param(\"p\", param)\n        ...\n\n\nif __name__ == \"__main__\":\n    params = [0.01, 0.02, ...]\n    threads = []\n    for p in params:\n        t = threading.Thread(target=train_model, args=(p,))\n        threads.append(t)\n        t.start()\n\n    for t in threads:\n        t.join()\n```\n\n----------------------------------------\n\nTITLE: Handling Module Path Limitations in MLflow\nDESCRIPTION: Demonstrates incorrect and correct approaches to handling module imports with code_paths in MLflow, showing why direct parent/child directory references fail.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        from src.utils import my_func\n\n        # .. your prediction logic\n        return prediction\n\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        artifact_path=\"model\",\n        input_example=input_data,\n        code_paths=[\n            \"src/utils.py\"\n        ],  # the file will be saved at code/utils.py not code/src/utils.py\n    )\n\n# => Model serving will fail with ModuleNotFoundError: No module named 'src'\n```\n\n----------------------------------------\n\nTITLE: Sphinx Automodule Configuration for MLflow Groq Module\nDESCRIPTION: Sphinx configuration directive that automatically generates documentation for the mlflow.groq module. The directive includes all members, undocumented members, and shows inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.groq.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.groq\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Creating Inference Pipeline with Fine-Tuned Model for Text Classification\nDESCRIPTION: Sets up an inference pipeline using the fine-tuned model for text classification tasks. Configures the pipeline with the trained model, tokenizer, batch size, and device type (optimized for Apple Silicon with 'mps' device setting).\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# If you're going to run this on something other than a Macbook Pro, change the device to the applicable type. \"mps\" is for Apple Silicon architecture in torch.\n\ntuned_pipeline = pipeline(\n    task=\"text-classification\",\n    model=trainer.model,\n    batch_size=8,\n    tokenizer=tokenizer,\n    device=\"mps\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring a URI-based Input Model Signature for Audio Models in MLflow\nDESCRIPTION: This snippet demonstrates how to specify a custom model signature for audio models that use URI strings as input when saving to MLflow. This is necessary to prevent conversion issues when deploying for inference through MLflow Model Server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import infer_signature\nfrom mlflow.transformers import generate_signature_output\n\nurl = \"https://www.mywebsite.com/sound/files/for/transcription/file111.mp3\"\nsignature = infer_signature(url, generate_signature_output(my_audio_pipeline, url))\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=my_audio_pipeline,\n        artifact_path=\"my_transcriber\",\n        signature=signature,\n    )\n```\n\n----------------------------------------\n\nTITLE: MLflow Experiment Creation for Semantic Similarity\nDESCRIPTION: Creates a new MLflow experiment for tracking semantic similarity model runs, allowing for organized model management and versioning.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-similarity/semantic-similarity-sentence-transformers.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(\"Semantic Similarity\")\n```\n\n----------------------------------------\n\nTITLE: Customizing MLflow LangChain Tracing in Python\nDESCRIPTION: This code demonstrates how to create a custom callback handler by inheriting from MlflowLangchainTracer to customize the tracing behavior for LangChain.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/langchain.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.langchain.langchain_tracer import MlflowLangchainTracer\n\n\nclass CustomLangchainTracer(MlflowLangchainTracer):\n    # Override the handler functions to customize the behavior. The method signature is defined by LangChain Callbacks.\n    def on_chat_model_start(\n        self,\n        serialized: Dict[str, Any],\n        messages: List[List[BaseMessage]],\n        *,\n        run_id: UUID,\n        tags: Optional[List[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Run when a chat model starts running.\"\"\"\n        attributes = {\n            **kwargs,\n            **metadata,\n            # Add additional attribute to the span\n            \"version\": \"1.0.0\",\n        }\n\n        # Call the _start_span method at the end of the handler function to start a new span.\n        self._start_span(\n            span_name=name or self._assign_span_name(serialized, \"chat model\"),\n            parent_run_id=parent_run_id,\n            span_type=SpanType.CHAT_MODEL,\n            run_id=run_id,\n            inputs=messages,\n            attributes=kwargs,\n        )\n```\n\n----------------------------------------\n\nTITLE: Generating Scatter Plot of Demand vs Price in Python\nDESCRIPTION: This function creates a scatter plot to explore the relationship between demand and price per kg. It includes separate regression lines for weekends and weekdays, and uses color-coding and transparency for better visualization. The function returns a figure object for MLflow logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef plot_scatter_demand_price(df, style=\"seaborn\", plot_size=(10, 8)):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n        # Scatter plot with jitter, transparency, and color-coded based on weekend\n        sns.scatterplot(\n            data=df,\n            x=\"price_per_kg\",\n            y=\"demand\",\n            hue=\"weekend\",\n            palette={0: \"blue\", 1: \"green\"},\n            alpha=0.15,\n            ax=ax,\n        )\n        # Fit a simple regression line for each subgroup\n        sns.regplot(\n            data=df[df[\"weekend\"] == 0],\n            x=\"price_per_kg\",\n            y=\"demand\",\n            scatter=False,\n            color=\"blue\",\n            ax=ax,\n        )\n        sns.regplot(\n            data=df[df[\"weekend\"] == 1],\n            x=\"price_per_kg\",\n            y=\"demand\",\n            scatter=False,\n            color=\"green\",\n            ax=ax,\n        )\n\n        ax.set_title(\"Scatter Plot of Demand vs Price per kg with Regression Line\", fontsize=14)\n        ax.set_xlabel(\"Price per kg\", fontsize=12)\n        ax.set_ylabel(\"Demand\", fontsize=12)\n        for i in ax.get_xticklabels() + ax.get_yticklabels():\n            i.set_fontsize(10)\n        plt.tight_layout()\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Testing Translation Pipeline\nDESCRIPTION: Loads the saved model as a translation pipeline and tests it with a sample sentence to validate performance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntranslation_pipeline = mlflow.transformers.load_model(model_info.model_uri)\nresponse = translation_pipeline(\"I have heard that Nice is nice this time of year.\")\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Logging test metrics to local MLflow server\nDESCRIPTION: Python code that creates a new experiment, starts a run, and logs two metrics. This demonstrates basic MLflow tracking functionality with a local server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/tracking-server-overview/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"check-localhost-connection\")\n\nwith mlflow.start_run():\n    mlflow.log_metric(\"foo\", 1)\n    mlflow.log_metric(\"bar\", 2)\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset with TensorFlow Datasets\nDESCRIPTION: Loads the MNIST dataset using TensorFlow datasets, splitting it into train and test sets.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_ds, test_ds = tfds.load(\n    \"mnist\",\n    split=[\"train\", \"test\"],\n    shuffle_files=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Tracing Python Functions with MLflow Decorator\nDESCRIPTION: Demonstrates how to use the @mlflow.trace decorator to instrument Python functions for tracing. It includes examples of adding custom attributes and creating parent-child relationships between functions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n@mlflow.trace(span_type=\"func\", attributes={\"key\": \"value\"})\ndef add_1(x):\n    return x + 1\n\n@mlflow.trace(span_type=\"func\", attributes={\"key1\": \"value1\"})\ndef minus_1(x):\n    return x - 1\n\n@mlflow.trace(name=\"Trace Test\")\ndef trace_test(x):\n    step1 = add_1(x)\n    return minus_1(step1)\n\ntrace_test(4)\n```\n\n----------------------------------------\n\nTITLE: Configuring DSPy Language Model\nDESCRIPTION: Sets up the DSPy language model using OpenAI's gpt-4o-mini model and configures global settings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\nmodel_name = \"gpt-4o-mini\"\n\nlm = dspy.LM(\n    model=f\"openai/{model_name}\",\n    max_tokens=500,\n    temperature=0.1,\n)\ndspy.settings.configure(lm=lm)\n```\n\n----------------------------------------\n\nTITLE: Loading LlamaIndex Model\nDESCRIPTION: Demonstrates loading a logged LlamaIndex model and using it for queries\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nindex = mlflow.llama_index.load_model(model_info.model_uri)\nindex.as_query_engine().query(\"What is MLflow?\")\n```\n\n----------------------------------------\n\nTITLE: Sending Request to PyFunc Model with OpenAI-compatible Chat Requests\nDESCRIPTION: Python code to send a JSON request to a PyFunc model that accepts OpenAI-compatible chat requests.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Prerequisite: serve a Pyfunc model accepts OpenAI-compatible chat requests on localhost:5678 that defines\n#   `temperature` and `max_tokens` as parameters within the logged model signature\n\nimport json\nimport requests\n\npayload = json.dumps(\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Tell a joke!\"}],\n        \"temperature\": 0.5,\n        \"max_tokens\": 20,\n    }\n)\nrequests.post(\n    url=f\"http://localhost:5678/invocations\",\n    data=payload,\n    headers={\"Content-Type\": \"application/json\"},\n)\nprint(requests.json())\n```\n\n----------------------------------------\n\nTITLE: Defining MLflow Extra Dependencies\nDESCRIPTION: This snippet lists all the required dependencies for various MLflow Python modules. Each line specifies a package name with optional version constraints that users must install to use specific MLflow functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/requirements/extra-ml-requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n## This file describes extra ML library dependencies that you, as an end user,\n## must install in order to use various MLflow Python modules.\n# Required by mlflow.fastai\nfastai>=2.4.1\n# Required by mlflow.spacy\n# TODO: Remove `<3.8` once we bump the minimim supported python version of MLflow to 3.9.\nspacy>=3.3.0,<3.8\n# Required by mlflow.tensorflow\ntensorflow>=2.10.0\n# tensorflow-macos>=2.10.0  # Comment out the line above and uncomment this line if setting up dev\n                            # environment on local ARM macOS.\n                            # Only do this for the purpose of setting up the dev environment, do not\n                            # commit this change to the repo.\n# Required by mlflow.pytorch\ntorch>=1.11.0\ntorchvision>=0.12.0\nlightning>=1.8.1\n# Required by mlflow.xgboost\nxgboost>=0.82\n# Required by mlflow.lightgbm\nlightgbm\n# Required by mlflow.catboost\ncatboost\n# Required by mlflow.statsmodels\nstatsmodels\n# Required by mlflow.h2o\nh2o\n# Required by mlflow.onnx\nonnx>=1.11.0\nonnxruntime\ntf2onnx\n# Required by mlflow.spark and using Delta with MLflow Tracking datasets\npyspark\n# Required by mlflow.paddle\npaddlepaddle\n# Required by mlflow.prophet\n# NOTE: Prophet's whl build process will fail with dependencies not being present.\n#   Installation will default to setup.py in order to install correctly.\n#   To install in dev environment, ensure that gcc>=8 is installed to allow pystan\n#   to compile the model binaries. See: https://gcc.gnu.org/install/\n# Avoid 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200\nholidays!=0.25\nprophet\n# Required by mlflow.shap\n# and shap evaluation functionality\nshap>=0.42.1\n# Required by mlflow.pmdarima\npmdarima\n# Required by mlflow.diviner\ndiviner\n# Required for using Hugging Face datasets with MLflow Tracking\ndatasets\n# Required by mlflow.transformers\ntransformers\nsentencepiece\nsetfit\nlibrosa\nffmpeg\naccelerate\n# Required by mlflow.openai\nopenai\ntiktoken\ntenacity\n# Required by mlflow.llama_index\nllama_index\n# Required for an agent example of mlflow.llama_index\nllama-index-agent-openai\n# Required by mlflow.langchain\nlangchain\n# Required by mlflow.promptflow\npromptflow\n# Required by mlflow.sentence_transformers\nsentence-transformers\n# Required by mlflow.anthropic\nanthropic\n# Required by mlflow.autogen\nautogen\n# Required by mlflow.dspy\n# In dspy 2.6.9, `dspy.__name__` is not 'dspy', but 'dspy.__metadata__',\n# which causes auto-logging tests to fail.\ndspy!=2.6.9\n# Required by mlflow.litellm\nlitellm\n# Required by mlflow.gemini\ngoogle-genai\n# Required by mlflow.groq\ngroq\n# Required by mlflow.mistral\nmistralai\n```\n\n----------------------------------------\n\nTITLE: Loading and Predicting with MLflow Model\nDESCRIPTION: Example of loading a saved MLflow model and making predictions using the input example.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\nresult = loaded_model.predict(input_example)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Querying Served MLflow Model in Python\nDESCRIPTION: This function demonstrates how to query the served MLflow model using HTTP requests. It sends a POST request to the local endpoint with input data and parameters, then returns the model's predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef score_model(pdf, params):\n    headers = {\"Content-Type\": \"application/json\"}\n    url = f\"http://127.0.0.1:{PORT}/invocations\"\n    ds_dict = {\"dataframe_split\": pdf, \"params\": params}\n    data_json = json.dumps(ds_dict, allow_nan=True)\n\n    response = requests.request(method=\"POST\", headers=headers, url=url, data=data_json)\n    response.raise_for_status()\n\n    return response.json()\n\n\nprint(\"Inference on dow model 1 (Tuesday):\")\ninference_df = head_of_training_data.reset_index(drop=True).to_dict(orient=\"split\")\nprint(score_model(inference_df, params={\"dow\": 1}))\n```\n\n----------------------------------------\n\nTITLE: Logging and Loading MLflow Model with Inferred Signature in Python\nDESCRIPTION: Demonstrates the process of logging an MLflow model with an inferred signature, then loading and using the model for predictions. This showcases the full cycle of model management with signatures in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Generate a prediction that will serve as the model output example for signature inference\nmodel_output = MyModel().predict(context=None, model_input=data)\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        artifact_path=\"test_model\",\n        signature=infer_signature(model_input=data, model_output=model_output),\n    )\n\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\nprediction = loaded_model.predict(data)\n\nprediction\n```\n\n----------------------------------------\n\nTITLE: Inferring Native Transformers Task Type in Python\nDESCRIPTION: This snippet demonstrates how to automatically infer the task type from a Transformers pipeline when saving it with MLflow. It uses a text generation pipeline as an example.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/task/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport transformers\n\npipeline = transformers.pipeline(\"text-generation\", model=\"gpt2\")\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.save_model(\n        transformers_model=pipeline,\n        artifact_path=\"model\",\n        save_pretrained=False,\n    )\n\nprint(f\"Inferred task: {model_info.flavors['transformers']['task']}\")\n# >> Inferred task: text-generation\n```\n\n----------------------------------------\n\nTITLE: Setting a Production Alias for a Specific Prompt Version in Python\nDESCRIPTION: This code demonstrates how to set a production alias for a specific version of a prompt using MLflow's Python API. This enables environment-specific prompt management without hardcoding version numbers.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/cm.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Set a production alias for a specific version\nmlflow.set_prompt_alias(\"summarization-prompt\", alias=\"production\", version=2)\n```\n\n----------------------------------------\n\nTITLE: Saving OpenAI-Compatible Chat Model\nDESCRIPTION: Saving the model with llm/v1/chat task type to enable OpenAI-compatible chat functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/pyfunc-chat-model.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmlflow.transformers.save_model(\n    path=\"tinyllama-chat\", transformers_model=generator, task=\"llm/v1/chat\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow From Local Source in Development Environment\nDESCRIPTION: Command to install MLflow from the local source directory, which is used during development rather than installing from PyPI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npip install .\n```\n\n----------------------------------------\n\nTITLE: Logging LangChain Model with Explicit Prompt Reference\nDESCRIPTION: Example of logging a LangChain model to MLflow with an explicit reference to a registered prompt. This associates the prompt version with the model in MLflow's tracking system.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/run-and-model.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run(run_name=\"summarizer-model\"):\n    mlflow.langchain.log_model(\n        chain, artifact_path=\"model\", prompts=[\"prompts:/summarization-prompt/2\"]\n    )\n```\n\n----------------------------------------\n\nTITLE: Renaming an MLflow Registered Model\nDESCRIPTION: Demonstrates how to rename an existing registered model using the MLflow Client API. This changes the name of the model while preserving all versions and their metadata.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclient = MlflowClient()\nclient.rename_registered_model(\n    name=\"sk-learn-random-forest-reg-model\",\n    new_name=\"sk-learn-random-forest-reg-model-100\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading an MLflow Model and Making Predictions for a Recipe\nDESCRIPTION: Loads the previously logged LangChain model from MLflow and uses it to generate sous chef preparation instructions for a boeuf bourginon recipe serving 4 people.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-quickstart.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\ndish1 = loaded_model.predict({\"recipe\": \"boeuf bourginon\", \"customer_count\": \"4\"})\n\nprint(dish1[0])\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic MLflow Run Logging without Child Runs\nDESCRIPTION: Example implementation showing a naive approach to logging MLflow runs without parent-child relationships. The code demonstrates logging parameters and metrics for multiple runs using random values, with functions for run name generation and execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport mlflow\nfrom functools import partial\nfrom itertools import starmap\nfrom more_itertools import consume\n\n\n# Define a function to log parameters and metrics\ndef log_run(run_name, test_no):\n    with mlflow.start_run(run_name=run_name):\n        mlflow.log_param(\"param1\", random.choice([\"a\", \"b\", \"c\"]))\n        mlflow.log_param(\"param2\", random.choice([\"d\", \"e\", \"f\"]))\n        mlflow.log_metric(\"metric1\", random.uniform(0, 1))\n        mlflow.log_metric(\"metric2\", abs(random.gauss(5, 2.5)))\n\n\n# Generate run names\ndef generate_run_names(test_no, num_runs=5):\n    return (f\"run_{i}_test_{test_no}\" for i in range(num_runs))\n\n\n# Execute tuning function\ndef execute_tuning(test_no):\n    # Partial application of the log_run function\n    log_current_run = partial(log_run, test_no=test_no)\n    # Generate run names and apply log_current_run function to each run name\n    runs = starmap(\n        log_current_run, ((run_name,) for run_name in generate_run_names(test_no))\n    )\n    # Consume the iterator to execute the runs\n    consume(runs)\n\n\n# Set the tracking uri and experiment\nmlflow.set_tracking_uri(\"http://localhost:8080\")\nmlflow.set_experiment(\"No Child Runs\")\n```\n\n----------------------------------------\n\nTITLE: Inferring Optional Column Signatures\nDESCRIPTION: Example showing how columns containing None values are inferred as optional in the signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import infer_signature\n\ninfer_signature(model_input=pd.DataFrame({\"col\": [1.0, 2.0, None]}))\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsignature:\n    input: '[\n        {\"name\": \"col\", \"type\": \"double\", \"required\": false}\n    ]'\n    output: null\n    params: null\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model with No Input Variables\nDESCRIPTION: Shows how to log and use an OpenAI model with fixed system message and different input formats for predictions including DataFrame, list of dictionaries, and list of strings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/guide/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-4o-mini\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=[{\"role\": \"system\", \"content\": \"You are Elon Musk\"}],\n    )\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        \"question\": [\n            \"Let me hear your thoughts on AI\",\n            \"Let me hear your thoughts on ML\",\n        ],\n    }\n)\nprint(model.predict(df))\n```\n\n----------------------------------------\n\nTITLE: Creating Unity Catalog Function (SQL)\nDESCRIPTION: SQL command to create a sample Unity Catalog function that adds two integers. This function will be used in the example script.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/uc_integration/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE FUNCTION\nmy.uc_func.add (\n  x INTEGER COMMENT 'The first number to add.',\n  y INTEGER COMMENT 'The second number to add.'\n)\nRETURNS INTEGER\nLANGUAGE SQL\nRETURN x + y\n```\n\n----------------------------------------\n\nTITLE: Registering MLflow Model\nDESCRIPTION: Registers a trained MLflow model with the Model Registry for deployment\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/prompt-engineering/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmlflow.register_model(\n    model_uri=\"runs:/8451075c46964f82b85fe16c3d2b7ea0/model\",\n    name=\"mlflow_docs_qa_model\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Langchain Model with MLflow\nDESCRIPTION: Defines a langchain model using OpenAI that evaluates homework assignments. The model includes prompt templates, custom functions for question and answer extraction, and proper output parsing for MLflow signature inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom operator import itemgetter\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_openai import OpenAI\n\nimport mlflow\n\nmlflow.set_experiment(\"Homework Helper\")\n\nmlflow.langchain.autolog()\n\nprompt = PromptTemplate(\n    template=\"You are a helpful tutor that evaluates my homework assignments and provides suggestions on areas for me to study further.\"\n    \" Here is the question: {question} and my answer which I got wrong: {answer}\",\n    input_variables=[\"question\", \"answer\"],\n)\n\n\ndef get_question(input):\n    default = \"What is your name?\"\n    if isinstance(input_data[0], dict):\n        return input_data[0].get(\"content\").get(\"question\", default)\n    return default\n\n\ndef get_answer(input):\n    default = \"My name is Bobo\"\n    if isinstance(input_data[0], dict):\n        return input_data[0].get(\"content\").get(\"answer\", default)\n    return default\n\n\nmodel = OpenAI(temperature=0.95)\n\nchain = (\n    {\n        \"question\": itemgetter(\"messages\") | RunnableLambda(get_question),\n        \"answer\": itemgetter(\"messages\") | RunnableLambda(get_answer),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nmlflow.models.set_model(chain)\n```\n\n----------------------------------------\n\nTITLE: Generating Java API Documentation\nDESCRIPTION: Commands to generate Java API documentation files in RST format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nmake javadocs\n```\n\n----------------------------------------\n\nTITLE: Using Diviner pyfunc for Prediction in Python\nDESCRIPTION: This example demonstrates how to use the MLflow pyfunc interface for Diviner models. It shows model saving, loading, and prediction using a configuration DataFrame to control prediction behavior. The snippet includes setting up a GroupedPmdarima model and making predictions for specific groups.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport pandas as pd\nfrom pmdarima.arima.auto import AutoARIMA\nfrom diviner import GroupedPmdarima\n\nwith mlflow.start_run():\n    base_model = AutoARIMA(out_of_sample_size=96, maxiter=200)\n    model = GroupedPmdarima(model_template=base_model).fit(\n        df=df,\n        group_key_columns=[\"country\", \"city\"],\n        y_col=\"watts\",\n        datetime_col=\"datetime\",\n        silence_warnings=True,\n    )\n\n    mlflow.diviner.save_model(diviner_model=model, path=\"/tmp/diviner_model\")\n\ndiviner_pyfunc = mlflow.pyfunc.load_model(model_uri=\"/tmp/diviner_model\")\n\npredict_conf = pd.DataFrame(\n    {\n        \"n_periods\": 120,\n        \"groups\": [\n            (\"US\", \"NewYork\"),\n            (\"CA\", \"Toronto\"),\n            (\"MX\", \"MexicoCity\"),\n        ],  # NB: List of tuples required.\n        \"predict_col\": \"wattage_forecast\",\n        \"alpha\": 0.1,\n        \"return_conf_int\": True,\n        \"on_error\": \"warn\",\n    },\n    index=[0],\n)\n\nsubset_forecasts = diviner_pyfunc.predict(predict_conf)\n```\n\n----------------------------------------\n\nTITLE: Creating a New Registered Model\nDESCRIPTION: Demonstrates creating an empty registered model using the MlflowClient API. This creates the model container without any versions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\nclient.create_registered_model(\"sk-learn-random-forest-reg-model\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Questions Based on Retrieval Performance\nDESCRIPTION: This snippet filters the evaluation results to separate questions with perfect precision@1 (successful retrievals) from those with zero precision@1 (failed retrievals). This separation allows for targeted analysis of what types of questions the retrieval system handles well or poorly.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df = eval_results_table[eval_results_table[\"precision_at_1/score\"] == 1]\nhit_questions = filtered_df[\"question\"].tolist()\nfiltered_df = eval_results_table[eval_results_table[\"precision_at_1/score\"] == 0]\nmiss_questions = filtered_df[\"question\"].tolist()\n```\n\n----------------------------------------\n\nTITLE: Customizing Span Names in MLflow LangChain Traces\nDESCRIPTION: Demonstrates two methods for customizing span names in LangChain traces - using the name parameter in constructors and the with_config method for runnables.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Enable auto-tracing for LangChain\nmlflow.langchain.autolog()\n\n# Method 1: Pass `name` parameter to the constructor\nmodel = ChatOpenAI(name=\"custom-llm\", model=\"gpt-4o-mini\")\n# Method 2: Use `with_config` method to set the name for the runnables\nrunnable = (model | StrOutputParser()).with_config({\"run_name\": \"custom-chain\"})\n\nrunnable.invoke(\"Hi\")\n```\n\n----------------------------------------\n\nTITLE: Manual Metric Logging in Python with MLflow Tracking\nDESCRIPTION: This snippet demonstrates how to manually log metrics within an MLflow run using Python. It creates a run context and logs quality metrics for each training epoch.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    for epoch in range(0, 3):\n        mlflow.log_metric(key=\"quality\", value=2 * epoch, step=epoch)\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Retrieve Document IDs\nDESCRIPTION: Creates a helper function that takes a question string as input and returns a list of relevant document IDs using the retriever.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Define a function to return a list of retrieved doc ids\ndef retrieve_doc_ids(question: str) -> list[str]:\n    docs = retriever.get_relevant_documents(question)\n    return [doc.metadata[\"source\"] for doc in docs]\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation for MLflow AutoGen Module using Sphinx\nDESCRIPTION: This reStructuredText code snippet uses Sphinx's automodule directive to automatically generate comprehensive API documentation for the mlflow.autogen module. It includes all members, undocumented members, and shows inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.autogen.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: mlflow.autogen\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Assigning Model Aliases in MLflow\nDESCRIPTION: Python function to assign an alias to the latest version of a registered model within a specified stage. Uses the MLflow Client to retrieve the latest model version and set an alias mapping. Helps transition from stage-based to alias-based model management.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\n\n# Initialize an MLflow Client\nclient = MlflowClient()\n\n\ndef assign_alias_to_stage(model_name, stage, alias):\n    \"\"\"\n    Assign an alias to the latest version of a registered model within a specified stage.\n\n    :param model_name: The name of the registered model.\n    :param stage: The stage of the model version for which the alias is to be assigned. Can be\n                \"Production\", \"Staging\", \"Archived\", or \"None\".\n    :param alias: The alias to assign to the model version.\n    :return: None\n    \"\"\"\n    latest_mv = client.get_latest_versions(model_name, stages=[stage])[0]\n    client.set_registered_model_alias(model_name, alias, latest_mv.version)\n```\n\n----------------------------------------\n\nTITLE: Logging Model with Example Containing Params in Python\nDESCRIPTION: This snippet illustrates how to log a Transformers model with an input example that includes additional parameters for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# input_example could be column-based or tensor-based example as shown above\n# params must be a valid dictionary of params\ninput_data = \"Hello, Dolly!\"\nparams = {\"temperature\": 0.5, \"top_k\": 1}\ninput_example = (input_data, params)\nmlflow.transformers.log_model(..., input_example=input_example)\n```\n\n----------------------------------------\n\nTITLE: Sending Complex Data Types to MLflow Model Server\nDESCRIPTION: Curl commands to send JSON inputs with complex data types (binary and datetime) to the local MLflow model server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# record-oriented DataFrame input with binary column \"b\"\ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '[\n    {\"a\": 0, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAw\"},\n    {\"a\": 1, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAx\"},\n    {\"a\": 2, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAy\"}\n]'\n\n# record-oriented DataFrame input with datetime column \"b\"\ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '[\n    {\"a\": 0, \"b\": \"2020-01-01T00:00:00Z\"},\n    {\"a\": 1, \"b\": \"2020-02-01T12:34:56Z\"},\n    {\"a\": 2, \"b\": \"2021-03-01T00:00:00Z\"}\n]'\n```\n\n----------------------------------------\n\nTITLE: Testing Offline Prediction with MLflow Models CLI (Bash)\nDESCRIPTION: Demonstrates using the MLflow CLI to test offline predictions with a model in a virtual environment for validation before deployment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models predict -m runs:/<run_id>/model-i <input_path>\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks LLM in LlamaIndex Settings\nDESCRIPTION: Instantiates a Databricks LLM (Llama3.1 70B instruct) and sets it as the global LLM in LlamaIndex Settings for use throughout the application.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import Settings\nfrom llama_index.llms.databricks import Databricks\n\nllm = Databricks(model=\"databricks-meta-llama-3-1-70b-instruct\")\nSettings.llm = llm\n```\n\n----------------------------------------\n\nTITLE: Creating and Demonstrating LlamaIndex Vector Store Index\nDESCRIPTION: Creates a VectorStoreIndex from an example document and demonstrates query engine, chat engine, and retriever functionalities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_quickstart.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(\"------------- Example Document used to Enrich LLM Context -------------\")\nllama_index_example_document = Document.example()\nprint(llama_index_example_document)\n\nindex = VectorStoreIndex.from_documents([llama_index_example_document])\n\nprint(\"\\n------------- Example Query Engine -------------\")\nquery_response = index.as_query_engine().query(\"What is llama_index?\")\nprint(query_response)\n\nprint(\"\\n------------- Example Chat Engine  -------------\")\nchat_response = index.as_chat_engine().chat(\n    \"What is llama_index?\",\n    chat_history=[ChatMessage(role=\"system\", content=\"You are an expert on RAG!\")],\n)\nprint(chat_response)\n\n\nprint(\"\\n------------- Example Retriever   -------------\")\nretriever_response = index.as_retriever().retrieve(\"What is llama_index?\")\nprint(retriever_response)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Sktime Flavor with MLflow in Python\nDESCRIPTION: This example demonstrates how to use the custom Sktime flavor with MLflow. It includes model training, logging parameters and metrics, and generating predictions using both native Sktime and MLflow pyfunc flavors.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_70\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport flavor\nimport pandas as pd\nfrom sktime.datasets import load_longley\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.forecasting.naive import NaiveForecaster\nfrom sktime.performance_metrics.forecasting import (\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n)\n\nimport mlflow\n\nARTIFACT_PATH = \"model\"\n\nwith mlflow.start_run() as run:\n    y, X = load_longley()\n    y_train, y_test, X_train, X_test = temporal_train_test_split(y, X)\n\n    forecaster = NaiveForecaster()\n    forecaster.fit(\n        y_train,\n        X=X_train,\n    )\n\n    # Extract parameters\n    parameters = forecaster.get_params()\n\n    # Evaluate model\n    y_pred = forecaster.predict(fh=[1, 2, 3, 4], X=X_test)\n    metrics = {\n        \"mae\": mean_absolute_error(y_test, y_pred),\n        \"mape\": mean_absolute_percentage_error(y_test, y_pred),\n    }\n\n    print(f\"Parameters: \\n{json.dumps(parameters, indent=2)}\")\n    print(f\"Metrics: \\n{json.dumps(metrics, indent=2)}\")\n\n    # Log parameters and metrics\n    mlflow.log_params(parameters)\n    mlflow.log_metrics(metrics)\n\n    # Log model using custom model flavor with pickle serialization (default).\n    flavor.log_model(\n        sktime_model=forecaster,\n        artifact_path=ARTIFACT_PATH,\n        serialization_format=\"pickle\",\n    )\n    model_uri = mlflow.get_artifact_uri(ARTIFACT_PATH)\n\n# Load model in native sktime flavor and pyfunc flavor\nloaded_model = flavor.load_model(model_uri=model_uri)\nloaded_pyfunc = flavor.pyfunc.load_model(model_uri=model_uri)\n\n# Convert test data to 2D numpy array so it can be passed to pyfunc predict using\n# a single-row Pandas DataFrame configuration argument\nX_test_array = X_test.to_numpy()\n\n# Create configuration DataFrame\npredict_conf = pd.DataFrame(\n    [\n        {\n            \"fh\": [1, 2, 3, 4],\n            \"predict_method\": \"predict_interval\",\n            \"coverage\": [0.9, 0.95],\n            \"X\": X_test_array,\n        }\n    ]\n)\n\n# Generate interval forecasts with native sktime flavor and pyfunc flavor\nprint(\n    f\"\\nNative sktime 'predict_interval':\\n${loaded_model.predict_interval(fh=[1, 2, 3], X=X_test, coverage=[0.9, 0.95])}\"\n)\nprint(f\"\\nPyfunc 'predict_interval':\\n${loaded_pyfunc.predict(predict_conf)}\")\n\n# Print the run id which is used for serving the model to a local REST API endpoint\nprint(f\"\\nMLflow run id:\\n{run.info.run_id}\")\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow via pip\nDESCRIPTION: Commands to install the stable release or a release candidate of MLflow using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\nLANGUAGE: bash\nCODE:\n```\n# install the latest release candidate\npip install --pre mlflow\n\n# or install a specific rc version\npip install mlflow==2.14.0rc0\n```\n\n----------------------------------------\n\nTITLE: Accessing Autologged MLflow Run Results in Python\nDESCRIPTION: Demonstrates how to retrieve the MLflow Run instance associated with autologged results using the mlflow.last_active_run() function. This is useful for accessing run information after autologging has completed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\nmlflow.autolog()\n\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\n# Create and train models.\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\nrf.fit(X_train, y_train)\n\n# Use the model to make predictions on the test dataset.\npredictions = rf.predict(X_test)\nautolog_run = mlflow.last_active_run()\nprint(autolog_run)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for System Metrics\nDESCRIPTION: Methods to enable system metrics logging using environment variables in both shell and Python environments.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/system-metrics/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING=true\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx autodoc for MLflow SHAP Module\nDESCRIPTION: RST directive that configures automatic documentation generation for the mlflow.shap module. Includes all module members and inheritance details while excluding the save_model method.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.shap.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.shap\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :exclude-members: save_model\n```\n\n----------------------------------------\n\nTITLE: Logging Custom SimilarityModel with MLflow\nDESCRIPTION: Shows how to log a custom SimilarityModel using MLflow's Python function interface. The model is logged with input examples, signature, required artifacts, and dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-similarity/semantic-similarity-sentence-transformers.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npyfunc_path = \"/tmp/sbert_pyfunc\"\n\nwith mlflow.start_run() as run:\n    model_info = mlflow.pyfunc.log_model(\n        \"similarity\",\n        python_model=SimilarityModel(),\n        input_example=input_example,\n        signature=signature,\n        artifacts=artifacts,\n        pip_requirements=[\"sentence_transformers\", \"numpy\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI for Model Tracking Visualization\nDESCRIPTION: This command launches the MLflow user interface in a separate terminal, allowing users to view tracked experiments, runs, parameters, metrics, and models through a web browser.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/skinny/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Configuring UnityCatalog Function Client\nDESCRIPTION: Sets up the UnityCatalog function client for Databricks, which will be used to create and manage UnityCatalog functions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom unitycatalog.ai.core.base import set_uc_function_client\nfrom unitycatalog.ai.core.databricks import DatabricksFunctionClient\n\nclient = DatabricksFunctionClient()\n\n# sets the default uc function client\nset_uc_function_client(client)\n```\n\n----------------------------------------\n\nTITLE: Initializing Evaluation Dataset in Python\nDESCRIPTION: Creates a pandas DataFrame containing questions and their corresponding source documents for evaluating a retrieval system. This dataset serves as the basis for subsequent embedding and retrieval evaluations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\neval_data = pd.DataFrame(\n    {\n        \"question\": [\n            \"What is MLflow?\",\n            \"What is Databricks?\",\n            \"How to serve a model on Databricks?\",\n            \"How to enable MLflow Autologging for my workspace by default?\",\n        ],\n        \"source\": [\n            [\"https://mlflow.org/docs/latest/index.html\"],\n            [\"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\"],\n            [\"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\"],\n            [\"https://mlflow.org/docs/latest/tracking/autolog.html\"],\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing and Using a Streamable PyFunc Model in Python\nDESCRIPTION: This code snippet defines a custom StreamableModel class that supports streaming predictions, saves and loads the model using MLflow, and demonstrates how to use the predict_stream method. It also shows how to consume the generated stream output.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Define a custom model that supports streaming\nclass StreamableModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        # Regular predict method implementation (optional for this demo)\n        return \"regular-predict-output\"\n\n    def predict_stream(self, context, model_input, params=None):\n        # Yielding elements one at a time\n        for element in [\"a\", \"b\", \"c\", \"d\", \"e\"]:\n            yield element\n\n\n# Save the model to a directory\ntmp_path = \"/tmp/test_model\"\npyfunc_model_path = os.path.join(tmp_path, \"pyfunc_model\")\npython_model = StreamableModel()\nmlflow.pyfunc.save_model(path=pyfunc_model_path, python_model=python_model)\n\n# Load the model\nloaded_pyfunc_model = mlflow.pyfunc.load_model(model_uri=pyfunc_model_path)\n\n# Use predict_stream to get a generator\nstream_output = loaded_pyfunc_model.predict_stream(\"single-input\")\n\n# Consuming the generator using next\nprint(next(stream_output))  # Output: 'a'\nprint(next(stream_output))  # Output: 'b'\n\n# Alternatively, consuming the generator using a for-loop\nfor response in stream_output:\n    print(response)  # This will print 'c', 'd', 'e'\n```\n\n----------------------------------------\n\nTITLE: Testing Model with Valid and Invalid Inputs\nDESCRIPTION: Tests the loaded model with test data containing a mix of dictionaries and lists. This demonstrates how the model handles different input types before schema enforcement is applied.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntest_data = [{\"a\": \"we are expecting strings\", \"b\": \"and only strings\"}, [1, 2, 3]]\nloaded_model.predict(test_data)\n```\n\n----------------------------------------\n\nTITLE: Creating a PEFT Model with LoRA Configuration\nDESCRIPTION: This snippet shows how to create a Parameter-Efficient Fine-Tuning (PEFT) model using the LoRA configuration. It demonstrates setting up a base model from HuggingFace and applying LoRA optimization for efficient fine-tuning.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom peft import LoraConfig, get_peft_model\n\nbase_model = AutoModelForCausalLM.from_pretrained(...)\nlora_config = LoraConfig(...)\npeft_model = get_peft_model(base_model, lora_config)\n```\n\n----------------------------------------\n\nTITLE: Creating Residual Plots for Regression Model Analysis in Python\nDESCRIPTION: This function generates a residual plot for analyzing the difference between observed and predicted values. It includes a lowess smoothing line to identify trends in residuals and a reference line at zero, helping to assess model assumptions like linearity and homoscedasticity.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef plot_residuals(y_test, y_pred, style=\"seaborn\", plot_size=(10, 8)):\n    residuals = y_test - y_pred\n\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n        sns.residplot(\n            x=y_pred,\n            y=residuals,\n            lowess=True,\n            ax=ax,\n            line_kws={\"color\": \"red\", \"lw\": 1},\n        )\n\n        ax.axhline(y=0, color=\"black\", linestyle=\"--\")\n        ax.set_title(\"Residual Plot\", fontsize=14)\n        ax.set_xlabel(\"Predicted values\", fontsize=12)\n        ax.set_ylabel(\"Residuals\", fontsize=12)\n\n        for label in ax.get_xticklabels() + ax.get_yticklabels():\n            label.set_fontsize(10)\n\n        plt.tight_layout()\n\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Initializing DialoGPT Conversational Pipeline with MLflow Signature Inference\nDESCRIPTION: Creates a conversational pipeline using the DialoGPT-medium model from the transformers library and infers the model signature using MLflow. The signature defines the expected input and output formats for the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/conversational-model.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\n\nimport mlflow\n\n# Define our pipeline, using the default configuration specified in the model card for DialoGPT-medium\nconversational_pipeline = transformers.pipeline(model=\"microsoft/DialoGPT-medium\")\n\n# Infer the signature by providing a representnative input and the output from the pipeline inference abstraction in the transformers flavor in MLflow\nsignature = mlflow.models.infer_signature(\n    \"Hi there, chatbot!\",\n    mlflow.transformers.generate_signature_output(conversational_pipeline, \"Hi there, chatbot!\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Model with MLflow\nDESCRIPTION: Shows how to load a trained XGBoost model using MLflow's native model loader, which preserves model fidelity and enables access to framework-specific functionality\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nloaded = mlflow.xgboost.load_model(model_uri)\n```\n\n----------------------------------------\n\nTITLE: Running Storybook Development Server\nDESCRIPTION: Command to start the Storybook development server for the Du Bois design system. The server runs on localhost:6007.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/server/js/vendor/design-system/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd js/packages/du-bois && yarn storybook\n```\n\n----------------------------------------\n\nTITLE: Using Model with Inference Parameters\nDESCRIPTION: Shows how to save and use a transformers model with both model configuration and inference parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom mlflow.transformers import generate_signature_output\nimport transformers\n\narchitecture = \"mrm8488/t5-base-finetuned-common_gen\"\nmodel = transformers.pipeline(\n    task=\"text2text-generation\",\n    tokenizer=transformers.T5TokenizerFast.from_pretrained(architecture),\n    model=transformers.T5ForConditionalGeneration.from_pretrained(architecture),\n)\ndata = \"pencil draw paper\"\n\n# Define an model_config\nmodel_config = {\n    \"num_beams\": 5,\n    \"remove_invalid_values\": True,\n}\n\n# Define the inference parameters params\ninference_params = {\n    \"max_length\": 30,\n    \"do_sample\": True,\n}\n\n# Infer the signature including params\nsignature_with_params = infer_signature(\n    data,\n    generate_signature_output(model, data),\n    params=inference_params,\n)\n\n# Saving model with signature and model config\nmlflow.transformers.save_model(\n    model,\n    path=\"text2text\",\n    model_config=model_config,\n    signature=signature_with_params,\n)\n\npyfunc_loaded = mlflow.pyfunc.load_model(\"text2text\")\n\n# Pass params at inference time\nparams = {\n    \"max_length\": 20,\n    \"do_sample\": False,\n}\n\nresult = pyfunc_loaded.predict(data, params=params)\n```\n\n----------------------------------------\n\nTITLE: Python Version Support Update\nDESCRIPTION: MLflow now requires Python 3.9 as the minimum supported version, as Python 3.8 has reached end-of-life.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Minimum required Python version\npython_version >= \"3.9\"\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Dataset for MLflow RAG System\nDESCRIPTION: Creates a pandas DataFrame containing sample questions for evaluating the RAG system. The dataset includes questions about MLflow, Databricks, and related functionalities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\neval_df = pd.DataFrame(\n    {\n        \"questions\": [\n            \"What is MLflow?\",\n            \"What is Databricks?\",\n            \"How to serve a model on Databricks?\",\n            \"How to enable MLflow Autologging for my workspace by default?\",\n        ],\n    }\n)\ndisplay(eval_df)\n```\n\n----------------------------------------\n\nTITLE: Saving Model with Configuration and Signature\nDESCRIPTION: Example of saving a transformers model with model configuration and inferred signature for text generation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom mlflow.transformers import generate_signature_output\nimport transformers\n\narchitecture = \"mrm8488/t5-base-finetuned-common_gen\"\nmodel = transformers.pipeline(\n    task=\"text2text-generation\",\n    tokenizer=transformers.T5TokenizerFast.from_pretrained(architecture),\n    model=transformers.T5ForConditionalGeneration.from_pretrained(architecture),\n)\ndata = \"pencil draw paper\"\n\n# Infer the signature\nsignature = infer_signature(\n    data,\n    generate_signature_output(model, data),\n)\n\n# Define an model_config\nmodel_config = {\n    \"num_beams\": 5,\n    \"max_length\": 30,\n    \"do_sample\": True,\n    \"remove_invalid_values\": True,\n}\n\n# Saving model_config with the model\nmlflow.transformers.save_model(\n    model,\n    path=\"text2text\",\n    model_config=model_config,\n    signature=signature,\n)\n\npyfunc_loaded = mlflow.pyfunc.load_model(\"text2text\")\n# model_config will be applied\nresult = pyfunc_loaded.predict(data)\n\n# overriding some inference configuration with different values\npyfunc_loaded = mlflow.pyfunc.load_model(\n    \"text2text\", model_config=dict(do_sample=False)\n)\n```\n\n----------------------------------------\n\nTITLE: Multiple Output Model Evaluation with Column Mapping\nDESCRIPTION: Example of evaluating a model with multiple outputs using column mapping configuration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndef model(x):\n    return pd.DataFrame({\"retrieved_context\": x[\"inputs\"] + 1, \"answer\": x[\"inputs\"]})\n\neval_dataset = pd.DataFrame(\n    {\n        \"targets\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        \"inputs\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nconfig = {\"col_mapping\": {\"context\": \"retrieved_context\"}}\n\nresult = mlflow.evaluate(\n    model,\n    eval_dataset,\n    predictions=\"answer\",\n    targets=\"targets\",\n    extra_metrics=[mymetric],\n    evaluator_config=config,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Explicit Model Signatures with Parameters in MLflow\nDESCRIPTION: This example shows how to manually create a signature for a model that requires parameters during inference. The signature includes input schema, output schema, and a parameter schema that defines the parameter types, default values, and shapes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import ModelSignature\nfrom mlflow.types.schema import ColSpec, ParamSchema, ParamSpec, Schema\n\ninput_schema = Schema([ColSpec(type=\"string\")])\noutput_schema = Schema([ColSpec(type=\"string\")])\nparams_schema = ParamSchema(\n    [\n        ParamSpec(\"top_k\", \"long\", 2),\n        ParamSpec(\"num_beams\", \"long\", 5),\n        ParamSpec(\"max_length\", \"long\", 30),\n        ParamSpec(\"temperature\", \"double\", 0.62),\n        ParamSpec(\"top_p\", \"double\", 0.85),\n        ParamSpec(\"repetition_penalty\", \"double\", 1.15),\n        ParamSpec(\"begin_suppress_tokens\", \"long\", [1, 2, 3], (-1,)),\n    ]\n)\nsignature = ModelSignature(\n    inputs=input_schema, outputs=output_schema, params=params_schema\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Single Endpoint for MLflow AI Gateway in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure a single endpoint for the MLflow AI Gateway. It defines an endpoint named 'chat' using the OpenAI GPT-4 model, with rate limiting of 10 calls per minute.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n    limit:\n      renewal_period: minute\n      calls: 10\n```\n\n----------------------------------------\n\nTITLE: Defining Math Tools for Agent\nDESCRIPTION: Creates two simple math function tools (add and multiply) that will be used by the agent. The tools are wrapped as FunctionTool objects from LlamaIndex's tools module.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# [USE IN MODEL]\nfrom llama_index.core.tools import FunctionTool\n\n\ndef add(x: int, y: int) -> int:\n    \"\"\"Useful function to add two numbers.\"\"\"\n    return x + y\n\n\ndef multiply(x: int, y: int) -> int:\n    \"\"\"Useful function to multiply two numbers.\"\"\"\n    return x * y\n\n\ntools = [\n    FunctionTool.from_defaults(add),\n    FunctionTool.from_defaults(multiply),\n]\n```\n\n----------------------------------------\n\nTITLE: Saving a Custom Python Model with MLflow\nDESCRIPTION: Saves a custom CodeHelper model using MLflow's pyfunc module. The model references a base OpenAI model as an artifact and includes input examples and signature information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define the location of the base model that we'll be using within our custom pyfunc implementation\nartifacts = {\"model_path\": model_info.model_uri}\n\nwith mlflow.start_run():\n    helper_model = mlflow.pyfunc.log_model(\n        artifact_path=\"code_helper\",\n        python_model=CodeHelper(),\n        input_example=[\"x = 1\"],\n        signature=signature,\n        artifacts=artifacts,\n    )\n```\n\n----------------------------------------\n\nTITLE: California Housing Dataset Evaluation Example\nDESCRIPTION: Complete example of evaluating a linear regression model on the California Housing Dataset using custom metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\nfrom mlflow.models import infer_signature, make_metric\n\n# loading the California housing dataset\ncali_housing = fetch_california_housing(as_frame=True)\n\n# split the dataset into train and test partitions\nX_train, X_test, y_train, y_test = train_test_split(\n    cali_housing.data, cali_housing.target, test_size=0.2, random_state=123\n)\n\n# train the model\nlin_reg = LinearRegression().fit(X_train, y_train)\n\n# Infer model signature\npredictions = lin_reg.predict(X_train)\nsignature = infer_signature(X_train, predictions)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple MLflow Model From Code in Python\nDESCRIPTION: Creates a basic MLflow model that calculates 2 raised to the power of input values. The model is defined in a separate Python file and logged using MLflow's PythonModel class.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/models-from-code/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# If running in a Jupyter or Databricks notebook cell, uncomment the following line:\n# %%writefile \"./basic.py\"\n\nimport pandas as pd\nfrom typing import List, Dict\nfrom mlflow.pyfunc import PythonModel\nfrom mlflow.models import set_model\n\n\nclass BasicModel(PythonModel):\n    def exponential(self, numbers):\n        return {f\"{x}\": 2**x for x in numbers}\n\n    def predict(self, context, model_input) -> Dict[str, float]:\n        if isinstance(model_input, pd.DataFrame):\n            model_input = model_input.to_dict()[0].values()\n        return self.exponential(model_input)\n\n\n# Specify which definition in this script represents the model instance\nset_model(BasicModel())\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Answer Quality Metric\nDESCRIPTION: Creating a custom LLM-judged metric for evaluating answer quality using make_genai_metric\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# The definition explains what \"answer quality\" entails\nanswer_quality_definition = \"\"\"Please evaluate answer quality for the provided output on the following criteria:\nfluency, clarity, and conciseness. Each of the criteria is defined as follows:\n  - Fluency measures how naturally and smooth the output reads.\n  - Clarity measures how understandable the output is.\n  - Conciseness measures the brevity and efficiency of the output without compromising meaning.\nThe more fluent, clear, and concise a text, the higher the score it deserves.\n\"\"\"\n\n# The grading prompt explains what each possible score means\nanswer_quality_grading_prompt = \"\"\"Answer quality: Below are the details for different scores:\n  - Score 1: The output is entirely incomprehensible and cannot be read.\n  - Score 2: The output conveys some meaning, but needs lots of improvement in to improve fluency, clarity, and conciseness.\n  - Score 3: The output is understandable but still needs improvement.\n  - Score 4: The output performs well on two of fluency, clarity, and conciseness, but could be improved on one of these criteria.\n  - Score 5: The output reads smoothly, is easy to understand, and clear. There is no clear way to improve the output on these criteria.\n\"\"\"\n\n# We provide an example of a \"bad\" output\nexample1 = EvaluationExample(\n    input=\"What is MLflow?\",\n    output=\"MLflow is an open-source platform. For managing machine learning workflows, it \"\n    \"including experiment tracking model packaging versioning and deployment as well as a platform \"\n    \"simplifying for on the ML lifecycle.\",\n    score=2,\n    justification=\"The output is difficult to understand and demonstrates extremely low clarity. \"\n    \"However, it still conveys some meaning so this output deserves a score of 2.\",\n)\n\n# We also provide an example of a \"good\" output\nexample2 = EvaluationExample(\n    input=\"What is MLflow?\",\n    output=\"MLflow is an open-source platform for managing machine learning workflows, including \"\n    \"experiment tracking, model packaging, versioning, and deployment.\",\n    score=5,\n    justification=\"The output is easily understandable, clear, and concise. It deserves a score of 5.\",\n)\n\nanswer_quality_metric = make_genai_metric(\n    name=\"answer_quality\",\n    definition=answer_quality_definition,\n    grading_prompt=answer_quality_grading_prompt,\n    version=\"v1\",\n    examples=[example1, example2],\n    model=\"openai:/gpt-4\",\n    greater_is_better=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading LLM Model from Hugging Face Hub\nDESCRIPTION: Code for downloading the MPT-7B instruct model and tokenizer from Hugging Face Hub to a local directory cache using the snapshot_download function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Download the MPT-7B instruct model and tokenizer to a local directory cache\nsnapshot_location = snapshot_download(repo_id=\"mosaicml/mpt-7b-instruct\", local_dir=\"mpt-7b\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Spans in MLflow Tracing\nDESCRIPTION: Shows how to customize spans using the @mlflow.trace decorator by overriding the span name, setting the span type, and adding custom attributes. It also demonstrates updating span attributes dynamically within the function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@mlflow.trace(\n    name=\"call-local-llm\", span_type=SpanType.LLM, attributes={\"model\": \"gpt-4o-mini\"}\n)\ndef invoke(prompt: str):\n    return client.invoke(\n        messages=[{\"role\": \"user\", \"content\": prompt}], model=\"gpt-4o-mini\"\n    )\n```\n\nLANGUAGE: python\nCODE:\n```\n@mlflow.trace(span_type=SpanType.LLM)\ndef invoke(prompt: str):\n    model_id = \"gpt-4o-mini\"\n    # Get the current span (created by the @mlflow.trace decorator)\n    span = mlflow.get_current_active_span()\n    # Set the attribute to the span\n    span.set_attributes({\"model\": model_id})\n    return client.invoke(messages=[{\"role\": \"user\", \"content\": prompt}], model=model_id)\n```\n\n----------------------------------------\n\nTITLE: Testing Custom PyFunc Model Implementation in Python\nDESCRIPTION: This code tests the custom PyFunc model implementation by instantiating the model, loading day-of-week models, and performing inference. It compares the results with the previously trained individual model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate our DOW MME\nmodel_uris = [f\"models:/{DOW_MODEL_NAME_PREFIX}{i}/latest\" for i in df[\"dow\"].unique()]\ndow_model = DOWModel(model_uris)\ndow_model.load_context(None)\nprint(\"Model URIs:\")\nprint(model_uris)\n\n# Perform inference using our training data for Tuesday\nparams = {\"dow\": 1}\nmme_tuesday_fitted_values = dow_model.predict(None, head_of_training_data, params=params)\nassert all(tuesday_fitted_values == mme_tuesday_fitted_values)\n\nprint(\"\\nTuesday fitted values:\")\nprint(mme_tuesday_fitted_values)\n```\n\n----------------------------------------\n\nTITLE: Using fastai Model with MLflow PyFunc Interface\nDESCRIPTION: This example shows how to load a previously saved fastai model using MLflow's PyFunc interface and generate predictions from a DataFrame. The PyFunc interface provides a standardized way to make predictions regardless of the underlying model framework.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom fastai.data.external import URLs, untar_data\nfrom fastai.tabular.core import Categorify, FillMissing, Normalize, TabularPandas\nfrom fastai.tabular.data import TabularDataLoaders\nfrom fastai.tabular.learner import tabular_learner\nfrom fastai.data.transforms import RandomSplitter\nfrom fastai.metrics import accuracy\nfrom fastcore.basics import range_of\nimport pandas as pd\nimport mlflow\nimport mlflow.fastai\n\nmodel_uri = ...\n\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path / \"adult.csv\")\ntest_df = df.copy()\ntest_df.drop([\"salary\"], axis=1, inplace=True)\n\nloaded_model = mlflow.pyfunc.load_model(model_uri)\nloaded_model.predict(test_df)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Tensorflow Models with MLflow\nDESCRIPTION: Demonstrates how to manually save a Tensorflow model to MLflow and then load it back for inference. It uses mlflow.tensorflow.log_model() to save the model and mlflow.tensorflow.load_model() to load it.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/guide/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.Sequential(\n    [\n        keras.Input([28, 28, 3]),\n        keras.layers.Conv2D(8, 2),\n        keras.layers.MaxPool2D(2),\n        keras.layers.Flatten(),\n        keras.layers.Dense(2),\n        keras.layers.Softmax(),\n    ]\n)\n\nsave_path = \"model\"\nwith mlflow.start_run() as run:\n    mlflow.tensorflow.log_model(model, \"model\")\n\n# Load back the model.\nloaded_model = mlflow.tensorflow.load_model(f\"runs:/{run.info.run_id}/{save_path}\")\n\nprint(loaded_model.predict(tf.random.uniform([1, 28, 28, 3])))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Embedding Function with MLflow in Python\nDESCRIPTION: Defines a function to evaluate an embedding model's performance using MLflow. It includes document chunking, retriever initialization, and evaluation using MLflow's evaluate function. The function returns evaluation results for analysis.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_embedding(embedding_function):\n    CHUNK_SIZE = 1000\n    list_of_documents = loader.load()\n    text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n    docs = text_splitter.split_documents(list_of_documents)\n    retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\n\n    def retrieve_doc_ids(question: str) -> list[str]:\n        docs = retriever.get_relevant_documents(question)\n        return [doc.metadata[\"source\"] for doc in docs]\n\n    def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\n        return question_df[\"question\"].apply(retrieve_doc_ids)\n\n    with mlflow.start_run():\n        return mlflow.evaluate(\n            model=retriever_model_function,\n            data=eval_data,\n            model_type=\"retriever\",\n            targets=\"source\",\n            evaluators=\"default\",\n        )\n\n\nresult1 = evaluate_embedding(DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\"))\n# To validate the results of a different model, comment out the above line and uncomment the below line:\n# result2 = evaluate_embedding(SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n\neval_results_of_retriever_df_bge = result1.tables[\"eval_results_table\"]\n# To validate the results of a different model, comment out the above line and uncomment the below line:\n# eval_results_of_retriever_df_MiniLM = result2.tables[\"eval_results_table\"]\ndisplay(eval_results_of_retriever_df_bge)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-judged Answer Similarity Metric\nDESCRIPTION: Creates a custom answer similarity metric using GPT-4 as the judge, with example-based metric configuration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/qa-evaluation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.metrics.genai import EvaluationExample, answer_similarity\n\nexample = EvaluationExample(\n    input=\"What is MLflow?\",\n    output=\"MLflow is an open-source platform for managing machine \"\n    \"learning workflows, including experiment tracking, model packaging, \"\n    \"versioning, and deployment, simplifying the ML lifecycle.\",\n    score=4,\n    justification=\"The definition effectively explains what MLflow is \"\n    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n    grading_context={\n        \"targets\": \"MLflow is an open-source platform for managing \"\n        \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n        \"a company that specializes in big data and machine learning solutions. MLflow is \"\n        \"designed to address the challenges that data scientists and machine learning \"\n        \"engineers face when developing, training, and deploying machine learning models.\"\n    },\n)\n\nanswer_similarity_metric = answer_similarity(model=\"openai:/gpt-4\", examples=[example])\n\nprint(answer_similarity_metric)\n```\n\n----------------------------------------\n\nTITLE: Defining CNN Model with Keras\nDESCRIPTION: Creates a convolutional neural network model using Keras Sequential API for MNIST classification.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninput_shape = (28, 28, 1)\nnum_classes = 10\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.Flatten(),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Type Hint-Based Model Signature Example\nDESCRIPTION: Illustrates MLflow's new type hint-based model signature feature which allows defining model signatures using Python type hints in PythonModel's predict function for input validation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Implementing Custom PyFunc Model for Day-of-Week Predictions in Python\nDESCRIPTION: This class defines a custom PyFunc model that loads and manages multiple day-of-week models. It includes methods for loading models and making predictions based on the specified day of the week.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass DOWModel(mlflow.pyfunc.PythonModel):\n    def __init__(self, model_uris):\n        self.model_uris = model_uris\n        self.models = {}\n\n    @staticmethod\n    def _model_uri_to_dow(model_uri: str) -> int:\n        return int(model_uri.split(\"/\")[-2].split(\"_\")[-1])\n\n    def load_context(self, context):\n        self.models = {\n            self._model_uri_to_dow(model_uri): mlflow.sklearn.load_model(model_uri)\n            for model_uri in self.model_uris\n        }\n\n    def predict(self, context, model_input, params):\n        # Parse the dow parameter\n        dow = params.get(\"dow\")\n        if dow is None:\n            raise ValueError(\"DOW param is not passed.\")\n\n        # Get the model associated with the dow parameter\n        model = self.models.get(dow)\n        if model is None:\n            raise ValueError(f\"Model {dow} version was not found: {self.models.keys()}.\")\n\n        # Perform inference\n        return model.predict(model_input)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Compiled Model Predictions\nDESCRIPTION: Loop to display prediction results from the compiled model, showing correct vs incorrect classifications.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor compiled_residual, compiled_prediction in zip(compiled_residuals, compiled_predictions):\n    is_correct = \"Correct\" if bool(compiled_residual) else \"Incorrect\"\n    prediction = compiled_prediction.label\n    print(f\"{is_correct} prediction: {' ' * (12 - len(is_correct))}{prediction}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Metric Object using make_metric\nDESCRIPTION: Wrapping a custom evaluation function into a metric object using make_metric with optimization parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.metrics import make_metric\n\nmymetric = make_metric(eval_fn=my_metric_eval_fn, greater_is_better=False)\n```\n\n----------------------------------------\n\nTITLE: Logging Plots and Model Metrics in MLflow\nDESCRIPTION: This code snippet demonstrates how to train a Ridge regression model, generate various plots, and log them along with model metrics and parameters to MLflow. It showcases both direct logging of matplotlib figures and logging of locally saved plot images.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/part2-logging-plots/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(\"Visualizations Demo\")\n\nX = my_data.drop(columns=[\"demand\", \"date\"])\ny = my_data[\"demand\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nfig1 = plot_time_series_demand(my_data, window_size=28)\nfig2 = plot_box_weekend(my_data)\nfig3 = plot_scatter_demand_price(my_data)\nfig4 = plot_density_weekday_weekend(my_data)\n\n# Execute the correlation plot, saving the plot to a local temporary directory\nplot_correlation_matrix_and_save(my_data)\n\n# Define our Ridge model\nmodel = Ridge(alpha=1.0)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Calculate error metrics\nmse = mean_squared_error(y_test, y_pred)\nrmse = math.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nmsle = mean_squared_log_error(y_test, y_pred)\nmedae = median_absolute_error(y_test, y_pred)\n\n# Generate prediction-dependent plots\nfig5 = plot_residuals(y_test, y_pred)\nfig6 = plot_coefficients(model, X_test.columns)\nfig7 = plot_prediction_error(y_test, y_pred)\nfig8 = plot_qq(y_test, y_pred)\n\n# Start an MLflow run for logging metrics, parameters, the model, and our figures\nwith mlflow.start_run() as run:\n    # Log the model\n    mlflow.sklearn.log_model(\n        sk_model=model, input_example=X_test, artifact_path=\"model\"\n    )\n\n    # Log the metrics\n    mlflow.log_metrics(\n        {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"msle\": msle, \"medae\": medae}\n    )\n\n    # Log the hyperparameter\n    mlflow.log_param(\"alpha\", 1.0)\n\n    # Log plots\n    mlflow.log_figure(fig1, \"time_series_demand.png\")\n    mlflow.log_figure(fig2, \"box_weekend.png\")\n    mlflow.log_figure(fig3, \"scatter_demand_price.png\")\n    mlflow.log_figure(fig4, \"density_weekday_weekend.png\")\n    mlflow.log_figure(fig5, \"residuals_plot.png\")\n    mlflow.log_figure(fig6, \"coefficients_plot.png\")\n    mlflow.log_figure(fig7, \"prediction_errors.png\")\n    mlflow.log_figure(fig8, \"qq_plot.png\")\n\n    # Log the saved correlation matrix plot by referring to the local file system location\n    mlflow.log_artifact(\"/tmp/corr_plot.png\")\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Example with Custom Epoch Parameter\nDESCRIPTION: Command to run the MLflow example with a custom value for the max_epochs parameter. This allows users to specify how many training epochs should be performed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/CaptumExample/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P max_epochs=X\n```\n\n----------------------------------------\n\nTITLE: Logging a Transformers Model with MLflow\nDESCRIPTION: Demonstrates how to log a text generation pipeline from the Transformers library to MLflow with the option to use reference-only mode for storage efficiency. The code includes options for input examples, signatures, and the save_pretrained parameter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/text-generation/text-generation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=generation_pipeline,\n        artifact_path=\"text_generator\",\n        input_example=input_example,\n        signature=signature,\n        # Transformer model does not use Pandas Dataframe as input, internal input type conversion should be skipped.\n        example_no_conversion=True,\n        # Uncomment the following line to save the model in 'reference-only' mode:\n        # save_pretrained=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Models\nDESCRIPTION: Sets up alternative configuration using Databricks hosted LLMs and embeddings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%pip install llama-index-llms-databricks -qU\n```\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = \"https://YOUR_DATABRICKS_HOST/serving-endpoints/\"\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass(\"Enter Databricks API Key\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.databricks import DatabricksEmbedding\nfrom llama_index.llms.databricks import Databricks\n\nSettings.embed_model = DatabricksEmbedding(model=\"databricks-gte-large-en\")\nSettings.llm = Databricks(model=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Trace Timeout in MLflow Python\nDESCRIPTION: Demonstrates how to set and use trace timeout functionality in MLflow using environment variables. The example shows a long-running operation that exceeds the timeout limit, causing MLflow to automatically halt the trace with ERROR status.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/faq.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport os\nimport time\n\n# Set the timeout to 5 seconds for demonstration purposes\nos.environ[\"MLFLOW_TRACE_TIMEOUT_SECONDS\"] = \"5\"\n\n\n# Simulate a long-running operation\n@mlflow.trace\ndef long_running():\n    for _ in range(10):\n        child()\n\n\n@mlflow.trace\ndef child():\n    time.sleep(1)\n\n\nlong_running()\n```\n\n----------------------------------------\n\nTITLE: MLflow Search Runs with Pandas\nDESCRIPTION: New API to return MLflow experiment search results as a pandas dataframe for easier data analysis and manipulation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.search_runs\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG System with LangChain\nDESCRIPTION: Creating a RAG system using LangChain and Chroma for document retrieval and question answering based on MLflow documentation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nloader = WebBaseLoader(\n    [\n        \"https://mlflow.org/docs/latest/index.html\",\n        \"https://mlflow.org/docs/latest/tracking/autolog.html\",\n        \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\",\n        \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\",\n    ]\n)\n\ndocuments = loader.load()\nCHUNK_SIZE = 1000\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n\nllm = Databricks(\n    endpoint_name=\"<your-endpoint-name>\",  # replace this!\n    extra_params={\n        \"temperature\": 0.1,\n        \"top_p\": 0.1,\n        \"max_tokens\": 500,\n    },\n)\n\nembedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\ndocsearch = Chroma.from_documents(texts, embedding_function)\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(fetch_k=3),\n    return_source_documents=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Flask Server Secret Key\nDESCRIPTION: Command to set the required environment variable for CSRF protection in MLflow's basic auth app.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_FLASK_SERVER_SECRET_KEY=\"my-secret-key\"\n```\n\n----------------------------------------\n\nTITLE: Type Hints in PythonModel Class\nDESCRIPTION: Example of implementing a PythonModel with type hints for input validation using Pydantic models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pydantic\nimport mlflow\n\n\nclass Message(pydantic.BaseModel):\n    role: str\n    content: str\n\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def predict(self, model_input: list[Message], params=None) -> list[str]:\n        return [msg.content for msg in model_input]\n```\n\n----------------------------------------\n\nTITLE: Setting Model Signature Post-Logging\nDESCRIPTION: Demonstrates how to set a signature on an already logged model by loading it, inferring the signature, and updating it.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn import datasets\nimport mlflow\nfrom mlflow.models.model import get_model_info\nfrom mlflow.models import infer_signature, set_signature\n\nmodel_uri = f\"runs:/{run.info.run_id}/iris_rf\"\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nX_test, _ = datasets.load_iris(return_X_y=True, as_frame=True)\nsignature = infer_signature(X_test, model.predict(X_test))\n\nset_signature(model_uri, signature)\n\nassert get_model_info(model_uri).signature == signature\n```\n\n----------------------------------------\n\nTITLE: Complete Hyperparameter Tuning Implementation with Child Runs\nDESCRIPTION: Full implementation of hyperparameter tuning using MLflow's parent and child runs, including parameter logging, run generation, and execution functions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport mlflow\nfrom functools import partial\nfrom itertools import starmap\nfrom more_itertools import consume\n\n\n# Define a function to log parameters and metrics and add tag\n# logging for search_runs functionality\ndef log_run(run_name, test_no, param1_choices, param2_choices, tag_ident):\n    with mlflow.start_run(run_name=run_name, nested=True):\n        mlflow.log_param(\"param1\", random.choice(param1_choices))\n        mlflow.log_param(\"param2\", random.choice(param2_choices))\n        mlflow.log_metric(\"metric1\", random.uniform(0, 1))\n        mlflow.log_metric(\"metric2\", abs(random.gauss(5, 2.5)))\n        mlflow.set_tag(\"test_identifier\", tag_ident)\n\n\n# Generate run names\ndef generate_run_names(test_no, num_runs=5):\n    return (f\"run_{i}_test_{test_no}\" for i in range(num_runs))\n\n\n# Execute tuning function, allowing for param overrides,\n# run_name disambiguation, and tagging support\ndef execute_tuning(\n    test_no,\n    param1_choices=[\"a\", \"b\", \"c\"],\n    param2_choices=[\"d\", \"e\", \"f\"],\n    test_identifier=\"\",\n):\n    ident = \"default\" if not test_identifier else test_identifier\n    # Use a parent run to encapsulate the child runs\n    with mlflow.start_run(run_name=f\"parent_run_test_{ident}_{test_no}\"):\n        # Partial application of the log_run function\n        log_current_run = partial(\n            log_run,\n            test_no=test_no,\n            param1_choices=param1_choices,\n            param2_choices=param2_choices,\n            tag_ident=ident,\n        )\n        mlflow.set_tag(\"test_identifier\", ident)\n        # Generate run names and apply log_current_run function to each run name\n        runs = starmap(\n            log_current_run, ((run_name,) for run_name in generate_run_names(test_no))\n        )\n        # Consume the iterator to execute the runs\n        consume(runs)\n\n\n# Set the tracking uri and experiment\nmlflow.set_tracking_uri(\"http://localhost:8080\")\nmlflow.set_experiment(\"Nested Child Association\")\n\n# Define custom parameters\nparam_1_values = [\"x\", \"y\", \"z\"]\nparam_2_values = [\"u\", \"v\", \"w\"]\n\n# Execute hyperparameter tuning runs with custom parameter choices\nconsume(\n    starmap(execute_tuning, ((x, param_1_values, param_2_values) for x in range(5)))\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using an Autologged OpenAI Model in Python\nDESCRIPTION: Example showing how to load a previously autologged and registered OpenAI model using MLflow's PyFunc API, allowing for standardized inference interface regardless of the underlying model type.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/autologging/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nloaded_autologged_model = mlflow.pyfunc.load_model(\n    f\"models:/{REGISTERED_MODEL_NAME}/{MODEL_VERSION}\"\n)\n\nloaded_autologged_model.predict(\n    \"How much relative time difference would occur between an astronaut travelling at 0.98c for 14 years \"\n    \"as measured by an on-board clock on the spacecraft and humans on Earth, assuming constant speed?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Logging DialoGPT Model with MLflow\nDESCRIPTION: Records the conversational pipeline model in MLflow, including its signature, task type, and an input example. This enables tracking, versioning, and later retrieval of the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/conversational-model.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=conversational_pipeline,\n        artifact_path=\"chatbot\",\n        task=\"conversational\",\n        signature=signature,\n        input_example=\"A clever and witty question\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Inspecting PyFunc Model Configuration in Python\nDESCRIPTION: This code snippet demonstrates how to inspect the configuration of a PyFunc model, including how to retrieve model information and configuration details. It also shows how to load a model with a custom configuration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel_info = mlflow.models.get_model_info(model_uri)\nmodel_info.flavors[mlflow.pyfunc.FLAVOR_NAME][mlflow.pyfunc.MODEL_CONFIG]\n\n# Alternatively, you can load the PyFunc model and inspect the `model_config` property:\npyfunc_model = mlflow.pyfunc.load_model(model_uri)\npyfunc_model.model_config\n\n# Model configuration can be changed at loading time:\npyfunc_model = mlflow.pyfunc.load_model(model_uri, model_config=dict(temperature=0.93))\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Tracing for LlamaIndex Workflow in Python\nDESCRIPTION: This snippet demonstrates how to enable automatic tracing for LlamaIndex Workflow using MLflow. It uses the mlflow.llama_index.autolog() function to set up tracing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.llama_index.autolog()\n```\n\n----------------------------------------\n\nTITLE: Tracing Streaming Functions in MLflow\nDESCRIPTION: Shows how to use the @mlflow.trace decorator to trace functions that return a generator or an iterator. It includes examples of basic streaming and using a custom output reducer for aggregating stream elements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@mlflow.trace\ndef stream_data():\n    for i in range(5):\n        yield i\n```\n\nLANGUAGE: python\nCODE:\n```\n@mlflow.trace(output_reducer=lambda x: \",\".join(x))\ndef stream_data():\n    for c in \"hello\":\n        yield c\n```\n\n----------------------------------------\n\nTITLE: Fixing Missing Dependencies with MLflow Predict CLI (Bash)\nDESCRIPTION: Shows how to use the pip-requirements-override option in the MLflow CLI to add missing dependencies without re-logging the model during troubleshooting.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models predict \\\n    -m runs:/<run_id>/<model_path> \\\n    -I <input_path> \\\n    --pip-requirements-override opencv-python==4.8.0\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Validating OpenAI API Key\nDESCRIPTION: Imports necessary libraries and validates that the OpenAI API key is set in the environment variables, which is required for making API calls to GPT-4.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-chat-completions.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport openai\nimport pandas as pd\nfrom IPython.display import HTML\n\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import ColSpec, ParamSchema, ParamSpec, Schema\n\n# Run a quick validation that we have an entry for the OPEN_API_KEY within environment variables\nassert \"OPENAI_API_KEY\" in os.environ, \"OPENAI_API_KEY environment variable must be set\"\n```\n\n----------------------------------------\n\nTITLE: Testing Model Prediction with Optional Inputs in Python\nDESCRIPTION: Illustrates how the loaded model handles prediction when optional inputs are not provided. This example showcases MLflow's flexibility in dealing with partial data inputs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Using the model while not providing an optional input (note the output return structure and the non existent optional columns)\n\nloaded_model.predict([{\"a\": [\"a\", \"b\", \"c\"], \"c\": {\"d\": \"d\"}}])\n```\n\n----------------------------------------\n\nTITLE: Inspecting Dataset Information in PyTorch\nDESCRIPTION: Prints the image size and the sizes of training and test datasets to understand the data structure before training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Image size: {training_data[0][0].shape}\")\nprint(f\"Size of training dataset: {len(training_data)}\")\nprint(f\"Size of test dataset: {len(test_data)}\")\n```\n\n----------------------------------------\n\nTITLE: Batched Metric Logging in Python\nDESCRIPTION: Example of using the new batched logging API to log multiple metrics at the end of a model training epoch. This improves performance when logging many metrics at once.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_61\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.log_metrics({\"metric1\": value1, \"metric2\": value2})\n```\n\n----------------------------------------\n\nTITLE: Defining Faithfulness Metric for RAG Evaluation\nDESCRIPTION: Creates a faithfulness metric using MLflow's genai module, with examples for scoring model outputs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.metrics.genai import EvaluationExample, faithfulness\n\n# Create a good and bad example for faithfulness in the context of this problem\nfaithfulness_examples = [\n    EvaluationExample(\n        input=\"How do I disable MLflow autologging?\",\n        output=\"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \",\n        score=2,\n        justification=\"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\",\n        grading_context={\n            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n        },\n    ),\n    EvaluationExample(\n        input=\"How do I disable MLflow autologging?\",\n        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n        score=5,\n        justification=\"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\",\n        grading_context={\n            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n        },\n    ),\n]\n\nfaithfulness_metric = faithfulness(model=\"openai:/gpt-4\", examples=faithfulness_examples)\nprint(faithfulness_metric)\n```\n\n----------------------------------------\n\nTITLE: Loading Saved MLflow Model\nDESCRIPTION: Shows how to load a previously saved custom MLflow model from disk using the pyfunc module.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/introduction.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load the saved model\nloaded_model = mlflow.pyfunc.load_model(model_path)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for MLflow and Scikit-learn\nDESCRIPTION: Import necessary Python packages including pandas, scikit-learn components, and MLflow modules for model tracking and signature inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/notebooks/tracking_quickstart.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\nfrom mlflow.models import infer_signature\n```\n\n----------------------------------------\n\nTITLE: Validating MLflow Model with Virtual Environment in Python\nDESCRIPTION: Demonstrates how to use mlflow.models.predict to validate a logged MLflow model in a virtual environment. This checks model dependencies, input data, and extra environment variables before deployment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        return model_input\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        \"model\",\n        python_model=MyModel(),\n        input_example=[\"a\", \"b\", \"c\"],\n    )\n\nmlflow.models.predict(\n    model_uri=model_info.model_uri,\n    input_data=[\"a\", \"b\", \"c\"],\n    pip_requirements_override=[\"...\"],\n    extra_envs={\"MY_ENV_VAR\": \"my_value\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling TensorFlow Autologging in Python\nDESCRIPTION: Code to enable automatic logging of metrics and optimizer parameters from TensorFlow to MLflow. Works with TensorFlow versions between 1.12 and 2.0.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.tensorflow.autolog()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Uncompiled Model Predictions\nDESCRIPTION: Loop to display prediction results from the uncompiled model, showing correct vs incorrect classifications.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor uncompiled_residual, uncompiled_prediction in zip(uncompiled_residuals, uncompiled_predictions):\n    is_correct = \"Correct\" if bool(uncompiled_residual) else \"Incorrect\"\n    prediction = uncompiled_prediction.label\n    print(f\"{is_correct} prediction: {' ' * (12 - len(is_correct))}{prediction}\")\n```\n\n----------------------------------------\n\nTITLE: Equivalent Implementation Without Gateway (Python)\nDESCRIPTION: Python code showing the equivalent implementation of using a Unity Catalog function without the MLflow AI Gateway, demonstrating the underlying process.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/uc_integration/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfunction = {\n    \"type\": \"function\",\n    \"function\": {\n        \"description\": None,\n        \"name\": \"my.uc_func.add\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"x\": {\n                    \"type\": \"integer\",\n                    \"name\": \"x\",\n                    \"description\": \"The first number to add.\",\n                },\n                \"y\": {\n                    \"type\": \"integer\",\n                    \"name\": \"y\",\n                    \"description\": \"The second number to add.\",\n                },\n            },\n            \"required\": [\"x\", \"y\"],\n        },\n    },\n}\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the result of 1 + 2?\",\n    }\n]\n\nresp = client.chat.completions.create(\n    model=\"chat\",\n    tools=[function],\n)\n\nresp_message = resp.choices[0].message\nmessages.append(resp_message)\ntool_call = tool_calls[0]\narguments = json.loads(tool_call.function.arguments)\nresult = arguments[\"x\"] + arguments[\"y\"]\nmessages.append(\n    {\n        \"tool_call_id\": tool_call.id,\n        \"role\": \"tool\",\n        \"name\": \"my.uc_func.add\",\n        \"content\": str(result),\n    }\n)\n\nfinal_resp = client.chat.messages.create(\n    model=\"chat\",\n    messages=messages,\n)\n\nprint(final_resp.choices[0].message.content)  # -> The result of 1 + 2 is 3\n```\n\n----------------------------------------\n\nTITLE: Creating Spark UDFs with StructType Results\nDESCRIPTION: Extends the mlflow.pyfunc.spark_udf() function to support StructType results when creating Spark User-Defined Functions (UDFs) from MLflow models. This allows for more complex return types in Spark transformations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmlflow.pyfunc.spark_udf()\n```\n\n----------------------------------------\n\nTITLE: Testing Uncompiled DSPy Text Classifier\nDESCRIPTION: Demonstrates the use of the uncompiled DSPy text classifier on sample inputs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import copy\n\ntext_classifier = copy(TextClassifier())\n\nmessage = \"I am interested in space\"\nprint(text_classifier(text=message))\n\nmessage = \"I enjoy ice skating\"\nprint(text_classifier(text=message))\n```\n\n----------------------------------------\n\nTITLE: Setting Signature on Registered Model in Python\nDESCRIPTION: This code shows how to set a signature on a registered model by creating a new model version with the updated signature. It uses the MLflow client to retrieve the existing model version and create a new one with the signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.ensemble import RandomForestClassifier\nimport mlflow\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n\nclient = mlflow.client.MlflowClient()\nmodel_name = \"add_signature_model\"\nmodel_version = 1\nmv = client.get_model_version(name=model_name, version=model_version)\n\n# set a dummy signature on the model version source\nsignature = infer_signature(np.array([1]))\nset_signature(mv.source, signature)\n\n# create a new model version with the updated source\nclient.create_model_version(name=model_name, source=mv.source, run_id=mv.run_id)\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Model to Correct Misheard Lyrics\nDESCRIPTION: This snippet demonstrates how to use the loaded MLflow model to correct a set of misheard lyrics. It creates a DataFrame of misheard lyrics, predicts the corrections using the model, and formats the output for display.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Generate some questionable lyrics\nbad_lyrics = pd.DataFrame(\n    {\n        \"lyric\": [\n            \"We built this city on sausage rolls\",\n            \"Hold me closer, Tony Danza\",\n            \"Sweet dreams are made of cheese. Who am I to dis a brie? I cheddar the world and a feta cheese\",\n            \"Excuse me while I kiss this guy\",\n            \"I want to rock and roll all night and part of every day\",\n            \"Don't roam out tonight, it's bound to take your sight, there's a bathroom on the right.\",\n            \"I think you'll understand, when I say that somethin', I want to take your land\",\n        ]\n    }\n)\n\n# Submit our faulty lyrics to the model\nfix_my_lyrics = model.predict(bad_lyrics, params={\"max_tokens\": 500, \"temperature\": 0})\n\n# See what the response is\nformatted_output = \"<br>\".join(\n    [f\"<p><strong>{line.strip()}</strong></p>\" for line in fix_my_lyrics]\n)\ndisplay(HTML(formatted_output))\n```\n\n----------------------------------------\n\nTITLE: Using List of Pydantic Models as Input Type Hint in MLflow PythonModel\nDESCRIPTION: Example showing how to use a list of Pydantic Message objects as the type hint for MLflow model input validation. The model validates input against the Message schema and processes a list of messages.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# The input_example can also be a list of dict with the same schema as Message\ninput_example = [\n    {\"role\": \"system\", \"content\": \"Hello\"},\n    {\"role\": \"user\", \"content\": \"Hi\"},\n]\nprint(predict(input_example))  # Output: ['Hello', 'Hi']\n\n# If your input doesn't match the schema, it will raise an exception\n# e.g. passing a list of string here will raise an exception\npredict([\"hello\"])\n# Output: Failed to validate data against type hint `list[Message]`, invalid elements:\n# [('hello', \"Expecting example to be a dictionary or pydantic model instance for Pydantic type hint, got <class 'str'>\")]\n```\n\n----------------------------------------\n\nTITLE: Evaluating MLflow RAG Workflow with Different Retriever Strategies\nDESCRIPTION: This code snippet demonstrates how to use the mlflow.evaluate() API to assess the performance of different retriever strategies. It computes latency and answer correctness metrics for each model, using a specified evaluation dataset and logged model URIs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.metrics import latency\nfrom mlflow.metrics.genai import answer_correctness\n\nfor model_info in models:\n    with mlflow.start_run(run_id=model_info.run_id):\n        result = mlflow.evaluate(\n            # Pass the URI of the logged model above\n            model=model_info.model_uri,\n            data=eval_df,\n            # Specify the column for ground truth answers.\n            targets=\"ground_truth\",\n            # Define the metrics to compute.\n            extra_metrics=[\n                latency(),\n                answer_correctness(\"openai:/gpt-4o-mini\"),\n            ],\n            # The answer_correctness metric requires \"inputs\" column to be\n            # present in the dataset. We have \"query\" instead so need to\n            # specify the mapping in `evaluator_config` parameter.\n            evaluator_config={\"col_mapping\": {\"inputs\": \"query\"}},\n        )\n```\n\n----------------------------------------\n\nTITLE: Searching Experiments with the New API in MLflow Tracking\nDESCRIPTION: The new `mlflow.search_experiments()` API allows searching experiments by name and tags, enhancing the filtering capabilities for experiment management.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.search_experiments()\n```\n\n----------------------------------------\n\nTITLE: Implementing Webpage Content Comparison Function using OpenAI Embeddings in Python\nDESCRIPTION: A function that compares the content of two webpages by extracting text, generating embeddings using an OpenAI model, and calculating similarity metrics (cosine similarity and Euclidean distance). The function requires URLs and content IDs for targeted text extraction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef compare_pages(url1, url2, id1, id2):\n    \"\"\"\n    Compare two webpages and return the similarity score.\n\n    Args:\n        url1: URL of the first webpage.\n        url2: URL of the second webpage.\n        id1: The target id for the div containing the main text content of the first page\n        id2: The target id for the div containing the main text content of the second page\n\n    Returns:\n        A tuple of floats representing the similarity score for cosine similarity and euclidean distance.\n    \"\"\"\n    text1 = extract_text_from_url(url1, id1)\n    text2 = extract_text_from_url(url2, id2)\n\n    if text1 and text2:\n        embedding1 = model.predict([text1])\n        embedding2 = model.predict([text2])\n\n        return (\n            cosine_similarity(embedding1, embedding2),\n            euclidean_distances(embedding1, embedding2),\n        )\n    else:\n        return \"Failed to retrieve content.\"\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Dataset with Evaluate API for Classification\nDESCRIPTION: This example demonstrates how to use a logged Dataset with MLflow's evaluate API for a classification task. It includes data preparation, model training, dataset creation, and evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/dataset/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost\n\nimport mlflow\nfrom mlflow.data.pandas_dataset import PandasDataset\n\n\ndataset_source_url = \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\"\nraw_data = pd.read_csv(dataset_source_url, delimiter=\";\")\n\n# Extract the features and target data separately\ny = raw_data[\"quality\"]\nX = raw_data.drop(\"quality\", axis=1)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=17\n)\n\n# Create a label encoder object\nle = LabelEncoder()\n\n# Fit and transform the target variable\ny_train_encoded = le.fit_transform(y_train)\ny_test_encoded = le.transform(y_test)\n\n# Fit an XGBoost binary classifier on the training data split\nmodel = xgboost.XGBClassifier().fit(X_train, y_train_encoded)\n\n# Build the Evaluation Dataset from the test set\ny_test_pred = model.predict(X=X_test)\n\neval_data = X_test\neval_data[\"label\"] = y_test\n\n# Assign the decoded predictions to the Evaluation Dataset\neval_data[\"predictions\"] = le.inverse_transform(y_test_pred)\n\n# Create the PandasDataset for use in mlflow evaluate\npd_dataset = mlflow.data.from_pandas(\n    eval_data, predictions=\"predictions\", targets=\"label\"\n)\n\nmlflow.set_experiment(\"White Wine Quality\")\n\n# Log the Dataset, model, and execute an evaluation run using the configured Dataset\nwith mlflow.start_run() as run:\n    mlflow.log_input(pd_dataset, context=\"training\")\n\n    mlflow.xgboost.log_model(\n        artifact_path=\"white-wine-xgb\", xgb_model=model, input_example=X_test\n    )\n\n    result = mlflow.evaluate(data=pd_dataset, predictions=None, model_type=\"classifier\")\n```\n\n----------------------------------------\n\nTITLE: Logging an MLflow Model\nDESCRIPTION: Demonstrates how to log an MLflow model with an input example. This creates a model artifact that can be later loaded or deployed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# log the model\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\"model\", python_model=model, input_example=messages)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embeddings for Document Retrieval\nDESCRIPTION: Creates an instance of OpenAIEmbeddings to be used for embedding documents and queries in the vector database.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nembeddings = OpenAIEmbeddings()\n```\n\n----------------------------------------\n\nTITLE: Environment variables file format example\nDESCRIPTION: Example of the environment_variables.txt file generated by MLflow when environment variables are used during model inference. The file records only the variable names, not their values.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n# This file records environment variable names that are used during model inference.\n# They might need to be set when creating a serving endpoint from this model.\n# Note: it is not guaranteed that all environment variables listed here are required\nTEST_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Validating Serving Input for MLflow Models in Python\nDESCRIPTION: This code snippet shows how to validate the serving input example before actually serving the model. It uses the 'validate_serving_input' function from MLflow to ensure the input works correctly.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import validate_serving_input\n\nresult = validate_serving_input(model_info.model_uri, serving_example)\nprint(f\"prediction result: {result}\")\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Different Tree Counts\nDESCRIPTION: Executes random forest training with different numbers of trees (10, 20, 50, 100, 200) to compare model performance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/h2o/random_forest.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfor ntrees in [10, 20, 50, 100, 200]:\n    train_random_forest(ntrees)\n```\n\n----------------------------------------\n\nTITLE: Creating MLflow Experiment for Semantic Search\nDESCRIPTION: Sets up a new MLflow experiment for semantic similarity model tracking. Includes optional tracking URI configuration for local or remote servers.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(\"Semantic Similarity\")\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Custom Parameters in MLflow pyfunc\nDESCRIPTION: Demonstrates how to generate predictions using a loaded MLflow pyfunc model with custom parameters. The example overrides the temperature parameter to control the randomness of text generation and shows how to format the output for readability.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/text-generation/text-generation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Validate that our loaded pipeline, as a generic pyfunc, can produce an output that makes sense\npredictions = sentence_generator.predict(\n    data=[\n        \"I can't decide whether to go hiking or kayaking this weekend. Can you help me decide?\",\n        \"Please tell me a joke about hiking.\",\n    ],\n    params={\"temperature\": 0.7},\n)\n\n# Format each prediction for notebook readability\nformatted_predictions = format_predictions(predictions)\n\nfor i, formatted_text in enumerate(formatted_predictions):\n    print(f\"Response to prompt {i + 1}:\\n{formatted_text}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Enabling PyTorch Autologging in MLflow\nDESCRIPTION: Shows how to enable automatic logging of metrics, parameters, and models from PyTorch Lightning training using MLflow's new PyTorch autolog API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.pytorch.autolog()\n```\n\n----------------------------------------\n\nTITLE: Accessing Metric Calculation Details\nDESCRIPTION: This code snippet shows how to access the metric_details property of an EvaluationMetric, which provides information about how the metric is calculated, such as the grading prompt used.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.metrics.rst#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.metrics.genai import relevance\n\nmy_relevance_metric = relevance()\nprint(my_relevance_metric.metric_details)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for MLflow XGBoost Module\nDESCRIPTION: ReStructuredText directive for auto-generating documentation from the mlflow.xgboost module, including all members, undocumented members, and inheritance information\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.xgboost.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.xgboost\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Setting Registry URI API Example\nDESCRIPTION: Example demonstrating how to set and get the model registry URI to specify where model registration APIs communicate.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.set_registry_uri(\"uri\")\nmlflow.get_registry_uri()\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Auto Tracing for DSPy\nDESCRIPTION: Turns on automatic tracing for DSPy modules using MLflow's tracing functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmlflow.dspy.autolog()\n```\n\n----------------------------------------\n\nTITLE: Adding Child Span to MLflow Trace\nDESCRIPTION: Shows how to add a child span to an existing trace using the start_span API. Includes setting span name, request_id, parent_id, inputs, and attributes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/client.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a child span\nchild_span = client.start_span(\n    name=\"child_span\",\n    request_id=request_id,\n    parent_id=root_span.span_id,\n    inputs={\"input_key\": \"input_value\"},\n    attributes={\"attribute_key\": \"attribute_value\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Apple Sales Dataset with Python\nDESCRIPTION: Implements a function to generate synthetic apple sales data with various features including temperature, rainfall, pricing, seasonality, and promotional effects. The function creates a pandas DataFrame with realistic correlations between features and target demand variable, including temporal effects like inflation and harvest seasons.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step5-synthetic-data/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n\ndef generate_apple_sales_data_with_promo_adjustment(\n    base_demand: int = 1000, n_rows: int = 5000\n):\n    \"\"\"\n    Generates a synthetic dataset for predicting apple sales demand with seasonality\n    and inflation.\n\n    This function creates a pandas DataFrame with features relevant to apple sales.\n    The features include date, average_temperature, rainfall, weekend flag, holiday flag,\n    promotional flag, price_per_kg, and the previous day's demand. The target variable,\n    'demand', is generated based on a combination of these features with some added noise.\n\n    Args:\n        base_demand (int, optional): Base demand for apples. Defaults to 1000.\n        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.\n\n    Returns:\n        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.\n\n    Example:\n        >>> df = generate_apple_sales_data_with_seasonality(base_demand=1200, n_rows=6000)\n        >>> df.head()\n    \"\"\"\n\n    # Set seed for reproducibility\n    np.random.seed(9999)\n\n    # Create date range\n    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]\n    dates.reverse()\n\n    # Generate features\n    df = pd.DataFrame(\n        {\n            \"date\": dates,\n            \"average_temperature\": np.random.uniform(10, 35, n_rows),\n            \"rainfall\": np.random.exponential(5, n_rows),\n            \"weekend\": [(date.weekday() >= 5) * 1 for date in dates],\n            \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),\n            \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),\n            \"month\": [date.month for date in dates],\n        }\n    )\n\n    # Introduce inflation over time (years)\n    df[\"inflation_multiplier\"] = (\n        1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03\n    )\n\n    # Incorporate seasonality due to apple harvests\n    df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(\n        2 * np.pi * (df[\"month\"] - 9) / 12\n    )\n\n    # Modify the price_per_kg based on harvest effect\n    df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5\n\n    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month\n    peak_months = [4, 10]  # months following the peak availability\n    df[\"promo\"] = np.where(\n        df[\"month\"].isin(peak_months),\n        1,\n        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),\n    )\n\n    # Generate target variable based on features\n    base_price_effect = -df[\"price_per_kg\"] * 50\n    seasonality_effect = df[\"harvest_effect\"] * 50\n    promo_effect = df[\"promo\"] * 200\n\n    df[\"demand\"] = (\n        base_demand\n        + base_price_effect\n        + seasonality_effect\n        + promo_effect\n        + df[\"weekend\"] * 300\n        + np.random.normal(0, 50, n_rows)\n    ) * df[\n        \"inflation_multiplier\"\n    ]  # adding random noise\n\n    # Add previous day's demand\n    df[\"previous_days_demand\"] = df[\"demand\"].shift(1)\n    df[\"previous_days_demand\"].fillna(\n        method=\"bfill\", inplace=True\n    )  # fill the first row\n\n    # Drop temporary columns\n    df.drop(columns=[\"inflation_multiplier\", \"harvest_effect\", \"month\"], inplace=True)\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Logging a LlamaIndex Index to MLflow\nDESCRIPTION: Sets up an MLflow experiment and logs a LlamaIndex index with chat engine type. This example demonstrates how to save the index to MLflow with an input example for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_experiment(\"llama-index-demo\")\n\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        index,\n        artifact_path=\"index\",\n        engine_type=\"chat\",\n        input_example=\"What did the author do growing up?\",\n    )\n```\n\n----------------------------------------\n\nTITLE: MLflow TensorFlow Model State Loading\nDESCRIPTION: Example of loading TensorFlow model state dict using MLflow's PyTorch integration APIs\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.pytorch.log_state_dict()\nmlflow.pytorch.load_state_dict()\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI in R\nDESCRIPTION: Function to specify the URI of the remote MLflow server for tracking experiments.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_52\n\nLANGUAGE: r\nCODE:\n```\nmlflow_set_tracking_uri(uri)\n```\n\n----------------------------------------\n\nTITLE: Getting Experiment Metadata in R with MLflow\nDESCRIPTION: Retrieves metadata for an experiment and a list of its runs. Can obtain the active experiment if no ID or name is specified. Requires experiment ID or name, and optionally an MLflow client object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_16\n\nLANGUAGE: r\nCODE:\n```\nmlflow_get_experiment(experiment_id = NULL, name = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Running Docker-based MLflow Projects with Arguments in Python\nDESCRIPTION: Demonstrates how to pass arguments to 'docker run' when executing Docker-based MLflow projects. This functionality was added in MLflow 1.8.0.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nmlflow.projects.run(\"path/to/project\", docker_args=[\"--network=host\"])\n```\n\n----------------------------------------\n\nTITLE: Defining Keras Model Training Function\nDESCRIPTION: Implements the model training function with MLflow tracking for parameters, metrics, and model artifacts\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_model(params, epochs, train_x, train_y, valid_x, valid_y, test_x, test_y):\n    # Define model architecture\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    # Compile model\n    model.compile(\n        optimizer=keras.optimizers.SGD(\n            learning_rate=params[\"lr\"], momentum=params[\"momentum\"]\n        ),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    # Train model with MLflow tracking\n    with mlflow.start_run(nested=True):\n        model.fit(\n            train_x,\n            train_y,\n            validation_data=(valid_x, valid_y),\n            epochs=epochs,\n            batch_size=64,\n        )\n        # Evaluate the model\n        eval_result = model.evaluate(valid_x, valid_y, batch_size=64)\n        eval_rmse = eval_result[1]\n\n        # Log parameters and results\n        mlflow.log_params(params)\n        mlflow.log_metric(\"eval_rmse\", eval_rmse)\n\n        # Log model\n        mlflow.tensorflow.log_model(model, \"model\", signature=signature)\n\n        return {\"loss\": eval_rmse, \"status\": STATUS_OK, \"model\": model}\n```\n\n----------------------------------------\n\nTITLE: Logging LlamaIndex Model with External Vector Store\nDESCRIPTION: Shows how to log a LlamaIndex model using model-from-code approach with external vector store\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        \"index.py\",\n        artifact_path=\"index\",\n        engine_type=\"query\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Serving MLflow Model Locally Using Command Line\nDESCRIPTION: This bash command demonstrates how to serve an MLflow model locally using the 'mlflow models serve' command. It requires specifying the model URI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve --model-uri \"<YOUR_MODEL_URI>\"\n```\n\n----------------------------------------\n\nTITLE: Implementing MPT PyFunc Model for Language Generation\nDESCRIPTION: Defines a custom PyFunc model class for MPT-7B that handles model loading, prompt building, and text generation. Includes GPU support and uses transformers library for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nimport mlflow\nimport torch\n\n\nclass MPT(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        \"\"\"\n        This method initializes the tokenizer and language model\n        using the specified model snapshot directory.\n        \"\"\"\n        # Initialize tokenizer and language model\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            context.artifacts[\"snapshot\"], padding_side=\"left\"\n        )\n\n        config = transformers.AutoConfig.from_pretrained(\n            context.artifacts[\"snapshot\"], trust_remote_code=True\n        )\n        # Comment out this configuration setting if not running on a GPU or if triton is not installed.\n        # Note that triton dramatically improves the inference speed performance\n        config.attn_config[\"attn_impl\"] = \"triton\"\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            context.artifacts[\"snapshot\"],\n            config=config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n        # this setting will not function correctly. Setting device to 'cpu' is valid, but\n        # the performance will be very slow.\n        self.model.to(device=\"cuda\")\n\n        self.model.eval()\n\n    def _build_prompt(self, instruction):\n        \"\"\"\n        This method generates the prompt for the model.\n        \"\"\"\n        INSTRUCTION_KEY = \"### Instruction:\"\n        RESPONSE_KEY = \"### Response:\"\n        INTRO_BLURB = (\n            \"Below is an instruction that describes a task. \"\n            \"Write a response that appropriately completes the request.\"\n        )\n\n        return f\"\"\"{INTRO_BLURB}\n        {INSTRUCTION_KEY}\n        {instruction}\n        {RESPONSE_KEY}\n        \"\"\"\n\n    def predict(self, context, model_input, params=None):\n        \"\"\"\n        This method generates prediction for the given input.\n        \"\"\"\n        prompt = model_input[\"prompt\"][0]\n        temperature = model_input.get(\"temperature\", [1.0])[0]\n        max_tokens = model_input.get(\"max_tokens\", [100])[0]\n\n        # Build the prompt\n        prompt = self._build_prompt(prompt)\n\n        # Encode the input and generate prediction\n        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n        # If attempting to run this with only CPU support, change 'cuda' to 'cpu'\n        encoded_input = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n        output = self.model.generate(\n            encoded_input,\n            do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n        )\n\n        # Decode the prediction to text\n        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        # Removing the prompt from the generated text\n        prompt_length = len(self.tokenizer.encode(prompt, return_tensors=\"pt\")[0])\n        generated_response = self.tokenizer.decode(\n            output[0][prompt_length:], skip_special_tokens=True\n        )\n\n        return {\"candidates\": [generated_response]}\n```\n\n----------------------------------------\n\nTITLE: Logging PythonModel\nDESCRIPTION: Example of logging a custom PythonModel using mlflow.pyfunc.log_model within an MLflow run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        artifact_path=\"model\",\n        python_model=MyModel(),\n        input_example=input_example,\n    )\n```\n\n----------------------------------------\n\nTITLE: Requesting Predictions from Served Sktime Model in Python\nDESCRIPTION: This example shows how to request predictions from a served Sktime model using Python requests. It includes preparing the input data and sending a POST request to the local endpoint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport requests\n\nfrom sktime.datasets import load_longley\nfrom sktime.forecasting.model_selection import temporal_train_test_split\n\ny, X = load_longley()\ny_train, y_test, X_train, X_test = temporal_train_test_split(y, X)\n\n# Define local host and endpoint url\nhost = \"127.0.0.1\"\nurl = f\"http://{host}:5000/invocations\"\n\n# Create configuration DataFrame\nX_test_list = X_test.to_numpy().tolist()\npredict_conf = pd.DataFrame(\n    [\n        {\n            \"fh\": [1, 2, 3, 4],\n            \"predict_method\": \"predict_interval\",\n            \"coverage\": [0.9, 0.95],\n            \"X\": X_test_list,\n        }\n    ]\n)\n\n# Create dictionary with pandas DataFrame in the split orientation\njson_data = {\"dataframe_split\": predict_conf.to_dict(orient=\"split\")}\n\n# Score model\nresponse = requests.post(url, json=json_data)\nprint(f\"\\nPyfunc 'predict_interval':\\n${response.json()}\")\n```\n\n----------------------------------------\n\nTITLE: Enabling LlamaIndex Tracing in MLflow\nDESCRIPTION: Shows how to enable tracing for LlamaIndex code using MLflow's autolog function to log input and output of LlamaIndex execution\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.llama_index.autolog()\n\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"What was the first program the author wrote?\")\n```\n\n----------------------------------------\n\nTITLE: Logging MLflow Model\nDESCRIPTION: Logs the created model to MLflow, including an input example for future reference and testing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninput_example = [{\"messages\": [{\"role\": \"user\", \"content\": \"What is DSPy?\"}]}]\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        \"model\",\n        # Pass the path to the saved model file\n        python_model=\"agent.py\",\n        input_example=input_example,\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging Model with MLflow\nDESCRIPTION: Setting up MLflow experiment and logging the Transformers model with appropriate signature\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"Evaluate Hugging Face Text Pipeline\")\n\n# Define the signature\nsignature = mlflow.models.infer_signature(\n    model_input=\"What are the three primary colors?\",\n    model_output=\"The three primary colors are red, yellow, and blue.\",\n)\n\n# Log the model using mlflow\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=mpt_pipeline,\n        artifact_path=\"mpt-7b\",\n        signature=signature,\n        registered_model_name=\"mpt-7b-chat\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Importance for XGBoost Model in Python\nDESCRIPTION: This function creates a feature importance plot for a trained XGBoost model. It uses different importance types based on the booster type and returns a matplotlib figure object. The plot can be logged to MLflow for record-keeping.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef plot_feature_importance(model, booster):  # noqa: D417\n    \"\"\"\n    Plots feature importance for an XGBoost model.\n\n    Args:\n    - model: A trained XGBoost model\n\n    Returns:\n    - fig: The matplotlib figure object\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n    importance_type = \"weight\" if booster == \"gblinear\" else \"gain\"\n    xgb.plot_importance(\n        model,\n        importance_type=importance_type,\n        ax=ax,\n        title=f\"Feature Importance based on {importance_type}\",\n    )\n    plt.tight_layout()\n    plt.close(fig)\n\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for MLflow Model using Python API\nDESCRIPTION: Python code to build a Docker image containing an MLflow model and MLServer inference server. The function requires a model URI and image name, with an optional parameter to enable MLServer instead of Flask.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-to-kubernetes/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.models.build_docker(\n    model_uri=f\"runs:/{run_id}/model\",\n    name=\"<image_name>\",\n    enable_mlserver=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Model Serving Validation Example\nDESCRIPTION: Example code for validating a deployed model's serving input using MLflow's validate_serving_input API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import validate_serving_input\n\nmodel_uri = \"runs:/8935b7aff5a84f559b5fcc2af3e2ea31/model\"\n\n# The model is logged with an input example. MLflow converts\n# it into the serving payload format for the deployed model endpoint,\n# and saves it to 'serving_input_payload.json'\nserving_payload = \"\"\"{\n\"messages\": [\n    {\n    \"role\": \"user\",\n    \"content\": \"What is a good recipe for baking scones that doesn't require a lot of skill?\"\n    }\n],\n\"temperature\": 1.0,\n\"n\": 1,\n\"stream\": false\n}\"\"\"\n\n# Validate the serving payload works on the model\nvalidate_serving_input(model_uri, serving_payload)\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Pipelines in Python\nDESCRIPTION: MLflow 1.27.0 introduces MLflow Pipelines, an opinionated framework for structuring MLOps workflows. This snippet references the documentation link for getting started with MLflow Pipelines.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_24\n\nLANGUAGE: Text\nCODE:\n```\nhttps://mlflow.org/docs/latest/pipelines.html\n```\n\n----------------------------------------\n\nTITLE: Creating MLflow Experiment in R\nDESCRIPTION: Function to create a new MLflow experiment and return its ID, with options for artifact location and tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_6\n\nLANGUAGE: r\nCODE:\n```\nmlflow_create_experiment(\n  name,\n  artifact_location = NULL,\n  client = NULL,\n  tags = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Logging TensorFlow 2.0 Models in Python\nDESCRIPTION: MLflow 1.3 adds support for logging and loading models using TensorFlow 2.0 in the Python client.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.tensorflow.log_model(tf_model, \"model\")\n```\n\n----------------------------------------\n\nTITLE: Running MLflow MNIST Example\nDESCRIPTION: Command to run the MNIST example using MLflow with default parameters. This executes the mnist_torchscript.py script with predefined settings like max_epochs=5.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/MNIST/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run .\n```\n\n----------------------------------------\n\nTITLE: Enabling DSPy Optimizer Autologging with MLflow\nDESCRIPTION: Initial setup code to enable autologging for DSPy optimization using MLflow. This configuration logs compiles, evaluations, and traces from the compile process.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/optimizer.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.dspy.autolog(log_compiles=True, log_evals=True, log_traces_from_compile=True)\n\n# Your DSPy code here\n...\n```\n\n----------------------------------------\n\nTITLE: Inferring Signature for Complex Nested Data Structure in Python\nDESCRIPTION: Demonstrates signature inference for a complex nested data structure containing lists, dictionaries, and optional fields, showcasing MLflow's ability to handle diverse data formats.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndata = [{\"a\": [\"a\", \"b\", \"c\"], \"b\": \"b\", \"c\": {\"d\": \"d\"}}, {\"a\": [\"a\"], \"c\": {\"d\": \"d\", \"e\": \"e\"}}]\n\nreport_signature_info(data)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Static Retriever Results with MLflow\nDESCRIPTION: Uses MLflow.evaluate() to assess the performance of the retriever on the static evaluation dataset, comparing retrieved document IDs against ground truth sources.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Case 1: Evaluating a static evaluation dataset\nwith mlflow.start_run() as run:\n    evaluate_results = mlflow.evaluate(\n        data=data,\n        model_type=\"retriever\",\n        targets=\"source\",\n        predictions=\"retrieved_doc_ids\",\n        evaluators=\"default\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Autologging for DSPy\nDESCRIPTION: Demonstrates how to enable automatic tracing for DSPy programs using MLflow's autolog function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.dspy.autolog()\n```\n\n----------------------------------------\n\nTITLE: Creating and Logging a LangChain Model to MLflow\nDESCRIPTION: Sets up a PromptTemplate with the sous chef template instruction, creates an LLMChain combining the template with an OpenAI model, and logs the chain to MLflow within a tracked experiment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompt = PromptTemplate(\n    input_variables=[\"recipe\", \"customer_count\"],\n    template=template_instruction,\n)\nchain = LLMChain(llm=llm, prompt=prompt)\n\nmlflow.set_experiment(\"Cooking Assistant\")\n\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(chain, \"langchain_model\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Model Coefficients with Horizontal Bar Plot in Python\nDESCRIPTION: This function creates a horizontal bar plot to visualize the coefficients of features in a trained model. It helps in understanding feature importance and their impact on predictions by displaying the magnitude and direction of each coefficient.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef plot_coefficients(model, feature_names, style=\"seaborn\", plot_size=(10, 8)):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n        ax.barh(feature_names, model.coef_)\n        ax.set_title(\"Coefficient Plot\", fontsize=14)\n        ax.set_xlabel(\"Coefficient Value\", fontsize=12)\n        ax.set_ylabel(\"Features\", fontsize=12)\n        plt.tight_layout()\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Importing Card Components in JSX\nDESCRIPTION: This code snippet imports CardGroup and PageCard components from a local file. These components are likely used to create a card-based layout for displaying information about the MLflow Tracking Quickstart guide.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/notebooks/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { CardGroup, PageCard } from \"@site/src/components/Card\";\n```\n\n----------------------------------------\n\nTITLE: Accessing MLflow Version in Python\nDESCRIPTION: Shows how to access the MLflow version programmatically in Python code. The version is exposed through the __version__ attribute in the mlflow module.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_66\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.__version__\n```\n\n----------------------------------------\n\nTITLE: Defining a Type Checking Model in MLflow\nDESCRIPTION: Creates a custom PythonModel that validates input types. This model checks if the input is either a pandas DataFrame or a list, raising a ValueError if the validation fails.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Updating an existing model that wasn't saved with a signature\n\n\nclass MyTypeCheckerModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        print(type(model_input))\n        print(model_input)\n        if not isinstance(model_input, (pd.DataFrame, list)):\n            raise ValueError(\"The input must be a list.\")\n        return \"Input is valid.\"\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=MyTypeCheckerModel(),\n        artifact_path=\"test_model\",\n    )\n\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\nloaded_model.metadata.signature\n```\n\n----------------------------------------\n\nTITLE: Rendering Card Components for Question Generation Tutorial in JSX\nDESCRIPTION: Creates a card interface using CardGroup and PageCard components to link to the tutorial on generating question datasets for RAG evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <PageCard headerText=\"Question Generation for RAG Evaluation\" link=\"/llms/rag/notebooks/question-generation-retrieval-evaluation/\" text=\"Step-by-step demonstration for how to automatically generate a question-answering dataset for RAG evaluation\" />\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Exporting John Snow Labs Model to MLflow\nDESCRIPTION: Example demonstrating how to export a John Snow Labs NLP model to MLflow format, including license setup, model loading, and inference using both johnsnowlabs and pyfunc flavors.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\n\nimport pandas as pd\nfrom johnsnowlabs import nlp\n\nimport mlflow\nfrom mlflow.pyfunc import spark_udf\n\n# 1) Write your raw license.json string into the 'JOHNSNOWLABS_LICENSE_JSON' env variable for MLflow\ncreds = {\n    \"AWS_ACCESS_KEY_ID\": \"...\",\n    \"AWS_SECRET_ACCESS_KEY\": \"...\",\n    \"SPARK_NLP_LICENSE\": \"...\",\n    \"SECRET\": \"...\",\n}\nos.environ[\"JOHNSNOWLABS_LICENSE_JSON\"] = json.dumps(creds)\n\n# 2) Install enterprise libraries\nnlp.install()\n# 3) Start a Spark session with enterprise libraries\nspark = nlp.start()\n\n# 4) Load a model and test it\nnlu_model = \"en.classify.bert_sequence.covid_sentiment\"\nmodel_save_path = \"my_model\"\njohnsnowlabs_model = nlp.load(nlu_model)\njohnsnowlabs_model.predict([\"I hate COVID,\", \"I love COVID\"])\n\n# 5) Export model with pyfunc and johnsnowlabs flavors\nwith mlflow.start_run():\n    model_info = mlflow.johnsnowlabs.log_model(johnsnowlabs_model, model_save_path)\n\n# 6) Load model with johnsnowlabs flavor\nmlflow.johnsnowlabs.load_model(model_info.model_uri)\n\n# 7) Load model with pyfunc flavor\nmlflow.pyfunc.load_model(model_save_path)\n\npandas_df = pd.DataFrame({\"text\": [\"Hello World\"]})\nspark_df = spark.createDataFrame(pandas_df).coalesce(1)\npyfunc_udf = spark_udf(\n    spark=spark,\n    model_uri=model_save_path,\n    env_manager=\"virtualenv\",\n    result_type=\"string\",\n)\nnew_df = spark_df.withColumn(\"prediction\", pyfunc_udf(*pandas_df.columns))\n\n# 9) You can now use the mlflow models serve command to serve the model see next section\n\n# 10)  You can also use x command to deploy model inside of a container see next section\n```\n\n----------------------------------------\n\nTITLE: Sending Request to Custom PyFunc OpenAI Model Server\nDESCRIPTION: Python code to send a JSON request to a custom PyFunc OpenAI model server with inputs and parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Prerequisite: serve a custom pyfunc OpenAI model (not mlflow.openai) on localhost:5678\n#   that defines inputs in the below format and params of `temperature` and `max_tokens`\n\nimport json\nimport requests\n\npayload = json.dumps(\n    {\n        \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"Tell a joke!\"}]},\n        \"params\": {\n            \"temperature\": 0.5,\n            \"max_tokens\": 20,\n        },\n    }\n)\nresponse = requests.post(\n    url=f\"http://localhost:5678/invocations\",\n    data=payload,\n    headers={\"Content-Type\": \"application/json\"},\n)\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: MLflow CLI Artifacts Command\nDESCRIPTION: Command-line interface for interacting with MLflow artifacts. This command allows listing, downloading, and uploading artifacts to run artifact repositories.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_68\n\nLANGUAGE: Shell\nCODE:\n```\nmlflow artifacts\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Model as Python Function\nDESCRIPTION: Load the saved model as a generic Python Function (pyfunc) for making predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/notebooks/tracking_quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Running Prompt Evaluation with MLflow\nDESCRIPTION: Evaluates the prompt using the MLflow.evaluate API with the prediction function and evaluation dataset. Uses built-in metrics including latency and answer similarity with GPT-4 as a judge model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/evaluate.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run(run_name=\"prompt-evaluation\"):\n    mlflow.log_param(\"model\", \"gpt-4o-mini\")\n    mlflow.log_param(\"temperature\", 0.1)\n\n    results = mlflow.evaluate(\n        model=predict,\n        data=eval_data,\n        targets=\"targets\",\n        extra_metrics=[\n            mlflow.metrics.latency(),\n            # Specify GPT4 as a judge model for answer similarity. Other models such as Anthropic,\n            # Bedrock, Databricks, are also supported.\n            mlflow.metrics.genai.answer_similarity(model=\"openai:/gpt-4\"),\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Up MLflow Experiment and Logging OpenAI Model for Lyrics Correction\nDESCRIPTION: This snippet creates an MLflow experiment for a lyrics corrector, logs an OpenAI model with a custom prompt, and defines the model signature. It then loads the model as a Python function for later use.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create a new experiment (or reuse the existing one if we've run this cell more than once)\nmlflow.set_experiment(\"Lyrics Corrector\")\n\n# Start our run and log our model\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-4o-mini\",\n        task=openai.completions,\n        artifact_path=\"model\",\n        prompt=lyrics_prompt,\n        signature=ModelSignature(\n            inputs=Schema([ColSpec(type=\"string\", name=None)]),\n            outputs=Schema([ColSpec(type=\"string\", name=None)]),\n            params=ParamSchema(\n                [\n                    ParamSpec(name=\"max_tokens\", default=16, dtype=\"long\"),\n                    ParamSpec(name=\"temperature\", default=0, dtype=\"float\"),\n                    ParamSpec(name=\"best_of\", default=1, dtype=\"long\"),\n                ]\n            ),\n        ),\n    )\n\n# Load the model as a generic python function that can be used for completions\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Logging a LangChain RetrievalQA Chain in MLflow\nDESCRIPTION: Complete example of creating and logging a RetrievalQA chain with MLflow. This demonstrates document loading, text splitting, embedding generation, vector database creation, and finally logging the retrieval chain to MLflow for later use.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/guide/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\nimport mlflow\n\nassert (\n    \"OPENAI_API_KEY\" in os.environ\n), \"Please set the OPENAI_API_KEY environment variable.\"\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    persist_dir = os.path.join(temp_dir, \"faiss_index\")\n\n    # Create the vector db, persist the db to a local fs folder\n    loader = TextLoader(\"tests/langchain/state_of_the_union.txt\")\n    documents = loader.load()\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    docs = text_splitter.split_documents(documents)\n    embeddings = OpenAIEmbeddings()\n    db = FAISS.from_documents(docs, embeddings)\n    db.save_local(persist_dir)\n\n    # Create the RetrievalQA chain\n    retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=db.as_retriever())\n\n    # Log the retrievalQA chain\n    def load_retriever(persist_directory):\n        embeddings = OpenAIEmbeddings()\n        vectorstore = FAISS.load_local(persist_directory, embeddings)\n        return vectorstore.as_retriever()\n\n    with mlflow.start_run() as run:\n        logged_model = mlflow.langchain.log_model(\n            retrievalQA,\n            artifact_path=\"retrieval_qa\",\n            loader_fn=load_retriever,\n            persist_dir=persist_dir,\n        )\n\n# Load the retrievalQA chain\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\nprint(\n    loaded_model.predict(\n        [{\"query\": \"What did the president say about Ketanji Brown Jackson\"}]\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Basic PythonModel Implementation\nDESCRIPTION: Simple example of implementing a custom PythonModel by subclassing mlflow.pyfunc.PythonModel to process string lists.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, model_input: list[str], params=None) -> list[str]:\n        return model_input\n```\n\n----------------------------------------\n\nTITLE: Setting Up Utility Functions and Caching for LLM Operations\nDESCRIPTION: Implements a caching system to save costs when using LLMs. The code defines a Cache class to store LLM responses, along with utility functions for OpenAI chat completions and document embeddings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\n\n# For cost-saving, create a cache for the LLM responses\nimport threading\n\n# For data analysis and visualization\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport openai\nimport pandas as pd\n\n# For scraping\nimport requests\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n\nclass Cache:\n    def __init__(self, persist_path, cache_loading_fn):\n        \"\"\"\n        The cache_loading_fn should be a function that takes arbitrary\n        serializable arguments and returns a serilaizable value.\n          value = cache_loading_fn(**kwargs)\n        For example, for openai.chat.completions.create(...), the\n        cache_loading_fn should be:\n          def cache_loading_fn(**kwargs):\n            result = openai.chat.completions.create(**kwargs)\n            return result.to_dict_recursive()\n        \"\"\"\n        self._cache = self._get_or_create_cache_dict(persist_path)\n        self._persist_path = persist_path\n        self._cache_loading_fn = cache_loading_fn\n        self._cache_lock = threading.Lock()\n\n    @classmethod\n    def _get_or_create_cache_dict(cls, persist_path):\n        if os.path.exists(persist_path):\n            # File exists, load it as a JSON string into a dict\n            with open(persist_path) as f:\n                cache = json.load(f)\n        else:\n            # File does not exist, create an empty dict\n            cache = {}\n        return cache\n\n    def _save_to_file(self):\n        with open(self._persist_path, \"w\") as file:\n            json.dump(self._cache, file)\n\n    def _update_cache(self, key, value):\n        with self._cache_lock:\n            self._cache[key] = value\n            self._save_to_file()\n\n    def get_from_cache_or_load_cache(self, **kwargs):\n        key = json.dumps(kwargs)\n\n        with self._cache_lock:\n            value = self._cache.get(key, None)\n\n        if value is None:\n            value = self._cache_loading_fn(**kwargs)\n            self._update_cache(key, value)\n        else:\n            print(\"Loaded from cache\")\n\n        return value\n\n\ndef chat_completion_create_fn(**kwargs):\n    result = openai.chat.completions.create(**kwargs)\n    return result.to_dict_recursive()\n\n\ndef cached_openai_ChatCompletion_create(**kwargs):\n    cache = kwargs.pop(\"cache\")\n    return cache.get_from_cache_or_load_cache(**kwargs)\n\n\ndef embeddings_embed_documents_fn(**kwargs):\n    chunk = kwargs.get(\"chunk\")\n    return embeddings.embed_documents([chunk])\n\n\ndef cached_langchain_openai_embeddings(**kwargs):\n    cache = kwargs.pop(\"cache\")\n    return cache.get_from_cache_or_load_cache(**kwargs)\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI\nDESCRIPTION: Sets the MLflow Tracking URI to point to the running Tracking Server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_TRACKING_URI=http://127.0.0.1:5000  # Replace with remote host name or IP address in an actual environment\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for MLflow pmdarima Module\nDESCRIPTION: A Sphinx documentation configuration that uses the automodule directive to generate documentation for the MLflow pmdarima module, including all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.pmdarima.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.pmdarima\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Searching MLflow Traces by Session ID\nDESCRIPTION: This snippet shows how to use the mlflow.search_traces function to retrieve traces with a specific session ID tag. It demonstrates programmatic access to traces grouped by session.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/session.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntraces = mlflow.search_traces(filter_string=\"tag.session_id = '123456'\")\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Tags to MLflow Runs using MlflowClient in Python\nDESCRIPTION: Shows how to add custom tags to MLflow runs using the MlflowClient.set_tag() function. This allows for better organization and filtering of runs in the MLflow Tracking UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclient.set_tag(run.info.run_id, \"tag_key\", \"tag_value\")\n```\n\n----------------------------------------\n\nTITLE: Scraping MLflow Documentation from the Official Website\nDESCRIPTION: Scrapes documentation from the MLflow website by fetching HTML content, parsing it with BeautifulSoup, and extracting text from the documentation pages. The scraped text is stored in a pandas DataFrame with source information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npage = requests.get(\"https://mlflow.org/docs/latest/index.html\")\nsoup = BeautifulSoup(page.content, \"html.parser\")\n\nmainLocation = \"https://mlflow.org/docs/latest/\"\nheader = {\n    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n    \"Accept-Language\": \"en-US,en;q=0.8\",\n    \"Connection\": \"keep-alive\",\n}\n\ndata = []\nfor a_link in soup.find_all(\"a\"):\n    document_url = mainLocation + a_link[\"href\"]\n    page = requests.get(document_url, headers=header)\n    soup = BeautifulSoup(page.content, \"html.parser\")\n    file_to_store = a_link.get(\"href\")\n    if soup.find(\"div\", {\"class\": \"rst-content\"}):\n        data.append(\n            [\n                file_to_store,\n                soup.find(\"div\", {\"class\": \"rst-content\"}).text.replace(\"\\n\", \" \"),\n            ]\n        )\n\ndf = pd.DataFrame(data, columns=[\"source\", \"text\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoModule Documentation for mlflow.diviner\nDESCRIPTION: Sphinx configuration directive that enables automatic documentation generation for the mlflow.diviner module. It includes all members, undocumented members, and shows inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.diviner.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.diviner\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Launching the MLflow UI to View Results\nDESCRIPTION: Command to start the MLflow UI server on port 8080, allowing you to view the automatically logged metrics, parameters, and artifacts in a web browser.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/autolog/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui --port 8080\n```\n\n----------------------------------------\n\nTITLE: Registering a Pre-existing Scikit-learn Model with MLflow\nDESCRIPTION: This code demonstrates how to load a pre-existing scikit-learn model from a pickle file, create a model signature, and register it with the MLflow Model Registry using the sklearn flavor.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\nimport numpy as np\nfrom sklearn import datasets\n\nloaded_model = pickle.load(open(filename, \"rb\"))\n\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\ndiabetes_X = diabetes_X[:, np.newaxis, 2]\nsignature = infer_signature(diabetes_X, diabetes_y)\n\nmlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\nreg_model_name = \"SklearnLinearRegression\"\nmlflow.sklearn.log_model(\n    loaded_model,\n    \"sk_learn\",\n    serialization_format=\"cloudpickle\",\n    signature=signature,\n    registered_model_name=reg_model_name,\n)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Database Test Services and Executing Tests\nDESCRIPTION: This snippet shows how to run MLflow database test services using Docker Compose. It includes commands for running a single service, all services, executing specific tests, and running Python scripts within the Docker environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/db/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Run a service (`pytest tests/db` is executed by default)\n./tests/db/compose.sh run --rm $service\n\n# Run all services\nfor service in $(./tests/db/compose.sh config --services | grep '^mlflow-')\ndo\n  ./tests/db/compose.sh run --rm \"$service\"\ndone\n\n# Run tests\n./tests/db/compose.sh run --rm $service pytest /path/to/directory/or/script\n\n# Run a python script\n./tests/db/compose.sh run --rm $service python /path/to/script\n```\n\n----------------------------------------\n\nTITLE: Displaying DialoGPT Follow-up Response\nDESCRIPTION: Prints the chatbot's response to the follow-up question, showing how the model maintains context between interactions when loaded with MLflow's pyfunc implementation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/conversational-model.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Response: {second}\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Prediction Errors with Scatter Plot in Python\nDESCRIPTION: This function creates a scatter plot to visualize prediction errors, plotting predicted values against the difference between actual and predicted values. It includes a reference line at zero to easily identify over-predictions and under-predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef plot_prediction_error(y_test, y_pred, style=\"seaborn\", plot_size=(10, 8)):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n        ax.scatter(y_pred, y_test - y_pred)\n        ax.axhline(y=0, color=\"red\", linestyle=\"--\")\n        ax.set_title(\"Prediction Error Plot\", fontsize=14)\n        ax.set_xlabel(\"Predicted Values\", fontsize=12)\n        ax.set_ylabel(\"Errors\", fontsize=12)\n        plt.tight_layout()\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoGen Multi-Agent Workflow with MLflow Tracing (Python)\nDESCRIPTION: This code snippet shows a complete example of implementing a multi-agent workflow using AutoGen with MLflow tracing enabled. It includes setting up the environment, defining agents, registering tools, and initiating a chat between agents.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/autogen.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import Annotated, Literal\n\nfrom autogen import ConversableAgent\n\nimport mlflow\n\n# Turn on auto tracing for AutoGen\nmlflow.autogen.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"AutoGen\")\n\n\n# Define a simple multi-agent workflow using AutoGen\nconfig_list = [\n    {\n        \"model\": \"gpt-4o-mini\",\n        # Please set your OpenAI API Key to the OPENAI_API_KEY env var before running this example\n        \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n    }\n]\n\nOperator = Literal[\"+\", \"-\", \"*\", \"/\"]\n\n\ndef calculator(a: int, b: int, operator: Annotated[Operator, \"operator\"]) -> int:\n    if operator == \"+\":\n        return a + b\n    elif operator == \"-\":\n        return a - b\n    elif operator == \"*\":\n        return a * b\n    elif operator == \"/\":\n        return int(a / b)\n    else:\n        raise ValueError(\"Invalid operator\")\n\n\n# First define the assistant agent that suggests tool calls.\nassistant = ConversableAgent(\n    name=\"Assistant\",\n    system_message=\"You are a helpful AI assistant. \"\n    \"You can help with simple calculations. \"\n    \"Return 'TERMINATE' when the task is done.\",\n    llm_config={\"config_list\": config_list},\n)\n\n# The user proxy agent is used for interacting with the assistant agent\n# and executes tool calls.\nuser_proxy = ConversableAgent(\n    name=\"Tool Agent\",\n    llm_config=False,\n    is_termination_msg=lambda msg: msg.get(\"content\") is not None\n    and \"TERMINATE\" in msg[\"content\"],\n    human_input_mode=\"NEVER\",\n)\n\n# Register the tool signature with the assistant agent.\nassistant.register_for_llm(name=\"calculator\", description=\"A simple calculator\")(\n    calculator\n)\nuser_proxy.register_for_execution(name=\"calculator\")(calculator)\nresponse = user_proxy.initiate_chat(\n    assistant, message=\"What is (44231 + 13312 / (230 - 20)) * 4?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Training Environment with PyTorch and MLflow\nDESCRIPTION: Sets up the training hyperparameters, model, loss function, metric, and optimizer for a PyTorch image classification task. These components are essential for the training process and will be logged to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nepochs = 3\nloss_fn = nn.CrossEntropyLoss()\nmetric_fn = Accuracy(task=\"multiclass\", num_classes=10).to(device)\nmodel = ImageClassifier().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n```\n\n----------------------------------------\n\nTITLE: Updating Model Signature with Expected Data Structure\nDESCRIPTION: Updates a model's signature to match an expected data structure. This process involves defining the structure, generating a signature using infer_signature, and applying it with set_signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nexpected_data_structure = [{\"a\": \"string\", \"b\": \"another string\"}, {\"a\": \"string\"}]\n\nsignature = infer_signature(expected_data_structure, loaded_model.predict(expected_data_structure))\n\nset_signature(model_info.model_uri, signature)\n```\n\n----------------------------------------\n\nTITLE: Data Preparation and Dataset Creation\nDESCRIPTION: Defines functions to read and process the Reuters 21578 dataset, and creates a custom Dataset class for DSPy.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nfrom dspy.datasets.dataset import Dataset\n\n\ndef read_data_and_subset_to_categories() -> tuple[pd.DataFrame]:\n    \"\"\"Read the reuters-21578 dataset.\"\"\"\n    file_path = \"hf://datasets/yangwang825/reuters-21578/{}.json\"\n    train = pd.read_json(file_path.format(\"train\"))\n    test = pd.read_json(file_path.format(\"test\"))\n\n    label_map = {0: \"acq\", 1: \"crude\", 2: \"earn\", 3: \"grain\", 4: \"interest\", 5: \"money-fx\", 6: \"ship\", 7: \"trade\"}\n\n    train[\"label\"] = train[\"label\"].map(label_map)\n    test[\"label\"] = test[\"label\"].map(label_map)\n\n    return train, test\n\n\nclass CSVDataset(Dataset):\n    def __init__(self, n_train_per_label: int = 20, n_test_per_label: int = 10, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.n_train_per_label = n_train_per_label\n        self.n_test_per_label = n_test_per_label\n        self._create_train_test_split_and_ensure_labels()\n\n    def _create_train_test_split_and_ensure_labels(self) -> None:\n        train_df, test_df = read_data_and_subset_to_categories()\n        train_samples_df = pd.concat([group.sample(n=self.n_train_per_label) for _, group in train_df.groupby(\"label\")])\n        test_samples_df = pd.concat([group.sample(n=self.n_test_per_label) for _, group in test_df.groupby(\"label\")])\n        self._train = train_samples_df.to_dict(orient=\"records\")\n        self._dev = test_samples_df.to_dict(orient=\"records\")\n\n\ndataset = CSVDataset(n_train_per_label=3, n_test_per_label=1)\ntrain_dataset = [example.with_inputs(\"text\") for example in dataset.train]\ntest_dataset = [example.with_inputs(\"text\") for example in dataset.dev]\nunique_train_labels = {example.label for example in dataset.train}\n\nprint(len(train_dataset), len(test_dataset))\nprint(f\"Train labels: {unique_train_labels}\")\nprint(train_dataset[0])\n```\n\n----------------------------------------\n\nTITLE: Removing Module from Cache in MLflow PyFunc Models\nDESCRIPTION: Demonstrates a workaround for module caching issues by removing the module from sys.modules cache before loading another model that uses the same module name.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel1 = mlflow.pyfunc.load_model(info1.model_uri)\nsys.modules.pop(\"my_model\")\nmodel2 = mlflow.pyfunc.load_model(info2.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Running ML Code with MLflow Auto Logging\nDESCRIPTION: Command to execute your ML script that has MLflow auto logging enabled. This will run your training code and automatically log results to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/autolog/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython YOUR_ML_CODE.py\n```\n\n----------------------------------------\n\nTITLE: Enabling LightGBM Autologging in MLflow\nDESCRIPTION: Code snippet showing how to enable automatic logging of LightGBM models in MLflow. This single function call enables autologging for both native LightGBM models and scikit-learn wrapper models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/lightgbm/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmlflow.lightgbm.autolog()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RAG Evaluation with MLflow\nDESCRIPTION: Imports necessary libraries including Pandas, Langchain components, and MLflow for creating and evaluating a RAG system.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain_openai import OpenAI, OpenAIEmbeddings\n\nimport mlflow\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Project for BERT News Classification\nDESCRIPTION: This command runs the MLflow project with default parameters. It executes the bert_classification.py script with predefined settings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/BertNewsClassification/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run .\n```\n\n----------------------------------------\n\nTITLE: Defining LangGraph Chatbot Using Registered Prompt\nDESCRIPTION: Python script (chatbot.py) that defines a LangGraph chatbot using a prompt from MLflow Prompt Registry. The script loads a registered prompt, creates a simple state graph for a chatbot, and exposes it as a model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/run-and-model.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# %%writefile chatbot.py\n\nimport mlflow\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\n\nclass State(TypedDict):\n    messages: list\n\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\nsystem_prompt = mlflow.load_prompt(\"prompts:/chat-prompt/1\")\n\n\ndef add_system_message(state: State):\n    return {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt.to_single_brace_format(),\n            },\n            *state[\"messages\"],\n        ]\n    }\n\n\ndef chatbot(state: State):\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"add_system_message\", add_system_message)\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_edge(START, \"add_system_message\")\ngraph_builder.add_edge(\"add_system_message\", \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\n\ngraph = graph_builder.compile()\n\nmlflow.models.set_model(graph)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using the Registered Model for Inference\nDESCRIPTION: Demonstrates how to load a model from the Model Registry using its URI and version, then use it for inference. This shows the end-to-end workflow from model creation to utilization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Load the model from the model registry and score\nmodel_uri = f\"models:/{reg_model_name}/1\"\nloaded_model = mlflow.pyfunc.load_model(model_uri)\nscore_model(loaded_model)\n```\n\n----------------------------------------\n\nTITLE: Fetching a Specific Model Version from MLflow Model Registry\nDESCRIPTION: Demonstrates how to load a specific model version from the Model Registry using the model name and version number. The loaded model can be used for predictions or inference workloads.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow.pyfunc\n\nmodel_name = \"sk-learn-random-forest-reg-model\"\nmodel_version = 1\n\nmodel = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n\nmodel.predict(data)\n```\n\n----------------------------------------\n\nTITLE: Setting Signatures on Existing Models\nDESCRIPTION: Shows how to log a model without a signature and then add or update the signature using MLflow's set_signature API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\nimport mlflow\n\nX, y = datasets.load_iris(return_X_y=True, as_frame=True)\nclf = RandomForestClassifier(max_depth=7, random_state=0)\nwith mlflow.start_run() as run:\n    clf.fit(X, y)\n    mlflow.sklearn.log_model(clf, \"iris_rf\")\n```\n\n----------------------------------------\n\nTITLE: Querying RetrievalQA Model about Leasing Land for Spa and Hotel\nDESCRIPTION: This code snippet queries the RetrievalQA model about leasing land for a buffalo-themed day spa and hotel in Yellowstone. It tests the model's ability to discern nuanced information from multiple acts and provide a comprehensive response.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nanswer5 = loaded_model.predict(\n    [\n        {\n            \"query\": \"Can I lease a small parcel of land from the Federal Government for a small \"\n            \"buffalo-themed day spa and hotel for visitors to stay in and relax at while visiting the park?\"\n        }\n    ]\n)\nprint_formatted_response(answer5)\n```\n\n----------------------------------------\n\nTITLE: Querying Traces with Different Return Types in Python\nDESCRIPTION: Shows how to use the mlflow.search_traces() API with different return types, including Pandas DataFrame and list of Trace objects.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/search.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Default: Return as Pandas DataFrame\ntrace_df = mlflow.search_traces(experiment_ids=[morning_experiment.experiment_id])\n\n# Return as list of Trace objects\ntrace_list = mlflow.search_traces(\n    experiment_ids=[morning_experiment.experiment_id], return_type=\"list\"\n)\n```\n\n----------------------------------------\n\nTITLE: Tracing DSPy Evaluation with MLflow\nDESCRIPTION: An example showing how to enable tracing during DSPy evaluation, create a simple evaluation set, define a program, and log evaluation metrics to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/dspy.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nfrom dspy.evaluate.metrics import answer_exact_match\n\nimport mlflow\n\n# Enabling tracing for DSPy evaluation\nmlflow.dspy.autolog(log_traces_from_eval=True)\n\n# Define a simple evaluation set\neval_set = [\n    dspy.Example(\n        question=\"How many 'r's are in the word 'strawberry'?\", answer=\"3\"\n    ).with_inputs(\"question\"),\n    dspy.Example(\n        question=\"How many 'a's are in the word 'banana'?\", answer=\"3\"\n    ).with_inputs(\"question\"),\n    dspy.Example(\n        question=\"How many 'e's are in the word 'elephant'?\", answer=\"2\"\n    ).with_inputs(\"question\"),\n]\n\n\n# Define a program\nclass Counter(dspy.Signature):\n    question: str = dspy.InputField()\n    answer: str = dspy.OutputField(\n        desc=\"Should only contain a single number as an answer\"\n    )\n\n\ncot = dspy.ChainOfThought(Counter)\n\n# Evaluate the programs\nwith mlflow.start_run(run_name=\"CoT Evaluation\"):\n    evaluator = dspy.evaluate.Evaluate(\n        devset=eval_set,\n        return_all_scores=True,\n        return_outputs=True,\n        show_progress=True,\n    )\n    aggregated_score, outputs, all_scores = evaluator(cot, metric=answer_exact_match)\n\n    # Log the aggregated score\n    mlflow.log_metric(\"exact_match\", aggregated_score)\n    # Log the detailed evaluation results as a table\n    mlflow.log_table(\n        {\n            \"question\": [example.question for example in eval_set],\n            \"answer\": [example.answer for example in eval_set],\n            \"output\": outputs,\n            \"exact_match\": all_scores,\n        },\n        artifact_file=\"eval_results.json\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Testing MLflow Model for Inference in Python (Commented)\nDESCRIPTION: This code snippet shows how to use the loaded model for inference. It's designed to predict based on a prompt about machine learning. The code is commented out to avoid runtime issues on CPU, but can be uncommented for use on systems with powerful GPUs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# loaded_model.predict(pd.DataFrame(\n#     {\"prompt\": [\"What is machine learning?\"]}), params={\"temperature\": 0.6}\n# )\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Whisper Model from MLflow in Python\nDESCRIPTION: This code snippet shows how to load the Whisper model from MLflow and use it for audio transcription. It demonstrates loading the model in its native format and performing inference on an audio file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Load the pipeline in its native format\nloaded_transcriber = mlflow.transformers.load_model(model_uri=model_info.model_uri)\n\n# Perform transcription with the native pipeline implementation\ntranscription = loaded_transcriber(audio)\n\nprint(f\"\\nWhisper native output transcription:\\n{format_transcription(transcription['text'])}\")\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Autologging with XGBoost\nDESCRIPTION: Call mlflow.xgboost.autolog() before training to automatically log XGBoost training metrics, parameters, and artifacts. Captures model parameters, user-specified metrics, and creates MLflow Model artifacts including feature importance and input examples. Supports early stopping with best iteration metrics logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/autolog/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmlflow.xgboost.autolog()\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow AI Gateway Endpoint\nDESCRIPTION: This YAML configuration snippet adds the TGI server as a new endpoint in the MLflow AI Gateway configuration file, specifying the endpoint type and server URL.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/huggingface/README.md#2025-04-07_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: \"huggingface-text-generation-inference\"\n      name: llm\n      config:\n        hf_server_url: http://127.0.0.1:8000/generate\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Project with Custom Epochs\nDESCRIPTION: This command demonstrates how to run the MLflow project with a custom number of epochs. The -P flag is used to pass parameters to the project.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/BertNewsClassification/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P max_epochs=X\n```\n\n----------------------------------------\n\nTITLE: Setting Model Signature for Tensorflow Model in MLflow\nDESCRIPTION: Demonstrates how to manually set a model signature when logging a Tensorflow model to MLflow. It uses mlflow.types.TensorSpec to specify the input schema and mlflow.models.ModelSignature to create the signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/guide/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow import keras\nfrom mlflow.types import Schema, TensorSpec\nfrom mlflow.models import ModelSignature\n\nmodel = keras.Sequential(\n    [\n        keras.Input([28, 28, 3]),\n        keras.layers.Conv2D(8, 2),\n        keras.layers.MaxPool2D(2),\n        keras.layers.Flatten(),\n        keras.layers.Dense(2),\n        keras.layers.Softmax(),\n    ]\n)\n\ninput_schema = Schema(\n    [\n        TensorSpec(np.dtype(np.float32), (-1, 28, 28, 3), \"input\"),\n    ]\n)\nsignature = ModelSignature(inputs=input_schema)\n\nwith mlflow.start_run() as run:\n    mlflow.tensorflow.log_model(model, \"model\", signature=signature)\n```\n\n----------------------------------------\n\nTITLE: Managing Model Aliases in MLflow Model Registry\nDESCRIPTION: Demonstrates how to create, update, retrieve and delete model aliases using the MLflow Client API. Aliases help organize and deploy models by providing named references to specific model versions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\n\n# create \"champion\" alias for version 1 of model \"example-model\"\nclient.set_registered_model_alias(\"example-model\", \"champion\", 1)\n\n# reassign the \"Champion\" alias to version 2\nclient.set_registered_model_alias(\"example-model\", \"Champion\", 2)\n\n# get a model version by alias\nclient.get_model_version_by_alias(\"example-model\", \"Champion\")\n\n# delete the alias\nclient.delete_registered_model_alias(\"example-model\", \"Champion\")\n```\n\n----------------------------------------\n\nTITLE: Inferring Signature for Complex Nested Dictionary in Python\nDESCRIPTION: Demonstrates signature inference for a complex nested dictionary structure, showcasing MLflow's ability to handle multi-level data hierarchies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Complex list of Dictionaries\nreport_signature_info([{\"a\": \"b\", \"b\": [1, 2, 3], \"c\": {\"d\": [4, 5, 6]}}])\n```\n\n----------------------------------------\n\nTITLE: Basic Example of DeepSeek Tracing with MLflow\nDESCRIPTION: This example demonstrates how to set up MLflow tracing for a basic DeepSeek chat completion call using the OpenAI SDK.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/deepseek.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport mlflow\n\n# Enable auto-tracing for OpenAI (works with DeepSeek)\nmlflow.openai.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"DeepSeek\")\n\n# Initialize the OpenAI client with DeepSeek API endpoint\nclient = openai.OpenAI(\n    base_url=\"https://api.deepseek.com\", api_key=\"<your_deepseek_api_key>\"\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n]\n\nresponse = client.chat.completions.create(\n    model=\"deepseek-chat\",\n    messages=messages,\n    temperature=0.1,\n    max_tokens=100,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Evaluating Professional QA Model\nDESCRIPTION: Implements an enhanced model with extreme formality requirements and evaluates it using the custom professionalism metric.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/qa-evaluation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    system_prompt = \"Answer the following question using extreme formality.\"\n    professional_qa_model = mlflow.openai.log_model(\n        model=\"gpt-4o-mini\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": \"{question}\"},\n        ],\n    )\n    results = mlflow.evaluate(\n        professional_qa_model.model_uri,\n        eval_df,\n        model_type=\"question-answering\",\n        evaluators=\"default\",\n        extra_metrics=[professionalism_metric],\n    )\nprint(results.metrics)\n```\n\n----------------------------------------\n\nTITLE: Logging a Model from a File Path\nDESCRIPTION: Demonstrates how to log a model from a Python file path using the MLflow pyfunc module. This example shows the workflow for the Models from Code feature, where the model is defined in a separate file and then logged.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmodel_path = \"my_model.py\"\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        python_model=model_path,  # Define the model as the path to the Python file\n        artifact_path=\"my_model\",\n    )\n\n# Loading the model behaves exactly as if an instance of MyModel had been logged\nmy_model = mlflow.pyfunc.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Setting up Core Dependencies and Environment\nDESCRIPTION: Python imports and environment setup for MLflow, LangChain, and OpenAI integration. Includes assertions to verify necessary environment variables.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport shutil\nimport tempfile\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain_openai import OpenAI, OpenAIEmbeddings\n\nimport mlflow\n\nassert \"OPENAI_API_KEY\" in os.environ, \"Please set the OPENAI_API_KEY environment variable.\"\n```\n\n----------------------------------------\n\nTITLE: Combining MLflow Auto-Tracing with Manual Tracing\nDESCRIPTION: Illustrates how to use the @mlflow.trace decorator in conjunction with auto-tracing, specifically combining OpenAI auto-tracing with manually defined spans in a single integrated trace.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport openai\n\nmlflow.openai.autolog()\n\n@mlflow.trace(span_type=SpanType.CHAIN)\ndef run(question):\n    messages = build_messages()\n    # MLflow automatically generates a span for OpenAI invocation\n    response = openai.OpenAI().chat.completions.create(\n        model=\"gpt-4o-mini\",\n        max_tokens=100,\n        messages=messages,\n    )\n    return parse_response(response)\n\n@mlflow.trace\ndef build_messages(question):\n    return [\n        {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n        {\"role\": \"user\", \"content\": question},\n    ]\n\n@mlflow.trace\ndef parse_response(response):\n    return response.choices[0].message.content\n\nrun(\"What is MLflow?\")\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow UI\nDESCRIPTION: This command starts the MLflow UI, allowing you to view the run's metrics, parameters, and details in a web interface at http://localhost:5000.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/BertNewsClassification/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Checking MLflow Server Version in Python\nDESCRIPTION: This code snippet shows how to query the MLflow server version and compare it with the client-side version to ensure compatibility.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport mlflow\n\nresponse = requests.get(\"http://<mlflow-host>:<mlflow-port>/version\")\nassert response.text == mlflow.__version__  # Checking for a strict version match\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Chunks and Queries in Python\nDESCRIPTION: This snippet creates embeddings for chunks and queries using a cached OpenAI embedding model. It applies the embedding function to each chunk and question in the dataset.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nembedded_queries = all_result_df.copy()\nembedded_queries[\"chunk_emb\"] = all_result_df[\"chunk\"].apply(\n    lambda x: np.squeeze(cached_langchain_openai_embeddings(chunk=x, cache=embeddings_cache))\n)\nembedded_queries[\"question_emb\"] = all_result_df[\"question\"].apply(\n    lambda x: np.squeeze(cached_langchain_openai_embeddings(chunk=x, cache=embeddings_cache))\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key for LlamaIndex\nDESCRIPTION: Sets the OpenAI API key as an environment variable for use with LlamaIndex, which uses OpenAI APIs for LLMs and embeddings models by default.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n```\n\n----------------------------------------\n\nTITLE: Listing and Searching MLflow Models in Python\nDESCRIPTION: This snippet demonstrates how to list all registered models and search for specific model versions using the MLflow Client API. It uses the search_registered_models() and search_model_versions() methods.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\n\nclient = MlflowClient()\nfor rm in client.search_registered_models():\n    pprint(dict(rm), indent=4)\n\nclient = MlflowClient()\nfor mv in client.search_model_versions(\"name='sk-learn-random-forest-reg-model'\"):\n    pprint(dict(mv), indent=4)\n```\n\n----------------------------------------\n\nTITLE: Defining a Dependent Function for MLflow Model From Code in Python\nDESCRIPTION: Creates a separate Python file containing a simple addition function to be used as a dependency for a more complex MLflow model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/models-from-code/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# If running in a Jupyter or Databricks notebook cell, uncomment the following line:\n# %%writefile \"./calculator.py\"\n\n\ndef add(x, y):\n    return x + y\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for MLflow RAG Evaluation\nDESCRIPTION: Installs the specific versions of langchain, langchain-community, langchain-openai, and openai packages required for this notebook to ensure compatibility.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain==0.1.16 langchain-community==0.0.33 langchain-openai==0.0.8 openai==1.12.0\n```\n\n----------------------------------------\n\nTITLE: Defining MetricValue Object Structure in Python\nDESCRIPTION: Example structure of a MetricValue object showing accuracy score implementation with aggregate results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"accuracy_score\": MetricValue(\n        scores=None, justifications=None, aggregate_results={\"accuracy_score\": 1.0}\n    )\n}\n```\n\n----------------------------------------\n\nTITLE: Running MLflow MNIST Example with Local Environment\nDESCRIPTION: Command to run the MNIST example using MLflow without creating a new conda environment. This assumes all required dependencies are already installed locally.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/MNIST/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . --env-manager=local\n```\n\n----------------------------------------\n\nTITLE: Serving an MLflow Model from Model Registry\nDESCRIPTION: Shell script to serve a model from the Model Registry as a service. Uses the mlflow models serve command to deploy a model referenced by its alias.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n#!/usr/bin/env sh\n\n# Set environment variable for the tracking URL where the Model Registry resides\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n\n# Serve the production model from the model registry\nmlflow models serve -m \"models:/sk-learn-random-forest-reg-model@champion\"\n```\n\n----------------------------------------\n\nTITLE: Creating MLflow Experiment with Tags in Python\nDESCRIPTION: Demonstrates how to create a new MLflow experiment for apple produce forecasting models with descriptive tags. The code shows setting up experiment description, defining searchable tags for project organization, and creating the experiment using MLflow client.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step3-create-experiment/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Provide an Experiment description that will appear in the UI\nexperiment_description = (\n    \"This is the grocery forecasting project. \"\n    \"This experiment contains the produce models for apples.\"\n)\n\n# Provide searchable tags that define characteristics of the Runs that\n# will be in this Experiment\nexperiment_tags = {\n    \"project_name\": \"grocery-forecasting\",\n    \"store_dept\": \"produce\",\n    \"team\": \"stores-ml\",\n    \"project_quarter\": \"Q3-2023\",\n    \"mlflow.note.content\": experiment_description,\n}\n\n# Create the Experiment, providing a unique name\nproduce_apples_experiment = client.create_experiment(\n    name=\"Apple_Models\", tags=experiment_tags\n)\n```\n\n----------------------------------------\n\nTITLE: MLflow Filter Expression Example - SQL-like Syntax\nDESCRIPTION: Example of SQL-like filter syntax for searching MLflow runs based on metrics, parameters and tags. Shows how to combine conditions and handle special characters in column names.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nmetrics.rmse < 1 and params.model_class = 'LogisticRegression'\n```\n\nLANGUAGE: sql\nCODE:\n```\nmetrics.\"model class\" = 'LinearRegression' and tags.\"user-name\" = 'Tomas'\n```\n\n----------------------------------------\n\nTITLE: Tracing Textractor Pipeline with MLflow\nDESCRIPTION: Example of tracing a txtai Textractor pipeline using MLflow autologging. It sets up MLflow tracking and runs a simple Textractor operation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/txtai.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom txtai.pipeline import Textractor\n\n# Enable MLflow auto-tracing for txtai\nmlflow.txtai.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"txtai\")\n\n# Define and run a simple Textractor pipeline.\ntextractor = Textractor()\ntextractor(\"https://github.com/neuml/txtai\")\n```\n\n----------------------------------------\n\nTITLE: Manually Tracing a Function with Tags in Python\nDESCRIPTION: Demonstrates how to use the @mlflow.trace decorator on a function and how to add tags to the active trace using mlflow.update_current_trace() during function execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/how-to.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@mlflow.trace\ndef my_func(x):\n    mlflow.update_current_trace(tags={\"fruit\": \"apple\"})\n    return x + 1\n```\n\n----------------------------------------\n\nTITLE: Querying RetrievalQA Model about Buying Yellowstone\nDESCRIPTION: This code snippet demonstrates querying the RetrievalQA model with a humorous question about buying Yellowstone National Park. It showcases the model's ability to provide factual responses to even outlandish queries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nanswer3 = loaded_model.predict(\n    [\n        {\n            \"query\": \"Can I buy Yellowstone from the Federal Government to set up a buffalo-themed day spa?\"\n        }\n    ]\n)\n\nprint_formatted_response(answer3)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow AI Gateway via pip\nDESCRIPTION: Command to install MLflow AI Gateway package with genai extras from PyPI\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install 'mlflow[genai]'\n```\n\n----------------------------------------\n\nTITLE: Authenticating to MLflow REST API with Basic Auth\nDESCRIPTION: Example of how to make authenticated requests to the MLflow REST API using the requests library in Python. Uses HTTP Basic Authentication with username and password.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresponse = requests.get(\n    \"https://<mlflow_tracking_uri>/\",\n    auth=(\"username\", \"password\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Retrieved Document IDs to Evaluation Dataset\nDESCRIPTION: Applies the retriever function to each question in the dataset and adds the retrieved document IDs as a new column.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndata[\"retrieved_doc_ids\"] = data[\"question\"].apply(retrieve_doc_ids)\ndata.head(3)\n```\n\n----------------------------------------\n\nTITLE: Compiling Keras Model\nDESCRIPTION: Configures the model compilation with loss function, optimizer, and metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(0.001),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n```\n\n----------------------------------------\n\nTITLE: Searching All MLflow Experiments in Python\nDESCRIPTION: Example of searching across all experiments for specific model runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.entities import ViewType\n\nmodel_of_interest = \"GPT-4\"\ngpt_4_runs_global = mlflow.search_runs(\n    filter_string=f\"params.model = '{model_of_interest}'\",\n    run_view_type=ViewType.ALL,\n    search_all_experiments=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI\nDESCRIPTION: Command to start the MLflow user interface for viewing run metrics, parameters, and details. After execution, the UI can be accessed at http://localhost:5000.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/MNIST/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Creating Test Data for Question-Answering Evaluation\nDESCRIPTION: Creates a pandas DataFrame containing test questions and their corresponding ground truth answers for model evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/qa-evaluation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\neval_df = pd.DataFrame(\n    {\n        \"inputs\": [\n            \"How does useEffect() work?\",\n            \"What does the static keyword in a function mean?\",\n            \"What does the 'finally' block in Python do?\",\n            \"What is the difference between multiprocessing and multithreading?\",\n        ],\n        \"ground_truth\": [\n            \"The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we'll refer to it as our \\\"effect\\\"), and call it later after performing the DOM updates.\",\n            \"Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.\",\n            \"'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.\",\n            \"Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating or Retrieving MLflow Experiment in Python\nDESCRIPTION: This function checks if an MLflow experiment with a given name exists. If it does, it returns the experiment ID. If not, it creates a new experiment and returns its ID. This helps in organizing and isolating different tasks or projects within MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_or_create_experiment(experiment_name):\n    \"\"\"\n    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n\n    This function checks if an experiment with the given name exists within MLflow.\n    If it does, the function returns its ID. If not, it creates a new experiment\n    with the provided name and returns its ID.\n\n    Parameters:\n    - experiment_name (str): Name of the MLflow experiment.\n\n    Returns:\n    - str: ID of the existing or newly created MLflow experiment.\n    \"\"\"\n\n    if experiment := mlflow.get_experiment_by_name(experiment_name):\n        return experiment.experiment_id\n    else:\n        return mlflow.create_experiment(experiment_name)\n```\n\n----------------------------------------\n\nTITLE: Defining Overridable Attributes in recipe.yaml\nDESCRIPTION: This YAML snippet shows how to define an overridable attribute 'RMSE_THRESHOLD' in the recipe.yaml configuration file. The attribute is used for validating model performance with a default value of 10.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsteps:\n  evaluate:\n    validation_criteria:\n      - metric: root_mean_squared_error\n        # The maximum RMSE value on the test dataset that a model can have\n        # to be eligible for production deployment\n        threshold: {{ RMSE_THRESHOLD|default(10) }}\n```\n\n----------------------------------------\n\nTITLE: Model Validation\nDESCRIPTION: Examples of validating a PythonModel using both mlflow.models.predict API and local model loading.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.models.predict(\n    model_uri=model_info.model_uri,\n    input_data=[\"a\", \"b\", \"c\"],\n    env_manager=\"uv\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\npyfunc_model.predict([\"hello\", \"world\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Topic Analysis for Retrieval Evaluation\nDESCRIPTION: This function performs topic modeling on a list of questions using NLTK and Gensim. It tokenizes the text, removes stopwords, creates a dictionary and corpus, applies LDA modeling to identify topics, and returns the model components for visualization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\nimport pyLDAvis.gensim_models as gensimvis\nfrom gensim import corpora, models\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Initialize NLTK resources\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n\n\ndef topical_analysis(questions: list[str]):\n    stop_words = set(stopwords.words(\"english\"))\n\n    # Tokenize and remove stop words\n    tokenized_data = []\n    for question in questions:\n        tokens = word_tokenize(question.lower())\n        filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n        tokenized_data.append(filtered_tokens)\n\n    # Create a dictionary and corpus\n    dictionary = corpora.Dictionary(tokenized_data)\n    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n\n    # Apply LDA model\n    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n\n    # Get topic distribution for each question\n    topic_distribution = []\n    for i, ques in enumerate(questions):\n        bow = dictionary.doc2bow(tokenized_data[i])\n        topics = lda_model.get_document_topics(bow)\n        topic_distribution.append(topics)\n        print(f\"Question: {ques}\\nTopic: {topics}\")\n\n    # Print all topics\n    print(\"\\nTopics found are:\")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n    return lda_model, corpus, dictionary\n```\n\n----------------------------------------\n\nTITLE: Loading Saved MLflow Model using pyfunc in Python\nDESCRIPTION: This snippet demonstrates how to load a previously saved MLflow model using the pyfunc module. It uses the model_uri from the model_info object to locate and load the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Retrieving First Document Chunk for Inspection\nDESCRIPTION: Displays the first chunk of the processed documents to verify the content and formatting before proceeding with question generation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df[\"chunk\"][0]\n```\n\n----------------------------------------\n\nTITLE: Training PyFunc Model\nDESCRIPTION: Command to run the training script that creates and logs the MLflow PyFunc model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pyfunc/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python train.py\n```\n\n----------------------------------------\n\nTITLE: Logging Model with Column-based Example in Python\nDESCRIPTION: This code demonstrates how to log a scikit-learn model with a column-based input example using a Pandas DataFrame.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ninput_example = pd.DataFrame(\n    [\n        {\n            \"sepal length (cm)\": 5.1,\n            \"sepal width (cm)\": 3.5,\n            \"petal length (cm)\": 1.4,\n            \"petal width (cm)\": 0.2,\n        }\n    ]\n)\nmlflow.sklearn.log_model(..., input_example=input_example)\n```\n\n----------------------------------------\n\nTITLE: Get Registered Model Permission API Endpoint\nDESCRIPTION: REST endpoint for retrieving permissions on registered models. Requires model name and username as input parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/auth/rest-api.rst#2025-04-07_snippet_1\n\nLANGUAGE: rest\nCODE:\n```\n2.0/mlflow/registered-models/permissions/get\n```\n\n----------------------------------------\n\nTITLE: Executing Additional Hyperparameter Tuning with Custom Parameters in MLflow\nDESCRIPTION: This code demonstrates running additional hyperparameter tuning experiments with custom parameter choices and a unique identifier for tagging. This facilitates parameter sensitivity analysis and efficient filtering of runs in the MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Execute additional hyperparameter tuning runs with custom parameter choices\nparam_1_values = [\"x\", \"y\", \"z\"]\nparam_2_values = [\"u\", \"v\", \"w\"]\nident = \"params_test_2\"\nconsume(starmap(execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))))\n```\n\n----------------------------------------\n\nTITLE: Advanced MLflow Tracing with OpenAI Streaming\nDESCRIPTION: Demonstrates an advanced example of using MLflow tracing with OpenAI streaming, including a custom output reducer to consolidate ChatCompletionChunk outputs into a single message object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport openai\nfrom openai.types.chat import *\nfrom typing import Optional\n\ndef aggregate_chunks(outputs: list[ChatCompletionChunk]) -> Optional[ChatCompletion]:\n    \"\"\"Consolidate ChatCompletionChunks to a single ChatCompletion\"\"\"\n    if not outputs:\n        return None\n\n    first_chunk = outputs[0]\n    delta = first_chunk.choices[0].delta\n    message = ChatCompletionMessage(\n        role=delta.role, content=delta.content, tool_calls=delta.tool_calls or []\n    )\n    finish_reason = first_chunk.choices[0].finish_reason\n    for chunk in outputs[1:]:\n        delta = chunk.choices[0].delta\n        message.content += delta.content or \"\"\n        message.tool_calls += delta.tool_calls or []\n        finish_reason = finish_reason or chunk.choices[0].finish_reason\n\n    base = ChatCompletion(\n        id=first_chunk.id,\n        choices=[Choice(index=0, message=message, finish_reason=finish_reason)],\n        created=first_chunk.created,\n        model=first_chunk.model,\n        object=\"chat.completion\",\n    )\n    return base\n\n@mlflow.trace(output_reducer=aggregate_chunks)\ndef predict(messages: list[dict]):\n    stream = openai.OpenAI().chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=messages,\n        stream=True,\n    )\n    for chunk in stream:\n        yield chunk\n\nfor chunk in predict([{\"role\": \"user\", \"content\": \"Hello\"}]):\n    print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Setting Databricks Tracking URI\nDESCRIPTION: Code to configure MLflow to use Databricks as the tracking server\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/databricks-trial/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_tracking_uri(\"databricks\")\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Model with Signature as Spark UDF in Python\nDESCRIPTION: Shows how to use an MLflow model with a signature as a Spark UDF without specifying column name arguments. The UDF uses column names from the model signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_76\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, \"<path-to-model-with-signature>\")\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf())\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages (Bash)\nDESCRIPTION: Command to install the necessary Python packages for running the example, including MLflow, OpenAI, and Databricks SDK.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/uc_integration/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow>=2.14.0 openai databricks-sdk\n```\n\n----------------------------------------\n\nTITLE: Loading LlamaIndex Model from MLflow in Python\nDESCRIPTION: This snippet shows how to load a previously logged LlamaIndex model from MLflow using the mlflow.llama_index.load_model() API. It then runs the loaded model with a sample input.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nloaded_model = mlflow.llama_index.load_model(\"runs:/f8e0a0d2dd5546d5ac93ce126358c444/model\")\nawait loaded_model.run(input=\"What is (123 + 456) * 789?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Caching Objects for API Calls\nDESCRIPTION: Creates cache objects for OpenAI chat completions and document embeddings to reduce API costs by storing and reusing previous responses.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncache = Cache(CACHE_PATH, chat_completion_create_fn)\nembeddings_cache = Cache(EMBEDDINGS_CACHE_PATH, embeddings_embed_documents_fn)\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost Training Script\nDESCRIPTION: Command to execute the XGBoost training script that demonstrates MLflow integration with XGBoost's Scikit-learn API using the diabetes dataset.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/xgboost/xgboost_sklearn/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Creating Feature Correlation Matrix and Saving to Disk in Python\nDESCRIPTION: A function that calculates feature correlations in a DataFrame, generates a heatmap visualization with masked upper triangle, and saves it to disk. This demonstrates the alternative logging mechanism using MLflow's log_artifact() API instead of direct figure logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef plot_correlation_matrix_and_save(\n    df, style=\"seaborn\", plot_size=(10, 8), path=\"/tmp/corr_plot.png\"\n):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n\n        # Calculate the correlation matrix\n        corr = df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(\n            corr,\n            mask=mask,\n            cmap=\"coolwarm\",\n            vmax=0.3,\n            center=0,\n            square=True,\n            linewidths=0.5,\n            annot=True,\n            fmt=\".2f\",\n        )\n\n        ax.set_title(\"Feature Correlation Matrix\", fontsize=14)\n        plt.tight_layout()\n\n    plt.close(fig)\n    # convert to filesystem path spec for os compatibility\n    save_path = pathlib.Path(path)\n    fig.savefig(save_path)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom PythonModel Class for MLflow in Python\nDESCRIPTION: Creates a simple custom PythonModel class named 'MyModel' for use with MLflow. This model simply returns the input data as its prediction, serving as a base for signature enforcement examples.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        return model_input\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Data for LlamaIndex Demo\nDESCRIPTION: Shell commands to create a data directory and download a sample essay for creating a test index.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/llama_index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n!mkdir -p data\n!curl -L https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -o ./data/paul_graham_essay.txt\n```\n\n----------------------------------------\n\nTITLE: Disabling LlamaIndex Tracing\nDESCRIPTION: Demonstrates how to disable LlamaIndex tracing in MLflow\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmlflow.llama_index.autolog(disable=True)\n```\n\n----------------------------------------\n\nTITLE: Deploying RAPIDS MLflow Model as a Local REST API Endpoint\nDESCRIPTION: This shell command serves the trained RAPIDS model as a local REST API endpoint using MLflow. It specifies the model version, port, and environment manager for deployment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/README.md#2025-04-07_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmlflow models serve --env-manager=local -m models:/rapids_mlflow_cli/[VERSION] -p 55755\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Unity Catalog Integration\nDESCRIPTION: Installation of necessary Python packages using pip to enable Unity Catalog and MLflow integration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/uc_functions/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow openai databricks-sdk\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Evaluates the trained model on the test dataset and prints the loss and accuracy.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nscore = model.evaluate(test_ds)\n\nprint(f\"Test loss: {score[0]:.4f}\")\nprint(f\"Test accuracy: {score[1]: .2f}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Apple Sales Data in Python\nDESCRIPTION: This snippet generates synthetic apple sales data using a custom function. It sets base demand, number of rows, and competitor price effect as parameters to create a dataset for further analysis.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmy_data = generate_apple_sales_data_with_promo_adjustment(\n    base_demand=1000, n_rows=10_000, competitor_price_effect=-25.0\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing Logged SHAP Explanations via MLflow UI\nDESCRIPTION: Instructions for launching the MLflow UI and navigating to the logged model explanations, which are stored in the model_explanations_shap artifacts folder.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/shap/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Inferring Signature for List of Integers in Python\nDESCRIPTION: Demonstrates how MLflow infers the signature for a list of integers, highlighting the conversion of int to long for broader compatibility and data integrity.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# List of integers\nreport_signature_info([1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Training Logistic Regression Model\nDESCRIPTION: Load Iris dataset and train a basic logistic regression model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\niris = load_iris()\nx = iris.data[:, 2:]\ny = iris.target\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9001)\n\nmodel = LogisticRegression(random_state=0, max_iter=5_000, solver=\"newton-cg\").fit(x_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Creating MLflow Model Version in R\nDESCRIPTION: Function to create a new model version in MLflow, specifying source, run ID, tags, and other metadata.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_7\n\nLANGUAGE: r\nCODE:\n```\nmlflow_create_model_version(\n  name,\n  source,\n  run_id = NULL,\n  tags = NULL,\n  run_link = NULL,\n  description = NULL,\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Using SQL Server as MLflow Artifact Store\nDESCRIPTION: Example showing how to configure and use SQL Server as an artifact store in MLflow using database URI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndb_uri = \"mssql+pyodbc://username:password@host:port/database?driver=ODBC+Driver+17+for+SQL+Server\"\n\nclient.create_experiment(exp_name, artifact_location=db_uri)\nmlflow.set_experiment(exp_name)\n\nmlflow.onnx.log_model(onnx, \"model\")\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow Server with Artifacts Service\nDESCRIPTION: This command starts an MLflow server with the artifacts service enabled. It specifies the backend store, artifacts destination, default artifact root, and logging options.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/mlflow_artifacts/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmlflow server \\\n    --backend-store-uri=mlruns \\\n    --artifacts-destination ./mlartifacts \\\n    --default-artifact-root http://localhost:5000/api/2.0/mlflow-artifacts/artifacts/experiments \\\n    --gunicorn-opts \"--log-level debug\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Metric Derived from Built-in Metrics in Python\nDESCRIPTION: Creates a custom metric that reuses existing built-in metrics by dividing the 'sum_on_target' metric by 2. This shows how to leverage pre-calculated metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndef sum_on_target_divided_by_two(_eval_df, builtin_metrics):\n    \"\"\"\n    This example custom metric function creates a metric derived from existing metrics in\n    ``builtin_metrics``.\n    \"\"\"\n    return builtin_metrics[\"sum_on_target\"] / 2\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI Configuration\nDESCRIPTION: Configures OpenAI API settings for LLM and embeddings models in LlamaIndex.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\n\n# LlamaIndex by default uses OpenAI APIs for LLMs and embeddings models. You can use the default\n# model (`gpt-3.5-turbo` and `text-embeddings-ada-002` as of Oct 2024), but we recommend using the\n# latest efficient models instead for getting better results with lower cost.\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\nSettings.llm = OpenAI(model=\"gpt-4o-mini\")\n```\n\n----------------------------------------\n\nTITLE: Tracing RAG Pipeline with MLflow\nDESCRIPTION: Example of tracing a Retrieval Augmented Generation (RAG) pipeline using MLflow autologging. It sets up a Wikipedia-based embedding and defines a RAG pipeline with a specific prompt template.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/txtai.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom txtai import Embeddings, RAG\n\n# Enable MLflow auto-tracing for txtai\nmlflow.txtai.autolog()\n\n\nwiki = Embeddings()\nwiki.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia-slim\")\n\n# Define prompt template\ntemplate = \"\"\"\nAnswer the following question using only the context below. Only include information\nspecifically discussed.\n\nquestion: {question}\ncontext: {context} \"\"\"\n\n# Create RAG pipeline\nrag = RAG(\n    wiki,\n    \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\",\n    system=\"You are a friendly assistant. You answer questions from users.\",\n    template=template,\n    context=10,\n)\n\nrag(\"Tell me about the Roman Empire\", maxlength=2048)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Visualization Artifact for Model Evaluation in Python\nDESCRIPTION: Generates a scatter plot artifact that visualizes the relationship between predictions and targets. The function saves the plot as an image in the artifacts directory and returns its path information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndef prediction_target_scatter(eval_df, _builtin_metrics, artifacts_dir):\n    \"\"\"\n    This example custom artifact generates and saves a scatter plot to ``artifacts_dir`` that\n    visualizes the relationship between the predictions and targets for the given model to a\n    file as an image artifact.\n    \"\"\"\n    plt.scatter(eval_df[\"prediction\"], eval_df[\"target\"])\n    plt.xlabel(\"Targets\")\n    plt.ylabel(\"Predictions\")\n    plt.title(\"Targets vs. Predictions\")\n    plot_path = os.path.join(artifacts_dir, \"example_scatter_plot.png\")\n    plt.savefig(plot_path)\n    return {\"example_scatter_plot_artifact\": plot_path}\n```\n\n----------------------------------------\n\nTITLE: Verifying TGI Server Functionality\nDESCRIPTION: This Python script sends a request to the TGI server to generate text based on a given input, demonstrating how to interact with the server and verify its functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/huggingface/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nheaders = {\n    \"Content-Type\": \"application/json\",\n}\ndata = {\n    'inputs': 'What is Deep Learning?',\n    'parameters': {\n        'max_new_tokens': 20,\n    },\n}\nresponse = requests.post('http://127.0.0.1:8000/generate', headers=headers, json=data)\nprint(response.json())\n# {'generated_text': '\\nDeep learning is a branch of machine learning that uses artificial neural networks to learn and make decisions.'}\n```\n\n----------------------------------------\n\nTITLE: Defining LangGraph Model for Agent\nDESCRIPTION: Creates a LangGraph model using ChatOpenAI and the defined tools, setting up the workflow for the agent's decision-making process.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langgraph.graph import END, START, MessagesState, StateGraph\nfrom langgraph.prebuilt import ToolNode\n\ntool_node = ToolNode(tools)\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"tools\", \"agent\")\n\napp = workflow.compile()\n```\n\n----------------------------------------\n\nTITLE: Deleting Logged MLflow Artifacts\nDESCRIPTION: This command uses the MLflow garbage collection (gc) feature to delete logged artifacts for specific run IDs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/mlflow_artifacts/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow gc --backend-store-uri=mlruns --run-ids <run_id>\n```\n\n----------------------------------------\n\nTITLE: Default Experiment Output\nDESCRIPTION: Shows the formatted output of the default experiment's metadata as a dictionary.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step2-mlflow-client/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n{'name': 'Default', 'lifecycle_stage': 'active'}\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto-Tracing for OpenAI with MLflow\nDESCRIPTION: Code snippet showing how to enable automatic tracing for OpenAI calls using MLflow's openai.autolog() function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/openai.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.openai.autolog()\n```\n\n----------------------------------------\n\nTITLE: Federal Document Scraping Function\nDESCRIPTION: Function to scrape and extract transcript content from federal document webpages using BeautifulSoup. Takes URL and div class parameters to locate and extract specific content sections.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fetch_federal_document(url, div_class):  # noqa: D417\n    \"\"\"\n    Scrapes the transcript of the Act Establishing Yellowstone National Park from the given URL.\n\n    Args:\n    url (str): URL of the webpage to scrape.\n\n    Returns:\n    str: The transcript text of the Act.\n    \"\"\"\n    # Sending a request to the URL\n    response = requests.get(url)\n    if response.status_code == 200:\n        # Parsing the HTML content of the page\n        soup = BeautifulSoup(response.text, \"html.parser\")\n\n        # Finding the transcript section by its HTML structure\n        transcript_section = soup.find(\"div\", class_=div_class)\n        if transcript_section:\n            transcript_text = transcript_section.get_text(separator=\"\\n\", strip=True)\n            return transcript_text\n        else:\n            return \"Transcript section not found.\"\n    else:\n        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n```\n\n----------------------------------------\n\nTITLE: Environment Variable for SQLAlchemy Connection Recycling in MLflow\nDESCRIPTION: The `MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE` environment variable allows configuration of SQLAlchemy connection recycling to manage database connections efficiently.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_17\n\nLANGUAGE: Shell\nCODE:\n```\nMLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE\n```\n\n----------------------------------------\n\nTITLE: Visualizing Retrieval Metrics with Matplotlib\nDESCRIPTION: This snippet demonstrates how to visualize precision, recall, and NDCG metrics at different k values using matplotlib. It extracts metrics from the evaluation results and plots them for comparison, showing how retrieval performance changes as more results are considered.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\n# Plotting each metric\nfor metric_name in [\"precision\", \"recall\", \"ndcg\"]:\n    y = [evaluate_results.metrics[f\"{metric_name}_at_{k}/mean\"] for k in range(1, 4)]\n    plt.plot([1, 2, 3], y, label=f\"{metric_name}@k\")\n\n# Adding labels and title\nplt.xlabel(\"k\")\nplt.ylabel(\"Metric Value\")\nplt.title(\"Metrics Comparison at Different Ks\")\n# Setting x-axis ticks\nplt.xticks([1, 2, 3])\nplt.legend()\n\n# Display the plot\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Authentication Provider for MLflow\nDESCRIPTION: Python code example showing how to implement a custom RequestAuthProvider class for client-side authentication in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.tracking.request_auth.abstract_request_auth_provider import (\n    RequestAuthProvider,\n)\n\n\nclass DummyAuthProvider(RequestAuthProvider):\n    def get_name(self):\n        return \"dummy_auth_provider_name\"\n\n    def get_auth(self):\n        return DummyAuth()\n```\n\n----------------------------------------\n\nTITLE: Ignoring False Positives in Python Code\nDESCRIPTION: Examples of how to ignore false positives in Python code using inline comments. This can be done for a single line or a block of code.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/typos.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Ignore a line containing a typo:\n\n\"<false_positive>\"  # spellchecker: disable-line\n\n# Ignore a block containing typos:\n\n# spellchecker: off\n\"<false_positive>\"\n\"<another_false_positive>\"\n# spellchecker: on\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Remote Artifact Storage with MLflow\nDESCRIPTION: This snippet demonstrates how to set up MLflow to log artifacts directly to remote storage (like S3) without requiring a tracking server as proxy. It creates an experiment with an explicit artifact location in S3.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nexperiment_name = \"your_experiment_name\"\nmlflow.create_experiment(experiment_name, artifact_location=\"s3://your-bucket\")\nmlflow.set_experiment(experiment_name)\n```\n\n----------------------------------------\n\nTITLE: Creating Historical Pyfunc Model in MLflow\nDESCRIPTION: Python script that creates a simple MLflow Pyfunc model and saves it with a specific MLflow version for backwards compatibility testing. The model implements a basic predict function that returns the input unchanged.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/resources/pyfunc_models/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return model_input\n\n\nmodel = MyModel()\n\nmlflow.pyfunc.save_model(\n    python_model=model,\n    path=f\"tests/resources/pyfunc_models/{mlflow.__version__}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Logging a Metric in MLflow using R\nDESCRIPTION: This function logs a metric for a run in MLflow. It allows logging a single float measure with optional timestamp and step. The MLflow Backend tracks historical metric values by timestamp and step.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_29\n\nLANGUAGE: r\nCODE:\n```\nmlflow_log_metric(\n  key,\n  value,\n  timestamp = NULL,\n  step = NULL,\n  run_id = NULL,\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a DSPy Chain of Thought Module\nDESCRIPTION: Shows how to create a DSPy Chain of Thought module and configure the language model settings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\n\n# Define our language model\nlm = dspy.LM(model=\"openai/gpt-4o-mini\", max_tokens=250)\ndspy.settings.configure(lm=lm)\n\n\n# Define a Chain of Thought module\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought(\"question -> answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\ndspy_model = CoT()\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom LLM Provider in MLflow AI Gateway YAML\nDESCRIPTION: Demonstrates how to specify the custom plugin provider in the MLflow AI Gateway configuration file. This includes setting the provider name and any required configuration parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_23\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: my_llm\n      name: my-model-0.1.2\n      config:\n        my_llm_api_key: $MY_LLM_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Building and Running MLflow Development Version\nDESCRIPTION: These commands build the MLflow services using the development version and run the example script in the client container.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/mlflow_artifacts/README.md#2025-04-07_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n# Build services using the dev version of mlflow\n$ ./build.sh\n$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto Tracing for OpenAI Agents SDK in Python\nDESCRIPTION: Basic code snippet to enable automatic tracing for OpenAI Agents SDK by calling the mlflow.openai.autolog() function. This allows MLflow to capture traces and log them to the active MLflow Experiment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/openai-agent.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.openai.autolog()\n```\n\n----------------------------------------\n\nTITLE: Instantiating ReActAgent in Python\nDESCRIPTION: This snippet shows how to create an instance of the ReActAgent with a specified timeout.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nagent = ReActAgent(timeout=180)\n```\n\n----------------------------------------\n\nTITLE: Setting Up MLflow Experiment and Data Preprocessing for XGBoost in Python\nDESCRIPTION: This code snippet initializes an MLflow experiment and prepares the dataset for XGBoost model training. It sets the active experiment, splits the data into features and target, performs train-test split, and converts the data into XGBoost's DMatrix format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Set the current active MLflow experiment\nmlflow.set_experiment(experiment_id=experiment_id)\n\n# Preprocess the dataset\nX = df.drop(columns=[\"date\", \"demand\"])\ny = df[\"demand\"]\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\ndtrain = xgb.DMatrix(train_x, label=train_y)\ndvalid = xgb.DMatrix(valid_x, label=valid_y)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Autologging Options in Python\nDESCRIPTION: An example showing how to customize the autologging configuration by disabling trace logging and enabling model logging instead. This demonstrates the flexibility of MLflow's OpenAI autologging feature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/autologging/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.openai.autolog(\n    log_traces=False,\n    log_models=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Using BigMLFlow to Log a Model\nDESCRIPTION: Example of using BigMLFlow to log a BigML model in MLflow format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport mlflow\nimport bigmlflow\n\nMODEL_FILE = \"logistic_regression.json\"\nwith mlflow.start_run():\n    with open(MODEL_FILE) as handler:\n        model = json.load(handler)\n        bigmlflow.log_model(\n            model, artifact_path=\"model\", registered_model_name=\"my_model\"\n        )\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Declaration\nDESCRIPTION: ReStructuredText directive for generating a local table of contents with depth of 4 levels\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CODE_OF_CONDUCT.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. contents:: **Table of Contents**\n  :local:\n  :depth: 4\n```\n\n----------------------------------------\n\nTITLE: Routing Retrieval Steps in Python Workflow\nDESCRIPTION: This snippet shows how the workflow routes retrieval steps based on the specified retrievers. It triggers vector search, BM25, or web search events as configured.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    # If no retriever is specified, proceed directly to the final query step with an empty context\n    if len(self.retrievers) == 0:\n        return QueryEvent(context=\"\")\n\n    # Trigger the retrieval steps based on the configuration\n    if self._use_vs_retriever:\n        ctx.send_event(VectorSearchRetrieveEvent(query=query))\n    if self._use_bm25_retriever:\n        ctx.send_event(BM25RetrieveEvent(query=query))\n    if self._use_web_search:\n        ctx.send_event(TransformQueryEvent(query=query))\n```\n\n----------------------------------------\n\nTITLE: Running Question Answering Example as MLflow Project\nDESCRIPTION: Command to execute the question answering example as an MLflow project. The script uses MLflow.openai flavor and requires MLflow 2.4.0+, OpenAI client, tiktoken, tenacity, and OpenAI API key.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cd question_answering && mlflow run .\n```\n\n----------------------------------------\n\nTITLE: Auto Logging with Scikit-learn Random Forest Regressor\nDESCRIPTION: Complete example showing MLflow autologging with a scikit-learn random forest model. This demonstrates how metrics, parameters, and model artifacts are automatically captured without explicit logging statements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/autolog/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\nmlflow.autolog()\n\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Integration\nDESCRIPTION: Setup code for using Azure OpenAI with LangChain, including environment configuration and client initialization for both the LLM and embeddings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings\n\n# Set this to `azure`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n# The API version you want to use: set this to `2023-05-15` for the released version.\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\nassert \"AZURE_OPENAI_ENDPOINT\" in os.environ, (\n    \"Please set the AZURE_OPENAI_ENDPOINT environment variable. It is the base URL for your Azure OpenAI resource. You can find this in the Azure portal under your Azure OpenAI resource.\"\n)\nassert \"OPENAI_API_KEY\" in os.environ, (\n    \"Please set the OPENAI_API_KEY environment variable. It is the API key for your Azure OpenAI resource. You can find this in the Azure portal under your Azure OpenAI resource.\"\n)\n\nazure_openai_llm = AzureOpenAI(\n    deployment_name=\"<your-deployment-name>\",\n    model_name=\"gpt-4o-mini\",\n)\nazure_openai_embeddings = AzureOpenAIEmbeddings(\n    azure_deployment=\"<your-deployment-name>\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using XetHub as MLflow Artifact Store\nDESCRIPTION: Example showing how to configure and use XetHub as an artifact store with a custom Python model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport mlflow.pyfunc\n\n\nclass Mod(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        return 7\n\n\nexp_name = \"myexp\"\nmlflow.create_experiment(\n    exp_name, artifact_location=\"xet://<your_username>/mlflow-test/main\"\n)\nmlflow.set_experiment(exp_name)\nmlflow.pyfunc.log_model(\"model_test\", python_model=Mod())\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Static Dataset with Pre-computed Predictions in MLflow\nDESCRIPTION: Shows how to evaluate a static dataset with pre-computed model predictions without re-running the model. This approach is useful when model outputs are already saved in a DataFrame column.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nimport shap\nimport xgboost\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n\n# Load the UCI Adult Dataset\nX, y = shap.datasets.adult()\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# Fit an XGBoost binary classifier on the training data split\nmodel = xgboost.XGBClassifier().fit(X_train, y_train)\n\n# Build the Evaluation Dataset from the test set\ny_test_pred = model.predict(X=X_test)\neval_data = X_test\neval_data[\"label\"] = y_test\neval_data[\"predictions\"] = y_test_pred\n\n\nwith mlflow.start_run() as run:\n    # Evaluate the static dataset without providing a model\n    result = mlflow.evaluate(\n        data=eval_data,\n        targets=\"label\",\n        predictions=\"predictions\",\n        model_type=\"classifier\",\n    )\n\nprint(f\"metrics:\\n{result.metrics}\")\nprint(f\"artifacts:\\n{result.artifacts}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Tracking Client\nDESCRIPTION: Initializes MlflowClient with a specific tracking URI to connect to the MLflow tracking server running on localhost port 8080.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step2-mlflow-client/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient = MlflowClient(tracking_uri=\"http://127.0.0.1:8080\")\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Pyfunc Flavor for Whisper Model Inference in Python\nDESCRIPTION: This snippet illustrates how to use MLflow's pyfunc flavor to load and use the Whisper model for inference. It showcases the flexibility of pyfunc for deploying models across different environments and frameworks.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Load the saved transcription pipeline as a generic python function\npyfunc_transcriber = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# Ensure that the pyfunc wrapper is capable of transcribing passed-in audio\npyfunc_transcription = pyfunc_transcriber.predict([audio])\n\n# Note: the pyfunc return type if `return_timestamps` is set is a JSON encoded string.\nprint(f\"\\nPyfunc output transcription:\\n{format_transcription(pyfunc_transcription[0])}\")\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow via pip\nDESCRIPTION: Command to install the MLflow package using pip, which is required before using any MLflow functionality including auto logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/autolog/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Evaluating Chunking Strategy with MLflow in Python\nDESCRIPTION: Defines a function to evaluate different text chunking strategies using MLflow. It tests various chunk sizes to assess their impact on retrieval accuracy and system performance, using a SentenceTransformer embedding model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_chunk_size(chunk_size):\n    list_of_documents = loader.load()\n    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n    docs = text_splitter.split_documents(list_of_documents)\n    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\n\n    def retrieve_doc_ids(question: str) -> list[str]:\n        docs = retriever.get_relevant_documents(question)\n        return [doc.metadata[\"source\"] for doc in docs]\n\n    def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\n        return question_df[\"question\"].apply(retrieve_doc_ids)\n\n    with mlflow.start_run():\n        return mlflow.evaluate(\n            model=retriever_model_function,\n            data=eval_data,\n            model_type=\"retriever\",\n            targets=\"source\",\n            evaluators=\"default\",\n        )\n\n\nresult1 = evaluate_chunk_size(1000)\nresult2 = evaluate_chunk_size(2000)\n\n\ndisplay(result1.tables[\"eval_results_table\"])\ndisplay(result2.tables[\"eval_results_table\"])\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Training Script with Different Hyperparameters\nDESCRIPTION: This command shows an alternative execution of the training script with different hyperparameter values, including learning rate, colsample-bytree, and subsample. It illustrates how to experiment with various parameter combinations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/lightgbm/lightgbm_native/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --learning-rate 0.4 --colsample-bytree 0.7 --subsample 0.8\n```\n\n----------------------------------------\n\nTITLE: Executing Hyperparameter Tuning without Child Runs in MLflow\nDESCRIPTION: This snippet demonstrates how to run multiple hyperparameter tuning experiments in MLflow without using child runs. It includes functions for logging parameters and metrics, generating run names, and executing multiple tuning iterations sequentially.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom functools import partial\nfrom itertools import starmap\n\nfrom more_itertools import consume\n\nimport mlflow\n\n\n# Define a function to log parameters and metrics\ndef log_run(run_name, test_no):\n    with mlflow.start_run(run_name=run_name):\n        mlflow.log_param(\"param1\", random.choice([\"a\", \"b\", \"c\"]))\n        mlflow.log_param(\"param2\", random.choice([\"d\", \"e\", \"f\"]))\n        mlflow.log_metric(\"metric1\", random.uniform(0, 1))\n        mlflow.log_metric(\"metric2\", abs(random.gauss(5, 2.5)))\n\n\n# Generate run names\ndef generate_run_names(test_no, num_runs=5):\n    return (f\"run_{i}_test_{test_no}\" for i in range(num_runs))\n\n\n# Execute tuning function\ndef execute_tuning(test_no):\n    # Partial application of the log_run function\n    log_current_run = partial(log_run, test_no=test_no)\n    # Generate run names and apply log_current_run function to each run name\n    runs = starmap(log_current_run, ((run_name,) for run_name in generate_run_names(test_no)))\n    # Consume the iterator to execute the runs\n    consume(runs)\n\n\n# Set the tracking uri and experiment\nmlflow.set_tracking_uri(\"http://localhost:8080\")\nmlflow.set_experiment(\"No Child Runs\")\n\n# Execute 5 hyperparameter tuning runs\nconsume(starmap(execute_tuning, ((x,) for x in range(5))))\n```\n\n----------------------------------------\n\nTITLE: Setting up MLflow Tracking URI and Experiment\nDESCRIPTION: Configures MLflow tracking to use a local server and creates a new experiment for the demo. The tracking URI must be set to an HTTP tracking server for the Trace UI to be displayed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# replace with your own URI\ntracking_uri = \"http://localhost:5000\"\nmlflow.set_tracking_uri(tracking_uri)\n\n# set a new experiment to avoid\n# cluttering the default experiment\nexperiment = mlflow.set_experiment(\"mlflow-trace-ui-demo\")\n```\n\n----------------------------------------\n\nTITLE: Logging GPT-4 Model with MLflow for Text Analysis\nDESCRIPTION: Creates an MLflow run to log a GPT-4 model configured for chat completions. The model is set up with a specific signature that defines inputs, outputs, and parameters like max_tokens and temperature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-chat-completions.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-4\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=messages,\n        signature=ModelSignature(\n            inputs=Schema([ColSpec(type=\"string\", name=None)]),\n            outputs=Schema([ColSpec(type=\"string\", name=None)]),\n            params=ParamSchema(\n                [\n                    ParamSpec(name=\"max_tokens\", default=16, dtype=\"long\"),\n                    ParamSpec(name=\"temperature\", default=0, dtype=\"float\"),\n                ]\n            ),\n        ),\n    )\n```\n\n----------------------------------------\n\nTITLE: Enabling Gemini Autologging in MLflow\nDESCRIPTION: Basic setup code to enable automatic tracing for Gemini interactions in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/gemini.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.gemini.autolog()\n```\n\n----------------------------------------\n\nTITLE: Direct Component Interaction\nDESCRIPTION: Shows advanced usage by directly interacting with model and tokenizer components for custom translation processing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = translation_components[\"tokenizer\"]\nmodel = translation_components[\"model\"]\n\nquery = \"Translate to French: Liberty, equality, fraternity, or death.\"\n\ninputs = tokenizer.encode(query, return_tensors=\"pt\").to(\"mps\")\noutputs = model.generate(inputs).to(\"mps\")\nresult = tokenizer.decode(outputs[0])\n\nprint(result.replace(\"<pad> \", \"\\n\").replace(\"</s>\", \"\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark MLflow Autologging Path\nDESCRIPTION: Spark configuration setting to specify the custom log model allowlist file path for MLflow autologging\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/synapseml/autologging.md#2025-04-07_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nspark.mlflow.pysparkml.autolog.logModelAllowlistFile /dbfs/FileStore/PATH_TO_YOUR/log_model_allowlist.txt\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow UI for Experiment Tracking\nDESCRIPTION: Command to start the MLflow UI, which allows users to track experiments, view metrics, and compare different runs after training the Statsmodels model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/statsmodels/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: MLflow Autologging for LangGraph\nDESCRIPTION: Example of enabling MLflow autologging for LangGraph, demonstrating trace logging for a simple weather query agent.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nimport mlflow\n\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\n# Enabling tracing for LangGraph (LangChain)\nmlflow.langchain.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"LangGraph\")\n\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\ntools = [get_weather]\ngraph = create_react_agent(llm, tools)\n\n# Invoke the graph\nresult = graph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf?\"}]}\n)\n```\n\n----------------------------------------\n\nTITLE: Tracing Asynchronous OpenAI API Calls with MLflow\nDESCRIPTION: Example demonstrating MLflow tracing support for OpenAI's asynchronous API, introduced in MLflow 2.21.0, which uses the same setup as synchronous calls.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/openai.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\n# Enable trace logging\nmlflow.openai.autolog()\n\nclient = openai.AsyncOpenAI()\n\nresponse = await client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"How fast would a glass of water freeze on Titan?\"}\n    ],\n    # Async streaming is also supported\n    # stream=True\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Cosine Similarity Scores with Matplotlib in Python\nDESCRIPTION: This snippet creates a histogram of the cosine similarity scores using matplotlib. It helps visualize the distribution of relevancy scores across all question-chunk pairs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nscores = embedded_queries[\"cossim\"].to_list()\nplt.hist(scores, bins=5)\n```\n\n----------------------------------------\n\nTITLE: Creating Weekly Dataset for MLflow Model Training in Python\nDESCRIPTION: This function generates a sample dataset with daily observations for model training. It creates a DataFrame with random values, date index, and day of week information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_weekly_dataset(n_dates, n_observations_per_date):\n    rng = pd.date_range(start=\"today\", periods=n_dates, freq=\"D\")\n    df = pd.DataFrame(\n        np.random.randn(n_dates * n_observations_per_date, 4),\n        columns=[\"x1\", \"x2\", \"x3\", \"y\"],\n        index=np.tile(rng, n_observations_per_date),\n    )\n    df[\"dow\"] = df.index.dayofweek\n    return df\n\n\ndf = create_weekly_dataset(n_dates=30, n_observations_per_date=500)\nprint(df.shape)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Using the Apple Sales Data Generator\nDESCRIPTION: Example code showing how to use the data generator function to create a dataset with 1000 rows and a base demand of 1000 units. The code generates the data and displays the last 20 rows of the resulting DataFrame.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step5-synthetic-data/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=1_000)\n\ndata[-20:]\n```\n\n----------------------------------------\n\nTITLE: Using UV Environment Manager for MLflow Model Prediction in Python\nDESCRIPTION: Shows how to use the UV environment manager, an extremely fast alternative to virtualenv, for creating a virtual environment to run MLflow model predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_74\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.models.predict(\n    model_uri=\"runs:/<run_id>/<model_path>\",\n    input_data=\"your_data\",\n    env_manager=\"uv\",\n)\n```\n\n----------------------------------------\n\nTITLE: Sending CSV Input to MLflow Model Server\nDESCRIPTION: Curl command to send a CSV input to the local MLflow model server for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/csv' --data '1,2,3,4'\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for RAPIDS MLflow Integration in Python\nDESCRIPTION: This snippet imports necessary libraries for RAPIDS, MLflow, and data handling. It includes cuML for GPU-accelerated machine learning, MLflow for experiment tracking, and data processing utilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/notebooks/rapids_mlflow.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nfrom cuml.ensemble import RandomForestClassifier\nfrom cuml.metrics.accuracy import accuracy_score\nfrom cuml.preprocessing.model_selection import train_test_split\n\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.models import infer_signature\n```\n\n----------------------------------------\n\nTITLE: Loading and Using a Prompt with OpenAI in Python\nDESCRIPTION: Demonstrates how to load a prompt using mlflow.load_prompt, format it with variables, and use it with the OpenAI API to generate a summary.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport openai\n\ntarget_text = \"\"\"\nMLflow is an open source platform for managing the end-to-end machine learning lifecycle.\nIt tackles four primary functions in the ML lifecycle: Tracking experiments, packaging ML\ncode for reuse, managing and deploying models, and providing a central model registry.\nMLflow currently offers these functions as four components: MLflow Tracking,\nMLflow Projects, MLflow Models, and MLflow Registry.\n\"\"\"\n\n# Load the prompt\nprompt = mlflow.load_prompt(\"prompts:/summarization-prompt/2\")\n\n# Use the prompt with an LLM\nclient = openai.OpenAI()\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt.format(num_sentences=1, sentences=target_text),\n        }\n    ],\n    model=\"gpt-4o-mini\",\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Adding Trace Tags in MLflow\nDESCRIPTION: Demonstrates how to add tags to traces for additional metadata at the trace level using the mlflow.update_current_trace function within a traced function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@mlflow.trace\ndef my_func(x):\n    mlflow.update_current_trace(tags={\"fruit\": \"apple\"})\n    return x + 1\n```\n\n----------------------------------------\n\nTITLE: Basic Gemini Integration with MLflow\nDESCRIPTION: Complete example showing how to set up MLflow tracking with Gemini, including configuration of tracking URI, experiment creation, and generating content using the Gemini API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/gemini.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport google.genai as genai\nimport os\n\n# Turn on auto tracing for Gemini\nmlflow.gemini.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"Gemini\")\n\n\n# Configure the SDK with your API key.\nclient = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n\n# Use the generate_content method to generate responses to your prompts.\nresponse = client.models.generate_content(\n    model=\"gemini-1.5-flash\", contents=\"The opposite of hot is\"\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling or Disabling Auto Logging for Specific Libraries\nDESCRIPTION: Shows two approaches to control which libraries have autologging enabled: either enabling it only for specific libraries (PyTorch) or disabling it for certain libraries while enabling it for others.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/autolog/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Option 1: Enable autologging only for PyTorch\nmlflow.pytorch.autolog()\n\n# Option 2: Disable autologging for scikit-learn, but enable it for other libraries\nmlflow.sklearn.autolog(disable=True)\nmlflow.autolog()\n```\n\n----------------------------------------\n\nTITLE: Accessing Evaluation Results\nDESCRIPTION: Retrieves evaluation metrics and results table from the MLflow evaluation output. Shows how to access aggregate metrics and detailed per-row results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresults.metrics\nresults.tables[\"eval_results_table\"]\n```\n\n----------------------------------------\n\nTITLE: Using Sktime with MLflow\nDESCRIPTION: Example of using Sktime with MLflow to save, load, and make predictions with a time series forecasting model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.arima import AutoARIMA\nfrom sktime.utils import mlflow_sktime\n\nairline = load_airline()\nmodel_path = \"model\"\n\n\nauto_arima_model = AutoARIMA(sp=12, d=0, max_p=2, max_q=2, suppress_warnings=True).fit(\n    airline, fh=[1, 2, 3]\n)\n\nmlflow_sktime.save_model(\n    sktime_model=auto_arima_model,\n    path=model_path,\n)\n\nloaded_model = mlflow_sktime.load_model(\n    model_uri=model_path,\n)\nloaded_pyfunc = mlflow_sktime.pyfunc.load_model(\n    model_uri=model_path,\n)\n\nprint(loaded_model.predict())\nprint(loaded_pyfunc.predict(pd.DataFrame()))\n```\n\n----------------------------------------\n\nTITLE: Enabling Multipart Upload for Proxied Artifact Access in MLflow\nDESCRIPTION: Environment variable setting to enable experimental multipart upload feature for large artifacts in MLflow. This allows direct uploads to the underlying storage without data passing through the Tracking Server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/artifacts-stores/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD=true\n```\n\n----------------------------------------\n\nTITLE: Loading Sentence Transformer in Native Mode for Extended Functionality\nDESCRIPTION: Shows how to load the Sentence Transformer model in its native form using MLflow's specialized loader, which provides access to the full range of model functionalities beyond the standard PyFunc interface.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Load the saved model as a native Sentence Transformers model (unlike above, where we loaded as a generic python function)\nloaded_model_native = mlflow.sentence_transformers.load_model(logged_model.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Querying MLflow AI Gateway with OpenAI Client in Python\nDESCRIPTION: Demonstrates how to use the OpenAI client to query the MLflow AI Gateway server. This example shows how to create a chat completion request.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:7000/v1\")\ncompletion = client.chat.completions.create(\n    model=\"my-chat\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n)\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Retriever Model with MLflow (Python)\nDESCRIPTION: This snippet demonstrates how to evaluate a retriever model using MLflow, specifying the model type and evaluator configuration. It calculates metrics like precision at k for the retriever.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nevaluate_results = mlflow.evaluate(\n    data=data,\n    model_type=\"retriever\",\n    targets=\"ground_truth_context\",\n    predictions=\"retrieved_context\",\n    evaluators=\"default\",\n    evaluator_config={\"retriever_k\": 5}\n  )\n```\n\n----------------------------------------\n\nTITLE: Loading and Using DSPy Model for Inference\nDESCRIPTION: Shows how to load a saved DSPy model as an MLflow PythonModel and use it for prediction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Load the model as an MLflow PythonModel\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Predict with the object\nresponse = model.predict(\"What kind of bear is best?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: MLflow Model YAML Configuration\nDESCRIPTION: YAML configuration file (MLmodel) defining the model flavors and their attributes\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_62\n\nLANGUAGE: yaml\nCODE:\n```\nflavors:\n  python_function:\n    env:\n      conda: conda.yaml\n      virtualenv: python_env.yaml\n    loader_module: flavor\n    model_path: model.pkl\n    python_version: 3.8.15\n  sktime:\n    code: null\n    pickled_model: model.pkl\n    serialization_format: pickle\n    sktime_version: 0.16.0\n```\n\n----------------------------------------\n\nTITLE: Adding Extra Metadata to MLflow LangChain Spans\nDESCRIPTION: Shows how to add custom metadata to spans using RunnableConfig dictionary, both during construction and at runtime.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom langchain_openai import ChatOpenAI\n\n# Enable auto-tracing for LangChain\nmlflow.langchain.autolog()\n\n# Pass metadata to the constructor using `with_config` method\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").with_config({\"metadata\": {\"key1\": \"value1\"}})\n\n# Pass metadata at runtime using the `config` parameter\nmodel.invoke(\"Hi\", config={\"metadata\": {\"key2\": \"value2\"}})\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity Between Embeddings in Python\nDESCRIPTION: This code defines a cosine similarity function and applies it to calculate the similarity between question and chunk embeddings. The results are stored in a new column of the dataframe.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef cossim(x, y):\n    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n\n\nembedded_queries[\"cossim\"] = embedded_queries.apply(\n    lambda row: cossim(row[\"question_emb\"], row[\"chunk_emb\"]), axis=1\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying Model with MLflow - Bash\nDESCRIPTION: Command to deploy a registered model from MLflow to Ray Serve, specifying the target platform and model URI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/ray_serve/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow deployments create -t ray-serve -m models:/RayMLflowIntegration/1 --name iris:v1\n```\n\n----------------------------------------\n\nTITLE: Setting Up Evaluation Metrics\nDESCRIPTION: Defines and implements the accuracy metric computation function for model evaluation during training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define the target optimization metric\nmetric = evaluate.load(\"accuracy\")\n\n\n# Define a function for calculating our defined target optimization metric during training\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Registration Output\nDESCRIPTION: This bash output shows the successful registration of the scikit-learn model in MLflow. It confirms the creation of version 1 of the registered model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/step1-register-model/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nSuccessfully registered model 'sk-learn-random-forest-reg-model'.\nCreated version '1' of model 'sk-learn-random-forest-reg-model'.\n```\n\n----------------------------------------\n\nTITLE: Loading and Using a BigMLFlow Model\nDESCRIPTION: Example of loading a BigMLFlow model and making predictions using the PyFunc flavor.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# saving the model\nsave_model(model, path=model_path)\n# retrieving model\npyfunc_model = pyfunc.load_model(model_path)\npyfunc_predictions = pyfunc_model.predict(dataframe)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoModule for MLflow JohnSnowLabs\nDESCRIPTION: RST directive configuration for automatically generating documentation from the mlflow.johnsnowlabs module. Includes settings to show all members, undocumented members, and inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.johnsnowlabs.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.johnsnowlabs\n===================\n\n.. automodule:: mlflow.johnsnowlabs\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI\nDESCRIPTION: Command to start the MLflow UI for viewing experiment results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/MNIST/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Querying Runs with MlflowClient in Python\nDESCRIPTION: Example of using MlflowClient to programmatically search for the best run based on a validation loss metric. The client API allows you to query and retrieve experiment data without using the UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclient = mlflow.tracking.MlflowClient()\nexperiment_id = \"0\"\nbest_run = client.search_runs(\n    experiment_id, order_by=[\"metrics.val_loss ASC\"], max_results=1\n)[0]\nprint(best_run.info)\n# {'run_id': '...', 'metrics': {'val_loss': 0.123}, ...}\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Sets the OpenAI API key as an environment variable using a secure input method.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI Key:\")\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Neural Network Model\nDESCRIPTION: This snippet defines a simple neural network class using PyTorch. The network consists of a flatten layer followed by a sequence of linear and ReLU layers. It's designed to process 28x28 input images and output 10 classes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/guide/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nmodel = NeuralNetwork()\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Settings for Inference\nDESCRIPTION: Demonstrates how to configure and update LLM settings for inference with loaded models\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom llama_index.core import Settings\nfrom llama_index.llms.openai import OpenAI\n\nSettings.llm = OpenAI(\"gpt-4o-mini\")\n\n# MLflow saves GPT-4o-Mini as the LLM to use for inference\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        index, artifact_path=\"index\", engine_type=\"chat\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Auth Provider in Package Entry Points\nDESCRIPTION: Python setup.py code snippet showing how to register a custom auth provider in the package entry points for MLflow discovery.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsetup(\n    entry_points={\n        \"mlflow.request_auth_provider\": \"dummy-backend=DummyAuthProvider\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Invoking LangGraph Chatbot with MLflow Tracing\nDESCRIPTION: Example of loading a saved LangGraph model from MLflow and invoking it. This demonstrates enabling MLflow tracing for LangChain to view the prompt passed to the LLM during execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/run-and-model.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Enable MLflow tracing for LangChain to view the prompt passed to LLM.\nmlflow.langchain.autolog()\n\n# Load the graph\ngraph = mlflow.langchain.load_model(model_info.model_uri)\n\ngraph.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What is the difference between multi-threading and multi-processing?\",\n            }\n        ]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring PEFT/LoRA for Fine-tuning\nDESCRIPTION: Sets up LoRA configuration for efficient fine-tuning using PEFT library. Configures adapter rank, alpha, dropout and target modules while enabling gradient checkpointing for memory efficiency.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    r=32,\n    lora_alpha=64,\n    lora_dropout=0.1,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n)\n\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow Gateway Modules with reStructuredText\nDESCRIPTION: A reStructuredText file that defines documentation structure for MLflow Gateway modules. It uses automodule directives to include documentation for the main gateway module, base_models, client, and config submodules.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.gateway.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.gateway\n==============\n\n.. automodule:: mlflow.gateway\n    :members:\n    :undoc-members:\n\n.. automodule:: mlflow.gateway.base_models\n    :members: ConfigModel\n\n.. automodule:: mlflow.gateway.client\n    :members:\n    :undoc-members:\n\n.. automodule:: mlflow.gateway.config\n    :members:\n    :undoc-members:\n    :exclude-members: model_computed_fields\n```\n\n----------------------------------------\n\nTITLE: Displaying MLflow Experiment Metadata in Bash\nDESCRIPTION: This snippet shows the output of printing the metadata associated with a created MLflow Experiment. It includes details such as artifact location, creation time, experiment ID, and custom tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step4-experiment-search/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n<Experiment: artifact_location='mlflow-artifacts:/926031323154788454',\n              creation_time=1694018173427,\n              experiment_id='926031323154788454',\n              last_update_time=1694018173427,\n              lifecycle_stage='active',\n              name='Apple_Models',\n              tags={\n                'mlflow.note.content': 'This is the grocery forecasting project. This '\n                        'experiment contains the produce models for apples.',\n                'project_name': 'grocery-forecasting',\n                'project_quarter': 'Q3-2023',\n                'team': 'stores-ml'}\n>\n```\n\n----------------------------------------\n\nTITLE: Displaying MLflow Experiment Metadata in Bash\nDESCRIPTION: This snippet shows the output of printing the metadata associated with a created MLflow Experiment. It includes details such as artifact location, creation time, experiment ID, and custom tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step4-experiment-search/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n<Experiment: artifact_location='mlflow-artifacts:/926031323154788454',\n              creation_time=1694018173427,\n              experiment_id='926031323154788454',\n              last_update_time=1694018173427,\n              lifecycle_stage='active',\n              name='Apple_Models',\n              tags={\n                'mlflow.note.content': 'This is the grocery forecasting project. This '\n                        'experiment contains the produce models for apples.',\n                'project_name': 'grocery-forecasting',\n                'project_quarter': 'Q3-2023',\n                'team': 'stores-ml'}\n>\n```\n\n----------------------------------------\n\nTITLE: Running MLflow MNIST Example with Custom Parameters\nDESCRIPTION: Command to run the MNIST example using MLflow with multiple custom parameters including epochs, devices, batch size, workers, learning rate, and strategy.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/MNIST/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy=\"ddp\"\n```\n\n----------------------------------------\n\nTITLE: Preparing Evaluation DataFrame with Questions and Ground Truth Sources\nDESCRIPTION: Creates a DataFrame with the required format for evaluation, containing questions and their associated ground truth document sources.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Prepare dataframe `data` with the required format\ndata = pd.DataFrame({})\ndata[\"question\"] = generated_df[\"question\"].copy(deep=True)\ndata[\"source\"] = generated_df[\"source\"].apply(lambda x: [x])\ndata.head(3)\n```\n\n----------------------------------------\n\nTITLE: Saving Tensorflow Model in H5 Format with MLflow\nDESCRIPTION: Shows how to save a Tensorflow model in H5 format using MLflow. It demonstrates the use of keras_model_kwargs parameter in mlflow.tensorflow.log_model() to specify the save format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/guide/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.Sequential(\n    [\n        keras.Input([28, 28, 3]),\n        keras.layers.Conv2D(8, 2),\n        keras.layers.MaxPool2D(2),\n        keras.layers.Flatten(),\n        keras.layers.Dense(2),\n        keras.layers.Softmax(),\n    ]\n)\n\nsave_path = \"model\"\nwith mlflow.start_run() as run:\n    mlflow.tensorflow.log_model(\n        model, \"model\", keras_model_kwargs={\"save_format\": \"h5\"}\n    )\n\n# Load back the model.\nloaded_model = mlflow.tensorflow.load_model(f\"runs:/{run.info.run_id}/{save_path}\")\n\nprint(loaded_model.predict(tf.random.uniform([1, 28, 28, 3])))\n```\n\n----------------------------------------\n\nTITLE: Loading MLflow Model Using Version Alias\nDESCRIPTION: Shows how to set and use model version aliases for loading models. Demonstrates creating an alias, retrieving model information, and loading the model using the alias in the URI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/step3-load-model/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow.sklearn\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\n\n# Set model version alias\nmodel_name = \"sk-learn-random-forest-reg-model\"\nmodel_version_alias = \"the_best_model_ever\"\nclient.set_registered_model_alias(\n    model_name, model_version_alias, \"1\"\n)  # Duplicate of step in UI\n\n# Get information about the model\nmodel_info = client.get_model_version_by_alias(model_name, model_version_alias)\nmodel_tags = model_info.tags\nprint(model_tags)\n\n# Get the model version using a model URI\nmodel_uri = f\"models:/{model_name}@{model_version_alias}\"\nmodel = mlflow.sklearn.load_model(model_uri)\n\nprint(model)\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow tracking URI to local server\nDESCRIPTION: Python code to configure MLflow to use a local tracking server at http://localhost:5000. This must be done before logging any metrics or artifacts.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/tracking-server-overview/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n```\n\n----------------------------------------\n\nTITLE: Basic Spark UDF Integration with MLflow Model\nDESCRIPTION: Shows how to create a Spark UDF from an MLflow model with prediction and probability outputs. The example demonstrates setting up a basic UDF with multiple return fields.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_78\n\nLANGUAGE: python\nCODE:\n```\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, \"<path-to-model>\", result_type=\"prediction float, probability: array<float>\"\n)\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf())\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing Admin Users in MLflow\nDESCRIPTION: Example showing how to authenticate as a built-in admin user and then create and promote another user to admin status. This demonstrates using environment variables for authentication and the AuthServiceClient for user management.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# authenticate as built-in admin user\nexport MLFLOW_TRACKING_USERNAME=admin\nexport MLFLOW_TRACKING_PASSWORD=password\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.server import get_app_client\n\ntracking_uri = \"http://localhost:5000/\"\n\nauth_client = get_app_client(\"basic-auth\", tracking_uri=tracking_uri)\nauth_client.create_user(username=\"user1\", password=\"pw1\")\nauth_client.update_user_admin(username=\"user1\", is_admin=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a Model Function for RAG Evaluation\nDESCRIPTION: Creates a simple wrapper function that processes input questions through the RAG chain and returns answers, designed to be used with MLflow's evaluation framework.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef model(input_df):\n    answer = []\n    for index, row in input_df.iterrows():\n        answer.append(qa(row[\"questions\"]))\n\n    return answer\n```\n\n----------------------------------------\n\nTITLE: React JSX Card Component\nDESCRIPTION: React component showing an image inside a Card wrapper for displaying experiment tracking visualization\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<Card>\n![Experiment Tracking](/images/llms/experiment-tracking.png)\n</Card>\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving a Scikit-learn Model Outside MLflow\nDESCRIPTION: This snippet creates a simple linear regression model using scikit-learn, trains it on the diabetes dataset, and saves it using pickle. This represents a model created outside of MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pickle\n\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\ndiabetes_X = diabetes_X[:, np.newaxis, 2]\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\nlr_model = linear_model.LinearRegression()\nlr_model.fit(diabetes_X_train, diabetes_y_train)\ndiabetes_y_pred = lr_model.predict(diabetes_X_test)\n\nfilename = \"lr_model.pkl\"\npickle.dump(lr_model, open(filename, \"wb\"))\n```\n\n----------------------------------------\n\nTITLE: Loading and Viewing Evaluation Results Table in MLflow\nDESCRIPTION: This snippet shows how to load the evaluation results table from MLflow artifacts. The table contains per-row scores and can be used for detailed analysis of the model's performance on individual examples.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\neval_results_table = evaluate_results.tables[\"eval_results_table\"]\neval_results_table.head(5)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing MNIST Dataset\nDESCRIPTION: Defines a preprocessing function to scale pixel values and applies it to the dataset, along with batching and prefetching.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_fn(data):\n    image = tf.cast(data[\"image\"], tf.float32) / 255\n    label = data[\"label\"]\n    return (image, label)\n\n\ntrain_ds = train_ds.map(preprocess_fn).batch(128).prefetch(tf.data.AUTOTUNE)\ntest_ds = test_ds.map(preprocess_fn).batch(128).prefetch(tf.data.AUTOTUNE)\n```\n\n----------------------------------------\n\nTITLE: Experiment Structure Definition in MLflow REST API\nDESCRIPTION: Defines the core Experiment structure containing experiment metadata including ID, name, artifact location, and lifecycle information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_36\n\nLANGUAGE: rest\nCODE:\n```\n+-------------------+----------------------------------------+--------------------------------------------------------------------+\n|    Field Name     |                  Type                  |                            Description                             |\n+===================+========================================+====================================================================+\n| experiment_id     | ``STRING``                             | Unique identifier for the experiment.                              |\n+-------------------+----------------------------------------+--------------------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Configuration for MLflow Spark Module\nDESCRIPTION: This RST (reStructuredText) code configures Sphinx to automatically generate documentation for the MLflow.spark module. It includes all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.spark.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.spark\n===============\n\n.. automodule:: mlflow.spark\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with Sentence Transformers Native Model in MLflow\nDESCRIPTION: This code snippet demonstrates how to use the native Sentence Transformer model's encode() method to generate text embeddings from test data and print sample values from each embedding vector.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Use the native model to generate embeddings by calling encode() (unlike for the generic python function which uses the single entrypoint of `predict`)\nnative_embeddings = loaded_model_native.encode(inference_test)\n\nfor i, embedding in enumerate(native_embeddings):\n    print(\n        f\"The sample of the native library encoding call for embedding {i + 1} is: {embedding[:10]}\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Running ReActAgent Workflow in Python\nDESCRIPTION: This snippet shows how to run the ReActAgent workflow with a specific input question. It demonstrates the asynchronous execution of the agent.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nawait agent.run(input=\"What is (123 + 456) * 789?\")\n```\n\n----------------------------------------\n\nTITLE: Displaying MLflow Recipe Template Directory Structure\nDESCRIPTION: This snippet shows the typical directory structure of an MLflow Recipe template, including key files and directories such as recipe.yaml, requirements.txt, and the steps folder.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n├── recipe.yaml\n├── requirements.txt\n├── steps\n│   ├── ingest.py\n│   ├── split.py\n│   ├── transform.py\n│   ├── train.py\n│   ├── custom_metrics.py\n├── profiles\n│   ├── local.yaml\n│   ├── databricks.yaml\n├── tests\n│   ├── ingest_test.py\n│   ├── ...\n│   ├── train_test.py\n│   ├── ...\n```\n\n----------------------------------------\n\nTITLE: Demonstrating predict_stream Method Implementation\nDESCRIPTION: Incomplete code snippet that starts to demonstrate how to implement and use the predict_stream method for streaming model predictions. This method returns a generator that yields a stream of responses for efficient data processing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport os\n```\n\n----------------------------------------\n\nTITLE: Configuring Centralized Database for MLflow Authentication (INI)\nDESCRIPTION: This snippet shows how to set up a configuration file to connect MLflow Authentication to a centralized PostgreSQL database. It specifies the database URI in the MLflow section of an INI file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: ini\nCODE:\n```\n[mlflow]\ndatabase_uri = postgresql://username:password@hostname:port/database\n```\n\n----------------------------------------\n\nTITLE: Demonstrating predict_stream Method Implementation\nDESCRIPTION: Incomplete code snippet that starts to demonstrate how to implement and use the predict_stream method for streaming model predictions. This method returns a generator that yields a stream of responses for efficient data processing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport os\n```\n\n----------------------------------------\n\nTITLE: Generating Model Script for MLflow Logging in Python\nDESCRIPTION: This snippet defines a utility function to generate a ready-to-log Python script from the current notebook. The generated script contains necessary library imports and model definitions for MLflow logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef generate_model_script(output_path, notebook_path=\"llama_index_workflow_tutorial.ipynb\"):\n    \"\"\"\n    A utility function to generate a ready-to-log .py script that\n    contains necessary library imports and model definitions.\n\n    Args:\n       output_path: The path to write the .py file to.\n       notebook_path: The path to the tutorial notebook.\n    \"\"\"\n    import nbformat\n\n    with open(notebook_path, encoding=\"utf-8\") as f:\n        notebook = nbformat.read(f, as_version=4)\n\n    # Filter cells that are code cells and contain the specified marker\n    merged_code = (\n        \"\\n\\n\".join(\n            [\n                cell.source\n                for cell in notebook.cells\n                if cell.cell_type == \"code\" and cell.source.startswith(\"# [USE IN MODEL]\")\n            ]\n        )\n        + \"\\n\\nimport mlflow\\n\\nmlflow.models.set_model(agent)\"\n    )\n\n    # Write to the output .py file\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(merged_code)\n\n    print(f\"Model code saved to {output_path}\")\n\n\n# Pass `notebook_path` argument if you changed the notebook name\ngenerate_model_script(output_path=\"react_agent.py\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Training with MLflow Tracking\nDESCRIPTION: Configures and initializes the training process with MLflow integration for experiment tracking. Sets up training arguments including batch size, learning rate, and optimization settings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nimport transformers\nfrom transformers import TrainingArguments\nimport mlflow\n\nmlflow.set_experiment(\"MLflow PEFT Tutorial\")\n\ntraining_args = TrainingArguments(\n    report_to=\"mlflow\",\n    run_name=f\"Mistral-7B-SQL-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\",\n    output_dir=\"YOUR_OUTPUT_DIR\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",\n    bf16=True,\n    learning_rate=2e-5,\n    lr_scheduler_type=\"constant\",\n    max_steps=500,\n    save_steps=100,\n    logging_steps=100,\n    warmup_steps=5,\n    ddp_find_unused_parameters=False,\n)\n\ntrainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=tokenized_train_dataset,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    args=training_args,\n)\n\npeft_model.config.use_cache = False\n```\n\n----------------------------------------\n\nTITLE: Creating Registered Model in MLflow R API\nDESCRIPTION: Function to create a new registered model in the MLflow model registry, with options for tags and description.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_8\n\nLANGUAGE: r\nCODE:\n```\nmlflow_create_registered_model(\n  name,\n  tags = NULL,\n  description = NULL,\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Python Package\nDESCRIPTION: Command to install the MLflow Python package which is a prerequisite for using the R interface\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/R/mlflow/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Overriding Attributes in prod.yaml Profile\nDESCRIPTION: This YAML snippet demonstrates how to override the 'RMSE_THRESHOLD' attribute in a prod.yaml profile. It sets a custom value to more aggressively validate model quality for production.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nRMSE_THRESHOLD: 5.2\n```\n\n----------------------------------------\n\nTITLE: Implementing Session ID Tracking in MLflow ChatModel\nDESCRIPTION: This snippet demonstrates how to create a custom ChatModel class that attaches session IDs as tags to MLflow traces. It uses the OpenAI API for chat completions and sets the session ID as a trace tag when provided.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/session.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.entities import SpanType\nfrom mlflow.types.llm import ChatMessage, ChatParams, ChatCompletionResponse\n\nimport openai\nfrom typing import Optional\n\nmlflow.set_experiment(\"Tracing Session ID Demo\")\n\n\nclass ChatModelWithSession(mlflow.pyfunc.ChatModel):\n    @mlflow.trace(span_type=SpanType.CHAT_MODEL)\n    def predict(\n        self, context, messages: list[ChatMessage], params: Optional[ChatParams] = None\n    ) -> ChatCompletionResponse:\n        if session_id := (params.custom_inputs or {}).get(\"session_id\"):\n            # Set session ID tag on the current trace\n            mlflow.update_current_trace(tags={\"session_id\": session_id})\n\n        response = openai.OpenAI().chat.completions.create(\n            messages=[m.to_dict() for m in messages],\n            model=\"gpt-4o-mini\",\n        )\n\n        return ChatCompletionResponse.from_dict(response.to_dict())\n\n\nmodel = ChatModelWithSession()\n\n# Invoke the chat model multiple times with the same session ID\nsession_id = \"123\"\nmessages = [ChatMessage(role=\"user\", content=\"What is MLflow Tracing?\")]\nresponse = model.predict(\n    None, messages, ChatParams(custom_inputs={\"session_id\": session_id})\n)\n\n# Invoke again with the same session ID\nmessages.append(\n    ChatMessage(role=\"assistant\", content=response.choices[0].message.content)\n)\nmessages.append(ChatMessage(role=\"user\", content=\"How to get started?\"))\nresponse = model.predict(\n    None, messages, ChatParams(custom_inputs={\"session_id\": session_id})\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Density Plot Comparison Between Weekday and Weekend Data in Python\nDESCRIPTION: This function generates a density plot comparing the distribution of demand between weekdays and weekends. It uses Seaborn's kdeplot to create filled density curves with different colors for each category and includes proper labeling and formatting.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef plot_density_weekday_weekend(df, style=\"seaborn\", plot_size=(10, 8)):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n\n        # Plot density for weekdays\n        sns.kdeplot(\n            df[df[\"weekend\"] == 0][\"demand\"],\n            color=\"blue\",\n            label=\"Weekday\",\n            ax=ax,\n            fill=True,\n            alpha=0.15,\n        )\n\n        # Plot density for weekends\n        sns.kdeplot(\n            df[df[\"weekend\"] == 1][\"demand\"],\n            color=\"green\",\n            label=\"Weekend\",\n            ax=ax,\n            fill=True,\n            alpha=0.15,\n        )\n\n        ax.set_title(\"Density Plot of Demand by Weekday/Weekend\", fontsize=14)\n        ax.set_xlabel(\"Demand\", fontsize=12)\n        ax.legend(fontsize=12)\n        for i in ax.get_xticklabels() + ax.get_yticklabels():\n            i.set_fontsize(10)\n\n        plt.tight_layout()\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Handling None Values in Model Output\nDESCRIPTION: Shows how MLflow handles None values in model output by inferring them as AnyType in the signature schema.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import infer_signature\n\ndata = [\n    {\n        \"id\": None,\n        \"object\": \"chat.completion\",\n        \"created\": 1731491873,\n        \"model\": None,\n        \"choices\": [\n            {\n                \"index\": 0,\n                \"message\": {\n                    \"role\": \"assistant\",\n                    \"content\": \"MLflow\",\n                },\n                \"finish_reason\": None,\n            }\n        ],\n        \"usage\": {\n            \"prompt_tokens\": None,\n            \"completion_tokens\": None,\n            \"total_tokens\": None,\n        },\n    }\n]\n\nprint(infer_signature(data))\n```\n\n----------------------------------------\n\nTITLE: Authenticating to MLflow Using Environment Variables\nDESCRIPTION: Example of how to authenticate to MLflow using environment variables. Sets the required username and password environment variables and then starts an MLflow run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_TRACKING_USERNAME=username\nexport MLFLOW_TRACKING_PASSWORD=password\n```\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_tracking_uri(\"https://<mlflow_tracking_uri>/\")\nwith mlflow.start_run():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Saved Spark ML Model in MLflow\nDESCRIPTION: Creates a test DataFrame and uses a saved Spark ML logistic regression model to make predictions. The test data is converted to a Pandas DataFrame for prediction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Make predictions on test data.\n# The DataFrame used in the predict method must be a Pandas DataFrame\ntest = spark.createDataFrame(\n    [\n        (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n        (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n        (1.0, Vectors.dense([0.0, 2.2, -1.5])),\n    ],\n    [\"label\", \"features\"],\n).toPandas()\n\nprediction = lr_model_saved.predict(test)\n```\n\n----------------------------------------\n\nTITLE: Training and Logging Random Forest Model with MLflow in Python\nDESCRIPTION: This snippet demonstrates the process of training a RandomForestRegressor model, calculating performance metrics, and logging the model, parameters, and metrics using MLflow. It includes data preparation, model training, and evaluation steps.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nX = data.drop(columns=[\"date\", \"demand\"])\ny = data[\"demand\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nparams = {\n    \"n_estimators\": 100,\n    \"max_depth\": 6,\n    \"min_samples_split\": 10,\n    \"min_samples_leaf\": 4,\n    \"bootstrap\": True,\n    \"oob_score\": False,\n    \"random_state\": 888,\n}\n\nrf = RandomForestRegressor(**params)\n\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_val)\n\nmae = mean_absolute_error(y_val, y_pred)\nmse = mean_squared_error(y_val, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_val, y_pred)\n\nmetrics = {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n\nwith mlflow.start_run(run_name=run_name) as run:\n    mlflow.log_params(params)\n\n    mlflow.log_metrics(metrics)\n\n    mlflow.sklearn.log_model(sk_model=rf, input_example=X_val, artifact_path=artifact_path)\n```\n\n----------------------------------------\n\nTITLE: Setting up Model Variables and Environment\nDESCRIPTION: Initializes variables for model path, name, and the model instance. Also defines the conda environment configuration required for the model deployment with necessary dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nmodel_path = \"vader\"\nreg_model_name = \"PyFuncVaderSentiments\"\nvader_model = SocialMediaAnalyserModel()\n\n# Set the tracking URI to use local SQLAlchemy db file and start the run\n# Log MLflow entities and save the model\nmlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n\n# Save the conda environment for this model.\nconda_env = {\n    \"channels\": [\"defaults\", \"conda-forge\"],\n    \"dependencies\": [f\"python={PYTHON_VERSION}\", \"pip\"],\n    \"pip\": [\n        \"mlflow\",\n        f\"cloudpickle=={cloudpickle.__version__}\",\n        \"vaderSentiment==3.3.2\",\n    ],\n    \"name\": \"mlflow-env\",\n}\n```\n\n----------------------------------------\n\nTITLE: Using Mock LLM for LlamaIndex Workflow in Python\nDESCRIPTION: This snippet demonstrates how to use a mock LLM in a LlamaIndex workflow. It configures a MockLLM with a maximum token limit and runs the agent with a sample input.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.llms import MockLLM\n\nSettings.llm = MockLLM(max_tokens=1)\n\nawait agent.run(input=\"What is (123 + 456) * 789?\")\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Databricks Workspace\nDESCRIPTION: Python code to authenticate with Databricks workspace using MLflow login function\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/databricks-trial/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.login()\n```\n\n----------------------------------------\n\nTITLE: Loading FashionMNIST Dataset in PyTorch\nDESCRIPTION: Loads the FashionMNIST dataset for training and testing, applying ToTensor transformation to normalize pixel values to the range [0,1).\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n```\n\n----------------------------------------\n\nTITLE: REST API Embeddings Endpoint Query\nDESCRIPTION: Curl command to query the embeddings endpoint in the MLflow AI Gateway. This example sends two customer service texts to generate vector embeddings for semantic analysis.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://my.deployments:8888/endpoints/embeddings/invocations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": [\"I would like to return my shipment of beanie babies, please\", \"Can I please speak to a human now?\"]}'\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Tracing for LlamaIndex in Python\nDESCRIPTION: This single-line command enables MLflow tracing for LlamaIndex executions, automatically tracking every LlamaIndex operation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmlflow.llama_index.autolog()\n```\n\n----------------------------------------\n\nTITLE: Running Wine Quality Model with Lower Regularization Parameters\nDESCRIPTION: Executes the training function with reduced regularization parameters (alpha=0.2, l1_ratio=0.2) to evaluate how decreased regularization impacts model performance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sklearn_elasticnet_wine/train.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain(0.2, 0.2)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Test Plugin from Source\nDESCRIPTION: Commands to clone the MLflow repository and install an example plugin for testing purposes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/mlflow/mlflow\ncd mlflow\npip install -e tests/resources/mlflow-test-plugin\n```\n\n----------------------------------------\n\nTITLE: Deleting Experiment via HTTP POST Request\nDESCRIPTION: HTTP POST endpoint for marking an experiment and its associated metadata, runs, metrics, params, and tags for deletion. For FileStore, associated artifacts are also deleted.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_6\n\nLANGUAGE: http\nCODE:\n```\n2.0/mlflow/experiments/delete\n```\n\n----------------------------------------\n\nTITLE: Defining Model Configuration for MLflow ChatModel Agent in Python\nDESCRIPTION: This code defines the model_config dictionary which specifies the configuration for different agents (judge and oracle) in the model. It includes endpoint, instruction, temperature, and max_tokens settings for each agent, as well as a general configuration for user responses.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel_config = {\n    \"models\": {\n        \"judge\": {\n            \"endpoint\": \"databricks-meta-llama-3-1-405b-instruct\",\n            \"instruction\": (\n                \"You are an evaluator of answers provided by others. Based on the context of both the question and the answer, \"\n                \"provide a corrected answer if it is incorrect; otherwise, enhance the answer with additional context and explanation.\"\n            ),\n            \"temperature\": 0.5,\n            \"max_tokens\": 2000,\n        },\n        \"oracle\": {\n            \"endpoint\": \"databricks-mixtral-8x7b-instruct\",\n            \"instruction\": (\n                \"You are a knowledgeable source of information that excels at providing detailed, but brief answers to questions. \"\n                \"Provide an answer to the question based on the information provided.\"\n            ),\n            \"temperature\": 0.9,\n            \"max_tokens\": 5000,\n        },\n    },\n    \"configuration\": {\n        \"user_response_instruction\": \"Can you evaluate and enhance this answer with the provided contextual history?\"\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Ending MLflow Run in R\nDESCRIPTION: Function to terminate an MLflow run. If run_id is not specified, attempts to end the current active run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_15\n\nLANGUAGE: r\nCODE:\n```\nmlflow_end_run(\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for MLflow Project\nDESCRIPTION: A requirements specification listing MLflow and scikit-learn 1.4.2 as dependencies. This file would be used with pip to install the necessary packages for an MLflow project environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/resources/example_virtualenv_no_python_env/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlflow\nscikit-learn==1.4.2\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Tracking Server in Artifacts-Only Mode in Bash\nDESCRIPTION: Command to start the MLflow tracking server in artifacts-only mode. This configures the server to handle only artifact-related operations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --artifacts-only ...\n```\n\n----------------------------------------\n\nTITLE: MLflow Server Startup Output\nDESCRIPTION: Example output when the MLflow Tracking Server starts successfully. It shows the server listening on the specified host and port, and indicates the worker processes that have been initiated.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step1-tracking-server/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n[2023-11-01 10:28:12 +0900] [28550] [INFO] Starting gunicorn 20.1.0\n[2023-11-01 10:28:12 +0900] [28550] [INFO] Listening at: http://127.0.0.1:8080 (28550)\n[2023-11-01 10:28:12 +0900] [28550] [INFO] Using worker: sync\n[2023-11-01 10:28:12 +0900] [28552] [INFO] Booting worker with pid: 28552\n[2023-11-01 10:28:12 +0900] [28553] [INFO] Booting worker with pid: 28553\n[2023-11-01 10:28:12 +0900] [28555] [INFO] Booting worker with pid: 28555\n[2023-11-01 10:28:12 +0900] [28558] [INFO] Booting worker with pid: 28558\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Tracking Server with Remote Artifact Store in Bash\nDESCRIPTION: Command to start the MLflow tracking server with a remote S3 bucket as the artifact store. This configures the server to proxy artifact access to an S3 bucket.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server \\\n    --host 0.0.0.0 \\\n    --port 8885 \\\n    --artifacts-destination s3://my-bucket\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Flavors\nDESCRIPTION: Retrieves and displays the model flavors information from MLflow, showing supported deployment options and requirements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel_info.flavors\n```\n\n----------------------------------------\n\nTITLE: Setting Up Paraphrase Mining Model with Sentence Transformers in Python\nDESCRIPTION: This snippet sets up the Sentence Transformer model for paraphrase mining. It loads the model, prepares input examples, saves the model, defines artifacts, generates test output, and creates a model signature for MLflow integration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/paraphrase-mining/paraphrase-mining-sentence-transformers.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load a pre-trained sentence transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Create an input example DataFrame\ninput_example = pd.DataFrame({\"query\": [\"This product works well. I'm satisfied.\"]})\n\n# Save the model in the /tmp directory\nmodel_directory = \"/tmp/paraphrase_search_model\"\nmodel.save(model_directory)\n\n# Define the path for the corpus file\ncorpus_file = \"/tmp/feedback.txt\"\n\n# Define the artifacts (paths to the model and corpus file)\nartifacts = {\"model_path\": model_directory, \"corpus_file\": corpus_file}\n\n# Generate test output for signature\n# Sample output for paraphrase mining could be a list of tuples (paraphrase, score)\ntest_output = [{\"This product is satisfactory and functions as expected.\": \"0.8\"}]\n\n# Define the signature associated with the model\n# The signature includes the structure of the input and the expected output, as well as any parameters that\n# we would like to expose for overriding at inference time (including their default values if they are not overridden).\nsignature = infer_signature(\n    model_input=input_example, model_output=test_output, params={\"similarity_threshold\": 0.5}\n)\n\n# Visualize the signature, showing our overridden inference parameter and its default.\nsignature\n```\n\n----------------------------------------\n\nTITLE: Searching Traces Associated with a Specific Run in Python\nDESCRIPTION: Demonstrates how to search for traces associated with a specific MLflow run using the run_id parameter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/search.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Search traces associated with a specific run\ntraces = mlflow.search_traces(run_id=\"run_id_123456\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Input Example and Model Setup in Python\nDESCRIPTION: Creates input examples and saves the model with required artifacts. Defines model signature with parameters for top_k matches and minimum relevancy threshold.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninput_example = [\"Something I want to find matches for.\"]\n\nmodel_directory = \"/tmp/search_model\"\nmodel.save(model_directory)\n\nartifacts = {\"model_path\": model_directory, \"corpus_file\": corpus_file}\n\ntest_output = [\"match 1\", \"match 2\", \"match 3\"]\n\nsignature = infer_signature(\n    input_example, test_output, params={\"top_k\": 3, \"minimum_relevancy\": 0.2}\n)\n\nsignature\n```\n\n----------------------------------------\n\nTITLE: Getting Remote Tracking URI in MLflow with R\nDESCRIPTION: Function to retrieve the current remote tracking URI where MLflow is storing run and experiment data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_22\n\nLANGUAGE: r\nCODE:\n```\nmlflow_get_tracking_uri()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for MLflow Trace UI Demo\nDESCRIPTION: Installs MLflow, OpenAI, and optional packages needed for the RAG demo. This pip install command ensures all dependencies are available for the tutorial.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install mlflow>=2.20 openai langchain langchain-community beautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Fixing Missing Dependencies with MLflow Predict API (Python)\nDESCRIPTION: Shows how to use the pip-requirements-override option in the MLflow predict API to add missing dependencies without re-logging the model during troubleshooting.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.models.predict(\n    model_uri=\"runs:/<run_id>/<model_path>\",\n    input_data=\"<input_data>\",\n    pip_requirements_override=[\"opencv-python==4.8.0\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Retriever with Custom Metrics in MLflow (Python)\nDESCRIPTION: This code shows how to evaluate a retriever using MLflow with custom metrics specified in the extra_metrics parameter. It calculates precision at k for different k values.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nevaluate_results = mlflow.evaluate(\n    data=data,\n    targets=\"ground_truth_context\",\n    predictions=\"retrieved_context\",\n    extra_metrics=[\n      mlflow.metrics.precision_at_k(4),\n      mlflow.metrics.precision_at_k(5)\n    ],\n  )\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow OpenAI Auto-logging for Ollama\nDESCRIPTION: This snippet shows how to enable auto-logging for OpenAI SDK, which can be used to trace Ollama LLM interactions. It also demonstrates how to set a tracking URI and experiment name.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/ollama.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Enable auto-tracing for OpenAI\nmlflow.openai.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"Ollama\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Using MLflow Model for Inference\nDESCRIPTION: Python code to load the logged model using MLflow's pyfunc flavor and use it for making predictions on test data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load the model back for predictions as a generic Python Function model\nloaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n\npredictions = loaded_model.predict(X_test)\n\niris_feature_names = datasets.load_iris().feature_names\n\nresult = pd.DataFrame(X_test, columns=iris_feature_names)\nresult[\"actual_class\"] = y_test\nresult[\"predicted_class\"] = predictions\n\nresult[:4]\n```\n\n----------------------------------------\n\nTITLE: Updating MLflow Model Requirements with CLI\nDESCRIPTION: This snippet shows how to use the MLflow CLI to update a model's dependencies. It adds the 'opencv-python' requirement to an existing model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models update-pip-requirements -m runs:/<run_id>/<model_path> add \"opencv-python==4.8.0\"\n```\n\n----------------------------------------\n\nTITLE: Structured Spark UDF with Type Definitions\nDESCRIPTION: Demonstrates creating a Spark UDF with explicit type definitions using ArrayType and FloatType. Shows how to properly structure input parameters using struct.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_79\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.types import ArrayType, FloatType\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, \"path/to/model\", result_type=ArrayType(FloatType())\n)\n# The prediction column will contain all the numeric columns returned by the model as floats\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf(struct(\"name\", \"age\")))\n```\n\n----------------------------------------\n\nTITLE: Serving MLflow Model Locally\nDESCRIPTION: Command to serve a registered MLflow model locally on a specified port. Uses the models URI format to reference a specific model version.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve -m \"models:/wine-quality/1\" --port 5002\n```\n\n----------------------------------------\n\nTITLE: Logging Diviner Metrics and Parameters as JSON in Python\nDESCRIPTION: This code shows how to log Diviner model metrics and parameters directly as JSON artifacts using mlflow.log_dict(). It extracts model parameters and cross-validation metrics, performs necessary data type conversions, and logs the data as JSON dictionaries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nparams = model.extract_model_params()\nmetrics = model.cross_validate_and_score(\n    horizon=\"72 hours\",\n    period=\"240 hours\",\n    initial=\"480 hours\",\n    parallel=\"threads\",\n    rolling_window=0.1,\n    monthly=False,\n)\nparams[\"t_scale\"] = params[\"t_scale\"].astype(str)\nparams[\"start\"] = params[\"start\"].astype(str)\nparams = params.drop(\"stan_backend\", axis=1)\n\nmlflow.log_dict(params.to_dict(), \"params.json\")\nmlflow.log_dict(metrics.to_dict(), \"metrics.json\")\n```\n\n----------------------------------------\n\nTITLE: React JSX Import Statement\nDESCRIPTION: Importing React card components and API link component for documentation page layout\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { Card, LogoCard, CardGroup, PageCard, SmallLogoCard } from \"@site/src/components/Card\";\nimport { APILink } from \"@site/src/components/APILink\";\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow via pip\nDESCRIPTION: Command to install MLflow using pip package manager.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/local-database/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: ConditionalKNN Model Implementation with MLflow Autologging\nDESCRIPTION: Complete example demonstrating the implementation of a ConditionalKNN model with automatic MLflow logging, including data preparation, model training, and transformation\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/synapseml/autologging.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.ml.linalg import Vectors\nfrom synapse.ml.nn import ConditionalKNN\n\ndf = spark.createDataFrame(\n    [\n        (Vectors.dense(2.0, 2.0, 2.0), \"foo\", 1),\n        (Vectors.dense(2.0, 2.0, 4.0), \"foo\", 3),\n        (Vectors.dense(2.0, 2.0, 6.0), \"foo\", 4),\n        (Vectors.dense(2.0, 2.0, 8.0), \"foo\", 3),\n        (Vectors.dense(2.0, 2.0, 10.0), \"foo\", 1),\n        (Vectors.dense(2.0, 2.0, 12.0), \"foo\", 2),\n        (Vectors.dense(2.0, 2.0, 14.0), \"foo\", 0),\n        (Vectors.dense(2.0, 2.0, 16.0), \"foo\", 1),\n        (Vectors.dense(2.0, 2.0, 18.0), \"foo\", 3),\n        (Vectors.dense(2.0, 2.0, 20.0), \"foo\", 0),\n        (Vectors.dense(2.0, 4.0, 2.0), \"foo\", 2),\n        (Vectors.dense(2.0, 4.0, 4.0), \"foo\", 4),\n        (Vectors.dense(2.0, 4.0, 6.0), \"foo\", 2),\n        (Vectors.dense(2.0, 4.0, 8.0), \"foo\", 2),\n        (Vectors.dense(2.0, 4.0, 10.0), \"foo\", 4),\n        (Vectors.dense(2.0, 4.0, 12.0), \"foo\", 3),\n        (Vectors.dense(2.0, 4.0, 14.0), \"foo\", 2),\n        (Vectors.dense(2.0, 4.0, 16.0), \"foo\", 1),\n        (Vectors.dense(2.0, 4.0, 18.0), \"foo\", 4),\n        (Vectors.dense(2.0, 4.0, 20.0), \"foo\", 4),\n    ],\n    [\"features\", \"values\", \"labels\"],\n)\n\ncnn = ConditionalKNN().setOutputCol(\"prediction\")\ncnnm = cnn.fit(df)\n\ntest_df = spark.createDataFrame(\n    [\n        (Vectors.dense(2.0, 2.0, 2.0), \"foo\", 1, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 4.0), \"foo\", 4, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 6.0), \"foo\", 2, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 8.0), \"foo\", 4, [0, 1]),\n        (Vectors.dense(2.0, 2.0, 10.0), \"foo\", 4, [0, 1]),\n    ],\n    [\"features\", \"values\", \"labels\", \"conditioner\"],\n)\n\ndisplay(cnnm.transform(test_df))\n```\n\n----------------------------------------\n\nTITLE: Defining DSPy Signature and Module for Text Classification\nDESCRIPTION: Creates a DSPy Signature and Module for text classification, specifying input and output fields and the classification logic.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass TextClassificationSignature(dspy.Signature):\n    text = dspy.InputField()\n    label = dspy.OutputField(desc=f\"Label of predicted class. Possible labels are {unique_train_labels}\")\n\n\nclass TextClassifier(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_classification = dspy.Predict(TextClassificationSignature)\n\n    def forward(self, text: str):\n        return self.generate_classification(text=text)\n```\n\n----------------------------------------\n\nTITLE: Training Keras Model with MLflow Callback (Per Batch Logging) in Python\nDESCRIPTION: Demonstrates how to configure MLflow's MlflowCallback to log metrics per batch instead of per epoch, allowing for more frequent and granular logging during training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/keras/quickstart/quickstart_keras.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = initialize_model()\n\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(),\n    metrics=[\"accuracy\"],\n)\n\nwith mlflow.start_run() as run:\n    model.fit(\n        x_train,\n        y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_split=0.1,\n        callbacks=[mlflow.keras.MlflowCallback(run, log_every_epoch=False, log_every_n_steps=5)],\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging Spark ML Pipeline Model with MLflow\nDESCRIPTION: Creates a Spark ML pipeline with tokenizer, hashing TF, and logistic regression stages. The pipeline is fit on training data and logged to MLflow with a sample input.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntraining_df = spark.createDataFrame(\n    [\n        (0, \"a b c d e spark\", 1.0),\n        (1, \"b d\", 0.0),\n        (2, \"spark f g h\", 1.0),\n        (3, \"hadoop mapreduce\", 0.0),\n    ],\n    [\"id\", \"text\", \"label\"],\n)\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.001)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\nmodel = pipeline.fit(training_df)\n\nmlflow.spark.log_model(model, \"spark-model\", sample_input=training_df)\n```\n\n----------------------------------------\n\nTITLE: REST API Endpoint Information Query\nDESCRIPTION: Curl command to get information about a specific endpoint in the MLflow AI Gateway. This returns a serialized representation of the endpoint data structure including name, type, and model details.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET http://my.deployments:8888/api/2.0/endpoints/embeddings\n```\n\n----------------------------------------\n\nTITLE: Topic Analysis and Visualization for Failed Retrievals\nDESCRIPTION: This snippet performs topic analysis on unsuccessfully retrieved questions and prepares the results for interactive visualization. This helps identify common topics or patterns in questions that the retrieval model struggles with, informing potential improvements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlda_model, corpus, dictionary = topical_analysis(miss_questions)\nvis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n\n# Uncomment the following line to render the interactive widget\n# pyLDAvis.display(vis_data)\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Evaluation Dataset\nDESCRIPTION: Loads the saved evaluation dataset from disk and properly deserializes the source and retrieved document ID lists for evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Load the static evaluation dataset from disk and deserialize the source and retrieved doc ids\ndata = pd.read_csv(EVALUATION_DATASET_PATH)\ndata[\"source\"] = data[\"source\"].apply(ast.literal_eval)\ndata[\"retrieved_doc_ids\"] = data[\"retrieved_doc_ids\"].apply(ast.literal_eval)\ndata.head(3)\n```\n\n----------------------------------------\n\nTITLE: Training MLflow Image Classifier\nDESCRIPTION: Command to run the flower classifier example as an MLflow project, which downloads training data and trains a Keras model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/flower_classifier/README.rst#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run examples/flower_classifier\n```\n\n----------------------------------------\n\nTITLE: Accessing Individual MLflow Experiment Tag in Python\nDESCRIPTION: This snippet shows how to access a specific tag value from an MLflow experiment. It retrieves and prints the 'team' tag from the first experiment in the search results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(apples_experiment[0].tags[\"team\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Parameters and Cache Paths\nDESCRIPTION: Sets up configuration parameters including a random seed for reproducibility, file paths for caching LLM responses and embeddings, and paths for storing scraped documents and the final generated dataset.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Other configurations\n\n# Choose a seed for reproducible results\nSEED = 2023\n\n# For cost-saving purposes, choose a path to persist the responses for LLM calls\nCACHE_PATH = \"_cache.json\"\nEMBEDDINGS_CACHE_PATH = \"_embeddings_cache.json\"\n\n# To avoid re-running the scraping process, choose a path to save the scrapped docs\nSCRAPPED_DATA_PATH = \"mlflow_docs_scraped.csv\"\n\n# Choose a path to save the generated dataset\nOUTPUT_DF_PATH = \"question_answer_source.csv\"\n```\n\n----------------------------------------\n\nTITLE: Creating SQL Translation Function\nDESCRIPTION: Defines and creates a UnityCatalog SQL function for text translation, supporting English to Spanish and vice versa.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntranslate_function_name = f\"{CATALOG}.{SCHEMA}.translate\"\nsql_body = f\"\"\"CREATE OR REPLACE FUNCTION {translate_function_name}(content STRING COMMENT 'content to translate', language STRING COMMENT 'target language')\nRETURNS STRING\nCOMMENT 'translate the content to target language, currently only english <-> spanish translation is supported'\nRETURN SELECT ai_translate(content, language)\n\"\"\"\nclient.create_function(sql_function_body=sql_body)\n```\n\n----------------------------------------\n\nTITLE: Loading the GPT-4 Model for Text Message Analysis\nDESCRIPTION: Loads the previously logged GPT-4 model as a Python function (pyfunc) model from MLflow, making it ready for use in analyzing text messages.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-chat-completions.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: REST API List All Endpoints Query\nDESCRIPTION: Curl command to list all available endpoints in the MLflow AI Gateway. This provides a comprehensive view of all configured endpoints that can be used for different AI tasks.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET http://my.deployments:8888/api/2.0/endpoints/\n```\n\n----------------------------------------\n\nTITLE: Loading and Testing SimilarityModel Inference\nDESCRIPTION: Demonstrates how to load the logged model and perform inference on a test sentence pair. Shows basic usage of the model for computing semantic similarity between sentences.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-similarity/semantic-similarity-sentence-transformers.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load our custom semantic similarity model implementation by providing the uri that the model was logged to\nloaded_dynamic = mlflow.pyfunc.load_model(model_info.model_uri)\n\n# Create an evaluation test DataFrame\nsimilarity_data = pd.DataFrame([{\"sentence_1\": \"I like apples\", \"sentence_2\": \"I like oranges\"}])\n\n# Verify that the model generates a reasonable prediction\nsimilarity_score = loaded_dynamic.predict(similarity_data)\n\nprint(f\"The similarity between these sentences is: {similarity_score}\")\n```\n\n----------------------------------------\n\nTITLE: Renaming MLflow Experiment in R\nDESCRIPTION: This function renames an existing MLflow experiment. It requires the new name and the experiment ID, and optionally accepts an MLflow client object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_36\n\nLANGUAGE: r\nCODE:\n```\nmlflow_rename_experiment(new_name, experiment_id = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Disabling Warnings in Python\nDESCRIPTION: This snippet sets an environment variable to disable tokenizer parallelism and filters out specific UserWarnings from setuptools and pydantic.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Processing Generated Questions and Answers\nDESCRIPTION: This snippet processes the generated questions and answers, removes error records, and adds the results to an existing DataFrame. It also includes a function to save the results to a CSV file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nqueries = []\n\n# The requests sometimes get ratelimited, you can re-execute this cell without losing the existing results.\nn = len(filtered_df)\nfor i, row in filtered_df.iterrows():\n    chunk = row[\"chunk\"]\n    question, answer = generate_question_answer(chunk)\n    print(f\"{i + 1}/{n}: {question}\")\n    queries.append(\n        {\n            \"question\": question,\n            \"answer\": answer,\n            \"chunk\": chunk,\n            \"chunk_id\": row[\"chunk_index\"],\n            \"source\": row[\"source\"],\n        }\n    )\n\nresult_df = pd.DataFrame(queries)\nresult_df = result_df[result_df[\"answer\"] != \"N/A\"]\n\ndef add_to_output_df(result_df=pd.DataFrame({})):\n    \"\"\"\n    This function adds the records in result_df to the existing records saved at OUTPUT_DF_PATH,\n    remove the duplicate rows and save the new collection of records back to OUTPUT_DF_PATH.\n    \"\"\"\n    if os.path.exists(OUTPUT_DF_PATH):\n        all_result_df = pd.read_csv(OUTPUT_DF_PATH)\n    else:\n        all_result_df = pd.DataFrame({})\n    all_result_df = (\n        pd.concat([all_result_df, result_df], ignore_index=True)\n        .drop_duplicates()\n        .sort_values(by=[\"source\", \"chunk_id\"])\n        .reset_index(drop=True)\n    )\n    all_result_df.to_csv(OUTPUT_DF_PATH, index=False)\n    return all_result_df\n\nall_result_df = add_to_output_df(result_df)\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Recipe with YAML\nDESCRIPTION: This YAML configuration file (recipe.yaml) defines the structure and behavior of an MLflow regression recipe. It specifies the target column, primary metrics, and configurations for various steps in the machine learning pipeline.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrecipe: \"regression/v1\"\ntarget_col: \"fare_amount\"\nprimary_metrics: \"root_mean_squared_error\"\nsteps:\n  ingest: {{INGEST_CONFIG}}\n  split:\n    split_ratios: {{SPLIT_RATIOS|default([0.75, 0.125, 0.125])}}\n  transform:\n    using: custom\n    transformer_method: transformer_fn\n  train:\n    using: custom\n    estimator_method: estimator_fn\n  evaluate:\n    validation_criteria:\n      - metric: root_mean_squared_error\n        threshold: 10\n      - metric: weighted_mean_squared_error\n        threshold: 20\n  register:\n    allow_non_validated_model: false\ncustom_metrics:\n  - name: weighted_mean_squared_error\n    function: weighted_mean_squared_error\n    greater_is_better: False\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for MLflow Statsmodels Module\nDESCRIPTION: This snippet defines the Sphinx documentation configuration for the MLflow Statsmodels module. It uses the automodule directive to automatically generate API documentation from docstrings, showing all members, undocumented members, and inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.statsmodels.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.statsmodels\n==================\n\n.. automodule:: mlflow.statsmodels\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Filtering Traces by Name, Timestamp, Status, and Tags in Python\nDESCRIPTION: Shows various filtering options for searching traces using the MLflow API, including filtering by name, timestamp, status, and tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/search.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmlflow.search_traces(filter_string=\"trace.name = 'predict'\")\n\n# Get traces created after a specific timestamp (in milliseconds)\ntimestamp = int(time.time() * 1000)\nmlflow.search_traces(\n    filter_string=f\"trace.timestamp > {timestamp - 3600000}\"  # Last hour\n)\n\n# Get successful traces\nmlflow.search_traces(filter_string=\"trace.status = 'OK'\")\n\n# Get failed traces\nmlflow.search_traces(filter_string=\"trace.status = 'ERROR'\")\n\nmlflow.search_traces(filter_string=\"tag.model_name = 'gpt-4'\")\n\nmlflow.search_traces(filter_string=\"trace.status = 'OK' AND tag.importance = 'high'\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom MLflow Callback for Keras in Python\nDESCRIPTION: Defines a custom MLflow callback by subclassing MlflowCallback to modify the logging behavior, allowing for more precise control over how steps are counted and logged during training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/keras/quickstart/quickstart_keras.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MlflowCallbackLogPerBatch(mlflow.keras.MlflowCallback):\n    def on_batch_end(self, batch, logs=None):\n        if self.log_every_n_steps is None or logs is None:\n            return\n        if (batch + 1) % self.log_every_n_steps == 0:\n            self.metrics_logger.record_metrics(logs, self._log_step)\n            self._log_step += self.log_every_n_steps\n```\n\n----------------------------------------\n\nTITLE: Training a Scikit-learn Model with MLflow\nDESCRIPTION: Command to run the sklearn_logistic_regression example that trains a model and logs it to MLflow, creating a run with model artifacts.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/quickstart_drilldown/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/sklearn_logistic_regression/train.py\n```\n\n----------------------------------------\n\nTITLE: Formatting Transcription Output in Python\nDESCRIPTION: This utility function formats a long string of transcribed text by splitting it into sentences and adding newline characters for improved readability in a Jupyter notebook.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef format_transcription(transcription):\n    \"\"\"\n    Function for formatting a long string by splitting into sentences and adding newlines.\n    \"\"\"\n    # Split the transcription into sentences, ensuring we don't split on abbreviations or initials\n    sentences = [\n        sentence.strip() + (\".\" if not sentence.endswith(\".\") else \"\")\n        for sentence in transcription.split(\". \")\n        if sentence\n    ]\n\n    # Join the sentences with a newline character\n    return \"\\n\".join(sentences)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Default Method in MLflow PyFunc Model\nDESCRIPTION: Uses the loaded MLflow PyFunc model to make predictions using the default prediction method (predict_proba) specified in the model signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nloaded_dynamic.predict(x_test)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Project from GitHub\nDESCRIPTION: Example command showing how to run an MLflow project from a GitHub repository with a parameter alpha set to 0.5\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run git@github.com:mlflow/mlflow-example.git -P alpha=0.5\n```\n\n----------------------------------------\n\nTITLE: Adding Extra Dependencies to MLflow Model in Python\nDESCRIPTION: Example of logging a custom MLflow model with extra pip requirements, specifically adding pandas as a dependency.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        # your model depends on pandas\n        import pandas as pd\n\n        ...\n        return prediction\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=CustomModel(),\n        artifact_path=\"model\",\n        extra_pip_requirements=[\"pandas==2.0.3\"],\n        input_example=input_data,\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Experiment Request Structure in MLflow REST API\nDESCRIPTION: JSON structure for creating an experiment via the MLflow REST API. It includes fields for experiment name, artifact location, and tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"STRING\",\n  \"artifact_location\": \"STRING\",\n  \"tags\": [\n    {\n      \"key\": \"STRING\",\n      \"value\": \"STRING\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring an OpenAI Chat Endpoint in MLflow AI Gateway\nDESCRIPTION: This YAML snippet demonstrates how to configure an endpoint for the OpenAI GPT-4 model in the MLflow AI Gateway. It specifies the endpoint name, type, model details, and rate limiting parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4\n      config:\n        openai_api_key: $OPENAI_API_KEY\n    limit:\n      renewal_period: minute\n      calls: 10\n```\n\n----------------------------------------\n\nTITLE: Creating a Vector Index in LlamaIndex\nDESCRIPTION: Code to load documents from a directory and create a simple in-memory vector index using LlamaIndex.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/llama_index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Disabling the Trace UI in Jupyter\nDESCRIPTION: Demonstrates how to disable the automatic display of the Trace UI. After disabling, trace objects will no longer automatically render in the notebook.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmlflow.tracing.disable_notebook_display()\n\n# no UI will be rendered\ntrace\n```\n\n----------------------------------------\n\nTITLE: Output DataFrame Evaluation Example\nDESCRIPTION: Example of evaluating predictions directly from a DataFrame without a model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_43\n\nLANGUAGE: python\nCODE:\n```\neval_dataset = pd.DataFrame(\n    {\n        \"targets\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n        \"predictions\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nresult = mlflow.evaluate(\n    data=eval_dataset,\n    predictions=\"predictions\",\n    targets=\"targets\",\n    extra_metrics=[mymetric],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameter Search Space\nDESCRIPTION: Configures the hyperparameter search space for learning rate and momentum using Hyperopt\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspace = {\n    \"lr\": hp.loguniform(\"lr\", np.log(1e-5), np.log(1e-1)),\n    \"momentum\": hp.uniform(\"momentum\", 0.0, 1.0),\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Entry Point in pyproject.toml\nDESCRIPTION: Shows how to configure the plugin entry point in the pyproject.toml file. This is necessary for MLflow to detect and load the custom provider.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_22\n\nLANGUAGE: toml\nCODE:\n```\n[project]\nname = \"my_llm\"\nversion = \"1.0\"\n\n[project.entry-points.\"mlflow.gateway.providers\"]\nmy_llm = \"my_llm.providers:MyLLMProvider\"\n\n[tool.setuptools.packages.find]\ninclude = [\"my_llm*\"]\nnamespaces = false\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving Correlation Matrix Plot in Python\nDESCRIPTION: This function creates a correlation matrix heatmap using seaborn and matplotlib, then saves it to a specified local path. It demonstrates how to generate a plot that can be later logged to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/part2-logging-plots/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef plot_correlation_matrix_and_save(\n    df, style=\"seaborn\", plot_size=(10, 8), path=\"/tmp/corr_plot.png\"\n):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n\n        # Calculate the correlation matrix\n        corr = df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(\n            corr,\n            mask=mask,\n            cmap=\"coolwarm\",\n            vmax=0.3,\n            center=0,\n            square=True,\n            linewidths=0.5,\n            annot=True,\n            fmt=\".2f\",\n        )\n\n        ax.set_title(\"Feature Correlation Matrix\", fontsize=14)\n        plt.tight_layout()\n\n    plt.close(fig)\n    # convert to filesystem path spec for os compatibility\n    save_path = pathlib.Path(path)\n    fig.savefig(path)\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving Correlation Matrix Plot in Python\nDESCRIPTION: This function creates a correlation matrix heatmap using seaborn and matplotlib, then saves it to a specified local path. It demonstrates how to generate a plot that can be later logged to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/part2-logging-plots/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef plot_correlation_matrix_and_save(\n    df, style=\"seaborn\", plot_size=(10, 8), path=\"/tmp/corr_plot.png\"\n):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n\n        # Calculate the correlation matrix\n        corr = df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(\n            corr,\n            mask=mask,\n            cmap=\"coolwarm\",\n            vmax=0.3,\n            center=0,\n            square=True,\n            linewidths=0.5,\n            annot=True,\n            fmt=\".2f\",\n        )\n\n        ax.set_title(\"Feature Correlation Matrix\", fontsize=14)\n        plt.tight_layout()\n\n    plt.close(fig)\n    # convert to filesystem path spec for os compatibility\n    save_path = pathlib.Path(path)\n    fig.savefig(path)\n```\n\n----------------------------------------\n\nTITLE: Automatic Code Dependency Inference for MLflow Model in Python\nDESCRIPTION: Example of logging an MLflow Python Function model with automatic code dependency inference enabled, capturing external module dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Optional\n\nfrom custom_code import map_iris_types  # import the external reference\n\nimport mlflow\n\n\nclass FlowerMapping(mlflow.pyfunc.PythonModel):\n    \"\"\"Custom model with an external dependency\"\"\"\n\n    def predict(\n        self, context, model_input, params: Optional[Dict[str, Any]] = None\n    ) -> List[str]:\n        predictions = [pred % 3 for pred in model_input]\n\n        # Call the external function\n        return map_iris_types(predictions)\n\n\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        artifact_path=\"flowers\",\n        python_model=FlowerMapping(),\n        infer_code_paths=True,  # Enabling automatic code dependency inference\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Neural Network Model\nDESCRIPTION: Creates a simple neural network class with two fully connected layers for iris classification.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/logging/pytorch_log_model.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define a simple neural network model\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(4, 10)\n        self.fc2 = nn.Linear(10, 3)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\nmodel = SimpleNN()\nloss = nn.CrossEntropyLoss()\n```\n\n----------------------------------------\n\nTITLE: Disabling Warnings for Tokenizers and Setup\nDESCRIPTION: Initializes the environment by disabling tokenizers parallelism warnings and filtering out less useful warnings from setuptools and pydantic to create a cleaner output experience.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Disable tokenizers warnings when constructing pipelines\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: YAML Safe Dump Reference\nDESCRIPTION: References the safe_dump function from YAML module, used for safely serializing Python objects to YAML format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/h2o/random_forest.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nyaml.safe_dump\n```\n\n----------------------------------------\n\nTITLE: Model Configuration\nDESCRIPTION: Setting up the DistilledBert model for sequence classification with label mappings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/transformers/MLFlow_X_HuggingFace_Finetune_a_text_classification_model.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForSequenceClassification\n\n# Set the mapping between int label and its meaning.\nid2label = {0: \"ham\", 1: \"spam\"}\nlabel2id = {\"ham\": 0, \"spam\": 1}\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2,\n    label2id=label2id,\n    id2label=id2label,\n)\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI with Custom Backend Store\nDESCRIPTION: Commands to change directory and start the MLflow server with a specific backend store URI to view results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ..\nmlflow server --backend-store-uri ./mlflow/mlruns\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Model Signature with OpenAI\nDESCRIPTION: Sets up the MLflow ModelSignature for the OpenAI model, defining input/output schemas and model parameters. Includes logging the base OpenAI model with the instruction set and signature configuration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsignature = ModelSignature(\n    inputs=Schema([ColSpec(type=\"string\", name=None)]),\n    outputs=Schema([ColSpec(type=\"string\", name=None)]),\n    params=ParamSchema(\n        [\n            ParamSpec(name=\"max_tokens\", default=500, dtype=\"long\"),\n            ParamSpec(name=\"temperature\", default=0, dtype=\"float\"),\n        ]\n    ),\n)\n\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-4\",\n        task=openai.chat.completions,\n        artifact_path=\"base_model\",\n        messages=instruction,\n        signature=signature,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Tracking Server Access\nDESCRIPTION: Sets environment variables for MLflow to access the MinIO server as an S3-compatible storage.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_S3_ENDPOINT_URL=http://localhost:9000 # Replace this with remote storage endpoint e.g. s3://my-bucket in real use cases\nexport AWS_ACCESS_KEY_ID=minio_user\nexport AWS_SECRET_ACCESS_KEY=minio_password\n```\n\n----------------------------------------\n\nTITLE: Creating and Registering Prompt in MLflow\nDESCRIPTION: Example of creating and registering a new prompt in MLflow Prompt Registry. This shows the basic syntax for creating a prompt that can later be associated with models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/run-and-model.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Register a new prompt\nprompt = mlflow.register_prompt(\n    name=\"chat-prompt\",\n    template=\"You are an expert in programming. Please answer the user's question about programming.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Topical Analysis of Retriever Results (Python)\nDESCRIPTION: This function performs topical analysis on a list of questions using LDA (Latent Dirichlet Allocation). It tokenizes the questions, removes stop words, and applies the LDA model to identify topics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\nimport pyLDAvis.gensim_models as gensimvis\nfrom gensim import corpora, models\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Initialize NLTK resources\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n\n\ndef topical_analysis(questions: list[str]):\n    stop_words = set(stopwords.words(\"english\"))\n\n    # Tokenize and remove stop words\n    tokenized_data = []\n    for question in questions:\n        tokens = word_tokenize(question.lower())\n        filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n        tokenized_data.append(filtered_tokens)\n\n    # Create a dictionary and corpus\n    dictionary = corpora.Dictionary(tokenized_data)\n    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n\n    # Apply LDA model\n    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n\n    # Get topic distribution for each question\n    topic_distribution = []\n    for i, ques in enumerate(questions):\n        bow = dictionary.doc2bow(tokenized_data[i])\n        topics = lda_model.get_document_topics(bow)\n        topic_distribution.append(topics)\n        print(f\"Question: {ques}\\nTopic: {topics}\")\n\n    # Print all topics\n    print(\"\\nTopics found are:\")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n    return lda_model, corpus, dictionary\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for MLflow and Optuna\nDESCRIPTION: Imports necessary Python libraries including MLflow, Optuna, NumPy, Pandas, XGBoost and scikit-learn for machine learning and hyperparameter optimization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nfrom datetime import datetime, timedelta\n\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Dataset\nDESCRIPTION: Loading the SMS spam dataset from HuggingFace and splitting it into train/test sets with an 80/20 ratio.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/transformers/MLFlow_X_HuggingFace_Finetune_a_text_classification_model.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\n# Load \"sms_spam\" dataset.\nsms_dataset = load_dataset(\"sms_spam\")\n\n# Split train/test by 8:2.\nsms_train_test = sms_dataset[\"train\"].train_test_split(test_size=0.2)\ntrain_dataset = sms_train_test[\"train\"]\ntest_dataset = sms_train_test[\"test\"]\n```\n\n----------------------------------------\n\nTITLE: Basic System Metrics Logging Example\nDESCRIPTION: Example showing how to log system metrics in an MLflow run with a sleep interval to generate metrics data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/system-metrics/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport time\n\nwith mlflow.start_run() as run:\n    time.sleep(15)\n\nprint(mlflow.MlflowClient().get_run(run.info.run_id).data)\n```\n\n----------------------------------------\n\nTITLE: Building CNN Model with Keras 3.0 in Python\nDESCRIPTION: Defines a function to initialize a simple CNN model using Keras 3.0 sequential API for MNIST classification.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/keras/quickstart/quickstart_keras.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNUM_CLASSES = 10\nINPUT_SHAPE = (28, 28, 1)\n\n\ndef initialize_model():\n    return keras.Sequential(\n        [\n            keras.Input(shape=INPUT_SHAPE),\n            keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n            keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n            keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n            keras.layers.GlobalAveragePooling2D(),\n            keras.layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n        ]\n    )\n\n\nmodel = initialize_model()\nmodel.summary()\n```\n\n----------------------------------------\n\nTITLE: Creating and Using MLflow Deployment Client in Python\nDESCRIPTION: This snippet demonstrates how to create an MLflow Deployment Client and use it to list endpoints and make predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client(\"http://my.deployments:8888\")\n\n# List all endpoints\nendpoint = client.list_endpoints()\nfor endpoint in endpoints:\n    print(endpoint)\n\n# Query an endpoint\nresponse = client.predict(\n    endpoint=\"chat\",\n    inputs={\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about rabbits\"}]},\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Dataset for RAG System\nDESCRIPTION: Generates a pandas DataFrame with sample questions to evaluate the RAG system.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\neval_df = pd.DataFrame(\n    {\n        \"questions\": [\n            \"What is MLflow?\",\n            \"How to run mlflow.evaluate()?\",\n            \"How to log_table()?\",\n            \"How to load_table()?\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow UI from Command Line\nDESCRIPTION: Command to launch the MLflow UI server locally, allowing users to view experiments and runs through a web interface. The server runs on port 5000 and serves the UI from the local mlruns directory.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui --port 5000\n```\n\n----------------------------------------\n\nTITLE: Creating Model Signature with Inference Parameters\nDESCRIPTION: Sets up MLflow model signature using infer_signature API to define input/output formats and default inference parameters like max_new_tokens and repetition_penalty.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import infer_signature\n\nsample = train_dataset[1]\n\n# MLflow infers schema from the provided sample input/output/params\nsignature = infer_signature(\n    model_input=sample[\"prompt\"],\n    model_output=sample[\"answer\"],\n    # Parameters are saved with default values if specified\n    params={\"max_new_tokens\": 256, \"repetition_penalty\": 1.15, \"return_full_text\": False},\n)\nsignature\n```\n\n----------------------------------------\n\nTITLE: Logging Single-Element NumPy Arrays as Metrics in Python\nDESCRIPTION: MLflow 1.27.0 adds support for logging single-element NumPy arrays and tensors as metrics using the mlflow.log_metric() API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.log_metric()\n```\n\n----------------------------------------\n\nTITLE: Querying RetrievalQA Model about Bridle-Paths in Yellowstone\nDESCRIPTION: This snippet demonstrates how to use the loaded RetrievalQA model to answer a query about bridle-paths in Yellowstone National Park. It showcases the model's ability to combine specific document information with general knowledge.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nanswer2 = loaded_model.predict(\n    [{\"query\": \"What is a bridle-path and can I use one at Yellowstone?\"}]\n)\n\nprint_formatted_response(answer2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment and Warnings\nDESCRIPTION: Disabling tokenizers parallelism warnings and filtering out less useful UserWarnings from setuptools and pydantic.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/pyfunc-chat-model.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Defining Model Parameters and Inferring Signature for MLflow\nDESCRIPTION: Sets up model parameters and infers the model signature for MLflow logging. The signature captures the expected input/output schema and available parameters for inference, which is crucial for proper model deployment and serving.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define the parameters that we are permitting to be used at inference time, along with their default values if not overridden\nmodel_params = {\"max_length\": 1000}\n\n# Generate the model signature by providing an input, the expected output, and (optionally), parameters available for overriding at inference time\nsignature = mlflow.models.infer_signature(\n    \"This is a sample input sentence.\",\n    mlflow.transformers.generate_signature_output(translation_pipeline, \"This is another sample.\"),\n    params=model_params,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for OpenAI Autologging\nDESCRIPTION: Command to install the necessary Python packages before running the OpenAI autologging examples. This includes tenacity for retry logic, tiktoken for token counting, and OpenAI version 1.17 or greater.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/openai/autologging/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install tenacity tiktoken 'openai>=1.17'\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Conda Environment for MLflow Development\nDESCRIPTION: Commands to create and configure a Conda environment for MLflow development, including installing dependencies and the MLflow package.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name mlflow-dev-env python=3.8\nconda activate mlflow-dev-env\npip install -e '.[extras]' # installs mlflow from current checkout with some useful extra utilities\n\npip install -r requirements/dev-requirements.txt\npip install -e '.[extras]'  # installs mlflow from current checkout\npip install -e tests/resources/mlflow-test-plugin # installs `mlflow-test-plugin` that is required for running certain MLflow tests\n```\n\n----------------------------------------\n\nTITLE: Accessing MLflow Model URI\nDESCRIPTION: Demonstrates how to reference a model URI in MLflow which provides a unique identifier for accessing model artifacts and metadata\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel_uri\n```\n\n----------------------------------------\n\nTITLE: Substituting Attributes in dev.yaml Profile\nDESCRIPTION: This YAML snippet demonstrates how to provide a value for the 'INGEST_CONFIG' attribute in a dev.yaml profile. It specifies a small dataset for development purposes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nINGEST_CONFIG:\n  location: ./data/taxi-small.parquet\n  format: parquet\n```\n\n----------------------------------------\n\nTITLE: Alternative Parameters for XGBoost Training in Bash\nDESCRIPTION: An alternative set of hyperparameters for the XGBoost training script, demonstrating how different values can be tested to optimize model performance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/xgboost/xgboost_native/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --learning-rate 0.4 --colsample-bytree 0.7 --subsample 0.8\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Training Script with Python\nDESCRIPTION: This command executes the LightGBM training script 'train.py' with specific hyperparameters for colsample-bytree and subsample. It demonstrates how to pass command-line arguments to the script.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/lightgbm/lightgbm_native/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --colsample-bytree 0.8 --subsample 0.9\n```\n\n----------------------------------------\n\nTITLE: Batch Inference with MLflow CLI\nDESCRIPTION: Command to run batch inference on a local CSV file using the MLflow CLI. This executes a single prediction job on 'input.csv' and outputs results to 'output.csv'.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models predict -m runs:/<run_id>/model -i input.csv -o output.csv\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries\nDESCRIPTION: Imports necessary Python packages for deep learning, data processing, hyperparameter optimization, and MLflow tracking\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\nfrom mlflow.models import infer_signature\n```\n\n----------------------------------------\n\nTITLE: Logging to MLflow Tracking Server in R\nDESCRIPTION: R code to connect to a remote MLflow tracking server, set the experiment, and log a parameter. This shows how to use the MLflow R API to log data to a tracking server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: r\nCODE:\n```\nlibrary(mlflow)\ninstall_mlflow()\nremote_server_uri = \"...\" # set to your server URI\nmlflow_set_tracking_uri(remote_server_uri)\nmlflow_set_experiment(\"/my-experiment\")\nmlflow_log_param(\"a\", \"1\")\n```\n\n----------------------------------------\n\nTITLE: Running OpenAI Autologging with Module-level Client\nDESCRIPTION: Commands to set the OpenAI API key as an environment variable and run the example script for MLflow autologging with OpenAI's module-level client.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/openai/autologging/README.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key\"\npython examples/openai/autologging/module_client.py\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Scraped MLflow Documentation\nDESCRIPTION: Saves the scraped MLflow documentation to a CSV file and then loads it back. This allows for persistence of the scraped data to avoid repeating the scraping process in future runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.to_csv(SCRAPPED_DATA_PATH, index=False)\ndf = pd.read_csv(SCRAPPED_DATA_PATH)\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series Demand for Apple Sales in Python\nDESCRIPTION: This function creates a time series plot of demand data with a rolling average. It handles date conversion, calculates the rolling average, and plots both original and averaged data. The function returns a figure object for MLflow logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef plot_time_series_demand(data, window_size=7, style=\"seaborn\", plot_size=(16, 12)):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"df must be a pandas DataFrame.\")\n\n    df = data.copy()\n\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n\n    # Calculate the rolling average\n    df[\"rolling_avg\"] = df[\"demand\"].rolling(window=window_size).mean()\n\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n        # Plot the original time series data with low alpha (transparency)\n        ax.plot(df[\"date\"], df[\"demand\"], \"b-o\", label=\"Original Demand\", alpha=0.15)\n        # Plot the rolling average\n        ax.plot(\n            df[\"date\"],\n            df[\"rolling_avg\"],\n            \"r\",\n            label=f\"{window_size}-Day Rolling Average\",\n        )\n\n        # Set labels and title\n        ax.set_title(\n            f\"Time Series Plot of Demand with {window_size} day Rolling Average\",\n            fontsize=14,\n        )\n        ax.set_xlabel(\"Date\", fontsize=12)\n        ax.set_ylabel(\"Demand\", fontsize=12)\n\n        # Add legend to explain the lines\n        ax.legend()\n        plt.tight_layout()\n\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Starting Model Training\nDESCRIPTION: Initiates the training process for the PEFT model using the configured trainer.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Setting Span Types in MLflow Python\nDESCRIPTION: Demonstrates how to set span types using the @mlflow.trace decorator and mlflow.start_span context manager. It shows both built-in and custom span type usage.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tracing-schema.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.entities import SpanType\n\n\n# Using a built-in span type\n@mlflow.trace(span_type=SpanType.RETRIEVER)\ndef retrieve_documents(query: str):\n    ...\n\n\n# Setting a custom span type\nwith mlflow.start_span(name=\"add\", span_type=\"MATH\") as span:\n    span.set_inputs({\"x\": z, \"y\": y})\n    z = x + y\n    span.set_outputs({\"z\": z})\n\n    print(span.span_type)\n    # Output: MATH\n```\n\n----------------------------------------\n\nTITLE: Deploying MLflow Model to SageMaker Endpoint\nDESCRIPTION: This MLflow CLI command deploys the specified MLflow model to an Amazon SageMaker endpoint, configuring instance type, count, and environment variables.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-to-sagemaker/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ mlflow deployments create -t sagemaker -m runs:/<run_id>/model \\\n    -C region_name=<your-region> \\\n    -C instance-type=ml.m4.xlarge \\\n    -C instance-count=1 \\\n    -C env='{\"DISABLE_NGINX\": \"true\"}'\n```\n\n----------------------------------------\n\nTITLE: Logging SHAP Model Explanations in MLflow\nDESCRIPTION: Demonstrates how to log model explanations generated by SHAP using the new mlflow.shap.log_explanation function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.shap.log_explanation()\n```\n\n----------------------------------------\n\nTITLE: LangChain Integration with MLflow Gateway\nDESCRIPTION: Example showing how to use LangChain with MLflow AI Gateway for LLM completions. The code demonstrates creating an LLM chain, running it, and logging the model with MLflow for later deployment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom langchain import LLMChain, PromptTemplate\nfrom langchain.llms import Mlflow\n\nllm = Mlflow(target_uri=\"http://127.0.0.1:5000\", endpoint=\"completions\")\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate(\n        input_variables=[\"adjective\"],\n        template=\"Tell me a {adjective} joke\",\n    ),\n)\nresult = llm_chain.run(adjective=\"funny\")\nprint(result)\n\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(llm_chain, \"model\")\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\nprint(model.predict([{\"adjective\": \"funny\"}]))\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Tracking Server in R\nDESCRIPTION: Wrapper function for launching the MLflow tracking server from R. Configures file storage, artifact storage, networking, and worker settings for the server instance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_47\n\nLANGUAGE: r\nCODE:\n```\nmlflow_server(\n  file_store = \"mlruns\",\n  default_artifact_root = NULL,\n  host = \"127.0.0.1\",\n  port = 5000,\n  workers = NULL,\n  static_prefix = NULL,\n  serve_artifacts = FALSE\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Retriever Evaluation Metrics (Python)\nDESCRIPTION: This code creates a plot to visualize the performance of a retriever model across different metrics (precision, recall, NDCG) at various k values. It uses matplotlib to generate the plot.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\n# Plotting each metric\nfor metric_name in [\"precision\", \"recall\", \"ndcg\"]:\n    y = [evaluate_results.metrics[f\"{metric_name}_at_{k}/mean\"] for k in range(1, 4)]\n    plt.plot([1, 2, 3], y, label=f\"{metric_name}@k\")\n\n# Adding labels and title\nplt.xlabel(\"k\")\nplt.ylabel(\"Metric Value\")\nplt.title(\"Metrics Comparison at Different Ks\")\n# Setting x-axis ticks\nplt.xticks([1, 2, 3])\nplt.legend()\n\n# Display the plot\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Cluster - Bash\nDESCRIPTION: Command to initialize a Ray cluster head node.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/ray_serve/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nray start --head\n```\n\n----------------------------------------\n\nTITLE: Using Pydantic BaseModel Type Hints with Inheritance in MLflow\nDESCRIPTION: Demonstrates how to use a Pydantic BaseModel as a type hint while preserving fields in subclasses. The example shows inheritance with SystemMessage and UserMessage classes and how MLflow handles data conversion.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ConfigDict\nfrom mlflow.pyfunc.utils import pyfunc\n\n\nclass BaseMessage(BaseModel):\n    # set extra='allow' to allow extra fields in the subclass\n    model_config = ConfigDict(extra=\"allow\")\n\n    role: str\n    content: str\n\n\nclass SystemMessage(BaseMessage):\n    system_prompt: str\n\n\nclass UserMessage(BaseMessage):\n    user_prompt: str\n\n\n@pyfunc\ndef predict(model_input: list[BaseMessage]) -> list[str]:\n    result = []\n    for msg in model_input:\n        if hasattr(msg, \"system_prompt\"):\n            result.append(msg.system_prompt)\n        elif hasattr(msg, \"user_prompt\"):\n            result.append(msg.user_prompt)\n    return result\n\n\ninput_example = [\n    {\"role\": \"system\", \"content\": \"Hello\", \"system_prompt\": \"Hi\"},\n    {\"role\": \"user\", \"content\": \"Hi\", \"user_prompt\": \"Hello\"},\n]\nprint(predict(input_example))  # Output: ['Hi', 'Hello']\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Model for Predictions\nDESCRIPTION: Demonstrates how to use the loaded model to make predictions on sample input data using a Pandas DataFrame.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/introduction.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# Define a sample input DataFrame\nmodel_input = pd.DataFrame([range(10)])\n\n# Use the loaded model to make predictions\nmodel_output = loaded_model.predict(model_input)\n```\n\n----------------------------------------\n\nTITLE: Training Configuration\nDESCRIPTION: Setting up the training arguments and trainer for model finetuning with specified hyperparameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/transformers/MLFlow_X_HuggingFace_Finetune_a_text_classification_model.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import Trainer, TrainingArguments\n\n# Checkpoints will be output to this `training_output_dir`.\ntraining_output_dir = \"sms_trainer\"\ntraining_args = TrainingArguments(\n    output_dir=training_output_dir,\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_steps=8,\n    num_train_epochs=3,\n)\n\n# Put things together with a `Trainer` instance.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tokenized,\n    eval_dataset=test_tokenized,\n    compute_metrics=compute_metrics,\n)\n```\n\n----------------------------------------\n\nTITLE: Model Training Execution\nDESCRIPTION: Starting the model training process with MLflow tracking.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/transformers/MLFlow_X_HuggingFace_Finetune_a_text_classification_model.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Configuring JFrog MLflow Plugin Options\nDESCRIPTION: Bash commands to set additional configuration options for the JFrog MLflow plugin, including SSL settings, debug logging, and artifact deletion behavior.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nexport ARTIFACTORY_NO_SSL=true\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport ARTIFACTORY_DEBUG=true\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport ARTIFACTORY_ARTIFACTS_DELETE_SKIP=true\n```\n\n----------------------------------------\n\nTITLE: Defining Improved Prompt for Lyrics Correction Model\nDESCRIPTION: This snippet shows an improved prompt for the lyrics correction model. The new prompt provides clearer instructions and examples for rating the creativity of misheard lyrics on a scale of 1 to 3.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-quickstart.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define our prompt\nimproved_lyrics_prompt = (\n    \"Here's a misheard lyric: {lyric}. What's the actual lyric, which song does it come from, which artist performed it, and can \"\n    \"you give a funny explanation as to why the misheard version doesn't make sense? Additionally, please provide an objective rating to the \"\n    \"misheard lyric on a scale of 1 to 3, where 1 is 'not particularly creative' (minimal humor, closely resembles the \"\n    \"original lyrics and the intent of the song) and 3 is 'hilariously creative' (highly original, very humorous, significantly different from \"\n    \"the original). Explain your rating briefly. For example, 'I left my heart in San Francisco' misheard as 'I left my hat in San Francisco' \"\n    \"might be a 1, as it's a simple word swap with minimal humor. Conversely, 'I want to hold your hand' misheard as 'I want to steal your land' \"\n    \"could be a 3, as it significantly changes the meaning in a humorous and unexpected way.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Logging to MLflow Tracking Server in Scala\nDESCRIPTION: Scala code to connect to a remote MLflow tracking server, create an experiment, and log a parameter. This demonstrates using the MLflow Scala API to interact with a tracking server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nimport org.mlflow.tracking.MlflowClient\n\nval remoteServerUri = \"...\" // set to your server URI\nval client = new MlflowClient(remoteServerUri)\n\nval experimentId = client.createExperiment(\"my-experiment\")\nclient.setExperiment(experimentId)\n\nval run = client.createRun(experimentId)\nclient.logParam(run.getRunId(), \"a\", \"1\")\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Experiment for LLM Model Logging\nDESCRIPTION: Code to set up the MLflow experiment where the custom model will be logged. This creates a dedicated space for tracking the model and its related artifacts.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(experiment_name=\"mpt-7b-instruct-evaluation\")\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow AI Gateway from Repository\nDESCRIPTION: Command to install MLflow with genai extras in development mode from a local repository clone, suitable for development or accessing the latest features.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# Installation from the repository\npip install -e '.[genai]'\n```\n\n----------------------------------------\n\nTITLE: Creating a Model Signature for MLflow Transformers Pipeline\nDESCRIPTION: Creates an MLflow model signature for a Transformers pipeline. The signature defines the expected input, output, and parameters for the model, which MLflow will enforce during inference. It uses sample input and output generation to infer the appropriate signature structure.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/prompt-templating/prompt-templating.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nsample_input = \"Tell me the largest bird\"\nparams = {\"max_new_tokens\": 15}\nsignature = mlflow.models.infer_signature(\n    sample_input,\n    mlflow.transformers.generate_signature_output(generator, sample_input, params=params),\n    params=params,\n)\n\n# visualize the signature\nsignature\n```\n\n----------------------------------------\n\nTITLE: MLflow UI Usage Examples in R\nDESCRIPTION: Examples demonstrating how to launch the MLflow UI locally or connect to an existing MLflow tracking server to view experiment data and models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_58\n\nLANGUAGE: r\nCODE:\n```\nlibrary(mlflow)\n\n# launch mlflow ui locally\nmlflow_ui()\n\n# launch mlflow ui for existing mlflow server\nmlflow_set_tracking_uri(\"http://tracking-server:5000\")\nmlflow_ui()\n```\n\n----------------------------------------\n\nTITLE: Deleting Traces by Timestamp with MLflow Python Client\nDESCRIPTION: Demonstrates how to delete traces based on specific criteria using the MlflowClient.delete_traces method, which allows deletion by experiment ID, maximum timestamp, or request IDs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/how-to.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\n# Get the current timestamp in milliseconds\ncurrent_time = int(time.time() * 1000)\n\n# Delete traces older than a specific timestamp\ndeleted_count = client.delete_traces(\n    experiment_id=\"1\", max_timestamp_millis=current_time, max_traces=10\n)\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Status with nvidia-smi\nDESCRIPTION: Shell command to display NVIDIA GPU information and verify available VRAM\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnvidia-smi\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Recipes with Different Profiles using CLI\nDESCRIPTION: This shell script demonstrates how to use the MLflow CLI to run recipes with different profile customizations. It shows examples of running regression recipes with ElasticNet and SGD regressors, as well as running in a shared workspace.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/mlflow/recipes-regression-template\ncd recipes-regression-template\n# Run the regression recipe to train and evaluate the performance of an ElasticNet regressor\nmlflow recipes run --profile local-elasticnet\n# Run the recipe again to train and evaluate the performance of an SGD regressor\nmlflow recipes run --profile local-sgd\n# After finding the best model type and updating the 'shared-workspace' profile accordingly,\n# run the recipe again to retrain the best model in a workspace where teammates can view it\nmlflow recipes run --profile shared-workspace\n```\n\n----------------------------------------\n\nTITLE: Logging LangGraph Model with Automatic Prompt Detection\nDESCRIPTION: Example of logging a LangGraph model defined in a Python file to MLflow. This demonstrates the Models-from-Code feature with automatic prompt detection, where MLflow identifies and logs prompt references in the code.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/run-and-model.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(\n        lc_model=\"./chatbot.py\",\n        artifact_path=\"graph\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Saving and Reading Scraped Document Data\nDESCRIPTION: Saves the scraped MLflow documentation to a CSV file and then reads it back into a DataFrame to avoid re-scraping in future runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.to_csv(SCRAPPED_DATA_PATH, index=False)\ndf = pd.read_csv(SCRAPPED_DATA_PATH)\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample Document Chunk Content\nDESCRIPTION: Retrieves and displays the content of the first document chunk to verify the chunking process and examine the text that will be used for question generation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df[\"chunk\"][0]\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Serve - Bash\nDESCRIPTION: Command to start a Ray Serve instance on the Ray cluster.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/ray_serve/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nserve start\n```\n\n----------------------------------------\n\nTITLE: Create Registered Model Permission API Endpoint\nDESCRIPTION: REST endpoint for creating permissions on registered models. Requires model name, username, and permission level as input parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/auth/rest-api.rst#2025-04-07_snippet_0\n\nLANGUAGE: rest\nCODE:\n```\n2.0/mlflow/registered-models/permissions/create\n```\n\n----------------------------------------\n\nTITLE: Searching MLflow Runs with Filters in Java\nDESCRIPTION: Shows how to search for MLflow runs using the Java API. In Java, the entire conditional filter syntax is string-encapsulated since the Java API is a thin wrapper around the Python core APIs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nList<Long> experimentIds = Arrays.asList(\"1\", \"2\", \"4\", \"8\");\nList<RunInfo> searchResult = client.searchRuns(experimentIds, \"metrics.accuracy_score < 99.90\");\n```\n\n----------------------------------------\n\nTITLE: Model Preparation and Signature Generation with Sentence Transformers\nDESCRIPTION: Prepares a pre-trained Sentence Transformer model, creates input examples, and generates model signatures for MLflow integration. Includes model saving and test output calculation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-similarity/semantic-similarity-sentence-transformers.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load a pre-trained sentence transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Create an input example DataFrame\ninput_example = pd.DataFrame([{\"sentence_1\": \"I like apples\", \"sentence_2\": \"I like oranges\"}])\n\n# Save the model in the /tmp directory\nmodel_directory = \"/tmp/sbert_model\"\nmodel.save(model_directory)\n\n# Define artifacts with the absolute path\nartifacts = {\"model_path\": model_directory}\n\n# Generate test output for signature\ntest_output = np.array(\n    util.cos_sim(\n        model.encode(input_example[\"sentence_1\"][0]), model.encode(input_example[\"sentence_2\"][0])\n    ).tolist()\n)\n\n# Define the signature associated with the model\nsignature = infer_signature(input_example, test_output)\n\n# Visualize the signature\nsignature\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Server with Custom Authentication Plugin (Bash)\nDESCRIPTION: This bash command shows how to start the MLflow server using a custom authentication plugin. It specifies the plugin name using the --app-name argument.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --app-name my-auth\n```\n\n----------------------------------------\n\nTITLE: Enabling Universal Autologging in MLflow\nDESCRIPTION: Demonstrates how to enable autologging for all supported integrations in MLflow using the new universal autolog function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.autolog()\n```\n\n----------------------------------------\n\nTITLE: Sending JSON Input with Parameters to MLflow Model Server\nDESCRIPTION: Curl command to send a JSON input with additional parameters to the local MLflow model server for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{\n    \"inputs\": {\"question\": [\"What color is it?\"],\n                \"context\": [\"Some people said it was green but I know that it is pink.\"]},\n    \"params\": {\"max_answer_len\": 10}\n}'\n```\n\n----------------------------------------\n\nTITLE: Loading MLmodel Specifications from Remote Locations\nDESCRIPTION: Extends the Model.load() method to support loading MLmodel specifications from remote locations. This allows users to load model metadata from cloud storage or other remote sources.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nModel.load()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Python Model for Enhanced Output Formatting\nDESCRIPTION: Creates a custom CodeHelper class that inherits from PythonModel to improve the formatting of OpenAI model outputs. Includes methods for loading the model context, formatting responses, and making predictions with enhanced readability.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CodeHelper(PythonModel):\n    def __init__(self):\n        self.model = None\n\n    def load_context(self, context):\n        self.model = mlflow.pyfunc.load_model(context.artifacts[\"model_path\"])\n\n    @staticmethod\n    def _format_response(response):\n        formatted_output = \"\"\n        in_code_block = False\n\n        for item in response:\n            lines = item.split(\"\\n\")\n            for line in lines:\n                # Check for the start/end of a code block\n                if line.strip().startswith(\"```\"):\n                    in_code_block = not in_code_block\n                    formatted_output += line + \"\\n\"\n                    continue\n\n                if in_code_block:\n                    # Don't wrap lines inside code blocks\n                    formatted_output += line + \"\\n\"\n                else:\n                    # Wrap lines outside of code blocks\n                    wrapped_lines = textwrap.fill(line, width=80)\n                    formatted_output += wrapped_lines + \"\\n\"\n\n        return formatted_output\n\n    def predict(self, context, model_input, params):\n        # Call the loaded OpenAI model instance to get the raw response\n        raw_response = self.model.predict(model_input, params=params)\n\n        # Return the formatted response so that it is easier to read\n        return self._format_response(raw_response)\n```\n\n----------------------------------------\n\nTITLE: Executing Refined Hyperparameter Tuning in Python\nDESCRIPTION: This snippet demonstrates how to execute a refined hyperparameter tuning process. It uses specific parameter value subsets and tags runs with a unique identifier for easy filtering in the MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nparam_1_values = [\"b\", \"c\"]\nparam_2_values = [\"d\", \"f\"]\nident = \"params_test_3\"\nconsume(starmap(execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))))\n```\n\n----------------------------------------\n\nTITLE: Logging BERT Fill Mask Model\nDESCRIPTION: Python code to create and log a BERT model for mask filling using MLflow transformers flavor.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport mlflow\n\n\nlm_architecture = \"bert-base-cased\"\nartifact_path = \"mask_fill_model\"\n\ntokenizer = AutoTokenizer.from_pretrained(lm_architecture)\nmodel = AutoModelForMaskedLM.from_pretrained(lm_architecture)\n\ncomponents = {\"model\": model, \"tokenizer\": tokenizer}\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=components,\n        artifact_path=artifact_path,\n    )\n    model_uri = mlflow.get_artifact_uri(artifact_path)\n```\n\n----------------------------------------\n\nTITLE: Basic Ollama API Usage with Python\nDESCRIPTION: Demonstrates how to interact with an Ollama model using the Python client library, showing basic message handling and response formatting.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-intro/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ollama\nfrom ollama import Options\nfrom rich import print\n\nresponse = ollama.chat(\n    model=\"llama3.2:1b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is MLflow Tracking?\",\n        }\n    ],\n    options=Options({\"num_predict\": 25}),\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: MLflow Server Output\nDESCRIPTION: Example output when starting the MLflow server, showing the server listening on the specified host and port, and the worker processes being initialized.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/step2-explore-registered-model/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n[2024-02-05 12:43:10 -0500] [26393] [INFO] Starting gunicorn 20.1.0\n[2024-02-05 12:43:10 -0500] [26393] [INFO] Listening at: http://127.0.0.1:8080 (26393)\n[2024-02-05 12:43:10 -0500] [26393] [INFO] Using worker: sync\n[2024-02-05 12:43:10 -0500] [26414] [INFO] Booting worker with pid: 26414\n[2024-02-05 12:43:11 -0500] [26416] [INFO] Booting worker with pid: 26416\n[2024-02-05 12:43:11 -0500] [26428] [INFO] Booting worker with pid: 26428\n[2024-02-05 12:43:11 -0500] [26437] [INFO] Booting worker with pid: 26437\n```\n\n----------------------------------------\n\nTITLE: Loading Serving Payload Example in Python for MLflow Models\nDESCRIPTION: This snippet demonstrates how to load the serving payload from a logged MLflow model using the 'load_serving_example' function. It includes creating an input example, logging the model, and printing the serving example.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models.utils import load_serving_example\n\ninput_example = {\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"assistant\", \"content\": \"What would you like to ask?\"},\n        {\"role\": \"user\", \"content\": \"Who owns MLflow?\"},\n    ]\n}\nmodel_info = mlflow.langchain.log_model(..., input_example=input_example)\nprint(f\"model_uri: {model_info.model_uri}\")\nserving_example = load_serving_example(model_info.model_uri)\nprint(f\"serving_example: {serving_example}\")\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI for Run Inspection\nDESCRIPTION: Command to launch the MLflow UI for inspecting parameters, metrics, and artifacts logged by the mlflow.fastai.autolog() API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/fastai/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Component-Based Model Initialization\nDESCRIPTION: Shows initialization of transformers model components for text classification task.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport transformers\n\ntask = \"text-classification\"\narchitecture = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(architecture)\ntokenizer = transformers.AutoTokenizer.from_pretrained(architecture)\n```\n\n----------------------------------------\n\nTITLE: Component-Based Model Initialization\nDESCRIPTION: Shows initialization of transformers model components for text classification task.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport transformers\n\ntask = \"text-classification\"\narchitecture = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(architecture)\ntokenizer = transformers.AutoTokenizer.from_pretrained(architecture)\n```\n\n----------------------------------------\n\nTITLE: Loading a Prompt Using an Alias in Python\nDESCRIPTION: This code shows how to load a prompt using an alias rather than a specific version number. This approach supports flexible deployment strategies by allowing version changes without code modifications.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/cm.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprompt = mlflow.load_prompt(\"prompts:/summarization-prompt@production\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation Results Table in MLflow\nDESCRIPTION: Retrieves and displays the evaluation results table from the MLflow evaluation run, which contains detailed metrics for each test question.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresults.tables[\"eval_results_table\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment with Remote Registry Image\nDESCRIPTION: YAML configuration showing how to use a Docker image from a remote registry (AWS ECR) for an MLflow project, including a specific image tag.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndocker_env:\n  image: 012345678910.dkr.ecr.us-west-2.amazonaws.com/mlflow-docker-example-environment:7.0\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost Models with Specified Format\nDESCRIPTION: Saves or logs XGBoost models using mlflow.xgboost.save_model() or mlflow.xgboost.log_model() with a specified model_format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmlflow.xgboost.save_model(model_format=...)\n```\n\nLANGUAGE: python\nCODE:\n```\nmlflow.xgboost.log_model(model_format=...)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install the necessary Python packages including beautifulsoup4, FAISS, LangChain, and OpenAI with specific version requirements to ensure compatibility.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install beautifulsoup4 faiss-cpu==1.7.4 langchain==0.1.16 langchain-community==0.0.33 langchain-openai==0.0.8 openai==1.12.0 tiktoken==0.6.0\n```\n\n----------------------------------------\n\nTITLE: Loading and Using the MLflow Model\nDESCRIPTION: Demonstrates loading the saved model as a PyFunc model and using it to make predictions. The same validation rules apply to the loaded model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# load the pyfunc model\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\n# the same validation works for pyfunc model predict\npyfunc_model.predict(messages)\n```\n\n----------------------------------------\n\nTITLE: Running BERT Classification Script Directly with Custom Parameters\nDESCRIPTION: This command runs the bert_classification.py script directly with custom parameters for trainer settings, data configuration, and model hyperparameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/BertNewsClassification/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython bert_classification.py \\\n    --trainer.max_epochs 5 \\\n    --trainer.devices 1 \\\n    --trainer.strategy \"ddp\" \\\n    --trainer.accelerator \"gpu\" \\\n    --data.batch_size 64 \\\n    --data.num_workers 3 \\\n    --data.num_samples 2000 \\\n    --model.lr 0.001 \\\n    --data.dataset \"20newsgroups\"\n```\n\n----------------------------------------\n\nTITLE: Comparing MLflow LLMs and Plugins Documentation Pages in Python\nDESCRIPTION: Example demonstrating content comparison between MLflow's LLMs page and Plugins page. The code executes the compare_pages function with specific URLs and content IDs, then outputs the calculated cosine similarity and Euclidean distance metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Get the similarity between the main LLMs page in the MLflow Docs and the Plugins page for the 2.8.1 release of MLflow\n\nplugins_cosine, plugins_euclid = compare_pages(\n    url1=\"https://www.mlflow.org/docs/2.8.1/llms/index.html\",\n    url2=\"https://www.mlflow.org/docs/2.8.1/plugins.html\",\n    id1=\"llms\",\n    id2=\"mflow-plugins\",\n)\n\nprint(\n    f\"The cosine similarity between the LLMs page and the MLflow Projects page is: {plugins_cosine} and the euclidean distance is: {plugins_euclid}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI in Shell for RAPIDS Example\nDESCRIPTION: This shell command sets the MLflow tracking URI to a local SQLite database. This configuration is used to store experiment and run data for the RAPIDS MLflow project.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport MLFLOW_TRACKING_URI=sqlite:////tmp/mlflow-db.sqlite\n```\n\n----------------------------------------\n\nTITLE: Preparing Evaluation Dataset for Prompt Testing\nDESCRIPTION: Creates a sample dataset with input texts and target summaries for evaluating the summarization prompt. The dataset contains five different topics with corresponding single-sentence summaries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/evaluate.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\neval_data = pd.DataFrame(\n    {\n        \"inputs\": [\n            \"Artificial intelligence has transformed how businesses operate in the 21st century. Companies are leveraging AI for everything from customer service to supply chain optimization. The technology enables automation of routine tasks, freeing human workers for more creative endeavors. However, concerns about job displacement and ethical implications remain significant. Many experts argue that AI will ultimately create more jobs than it eliminates, though the transition may be challenging.\",\n            \"Climate change continues to affect ecosystems worldwide at an alarming rate. Rising global temperatures have led to more frequent extreme weather events including hurricanes, floods, and wildfires. Polar ice caps are melting faster than predicted, contributing to sea level rise that threatens coastal communities. Scientists warn that without immediate and dramatic reductions in greenhouse gas emissions, many of these changes may become irreversible. International cooperation remains essential but politically challenging.\",\n            \"The human genome project was completed in 2003 after 13 years of international collaborative research. It successfully mapped all of the genes of the human genome, approximately 20,000-25,000 genes in total. The project cost nearly $3 billion but has enabled countless medical advances and spawned new fields like pharmacogenomics. The knowledge gained has dramatically improved our understanding of genetic diseases and opened pathways to personalized medicine. Today, a complete human genome can be sequenced in under a day for about $1,000.\",\n            \"Remote work adoption accelerated dramatically during the COVID-19 pandemic. Organizations that had previously resisted flexible work arrangements were forced to implement digital collaboration tools and virtual workflows. Many companies reported surprising productivity gains, though concerns about company culture and collaboration persisted. After the pandemic, a hybrid model emerged as the preferred approach for many businesses, combining in-office and remote work. This shift has profound implications for urban planning, commercial real estate, and work-life balance.\",\n            \"Quantum computing represents a fundamental shift in computational capability. Unlike classical computers that use bits as either 0 or 1, quantum computers use quantum bits or qubits that can exist in multiple states simultaneously. This property, known as superposition, theoretically allows quantum computers to solve certain problems exponentially faster than classical computers. Major technology companies and governments are investing billions in quantum research. Fields like cryptography, material science, and drug discovery are expected to be revolutionized once quantum computers reach practical scale.\",\n        ],\n        \"targets\": [\n            \"AI has revolutionized business operations through automation and optimization, though ethical concerns about job displacement persist alongside predictions that AI will ultimately create more employment opportunities than it eliminates.\",\n            \"Climate change is causing accelerating environmental damage through extreme weather events and melting ice caps, with scientists warning that without immediate reduction in greenhouse gas emissions, many changes may become irreversible.\",\n            \"The Human Genome Project, completed in 2003, mapped approximately 20,000-25,000 human genes at a cost of $3 billion, enabling medical advances, improving understanding of genetic diseases, and establishing the foundation for personalized medicine.\",\n            \"The COVID-19 pandemic forced widespread adoption of remote work, revealing unexpected productivity benefits despite collaboration challenges, and resulting in a hybrid work model that impacts urban planning, real estate, and work-life balance.\",\n            \"Quantum computing uses qubits existing in multiple simultaneous states to potentially solve certain problems exponentially faster than classical computers, with major investment from tech companies and governments anticipating revolutionary applications in cryptography, materials science, and pharmaceutical research.\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing Evaluation Dataset for Prompt Testing\nDESCRIPTION: Creates a sample dataset with input texts and target summaries for evaluating the summarization prompt. The dataset contains five different topics with corresponding single-sentence summaries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/evaluate.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\neval_data = pd.DataFrame(\n    {\n        \"inputs\": [\n            \"Artificial intelligence has transformed how businesses operate in the 21st century. Companies are leveraging AI for everything from customer service to supply chain optimization. The technology enables automation of routine tasks, freeing human workers for more creative endeavors. However, concerns about job displacement and ethical implications remain significant. Many experts argue that AI will ultimately create more jobs than it eliminates, though the transition may be challenging.\",\n            \"Climate change continues to affect ecosystems worldwide at an alarming rate. Rising global temperatures have led to more frequent extreme weather events including hurricanes, floods, and wildfires. Polar ice caps are melting faster than predicted, contributing to sea level rise that threatens coastal communities. Scientists warn that without immediate and dramatic reductions in greenhouse gas emissions, many of these changes may become irreversible. International cooperation remains essential but politically challenging.\",\n            \"The human genome project was completed in 2003 after 13 years of international collaborative research. It successfully mapped all of the genes of the human genome, approximately 20,000-25,000 genes in total. The project cost nearly $3 billion but has enabled countless medical advances and spawned new fields like pharmacogenomics. The knowledge gained has dramatically improved our understanding of genetic diseases and opened pathways to personalized medicine. Today, a complete human genome can be sequenced in under a day for about $1,000.\",\n            \"Remote work adoption accelerated dramatically during the COVID-19 pandemic. Organizations that had previously resisted flexible work arrangements were forced to implement digital collaboration tools and virtual workflows. Many companies reported surprising productivity gains, though concerns about company culture and collaboration persisted. After the pandemic, a hybrid model emerged as the preferred approach for many businesses, combining in-office and remote work. This shift has profound implications for urban planning, commercial real estate, and work-life balance.\",\n            \"Quantum computing represents a fundamental shift in computational capability. Unlike classical computers that use bits as either 0 or 1, quantum computers use quantum bits or qubits that can exist in multiple states simultaneously. This property, known as superposition, theoretically allows quantum computers to solve certain problems exponentially faster than classical computers. Major technology companies and governments are investing billions in quantum research. Fields like cryptography, material science, and drug discovery are expected to be revolutionized once quantum computers reach practical scale.\",\n        ],\n        \"targets\": [\n            \"AI has revolutionized business operations through automation and optimization, though ethical concerns about job displacement persist alongside predictions that AI will ultimately create more employment opportunities than it eliminates.\",\n            \"Climate change is causing accelerating environmental damage through extreme weather events and melting ice caps, with scientists warning that without immediate reduction in greenhouse gas emissions, many changes may become irreversible.\",\n            \"The Human Genome Project, completed in 2003, mapped approximately 20,000-25,000 human genes at a cost of $3 billion, enabling medical advances, improving understanding of genetic diseases, and establishing the foundation for personalized medicine.\",\n            \"The COVID-19 pandemic forced widespread adoption of remote work, revealing unexpected productivity benefits despite collaboration challenges, and resulting in a hybrid work model that impacts urban planning, real estate, and work-life balance.\",\n            \"Quantum computing uses qubits existing in multiple simultaneous states to potentially solve certain problems exponentially faster than classical computers, with major investment from tech companies and governments anticipating revolutionary applications in cryptography, materials science, and pharmaceutical research.\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Large Artifact in MLflow with Python\nDESCRIPTION: This example demonstrates how to log a large artifact to MLflow, which may cause timeout issues on the server side.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_tracking_uri(\"<TRACKING_SERVER_URI>\")\nwith mlflow.start_run():\n    mlflow.log_artifact(\"large.txt\")\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Auto-Tracing for LlamaIndex\nDESCRIPTION: Basic code to enable automatic tracing for LlamaIndex applications using MLflow's autolog function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/llama_index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.llama_index.autolog()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Input Validation Error\nDESCRIPTION: Shows what happens when an incorrect input format is provided to the model. The type hints will trigger a validation error because the input doesn't match the expected list of Message objects.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# An incorrect input will trigger validation error\nmodel.predict([\"What is DSPy?\"])\n```\n\n----------------------------------------\n\nTITLE: Memory-Efficient Model Logging in MLflow Transformers\nDESCRIPTION: Demonstrates logging a model by specifying a path to a local checkpoint, avoiding loading the full model into memory. This approach maintains high disk usage but significantly reduces memory requirements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/large-models/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        # Pass a path to local checkpoint as a model\n        transformers_model=\"/path/to/local/checkpoint\",\n        # Task argument is required for this saving mode.\n        task=\"text-generation\",\n        artifact_path=\"model\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip\nDESCRIPTION: Installation of necessary Python packages including MLflow, OpenAI, ChromaDB, LangChain, and related dependencies for RAG system implementation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install mlflow>=2.8.1\n%pip install openai\n%pip install chromadb==0.4.15\n%pip install langchain==0.0.348\n%pip install tiktoken\n%pip install 'mlflow[genai]'\n%pip install databricks-sdk --upgrade\n```\n\n----------------------------------------\n\nTITLE: Enabling Asynchronous Trace Logging in MLflow\nDESCRIPTION: Shows how to enable asynchronous logging for MLflow traces to reduce performance overhead, especially when the MLflow Tracking Server is running on a remote server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/how-to.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.config.enable_async_logging()\n\n# Traces will be logged asynchronously\nwith mlflow.start_span(name=\"foo\") as span:\n    span.set_inputs({\"a\": 1})\n    span.set_outputs({\"b\": 2})\n\n# If you don't see the traces in the UI after waiting for a while, you can manually flush the traces\n# mlflow.flush_trace_async_logging()\n```\n\n----------------------------------------\n\nTITLE: Accessing Latest Model Version\nDESCRIPTION: Example showing model URI syntax for accessing the latest version of a model regardless of stage, introduced in version 1.22.0\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nmodel_uri = \"models:/my_model/latest\"\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Authentication Plugin in MLflow Setup (Python)\nDESCRIPTION: This Python code snippet demonstrates how to register a custom authentication plugin in the MLflow setup.py file. It adds entry points for the plugin's app factory and client class.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nsetup(\n    ...,\n    entry_points=\"\"\"\n        ...\n\n        [mlflow.app]\n        my-auth=my_auth:create_app\n\n        [mlflow.app.client]\n        my-auth=my_auth:MyAuthClient\n    \"\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: React Tabs Component Import in Markdown Documentation\nDESCRIPTION: Imports the Tabs and TabItem components from the theme, along with a Platform component for use in the MLflow documentation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\nimport Platform from \"@site/src/content/platform.mdx\"\n```\n\n----------------------------------------\n\nTITLE: Loading MLflow Model for Inference\nDESCRIPTION: Loads a saved MLflow model using its run ID for making predictions in a Python environment\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/prompt-engineering/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nlogged_model = \"runs:/8451075c46964f82b85fe16c3d2b7ea0/model\"\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n```\n\n----------------------------------------\n\nTITLE: Accessing MlflowClient from the Main MLflow Module\nDESCRIPTION: The MlflowClient class can now be imported directly from the main mlflow module, providing a more intuitive and convenient import path.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nfrom mlflow import MlflowClient\n```\n\n----------------------------------------\n\nTITLE: Using Aliyun OSS as MLflow Artifact Store\nDESCRIPTION: Example demonstrating how to use Alibaba Cloud OSS as an artifact store with a custom Python model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport mlflow.pyfunc\n\n\nclass Mod(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input, params=None):\n        return 7\n\n\nexp_name = \"myexp\"\nmlflow.create_experiment(exp_name, artifact_location=\"oss://mlflow-test/\")\nmlflow.set_experiment(exp_name)\nmlflow.pyfunc.log_model(\"model_test\", python_model=Mod())\n```\n\n----------------------------------------\n\nTITLE: Running Simple MLflow Spark UDF Example in Python\nDESCRIPTION: This command demonstrates how to run the basic spark_udf.py example, which uses a sklearn model for inference via Spark UDF in a reproducible environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/spark_udf/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython spark_udf.py\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow Tracking Server\nDESCRIPTION: Starts the MLflow Tracking Server with specified backend and artifact stores.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server \\\n  --backend-store-uri postgresql://user:password@localhost:5432/mlflowdb \\\n  --artifacts-destination s3://bucket \\\n  --host 0.0.0.0 \\\n  --port 5000\n```\n\n----------------------------------------\n\nTITLE: Creating Example MLflow Runs with Metrics and Parameters\nDESCRIPTION: Python script that creates 10 MLflow runs with various metrics, parameters, tags and dataset information for demonstration purposes\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport numpy as np\n\nmlflow.set_experiment(\"search-run-guide\")\n\naccuracy = np.arange(0, 1, 0.1)\nloss = np.arange(1, 0, -0.1)\nlog_scale_loss = np.log(loss)\nf1_score = np.arange(0, 1, 0.1)\n\nbatch_size = [2] * 5 + [4] * 5\nlearning_rate = [0.001, 0.01] * 5\nmodel = [\"GPT-2\", \"GPT-3\", \"GPT-3.5\", \"GPT-4\"] + [None] * 6\n\ntask = [\"classification\", \"regression\", \"causal lm\"] + [None] * 7\nenvironment = [\"notebook\"] * 5 + [None] * 5\n\ndataset_name = [\"custom\"] * 5 + [\"also custom\"] * 5\ndataset_digest = [\"s8ds293b\", \"jks834s2\"] + [None] * 8\ndataset_context = [\"train\"] * 5 + [\"test\"] * 5\n\nfor i in range(10):\n    with mlflow.start_run():\n        mlflow.log_metrics(\n            {\n                \"loss\": loss[i],\n                \"accuracy\": accuracy[i],\n                \"log-scale-loss\": log_scale_loss[i],\n                \"f1 score\": f1_score[i],\n            }\n        )\n\n        mlflow.log_params(\n            {\n                \"batch_size\": batch_size[i],\n                \"learning rate\": learning_rate[i],\n                \"model\": model[i],\n            }\n        )\n\n        mlflow.set_tags(\n            {\n                \"task\": task[i],\n                \"environment\": environment[i],\n            }\n        )\n\n        dataset = mlflow.data.from_numpy(\n            features=np.random.uniform(size=[20, 28, 28, 3]),\n            targets=np.random.randint(0, 10, size=[20]),\n            name=dataset_name[i],\n            digest=dataset_digest[i],\n        )\n        mlflow.log_input(dataset, context=dataset_context[i])\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Registry API Endpoints Table\nDESCRIPTION: HTML table containing MLflow model registry API endpoints including paths, methods, and required permissions for operations like version management, tagging, and aliases.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<table>\n  <tbody>\n    <tr>\n      <td>`2.0/mlflow/registered-models/get-latest-versions`</td>\n      <td>`POST`</td>\n      <td>can_read</td>\n    </tr>\n    <!-- Additional endpoints -->\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperopt Objective Function\nDESCRIPTION: Creates the objective function for hyperparameter optimization using Hyperopt\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef objective(params):\n    # MLflow will track the parameters and results for each run\n    result = train_model(\n        params,\n        epochs=3,\n        train_x=train_x,\n        train_y=train_y,\n        valid_x=valid_x,\n        valid_y=valid_y,\n        test_x=test_x,\n        test_y=test_y,\n    )\n    return result\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Java Client Sample\nDESCRIPTION: Command to run a simple sample of the MLflow Java client, specifying the class path and target server URL.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/java/client/README.md#2025-04-07_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\njava -cp target/mlflow-java-client-0.4.2.jar \\\n  com.databricks.mlflow.client.samples.QuickStartDriver http://localhost:5001\n```\n\n----------------------------------------\n\nTITLE: Testing Chat Model API\nDESCRIPTION: Demonstrating how to call the served model API using curl to generate a Python hello world program.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/pyfunc-chat-model.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://127.0.0.1:5000/invocations \\\n    -H 'Content-Type: application/json' \\\n    -d '{ \"messages\": [{\"role\": \"user\", \"content\": \"Write me a hello world program in python\"}] }' \\\n    | jq\n```\n\n----------------------------------------\n\nTITLE: Configuring Keras Model Save Format in TensorFlow Autolog\nDESCRIPTION: Sets the save_format for Keras models when using mlflow.tensorflow.autolog().\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmlflow.tensorflow.autolog(save_format=...)\n```\n\n----------------------------------------\n\nTITLE: Importing React Components in MLflow Documentation\nDESCRIPTION: JSX import statements for React components used in the MLflow documentation page, including Link and custom components for documentation structure.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/introduction/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Link from \"@docusaurus/Link\";\nimport BorderedContainer from \"@site/src/components/BorderedContainer\";\nimport Platform from \"@site/src/content/platform.mdx\"\n```\n\n----------------------------------------\n\nTITLE: Custom Conda Environment Creation\nDESCRIPTION: Example showing usage of MLFLOW_CONDA_CREATE_ENV_CMD environment variable for customizing Conda environment creation\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_CONDA_CREATE_ENV_CMD=\"conda create -n {name}\"\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoader Instances for Batch Processing\nDESCRIPTION: Creates DataLoader instances to handle batch processing for both training and testing datasets with a batch size of 64.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataloader = DataLoader(training_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Tracking Server without Artifact Proxying in Bash\nDESCRIPTION: Command to start the MLflow tracking server without proxying artifact access, redirecting to an S3 bucket. This allows clients to directly access the remote artifact store.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --no-serve-artifacts --default-artifact-root s3://my-bucket\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Dataset\nDESCRIPTION: Creating a pandas DataFrame with sample questions for evaluating the RAG system performance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/rag-evaluation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\neval_df = pd.DataFrame(\n    {\n        \"questions\": [\n            \"What is MLflow?\",\n            \"How to run mlflow.evaluate()?\",\n            \"How to log_table()?\",\n            \"How to load_table()?\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Using MLflavors with PyOD\nDESCRIPTION: Example of using MLflavors to train, log, and use a PyOD model for anomaly detection.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport mlflow\nimport pandas as pd\nfrom pyod.models.knn import KNN\nfrom pyod.utils.data import generate_data\nfrom sklearn.metrics import roc_auc_score\n\nimport mlflavors\n\nARTIFACT_PATH = \"model\"\n\nwith mlflow.start_run() as run:\n    contamination = 0.1  # percentage of outliers\n    n_train = 200  # number of training points\n    n_test = 100  # number of testing points\n\n    X_train, X_test, _, y_test = generate_data(\n        n_train=n_train, n_test=n_test, contamination=contamination\n    )\n\n    # Train kNN detector\n    clf = KNN()\n    clf.fit(X_train)\n\n    # Evaluate model\n    y_test_scores = clf.decision_function(X_test)\n\n    metrics = {\n        \"roc\": roc_auc_score(y_test, y_test_scores),\n    }\n\n    print(f\"Metrics: \\n{json.dumps(metrics, indent=2)}\")\n\n    # Log metrics\n    mlflow.log_metrics(metrics)\n\n    # Log model using pickle serialization (default).\n    mlflavors.pyod.log_model(\n        pyod_model=clf,\n        artifact_path=ARTIFACT_PATH,\n        serialization_format=\"pickle\",\n    )\n    model_uri = mlflow.get_artifact_uri(ARTIFACT_PATH)\n\n# Print the run id which is used below for serving the model to a local REST API endpoint\nprint(f\"\\nMLflow run id:\\n{run.info.run_id}\")\n\n# Loading the model in native format\nloaded_model = mlflavors.pyod.load_model(model_uri=model_uri)\nprint(loaded_model.decision_function(X_test))\n\n# Loading the model in pyfunc format\nloaded_pyfunc = mlflavors.pyod.pyfunc.load_model(model_uri=model_uri)\n\n# Create configuration DataFrame\npredict_conf = pd.DataFrame(\n    [\n        {\n            \"X\": X_test,\n            \"predict_method\": \"decision_function\",\n        }\n    ]\n)\n\nprint(loaded_pyfunc.predict(predict_conf)[0])\n```\n\n----------------------------------------\n\nTITLE: Defining AI Model Instructions in Python\nDESCRIPTION: Defines the instruction set for the AI model's behavior, setting up system and user roles for code review interactions. The system role specifies the AI as a code review expert while the user role allows for code submission.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninstruction = [\n    {\n        \"role\": \"system\",\n        \"content\": (\n            \"As an AI specializing in code review, your task is to analyze and critique the submitted code. For each code snippet, provide a detailed review that includes: \"\n            \"1. Identification of any errors or bugs. \"\n            \"2. Suggestions for optimizing code efficiency and structure. \"\n            \"3. Recommendations for enhancing code readability and maintainability. \"\n            \"4. Best practice advice relevant to the code's language and functionality. \"\n            \"Your feedback should help the user improve their coding skills and understand best practices in software development.\"\n        ),\n    },\n    {\"role\": \"user\", \"content\": \"Review my code and suggest improvements: {code}\"},\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Python Linting and Formatting Dependencies\nDESCRIPTION: Specifies the exact versions of Python packages used for code quality, formatting, and development workflow in the MLflow project. The file includes ruff for linting, black for code formatting (with Jupyter support), blacken-docs for formatting code in documentation, pre-commit for managing git hooks, toml for configuration file parsing, and a local development tool called 'clint'.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/requirements/lint-requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nruff==0.9.3\nblack[jupyter]==23.7.0\nblacken-docs==1.18.0\npre-commit==4.0.1\ntoml==0.10.2\n-e ./dev/clint\n```\n\n----------------------------------------\n\nTITLE: Querying Joined Data with GraphQL in MLflow\nDESCRIPTION: This GraphQL query demonstrates how to retrieve data about an Experiment, ModelVersions, and Run in a single query by using nested fields. The query fetches a run by ID and accesses its associated experiment name and model versions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/proto_to_graphql/README.md#2025-04-07_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\nquery testQuery {\n    mlflowGetRun(input: {runId: \"my-id\"}) {\n        run {\n            experiment {\n                name\n            }\n            modelVersions {\n                name\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Components for MLflow LangChain Documentation in React\nDESCRIPTION: This code snippet imports necessary components from Docusaurus and custom components for building the MLflow LangChain documentation page.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\nimport { PageCard, CardGroup } from \"@site/src/components/Card\";\nimport { APILink } from \"@site/src/components/APILink\";\nimport Link from \"@docusaurus/Link\";\n```\n\n----------------------------------------\n\nTITLE: Re-logging Index with Different Engine Type\nDESCRIPTION: Shows how to load an index and re-log it with a different engine type\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Log the index with the query engine type first\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        index,\n        artifact_path=\"index-query\",\n        engine_type=\"query\",\n    )\n\n# Load the index back and re-log it with the chat engine type\nindex = mlflow.llama_index.load_model(model_info.model_uri)\nwith mlflow.start_run():\n    model_info = mlflow.llama_index.log_model(\n        index,\n        artifact_path=\"index-chat\",\n        # Specify the chat engine type this time\n        engine_type=\"chat\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Saving Custom Lissajous PyFunc Model with MLflow\nDESCRIPTION: This code snippet demonstrates how to save a custom PyFunc model using MLflow. It creates an instance of the Lissajous model, infers the model signature, and saves the model to a specified path.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define the path to save the model\nmodel_path = \"lissajous_model\"\n\n# Create an instance of the model, overriding the default instance variables `A`, `B`, and `num_points`\nmodel_10k_standard = Lissajous(1, 1, 10_000)\n\n# Infer the model signature, ensuring that we define the params that will be available for customization at inference time\nsignature = infer_signature(pd.DataFrame([{\"a\": 1, \"b\": 2}]), params={\"delta\": np.pi / 5})\n\n# Save our custom model to the path we defined, with the signature that we declared\nmlflow.pyfunc.save_model(path=model_path, python_model=model_10k_standard, signature=signature)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Dependencies via pip\nDESCRIPTION: This command installs or upgrades the MLflow package using pip. It's a prerequisite for using MLflow functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/step1-register-model/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade mlflow\n```\n\n----------------------------------------\n\nTITLE: Setting Model Registry Backend URI with CLI Flag\nDESCRIPTION: The `--registry-store-uri` flag for `mlflow server` command enables specifying a dedicated URI for the Model Registry backend, allowing separate storage configurations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_19\n\nLANGUAGE: Shell\nCODE:\n```\nmlflow server --registry-store-uri <uri>\n```\n\n----------------------------------------\n\nTITLE: Running Database Migrations for MLflow Authentication\nDESCRIPTION: Command to run database migrations for the MLflow authentication system. This updates the permissions database schema at the specified location.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m mlflow.server.auth db upgrade --url <database_url>\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic MLproject File in YAML\nDESCRIPTION: Example of an MLproject file that defines project name, environment, and entry points with parameters. This file configures how MLflow will execute the project commands with specified parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: My Project\n\npython_env: python_env.yaml\n# or\n# conda_env: my_env.yaml\n# or\n# docker_env:\n#    image:  mlflow-docker-example\n\nentry_points:\n  main:\n    parameters:\n      data_file: path\n      regularization: { type: float, default: 0.1 }\n    command: \"python train.py -r {regularization} {data_file}\"\n  validate:\n    parameters:\n      data_file: path\n    command: \"python validate.py {data_file}\"\n```\n\n----------------------------------------\n\nTITLE: Automatic Pip Requirements Inference in MLflow Model Logging\nDESCRIPTION: Model logging and saving functions now automatically infer pip requirements based on the current software environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.*.log_model(...)\nmlflow.*.save_model(...)\n```\n\n----------------------------------------\n\nTITLE: Installing Aliyun OSS Plugin\nDESCRIPTION: Command to install MLflow with Alibaba Cloud OSS storage plugin support.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow[aliyun-oss]\n```\n\n----------------------------------------\n\nTITLE: Testing Containerized Model API\nDESCRIPTION: Curl command to test the Docker-containerized model API with sample wine quality data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl -d '{\"dataframe_split\": {\"columns\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"], \"data\": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]]}}' -H 'Content-Type: application/json' -X POST localhost:5002/invocations\n```\n\n----------------------------------------\n\nTITLE: Creating Apple Sales Dataset\nDESCRIPTION: Instantiates the synthetic apple sales dataset with specified base demand and number of rows for model training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=5000)\ndf\n```\n\n----------------------------------------\n\nTITLE: Setting S3 Bucket Region\nDESCRIPTION: Demonstrates how to set the AWS_DEFAULT_REGION environment variable for specifying a non-default region for MinIO or S3.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/artifacts-stores/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_DEFAULT_REGION=my_region\n```\n\n----------------------------------------\n\nTITLE: Accessing Inferred Signature from Model Info in Python\nDESCRIPTION: Shows how to directly access the inferred signature from the model information returned by the log_model function. This is useful for verifying the signature after model logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel_info.signature\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Dependencies with Pip\nDESCRIPTION: Requirements file specifying exact versions of MLflow and cloudpickle packages to be installed via pip. MLflow 2.7.1 and cloudpickle 2.2.1 are defined as the required versions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/resources/pyfunc_models/2.7.1/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\nmlflow==2.7.1\ncloudpickle==2.2.1\n```\n\n----------------------------------------\n\nTITLE: Complex Pydantic Model Example\nDESCRIPTION: Example of using nested Pydantic models as type hints in a PythonModel for structured data validation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.pyfunc.utils import pyfunc\nimport pydantic\nfrom typing import Optional\n\n\nclass Message(pydantic.BaseModel):\n    role: str\n    content: str\n\n\nclass FunctionParams(pydantic.BaseModel):\n    properties: dict[str, str]\n    type: str = \"object\"\n    required: Optional[list[str]] = None\n    additionalProperties: Optional[bool] = None\n\n\nclass ToolDefinition(pydantic.BaseModel):\n    name: str\n    description: Optional[str] = None\n    parameters: Optional[FunctionParams] = None\n    strict: Optional[bool] = None\n\n\nclass ChatRequest(pydantic.BaseModel):\n    messages: list[Message]\n    tool: Optional[ToolDefinition] = None\n\n\n@pyfunc\ndef predict(model_input: list[ChatRequest]) -> list[list[str]]:\n    return [[msg.content for msg in request.messages] for request in model_input]\n\n\ninput_example = [ChatRequest(messages=[Message(role=\"user\", content=\"Hello\")])]\nprint(predict(input_example))  # Output: [['Hello']]\n```\n\n----------------------------------------\n\nTITLE: Rendering RLHF Architecture Image in JSX\nDESCRIPTION: This code snippet renders an image of the RLHF architecture with a caption using JSX. It uses a figure element with custom styling to center the content and set the width.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/guide/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: jsx\nCODE:\n```\n<figure className=\"center-div\" style={{ width: \"90%\", textAlign: \"center\" }}>\n  ![A primer on RLHF for sophisticated LLM training](/images/tutorials/llms/RLHF-architecture.png)\n  <figcaption>Simplified overview of RLHF</figcaption>\n</figure>\n```\n\n----------------------------------------\n\nTITLE: Querying MLflow Runs with Case-Sensitive LIKE in Python\nDESCRIPTION: Example of using case-sensitive LIKE queries with the SearchRuns API when running against a SQLite backend. This feature was added in MLflow 1.8.0.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n'params.framework LIKE '%sklearn%''\n```\n\n----------------------------------------\n\nTITLE: Setting Document Chunk Size for Text Processing\nDESCRIPTION: Defines the size of document chunks for processing. This parameter controls how large each text segment will be when splitting documents for question generation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nCHUNK_SIZE = 1500\n```\n\n----------------------------------------\n\nTITLE: Searching Metrics in MLflow using SQL\nDESCRIPTION: Examples of filtering MLflow runs by metrics using SQL-like syntax. Metrics are stored as numbers and require numeric comparators.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nmetrics.accuracy > 0.72\nmetrics.\"accuracy\" > 0.72\nmetrics.loss <= 0.15\nmetrics.`log-scale-loss` <= 0\nmetrics.`f1 score` >= 0.5\nmetrics.accuracy > 0.72 AND metrics.loss <= 0.15\n```\n\n----------------------------------------\n\nTITLE: Update Model Version API Endpoint\nDESCRIPTION: REST endpoint for updating an existing model version in the MLflow registry. Requires model name and version number as mandatory fields, with optional description field.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_19\n\nLANGUAGE: REST\nCODE:\n```\nPATCH 2.0/mlflow/model-versions/update\n```\n\n----------------------------------------\n\nTITLE: Searching MLflow Experiments\nDESCRIPTION: Demonstrates how to search for experiments using the MLflow Client API and print the results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step2-mlflow-client/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nall_experiments = client.search_experiments()\n\nprint(all_experiments)\n```\n\n----------------------------------------\n\nTITLE: Extending Ignore List in pyproject.toml\nDESCRIPTION: Configuration in pyproject.toml to extend the list of ignored words for typos checking. This is useful for common false positives across multiple files.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/typos.md#2025-04-07_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n# pyproject.toml\n\n[tool.typos.default]\nextend-ignore-re = [\n  ...,\n  \"false_positive\",\n]\n```\n\n----------------------------------------\n\nTITLE: Generating SQL Query with Loaded PEFT Model\nDESCRIPTION: This code snippet shows how to use the loaded PEFT model to generate a SQL query from a given prompt. It demonstrates that the system prompt and default inference parameters are automatically applied by the MLflow model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# We only input table and question, since system prompt is adeed in the prompt template.\ntest_prompt = \"\"\"\n### Table:\nCREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)\n\n### Question:\nWhen Essendon played away; where did they play?\n\"\"\"\n\n# Inference parameters like max_tokens_length are set to default values specified in the Model Signature\ngenerated_query = mlflow_model.predict(test_prompt)[0]\ndisplay_table({\"prompt\": test_prompt, \"generated_query\": generated_query})\n```\n\n----------------------------------------\n\nTITLE: Generating SQL Query with Loaded PEFT Model\nDESCRIPTION: This code snippet shows how to use the loaded PEFT model to generate a SQL query from a given prompt. It demonstrates that the system prompt and default inference parameters are automatically applied by the MLflow model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# We only input table and question, since system prompt is adeed in the prompt template.\ntest_prompt = \"\"\"\n### Table:\nCREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)\n\n### Question:\nWhen Essendon played away; where did they play?\n\"\"\"\n\n# Inference parameters like max_tokens_length are set to default values specified in the Model Signature\ngenerated_query = mlflow_model.predict(test_prompt)[0]\ndisplay_table({\"prompt\": test_prompt, \"generated_query\": generated_query})\n```\n\n----------------------------------------\n\nTITLE: Creating Custom GenAI Metric for Answer Quality\nDESCRIPTION: Configures a custom metric using make_genai_metric function to evaluate answer quality using GPT-4. Specifies metric name, definition, grading prompt, and example cases.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nanswer_quality_metric = make_genai_metric(\n    name=\"answer_quality\",\n    definition=answer_quality_definition,\n    grading_prompt=answer_quality_grading_prompt,\n    version=\"v1\",\n    examples=[example1, example2],\n    model=\"openai:/gpt-4\",\n    greater_is_better=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for MLflow Artifacts Module using Sphinx\nDESCRIPTION: This reStructuredText snippet configures Sphinx to automatically generate documentation for the mlflow.artifacts module. It includes all members, undocumented members, and shows inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.artifacts.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nmlflow.artifacts\n================\n\n.. automodule:: mlflow.artifacts\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Serving the Model Locally Command\nDESCRIPTION: Provides the command to serve the model locally using the MLflow CLI. This allows the model to accept HTTP requests for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# run below command to serve the model locally\n# mlflow models serve -m runs:/33b7da4d1693490b97934a5781964766/model -p 6666\n```\n\n----------------------------------------\n\nTITLE: Logging MLflow Model with Transformers\nDESCRIPTION: Python code example demonstrating how to log an MLflow model using the Transformers library and the JFrog Artifactory plugin.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow import MlflowClient\nfrom transformers import pipeline\n\nmlflow.set_tracking_uri(\"<your mlflow tracking server uri>\")\nmlflow.create_experiment(\"<your_exp_name>\")\nclassifier = pipeline(\n    \"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\"\n)\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=classifier, artifact_path=MODEL_NAME\n    )\nmlflow.end_run()\n```\n\n----------------------------------------\n\nTITLE: Setting Document Chunk Size for Text Splitting\nDESCRIPTION: Defines the chunk size (1500 characters) to be used when splitting documents for question generation and retrieval evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nCHUNK_SIZE = 1500\n```\n\n----------------------------------------\n\nTITLE: Logging and Loading a Basic MLflow PyFunc Model\nDESCRIPTION: Demonstrates how to log a model using mlflow.pyfunc.log_model and then load it back with mlflow.pyfunc.load_model to make predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.pyfunc.log_model(\n        artifact_path=\"model\",\n        python_model=model,\n        input_example=input_example,\n    )\n\n# Load the model as pyfunc\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nassert pyfunc_model.predict(input_example) == [\"Hello\", \"Hi\"]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Models with Custom Metrics and Artifacts\nDESCRIPTION: Uses mlflow.evaluate() to evaluate models with custom metrics and artifacts.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmlflow.evaluate()\n```\n\n----------------------------------------\n\nTITLE: Installing Custom Authentication Plugin (Bash)\nDESCRIPTION: This bash command shows how to install a custom authentication plugin in the Python environment using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npip install my_auth\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for MLflow Bedrock Module\nDESCRIPTION: This snippet contains Sphinx documentation directives for the MLflow Bedrock module. It uses the automodule directive to automatically generate documentation from the module's docstrings, including all members, undocumented members, and showing inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.bedrock.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.bedrock\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Configuring Gunicorn for Model Serving in Python\nDESCRIPTION: MLflow 1.2 allows configuring Gunicorn options for the 'mlflow models serve' CLI using an environment variable.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_54\n\nLANGUAGE: bash\nCODE:\n```\nexport GUNICORN_CMD_ARGS=\"--timeout 120\"\nmlflow models serve -m /path/to/model\n```\n\n----------------------------------------\n\nTITLE: Verifying the Model Before Deployment\nDESCRIPTION: Shows how to verify that the model works correctly using the mlflow.models.predict function. This helps ensure the model is functioning as expected before deployment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# verify model before serving\nmlflow.models.predict(\n    model_uri=model_info.model_uri,\n    input_data=[{\"role\": \"user\", \"content\": \"Where is New York?\"}],\n    env_manager=\"uv\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating MLflow Endpoint for OpenAI\nDESCRIPTION: Setting up an MLflow endpoint for OpenAI model deployment with Azure configuration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclient = mlflow.deployments.get_deploy_client(\"databricks\")\n\nendpoint_name = \"<your-endpoint-name>\"  # replace this!\nclient.create_endpoint(\n    name=endpoint_name,\n    config={\n        \"served_entities\": [\n            {\n                \"name\": \"test-gpt\",\n                \"external_model\": {\n                    \"name\": \"gpt-4o-mini\",\n                    \"provider\": \"openai\",\n                    \"task\": \"llm/v1/completions\",\n                    \"openai_config\": {\n                        \"openai_api_type\": \"azure\",\n                        \"openai_api_key\": \"{{secrets/scope/openai_api_key}}\",\n                        \"openai_api_base\": \"{{secrets/scope/openai_api_base}}\",\n                        \"openai_deployment_name\": \"gpt-4o-mini\",\n                        \"openai_api_version\": \"2023-05-15\",\n                    },\n                },\n            }\n        ],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Searching Tags in MLflow using SQL\nDESCRIPTION: Examples of filtering MLflow runs by tags. Tags are stored as strings and require string comparators.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\ntags.\"environment\" = \"notebook\"\ntags.environment = \"notebook\"\ntags.task = \"Classification\"\ntags.task ILIKE \"classif%\"\n```\n\n----------------------------------------\n\nTITLE: Running MLflow example with local environment\nDESCRIPTION: Command to run the Ax hyperparameter optimization example without creating a conda environment. This uses the local Python environment with already installed dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/AxHyperOptimizationPTL/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . --env-manager=local\n```\n\n----------------------------------------\n\nTITLE: Cloning the MLflow Repository\nDESCRIPTION: Git command to clone the MLflow repository containing LlamaIndex workflow examples.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/mlflow/mlflow.git\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Tracking URI using Bash\nDESCRIPTION: Sets up the MLflow tracking server URI environment variable for logging experiments\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n```\n\n----------------------------------------\n\nTITLE: Serving MLflow Model as REST API Endpoint\nDESCRIPTION: Command to serve the trained Sktime model as a local REST API endpoint using MLflow's model serving functionality. Requires substituting the run_id with the actual MLflow run ID.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sktime/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n```\n\n----------------------------------------\n\nTITLE: Serving MLflow Model as REST API Endpoint\nDESCRIPTION: Command to serve the trained Sktime model as a local REST API endpoint using MLflow's model serving functionality. Requires substituting the run_id with the actual MLflow run ID.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sktime/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Import statements for required Python packages including MLflow, Transformers, and data handling libraries\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import pipeline\n\nimport mlflow\nfrom mlflow.metrics.genai import EvaluationExample, answer_correctness, make_genai_metric\n```\n\n----------------------------------------\n\nTITLE: Predicting with Valid Data After Signature Update\nDESCRIPTION: Tests the model with valid data that matches the updated signature. This demonstrates that properly formatted inputs are accepted by the model after signature enforcement is applied.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nloaded_with_signature.predict(expected_data_structure)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for MLflow Deployments Module\nDESCRIPTION: RST (reStructuredText) directives that configure how Sphinx should generate documentation for the MLflow.deployments module, including which members to document and which to exclude.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.deployments.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.deployments\n    :members:\n    :undoc-members:\n    :exclude-members: PredictionsResponse\n\n.. autoclass:: mlflow.deployments.PredictionsResponse\n    :members:\n    :undoc-members:\n    :exclude-members: from_json\n```\n\n----------------------------------------\n\nTITLE: Searching Registered Models in Python\nDESCRIPTION: Introduces a new fluent API for searching registered models in MLflow using Python. This allows users to programmatically search for registered models with specific criteria.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmlflow.search_registered_models()\n```\n\n----------------------------------------\n\nTITLE: Running the Example as MLflow Project\nDESCRIPTION: Command to execute the example as a complete MLflow project, which will handle dependencies and execution environment automatically.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sktime/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run .\n```\n\n----------------------------------------\n\nTITLE: Searching Run Metadata in MLflow using SQL\nDESCRIPTION: Examples of filtering MLflow runs by run metadata using both string and numeric comparators.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nattributes.status = \"ACTIVE\"\nattributes.user_id LIKE \"user1\"\nattributes.run_name = \"my-run\"\nattributes.run_id = \"a1b2c3d4\"\nattributes.run_id IN ('a1b2c3d4', 'e5f6g7h8')\n```\n\nLANGUAGE: sql\nCODE:\n```\nattributes.start_time >= 1664067852747\nattributes.end_time < 1664067852747\nattributes.created > 1664067852747\n```\n\n----------------------------------------\n\nTITLE: Get Model Version API Endpoint\nDESCRIPTION: REST endpoint for retrieving a specific model version from the MLflow registry. Requires model name and version number as mandatory fields.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_18\n\nLANGUAGE: REST\nCODE:\n```\nGET 2.0/mlflow/model-versions/get\n```\n\n----------------------------------------\n\nTITLE: Creating and Logging to MLflow Experiment\nDESCRIPTION: Example code showing how to create an experiment, set it as active, and log metrics using MLflow in Databricks\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/databricks-trial/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.create_experiment(\n    \"/Users/<your email>/check-databricks-connection\",\n    artifact_location=\"dbfs:/Volumes/test/mlflow/check-databricks-connection\",\n)\nmlflow.set_experiment(\"/Users/<your email>/check-databricks-connection\")\n\nwith mlflow.start_run():\n    mlflow.log_metric(\"foo\", 1)\n    mlflow.log_metric(\"bar\", 2)\n```\n\n----------------------------------------\n\nTITLE: Model Prediction Response Format\nDESCRIPTION: Example JSON response format from the model API showing the prediction result structure.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"predictions\": [{ \"0\": 5.310967445373535 }] }\n```\n\n----------------------------------------\n\nTITLE: Example Output from RetrievalQA Chain in MLflow\nDESCRIPTION: Sample output from the RetrievalQA chain example, showing the response format after querying the loaded model about a specific topic from the source document.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/guide/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[\" The president said...\"]\n```\n\n----------------------------------------\n\nTITLE: High-Level API References\nDESCRIPTION: List of key MLflow API functions for working with Sentence-Transformers models, including saving, logging, and loading models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nmlflow.sentence_transformers.save_model\nmlflow.sentence_transformers.log_model\nmlflow.sentence_transformers.load_model\nmlflow.pyfunc.load_model\nmlflow.pyfunc.PythonModel\n```\n\n----------------------------------------\n\nTITLE: Keras Module Documentation Structure in RST\nDESCRIPTION: ReStructuredText documentation structure defining the automated documentation generation for MLflow's Keras module components including autologging, callbacks, loading and saving functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.keras.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.keras\n==================\n\n.. automodule:: mlflow.keras.autolog\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: mlflow.keras.callback\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: mlflow.keras.load\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. automodule:: mlflow.keras.save\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Making HTTP Requests to the Served Model\nDESCRIPTION: Demonstrates how to make HTTP requests to the locally served model. The code converts an input example to the required serving format and sends a POST request to the model endpoint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nfrom mlflow.models.utils import convert_input_example_to_serving_input\n\npayload = convert_input_example_to_serving_input(\n    [{\"role\": \"user\", \"content\": \"Is British shorthair smart?\"}]\n)\nresp = requests.post(\n    \"http://127.0.0.1:6666/invocations\", data=payload, headers={\"Content-Type\": \"application/json\"}\n)\nresp.content\n```\n\n----------------------------------------\n\nTITLE: Creating Text Corpus for Semantic Search in Python\nDESCRIPTION: Creates a diverse corpus of text documents covering various topics and saves it to a file. The corpus includes 30 different entries ranging from science to culture, designed to demonstrate semantic search capabilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncorpus = [\n    \"Perfecting a Sourdough Bread Recipe: The Joy of Baking. Baking sourdough bread \"\n    \"requires patience, skill, and a good understanding of yeast fermentation. Each \"\n    \"loaf is unique, telling its own story of the baker's journey.\",\n    \"The Mars Rover's Discoveries: Unveiling the Red Planet. NASA's Mars rover has \"\n    \"sent back stunning images and data, revealing the planet's secrets. These \"\n    \"discoveries may hold the key to understanding Mars' history.\",\n    \"The Art of Growing Herbs: Enhancing Your Culinary Skills. Growing your own \"\n    \"herbs can transform your cooking, adding fresh and vibrant flavors. Whether it's \"\n    \"basil, thyme, or rosemary, each herb has its own unique characteristics.\",\n    \"AI in Software Development: Transforming the Tech Landscape. The rapid \"\n    \"advancements in artificial intelligence are reshaping how we approach software \"\n    \"development. From automation to machine learning, the possibilities are endless.\",\n    \"Backpacking Through Europe: A Journey of Discovery. Traveling across Europe by \"\n    \"backpack allows one to immerse in diverse cultures and landscapes. It's an \"\n    \"adventure that combines the thrill of exploration with personal growth.\",\n    \"Shakespeare's Timeless Influence: Reshaping Modern Storytelling. The works of \"\n    \"William Shakespeare continue to inspire and influence contemporary literature. \"\n    \"His mastery of language and deep understanding of human nature are unparalleled.\",\n    \"The Rise of Renewable Energy: A Sustainable Future. Embracing renewable energy \"\n    \"is crucial for achieving a sustainable and environmentally friendly lifestyle. \"\n    \"Solar, wind, and hydro power are leading the way in this green revolution.\",\n    \"The Magic of Jazz: An Exploration of Sound and Harmony. Jazz music, known for \"\n    \"its improvisation and complex harmonies, has a rich and diverse history. It \"\n    \"evokes a range of emotions, often reflecting the soul of the musician.\",\n    \"Yoga for Mind and Body: The Benefits of Regular Practice. Engaging in regular \"\n    \"yoga practice can significantly improve flexibility, strength, and mental \"\n    \"well-being. It's a holistic approach to health, combining physical and spiritual \"\n    \"aspects.\",\n    \"The Egyptian Pyramids: Monuments of Ancient Majesty. The ancient Egyptian \"\n    \"pyramids, monumental tombs for pharaohs, are marvels of architectural \"\n    \"ingenuity. They stand as a testament to the advanced skills of ancient builders.\",\n    \"Vegan Cuisine: A World of Flavor. Exploring vegan cuisine reveals a world of \"\n    \"nutritious and delicious possibilities. From hearty soups to delectable desserts, \"\n    \"plant-based dishes are diverse and satisfying.\",\n    \"Extraterrestrial Life: The Endless Search. The quest to find life beyond Earth \"\n    \"continues to captivate scientists and the public alike. Advances in space \"\n    \"technology are bringing us closer to answering this age-old question.\",\n    \"The Art of Plant Pruning: Promoting Healthy Growth. Regular pruning is essential \"\n    \"for maintaining healthy and vibrant plants. It's not just about cutting back, but \"\n    \"understanding each plant's growth patterns and needs.\",\n    \"Cybersecurity in the Digital Age: Protecting Our Data. With the rise of digital \"\n    \"technology, cybersecurity has become a critical concern. Protecting sensitive \"\n    \"information from cyber threats is an ongoing challenge for individuals and \"\n    \"businesses alike.\",\n    \"The Great Wall of China: A Historical Journey. Visiting the Great Wall offers \"\n    \"more than just breathtaking views; it's a journey through history. This ancient \"\n    \"structure tells stories of empires, invasions, and human resilience.\",\n    \"Mystery Novels: Crafting Suspense and Intrigue. A great mystery novel captivates \"\n    \"the reader with intricate plots and unexpected twists. It's a genre that combines \"\n    \"intellectual challenge with entertainment.\",\n    \"Conserving Endangered Species: A Global Effort. Protecting endangered species \"\n    \"is a critical task that requires international collaboration. From rainforests to \"\n    \"oceans, every effort counts in preserving our planet's biodiversity.\",\n    \"Emotions in Classical Music: A Symphony of Feelings. Classical music is not just \"\n    \"an auditory experience; it's an emotional journey. Each composition tells a story, \"\n    \"conveying feelings from joy to sorrow, tranquility to excitement.\",\n    \"CrossFit: A Test of Strength and Endurance. CrossFit is more than just a fitness \"\n    \"regimen; it's a lifestyle that challenges your physical and mental limits. It \"\n    \"combines various disciplines to create a comprehensive workout.\",\n    \"The Renaissance: An Era of Artistic Genius. The Renaissance marked a period of \"\n    \"extraordinary artistic and scientific achievements. It was a time when creativity \"\n    \"and innovation flourished, reshaping the course of history.\",\n    \"Exploring International Cuisines: A Culinary Adventure. Discovering international \"\n    \"cuisines is an adventure for the palate. Each dish offers a glimpse into the \"\n    \"culture and traditions of its origin.\",\n    \"Astronaut Training: Preparing for the Unknown. Becoming an astronaut involves \"\n    \"rigorous training to prepare for the extreme conditions of space. It's a journey \"\n    \"that tests both physical endurance and mental resilience.\",\n    \"Sustainable Gardening: Nurturing the Environment. Sustainable gardening is not \"\n    \"just about growing plants; it's about cultivating an ecosystem. By embracing \"\n    \"environmentally friendly practices, gardeners can have a positive impact on the \"\n    \"planet.\",\n    \"The Smartphone Revolution: Changing Communication. Smartphones have transformed \"\n    \"how we communicate, offering unprecedented connectivity and convenience. This \"\n    \"technology continues to evolve, shaping our daily interactions.\",\n    \"Experiencing African Safaris: Wildlife and Wilderness. An African safari is an \"\n    \"unforgettable experience that brings you face-to-face with the wonders of \"\n    \"wildlife. It's a journey that connects you with the raw beauty of nature.\",\n    \"Graphic Novels: A Blend of Art and Story. Graphic novels offer a unique medium \"\n    \"where art and narrative intertwine to tell compelling stories. They challenge \"\n    \"traditional forms of storytelling, offering visual and textual richness.\",\n    \"Addressing Ocean Pollution: A Call to Action. The increasing levels of pollution \"\n    \"in our oceans are a pressing environmental concern. Protecting marine life and \"\n    \"ecosystems requires concerted global efforts.\",\n    \"The Origins of Hip Hop: A Cultural Movement. Hip hop music, originating from the \"\n    \"streets of New York, has grown into a powerful cultural movement. Its beats and \"\n    \"lyrics reflect the experiences and voices of a community.\",\n    \"Swimming: A Comprehensive Workout. Swimming offers a full-body workout that is \"\n    \"both challenging and refreshing. It's an exercise that enhances cardiovascular \"\n    \"health, builds muscle, and improves endurance.\",\n    \"The Fall of the Berlin Wall: A Historical Turning Point. The fall of the Berlin \"\n    \"Wall was not just a physical demolition; it was a symbol of political and social \"\n    \"change. This historic event marked the end of an era and the beginning of a new \"\n    \"chapter in world history.\",\n]\n\n# Write the corpus to a file\ncorpus_file = \"/tmp/search_corpus.txt\"\nwith open(corpus_file, \"w\") as file:\n    for sentence in corpus:\n        file.write(sentence + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for MLflow\nDESCRIPTION: Steps to create a new Conda environment for MLflow, activate it, and install MLflow within the isolated environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n mlflow-env python\nconda activate mlflow-env\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Setting Model Version Tags in R\nDESCRIPTION: Adds a new method to the R client for setting tags on model versions in the Model Registry. This allows R users to add metadata to specific versions of registered models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_7\n\nLANGUAGE: r\nCODE:\n```\nmlflow_set_model_version_tag()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Command to set the OpenAI API key as an environment variable for gateway server authentication\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Starting an MLflow Run with R\nDESCRIPTION: Example of starting an MLflow run and logging a metric within the run context. This pattern ensures proper run cleanup when execution completes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_55\n\nLANGUAGE: r\nCODE:\n```\nwith(mlflow_start_run(), {\nmlflow_log_metric(\"test\", 10)\n})\n```\n\n----------------------------------------\n\nTITLE: Empty Lists vs None in Array Signatures\nDESCRIPTION: Example demonstrating the difference between empty lists and None values in array-type signatures.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninfer_signature(\n    model_input={\n        \"list_with_empty\": [[\"a\", \"b\"], []],\n        \"list_with_none\": [[\"a\", \"b\"], None],\n    }\n)\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsignature:\n    input: '[\n        {\"name\": \"list_with_empty\", \"type\": \"Array(str)\", \"required\": \"true\" },\n        {\"name\": \"list_with_none\" , \"type\": \"Array(str)\", \"required\": \"false\"},\n    ]'\n    output: null\n    params: null\n```\n\n----------------------------------------\n\nTITLE: Callable PythonModel Implementation\nDESCRIPTION: Example of creating a PythonModel using a callable function decorated with @pyfunc for input validation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.pyfunc.utils import pyfunc\n\n\n@pyfunc\ndef predict(model_input: list[str]) -> list[str]:\n    return model_input\n```\n\n----------------------------------------\n\nTITLE: Analyzing Low Cosine Similarity Scores in Python\nDESCRIPTION: This code filters and prints details of question-chunk pairs with low cosine similarity scores (below 0.75). It allows for manual inspection of less relevant or potentially problematic pairs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmask = embedded_queries[\"cossim\"] < 0.75\nlower_cossim = embedded_queries[mask]\nfor i, row in lower_cossim.iterrows():\n    print(f\"Question: {i}\")\n    print(row[\"question\"])\n    print(\"Chunk:\")\n    print(row[\"chunk\"])\n    print(\"cossim:\")\n    print(row[\"cossim\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Environment in YAML\nDESCRIPTION: Example of a python_env.yaml file that specifies Python version, build dependencies, and runtime dependencies for a virtualenv-based MLflow project environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Python version required to run the project.\npython: \"3.8.15\"\n# Dependencies required to build packages. This field is optional.\nbuild_dependencies:\n  - pip\n  - setuptools\n  - wheel==0.37.1\n# Dependencies required to run the project.\ndependencies:\n  - mlflow==2.3\n  - scikit-learn==1.0.2\n```\n\n----------------------------------------\n\nTITLE: Modified Parameter Test Execution\nDESCRIPTION: Execution of hyperparameter tuning with modified parameter choices and custom identifier.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Execute modified hyperparameter tuning runs with custom parameter choices\nparam_1_values = [\"a\", \"b\"]\nparam_2_values = [\"u\", \"v\", \"w\"]\nident = \"params_test_2\"\nconsume(\n    starmap(\n        execute_tuning, ((x, param_1_values, param_2_values, ident) for x in range(5))\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Running Fastai Example with Python\nDESCRIPTION: Commands to run the Fastai example using Python with default or custom arguments. This assumes all Fastai dependencies are installed in the development environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/fastai/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py\n```\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --lr=0.02 --epochs=3\n```\n\n----------------------------------------\n\nTITLE: Reinstalling PyYAML for LibYAML Support\nDESCRIPTION: Reinstalls PyYAML to ensure it uses the LibYAML bindings for improved performance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/backend-stores/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Reinstall PyYAML\npip --no-cache-dir install --force-reinstall -I pyyaml\n```\n\n----------------------------------------\n\nTITLE: React JSX Tracing Card Component\nDESCRIPTION: React component displaying a GIF animation inside a Card wrapper showing MLflow's tracing capabilities\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n<Card >\n![Tracing Gateway Video](/images/llms/tracing/tracing-top.gif)\n</Card>\n```\n\n----------------------------------------\n\nTITLE: Enhancing Streaming Outputs with Callbacks in LangChain\nDESCRIPTION: Using callback handlers with streaming outputs in LangChain. This example demonstrates how to integrate a StdOutCallbackHandler to enhance the streaming process, which can be used for logging or modifying responses before delivery.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/guide/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nhandler = StdOutCallbackHandler()\n\n# Attach callback to enhance the streaming process\nresponse_stream = loaded_model.predict_stream(input_data, callback_handlers=[handler])\nfor enhanced_response in response_stream:\n    print(\"Enhanced Streaming Response:\", enhanced_response)\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Documentation for MLflow LiteLLM Module\nDESCRIPTION: ReStructuredText directive for auto-generating documentation from the mlflow.litellm module, including all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.litellm.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.litellm\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Defining ReActAgent Workflow in Python\nDESCRIPTION: This snippet defines the ReActAgent class, which inherits from Workflow. It implements steps for handling user messages, preparing LLM prompts, invoking the LLM, parsing responses, and executing tool calls. The agent uses a memory buffer to store chat history.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass ReActAgent(Workflow):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.tools = tools\n        # Store the chat history in memory so the agent can handle multiple interactions with users.\n        self.memory = ChatMemoryBuffer.from_defaults(llm=Settings.llm)\n\n    @step\n    async def new_user_msg(self, ctx: Context, ev: StartEvent) -> PrepEvent:\n        \"\"\"Start workflow with the new user messsage\"\"\"\n        # StartEvent carries whatever keys passed to the workflow's run() method as attributes.\n        user_input = ev.input\n        user_msg = ChatMessage(role=\"user\", content=user_input)\n        self.memory.put(user_msg)\n\n        # We store the executed reasoning steps in the context. Clear it at the start.\n        await ctx.set(\"steps\", [])\n\n        return PrepEvent()\n\n    @step\n    async def prepare_llm_prompt(self, ctx: Context, ev: PrepEvent) -> LLMInputEvent:\n        \"\"\"Prepares the react prompt, using the chat history, tools, and current reasoning (if any)\"\"\"\n        steps = await ctx.get(\"steps\", default=[])\n        chat_history = self.memory.get()\n\n        # Construct an LLM from the chat history, tools, and current reasoning, using the\n        # built-in prompt template.\n        llm_input = ReActChatFormatter().format(self.tools, chat_history, current_reasoning=steps)\n        return LLMInputEvent(input=llm_input)\n\n    @step\n    async def invoke_llm(self, ev: LLMInputEvent) -> LLMOutputEvent:\n        \"\"\"Call the LLM with the react prompt\"\"\"\n        response = await Settings.llm.achat(ev.input)\n        return LLMOutputEvent(response=response)\n\n    @step\n    async def handle_llm_response(\n        self, ctx: Context, ev: LLMOutputEvent\n    ) -> ToolCallEvent | PrepEvent | StopEvent:\n        \"\"\"\n        Parse the LLM response to extract any tool calls requested.\n        If theere is no tool call, we can stop and emit a StopEvent. Otherwise, we emit a ToolCallEvent to handle tool calls.\n        \"\"\"\n        try:\n            step = ReActOutputParser().parse(ev.response.message.content)\n            (await ctx.get(\"steps\", default=[])).append(step)\n\n            if step.is_done:\n                # No additional tool call is required. Ending the workflow by emitting StopEvent.\n                return StopEvent(result=step.response)\n            elif isinstance(step, ActionReasoningStep):\n                # Tool calls are returned from LLM, trigger the tool call event.\n                return ToolCallEvent(\n                    tool_calls=[\n                        ToolSelection(\n                            tool_id=\"fake\",\n                            tool_name=step.action,\n                            tool_kwargs=step.action_input,\n                        )\n                    ]\n                )\n        except Exception as e:\n            error_step = ObservationReasoningStep(\n                observation=f\"There was an error in parsing my reasoning: {e}\"\n            )\n            (await ctx.get(\"steps\", default=[])).append(error_step)\n\n        # if no tool calls or final response, iterate again\n        return PrepEvent()\n\n    @step\n    async def handle_tool_calls(self, ctx: Context, ev: ToolCallEvent) -> PrepEvent:\n        \"\"\"\n        Safely calls tools with error handling, adding the tool outputs to the current reasoning. Then, by emitting a PrepEvent, we loop around for another round of ReAct prompting and parsing.\n        \"\"\"\n        tool_calls = ev.tool_calls\n        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n\n        # call tools -- safely!\n        for tool_call in tool_calls:\n            if tool := tools_by_name.get(tool_call.tool_name):\n                try:\n                    tool_output = tool(**tool_call.tool_kwargs)\n                    step = ObservationReasoningStep(observation=tool_output.content)\n                except Exception as e:\n                    step = ObservationReasoningStep(\n                        observation=f\"Error calling tool {tool.metadata.get_name()}: {e}\"\n                    )\n            else:\n                step = ObservationReasoningStep(\n                    observation=f\"Tool {tool_call.tool_name} does not exist\"\n                )\n            (await ctx.get(\"steps\", default=[])).append(step)\n\n        # prep the next iteration\n        return PrepEvent()\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow UI for Viewing Results\nDESCRIPTION: Command to start the MLflow UI server to view run metrics, parameters, and details. The UI will be accessible at http://localhost:5000.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/CaptumExample/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Databricks Connect Remote Client Integration\nDESCRIPTION: Shows how to use pre-built model environments with Databricks Connect for remote client execution using the spark_udf functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_82\n\nLANGUAGE: python\nCODE:\n```\nfrom databricks.connect import DatabricksSession\n\nspark = DatabricksSession.builder.remote(\n    host=os.environ[\"DATABRICKS_HOST\"],\n    token=os.environ[\"DATABRICKS_TOKEN\"],\n    cluster_id=\"<cluster id>\",  # get cluster id by spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n).getOrCreate()\n\n# The path generated by `build_model_env` in Databricks runtime.\nmodel_env_uc_uri = \"dbfs:/Volumes/.../.../XXXXX.tar.gz\"\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark, model_uri, prebuilt_env_uri=model_env_uc_uri\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for RAG Evaluation\nDESCRIPTION: Installs the necessary Python packages including beautifulsoup4 for web scraping, langchain for LLM operations, OpenAI for API access, pandas for data manipulation, seaborn for visualization, and scikit-learn for dimensionality reduction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install beautifulsoup4 langchain openai pandas seaborn scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Scaling Model Deployment - Bash\nDESCRIPTION: Command to update the deployment configuration to scale to multiple replicas for improved throughput.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/ray_serve/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmlflow deployments update -t ray-serve --name iris:v1 --config num_replicas=2\n```\n\n----------------------------------------\n\nTITLE: Delete Registered Model Endpoint\nDESCRIPTION: REST endpoint for deleting a registered model. Requires the model name to identify which model to delete.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_15\n\nLANGUAGE: rest\nCODE:\n```\nDELETE 2.0/mlflow/registered-models/delete\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Installs required Python packages MLflow, PyTorch, and TorchMetrics using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/logging/pytorch_log_model.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -q mlflow torch torchmetrics\n```\n\n----------------------------------------\n\nTITLE: Verifying Input and Output Signatures in Python\nDESCRIPTION: Demonstrates how to verify that the logged input signature matches the signature inference and generates an output signature. This step is important for ensuring consistency in model input and output schemas.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nreport_signature_info(data, prediction)\n```\n\n----------------------------------------\n\nTITLE: Listing Artifacts in R\nDESCRIPTION: Demonstrates the usage of the fixed mlflow_list_artifacts() function in the R package to list artifacts for a run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_49\n\nLANGUAGE: R\nCODE:\n```\nartifacts <- mlflow_list_artifacts(run_id)\n```\n\n----------------------------------------\n\nTITLE: Set Registered Model Tag Endpoint\nDESCRIPTION: API endpoint documentation for setting tags on registered models, including request parameters for model name, tag key and value.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_26\n\nLANGUAGE: rest\nCODE:\n```\n+------------------------------------------+-------------+\n|                 Endpoint                 | HTTP Method |\n+==========================================+=============+\n| ``2.0/mlflow/registered-models/set-tag`` | ``POST``    |\n+------------------------------------------+-------------+\n```\n\n----------------------------------------\n\nTITLE: Model Prediction Response Format\nDESCRIPTION: Example of the response format from an MLflow model server, showing classification results for the input data points.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/quickstart_drilldown/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n[1, 0]\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for MLflow Model Deployment in Python\nDESCRIPTION: This code block imports the required libraries for the MLflow model deployment project. It includes libraries for data manipulation, machine learning, and MLflow functionalities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport mlflow\nfrom mlflow.models import infer_signature\n\nwarnings.filterwarnings(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Deployments Config Environment Variable\nDESCRIPTION: Command to set the configuration file path as an environment variable\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_DEPLOYMENTS_CONFIG=/path/to/config.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating MLflow Experiment for Paraphrase Mining in Python\nDESCRIPTION: This snippet creates a new MLflow Experiment for the Paraphrase Mining Model. It sets up a dedicated experiment for contextually relevant logging of the model run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/paraphrase-mining/paraphrase-mining-sentence-transformers.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(\"Paraphrase Mining\")\n```\n\n----------------------------------------\n\nTITLE: Setting Prompt Template for Transformer Model\nDESCRIPTION: Defines a prompt template for a text-to-SQL model that includes placeholders for the prompt variable. The template structures how input will be formatted before being passed to the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprompt_template = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n{prompt}\n\n### Response:\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running MLflow MNIST Example with Local Environment\nDESCRIPTION: Command to run the MNIST example using MLflow without creating a conda environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/MNIST/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . --env-manager=local\n```\n\n----------------------------------------\n\nTITLE: Defining External Module for MLflow Model in Python\nDESCRIPTION: Example of defining an external module 'custom_code.py' that will be used as a dependency in an MLflow model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\niris_types = [\"setosa\", \"versicolor\", \"viginica\"]\n\n\ndef map_iris_types(predictions: int) -> List[str]:\n    return [iris_types[pred] for pred in predictions]\n```\n\n----------------------------------------\n\nTITLE: Search Registered Models API Endpoint\nDESCRIPTION: HTTP GET endpoint for searching registered models with filtering capabilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_24\n\nLANGUAGE: rest\nCODE:\n```\n2.0/mlflow/registered-models/search\n```\n\n----------------------------------------\n\nTITLE: Setting Up JavaScript and UI Development for MLflow\nDESCRIPTION: Steps for installing Node module dependencies, launching the development UI, running the JavaScript dev server, testing React components, and linting JavaScript code.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n### JavaScript and UI\n\n#### Install Node Module Dependencies\n\n#### Install Node Modules\n\n#### Launching the Development UI\n\n#### Running the Javascript Dev Server\n\n#### Testing a React Component\n\n#### Linting Javascript Code\n```\n\n----------------------------------------\n\nTITLE: Card Group Navigation Component\nDESCRIPTION: React component implementation showing a group of navigation cards linking to LLM evaluation tutorials and guides.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-evaluation/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <PageCard link=\"/llms/llm-evaluate/notebooks/rag-evaluation\" headerText=\"RAG Evaluation\" text=\"Learn how to evaluate a Retrieval Augmented Generation setup with MLflow Evaluate\" />\n  <PageCard link=\"/llms/llm-evaluate/notebooks/question-answering-evaluation/\" headerText=\"Question-Answering Evaluation\" text=\"See a working example of how to evaluate the quality of an LLM Question-Answering solution\" />\n  <PageCard link=\"/llms/rag/notebooks/question-generation-retrieval-evaluation/\" headerText=\"RAG Question Generation Evaluation\" text=\"See how to generate Questions for RAG generation and how to evaluate a RAG solution using MLflow\" />\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Single Run System Metrics Configuration\nDESCRIPTION: Example showing how to enable system metrics logging for a specific MLflow run using the log_system_metrics parameter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/system-metrics/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run(log_system_metrics=True) as run:\n    time.sleep(15)\n\nprint(mlflow.MlflowClient().get_run(run.info.run_id).data)\n```\n\n----------------------------------------\n\nTITLE: DatasetInput Structure Definition in MLflow REST API\nDESCRIPTION: Specifies the DatasetInput structure that combines dataset information with input tags. Used for tracking dataset usage in runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_35\n\nLANGUAGE: rest\nCODE:\n```\n+------------+-----------------------------------+----------------------------------------------------------------------------------+\n| Field Name |               Type                |                                   Description                                    |\n+============+===================================+==================================================================================+\n| tags       | An array of :ref:`mlflowinputtag` | A list of tags for the dataset input, e.g. a ?context? tag with value ?training? |\n+------------+-----------------------------------+----------------------------------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Tracking Server\nDESCRIPTION: R code to configure MLflow to use a remote tracking server\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/R/mlflow/README.md#2025-04-07_snippet_6\n\nLANGUAGE: r\nCODE:\n```\nmlflow_set_tracking_uri(\"http://tracking-server:5000\")\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Auto-Tracing for LiteLLM\nDESCRIPTION: Basic code to enable automatic tracing for LiteLLM calls in MLflow. This simple initialization is required before making any LiteLLM API calls that should be traced.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/litellm.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.litellm.autolog()\n```\n\n----------------------------------------\n\nTITLE: Running Ollama Server with LLM Model\nDESCRIPTION: This bash command starts the Ollama server with the llama3.2:1b model. This step is necessary before querying the LLM.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/ollama.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama3.2:1b\n```\n\n----------------------------------------\n\nTITLE: MLflow Gateway Endpoint Configuration\nDESCRIPTION: YAML configuration for setting up an embeddings endpoint in MLflow AI Gateway.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: mlflow-model-serving\n      name: sentence-transformer\n      config:\n        model_server_url: http://127.0.0.1:9020\n```\n\n----------------------------------------\n\nTITLE: MLflow AI Gateway Model Comparison Config\nDESCRIPTION: YAML configuration showing how to define multiple endpoints for comparing different models from the same provider\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n  - name: completions-gpt4\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4\n      config:\n        openai_api_key: $OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Running MLflow logging script\nDESCRIPTION: Command to execute the Python script that logs metrics to a local MLflow server. This assumes you've saved the script as log_mlflow_with_localhost.py.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/tracking-server-overview/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ python log_mlflow_with_localhost.py\n```\n\n----------------------------------------\n\nTITLE: Creating an MLflow Experiment\nDESCRIPTION: Sets up an MLflow experiment to track the execution of the LlamaIndex Workflow. This step can be skipped if running on a Databricks Notebook, which automatically creates an experiment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_experiment(\"MLflow LlamaIndex Workflow Tutorial\")\n```\n\n----------------------------------------\n\nTITLE: REST API Chat Endpoint Query\nDESCRIPTION: Curl command to query the chat endpoint in the MLflow AI Gateway. This example sends a creative writing request to generate a limerick about orange popsicles.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://my.deployments:8888/endpoints/chat/invocations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Can you write a limerick about orange flavored popsicles?\"}]}'\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Custom Auth Provider\nDESCRIPTION: Bash command to set the MLFLOW_TRACKING_AUTH environment variable to enable custom authentication in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_TRACKING_AUTH=dummy_auth_provider_name\n```\n\n----------------------------------------\n\nTITLE: Delete Registered Model Permission API Endpoint\nDESCRIPTION: REST endpoint for deleting permissions on registered models. Requires model name and username as input parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/auth/rest-api.rst#2025-04-07_snippet_3\n\nLANGUAGE: rest\nCODE:\n```\n2.0/mlflow/registered-models/permissions/delete\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Wine Quality Dataset\nDESCRIPTION: Loads the wine quality dataset, splits it into training/validation/test sets, and prepares the model signature\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load dataset\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n    sep=\";\",\n)\n\n# Split the data into training, validation, and test sets\ntrain, test = train_test_split(data, test_size=0.25, random_state=42)\ntrain_x = train.drop([\"quality\"], axis=1).values\ntrain_y = train[[\"quality\"]].values.ravel()\ntest_x = test.drop([\"quality\"], axis=1).values\ntest_y = test[[\"quality\"]].values.ravel()\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n    train_x, train_y, test_size=0.2, random_state=42\n)\nsignature = infer_signature(train_x, train_y)\n```\n\n----------------------------------------\n\nTITLE: Cloning and Configuring the MLflow Repository\nDESCRIPTION: Git commands to clone the MLflow repository, including submodules, and set up the upstream remote.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone --recurse-submodules git@github.com:<username>/mlflow.git\n# The alternative way of cloning through https may cause permission error during branch push\n# git clone --recurse-submodules https://github.com/<username>/mlflow.git\n\n# Add the upstream repository\ncd mlflow\ngit remote add upstream git@github.com:mlflow/mlflow.git\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Running model evaluation using MLflow evaluate with custom metrics on the dataset\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    results = mlflow.evaluate(\n        model_info.model_uri,\n        eval_df.head(10),\n        evaluators=\"default\",\n        model_type=\"text\",\n        targets=\"output\",\n        extra_metrics=[answer_correctness_metric, answer_quality_metric],\n        evaluator_config={\"col_mapping\": {\"inputs\": \"instruction\"}},\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Experiment Response Structure in MLflow REST API\nDESCRIPTION: JSON structure for the response when creating an experiment via the MLflow REST API. It contains the experiment ID.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"experiment_id\": \"STRING\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment Settings and Warnings\nDESCRIPTION: Sets up the environment by disabling tokenizer parallelism and filtering specific UserWarnings from setuptools and pydantic.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Prompt with LangChain in Python\nDESCRIPTION: Demonstrates how to load a prompt from MLflow and convert it to a format compatible with LangChain's PromptTemplate.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom langchain.prompts import PromptTemplate\n\n# Load prompt from MLflow\nprompt = mlflow.load_prompt(\"question_answering\")\n\n# Convert the prompt to single brace format for LangChain (MLflow uses double braces),\n# using the `to_single_brace_format` method.\nlangchain_prompt = PromptTemplate.from_template(prompt.to_single_brace_format())\nprint(langchain_prompt.input_variables)\n# Output: ['num_sentences', 'sentences']\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Serialization Helper Function\nDESCRIPTION: Helper function to serialize models using pickle or cloudpickle\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ndef _save_model(model, path, serialization_format):\n    with open(path, \"wb\") as out:\n        if serialization_format == SERIALIZATION_FORMAT_PICKLE:\n            pickle.dump(model, out)\n        else:\n            import cloudpickle\n\n            cloudpickle.dump(model, out)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow SQL Server Plugin\nDESCRIPTION: Command to install MLflow with SQL Server plugin support for using SQL Server as an artifact store.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow[sqlserver]\n```\n\n----------------------------------------\n\nTITLE: Setting Default Parameters in MLflow LLM Model Logging\nDESCRIPTION: Demonstrates how to override default parameter values when logging a transformers model with MLflow. Shows configuration of temperature and stop sequence parameters through the model_config dictionary during model logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/task/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        artifact_path=\"model\",\n        task=\"llm/v1/chat\",\n        model_config={\n            \"temperature\": 0.5,  # <= Set the default temperature\n            \"stop\": [\"foo\", \"bar\"],  # <= Set the default stop sequence\n        },\n        save_pretrained=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Extracting Experiment Metadata\nDESCRIPTION: Demonstrates how to extract specific metadata attributes from experiment objects into a dictionary structure.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step2-mlflow-client/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndefault_experiment = [\n    {\"name\": experiment.name, \"lifecycle_stage\": experiment.lifecycle_stage}\n    for experiment in all_experiments\n    if experiment.name == \"Default\"\n][0]\n\npprint(default_experiment)\n```\n\n----------------------------------------\n\nTITLE: Setting AI21 Labs API Key as Environment Variable in Shell\nDESCRIPTION: This command exports the AI21 Labs API key as an environment variable. Users need to replace the placeholder with their actual API key obtained from the AI21 Labs account.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/ai21_labs/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport AI21LABS_API_KEY=...\n```\n\n----------------------------------------\n\nTITLE: Initializing LangChain with OpenAI and MLflow\nDESCRIPTION: Python code to import required libraries and verify OpenAI API key environment variable\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\n\nimport mlflow\n\nassert \"OPENAI_API_KEY\" in os.environ, \"Please set the OPENAI_API_KEY environment variable.\"\n```\n\n----------------------------------------\n\nTITLE: Saving Custom MLflow Model\nDESCRIPTION: Demonstrates how to instantiate the custom AddN model and save it using MLflow's pyfunc module. Creates a model instance that adds 5 to all values.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/introduction.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define the path to save the model\nmodel_path = \"/tmp/add_n_model\"\n\n# Create an instance of the model with `n=5`\nadd5_model = AddN(n=5)\n\n# Save the model using MLflow\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n```\n\n----------------------------------------\n\nTITLE: Defining MlflowException class in Python\nDESCRIPTION: The MlflowException is the generic exception thrown when an MLflow operation fails. It accepts a message parameter for the error description, an optional error_code parameter (defaulting to 1), and additional keyword arguments.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/exceptions/mlflow.exceptions.rst#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmlflow.exceptions.MlflowException(message, error_code=1, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Search Experiments Output\nDESCRIPTION: Shows the output of searching experiments, displaying the default experiment's metadata.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step2-mlflow-client/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n[<Experiment: artifact_location='./mlruns/0', creation_time=None, experiment_id='0', last_update_time=None, lifecycle_stage='active', name='Default', tags={}>]\n```\n\n----------------------------------------\n\nTITLE: Running the Automated Development Environment Setup Script\nDESCRIPTION: Example of using the dev-env-setup.sh script to create a Python virtual environment for MLflow development.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndev/dev-env-setup.sh -d .venvs/mlflow-dev -q\n```\n\n----------------------------------------\n\nTITLE: Serving MLflow Models with MLServer\nDESCRIPTION: Command to deploy an MLflow model using MLServer as the serving engine. Requires installing additional dependencies with 'pip install mlflow[extras]' first.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve -m runs:/<run_id>/model -p 5000 --enable-mlserver\n```\n\n----------------------------------------\n\nTITLE: Installing JFrog MLflow Plugin\nDESCRIPTION: Commands to install the JFrog MLflow plugin using pip. This plugin allows storing MLflow artifacts in JFrog Artifactory.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow[jfrog]\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow-jfrog-plugin\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Traces for Demonstration in Python\nDESCRIPTION: Sets up sample traces using MLflow's tracing functionality to demonstrate various search and filtering capabilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/search.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport mlflow\nfrom mlflow.entities import SpanType\n\n\n# Define methods to be traced\n@mlflow.trace(span_type=SpanType.TOOL, attributes={\"time\": \"morning\"})\ndef morning_greeting(name: str):\n    time.sleep(1)\n    mlflow.update_current_trace(tags={\"person\": name})\n    return f\"Good morning {name}.\"\n\n\n@mlflow.trace(span_type=SpanType.TOOL, attributes={\"time\": \"evening\"})\ndef evening_greeting(name: str):\n    time.sleep(1)\n    mlflow.update_current_trace(tags={\"person\": name})\n    return f\"Good evening {name}.\"\n\n\n@mlflow.trace(span_type=SpanType.TOOL)\ndef goodbye():\n    raise Exception(\"Cannot say goodbye\")\n\n\n# Execute the methods within different experiments\nmorning_experiment = mlflow.set_experiment(\"Morning Experiment\")\nmorning_greeting(\"Tom\")\n\n# Get the timestamp in milliseconds\nmorning_time = int(time.time() * 1000)\n\nevening_experiment = mlflow.set_experiment(\"Evening Experiment\")\nexperiment_ids = [morning_experiment.experiment_id, evening_experiment.experiment_id]\nevening_greeting(\"Mary\")\ngoodbye()\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Recipes with Different Profiles in Python\nDESCRIPTION: This code demonstrates how to use the MLflow Recipe API to run recipes with different profile customizations. It shows examples of running regression recipes with ElasticNet and SGD regressors, as well as running in a shared workspace.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mlflow.recipes import Recipe\n\nos.chdir(\"~/recipes-regression-template\")\n# Run the regression recipe to train and evaluate the performance of an ElasticNet regressor\nregression_recipe_local_elasticnet = Recipe(profile=\"local-elasticnet\")\nregression_recipe_local_elasticnet.run()\n# Run the recipe again to train and evaluate the performance of an SGD regressor\nregression_recipe_local_sgd = Recipe(profile=\"local-sgd\")\nregression_recipe_local_sgd.run()\n# After finding the best model type and updating the 'shared-workspace' profile accordingly,\n# run the recipe again to retrain the best model in a workspace where teammates can view it\nregression_recipe_shared = Recipe(profile=\"shared-workspace\")\nregression_recipe_shared.run()\n```\n\n----------------------------------------\n\nTITLE: Testing Local MLflow Model Deployment with cURL\nDESCRIPTION: This cURL command sends a POST request to test the locally deployed MLflow model, simulating how it would behave in a SageMaker environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-to-sagemaker/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H \"Content-Type:application/json; format=pandas-split\" --data '{\"columns\":[\"a\",\"b\"],\"data\":[[1,2]]}' http://localhost:5000/invocations\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment with Volumes and Variables\nDESCRIPTION: Advanced Docker environment configuration that specifies volume mounts and environment variables for an MLflow project container, including both new variables and ones copied from the host.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ndocker_env:\n  image: mlflow-docker-example-environment\n  volumes: [\"/local/path:/container/mount/path\"]\n  environment: [[\"NEW_ENV_VAR\", \"new_var_value\"], \"VAR_TO_COPY_FROM_HOST_ENVIRONMENT\"]\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation for MLflow Environment Variables\nDESCRIPTION: ReStructuredText directive for generating automatic API documentation for the MLflow environment variables module. The configuration includes documenting all members and undocumented members of the module.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.environment_variables.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.environment_variables\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Deploying Model to REST Endpoint\nDESCRIPTION: Command to deploy the trained model as a local REST API endpoint using MLflow models serve command.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/flower_classifier/README.rst#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve --model-uri runs:/101/model --port 54321\n```\n\n----------------------------------------\n\nTITLE: Installing JavaScript Dependencies for MLflow UI\nDESCRIPTION: Commands to install JavaScript dependencies for the MLflow UI using Yarn.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncd mlflow/server/js\nyarn install\ncd - # return to root repository directory\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom LangChain Tracer in Python\nDESCRIPTION: Shows how to create a custom callback handler by inheriting from MlflowLangchainTracer to add additional attributes to chat model spans. Demonstrates overriding the on_chat_model_start handler to include custom version information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.langchain.langchain_tracer import MlflowLangchainTracer\n\n\nclass CustomLangchainTracer(MlflowLangchainTracer):\n    # Override the handler functions to customize the behavior. The method signature is defined by LangChain Callbacks.\n    def on_chat_model_start(\n        self,\n        serialized: Dict[str, Any],\n        messages: List[List[BaseMessage]],\n        *,\n        run_id: UUID,\n        tags: Optional[List[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Run when a chat model starts running.\"\"\"\n        attributes = {\n            **kwargs,\n            **metadata,\n            # Add additional attribute to the span\n            \"version\": \"1.0.0\",\n        }\n\n        # Call the _start_span method at the end of the handler function to start a new span.\n        self._start_span(\n            span_name=name or self._assign_span_name(serialized, \"chat model\"),\n            parent_run_id=parent_run_id,\n            span_type=SpanType.CHAT_MODEL,\n            run_id=run_id,\n            inputs=messages,\n            attributes=kwargs,\n        )\n```\n\n----------------------------------------\n\nTITLE: Setting MySQL SSL Environment Variables\nDESCRIPTION: Sets environment variables for SSL configuration when connecting to a MySQL database that requires SSL certificates.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/backend-stores/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Path to SSL CA certificate file\nexport MLFLOW_MYSQL_SSL_CA=/path/to/ca.pem\n\n# Path to SSL client certificate file (if needed)\nexport MLFLOW_MYSQL_SSL_CERT=/path/to/client-cert.pem\n\n# Path to SSL client key file (if needed)\nexport MLFLOW_MYSQL_SSL_KEY=/path/to/client-key.pem\n```\n\n----------------------------------------\n\nTITLE: Filtering and Chunking Documents for Question Generation\nDESCRIPTION: Selects a subset of MLflow documentation pages and splits them into manageable chunks using a character-based text splitter. Each chunk is associated with its source document and chunk index.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# For demonstration purposes, let's pick 5 popular MLflow documantation pages from the dataset\nmask = df[\"source\"].isin(\n    {\n        \"tracking.html\",\n        \"models.html\",\n        \"model-registry.html\",\n        \"search-runs.html\",\n        \"projects.html\",\n    }\n)\nsub_df = df[mask]\n\n# Split documents into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, separator=\" \")\n\n\ndef get_chunks(input_row):\n    new_rows = []\n    chunks = text_splitter.split_text(input_row[\"text\"])\n    for i, chunk in enumerate(chunks):\n        new_rows.append({\"chunk\": chunk, \"source\": input_row[\"source\"], \"chunk_index\": i})\n    return new_rows\n\n\nexpanded_df = pd.DataFrame(columns=[\"chunk\", \"source\", \"chunk_index\"])\n\nfor index, row in sub_df.iterrows():\n    new_rows = get_chunks(row)\n    expanded_df = pd.concat([expanded_df, pd.DataFrame(new_rows)], ignore_index=True)\n\nexpanded_df.head(3)\n```\n\n----------------------------------------\n\nTITLE: Delete Registered Model Alias Request Structure Definition\nDESCRIPTION: Defines the request structure for deleting an alias from a registered model. It requires both the registered model name and the exact alias name to be deleted.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_30\n\nLANGUAGE: proto\nCODE:\n```\n+------------+------------+---------------------------------------------------------------------------------------------------------------------+\n| Field Name |    Type    |                                                     Description                                                     |\n+============+============+=====================================================================================================================+\n| name       | ``STRING`` | Name of the registered model.                                                                                       |\n|            |            | This field is required.                                                                                             |\n|            |            |                                                                                                                     |\n+------------+------------+---------------------------------------------------------------------------------------------------------------------+\n| alias      | ``STRING`` | Name of the alias. The name must be an exact match; wild-card deletion is not supported. Maximum size is 256 bytes. |\n|            |            | This field is required.                                                                                             |\n|            |            |                                                                                                                     |\n+------------+------------+---------------------------------------------------------------------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Deploying John Snow Labs Model Without Container\nDESCRIPTION: Instructions for deploying a John Snow Labs model directly using MLflow's model serving functionality and querying the deployed endpoint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nexport JOHNSNOWLABS_LICENSE_JSON=your_json_string\nmlflow models serve -m <model_uri>\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{\n  \"dataframe_split\": {\n      \"columns\": [\"text\"],\n      \"data\": [[\"I hate covid\"], [\"I love covid\"]]\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation Metrics Results\nDESCRIPTION: Pretty-prints the evaluation metrics results that show the performance of the retriever model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\npp = pprint.PrettyPrinter(indent=4)\npp.pprint(evaluate_results.metrics)\n```\n\n----------------------------------------\n\nTITLE: Building MLflow Documentation for Release\nDESCRIPTION: Command to build documentation for both latest and versioned releases, generating static files in the build directory\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn build-all\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Service Integration with MLflow\nDESCRIPTION: Shows how to integrate Azure OpenAI Service with MLflow, including environment configuration and model usage with both native OpenAI and MLflow pyfunc interfaces.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/guide/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"<YOUR AZURE OPENAI MODEL>\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about {animal}.\"}],\n    )\n\n# Load native OpenAI model\nnative_model = mlflow.openai.load_model(model_info.model_uri)\ncompletion = openai.chat.completions.create(\n    deployment_id=native_model[\"deployment_id\"],\n    messages=native_model[\"messages\"],\n)\nprint(completion[\"choices\"][0][\"message\"][\"content\"])\n\n# Load as Pyfunc model\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame({\"animal\": [\"cats\", \"dogs\"]})\nprint(model.predict(df))\n```\n\n----------------------------------------\n\nTITLE: Setting Artifactory Authentication Token\nDESCRIPTION: Command to set the JFrog Artifactory authentication token as an environment variable for secure access.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport ARTIFACTORY_AUTH_TOKEN=<your artifactory token goes here>\n```\n\n----------------------------------------\n\nTITLE: Setting Up PyTorch Device Configuration\nDESCRIPTION: Detects and selects the appropriate device (GPU or CPU) for training the PyTorch model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Get cpu or gpu for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n```\n\n----------------------------------------\n\nTITLE: Creating Tabbed Interface for Trace Management Actions\nDESCRIPTION: Implements a tabbed interface to display different trace management actions (Search, Delete, Edit Tags) with separate content for each tab.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/ui.mdx#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<Tabs>\n  <TabItem value=\"search\" label=\"Search\" default>\n    Using the search bar in the UI, you can easily filter your traces based on name, tags, or other metadata. Check out the [search docs](/tracing/api/search/) for details about the query string format.\n\n    ![Searching traces](/images/llms/tracing/trace-session-id.gif)\n  </TabItem>\n  <TabItem value=\"delete\" label=\"Delete\">\n    The UI supports bulk deletion of traces. Simply select the traces you want to delete by checking the checkboxes, and then pressing the \"Delete\" button.\n\n    ![Deleting traces](/images/llms/tracing/trace-delete.gif)\n  </TabItem>\n  <TabItem value=\"edit-tags\" label=\"Edit Tags\">\n    You can also edit key-value tags on your traces via the UI.\n\n    ![Traces tag update](/images/llms/tracing/trace-set-tag.gif)\n  </TabItem>\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Compatible Endpoint in YAML\nDESCRIPTION: Shows how to configure an OpenAI-compatible endpoint in the MLflow AI Gateway configuration file. This allows the use of the OpenAI client to query the server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: my-chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Searching MLflow Experiments by Tag in Python\nDESCRIPTION: This code demonstrates how to search for experiments using a specific tag. It filters experiments based on the 'project_name' tag and prints the results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\napples_experiment = client.search_experiments(\n    filter_string=\"tags.`project_name` = 'grocery-forecasting'\"\n)\n\npprint(apples_experiment[0])\n```\n\n----------------------------------------\n\nTITLE: Batch Logging Metrics, Parameters, and Tags in MLflow with R\nDESCRIPTION: Function to log multiple metrics, parameters, and/or tags in a single batch operation. This is more efficient than logging items individually for multiple values.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_28\n\nLANGUAGE: r\nCODE:\n```\nmlflow_log_batch(\n  metrics = NULL,\n  params = NULL,\n  tags = NULL,\n  run_id = NULL,\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Project with Virtualenv\nDESCRIPTION: Command to execute a conda environment project as a virtualenv environment project using the MLflow CLI\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run /path/to/conda/project --env-manager virtualenv\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost Training Script with Custom Parameters in Bash\nDESCRIPTION: Command for executing the XGBoost training script with specific hyperparameters (learning rate, column subsampling, and row subsampling). This allows for experimentation with different model configurations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/xgboost/xgboost_native/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --learning-rate 0.2 --colsample-bytree 0.8 --subsample 0.9\n```\n\n----------------------------------------\n\nTITLE: Specifying MLflow Project Dependencies\nDESCRIPTION: This snippet defines the required Python packages and their versions for an MLflow project. It includes MLflow (version unspecified), cloudpickle version 2.2.1, and scikit-learn version 1.5.2.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/resources/example_mlflow_1x_sklearn_model/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlflow\ncloudpickle==2.2.1\nscikit-learn==1.5.2\n```\n\n----------------------------------------\n\nTITLE: Installing Typos CLI with Homebrew\nDESCRIPTION: Command to install the typos CLI tool using Homebrew package manager. The version should be replaced with the one specified in the dev/install-typos.sh file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/typos.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Replace `<version>` with the version installed in `dev/install-typos.sh`.\nbrew install typos-cli@<version>\n```\n\n----------------------------------------\n\nTITLE: Searching MLflow Experiments with Filter Strings\nDESCRIPTION: Examples demonstrating various filter string patterns for searching MLflow experiments. Shows how to filter by experiment name, tags, and combine multiple conditions using different comparators including equality, pattern matching (LIKE/ILIKE), and numeric comparisons.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-experiments/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Matches experiments with name equal to 'x'\n\"attributes.name = 'x'\"  # or \"name = 'x'\"\n\n# Matches experiments with name starting with 'x'\n\"attributes.name LIKE 'x%'\"\n\n# Matches experiments with 'group' tag value not equal to 'x'\n\"tags.group != 'x'\"\n\n# Matches experiments with 'group' tag value containing 'x' or 'X'\n\"tags.group ILIKE '%x%'\"\n\n# Matches experiments with name starting with 'x' and 'group' tag value equal to 'y'\n\"attributes.name LIKE 'x%' AND tags.group = 'y'\"\n```\n\n----------------------------------------\n\nTITLE: Starting the MLflow AI Gateway Server\nDESCRIPTION: Commands to start the MLflow AI Gateway service with a specific provider configuration file and port. Includes both a generic example and a specific example for OpenAI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/README.md#2025-04-07_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmlflow gateway start --config-path examples/gateway/<provider>/config.yaml --port 7000\n\n# For example:\nmlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000\n```\n\n----------------------------------------\n\nTITLE: Enabling txtai Autologging in Python\nDESCRIPTION: Python code snippet to import MLflow and enable autologging for txtai.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/txtai.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.txtai.autolog()\n```\n\n----------------------------------------\n\nTITLE: Restarting Python Runtime\nDESCRIPTION: Command to restart the Python runtime in Databricks to apply newly installed packages.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndbutils.library.restartPython()  # noqa: F821\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI Environment Variable\nDESCRIPTION: Sets the MLFLOW_TRACKING_URI environment variable to configure the backend store location.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/backend-stores/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_TRACKING_URI=\"file:/my/local/dir\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Sets the OpenAI API key as an environment variable for secure access by the MLflow AI Gateway.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/guides/step1-create-deployments/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: MLflow Credentials File Format\nDESCRIPTION: Shows the format for the MLflow credentials file used for authentication. The file is saved in INI format at ~/.mlflow/credentials and contains the username and password for MLflow authentication.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[mlflow]\nmlflow_tracking_username = username\nmlflow_tracking_password = password\n```\n\n----------------------------------------\n\nTITLE: Defining Docker Environment in MLproject File with YAML\nDESCRIPTION: YAML configuration in the MLproject file that specifies the Docker container environment to use for running the project. It references a local Docker image named 'mlflow-docker-example'.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/docker/README.rst#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndocker_env:\n  image:  mlflow-docker-example\n```\n\n----------------------------------------\n\nTITLE: Question Diversity Analysis using Embeddings\nDESCRIPTION: Implementation of question diversity analysis using PCA and TSNE for dimensionality reduction of question embeddings, with visualization using seaborn.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Apply embeddings\nembeddings = OpenAIEmbeddings()\nquestion_embeddings = embeddings.embed_documents(questions_to_embed)\n# PCA on embeddings to reduce to 10-dim\npca = PCA(n_components=10)\nquestion_embeddings_reduced = pca.fit_transform(question_embeddings)\n# TSNE on embeddings to reduce to 2-dim\ntsne = TSNE(n_components=2, random_state=SEED)\nlower_dim_embeddings = tsne.fit_transform(question_embeddings_reduced)\n\nlabels = np.concatenate(\n    [\n        np.full(len(lower_dim_embeddings) - len(benchmark_questions), \"generated\"),\n        np.full(len(benchmark_questions), \"benchmark\"),\n    ]\n)\ndata = pd.DataFrame(\n    {\"x\": lower_dim_embeddings[:, 0], \"y\": lower_dim_embeddings[:, 1], \"label\": labels}\n)\nsns.scatterplot(data=data, x=\"x\", y=\"y\", hue=\"label\")\n```\n\n----------------------------------------\n\nTITLE: Rendering Documentation Link Button in JSX\nDESCRIPTION: JSX code snippet for rendering a navigation link button to RAG tutorials using Docusaurus Link component\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<Link className=\"button button--primary\" to=\"notebooks\">\n  <span>View RAG Tutorials</span>\n</Link>\n```\n\n----------------------------------------\n\nTITLE: Update Registered Model Endpoint\nDESCRIPTION: REST endpoint for updating a registered model's metadata. Allows updating the description of a registered model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_14\n\nLANGUAGE: rest\nCODE:\n```\nPATCH 2.0/mlflow/registered-models/update\n```\n\n----------------------------------------\n\nTITLE: Evaluating Retriever Using MLflow\nDESCRIPTION: Demonstrates two approaches for evaluating retrievers: using static evaluation dataset and using a retriever function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Case 1: Evaluating a static evaluation dataset\nwith mlflow.start_run() as run:\n    evaluate_results = mlflow.evaluate(\n        data=data,\n        model_type=\"retriever\",\n        targets=\"source\",\n        predictions=\"retrieved_doc_ids\",\n        evaluators=\"default\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Up MLflow Experiment for Code Helper Application\nDESCRIPTION: This snippet sets up an MLflow experiment named 'Code Helper' to track all runs and models associated with the code assistant project, providing a foundation for model management and tracking.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"Code Helper\")\n```\n\n----------------------------------------\n\nTITLE: Rendering Feature Card Component in JSX/React\nDESCRIPTION: JSX/React component markup for displaying new feature cards with embedded links, images, and formatted descriptions\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/new-features/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <NewFeatureCard\n    description={\n      <>\n        <Link to=\"/llms/langchain\">LangGraph</Link>, the GenAI Agent authoring framework from LangChain, is now natively supported in MLflow using the <Link to=\"/model/models-from-code\">Models from Code</Link> feature.\n      </>\n    }\n    name=\"LangGraph Support\"\n    releaseVersion=\"2.16.0\"\n  >\n    <span>![LangGraph](/images/logos/langgraph-logo.png)</span>\n  </NewFeatureCard>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Unity Catalog Model Registration with MLflow Transformers\nDESCRIPTION: Demonstrates how to register a reference-only model to Databricks Unity Catalog Model Registry, where MLflow automatically handles downloading and registering model weights.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/large-models/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_registry_uri(\"databricks-uc\")\n\n# Log the repository ID as a model. The model weight will not be saved to the artifact store\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n        artifact_path=\"model\",\n    )\n\n# When registering the model to Unity Catalog Model Registry, MLflow will automatically\n# persist the model weight files. This may take a several minutes for large models.\nmlflow.register_model(model_info.model_uri, \"your.model.name\")\n```\n\n----------------------------------------\n\nTITLE: Complete Command Test Function Definition\nDESCRIPTION: Complete example showing how to add a new test case to the existing test_command_example function's parametrize decorator.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/examples/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize((\"directory\", \"command\"), [\n    ('sklearn_logistic_regression', ['python', 'train.py']),\n    ('h2o', ['python', 'random_forest.py']),\n    ('quickstart', ['python', 'mlflow_tracking.py']),\n    (\"new_example_dir\", [\"python\", \"train.py\"]),\n])\ndef test_command_example(tmpdir, directory, command):\n```\n\n----------------------------------------\n\nTITLE: Running OpenAI Autologging with Instantiated Client\nDESCRIPTION: Command to execute the example script that demonstrates MLflow autologging with an instantiated OpenAI client. Requires providing your OpenAI API key as a command-line argument.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/openai/autologging/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython examples/openai/autologging/instantiated_client.py --api-key=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Importing TensorFlow and MLflow Packages\nDESCRIPTION: Imports necessary packages including TensorFlow, TensorFlow datasets, and Keras.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\n```\n\n----------------------------------------\n\nTITLE: Serving an MLflow Model with Local REST API\nDESCRIPTION: Command to serve a trained MLflow model as a local REST API endpoint. This allows the model to be queried through HTTP requests.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n```\n\n----------------------------------------\n\nTITLE: Logging Metrics with Explicit Step and Timestamp in MLflow\nDESCRIPTION: Example showing how to manually specify both step and timestamp when logging metrics, which allows for more control over how metrics are displayed on the x-axis in MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.log_metric(key=\"train_loss\", value=train_loss, step=epoch, timestamp=now)\n```\n\n----------------------------------------\n\nTITLE: Searching Experiments with MLflow Client API in Python\nDESCRIPTION: This snippet shows how to search for all experiments using the MLflow client and print the results. It demonstrates basic experiment retrieval functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nall_experiments = client.search_experiments()\n\nprint(all_experiments)\n```\n\n----------------------------------------\n\nTITLE: Setting MosaicML API Key Environment Variable\nDESCRIPTION: Demonstrates how to set up the MosaicML API key as an environment variable for authentication purposes. This is required for accessing MosaicML's services including the mpt-7b-instruct, instructor-xl, and llama2-70b-chat models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mosaicml/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport MOSAICML_API_KEY=...\n```\n\n----------------------------------------\n\nTITLE: Rendering MLflow Feature Cards with NewFeatureCard Component in JSX\nDESCRIPTION: JSX code that uses NewFeatureCard components to display three new features in MLflow version 2.10.0: site overhaul, LangChain autologging, and object/array support for model signatures. Each card contains a name, description with links, release version, and an image.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/new-features/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: JSX\nCODE:\n```\n  <NewFeatureCard\n    name=\"MLflow Site Overhaul\"\n    description={\n      <>\n        MLflow has a new <Link to=\"https://mlflow.org\">homepage</Link> that has been completely modernized. Check it out today!\n      </>\n    }\n    releaseVersion=\"2.10.0\"\n  >\n    <span>![MLflow](/images/logos/homepage.png)</span>\n  </NewFeatureCard>\n  <NewFeatureCard\n    name=\"LangChain Autologging Support\"\n    description={\n      <>\n        Autologging support for <Link to=\"/llms/langchain\">LangChain</Link> is now available. Try it out the next time that you're building a Generative AI application with Langchain!\n      </>\n    }\n    releaseVersion=\"2.10.0\"\n  >\n    <span>![LangChain](/images/logos/langchain-logo.png)</span>\n  </NewFeatureCard>\n  <NewFeatureCard\n    name=\"Object and Array Support for complex Model Signatures\"\n    description={\n      <>\n        Complex input types for <Link to=\"/model/#model-signatures-and-input-examples\">model signatures</Link> are now supported with native support of Array and Object types.\n      </>\n    }\n    releaseVersion=\"2.10.0\"\n  >\n    <span>![MLflow](/images/logos/mlflow-logo.svg)</span>\n  </NewFeatureCard>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: A shell command that exports the OpenAI API key as an environment variable, which is required for OpenAI endpoint configurations in MLflow. Users need to replace the placeholder with their actual API key obtained from the OpenAI platform.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/openai/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport OPENAI_API_KEY=...\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for LLM Access\nDESCRIPTION: Sets up the OpenAI API key as an environment variable using a secure password input method.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/evaluate.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Running Fastai Example as MLflow Project (Local)\nDESCRIPTION: Commands to run the Fastai example as an MLflow Project from the current git directory, with default or custom parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/fastai/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -e main\n```\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -e main -P lr=0.02 -P epochs=3\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with Gemini\nDESCRIPTION: Example showing how to use Gemini's embedding API with MLflow tracing enabled.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/gemini.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult = client.models.embed_content(model=\"text-embedding-004\", contents=\"Hello world\")\n```\n\n----------------------------------------\n\nTITLE: Searching MLflow Experiments by Custom Tag in Python\nDESCRIPTION: This snippet demonstrates how to use the MLflow Client API to search for experiments based on a custom tag. It filters experiments where the 'project_name' tag matches 'grocery-forecasting'.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step4-experiment-search/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Use search_experiments() to search on the project_name tag key\n\napples_experiment = client.search_experiments(\n    filter_string=\"tags.`project_name` = 'grocery-forecasting'\"\n)\n\nprint(vars(apples_experiment[0]))\n```\n\n----------------------------------------\n\nTITLE: Loading Evaluation Dataset\nDESCRIPTION: Loads the Alpaca dataset from Hugging Face Hub and converts it to a pandas DataFrame for evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset = load_dataset(\"tatsu-lab/alpaca\")\neval_df = pd.DataFrame(dataset[\"train\"])\neval_df.head(10)\n```\n\n----------------------------------------\n\nTITLE: Tracing OpenAI Streaming Responses with MLflow\nDESCRIPTION: Example showing how to use MLflow tracing with OpenAI's streaming API, where MLflow automatically traces the streaming response and renders concatenated output in the span UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/openai.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport mlflow\n\n# Enable trace logging\nmlflow.openai.autolog()\n\nclient = openai.OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"How fast would a glass of water freeze on Titan?\"}\n    ],\n    stream=True,  # Enable streaming response\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n```\n\n----------------------------------------\n\nTITLE: Starting a local MLflow server with UI\nDESCRIPTION: Command to start a local MLflow server with user interface using the MLflow CLI tool. This will start a server that listens on localhost:5000 by default.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/tracking-server-overview/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ mlflow ui\n```\n\n----------------------------------------\n\nTITLE: Initializing H2O and Loading Wine Quality Dataset\nDESCRIPTION: Sets up H2O environment and loads wine quality dataset with train-test split. Creates a 70-30 random split of the data using H2O's runif functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/h2o/random_forest.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport h2o\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\n\nimport mlflow\nimport mlflow.h2o\n\nh2o.init()\n\nwine = h2o.import_file(path=\"wine-quality.csv\")\nr = wine[\"quality\"].runif()\ntrain = wine[r < 0.7]\ntest = wine[0.3 <= r]\n```\n\n----------------------------------------\n\nTITLE: Delete Model Version API Endpoint\nDESCRIPTION: REST endpoint for deleting a specific model version from the MLflow registry. Requires model name and version number as mandatory fields.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_20\n\nLANGUAGE: REST\nCODE:\n```\nDELETE 2.0/mlflow/model-versions/delete\n```\n\n----------------------------------------\n\nTITLE: Delete Registered Model Tag Endpoint\nDESCRIPTION: API endpoint documentation for deleting tags from registered models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_28\n\nLANGUAGE: rest\nCODE:\n```\n+---------------------------------------------+-------------+\n|                  Endpoint                   | HTTP Method |\n+=============================================+=============+\n| ``2.0/mlflow/registered-models/delete-tag`` | ``DELETE``  |\n+---------------------------------------------+-------------+\n```\n\n----------------------------------------\n\nTITLE: Filtering Document Chunks for Cost-Efficient Processing\nDESCRIPTION: Limits the number of chunks per document to reduce processing costs. This code selects only the first three chunks from each document source for demonstration purposes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# For cost-saving purposes, let's pick the first 3 chunks from each doc\n# To generate questions with more chunks, change the start index and end index in iloc[]\nstart, end = 0, 3\nfiltered_df = (\n    expanded_df.groupby(\"source\").apply(lambda x: x.iloc[start:end]).reset_index(drop=True)\n)\nfiltered_df.head(3)\n```\n\n----------------------------------------\n\nTITLE: Searching Datasets in MLflow using SQL\nDESCRIPTION: Examples of filtering MLflow runs by dataset information including name, digest, and context.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\ndatasets.name LIKE \"custom\"\ndatasets.digest IN ('s8ds293b', 'jks834s2')\ndatasets.context = \"train\"\n```\n\n----------------------------------------\n\nTITLE: Loading Documents and Creating Vector Index\nDESCRIPTION: Loads documents from URLs and creates a vector index using Qdrant vector store.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.readers.web import SimpleWebPageReader\n\nwith open(\"data/urls.txt\") as file:\n    urls = [line.strip() for line in file if line.strip()]\n\ndocuments = SimpleWebPageReader(html_to_text=True).load_data(urls)\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ docker pull qdrant/qdrant\n$ docker run -p 6333:6333 -p 6334:6334 \\\n    -v $(pwd)/.qdrant_storage:/qdrant/storage:z \\\n    qdrant/qdrant\n```\n\nLANGUAGE: python\nCODE:\n```\nimport qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nclient = qdrant_client.QdrantClient(host=\"localhost\", port=6333)\nvector_store = QdrantVectorStore(client=client, collection_name=\"mlflow_doc\")\n\nfrom llama_index.core import StorageContext, VectorStoreIndex\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents=documents, storage_context=storage_context)\n```\n\n----------------------------------------\n\nTITLE: Inferring Signature for DataFrame with Column Header in Python\nDESCRIPTION: Illustrates how to infer the signature for a pandas DataFrame with a single column of float64 values, showing MLflow's compatibility with structured data formats.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Adding a column header to a list of doubles\nmy_data = pd.DataFrame({\"input_data\": [np.float64(0.117), np.float64(1.99)]})\nreport_signature_info(my_data)\n```\n\n----------------------------------------\n\nTITLE: Loading Custom MLflow PyFunc Model\nDESCRIPTION: Loads the previously saved custom MLflow PyFunc model using the mlflow.pyfunc.load_model API. This allows the model to be used for predictions with dynamic method selection.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nloaded_dynamic = mlflow.pyfunc.load_model(pyfunc_path)\n```\n\n----------------------------------------\n\nTITLE: Suppressing Warning Messages in MLflow LangChain Autologging\nDESCRIPTION: Example showing how to suppress warning messages during MLflow LangChain autologging by setting the silent parameter to True.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.langchain.autolog(silent=True)\n\n# No warning messages will be emitted from autologging\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Multistep Workflow for Movie Rating Prediction\nDESCRIPTION: This bash command executes the multistep workflow from the project directory. It runs all steps in sequence, including data download, transformation, ALS model training, and Keras model training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/multistep_workflow/README.rst#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/multistep_workflow\nmlflow run .\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Server with SQLite Backend\nDESCRIPTION: Starts an MLflow tracking server using a SQLite database as the backend store.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/backend-stores/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --backend-store-uri sqlite:///mydb.sqlite\n```\n\n----------------------------------------\n\nTITLE: Running Fastai Example as MLflow Project (Remote)\nDESCRIPTION: Commands to run the Fastai example as an MLflow Project from outside the git repository, with default or custom parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/fastai/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run https://github.com/mlflow/mlflow/#examples/fastai\n```\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run https://github.com/mlflow/mlflow/#examples/fastai -P lr=0.02 -P epochs=3\n```\n\n----------------------------------------\n\nTITLE: Searching Traces with MLflow API in Python\nDESCRIPTION: Demonstrates how to use the mlflow.search_traces() API to query traces across experiments with various filtering options.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/search.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Search traces in the current experiment\ntraces = mlflow.search_traces(filter_string=\"status = 'ERROR'\")\n\n# Search traces in specific experiments\ntraces = mlflow.search_traces(\n    experiment_ids=[\"experiment_id_1\", \"experiment_id_2\"],\n    filter_string=\"name = 'predict'\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Project with Custom Parameters\nDESCRIPTION: This command demonstrates how to run the MLflow project with multiple custom parameters, including epochs, devices, batch size, workers, learning rate, strategy, and accelerator.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/BertNewsClassification/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P max_epochs=5 -P devices=1 -P batch_size=32 -P num_workers=2 -P learning_rate=0.01 -P strategy=\"ddp\" -P accelerator=gpu\n```\n\n----------------------------------------\n\nTITLE: Selecting and Chunking MLflow Documentation for Question Generation\nDESCRIPTION: Selects specific MLflow documentation pages and splits them into manageable chunks. The function creates a new dataframe with individual chunks along with their source document and position index.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# For demonstration purposes, let's pick 5 popular MLflow documantation pages from the dataset\nmask = df[\"source\"].isin(\n    {\n        \"tracking.html\",\n        \"models.html\",\n        \"model-registry.html\",\n        \"search-runs.html\",\n        \"projects.html\",\n    }\n)\nsub_df = df[mask]\n\n# Split documents into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, separator=\" \")\n\n\ndef get_chunks(input_row):\n    new_rows = []\n    chunks = text_splitter.split_text(input_row[\"text\"])\n    for i, chunk in enumerate(chunks):\n        new_rows.append({\"chunk\": chunk, \"source\": input_row[\"source\"], \"chunk_index\": i})\n    return new_rows\n\n\nexpanded_df = pd.DataFrame(columns=[\"chunk\", \"source\", \"chunk_index\"])\n\nfor index, row in sub_df.iterrows():\n    new_rows = get_chunks(row)\n    expanded_df = pd.concat([expanded_df, pd.DataFrame(new_rows)], ignore_index=True)\n\nexpanded_df.head(3)\n```\n\n----------------------------------------\n\nTITLE: Loading and Testing Semantic Search Model\nDESCRIPTION: Demonstrates loading the model and making predictions with a sample query to validate functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nloaded_dynamic = mlflow.pyfunc.load_model(model_info.model_uri)\n\nloaded_dynamic.predict([\"I'd like some ideas for a meal to cook.\"])\n```\n\n----------------------------------------\n\nTITLE: Loading Hugging Face Pipeline\nDESCRIPTION: Initializes a text generation pipeline using the MPT-7B-Chat model from MosaicML.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmpt_pipeline = pipeline(\"text-generation\", model=\"mosaicml/mpt-7b-chat\")\n```\n\n----------------------------------------\n\nTITLE: Updating LLM Settings After Loading\nDESCRIPTION: Shows how to update LLM settings after loading a model for different inference configurations\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Load the index back\nloaded_index = mlflow.llama_index.load_model(model_info.model_uri)\n\nassert Settings.llm.model == \"gpt-4o-mini\"\n\n\n# Update the settings to use GPT-4 instead\nSettings.llm = OpenAI(\"gpt-4\")\nquery_engine = loaded_index.as_query_engine()\nresponse = query_engine.query(\"What is the capital of France?\")\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Tracing for DSPy\nDESCRIPTION: A simple code snippet showing how to enable automatic tracing for DSPy modules using MLflow's autolog function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/dspy.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.dspy.autolog()\n```\n\n----------------------------------------\n\nTITLE: Building a Docker Image for MLflow Project\nDESCRIPTION: YAML and bash snippets showing how to specify a base image that MLflow will use to build a custom Docker image for running a project with the --build-image flag.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ndocker_env:\n  image: python:3.8\n```\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run ... --build-image\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Project with Local Environment\nDESCRIPTION: This command runs the MLflow project using the local environment, skipping the creation of a conda environment. It's useful when you already have the required modules installed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/BertNewsClassification/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . --env-manager=local\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for MLflow Retriever Evaluation\nDESCRIPTION: Installs necessary Python packages including MLflow, LangChain, OpenAI, FAISS, and other dependencies for retriever evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install mlflow==2.9.0 langchain==0.0.339 openai faiss-cpu gensim nltk pyLDAvis tiktoken\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for MLflow Retriever Evaluation\nDESCRIPTION: Installs necessary Python packages including MLflow, LangChain, OpenAI, FAISS, and other dependencies for retriever evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install mlflow==2.9.0 langchain==0.0.339 openai faiss-cpu gensim nltk pyLDAvis tiktoken\n```\n\n----------------------------------------\n\nTITLE: Logging and Registering a Scikit-learn Model with MLflow\nDESCRIPTION: Demonstrates training a RandomForestRegressor model and registering it in the MLflow Model Registry using log_model(). The example includes model training, signature inference, parameter/metric logging, and model registration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.models import infer_signature\n\nwith mlflow.start_run() as run:\n    X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    params = {\"max_depth\": 2, \"random_state\": 42}\n    model = RandomForestRegressor(**params)\n    model.fit(X_train, y_train)\n\n    # Infer the model signature\n    y_pred = model.predict(X_test)\n    signature = infer_signature(X_test, y_pred)\n\n    # Log parameters and metrics using the MLflow APIs\n    mlflow.log_params(params)\n    mlflow.log_metrics({\"mse\": mean_squared_error(y_test, y_pred)})\n\n    # Log the sklearn model and register as version 1\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        artifact_path=\"sklearn-model\",\n        signature=signature,\n        registered_model_name=\"sk-learn-random-forest-reg-model\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Gateway Server (Bash)\nDESCRIPTION: Bash commands to set environment variables and start the MLflow Gateway server with Unity Catalog integration enabled.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/uc_integration/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport DATABRICKS_HOST=\"...\"\nexport DATABRICKS_TOKEN=\"...\"\nexport DATABRICKS_WAREHOUSE_ID=\"...\"\nexport OPENAI_API_KEY=\"...\"\nexport MLFLOW_ENABLE_UC_FUNCTIONS=true\nmlflow gateway start --config-path examples/gateway/deployments_server/openai/config.yaml --port 7000\n```\n\n----------------------------------------\n\nTITLE: Model Version Stage Transition API Endpoint\nDESCRIPTION: HTTP POST endpoint for transitioning a model version's stage, with support for archiving existing versions. Deprecated feature that will be removed in future major release.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_22\n\nLANGUAGE: rest\nCODE:\n```\n2.0/mlflow/model-versions/transition-stage\n```\n\n----------------------------------------\n\nTITLE: Defining a Lyrics Correction Prompt Template for OpenAI GPT\nDESCRIPTION: Creates a template string for the lyrics correction application that will be used with OpenAI's Completions API. The prompt instructs the model to identify correct lyrics, provide song details, and give humorous explanations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-quickstart.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlyrics_prompt = (\n    \"Here's a misheard lyric: {lyric}. What's the actual lyric, which song does it come from, which artist performed it, and can you give a funny \"\n    \"explanation as to why the misheard version doesn't make sense? Also, rate the creativity of the lyric on a scale of 1 to 3, where 3 is good.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Inferring Tensor-Based Signatures\nDESCRIPTION: Example showing how to infer tensor-based signatures from numpy arrays with specific shapes and data types.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import infer_signature\n\ninfer_signature(\n    model_input=np.array(\n        [\n            [[1, 2, 3], [4, 5, 6]],\n            [[7, 8, 9], [1, 2, 3]],\n        ]\n    )\n)\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsignature:\n    input: '[{\"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"int64\", \"shape\": [-1, 2, 3]}}]'\n    output: None\n    params: None\n```\n\n----------------------------------------\n\nTITLE: Predicting with Missing Required Fields in MLflow Model\nDESCRIPTION: Demonstrates how omitting required fields in a model prediction call will raise an exception from schema enforcement. This example shows how MLflow's signature validation works by attempting to predict with an input missing the required fields 'a' and 'c'.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nloaded_model.predict([{\"b\": \"b\"}])\n```\n\n----------------------------------------\n\nTITLE: Importing RAG System Dependencies\nDESCRIPTION: Importing necessary Langchain components for creating the RAG system, including document loading, embedding, and vector store functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/rag-evaluation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n```\n\n----------------------------------------\n\nTITLE: Inferring Tensor-Based Signatures\nDESCRIPTION: Example showing how to infer tensor-based signatures from numpy arrays with specific shapes and data types.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import infer_signature\n\ninfer_signature(\n    model_input=np.array(\n        [\n            [[1, 2, 3], [4, 5, 6]],\n            [[7, 8, 9], [1, 2, 3]],\n        ]\n    )\n)\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsignature:\n    input: '[{\"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"int64\", \"shape\": [-1, 2, 3]}}]'\n    output: None\n    params: None\n```\n\n----------------------------------------\n\nTITLE: Running MLflow MNIST Example\nDESCRIPTION: Command to run the MNIST example using MLflow in the default configuration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/MNIST/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run .\n```\n\n----------------------------------------\n\nTITLE: Adding Optional Configurations in local.yaml Profile\nDESCRIPTION: This YAML snippet shows how to add optional configurations in a local.yaml profile. It specifies a sqlite-based MLflow Tracking store for local testing on a laptop.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nexperiment:\n  tracking_uri: \"sqlite:///metadata/mlflow/mlruns.db\"\n  name: \"sklearn_regression_experiment\"\n  artifact_location: \"./metadata/mlflow/mlartifacts\"\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for MLflow LLM Evaluation\nDESCRIPTION: Initial setup importing the necessary Python libraries including OpenAI, pandas, and MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/qa-evaluation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport pandas as pd\n\nimport mlflow\n```\n\n----------------------------------------\n\nTITLE: Streaming Response Request to MLflow AI Gateway Chat Endpoint using cURL\nDESCRIPTION: This example shows how to make a POST request to the MLflow AI Gateway chat endpoint with streaming enabled using cURL.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://my.deployments:8888/endpoints/chat/invocations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"hello\"}], \"stream\": true}'\n```\n\n----------------------------------------\n\nTITLE: Fetching a Model Version by Alias from MLflow Model Registry\nDESCRIPTION: Shows how to load a model version by alias from the Model Registry. This approach allows for dynamic model updates by changing the alias assignment without modifying code.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow.pyfunc\n\nmodel_name = \"sk-learn-random-forest-reg-model\"\nalias = \"champion\"\n\nchampion_version = mlflow.pyfunc.load_model(f\"models:/{model_name}@{alias}\")\n\nchampion_version.predict(data)\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Metadata Configuration\nDESCRIPTION: Shows the structure of MLflow metadata configuration including model flavors, prompt templates, and signature definitions in YAML format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nflavors:\n  transformers:\n    peft_adaptor: peft\n    pipeline_model_type: MistralForCausalLM\n    source_model_name: mistralai/Mistral-7B-v0.1.\n    source_model_revision: xxxxxxx\n    task: text-generation\n    torch_dtype: torch.bfloat16\n    tokenizer_type: LlamaTokenizerFast\n\nmetadata:\n  prompt_template: 'You are a powerful text-to-SQL model. Given the SQL tables and\n    natural language question, your job is to write SQL query that answers the question.\n\n\n    {prompt}\n\n\n    ### Response:\n\n    '\nsignature:\n  inputs: '[{\"type\": \"string\", \"required\": true}]'\n  outputs: '[{\"type\": \"string\", \"required\": true}]'\n  params: '[{\"name\": \"max_new_tokens\", \"type\": \"long\", \"default\": 256, \"shape\": null},\n    {\"name\": \"repetition_penalty\", \"type\": \"double\", \"default\": 1.15, \"shape\": null},\n    {\"name\": \"return_full_text\", \"type\": \"boolean\", \"default\": false, \"shape\": null}]'\n```\n\n----------------------------------------\n\nTITLE: Deleting a Prompt Version with MLflow Python API\nDESCRIPTION: Shows how to delete a specific version of a prompt using the mlflow.delete_prompt API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Delete a prompt version\nmlflow.delete_prompt(\"summarization-prompt\", version=2)\n```\n\n----------------------------------------\n\nTITLE: Training GBM Model with MLflow Parameters in R\nDESCRIPTION: This script trains a Gradient Boosting Machine (GBM) model on the Iris dataset using MLflow for parameter tracking. It demonstrates how to use MLflow parameters and can be run as an MLflow project with custom parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_33\n\nLANGUAGE: r\nCODE:\n```\n# This parametrized script trains a GBM model on the Iris dataset and can be run as an MLflow\n# project. You can run this script (assuming it's saved at /some/directory/params_example.R)\n# with custom parameters via:\n# mlflow_run(entry_point = \"params_example.R\", uri = \"/some/directory\",\n#   parameters = list(num_trees = 200, learning_rate = 0.1))\ninstall.packages(\"gbm\")\nlibrary(mlflow)\nlibrary(gbm)\n# define and read input parameters\nnum_trees <- mlflow_param(name = \"num_trees\", default = 200, type = \"integer\")\nlr <- mlflow_param(name = \"learning_rate\", default = 0.1, type = \"numeric\")\n# use params to fit a model\nir.adaboost <- gbm(Species ~., data=iris, n.trees=num_trees, shrinkage=lr)\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Run with Tags\nDESCRIPTION: Starts an MLflow run with tags, including the run name specified in the tags dictionary.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmlflow.start_run(tags={\"mlflow.runName\": \"run_name\", ...})\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Training as MLflow Project\nDESCRIPTION: This command executes the LightGBM training as an MLflow project, specifying hyperparameters as project parameters. It demonstrates how to run the experiment in a reproducible environment using MLflow's project functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/lightgbm/lightgbm_native/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Transformers Pipeline Components in Python\nDESCRIPTION: This snippet demonstrates how to save a Transformers pipeline and load its individual components. It shows the process of creating a translation pipeline, logging it with MLflow, and then loading and reconstructing the pipeline from its components.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nimport mlflow\n\ntranslation_pipeline = transformers.pipeline(\n    task=\"translation_en_to_fr\",\n    model=transformers.T5ForConditionalGeneration.from_pretrained(\"t5-small\"),\n    tokenizer=transformers.T5TokenizerFast.from_pretrained(\n        \"t5-small\", model_max_length=100\n    ),\n)\n\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=translation_pipeline,\n        artifact_path=\"french_translator\",\n    )\n\ntranslation_components = mlflow.transformers.load_model(\n    model_info.model_uri, return_type=\"components\"\n)\n\nfor key, value in translation_components.items():\n    print(f\"{key} -> {type(value).__name__}\")\n\n# >> task -> str\n# >> model -> T5ForConditionalGeneration\n# >> tokenizer -> T5TokenizerFast\n\nresponse = translation_pipeline(\"MLflow is great!\")\n\nprint(response)\n\n# >> [{'translation_text': 'MLflow est formidable!'}]\n\nreconstructed_pipeline = transformers.pipeline(**translation_components)\n\nreconstructed_response = reconstructed_pipeline(\n    \"transformers makes using Deep Learning models easy and fun!\"\n)\n\nprint(reconstructed_response)\n\n# >> [{'translation_text': \"Les transformateurs rendent l'utilisation de modèles Deep Learning facile et amusante!\"}]\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Model Signature for Validation\nDESCRIPTION: Displays the inferred model signature to validate the expected input and output formats along with configurable parameters. This visual confirmation ensures the signature correctly captures the model's requirements before logging to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Visualize the model signature\nsignature\n```\n\n----------------------------------------\n\nTITLE: Controlling Input Conversion for LangChain in MLflow PyFunc\nDESCRIPTION: This code demonstrates how to control whether MLflow converts input messages to LangChain BaseMessage objects. It shows how to set an environment variable to disable automatic conversion when using PyFunc predict.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport mlflow\nimport os\nfrom operator import itemgetter\nfrom langchain.schema.runnable import RunnablePassthrough\n\nmodel = RunnablePassthrough.assign(\n    problem=lambda x: x[\"messages\"][-1][\"content\"]\n) | itemgetter(\"problem\")\n\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Hello\",\n        }\n    ]\n}\n# this model accepts the input_example\nassert model.invoke(input_example) == \"Hello\"\n\n# set this environment variable to avoid input conversion\nos.environ[\"MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN\"] = \"false\"\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(model, \"model\", input_example=input_example)\n\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nassert pyfunc_model.predict(input_example) == [\"Hello\"]\n```\n\n----------------------------------------\n\nTITLE: Get Model Version by Alias Request Structure Definition\nDESCRIPTION: Defines the request structure for retrieving a model version using an alias. It requires the registered model name and the alias name to look up the corresponding version.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_31\n\nLANGUAGE: proto\nCODE:\n```\n+------------+------------+-----------------------------------------------+\n| Field Name |    Type    |                  Description                  |\n+============+============+===============================================+\n| name       | ``STRING`` | Name of the registered model.                 |\n|            |            | This field is required.                       |\n|            |            |                                               |\n+------------+------------+-----------------------------------------------+\n| alias      | ``STRING`` | Name of the alias. Maximum size is 256 bytes. |\n|            |            | This field is required.                       |\n|            |            |                                               |\n+------------+------------+-----------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Retrieving Run Data in R with MLflow\nDESCRIPTION: Gets metadata, params, tags, and metrics for a run. Returns the most recent metric value at the largest step for each metric key. Requires run ID and optionally an MLflow client object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_21\n\nLANGUAGE: r\nCODE:\n```\nmlflow_get_run(run_id = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Installing System Metrics Dependencies\nDESCRIPTION: Commands to install required dependencies for system metrics logging including psutil for basic metrics and pynvml/pyrsmi for GPU metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/system-metrics/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install psutil\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install pynvml\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install pyrsmi\n```\n\n----------------------------------------\n\nTITLE: Loading a Saved MLflow Custom Python Model\nDESCRIPTION: Loads a previously saved custom model using MLflow's pyfunc module, making it available for use in code review applications.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nloaded_helper = mlflow.pyfunc.load_model(helper_model.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Testing the Translation Pipeline with a Sample Sentence\nDESCRIPTION: Evaluates the translation pipeline by translating a sample English sentence to French. This step verifies the pipeline's functionality before proceeding with MLflow logging and ensures the model and tokenizer are working correctly together.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate the pipeline on a sample sentence prior to logging\ntranslation_pipeline(\n    \"translate English to French: I enjoyed my slow saunter along the Champs-Élysées.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Custom React Component in Markdown\nDESCRIPTION: This snippet imports a custom React component called NotebookDownloadButton from a specific file path. It's used to create a download button for the MLflow model notebook.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { NotebookDownloadButton } from \"@site/src/components/NotebookDownloadButton\";\n```\n\n----------------------------------------\n\nTITLE: Getting the Model URI\nDESCRIPTION: Shows how to retrieve the model URI which is needed for model serving and other operations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel_info.model_uri\n```\n\n----------------------------------------\n\nTITLE: Running MLflow example with specific parameter values\nDESCRIPTION: Command showing concrete parameter values for the Ax hyperparameter optimization example. This runs the experiment with 3 epochs and 3 total trials.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/AxHyperOptimizationPTL/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P max_epochs=3 -P total_trials=3\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Iris Classification Example with Custom Parameters\nDESCRIPTION: This command demonstrates how to override default parameters when running the MLflow project, specifically setting a custom value for the 'epochs' parameter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/IrisClassification/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P epochs=X\n```\n\n----------------------------------------\n\nTITLE: MLflow UI Server Setup and Logging\nDESCRIPTION: Example showing how to start the MLflow UI server and log system metrics to it for visualization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/system-metrics/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport time\n\nmlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\nwith mlflow.start_run() as run:\n    time.sleep(15)\n```\n\n----------------------------------------\n\nTITLE: Logging a Parameter in MLflow using R\nDESCRIPTION: This function logs a parameter for a run in MLflow. It's used for logging string key-value pairs such as hyperparameters or constant values. Each parameter can only be logged once per run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_31\n\nLANGUAGE: r\nCODE:\n```\nmlflow_log_param(key, value, run_id = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Bedrock/Groq Tracing Implementation\nDESCRIPTION: Shows how to implement auto-tracing for Amazon Bedrock and Groq LLMs using mlflow.bedrock.tracing or mlflow.groq.tracing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for PostgreSQL and MinIO\nDESCRIPTION: Sets up Docker Compose configuration for PostgreSQL database and MinIO server as backend and artifact stores.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"3.7\"\nservices:\n  # PostgreSQL database\n  postgres:\n    image: postgres:latest\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n      POSTGRES_DB: mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - ./postgres-data:/var/lib/postgresql/data\n  # MinIO server\n  minio:\n    image: minio/minio\n    expose:\n      - \"9000\"\n    ports:\n      - \"9000:9000\"\n      # MinIO Console is available at http://localhost:9001\n      - \"9001:9001\"\n    environment:\n      MINIO_ROOT_USER: \"minio_user\"\n      MINIO_ROOT_PASSWORD: \"minio_password\"\n    healthcheck:\n      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1\n      interval: 1s\n      timeout: 10s\n      retries: 5\n    command: server /data --console-address \":9001\"\n  # Create a bucket named \"bucket\" if it doesn't exist\n  minio-create-bucket:\n    image: minio/mc\n    depends_on:\n      minio:\n        condition: service_healthy\n    entrypoint: >\n      bash -c \"\n      mc alias set minio http://minio:9000 minio_user minio_password &&\n      if ! mc ls minio/bucket; then\n        mc mb minio/bucket\n      else\n        echo 'bucket already exists'\n      fi\n      \"\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for PostgreSQL and MinIO\nDESCRIPTION: Sets up Docker Compose configuration for PostgreSQL database and MinIO server as backend and artifact stores.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"3.7\"\nservices:\n  # PostgreSQL database\n  postgres:\n    image: postgres:latest\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n      POSTGRES_DB: mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - ./postgres-data:/var/lib/postgresql/data\n  # MinIO server\n  minio:\n    image: minio/minio\n    expose:\n      - \"9000\"\n    ports:\n      - \"9000:9000\"\n      # MinIO Console is available at http://localhost:9001\n      - \"9001:9001\"\n    environment:\n      MINIO_ROOT_USER: \"minio_user\"\n      MINIO_ROOT_PASSWORD: \"minio_password\"\n    healthcheck:\n      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1\n      interval: 1s\n      timeout: 10s\n      retries: 5\n    command: server /data --console-address \":9001\"\n  # Create a bucket named \"bucket\" if it doesn't exist\n  minio-create-bucket:\n    image: minio/mc\n    depends_on:\n      minio:\n        condition: service_healthy\n    entrypoint: >\n      bash -c \"\n      mc alias set minio http://minio:9000 minio_user minio_password &&\n      if ! mc ls minio/bucket; then\n        mc mb minio/bucket\n      else\n        echo 'bucket already exists'\n      fi\n      \"\n```\n\n----------------------------------------\n\nTITLE: Ending MLflow Span\nDESCRIPTION: Demonstrates how to properly end a child span using the end_span API. Shows setting of outputs and additional attributes when ending the span.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/client.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# End the child span\nclient.end_span(\n    request_id=child_span.request_id,\n    span_id=child_span.span_id,\n    outputs={\"output_key\": \"output_value\"},\n    attributes={\"custom_attribute\": \"value\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Increasing Gunicorn Timeout for MLflow Server in Bash\nDESCRIPTION: This bash command shows how to start the MLflow server with increased Gunicorn timeout to handle large artifact uploads/downloads.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --gunicorn-opts \"--timeout=60\" ...\n```\n\n----------------------------------------\n\nTITLE: Loading DSPy Program Directly\nDESCRIPTION: Demonstrates how to load a DSPy program directly without PyFunc wrapping.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = mlflow.dspy.load_model(model_uri)\n```\n\n----------------------------------------\n\nTITLE: Sending Test Request to MLflow Model Server\nDESCRIPTION: Curl command to send a test request with JSON input data to the local MLflow model server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-locally/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:5000/invocations -H \"Content-Type:application/json\"  --data '{\"inputs\": [[1, 2], [3, 4], [5, 6]]}'\n```\n\n----------------------------------------\n\nTITLE: Logging LlamaIndex Indices in MLflow\nDESCRIPTION: Shows how to log LlamaIndex indices within MLflow for later loading and deployment. This is part of the new LlamaIndex integration in MLflow 2.15.0.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Assuming 'index' is a LlamaIndex index object\nmlflow.llamaindex.log_model(index, \"my_llamaindex_model\")\n```\n\n----------------------------------------\n\nTITLE: Referencing Python Environment in MLproject File\nDESCRIPTION: YAML snippet showing how to reference a Python virtualenv configuration file in an MLproject file using the python_env field and a relative path.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\npython_env: files/config/python_env.yaml\n```\n\n----------------------------------------\n\nTITLE: Scoring Images via REST API\nDESCRIPTION: Command to score new images using the deployed REST API endpoint using the score_images_rest.py script.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/flower_classifier/README.rst#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython score_images_rest.py --host http://127.0.0.1 --port 54321 /path/to/images/for/scoring\n```\n\n----------------------------------------\n\nTITLE: Rendering Card Components for Retriever Evaluation Tutorial with Inline Code in JSX\nDESCRIPTION: Creates a card interface with JSX that includes inline code elements to highlight the metrics used in the retriever evaluation tutorial.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <PageCard\n    headerText=\"Retriever Evaluation with MLflow\"\n    link=\"/llms/rag/notebooks/retriever-evaluation-tutorial/\"\n    text={\n      <>\n        Learn how to leverage MLflow to evaluate the performance of a retriever in a RAG application, leveraging built-in retriever metrics <code>precision_at_k</code> and <code>recall_at_k</code>.\n      </>\n    }\n  />\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Running MLflow with Custom Tracking URI Plugin\nDESCRIPTION: Example command to run an MLflow quickstart script using a custom file-plugin tracking URI scheme.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nMLFLOW_TRACKING_URI=file-plugin:$(PWD)/mlruns python examples/quickstart/mlflow_tracking.py\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for MLflow and Scikit-learn in Python\nDESCRIPTION: This code snippet installs the necessary libraries (MLflow and scikit-learn) using pip. It uses the Jupyter magic command %pip to run the installation within the notebook environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade mlflow scikit-learn -q\n```\n\n----------------------------------------\n\nTITLE: Comparing Model Accuracy in Python\nDESCRIPTION: Function to evaluate classifier accuracy on test data by comparing predictions against actual values. Includes execution for both uncompiled and compiled models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef check_accuracy(classifier, test_data: pd.DataFrame = test_dataset) -> float:\n    residuals = []\n    predictions = []\n    for example in test_data:\n        prediction = classifier(text=example[\"text\"])\n        residuals.append(int(validate_classification(example, prediction)))\n        predictions.append(prediction)\n    return residuals, predictions\n\n\nuncompiled_residuals, uncompiled_predictions = check_accuracy(copy(TextClassifier()))\nprint(f\"Uncompiled accuracy: {np.mean(uncompiled_residuals)}\")\n\ncompiled_residuals, compiled_predictions = check_accuracy(compiled_pe)\nprint(f\"Compiled accuracy: {np.mean(compiled_residuals)}\")\n```\n\n----------------------------------------\n\nTITLE: Displaying MLflow UI in a Jupyter Notebook\nDESCRIPTION: Code to embed an iframe showing the MLflow UI at localhost:5000 within a Jupyter notebook. This allows interactive exploration of the MLflow interface without leaving the notebook environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_quickstart.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nIFrame(src=\"http://localhost:5000\", width=1000, height=600)\n```\n\n----------------------------------------\n\nTITLE: Listing Artifacts in MLflow with R\nDESCRIPTION: Function to list artifacts for a specific run. Parameters allow specifying the relative artifact path and run ID.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_24\n\nLANGUAGE: r\nCODE:\n```\nmlflow_list_artifacts(path = NULL, run_id = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Associating Traces with MLflow Runs\nDESCRIPTION: Shows how to associate traces with an MLflow run by generating them within a run context. The example creates multiple spans within a run context that will be associated with the active run in the MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/how-to.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Create and activate an Experiment\nmlflow.set_experiment(\"Run Associated Tracing\")\n\n# Start a new MLflow Run\nwith mlflow.start_run() as run:\n    # Initiate a trace by starting a Span context from within the Run context\n    with mlflow.start_span(name=\"Run Span\") as parent_span:\n        parent_span.set_inputs({\"input\": \"a\"})\n        parent_span.set_outputs({\"response\": \"b\"})\n        parent_span.set_attribute(\"a\", \"b\")\n        # Initiate a child span from within the parent Span's context\n        with mlflow.start_span(name=\"Child Span\") as child_span:\n            child_span.set_inputs({\"input\": \"b\"})\n            child_span.set_outputs({\"response\": \"c\"})\n            child_span.set_attributes({\"b\": \"c\", \"c\": \"d\"})\n```\n\n----------------------------------------\n\nTITLE: Evaluating Retriever Models with Custom K Value Using evaluator_config\nDESCRIPTION: This example demonstrates how to evaluate a retriever model using the mlflow.evaluate API with a custom retriever_k value. It shows how to use the evaluator_config parameter to set the value of retriever_k to 5.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.metrics.rst#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# with a model and using `evaluator_config`\nmlflow.evaluate(\n    model=retriever_function,\n    data=data,\n    targets=\"ground_truth\",\n    model_type=\"retriever\",\n    evaluators=\"default\",\n    evaluator_config={\"retriever_k\": 5}\n)\n```\n\n----------------------------------------\n\nTITLE: Inferring Signature for Complex Pandas DataFrame in Python\nDESCRIPTION: Shows how MLflow infers signatures for a pandas DataFrame containing mixed data types, including nested structures and optional fields.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Pandas DF input\n\ndata = [\n    {\"a\": \"a\", \"b\": [\"a\", \"b\", \"c\"], \"c\": {\"d\": 1, \"e\": 0.1}, \"f\": [{\"g\": \"g\"}, {\"h\": 1}]},\n    {\"b\": [\"a\", \"b\"], \"c\": {\"d\": 2, \"f\": \"f\"}, \"f\": [{\"g\": \"g\"}]},\n]\ndata = pd.DataFrame(data)\n\nreport_signature_info(data)\n```\n\n----------------------------------------\n\nTITLE: Component Area Labels for MLflow\nDESCRIPTION: Labels identifying different component areas within MLflow, such as artifacts, build infrastructure, documentation, and specific services.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/ISSUE_TRIAGE.rst#2025-04-07_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n- area/artifacts\n- area/build\n- area/docs\n- area/examples\n- area/gateway\n- area/model-registry\n- area/models\n- area/recipes\n- area/projects\n- area/scoring\n- area/server-infra\n- area/tracking\n```\n\n----------------------------------------\n\nTITLE: MLflow Multi-threaded Tracing Implementation\nDESCRIPTION: Demonstrates thread-safe MLflow tracing using context variables, ThreadPoolExecutor and OpenAI integration. Shows context propagation across threads.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport contextvars\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport mlflow\nfrom mlflow.entities import SpanType\nimport openai\n\nclient = openai.OpenAI()\n\n# Enable MLflow Tracing for OpenAI\nmlflow.openai.autolog()\n\n\n@mlflow.trace\ndef worker(question: str) -> str:\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": question},\n    ]\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=messages,\n        temperature=0.1,\n        max_tokens=100,\n    )\n    return response.choices[0].message.content\n\n\n@mlflow.trace\ndef main(questions: list[str]) -> list[str]:\n    results = []\n    # Almost same as how you would use ThreadPoolExecutor, but two additional steps\n    #  1. Copy the context in the main thread using copy_context()\n    #  2. Use ctx.run() to run the worker in the copied context\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        futures = []\n        for question in questions:\n            ctx = contextvars.copy_context()\n            futures.append(executor.submit(ctx.run, worker, question))\n        for future in as_completed(futures):\n            results.append(future.result())\n    return results\n\n\nquestions = [\n    \"What is the capital of France?\",\n    \"What is the capital of Germany?\",\n]\n\nmain(questions)\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow AI Gateway\nDESCRIPTION: Shell command to start the MLflow AI Gateway with specified configuration file and port.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nmlflow gateway start --config-path examples/gateway/mlflow_serving/config.yaml --port 7000\n```\n\n----------------------------------------\n\nTITLE: Re-enabling the Trace UI in Jupyter\nDESCRIPTION: Shows how to re-enable the Trace UI after it has been disabled. This restores the automatic display functionality for trace objects.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmlflow.tracing.enable_notebook_display()\n\n# re-enable display\ntrace\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Tracking Server\nDESCRIPTION: Command to start the MLflow tracking server in the /tmp directory\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rest_api/README.rst#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server\n```\n\n----------------------------------------\n\nTITLE: Making Predictions - Bash\nDESCRIPTION: Command to make predictions using the deployed model with input data from a JSON file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/ray_serve/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmlflow deployments predict -t ray-serve --name iris:v1 --input-path input.json\n```\n\n----------------------------------------\n\nTITLE: Defining Model Signature with Dynamic Prediction Method in MLflow\nDESCRIPTION: Creates a model signature using infer_signature, specifying a dynamic predict_method parameter. This allows for flexible prediction method selection when using the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Define the signature associated with the model\nsignature = infer_signature(x_train, params={\"predict_method\": \"predict_proba\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter for HTTP Protocol and Custom Headers in Bash\nDESCRIPTION: This snippet shows how to configure the OTLP Exporter to use HTTP protocol instead of the default gRPC and set custom headers. It sets environment variables for the OTLP traces endpoint, protocol, and headers.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/production.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=\"http://localhost:4317/v1/traces\"\nexport OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=\"http/protobuf\"\nexport OTEL_EXPORTER_OTLP_TRACES_HEADERS=\"api_key=12345\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Sentence Transformer Model\nDESCRIPTION: Imports necessary libraries and initializes the 'all-MiniLM-L6-v2' Sentence Transformer model, which is a compact and efficient model for generating meaningful sentence embeddings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nimport mlflow\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Sentence Transformer Model\nDESCRIPTION: Imports necessary libraries and initializes the 'all-MiniLM-L6-v2' Sentence Transformer model, which is a compact and efficient model for generating meaningful sentence embeddings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nimport mlflow\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\n----------------------------------------\n\nTITLE: Creating UnityCatalog Function Toolkit\nDESCRIPTION: Initializes a toolkit with the created UnityCatalog functions, which will be used as tools by the agent.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom unitycatalog.ai.langchain.toolkit import UCFunctionToolkit\n\ntoolkit = UCFunctionToolkit(\n    function_names=[\n        python_execution_function_name,\n        translate_function_name,\n    ]\n)\ntools = toolkit.tools\n```\n\n----------------------------------------\n\nTITLE: Complete MLflow logging script for local server\nDESCRIPTION: Full Python script that connects to a local MLflow server, creates an experiment, and logs metrics. This can be saved as a .py file and executed to test the connection.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/tracking-server-overview/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"check-localhost-connection\")\n\nwith mlflow.start_run():\n    mlflow.log_metric(\"foo\", 1)\n    mlflow.log_metric(\"bar\", 2)\n```\n\n----------------------------------------\n\nTITLE: Complete MLflow logging script for local server\nDESCRIPTION: Full Python script that connects to a local MLflow server, creates an experiment, and logs metrics. This can be saved as a .py file and executed to test the connection.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/tracking-server-overview/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"check-localhost-connection\")\n\nwith mlflow.start_run():\n    mlflow.log_metric(\"foo\", 1)\n    mlflow.log_metric(\"bar\", 2)\n```\n\n----------------------------------------\n\nTITLE: Language Surface Labels for MLflow\nDESCRIPTION: Labels indicating programming language-specific components and APIs in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/ISSUE_TRIAGE.rst#2025-04-07_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n- language/r\n- language/java\n- language/new\n```\n\n----------------------------------------\n\nTITLE: Creating a New MLflow User via REST API\nDESCRIPTION: Example of how to create a new MLflow user by sending a POST request to the user creation endpoint. This requires admin authentication to perform the user creation operation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresponse = requests.post(\n    \"https://<mlflow_tracking_uri>/api/2.0/mlflow/users/create\",\n    json={\n        \"username\": \"username\",\n        \"password\": \"password\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Setting GEMINI API Key in Shell Environment\nDESCRIPTION: This code snippet demonstrates how to set the GEMINI API key as an environment variable in a shell. The API key is required for authenticating requests to the GEMINI API when using it with MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/gemini/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport GEMINI_API_KEY=...\n```\n\n----------------------------------------\n\nTITLE: Viewing Model Components\nDESCRIPTION: Displays the available components in the loaded translation model dictionary.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntranslation_components.keys()\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for MLflow\nDESCRIPTION: This code snippet lists the required Python packages and their version constraints for the MLflow project. It includes web frameworks, asynchronous libraries, AWS SDK, and rate limiting tools.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/requirements/gateway-requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfastapi<1\nuvicorn[standard]<1\nwatchfiles<2\naiohttp<4\nboto3<2,>=1.28.56\ntiktoken<1\nslowapi<1,>=0.1.9\n```\n\n----------------------------------------\n\nTITLE: Testing Online Inference Endpoint with MLflow Serve\nDESCRIPTION: Shows how to test a model by running an online inference server in a virtual environment using the MLflow serve API, including sending test requests using curl.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve -m runs:/<run_id>/model -p <port>\n# In another terminal\ncurl -X POST -H \"Content-Type: application/json\" \\\n    --data '{\"inputs\": [[1, 2], [3, 4]]}' \\\n    http://localhost:<port>/invocations\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI\nDESCRIPTION: Command to start the MLflow user interface, which allows viewing of logged experiments and runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Training Logistic Regression Model on Iris Dataset\nDESCRIPTION: Load the Iris dataset, split it into training and test sets, and train a logistic regression model with specified hyperparameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/notebooks/tracking_quickstart.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load the Iris dataset\nX, y = datasets.load_iris(return_X_y=True)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the model hyperparameters\nparams = {\"solver\": \"lbfgs\", \"max_iter\": 1000, \"multi_class\": \"auto\", \"random_state\": 8888}\n\n# Train the model\nlr = LogisticRegression(**params)\nlr.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = lr.predict(X_test)\n\n# Calculate accuracy as a target loss metric\naccuracy = accuracy_score(y_test, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Defining Model Name Constants in Python\nDESCRIPTION: This snippet defines constants for model names used throughout the notebook. It sets prefixes for day-of-week models and a name for the main model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/serving-multiple-models-with-pyfunc/notebooks/MME_Tutorial.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDOW_MODEL_NAME_PREFIX = \"DOW_model_\"\nMME_MODEL_NAME = \"MME_DOW_model\"\n```\n\n----------------------------------------\n\nTITLE: Loading MLflow Model Flavor in R\nDESCRIPTION: Low-level function for loading a specific MLflow model flavor. This function is used internally by mlflow_load_model but is exposed for package authors to extend supported MLflow models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_25\n\nLANGUAGE: r\nCODE:\n```\nmlflow_load_flavor(flavor, model_path)\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Server with Custom Authentication Config (Bash)\nDESCRIPTION: This bash command demonstrates how to start the MLflow server using a custom authentication configuration file. It sets the MLFLOW_AUTH_CONFIG_PATH environment variable to the path of the configuration file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nMLFLOW_AUTH_CONFIG_PATH=/path/to/my_auth_config.ini mlflow server --app-name basic-auth\n```\n\n----------------------------------------\n\nTITLE: Displaying a Trace Object Explicitly with IPython\nDESCRIPTION: Shows how to explicitly display a trace object using IPython's display() function. Even when the last expression in a cell doesn't result in a trace, using display() will still trigger the UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Explicitly calling `display()`\ntrace = mlflow.get_last_active_trace()\ndisplay(trace)\n\n# Even if the last expression does not result in a trace,\n# display(trace) will still trigger the UI display\nprint(\"Test\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Retriever Models with Custom K Values Using extra_metrics\nDESCRIPTION: This code shows how to evaluate a retriever model with different k values by using the extra_metrics parameter in mlflow.evaluate. It demonstrates specifying multiple metrics with different k values for precision, recall, and NDCG.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.metrics.rst#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# with a static dataset and using `extra_metrics`\nmlflow.evaluate(\n    data=data,\n    predictions=\"predictions_param\",\n    targets=\"targets_param\",\n    model_type=\"retriever\",\n    extra_metrics = [\n        mlflow.metrics.precision_at_k(5),\n        mlflow.metrics.precision_at_k(6),\n        mlflow.metrics.recall_at_k(5),\n        mlflow.metrics.ndcg_at_k(5)\n    ]   \n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Answer Correctness Metric\nDESCRIPTION: Initializing the predefined answer correctness metric using GPT-4 as the judge\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nanswer_correctness_metric = answer_correctness(model=\"openai:/gpt-4\")\n```\n\n----------------------------------------\n\nTITLE: Building Model Environment for Databricks Connect\nDESCRIPTION: Demonstrates how to build and save a model environment for use with Databricks Connect in remote client scenarios.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_81\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.pyfunc import build_model_env\n\n# Build the model env and save it as an archive file to the provided UC volume directory\n# and print the saved model env archive file path (like '/Volumes/.../.../XXXXX.tar.gz')\nprint(build_model_env(model_uri, \"/Volumes/...\"))\n\n# print the cluster id. Databricks Connect client needs to use the cluster id.\nprint(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\"))\n```\n\n----------------------------------------\n\nTITLE: Preparing Movie Review Data for spaCy Training\nDESCRIPTION: Processes movie review data from NLTK, converting it to spaCy's DocBin format for sentiment analysis training. Includes functions for data preparation and format conversion.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport spacy\nfrom nltk.corpus import movie_reviews\nfrom spacy import Language\nfrom spacy.tokens import DocBin\n\nndef get_sentences(sentiment_type: str) -> pd.DataFrame:\n    file_ids = movie_reviews.fileids(sentiment_type)\n    sent_df = []\n    for file_id in file_ids:\n        sentence = \" \".join(movie_reviews.words(file_id))\n        sent_df.append({\"sentence\": sentence, \"sentiment\": sentiment_type})\n    return pd.DataFrame(sent_df)\n\ndef convert(data_df: pd.DataFrame, target_file: str):\n    nlp = spacy.blank(\"en\")\n    sentiment_labels = data_df.sentiment.unique()\n    spacy_doc = DocBin()\n\n    for _, row in data_df.iterrows():\n        sent_tokens = nlp.make_doc(row[\"sentence\"])\n        for label in sentiment_labels:\n            sent_tokens.cats[label] = 1.0 if label == row[\"sentiment\"] else 0.0\n        spacy_doc.add(sent_tokens)\n\n    spacy_doc.to_disk(target_file)\n\nreview_data = [get_sentences(sentiment_type) for sentiment_type in (\"pos\", \"neg\")]\nreview_data = pd.concat(review_data, axis=0)\n\ntrain_df = review_data.groupby(\"sentiment\", group_keys=False).apply(\n    lambda x: x.sample(frac=0.7, random_state=100)\n)\ndev_df = review_data.loc[review_data.index.difference(train_df.index), :]\n\nconvert(train_df, \"corpora.train\")\nconvert(dev_df, \"corpora.dev\")\n```\n\n----------------------------------------\n\nTITLE: Modified Parameter Logging Function\nDESCRIPTION: Example of modifying the parameter logging function to remove a parameter value from the choices.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef log_run(run_name, test_no):\n    with mlflow.start_run(run_name=run_name):\n        mlflow.log_param(\"param1\", random.choice([\"a\", \"c\"]))  # remove 'b'\n        # remainder of code ...\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow UI for Exploration\nDESCRIPTION: Starts the MLflow UI in a background process to explore logged artifacts and experiment details.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport subprocess\n\nfrom IPython.display import IFrame\n\n# Start the MLflow UI in a background process\nmlflow_ui_command = [\"mlflow\", \"ui\", \"--port\", \"5000\"]\nsubprocess.Popen(\n    mlflow_ui_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid\n)\n```\n\n----------------------------------------\n\nTITLE: Getting Registered Model in R with MLflow\nDESCRIPTION: Retrieves a registered model from the Model Registry. Requires the model name and optionally an MLflow client object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_20\n\nLANGUAGE: r\nCODE:\n```\nmlflow_get_registered_model(name, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Callback for Logging\nDESCRIPTION: Demonstrates using MLflow callback to log metrics during model training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.tensorflow import MlflowCallback\n\nmlflow.tensorflow.autolog(disable=True)\n\nwith mlflow.start_run() as run:\n    model.fit(\n        x=train_ds,\n        epochs=2,\n        callbacks=[MlflowCallback(run)],\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow with GenAI Dependencies\nDESCRIPTION: Installs MLflow package with genai extras to get access to serving-related dependencies including uvicorn and fastapi.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/guides/step1-create-deployments/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'mlflow[genai]'\n```\n\n----------------------------------------\n\nTITLE: Deprecating Requirements File Argument in MLflow Model Functions\nDESCRIPTION: The 'requirements_file' argument is deprecated for model saving and logging functions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\n# Deprecated:\nmlflow.*.save_model(..., requirements_file=...)\nmlflow.*.log_model(..., requirements_file=...)\n```\n\n----------------------------------------\n\nTITLE: Including Custom Library Package in MLflow Model\nDESCRIPTION: Example demonstrating how to include custom wheel files as dependencies when logging an MLflow model. Shows integration of custom packages that aren't available on PyPI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom custom_package import my_func\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        x = my_func(model_input)\n        # .. your prediction logic\n        return prediction\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        artifact_path=\"model\",\n        extra_pip_requirements=[\"code/custom_package.whl\"],\n        input_example=input_data,\n        code_paths=[\"custom_package.whl\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Live Python Documentation Preview\nDESCRIPTION: Commands to generate a live preview of Python and RST documentation. Note that R and Java API docs require separate generation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nmake livehtml\n```\n\n----------------------------------------\n\nTITLE: Using Links and Components in MDX\nDESCRIPTION: Import statements for React components used in the documentation including Cards and API links.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-evaluation/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport { CardGroup, PageCard } from \"@site/src/components/Card\";\nimport { APILink } from \"@site/src/components/APILink\";\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Script Directly with Custom Parameters\nDESCRIPTION: Command to run the Titanic_Captum_Interpret.py script directly with custom parameters instead of using MLflow. This allows direct execution with specified maximum epochs and learning rate.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/CaptumExample/README.md#2025-04-07_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython Titanic_Captum_Interpret.py \\\n    --max_epochs 50 \\\n    --lr 0.1\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Tracking Server with Authentication\nDESCRIPTION: Command to start the MLflow tracking server with basic authentication enabled using the app-name parameter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/auth/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmlflow ui --app-name=basic-auth\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure for MLflow Configuration Module in RST\nDESCRIPTION: This snippet defines how the MLflow configuration module documentation should be structured using Sphinx's RST format. It uses the automodule directive to automatically generate documentation for all members, undocumented members, and inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.config.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.config\n==============\n\n.. automodule:: mlflow.config\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI to view experiment results\nDESCRIPTION: Command to start the MLflow UI server for viewing experiment results. This launches a web interface accessible at http://localhost:5000.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/AxHyperOptimizationPTL/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Updated R API for Running MLflow Projects\nDESCRIPTION: The R API for running MLflow projects has been modified to more closely match the Python API. This example shows the new argument order and naming convention.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_63\n\nLANGUAGE: R\nCODE:\n```\nmlflow_run(entry_point = \"entry_point\", uri = \"uri\", parameters = list(param1 = \"value1\"))\n```\n\n----------------------------------------\n\nTITLE: Updating a Prompt with MLflow Python API\nDESCRIPTION: Shows how to update an existing prompt with a new version using the mlflow.register_prompt API. The example includes modifying the template and adding version metadata.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nnew_template = \"\"\"\nYou are an expert summarizer. Condense the following content into exactly {{ num_sentences }} clear and informative sentences that capture the key points.\n\nSentences: {{ sentences }}\n\nYour summary should:\n- Contain exactly {{ num_sentences }} sentences\n- Include only the most important information\n- Be written in a neutral, objective tone\n- Maintain the same level of formality as the original text\n\"\"\"\n\n# Register a new version of an existing prompt\nupdated_prompt = mlflow.register_prompt(\n    name=\"summarization-prompt\",  # Specify the existing prompt name\n    template=new_template,\n    commit_message=\"Improvement\",\n    version_metadata={\n        \"author\": \"author@example.com\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Web Search API\nDESCRIPTION: Configures Tavily AI search API for web search capability in the RAG system.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nos.environ[\"TAVILY_AI_API_KEY\"] = getpass.getpass(\"Enter your Tavily AI APi Key\")\n```\n\n----------------------------------------\n\nTITLE: Ordering Trace Search Results in Python\nDESCRIPTION: Demonstrates how to order trace search results using the order_by parameter in the mlflow.search_traces() API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/search.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmlflow.search_traces(\n    experiment_ids=experiment_ids, order_by=[\"timestamp_ms DESC\"]  # Most recent first\n)\n\nmlflow.search_traces(\n    experiment_ids=experiment_ids, order_by=[\"timestamp_ms DESC\", \"status ASC\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Sourcing MLflow Script in R\nDESCRIPTION: Function to source an R script with MLflow parameters. Designed for non-interactive use via Rscript or MLflow CLI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_53\n\nLANGUAGE: r\nCODE:\n```\nmlflow_source(uri)\n```\n\n----------------------------------------\n\nTITLE: Tracing LiteLLM's Streaming API with MLflow\nDESCRIPTION: Example showing how MLflow supports tracing streaming API calls with LiteLLM. MLflow will record the concatenated outputs from stream chunks as a span output.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/litellm.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.litellm.autolog()\n\nresponse = litellm.completion(\n    model=\"claude-3-5-sonnet-20240620\",\n    messages=[{\"role\": \"user\", \"content\": \"Hey! how's it going?\"}],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk.choices[0].delta.content, end=\"|\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Latest Model Versions in R with MLflow\nDESCRIPTION: Gets a list of the latest model versions for a given model. Requires the model name, optional list of stages, and optionally an MLflow client object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_17\n\nLANGUAGE: r\nCODE:\n```\nmlflow_get_latest_versions(name, stages = list(), client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Exploring MLflow Model directory structure example\nDESCRIPTION: Example directory structure created when saving a scikit-learn model using MLflow. The structure includes the MLmodel file, model serialization, dependency specifications, and optional input examples.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Directory written by mlflow.sklearn.save_model(model, \"model\", input_example=...)\nmodel/\n├── MLmodel\n├── model.pkl\n├── conda.yaml\n├── python_env.yaml\n├── requirements.txt\n├── input_example.json (optional, only logged when input example is provided and valid during model logging)\n├── serving_input_example.json (optional, only logged when input example is provided and valid during model logging)\n└── environment_variables.txt (optional, only logged when environment variables are used during model inference)\n```\n\n----------------------------------------\n\nTITLE: Defining All Dependencies Manually for MLflow Model in Python\nDESCRIPTION: Example of logging an MLflow scikit-learn model with manually specified pip requirements, overriding default dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.sklearn.log_model(\n        sk_model=model,\n        artifact_path=\"model\",\n        pip_requirements=[\n            \"mlflow-skinny==2.9.2\",\n            \"cloudpickle==2.5.8\",\n            \"scikit-learn==1.3.1\",\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Exploring MLflow Model directory structure example\nDESCRIPTION: Example directory structure created when saving a scikit-learn model using MLflow. The structure includes the MLmodel file, model serialization, dependency specifications, and optional input examples.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Directory written by mlflow.sklearn.save_model(model, \"model\", input_example=...)\nmodel/\n├── MLmodel\n├── model.pkl\n├── conda.yaml\n├── python_env.yaml\n├── requirements.txt\n├── input_example.json (optional, only logged when input example is provided and valid during model logging)\n├── serving_input_example.json (optional, only logged when input example is provided and valid during model logging)\n└── environment_variables.txt (optional, only logged when environment variables are used during model inference)\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow AutoLogging for DeepSeek via OpenAI SDK\nDESCRIPTION: This snippet shows how to enable MLflow's automatic logging for DeepSeek models using the OpenAI SDK integration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/deepseek.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Enable\nmlflow.openai.autolog()\n```\n\n----------------------------------------\n\nTITLE: Executing MLflow Spark UDF Example with Datetime Input in Python\nDESCRIPTION: This command shows how to run the spark_udf_datetime.py example, which demonstrates using Spark UDF with input data of datetime type.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/spark_udf/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython spark_udf_datetime.py\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow UI Server\nDESCRIPTION: Command to start the MLflow UI server locally on the default port 5000\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow AI Gateway\nDESCRIPTION: This command starts the MLflow AI Gateway server using the specified configuration file and port.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/huggingface/README.md#2025-04-07_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nmlflow deplyments start-server --config-path examples/gateway/huggingface/config.yaml --port 7000\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Test Container in Docker\nDESCRIPTION: Command to set up the MLflow development environment using a Docker container. This creates a containerized development environment with the name 'mlflow-test'.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ndev/run-test-container.sh\n```\n\n----------------------------------------\n\nTITLE: MLflow Tracing with OpenAI Chat Integration\nDESCRIPTION: Example combining MLflow tracing decorator, context manager and OpenAI auto-logging in a chat application. Shows span type specification and message handling.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.entities import SpanType\n\n\n@mlflow.trace(span_type=SpanType.CHAIN)\ndef start_session():\n    messages = [{\"role\": \"system\", \"content\": \"You are a friendly chat bot\"}]\n    while True:\n        with mlflow.start_span(name=\"User\") as span:\n            span.set_inputs(messages)\n            user_input = input(\">>\") \n            span.set_outputs(user_input)\n\n        if user_input == \"BYE\":\n            break\n\n        messages.append({\"role\": \"user\", \"content\": user_input})\n\n        response = openai.OpenAI().chat.completions.create(\n            model=\"gpt-4o-mini\",\n            max_tokens=100,\n            messages=messages,\n        )\n        answer = response.choices[0].message.content\n        print(f\"🤖: {answer}\")\n\n        messages.append({\"role\": \"assistant\", \"content\": answer})\n\n\nmlflow.openai.autolog()\nstart_session()\n```\n\n----------------------------------------\n\nTITLE: Extracting Run or Experiment ID from MLflow Objects in R\nDESCRIPTION: Functions to extract the ID from an MLflow run or experiment object. Three interface implementations are provided for different object types.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_23\n\nLANGUAGE: r\nCODE:\n```\nmlflow_id(object)\nlist(list(\"mlflow_id\"), list(\"mlflow_run\"))(object)\nlist(list(\"mlflow_id\"), list(\"mlflow_experiment\"))(object)\n```\n\n----------------------------------------\n\nTITLE: MLflow Server Host Configuration\nDESCRIPTION: Command-line option for configuring the host address when running an MLflow serving instance. This allows the server to listen on non-local addresses.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_67\n\nLANGUAGE: Shell\nCODE:\n```\nmlflow serve --host\n```\n\n----------------------------------------\n\nTITLE: Multiple Model Loading Limitation Example\nDESCRIPTION: Illustrates the limitation of loading multiple models with same module names but different implementations in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport importlib\nimport sys\nimport tempfile\nfrom pathlib import Path\n\nimport mlflow\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    tmpdir = Path(tmpdir)\n    my_model_path = tmpdir / \"my_model.py\"\n    code_template = \"\"\"\nimport mlflow\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        return [{n}] * len(model_input)\n\"\"\"\n\n    my_model_path.write_text(code_template.format(n=1))\n\n    sys.path.insert(0, str(tmpdir))\n    import my_model\n\n    # model 1\n    model1 = my_model.MyModel()\n    assert model1.predict(context=None, model_input=[0]) == [1]\n\n    with mlflow.start_run():\n        info1 = mlflow.pyfunc.log_model(\n            artifact_path=\"model\",\n            python_model=model1,\n            code_paths=[my_model_path],\n        )\n\n    # model 2\n    my_model_path.write_text(code_template.format(n=2))\n    importlib.reload(my_model)\n    model2 = my_model.MyModel()\n    assert model2.predict(context=None, model_input=[0]) == [2]\n\n    with mlflow.start_run():\n        info2 = mlflow.pyfunc.log_model(\n            artifact_path=\"model\",\n            python_model=model2,\n            code_paths=[my_model_path],\n        )\n\n# To simulate a fresh Python process, remove the `my_model` module from the cache\nsys.modules.pop(\"my_model\")\n\n# Now we have two models that depend on modules with the same name but different implementations.\n# Let's load them and check the prediction results.\npred = mlflow.pyfunc.load_model(info1.model_uri).predict([0])\nassert pred == [1], pred  # passes\n\n# As the `my_model` module was loaded and cached in the previous `load_model` call,\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow Types Module in reStructuredText\nDESCRIPTION: This RST code snippet sets up the documentation structure for the MLflow types module and its submodules. It uses automodule directives to generate documentation from the source code.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.types.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.types\n==============\n\n.. automodule:: mlflow.types\n    :members:\n    :show-inheritance:\n\n.. automodule:: mlflow.types.agent\n    :members:\n\n.. automodule:: mlflow.types.llm\n    :members:\n\n.. automodule:: mlflow.types.chat\n    :members:\n\n.. automodule:: mlflow.types.schema\n    :members: Array, Map, Object, Property, AnyType\n    :undoc-members:\n\n.. automodule:: mlflow.types.llm._BaseDataclass\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Invalid Embeddings JSON Response Format\nDESCRIPTION: Example of an invalid JSON response format that doesn't meet MLflow AI Gateway requirements for embeddings endpoint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"predictions\": [\n    {\n      \"embedding\": [0.0, 0.1]\n    },\n    {\n      \"embedding\": [1.0, 0.0]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Model Server\nDESCRIPTION: Command to start the MLflow model server for MPT-7B with specific timeout and worker configurations for large model serving.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_10\n\nLANGUAGE: commandline\nCODE:\n```\nmlflow models serve -m file:///Users/me/demos/mlruns/0/92d017e23ca04ffa919a935ed54e9334/artifacts/mpt-7b-instruct -h 127.0.0.1 -p 9030 -t 1200 -w 1 --no-conda\n```\n\n----------------------------------------\n\nTITLE: Renaming Registered MLflow Model in R\nDESCRIPTION: This function renames a model in the MLflow Model Registry. It requires the current name and the new name for the model, and optionally accepts an MLflow client object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_37\n\nLANGUAGE: r\nCODE:\n```\nmlflow_rename_registered_model(name, new_name, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow UI and JavaScript Dev Server\nDESCRIPTION: Commands to start the MLflow UI and JavaScript development server for frontend development.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# In one shell:\nmlflow ui\n\n# In another shell:\ncd mlflow/server/js\nyarn start\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI for Experiment Tracking\nDESCRIPTION: This command starts the MLflow user interface, allowing users to track experiments, compare runs, and visualize results from the LightGBM training sessions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/lightgbm/lightgbm_native/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow LangChain Autologging\nDESCRIPTION: Basic example of enabling MLflow autologging for LangChain models at the beginning of a script or notebook.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.langchain.autolog()\n\n# Enable other optional logging\n# mlflow.langchain.autolog(log_models=True, log_input_examples=True)\n\n# Your LangChain model code here\n...\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for MLflow Development\nDESCRIPTION: This snippet enumerates the Python packages required for MLflow development scripts. It includes packages for command-line interfaces, YAML processing, HTTP requests, package management, data validation, and YAML parsing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\n# Dev script dependencies\nclick\nruamel.yaml.clib!=0.2.7\nruamel.yaml\nrequests\npackaging\npydantic\npyyaml\n```\n\n----------------------------------------\n\nTITLE: MLflow Run View Type Filtering in Python\nDESCRIPTION: Example of filtering MLflow runs by view type and ordering results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.entities import ViewType\n\nactive_runs = mlflow.search_runs(\n    experiment_names=[\"search-run-guide\"],\n    run_view_type=ViewType.ACTIVE_ONLY,\n    order_by=[\"metrics.accuracy DESC\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Callable with Type Validation\nDESCRIPTION: Example of using the @pyfunc decorator to enable type hint-based validation for callable models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.pyfunc.utils import pyfunc\n\n\n@pyfunc\ndef predict(model_input: list[Message]) -> list[str]:\n    return [msg.content for msg in model_input]\n\n\n# The input_example can be a list of Message objects as defined in the type hint\ninput_example = [\n    Message(role=\"system\", content=\"Hello\"),\n    Message(role=\"user\", content=\"Hi\"),\n]\nprint(predict(input_example))  # Output: ['Hello', 'Hi']\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Environment Variables\nDESCRIPTION: Optional commands to set environment variables for specifying Python and MLflow binary locations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_PYTHON_BIN=/path/to/bin/python\nexport MLFLOW_BIN=/path/to/bin/mlflow\n```\n\n----------------------------------------\n\nTITLE: Implementing Tokenization\nDESCRIPTION: Defines and applies tokenization functions to prepare the text data for model training, including padding and truncation operations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load the tokenizer for \"distilbert-base-uncased\" model.\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n\ndef tokenize_function(examples):\n    # Pad/truncate each text to 512 tokens. Enforcing the same shape\n    # could make the training faster.\n    return tokenizer(\n        examples[\"sms\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\n\n\nseed = 22\n\n# Tokenize the train and test datasets\ntrain_tokenized = train_dataset.map(tokenize_function)\ntrain_tokenized = train_tokenized.remove_columns([\"sms\"]).shuffle(seed=seed)\n\ntest_tokenized = test_dataset.map(tokenize_function)\ntest_tokenized = test_tokenized.remove_columns([\"sms\"]).shuffle(seed=seed)\n```\n\n----------------------------------------\n\nTITLE: Serving MLflow Models with R\nDESCRIPTION: Example showing how to save and serve a simple model that makes constant predictions using MLflow's R API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_41\n\nLANGUAGE: R\nCODE:\n```\nlibrary(mlflow)\n\n# save simple model with constant prediction\nmlflow_save_model(function(df) 1, \"mlflow_constant\")\n\n# serve an existing model over a web interface\nmlflow_rfunc_serve(\"mlflow_constant\")\n\n# request prediction from server\nhttr::POST(\"http://127.0.0.1:8090/predict/\")\n```\n\n----------------------------------------\n\nTITLE: Storage-Efficient Model Logging in MLflow Transformers\nDESCRIPTION: Shows how to log a model by saving only a reference to the HuggingFace Hub repository instead of the full model weights. This method maintains high memory usage but minimizes disk space requirements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/large-models/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\n\npipeline = transformers.pipeline(\n    task=\"text-generation\",\n    model=\"meta-llama/Meta-Llama-3.1-70B\",\n    torch_dtype=\"torch.float16\",\n)\n\nwith mlflow.start_run():\n    mlflow.transformers.log_model(\n        transformers_model=pipeline,\n        artifact_path=\"model\",\n        # Set save_pretrained to False to save storage space\n        save_pretrained=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Up MLflow Tracking and Experiment in Python\nDESCRIPTION: This code sets up MLflow tracking, sets the active experiment, defines a run name, and specifies an artifact path for model saving. It prepares the environment for model training and logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n\nmlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\napple_experiment = mlflow.set_experiment(\"Apple_Models\")\n\nrun_name = \"apples_rf_test\"\n\nartifact_path = \"rf_apples\"\n```\n\n----------------------------------------\n\nTITLE: MLflow Run Example Without Parameters\nDESCRIPTION: Example tuple format for adding a test case without parameters to the pytest.mark.parametrize decorator.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/examples/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(\"new_example_dir\", []),\n```\n\n----------------------------------------\n\nTITLE: Exploring MLflow Directory Structure\nDESCRIPTION: Shows the directory structure created by MLflow for storing experiment runs, metrics, artifacts, and registered models. This structure includes experiment and run IDs, metrics, artifacts, tags, parameters, and model registry information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/step2-explore-registered-model/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlruns/\n├── 0/                                    # Experiment ID\n│   ├── bc6dc2a4f38d47b4b0c99d154bbc77ad/ # Run ID\n│   │   ├── metrics/\n│   │   │   └── mse                       # Example metric file for mean squared error\n│   │   ├── artifacts/                    # Artifacts associated with our run\n│   │   │   └── sklearn-model/\n│   │   │       ├── python_env.yaml\n│   │   │       ├── requirements.txt      # Python package requirements\n│   │   │       ├── MLmodel               # MLflow model file with model metadata\n│   │   │       ├── model.pkl             # Serialized model file\n│   │   │       ├── input_example.json\n│   │   │       └── conda.yaml\n│   │   ├── tags/\n│   │   │   ├── mlflow.user\n│   │   │   ├── mlflow.source.git.commit\n│   │   │   ├── mlflow.runName\n│   │   │   ├── mlflow.source.name\n│   │   │   ├── mlflow.log-model.history\n│   │   │   └── mlflow.source.type\n│   │   ├── params/\n│   │   │   ├── max_depth\n│   │   │   └── random_state\n│   │   └── meta.yaml\n│   └── meta.yaml\n├── models/                               # Model Registry Directory\n    ├── sk-learn-random-forest-reg-model/ # Registered model name\n    │   ├── version-1/                    # Model version directory\n    │   │   └── meta.yaml\n    │   └── meta.yaml\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Tracing Unit Tests with Pytest\nDESCRIPTION: Command to verify that the development environment is ready for tracing development by running the unit tests for the tracing module.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/contribute.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/tracing\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Pydantic Version\nDESCRIPTION: Installing a specific version of pydantic to resolve compatibility issues.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/transformers/MLFlow_X_HuggingFace_Finetune_a_text_classification_model.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%pip install pydantic==1.10.12\n```\n\n----------------------------------------\n\nTITLE: Reranking Retrieved Results using RankGPT in Python\nDESCRIPTION: This snippet shows the implementation of reranking retrieved results using the RankGPT reranker. It filters and selects the most relevant results from multiple retrievers.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    reranker = RankGPTRerank(llm=self.llm, top_n=5)\n    reranked_nodes = reranker.postprocess_nodes(ev.nodes, query_str=query)\n    reranked_context = \"\\n\".join(node.text for node in reranked_nodes)\n```\n\n----------------------------------------\n\nTITLE: Search Registered Models Response Structure\nDESCRIPTION: Defines the response format for searching registered models, including the array of matching models and pagination token.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_25\n\nLANGUAGE: rest\nCODE:\n```\n+-------------------+------------------------------------------+------------------------------------------------------+\n|    Field Name     |                   Type                   |                     Description                      |\n+===================+==========================================+======================================================+\n| registered_models | An array of :ref:`mlflowregisteredmodel` | Registered Models that match the search criteria.    |\n+-------------------+------------------------------------------+------------------------------------------------------+\n| next_page_token   | ``STRING``                               | Pagination token to request the next page of models. |\n+-------------------+------------------------------------------+------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Saving Function Definitions\nDESCRIPTION: Function signatures for saving different types of models in MLflow format, including generic models, H2O models, Keras models, and XGBoost models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_43\n\nLANGUAGE: R\nCODE:\n```\nlist(list(\"mlflow_save_model\"), list(\"crate\"))(model, path, model_spec = list(), ...)\nmlflow_save_model(model, path, model_spec = list(), ...)\nlist(list(\"mlflow_save_model\"), list(\"H2OModel\"))(model, path, model_spec = list(), conda_env = NULL, ...)\nlist(list(\"mlflow_save_model\"), list(\"keras.engine.training.Model\"))(model, path, model_spec = list(), conda_env = NULL, ...)\nlist(list(\"mlflow_save_model\"), list(\"xgb.Booster\"))(model, path, model_spec = list(), conda_env = NULL, ...)\n```\n\n----------------------------------------\n\nTITLE: Cloning MLflow Recipes Regression Template (Shell)\nDESCRIPTION: Shell command to clone the MLflow Recipes regression template repository locally for starting a new regression model project.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/mlflow/recipes-regression-template\n```\n\n----------------------------------------\n\nTITLE: Transitioning a Model Version Stage in MLflow\nDESCRIPTION: Definition of the mlflow_transition_model_version_stage function used to transition a model version to a different stage in the MLflow Model Registry.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_56\n\nLANGUAGE: r\nCODE:\n```\nmlflow_transition_model_version_stage(\n  name,\n  version,\n  stage,\n  archive_existing_versions = FALSE,\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI\nDESCRIPTION: Configures the MLflow tracking URI to point to a local MLflow server running on the default port.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# start the mlflow server with `mlflow server` first, then set the tracking uri\nmlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python packages for machine learning, including PyTorch, TorchMetrics, scikit-learn, and MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/logging/pytorch_log_model.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchmetrics\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport mlflow\nimport mlflow.pytorch\n```\n\n----------------------------------------\n\nTITLE: Importing MLflow Dependencies in Python\nDESCRIPTION: Imports required modules including MlflowClient for MLflow API access, pprint for formatted output, and RandomForestRegressor from scikit-learn.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step2-mlflow-client/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\nfrom pprint import pprint\nfrom sklearn.ensemble import RandomForestRegressor\n```\n\n----------------------------------------\n\nTITLE: Dataset Structure Definition in MLflow REST API\nDESCRIPTION: Defines the Dataset structure used to represent data references in MLflow. Contains fields for name, digest, source information, schema, and profile data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_34\n\nLANGUAGE: rest\nCODE:\n```\n+-------------+------------+----------------------------------------------------------------------------------------------+\n| Field Name  |    Type    |                                         Description                                          |\n+=============+============+==============================================================================================+\n| name        | ``STRING`` | The name of the dataset. E.g. ?my.uc.table@2? ?nyc-taxi-dataset?, ?fantastic-elk-3?          |\n|             |            | This field is required.                                                                      |\n|             |            |                                                                                              |\n+-------------+------------+----------------------------------------------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Updating Git Submodules\nDESCRIPTION: Command to update Git submodules if the repository was cloned without the --recurse-submodules flag.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Spark UDF Example with Struct and Array Inputs in Python\nDESCRIPTION: This command illustrates how to execute the structs_and_arrays.py example, which showcases the use of Spark UDF with input data of struct and array types.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/spark_udf/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython structs_and_arrays.py\n```\n\n----------------------------------------\n\nTITLE: Initializing MLflow Experiment\nDESCRIPTION: Creates a new MLflow experiment for tracking the RAG system development.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_experiment(\"LlamaIndex Workflow RAG\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authentication for HDFS in MLflow\nDESCRIPTION: Environment variables required for Kerberos authentication when using HDFS as an artifact store in MLflow. These variables specify the ticket cache location and the user name to use for authentication.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/artifacts-stores/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_KERBEROS_TICKET_CACHE=/tmp/krb5cc_22222222\nexport MLFLOW_KERBEROS_USER=user_name_to_use\n```\n\n----------------------------------------\n\nTITLE: Using Different Module Names for MLflow PyFunc Models\nDESCRIPTION: Shows how to avoid module caching issues by using different module names for different model implementations when logging models with MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmlflow.pyfunc.log_model(\n    artifact_path=\"model1\",\n    python_model=model1,\n    code_paths=[\"my_model1.py\"],\n)\n\nmlflow.pyfunc.log_model(\n    artifact_path=\"model\",\n    python_model=model2,\n    code_paths=[\"my_model2.py\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Logging a Model in MLflow using R\nDESCRIPTION: This function logs a model for the current run in MLflow. It stores the model as an artifact within the active run, similar to mlflow_save_model() but specifically for the current run context.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_30\n\nLANGUAGE: r\nCODE:\n```\nmlflow_log_model(model, artifact_path, ...)\n```\n\n----------------------------------------\n\nTITLE: Writing Python File in Jupyter Using Magic Command\nDESCRIPTION: Example showing how to use the %%writefile magic command in Jupyter notebooks to write cell contents to a Python file, which is needed for Models from Code functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/models-from-code/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%writefile \"./hello.py\"\n\nprint(\"hello!\")\n```\n\n----------------------------------------\n\nTITLE: Creating Run within Experiment via HTTP POST Request\nDESCRIPTION: HTTP POST endpoint for creating a new run within an experiment. A run represents a single execution of a machine learning or data ETL pipeline, used to track parameters, metrics, and tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_9\n\nLANGUAGE: http\nCODE:\n```\n2.0/mlflow/runs/create\n```\n\n----------------------------------------\n\nTITLE: Testing the LangGraph Model\nDESCRIPTION: Invokes the compiled LangGraph model with a sample user message to test its functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfinal_state = app.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What is MLflow? Keep the response concise and reply in Chinese. Try using as many tools as possible\",\n            }\n        ]\n    },\n)\nresponse = final_state[\"messages\"][-1].content\n```\n\n----------------------------------------\n\nTITLE: Initializing Hybrid RAG Workflow Class in Python\nDESCRIPTION: This code defines the HybridRAGWorkflow class, which initializes various retrieval methods based on the provided configuration. It sets up vector search, BM25, and web search retrievers as specified.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass HybridRAGWorkflow(Workflow):\n\n    VALID_RETRIEVERS = {\"vector_search\", \"bm25\", \"web_search\"}\n\n    def __init__(self, retrievers=None, **kwargs):\n        super().__init__(**kwargs)\n        self.llm = Settings.llm\n        self.retrievers = retrievers or []\n\n        if invalid_retrievers := set(self.retrievers) - self.VALID_RETRIEVERS:\n            raise ValueError(f\"Invalid retrievers specified: {invalid_retrievers}\")\n\n        self._use_vs_retriever = \"vector_search\" in self.retrievers\n        self._use_bm25_retriever = \"bm25\" in self.retrievers\n        self._use_web_search = \"web_search\" in self.retrievers\n\n        if self._use_vs_retriever:\n            qd_client = qdrant_client.QdrantClient(host=_QDRANT_HOST, port=_QDRANT_PORT)\n            vector_store = QdrantVectorStore(client=qd_client, collection_name=_QDRANT_COLLECTION_NAME)\n            index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n            self.vs_retriever = index.as_retriever()\n\n        if self._use_bm25_retriever:\n            self.bm25_retriever = BM25Retriever.from_persist_dir(_BM25_PERSIST_DIR)\n\n        if self._use_web_search:\n            self.tavily_tool = TavilyToolSpec(api_key=os.environ.get(\"TAVILY_AI_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Creating Card-Based Navigation in JSX for LLM Tutorial\nDESCRIPTION: Implements a CardGroup with a PageCard component to create a navigation card linking to a tutorial about serving LLMs with custom PyFunc in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <PageCard\n    headerText=\"Serving LLMs with MLflow: Leveraging Custom PyFunc\"\n    link=\"/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm\"\n    text=\"Learn how to use the MLflow Custom Pyfunc Model to serve Large Language Models (LLMs) in a RESTful environment.\"\n  />\n\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI - Bash\nDESCRIPTION: Command to set the MLflow tracking URI environment variable to a local SQLite database.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/ray_serve/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Python Package\nDESCRIPTION: Commands to install the MLflow Python package, which is a prerequisite for using the MLflow R API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Loading DialoGPT Model and Initial Interaction\nDESCRIPTION: Loads the previously logged DialoGPT model using MLflow's Python function flavor and tests it with an initial question to verify its conversational capabilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/conversational-model.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Load the model as a generic python function in order to leverage the integrated Conversational Context\n# Note that loading a conversational model with the native flavor (i.e., `mlflow.transformers.load_model()`) will not include anything apart from the\n# pipeline itself; if choosing to load in this way, you will need to manage your own Conversational Context instance to maintain state on the\n# conversation history.\nchatbot = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n\n# Validate that the model is capable of responding to a question\nfirst = chatbot.predict(\"What is the best way to get to Antarctica?\")\n```\n\n----------------------------------------\n\nTITLE: Logging Directory Artifacts in Python\nDESCRIPTION: MLflow 1.3 allows logging directories as artifacts using the Python fluent API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.log_artifact(\"path/to/directory\")\n```\n\n----------------------------------------\n\nTITLE: Loading MLflow Model Using Basic Model URI\nDESCRIPTION: Demonstrates the simplest way to load a model from the MLflow Model Registry using a specific model URI and perform inference with sklearn. Uses make_regression to generate sample data for prediction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/step3-load-model/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow.sklearn\nfrom sklearn.datasets import make_regression\n\nmodel_name = \"sk-learn-random-forest-reg-model\"\nmodel_version = \"latest\"\n\n# Load the model from the Model Registry\nmodel_uri = f\"models:/{model_name}/{model_version}\"\nmodel = mlflow.sklearn.load_model(model_uri)\n\n# Generate a new dataset for prediction and predict\nX_new, _ = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\ny_pred_new = model.predict(X_new)\n\nprint(y_pred_new)\n```\n\n----------------------------------------\n\nTITLE: Search Model Versions API Endpoint\nDESCRIPTION: REST endpoint for searching model versions in the MLflow registry.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_21\n\nLANGUAGE: REST\nCODE:\n```\nGET 2.0/mlflow/model-versions/search\n```\n\n----------------------------------------\n\nTITLE: Updating Accelerate Package\nDESCRIPTION: Updating the accelerate package to ensure compatibility with training requirements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/transformers/MLFlow_X_HuggingFace_Finetune_a_text_classification_model.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%pip install -U -q accelerate\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Setting up environment variable for OpenAI authentication\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/notebooks/chat-model-tool-calling.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Installing Custom MLflow Plugin Provider Package\nDESCRIPTION: This command installs the custom 'my_llm' plugin provider package in editable mode from the local './my-llm' directory.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/plugin/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -e ./my-llm\n```\n\n----------------------------------------\n\nTITLE: Using Model Version Stage Transition API Example\nDESCRIPTION: Example showing usage of MlflowClient.transition_model_version_stage with archive_existing_versions parameter to archive existing staging/production models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\nMlflowClient.transition_model_version_stage(archive_existing_versions=True)\n```\n\n----------------------------------------\n\nTITLE: Loading Evaluation Dataset\nDESCRIPTION: Loading the Alpaca dataset from Hugging Face Hub for model evaluation\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset = load_dataset(\"tatsu-lab/alpaca\")\neval_df = pd.DataFrame(dataset[\"train\"])\neval_df.head(10)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Example with Default Parameters\nDESCRIPTION: Command to run the Titanic Captum interpretation example using MLflow with default parameters. This executes the Titanic_Captum_Interpret.py script with predefined parameter values.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/CaptumExample/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run .\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Backend Configuration for MLflow Projects\nDESCRIPTION: JSON configuration file example for running MLflow projects on Kubernetes, specifying the Kubernetes context, Docker repository URI, and job template path\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"kube-context\": \"docker-for-desktop\",\n  \"repository-uri\": \"username/mlflow-kubernetes-example\",\n  \"kube-job-template-path\": \"/Users/username/path/to/kubernetes_job_template.yaml\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building MLflow Database Test Services with Docker Compose\nDESCRIPTION: This snippet demonstrates how to build Docker services for MLflow database testing. It shows commands for building a single service and all services, including the use of dependency requirements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/db/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Build a service\nservice=mlflow-sqlite\n./tests/db/compose.sh build --build-arg DEPENDENCIES=\"$(cat requirements/skinny-requirements.txt requirements/core-requirements.txt | grep -Ev '^(#|$)'\" $service\n\n# Build all services\n./tests/db/compose.sh build --build-arg DEPENDENCIES=\"$(cat requirements/skinny-requirements.txt requirements/core-requirements.txt | grep -Ev '^(#|$)')\"\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset Source in MLflow\nDESCRIPTION: This snippet shows how to load a dataset's source using mlflow.data.get_source() and access the local file where the data has been downloaded.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/dataset/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Loading the dataset's source\ndataset_source = mlflow.data.get_source(logged_dataset)\n\nlocal_dataset = dataset_source.load()\n\nprint(f\"The local file where the data has been downloaded to: {local_dataset}\")\n\n# Load the data again\nloaded_data = pd.read_csv(local_dataset, delimiter=\";\")\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Experiment in R\nDESCRIPTION: Function to set or create an active experiment in MLflow. Takes experiment name or ID as input and optionally an artifact location. Returns the ID of the active experiment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_49\n\nLANGUAGE: r\nCODE:\n```\nmlflow_set_experiment(\n  experiment_name = NULL,\n  experiment_id = NULL,\n  artifact_location = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating MLflow Model Performance\nDESCRIPTION: Performs metric-based evaluation of the model using MLflow's evaluate API with pre-defined metrics for text summarization\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/prompt-engineering/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport pandas as pd\n\nlogged_model = \"runs:/840a5c43f3fb46f2a2059b761557c1d0/model\"\n\narticle_text = \"\"\"\nAn MLflow Project is a format for packaging data science code in a reusable and reproducible way.\nThe MLflow Projects component includes an API and command-line tools for running projects, which\nalso integrate with the Tracking component to automatically record the parameters and git commit\nof your source code for reproducibility.\n\nThis article describes the format of an MLflow Project and how to run an MLflow project remotely\nusing the MLflow CLI, which makes it easy to vertically scale your data science code.\n\"\"\"\nquestion = \"What is an MLflow project?\"\n\ndata = pd.DataFrame(\n    {\n        \"article\": [article_text],\n        \"question\": [question],\n        \"ground_truth\": [\n            article_text\n        ],  # used for certain evaluation metrics, such as ROUGE score\n    }\n)\n\nwith mlflow.start_run():\n    results = mlflow.evaluate(\n        model=logged_model,\n        data=data,\n        targets=\"ground_truth\",\n        model_type=\"text-summarization\",\n    )\n\neval_table = results.tables[\"eval_results_table\"]\nprint(f\"See evaluation table below: \\n{eval_table}\")\n```\n\n----------------------------------------\n\nTITLE: Running Tests for the MLflow R Package\nDESCRIPTION: Commands to run the test suite for the MLflow R package, including package checks and unit tests.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nR CMD check --no-build-vignettes --no-manual --no-tests mlflow*tar.gz\ncd tests\nNOT_CRAN=true LINTR_COMMENT_BOT=false Rscript ../.run-tests.R\ncd -\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Questions Dataset\nDESCRIPTION: Shows the first three rows of the generated questions dataset to inspect its structure and contents.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngenerated_df.head(3)\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Evaluation Dataset\nDESCRIPTION: Creates a sample pandas DataFrame with questions, retrieved contexts, and ground truth contexts for evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = pd.DataFrame(\n    {\n        \"questions\": [\n            \"What is MLflow?\",\n            \"What is Databricks?\",\n            \"How to serve a model on Databricks?\",\n            \"How to enable MLflow Autologging for my workspace by default?\",\n        ],\n        \"retrieved_context\": [\n            [\n                \"mlflow/index.html\",\n                \"mlflow/quick-start.html\",\n            ],\n            [\n                \"introduction/index.html\",\n                \"getting-started/overview.html\",\n            ],\n            [\n                \"machine-learning/model-serving/index.html\",\n                \"machine-learning/model-serving/model-serving-intro.html\",\n            ],\n            [],\n        ],\n        \"ground_truth_context\": [\n            [\"mlflow/index.html\"],\n            [\"introduction/index.html\"],\n            [\n                \"machine-learning/model-serving/index.html\",\n                \"machine-learning/model-serving/llm-optimized-model-serving.html\",\n            ],\n            [\"mlflow/databricks-autologging.html\"],\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for MLflow Mistral Module\nDESCRIPTION: ReStructuredText directive for configuring auto-documentation of the mlflow.mistral module. Includes settings for documenting all members, undocumented members and showing inheritance details.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.mistral.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.mistral\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Artifact URI for Model Loading in Python\nDESCRIPTION: Example of using the new URI format for referring to artifacts in the context of a run. This change affects how models are loaded in various MLflow APIs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_62\n\nLANGUAGE: Python\nCODE:\n```\n<model-type>.load_model(\"runs:/<run_id>/relative/path/to/artifact\")\n```\n\n----------------------------------------\n\nTITLE: Setting up RAG System Components\nDESCRIPTION: Creating a RAG system by loading MLflow documentation, splitting text, generating embeddings, and setting up a retrieval QA chain using OpenAI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/rag-evaluation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nloader = WebBaseLoader(\"https://mlflow.org/docs/latest/index.html\")\n\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)\n\nqa = RetrievalQA.from_chain_type(\n    llm=OpenAI(temperature=0),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Iris Dataset in Python for RAPIDS MLflow Example\nDESCRIPTION: This Python code snippet creates an Iris dataset CSV file using scikit-learn. It loads the Iris dataset as a pandas DataFrame and saves it to 'iris.csv' without an index.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_iris; d = load_iris(as_frame=True); d.frame.to_csv('iris.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI and MLflow Dependencies\nDESCRIPTION: Imports required libraries and validates the presence of OpenAI API key in environment variables. Sets up essential dependencies for embedding analysis.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport numpy as np\nimport openai\nimport requests\nfrom bs4 import BeautifulSoup\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import ColSpec, ParamSchema, ParamSpec, Schema, TensorSpec\n\nassert \"OPENAI_API_KEY\" in os.environ, \" OPENAI_API_KEY environment variable must be set\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of PySpark ML Examples\nDESCRIPTION: A markdown table listing example Python files and their descriptions for PySpark ML autologging demonstrations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pyspark_ml_autologging/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| File                     | Description                        |\n| :----------------------- | :--------------------------------- |\n| `logistic_regression.py` | Train a `LogisticRegression` model |\n| `one_vs_rest.py`         | Train a `OneVsRest` model          |\n```\n\n----------------------------------------\n\nTITLE: Making Python Changes Available in R During Development\nDESCRIPTION: Command to make Python changes available in the R package during development by installing the local Python package in the R environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nRscript -e 'reticulate::conda_install(\"r-mlflow\", \"../../../.\", pip = TRUE)'\n```\n\n----------------------------------------\n\nTITLE: Example of conda.yaml with 'defaults' Channel\nDESCRIPTION: This YAML snippet shows an example of a conda.yaml file that includes the 'defaults' channel, which may need to be updated due to Anaconda's license changes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nname: mlflow-env\nchannels:\n  - defaults\ndependencies:\n  - python=3.8.8\n  - pip\n  - pip:\n      - mlflow==2.3\n      - scikit-learn==0.23.2\n      - cloudpickle==1.6.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for MLflow LightGBM Module\nDESCRIPTION: This RST code configures Sphinx to automatically generate documentation for the mlflow.lightgbm module. It includes all members, undocumented members, and shows inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.lightgbm.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.lightgbm\n===============\n\n.. automodule:: mlflow.lightgbm\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Model Training Output Display - Python\nDESCRIPTION: Example output from the model training script showing Mean Squared Error (MSE) and target class names for Iris classification.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/ray_serve/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nMSE: 1.04\nTarget names:  ['setosa' 'versicolor' 'virginica']\n```\n\n----------------------------------------\n\nTITLE: MLflow Silent Autologging Configuration\nDESCRIPTION: Example of configuring MLflow autologging with silent option to suppress warnings and logging statements\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.autolog(silent=False)\n```\n\n----------------------------------------\n\nTITLE: Serving Sktime Model with MLflow CLI\nDESCRIPTION: This bash command demonstrates how to serve a trained Sktime model using MLflow CLI, creating a local REST API endpoint for predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_71\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow AI Gateway\nDESCRIPTION: Command to start the MLflow AI Gateway with a specified configuration file and port.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/prompt-engineering/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow gateway start --config-path config.yaml --port 7000\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Tracking URI in Python\nDESCRIPTION: Python code to connect a notebook to an MLflow tracking server by setting the tracking URI. This enables logging of runs, experiments, traces, and artifacts to the specified tracking server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/running-notebooks/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for MLflow Model using CLI\nDESCRIPTION: CLI command to build a Docker image containing an MLflow model and MLServer inference server. The command requires a run ID and image name, with an optional flag to enable MLServer instead of Flask.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-to-kubernetes/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models build-docker -m runs:/<run_id>/model -n <image_name> --enable-mlserver\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenAI Tracing with Autologging in Python\nDESCRIPTION: Demonstrates how to enable automatic tracing of OpenAI API usage in MLflow. This feature logs metadata such as token usage and interaction history for OpenAI-powered applications.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmlflow.openai.autolog()\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Authentication Dependencies\nDESCRIPTION: Command to install all dependencies required for the MLflow basic authentication functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow[auth]\n```\n\n----------------------------------------\n\nTITLE: Setting Run Tags in MLflow R\nDESCRIPTION: Function to set tags on an MLflow run. Tags are run metadata that can be updated during or after a run completes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_51\n\nLANGUAGE: r\nCODE:\n```\nmlflow_set_tag(key, value, run_id = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Deploying John Snow Labs Model as Container\nDESCRIPTION: Instructions for deploying a John Snow Labs model as a Docker container and querying the deployed model endpoint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 5001:8080 -e JOHNSNOWLABS_LICENSE_JSON=your_json_string \"mlflow-pyfunc\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d '{\n  \"dataframe_split\": {\n      \"columns\": [\"text\"],\n      \"data\": [[\"I hate covid\"], [\"I love covid\"]]\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Running Typos Check with Pre-commit\nDESCRIPTION: Command to run the typos check on all files using pre-commit hook. This is used to find typos in the project.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/typos.md#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npre-commit run --all-files typos\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation Results\nDESCRIPTION: Accessing and displaying the evaluation results table from the MLflow evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/rag-evaluation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresults.tables[\"eval_results_table\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing Sentence Transformer Model in Python\nDESCRIPTION: Loads the all-MiniLM-L6-v2 Sentence Transformer model, which is optimized for semantic search tasks, providing a good balance between performance and speed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load a pre-trained sentence transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow TensorFlow Module with Sphinx in RST\nDESCRIPTION: Sphinx documentation directives for auto-generating API documentation from the MLflow TensorFlow module. It includes the main module documentation and the MlflowCallback class documentation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.tensorflow.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.tensorflow\n==================\n\n.. automodule:: mlflow.tensorflow\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. autoclass:: mlflow.tensorflow.MlflowCallback\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Defining MLflow Python Package Dependencies\nDESCRIPTION: Specifies required Python packages for MLflow project including MLflow itself and scikit-learn version 1.0.2.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/virtualenv/project/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlflow\nscikit-learn==1.0.2\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow 1.27.0 with SQLAlchemy Dependency\nDESCRIPTION: MLflow 1.27.0 now requires SQLAlchemy version 1.4.0 or higher for SQL-based tracking backends. This snippet shows the installation requirement.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_22\n\nLANGUAGE: Text\nCODE:\n```\nsqlalchemy>=1.4.0\n```\n\n----------------------------------------\n\nTITLE: Querying Deployed RAPIDS MLflow Model with Test Data\nDESCRIPTION: This shell command executes a script to query the deployed RAPIDS model with test data. The script 'sample_server_query.sh' is used to send requests to the local REST API endpoint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/README.md#2025-04-07_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbash src/sample_server_query.sh\n```\n\n----------------------------------------\n\nTITLE: MLflow OpenAI No Variables Message Format\nDESCRIPTION: Demonstrates logging an OpenAI model with fixed messages without variables. Prediction inputs are appended to the logged messages as user messages.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/openai/messages.rst#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        artifact_path=\"model\",\n        model=\"gpt-4o-mini\",\n        task=openai.chat.completions,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You're a frontend engineer.\",\n            }\n        ],\n    )\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\nprint(model.predict([\"Tell me a funny joke.\"]))\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow txtai Extension\nDESCRIPTION: Command to install the MLflow txtai extension package using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/txtai.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow-txtai\n```\n\n----------------------------------------\n\nTITLE: Querying Unique Metric Tuples in MLflow Database\nDESCRIPTION: SQL query to count unique combinations of run_id and metric_key. This helps evaluate query join latency during migration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/store/db_migrations/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT count(*) FROM (\n   SELECT metrics.key, run_uuid FROM metrics GROUP BY run_uuid, metrics.key\n) unique_metrics;\n```\n\n----------------------------------------\n\nTITLE: Listing Artifacts in R\nDESCRIPTION: Fixed a bug in MLflow 1.3 where listing artifacts for an empty run in R no longer throws an error.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_53\n\nLANGUAGE: R\nCODE:\n```\nmlflow_list_artifacts(run_id)\n```\n\n----------------------------------------\n\nTITLE: Specifying Test Dependencies for MLflow in Python\nDESCRIPTION: This code snippet defines the required Python packages and their versions for running tests in the MLflow project. It includes dependencies for parsing pip requirements, testing frameworks, cloud storage, machine learning libraries, and evaluation tools.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/requirements/test-requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n## Dependencies required to run tests\n# Required for testing utilities for parsing pip requirements\npip>=20.1\n## Test-only dependencies\npytest==8.1.1\npytest-asyncio\npytest-repeat\npytest-cov\npytest-timeout\npytest-localserver==0.5.0\nmoto>=4.2.0,<5,!=4.2.5\nazure-storage-blob>=12.0.0\nazure-storage-file-datalake>=12.9.1\nazure-identity>=1.6.1\npillow\nplotly\nkaleido\n# Required by tuning tests\nhyperopt\n# Required by recipes tests\nipython\n# Required by automl tests\nflaml[automl]\n# Required by evaluator tests\nshap\n# Required to evaluate language models in `mlflow.evaluate`\nevaluate\nnltk\nrouge_score\ntextstat\ntiktoken\n# Required by progress bar tests\nipywidgets\ntqdm\n# Required for LLM eval in `mlflow.evaluate`\nopenai\n# Required for showing pytest stats\npsutil\n# SQLAlchemy == 2.0.25 requires typing_extensions >= 4.6.0\ntyping_extensions>=4.6.0\n# Required for importing boto3 ClientError directly for testing\nbotocore>=1.34\npyspark\n# Required for testing the opentelemetry exporter of tracing\nopentelemetry-exporter-otlp-proto-grpc\nopentelemetry-exporter-otlp-proto-http\n# TODO: remove this once XGBoost releases >2.1.3\nscikit-learn<1.6\n# Required for testing mlflow.server.auth\nFlask-WTF<2\n```\n\n----------------------------------------\n\nTITLE: Building the MLflow R Package\nDESCRIPTION: Command to build the R client package for MLflow, creating a tarball of the package.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nR CMD build .\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Dataframe for Model Testing in Python\nDESCRIPTION: Prepares evaluation data by creating a copy of test data and adding target values for model evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# creating the evaluation dataframe\neval_data = X_test.copy()\neval_data[\"target\"] = y_test\n```\n\n----------------------------------------\n\nTITLE: Importing React Tab Components\nDESCRIPTION: Import statements for React tab components used to structure the documentation interface.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\nimport { CardGroup, LogoCard } from \"@site/src/components/Card\";\n```\n\n----------------------------------------\n\nTITLE: Searching for Traces Associated with an MLflow Run\nDESCRIPTION: Shows how to programmatically retrieve traces associated with a particular run using the MlflowClient.search_traces method.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/how-to.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\n\n# Retrieve traces associated with a specific Run\ntraces = client.search_traces(run_id=run.info.run_id)\n\nprint(traces)\n```\n\n----------------------------------------\n\nTITLE: Linting JavaScript Code for MLflow UI\nDESCRIPTION: Yarn command to lint and auto-fix JavaScript code in the MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# Note this command only fixes auto-fixable issues (e.g. remove trailing whitespace)\nyarn lint:fix\n```\n\n----------------------------------------\n\nTITLE: DataFrame Output Management\nDESCRIPTION: Function to handle the persistence of generated questions and answers, including deduplication and sorting of results.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef add_to_output_df(result_df=pd.DataFrame({})):\n    \"\"\"\n    This function adds the records in result_df to the existing records saved at OUTPUT_DF_PATH,\n    remove the duplicate rows and save the new collection of records back to OUTPUT_DF_PATH.\n    \"\"\"\n    if os.path.exists(OUTPUT_DF_PATH):\n        all_result_df = pd.read_csv(OUTPUT_DF_PATH)\n    else:\n        all_result_df = pd.DataFrame({})\n    all_result_df = (\n        pd.concat([all_result_df, result_df], ignore_index=True)\n        .drop_duplicates()\n        .sort_values(by=[\"source\", \"chunk_id\"])\n        .reset_index(drop=True)\n    )\n    all_result_df.to_csv(OUTPUT_DF_PATH, index=False)\n    return all_result_df\n```\n\n----------------------------------------\n\nTITLE: Model Registry Search API Example\nDESCRIPTION: Example showing the paginated search API for querying registered models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\nMlflowClient.search_registered_models()\n```\n\n----------------------------------------\n\nTITLE: Training and Logging LightGBM Model with MLflow\nDESCRIPTION: Loads the Iris dataset and prepares it for training a LightGBM classifier. The code snippet ends before the actual model training and logging steps.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom lightgbm import LGBMClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport mlflow\nfrom mlflow.models import infer_signature\n\ndata = load_iris()\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Gateway Server with Custom Provider\nDESCRIPTION: This command starts the MLflow gateway server using the specified configuration file and port. It also sets an environment variable for the API key.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/plugin/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nMY_LLM_API_KEY=some-api-key mlflow gateway start --config-path config.yaml --port 7000\n```\n\n----------------------------------------\n\nTITLE: Creating Python Code Execution Function\nDESCRIPTION: Defines and registers a UnityCatalog function for executing arbitrary Python code, which will be used as a tool by the agent.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef execute_python_code(code: str) -> str:\n    \"\"\"\n    Executes the given python code and returns its stdout.\n    Remember the code should print the final result to stdout.\n\n    Args:\n      code: Python code to execute. Remember to print the final result to stdout.\n    \"\"\"\n    # clint comment is used to disable lint check, you could delete them\n    import sys  # clint: disable=lazy-builtin-import\n    from io import StringIO  # clint: disable=lazy-builtin-import\n\n    stdout = StringIO()\n    sys.stdout = stdout\n    exec(code)\n    return stdout.getvalue()\n\n\nfunction_info = client.create_python_function(\n    func=execute_python_code, catalog=CATALOG, schema=SCHEMA, replace=True\n)\npython_execution_function_name = function_info.full_name\n```\n\n----------------------------------------\n\nTITLE: Applying Prompt Template to Dataset\nDESCRIPTION: Defines and applies a prompt template to format the input data for the model training, including context, question, and expected SQL response\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nPROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\n{context}\n\n### Question:\n{question}\n\n### Response:\n{output}\"\"\"\n\n\ndef apply_prompt_template(row):\n    prompt = PROMPT_TEMPLATE.format(\n        question=row[\"question\"],\n        context=row[\"context\"],\n        output=row[\"answer\"],\n    )\n    return {\"prompt\": prompt}\n\n\ntrain_dataset = train_dataset.map(apply_prompt_template)\ndisplay_table(train_dataset.select(range(1)))\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom \"Add N\" PyFunc Model\nDESCRIPTION: Defines a custom MLflow model that adds a specified numeric value to all columns of a pandas DataFrame input. The example shows how to define, save, load, and use a custom PyFunc model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow.pyfunc\n\n\n# Define the model class\nclass AddN(mlflow.pyfunc.PythonModel):\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input, params=None):\n        return model_input.apply(lambda column: column + self.n)\n\n\n# Construct and save the model\nmodel_path = \"add_n_model\"\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n\n# Evaluate the model\nimport pandas as pd\n\nmodel_input = pd.DataFrame([range(10)])\nmodel_output = loaded_model.predict(model_input)\nassert model_output.equals(pd.DataFrame([range(5, 15)]))\n```\n\n----------------------------------------\n\nTITLE: Defining MLflow Client Module Documentation Structure\nDESCRIPTION: ReStructuredText directives for documenting the MLflow client module, using automodule to automatically generate API documentation from source code docstrings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.client.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _mlflow.tracking:\n\nmlflow.client\n===============\n\n.. automodule:: mlflow.client\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Integration Labels for MLflow\nDESCRIPTION: Labels for various cloud and platform integrations supported by MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/ISSUE_TRIAGE.rst#2025-04-07_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n- integrations/azure\n- integrations/sagemaker\n- integrations/databricks\n```\n\n----------------------------------------\n\nTITLE: One-Liner Factorial Function with Code Inspector\nDESCRIPTION: A complex one-liner implementation of factorial calculation using nested lambda functions. While demonstrating Python's capabilities, it sacrifices readability and maintainability.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@code_inspector(loaded_helper)\ndef one_liner(n):\n    return (\n        (lambda f, n: f(f, n))(lambda f, n: n * f(f, n - 1) if n > 1 else 1, n)\n        if isinstance(n, int) and n >= 0\n        else \"Invalid input\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Incorrect ChatModel Implementation Example\nDESCRIPTION: Example showing incorrect implementation of load_context in a custom ChatModel subclass where instance attributes are not properly defined.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.pyfunc import ChatModel\n\n\nclass MyModel(ChatModel):\n    def __init__(self):\n        self.state = []\n\n    def load_context(self, context):\n        # This will fail on load as the instance attribute self.my_model_config is not defined\n        self.my_model_config = context.get(\"my_model_config\")\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for MLflow Model\nDESCRIPTION: Command to build a Docker image for the MLflow model using the models URI format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models build-docker --model-uri \"models:/wine-quality/1\" --name \"qs_mlops\"\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Auto Logging\nDESCRIPTION: Sets up MLflow experiment and enables auto logging for TensorFlow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/quickstart/quickstart_tensorflow.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"/Users/<your email>/mlflow-tf-keras-mnist\")\n\nmlflow.tensorflow.autolog()\n\nmodel.fit(x=train_ds, epochs=3)\n```\n\n----------------------------------------\n\nTITLE: Developing and Testing MLflow in Python\nDESCRIPTION: Guidelines for writing and running Python tests, working with the Python client and server, and handling database schema changes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n### Python\n\n#### Writing Python Tests\n\n#### Running Python Tests\n\n#### Python Client\n\n##### Python Model Flavors\n\n#### Python Server\n\n##### Building Protobuf Files\n\n##### Database Schema Changes\n```\n\n----------------------------------------\n\nTITLE: Jupyter Notebook Trace Rendering\nDESCRIPTION: Demonstrates the new capability to render trace UI directly within Jupyter notebooks for improved model debugging experience.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Configuring Warning Filters in Python\nDESCRIPTION: Sets up warning filters to suppress less relevant UserWarnings from setuptools and pydantic libraries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-embeddings-generation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Ending MLflow Trace\nDESCRIPTION: Shows how to end the root span and complete the trace using the end_trace API. Includes setting final outputs and attributes for the trace.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/client.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# End the root span (trace)\nclient.end_trace(\n    request_id=request_id,\n    outputs={\"final_output_key\": \"final_output_value\"},\n    attributes={\"token_usage\": \"1174\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Installing BigMLFlow\nDESCRIPTION: Command to install the BigMLFlow package using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install bigmlflow\n```\n\n----------------------------------------\n\nTITLE: Enabling Spark Autologging in Python\nDESCRIPTION: Demonstrates how to use the experimental mlflow.spark.autolog() API to automatically log Spark datasource information to the current active run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.spark.autolog()\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow AI Gateway from PyPI\nDESCRIPTION: Command to install MLflow with the genai extras package from PyPI, which includes all dependencies needed for the AI Gateway functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Installation from PyPI\npip install 'mlflow[genai]'\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow VizMod\nDESCRIPTION: Command to install the MLflow VizMod package using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow-vizmod\n```\n\n----------------------------------------\n\nTITLE: MLflow Deployments CLI Commands\nDESCRIPTION: Lists the available CLI commands for MLflow deployments functionality, showing how to get help and information about deployment operations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_83\n\nLANGUAGE: bash\nCODE:\n```\nmlflow deployments --help\nmlflow deployments create --help\nmlflow deployments delete --help\nmlflow deployments update --help\nmlflow deployments list --help\nmlflow deployments get --help\nmlflow deployments run-local --help\nmlflow deployments help --help\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Padding Dataset\nDESCRIPTION: Applies tokenization and left-padding to the training dataset using the Mistral tokenizer, ensuring consistent sequence lengths for batch processing\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\nbase_model_id = \"mistralai/Mistral-7B-v0.1\"\n\n# You can use a different max length if your custom dataset has shorter/longer input sequences.\nMAX_LENGTH = 256\n\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    model_max_length=MAX_LENGTH,\n    padding_side=\"left\",\n    add_eos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ndef tokenize_and_pad_to_fixed_length(sample):\n    result = tokenizer(\n        sample[\"prompt\"],\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\n\ntokenized_train_dataset = train_dataset.map(tokenize_and_pad_to_fixed_length)\n\nassert all(len(x[\"input_ids\"]) == MAX_LENGTH for x in tokenized_train_dataset)\n\ndisplay_table(tokenized_train_dataset.select(range(1)))\n```\n\n----------------------------------------\n\nTITLE: Querying RetrievalQA Model with MLflow in Python\nDESCRIPTION: This snippet demonstrates how to use the loaded model to predict an answer to a complex query about starting a buffalo-themed day spa near Yellowstone National Park. The answer is then formatted and printed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nanswer7 = loaded_model.predict(\n    [\n        {\n            \"query\": \"Can I start a buffalo themed day spa outside of Yellowstone National Park and stifle any competition?\"\n        }\n    ]\n)\n\nprint_formatted_response(answer7)\n```\n\n----------------------------------------\n\nTITLE: Java Client API Usage Example Path Reference\nDESCRIPTION: Reference to a Java code example demonstrating the MLflow Java client API usage. The example shows how to create and manage experiments, runs, and artifacts using the Java API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_65\n\nLANGUAGE: Java\nCODE:\n```\norg.mlflow.tracking.samples.QuickStartDriver.java\n```\n\n----------------------------------------\n\nTITLE: Extending Word List in pyproject.toml\nDESCRIPTION: Configuration in pyproject.toml to extend the list of recognized words for typos checking. This is useful when typos doesn't recognize a valid word.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/typos.md#2025-04-07_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n# pyproject.toml\n\n[tool.typos.default.extend-words]\n...\nmflow = \"mlflow\"\n```\n\n----------------------------------------\n\nTITLE: Disabling User Warnings in Python for OpenAI and MLflow Integration\nDESCRIPTION: This snippet imports the warnings module and disables less useful UserWarnings from setuptools and pydantic that might appear during the tutorial.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Development Environment for MLflow\nDESCRIPTION: Instructions for setting up the Python development environment for MLflow, including automated and manual configuration options.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Environment Setup and Python configuration\n\n#### Automated Python development environment configuration\n\n#### Manual Python development environment configuration\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for DSPy and MLflow\nDESCRIPTION: Installs the required packages for the tutorial, including OpenAI, DSPy, and MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -U openai dspy>=2.5.17 mlflow>=2.18.0\n```\n\n----------------------------------------\n\nTITLE: Customizing MLflow Multistep Workflow Parameters\nDESCRIPTION: This bash command demonstrates how to run the multistep workflow with custom parameters. It adjusts the number of ALS iterations to 20 and sets the number of Keras hidden units to 50.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/multistep_workflow/README.rst#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P als_max_iter=20 -P keras_hidden_units=50\n```\n\n----------------------------------------\n\nTITLE: Recommended Project Structure for MLflow Models\nDESCRIPTION: Illustrates the recommended project structure for organizing model code to efficiently manage code_paths during model logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_project/\n|-- model.py # Defines the custom pyfunc model\n|── train.py # Trains and logs the model\n|── core/    # Required modules for prediction\n|   |── preprocessing.py\n|   └── ...\n└── helper/  # Other helper modules used for training, evaluation\n    |── evaluation.py\n    └── ...\n```\n\n----------------------------------------\n\nTITLE: Recommended Project Structure for MLflow Models\nDESCRIPTION: Illustrates the recommended project structure for organizing model code to efficiently manage code_paths during model logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_project/\n|-- model.py # Defines the custom pyfunc model\n|── train.py # Trains and logs the model\n|── core/    # Required modules for prediction\n|   |── preprocessing.py\n|   └── ...\n└── helper/  # Other helper modules used for training, evaluation\n    |── evaluation.py\n    └── ...\n```\n\n----------------------------------------\n\nTITLE: Evaluating Top K Retrieval Strategy with MLflow in Python\nDESCRIPTION: Uses MLflow to evaluate different Top K retrieval strategies. It calculates precision, recall, and NDCG at various K values, providing insights into the retrieval model's performance at different levels of result relevance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_run() as run:\n    evaluate_results = mlflow.evaluate(\n        data=eval_results_of_retriever_df_bge,\n        targets=\"source\",\n        predictions=\"outputs\",\n        evaluators=\"default\",\n        extra_metrics=[\n            mlflow.metrics.precision_at_k(1),\n            mlflow.metrics.precision_at_k(2),\n            mlflow.metrics.precision_at_k(3),\n            mlflow.metrics.recall_at_k(1),\n            mlflow.metrics.recall_at_k(2),\n            mlflow.metrics.recall_at_k(3),\n            mlflow.metrics.ndcg_at_k(1),\n            mlflow.metrics.ndcg_at_k(2),\n            mlflow.metrics.ndcg_at_k(3),\n        ],\n    )\n\ndisplay(evaluate_results.tables[\"eval_results_table\"])\n```\n\n----------------------------------------\n\nTITLE: Training Sktime Model with MLflow\nDESCRIPTION: Command to run the train.py module which creates a new MLflow experiment and trains a Sktime NaiveForecaster model using the Longley dataset for forecasting.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sktime/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Docker Compose MLflow Services\nDESCRIPTION: This command removes all Docker resources created for the MLflow services, including containers, networks, volumes, and images.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/mlflow_artifacts/README.md#2025-04-07_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# Remove containers, networks, volumes, and images\n$ docker-compose down --rmi all --volumes --remove-orphans\n```\n\n----------------------------------------\n\nTITLE: Downloading MLflow Artifacts\nDESCRIPTION: Shows how to download artifacts from the MLflow Tracking Server using the proxy URI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nrun_id = \"YOUR_RUN_ID\"  # You can find run ID in the Tracking UI\nartifact_path = \"model\"\n\n# Download artifact via the tracking server\nmlflow_artifact_uri = f\"runs://{run_id}/{artifact_path}\"\nlocal_path = mlflow.artifacts.download_artifacts(mlflow_artifact_uri)\n\n# Load the model\nmodel = mlflow.sklearn.load_model(local_path)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Evaluation Dependencies\nDESCRIPTION: Command to install required Python packages for running MLflow evaluation examples including scikit-learn, XGBoost, SHAP, and matplotlib.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install scikit-learn xgboost shap>=0.40 matplotlib\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI\nDESCRIPTION: Configure MLflow to use a local tracking server running on localhost port 8080.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/notebooks/tracking_quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Signature in MLflow\nDESCRIPTION: Prints the model signature to inspect its structure and contents. This helps verify that the signature is correctly defined with the dynamic prediction method parameter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsignature\n```\n\n----------------------------------------\n\nTITLE: Making Predictions Against Deployed Model in Python\nDESCRIPTION: This script sends a POST request to the deployed model endpoint for prediction. It includes error handling and retries in case the server is not immediately ready. The script formats the input data and prints the classification result.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/notebooks/rapids_mlflow.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport requests\n\nhost = \"localhost\"\nport = \"55755\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n}\n\ndata = {\n    \"columns\": [\n        \"Year\",\n        \"Month\",\n        \"DayofMonth\",\n        \"DayofWeek\",\n        \"CRSDepTime\",\n        \"CRSArrTime\",\n        \"UniqueCarrier\",\n        \"FlightNum\",\n        \"ActualElapsedTime\",\n        \"Origin\",\n        \"Dest\",\n        \"Distance\",\n        \"Diverted\",\n    ],\n    \"data\": [[1987, 10, 1, 4, 1, 556, 0, 190, 247, 202, 162, 1846, 0]],\n}\n\n## Pause to let server start\ntime.sleep(5)\n\nwhile True:\n    try:\n        resp = requests.post(\n            url=f\"http://{host}:{port}/invocations\",\n            data=json.dumps({\"dataframe_split\": data}),\n            headers=headers,\n        )\n        print(\"Classification: %s\" % (\"ON-Time\" if resp.text == \"[0.0]\" else \"LATE\"))\n        break\n    except Exception as e:\n        errmsg = f\"Caught exception attempting to call model endpoint: {e}\"\n        print(errmsg, end=\"\")\n        print(\"Sleeping\")\n        time.sleep(20)\n```\n\n----------------------------------------\n\nTITLE: Logging DSPy Model with MLflow\nDESCRIPTION: Code to log the compiled DSPy model using MLflow, including an input example for documentation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nwith mlflow.start_run():\n    model_info = mlflow.dspy.log_model(\n        compiled_pe,\n        \"model\",\n        input_example=\"what is 2 + 2?\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Checking Installed Package Versions in Python\nDESCRIPTION: Command to display version information for an installed Python package using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip show ruff\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Gateway Server Routes\nDESCRIPTION: YAML configuration file defining multiple endpoints for the gateway server, including completions, chat, and embeddings routes using OpenAI models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/guides/step1-create-deployments/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: chat_3.5\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_key: $OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Rendering Multiple Page Cards in Card Group (JSX)\nDESCRIPTION: This code snippet shows how to render multiple PageCard components within a CardGroup to display advanced tutorials. Each PageCard includes a header text, link, and description for different OpenAI-related topics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <PageCard\n    headerText=\"OpenAI ChatCompletions\"\n    link=\"/llms/openai/notebooks/openai-chat-completions/\"\n    text=\"Learn how to leverage the ChatCompletions endpoint in the OpenAI flavor to create a useful text messaging screening tool within MLflow.\"\n  />\n  <PageCard\n    headerText=\"OpenAI Custom Python Model - Code Helper\"\n    link=\"/llms/openai/notebooks/openai-code-helper/\"\n    text=\"Learn how to leverage Custom Python Models with a useful Code Helper application that leverages OpenAI Models and MLflow.\"\n  />\n  <PageCard\n    headerText=\"OpenAI Embeddings - Document Comparison\"\n    link=\"/llms/openai/notebooks/openai-embeddings-generation/\"\n    text=\"Explore the application of embeddings with document comparison using an OpenAI model with MLflow.\"\n  />\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI with SQLite Backend\nDESCRIPTION: Command to start the MLflow UI, specifying the SQLite database as the backend store.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/local-database/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui --port 8080 --backend-store-uri sqlite:///mlruns.db\n```\n\n----------------------------------------\n\nTITLE: Generating RST-only Documentation\nDESCRIPTION: Commands to generate documentation only from RST source files.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nmake rsthtml\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Example as Python Script\nDESCRIPTION: Command to execute the summarization example directly as a Python script. Requires the same dependencies as the MLflow project version.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd summarization && python summarization.py\n```\n\n----------------------------------------\n\nTITLE: Specifying Test Dependencies for MLflow in Python\nDESCRIPTION: This snippet lists the Python packages required specifically for testing the MLflow project. It includes pytest for running tests and pytest-cov for generating test coverage reports.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/requirements/skinny-test-requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n## Test-only dependencies\npytest\npytest-cov\n```\n\n----------------------------------------\n\nTITLE: Running Statsmodels OLS Training Script in Python\nDESCRIPTION: Command to run the Statsmodels training script with the QR factorization inverse method. The inverse method can be set to either 'qr' or 'pinv' (default) to determine how matrix inversion is computed during model training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/statsmodels/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --inverse-method qr\n```\n\n----------------------------------------\n\nTITLE: Basic MLflow Span Context Manager Usage\nDESCRIPTION: Demonstrates the basic usage of MLflow's start_span context manager for manual span creation and management. Shows how to set inputs and outputs on the span object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_span(name=\"my_span\") as span:\n    span.set_inputs({\"x\": 1, \"y\": 2})\n    z = x + y\n    span.set_outputs(z)\n```\n\n----------------------------------------\n\nTITLE: Basic MLflow Span Context Manager Usage\nDESCRIPTION: Demonstrates the basic usage of MLflow's start_span context manager for manual span creation and management. Shows how to set inputs and outputs on the span object.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/api/manual-instrumentation.mdx#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_span(name=\"my_span\") as span:\n    span.set_inputs({\"x\": 1, \"y\": 2})\n    z = x + y\n    span.set_outputs(z)\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: Defines the sidebar position and label for the documentation page in markdown frontmatter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n---\nsidebar_position: 1\nsidebar_label: Tutorials\n---\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installing MLflow and OpenAI packages using pip\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/notebooks/chat-model-tool-calling.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install 'mlflow>=2.17.0' 'openai>=1.0' -qq\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Recipes with CLI Commands\nDESCRIPTION: Shell commands to clone the repository, run the full recipe, and inspect training and evaluation results using the MLflow command-line interface with the local profile.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/mlflow/recipes-regression-template\ncd recipes-regression-template\n# Run the full recipe\nmlflow recipes run --profile local\n# Inspect the model training results\nmlflow recipes inspect --step train --profile local\n# Inspect the resulting model performance evaluations\nmlflow recipes inspect --step evaluate --profile local\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Example with Multiple Custom Parameters\nDESCRIPTION: Command to run the MLflow example with multiple custom parameters for epochs, learning rate, and using a pretrained model. This demonstrates how to override default parameter values.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/CaptumExample/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P max_epochs=5 -P learning_rate=0.01 -P use_pretrained_model=True\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Artifact Example Script\nDESCRIPTION: This command runs the example.py script with the MLflow tracking URI set to the local server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/mlflow_artifacts/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nMLFLOW_TRACKING_URI=http://localhost:5000 python example.py\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Experiment\nDESCRIPTION: Create a new MLflow experiment named 'MLflow Quickstart' to group related runs together.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/notebooks/tracking_quickstart.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"MLflow Quickstart\")\n```\n\n----------------------------------------\n\nTITLE: Defining Substitutable Attributes in recipe.yaml\nDESCRIPTION: This YAML snippet shows how to define a substitutable attribute 'INGEST_CONFIG' in the recipe.yaml configuration file. The attribute specifies the dataset to use for model training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# Specifies the dataset to use for model training\ningest: {{ INGEST_CONFIG }}\n```\n\n----------------------------------------\n\nTITLE: Persisting Pretrained Transformer Model Weights in MLflow for Model Registry\nDESCRIPTION: This snippet demonstrates how to log a transformer model reference without saving weights, then explicitly persisting the model weights before registering it to a non-UC model registry. This approach saves resources during development while ensuring model availability for production.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/large-models/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Log the repository ID as a model. The model weight will not be saved to the artifact store\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n        artifact_path=\"model\",\n    )\n\n# Before registering the model to the non-UC model registry, persist the model weight\n# from the HuggingFace Hub to the artifact location.\nmlflow.transformers.persist_pretrained_model(model_info.model_uri)\n\n# Register the model\nmlflow.register_model(model_info.model_uri, \"your.model.name\")\n```\n\n----------------------------------------\n\nTITLE: Logging a LangChain Model from a Python File to MLflow\nDESCRIPTION: This code snippet demonstrates how to log a LangChain model defined in a separate Python file to MLflow. It uses mlflow.langchain.log_model to register the model from the specified Python file path.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\n\nimport mlflow\n\nchain_path = \"chain.py\"\n\nwith mlflow.start_run():\n    info = mlflow.langchain.log_model(lc_model=chain_path, artifact_path=\"chain\")\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Documentation Page in JSX\nDESCRIPTION: This code snippet sets up the configuration and imports for an MLflow documentation page. It specifies the sidebar position, hides the table of contents, and imports necessary components from Docusaurus and custom components.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/new-features/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n---\nsidebar_position: 4\nhide_table_of_contents: true\n---\n\nimport Link from \"@docusaurus/Link\";\nimport { CardGroup, NewFeatureCard } from \"@site/src/components/Card\";\nimport { APILink } from \"@site/src/components/APILink\";\n```\n\n----------------------------------------\n\nTITLE: Setting Up the LlamaIndex Workflow Environment\nDESCRIPTION: Commands to navigate to the LlamaIndex workflow directory, make the installation script executable, and run it to set up the environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd mlflow/examples/llama_index/workflow\nchmod +x install.sh\n./install.sh\n```\n\n----------------------------------------\n\nTITLE: Enabling System Metrics via MLflow API\nDESCRIPTION: Example demonstrating how to enable system metrics logging using MLflow's API functions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/system-metrics/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.enable_system_metrics_logging()\n\nwith mlflow.start_run() as run:\n    time.sleep(15)\n\nprint(mlflow.MlflowClient().get_run(run.info.run_id).data)\n```\n\n----------------------------------------\n\nTITLE: Including MLflow Tracing Destination Documentation\nDESCRIPTION: RST directive to include documentation for the mlflow.tracing.destination module and its members.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.tracing.rst#2025-04-07_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.tracing.destination\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Adding MLflow Run Example Test Parameter\nDESCRIPTION: Example tuple format for adding a new test case to the pytest.mark.parametrize decorator for mlflow run examples with parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/examples/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n(\"new_example_dir\", [\"-P\", \"param1=123\", \"-P\", \"param2=99\"])\n```\n\n----------------------------------------\n\nTITLE: Custom PythonModel Implementation\nDESCRIPTION: Define custom MLflow PythonModel class that supports multiple prediction methods through parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass ModelWrapper(PythonModel):\n    def __init__(self):\n        self.model = None\n\n    def load_context(self, context):\n        from joblib import load\n\n        self.model = load(context.artifacts[\"model_path\"])\n\n    def predict(self, context, model_input, params=None):\n        params = params or {\"predict_method\": \"predict\"}\n        predict_method = params.get(\"predict_method\")\n\n        if predict_method == \"predict\":\n            return self.model.predict(model_input)\n        elif predict_method == \"predict_proba\":\n            return self.model.predict_proba(model_input)\n        elif predict_method == \"predict_log_proba\":\n            return self.model.predict_log_proba(model_input)\n        else:\n            raise ValueError(f\"The prediction method '{predict_method}' is not supported.\")\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for MLflow fastai Module using Sphinx\nDESCRIPTION: This RST directive instructs Sphinx to automatically generate documentation for the mlflow.fastai module. It includes all members, undocumented members, and shows inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.fastai.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.fastai\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Creating Box Plot for Weekend vs Weekday Demand in Python\nDESCRIPTION: This function generates a box plot to visualize the distribution of demand on weekends versus weekdays. It uses Seaborn to create the plot and overlays individual data points for more context. The function returns a figure object for MLflow logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef plot_box_weekend(df, style=\"seaborn\", plot_size=(10, 8)):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n        sns.boxplot(data=df, x=\"weekend\", y=\"demand\", ax=ax, color=\"lightgray\")\n        sns.stripplot(\n            data=df,\n            x=\"weekend\",\n            y=\"demand\",\n            ax=ax,\n            hue=\"weekend\",\n            palette={0: \"blue\", 1: \"green\"},\n            alpha=0.15,\n            jitter=0.3,\n            size=5,\n        )\n\n        ax.set_title(\"Box Plot of Demand on Weekends vs. Weekdays\", fontsize=14)\n        ax.set_xlabel(\"Weekend (0: No, 1: Yes)\", fontsize=12)\n        ax.set_ylabel(\"Demand\", fontsize=12)\n        for i in ax.get_xticklabels() + ax.get_yticklabels():\n            i.set_fontsize(10)\n        ax.legend_.remove()\n        plt.tight_layout()\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Running Example as MLflow Project with Parameters\nDESCRIPTION: Command to execute the Statsmodels example as an MLflow project with a parameter specifying the inverse method. This runs the project in a reproducible environment defined by the MLproject file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/statsmodels/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P inverse_method=qr\n```\n\n----------------------------------------\n\nTITLE: Using Custom Lissajous PyFunc Model for Predictions\nDESCRIPTION: These code snippets demonstrate how to use the loaded custom PyFunc model to generate Lissajous curves. It shows two examples with different input parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define the input DataFrame. In our custom model, we're reading only the first row of data to generate a plot.\nmodel_input = pd.DataFrame({\"a\": [3], \"b\": [2]})\n\n# Define a params override for the `delta` parameter\nparams = {\"delta\": np.pi / 3}\n\n# Run predict, which will call our internal method `generate_lissajous` before generating a `matplotlib` plot showing the curve\nfig = loaded_pyfunc_model.predict(model_input, params)\n\n# Display the plot\nfig\n```\n\nLANGUAGE: python\nCODE:\n```\n# Try a different configuration of arguments\nfig2 = loaded_pyfunc_model.predict(\n    pd.DataFrame({\"a\": [15], \"b\": [17]}), params={\"delta\": np.pi / 5}\n)\n\nfig2\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Iris Classification Example with Default Parameters\nDESCRIPTION: This command runs the iris_classification.py script using MLflow with default parameters (such as max_epochs=5) defined in the MLproject file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/IrisClassification/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run .\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Apple Sales Data in Python\nDESCRIPTION: This function generates synthetic data for apple sales prediction. It creates features like date, temperature, rainfall, and price, and calculates a demand value based on these features and some randomness.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, timedelta\n\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_apple_sales_data_with_promo_adjustment(base_demand: int = 1000, n_rows: int = 5000):\n    \"\"\"\n    Generates a synthetic dataset for predicting apple sales demand with seasonality and inflation.\n\n    This function creates a pandas DataFrame with features relevant to apple sales.\n    The features include date, average_temperature, rainfall, weekend flag, holiday flag,\n    promotional flag, price_per_kg, and the previous day's demand. The target variable,\n    'demand', is generated based on a combination of these features with some added noise.\n\n    Args:\n        base_demand (int, optional): Base demand for apples. Defaults to 1000.\n        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.\n\n    Returns:\n        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.\n\n    Example:\n        >>> df = generate_apple_sales_data_with_seasonality(base_demand=1200, n_rows=6000)\n        >>> df.head()\n    \"\"\"\n\n    # Set seed for reproducibility\n    np.random.seed(9999)\n\n    # Create date range\n    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]\n    dates.reverse()\n\n    # Generate features\n    df = pd.DataFrame(\n        {\n            \"date\": dates,\n            \"average_temperature\": np.random.uniform(10, 35, n_rows),\n            \"rainfall\": np.random.exponential(5, n_rows),\n            \"weekend\": [(date.weekday() >= 5) * 1 for date in dates],\n            \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),\n            \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),\n            \"month\": [date.month for date in dates],\n        }\n    )\n\n    # Introduce inflation over time (years)\n    df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03\n\n    # Incorporate seasonality due to apple harvests\n    df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(\n        2 * np.pi * (df[\"month\"] - 9) / 12\n    )\n\n    # Modify the price_per_kg based on harvest effect\n    df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5\n\n    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month\n    peak_months = [4, 10]  # months following the peak availability\n    df[\"promo\"] = np.where(\n        df[\"month\"].isin(peak_months),\n        1,\n        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),\n    )\n\n    # Generate target variable based on features\n    base_price_effect = -df[\"price_per_kg\"] * 50\n    seasonality_effect = df[\"harvest_effect\"] * 50\n    promo_effect = df[\"promo\"] * 200\n\n    df[\"demand\"] = (\n        base_demand\n        + base_price_effect\n        + seasonality_effect\n        + promo_effect\n        + df[\"weekend\"] * 300\n        + np.random.normal(0, 50, n_rows)\n    ) * df[\"inflation_multiplier\"]  # adding random noise\n\n    # Add previous day's demand\n    df[\"previous_days_demand\"] = df[\"demand\"].shift(1)\n    df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n\n    # Drop temporary columns\n    df.drop(columns=[\"inflation_multiplier\", \"harvest_effect\", \"month\"], inplace=True)\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Generating Box Plot for Weekend Demand Analysis in Python\nDESCRIPTION: This function creates a box plot comparing demand on weekends vs weekdays using matplotlib and seaborn. It includes data points, customized styling, and proper labeling. The plot is returned as a figure object for easy logging with MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/part2-logging-plots/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef plot_box_weekend(df, style=\"seaborn\", plot_size=(10, 8)):\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n        sns.boxplot(data=df, x=\"weekend\", y=\"demand\", ax=ax, color=\"lightgray\")\n        sns.stripplot(\n            data=df,\n            x=\"weekend\",\n            y=\"demand\",\n            ax=ax,\n            hue=\"weekend\",\n            palette={0: \"blue\", 1: \"green\"},\n            alpha=0.15,\n            jitter=0.3,\n            size=5,\n        )\n\n        ax.set_title(\"Box Plot of Demand on Weekends vs. Weekdays\", fontsize=14)\n        ax.set_xlabel(\"Weekend (0: No, 1: Yes)\", fontsize=12)\n        ax.set_ylabel(\"Demand\", fontsize=12)\n        for i in ax.get_xticklabels() + ax.get_yticklabels():\n            i.set_fontsize(10)\n        ax.legend_.remove()\n        plt.tight_layout()\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Customizing System Metrics Logging Frequency\nDESCRIPTION: Example demonstrating how to customize the sampling interval and aggregation of system metrics logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/system-metrics/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_system_metrics_sampling_interval(1)\nmlflow.set_system_metrics_samples_before_logging(3)\n\nwith mlflow.start_run(log_system_metrics=True) as run:\n    time.sleep(15)\n\nmetric_history = mlflow.MlflowClient().get_metric_history(\n    run.info.run_id,\n    \"system/cpu_utilization_percentage\",\n)\nprint(metric_history)\n```\n\n----------------------------------------\n\nTITLE: Training, Logging, and Using ONNX Model with MLflow\nDESCRIPTION: Trains a PyTorch model, converts it to ONNX format, logs it to MLflow, and then loads the model to make predictions using the MLflow pyfunc interface.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport mlflow\nfrom mlflow.models import infer_signature\nimport onnx\nimport torch\nfrom torch import nn\n\n# define a torch model\nnet = nn.Linear(6, 1)\nloss_function = nn.L1Loss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n\nX = torch.randn(6)\ny = torch.randn(1)\n\n# run model training\nepochs = 5\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = net(X)\n\n    loss = loss_function(outputs, y)\n    loss.backward()\n\n    optimizer.step()\n\n# convert model to ONNX and load it\ntorch.onnx.export(net, X, \"model.onnx\")\nonnx_model = onnx.load_model(\"model.onnx\")\n\n# log the model into a mlflow run\nwith mlflow.start_run():\n    signature = infer_signature(X.numpy(), net(X).detach().numpy())\n    model_info = mlflow.onnx.log_model(onnx_model, \"model\", signature=signature)\n\n# load the logged model and make a prediction\nonnx_pyfunc = mlflow.pyfunc.load_model(model_info.model_uri)\n\npredictions = onnx_pyfunc.predict(X.numpy())\nprint(predictions)\n```\n\n----------------------------------------\n\nTITLE: Importing React Card Components\nDESCRIPTION: Import statement for custom card components used to display tutorial navigation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { CardGroup, PageCard } from \"@site/src/components/Card\";\n```\n\n----------------------------------------\n\nTITLE: Initializing MLflow Auto Logging with Basic Run Context in Python\nDESCRIPTION: Basic example of enabling MLflow autologging and creating a run context where training code would be executed. MLflow will automatically log metrics, parameters, and models within the run scope.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/autolog/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.autolog()\n\nwith mlflow.start_run():\n    # your training code goes here\n    ...\n```\n\n----------------------------------------\n\nTITLE: Serving PyFunc Model\nDESCRIPTION: Command to serve the trained PyFunc model using MLflow's model serving capability. Requires replacing <pyfunc_run_id> with the actual run ID from training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pyfunc/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ mlflow models serve -m \"runs:/<pyfunc_run_id>/model\" -p 5001\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for LLM Integration\nDESCRIPTION: Configures the OpenAI API key for authentication with their services. This is required to use OpenAI's language models for question generation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nopenai.api_key = \"<redacted>\"\nos.environ[\"OPENAI_API_KEY\"] = openai.api_key\n```\n\n----------------------------------------\n\nTITLE: Interface Surface Labels for MLflow\nDESCRIPTION: Labels for different interface surfaces in MLflow, including UI/UX, Docker, SQLAlchemy, and Windows support.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/ISSUE_TRIAGE.rst#2025-04-07_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n- area/uiux\n- area/docker\n- area/sqlalchemy\n- area/windows\n```\n\n----------------------------------------\n\nTITLE: Making Prediction Request\nDESCRIPTION: curl command to send a prediction request to the served model. Sends a single sample with 4 features for prediction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pyfunc/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d '{\n  \"dataframe_records\": [[1, 1, 1, 1]]\n}'\n```\n\n----------------------------------------\n\nTITLE: MLflow Custom Flavor Model Logging\nDESCRIPTION: Implementation of log_model() function for logging models to MLflow tracking\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ndef log_model(\n    sktime_model,\n    artifact_path,\n    conda_env=None,\n    code_paths=None,\n    registered_model_name=None,\n    signature=None,\n    input_example=None,\n    await_registration_for=DEFAULT_AWAIT_MAX_SLEEP_SECONDS,\n    pip_requirements=None,\n    extra_pip_requirements=None,\n    serialization_format=SERIALIZATION_FORMAT_PICKLE,\n    **kwargs,\n):\n    return Model.log(\n        artifact_path=artifact_path,\n        flavor=flavor,\n        registered_model_name=registered_model_name,\n        sktime_model=sktime_model,\n        conda_env=conda_env,\n        code_paths=code_paths,\n        signature=signature,\n        input_example=input_example,\n        await_registration_for=await_registration_for,\n        pip_requirements=pip_requirements,\n        extra_pip_requirements=extra_pip_requirements,\n        serialization_format=serialization_format,\n        **kwargs,\n    )\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow Deployment Structure\nDESCRIPTION: Markdown documentation outlining the structure and available examples for MLflow deployments, with specific reference to Databricks integration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/deployments/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# MLflow Deployments\n\nThe examples provided within this directory show how to get started with MLflow Deployments using:\n\n- Databricks (see the `databricks` subdirectory)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow MNIST Example with Custom Epochs\nDESCRIPTION: Command to run the MNIST example using MLflow with a custom number of epochs. Replace X with the desired number of epochs for training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/MNIST/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P epochs=X\n```\n\n----------------------------------------\n\nTITLE: Generating and Displaying Synthetic Data in Python\nDESCRIPTION: This snippet generates synthetic apple sales data using the previously defined function and displays the last 20 rows of the resulting DataFrame.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndata = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=1_000)\n\ndata[-20:]\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Output\nDESCRIPTION: Simple code to display the output from the model predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/introduction.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel_output\n```\n\n----------------------------------------\n\nTITLE: Initializing Answer Correctness Metric\nDESCRIPTION: Creates an answer correctness metric using GPT-4 as the evaluation model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nanswer_correctness_metric = answer_correctness(model=\"openai:/gpt-4\")\n```\n\n----------------------------------------\n\nTITLE: Displaying a Trace as Last Cell Expression\nDESCRIPTION: Demonstrates how a trace object is automatically displayed when it's the last expression evaluated in a cell. This is the simplest way to view a trace in the notebook.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Displaying as a result of the trace being the last expression\ntrace\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Documentation Dependencies\nDESCRIPTION: Command to install required node dependencies using yarn package manager\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn\n```\n\n----------------------------------------\n\nTITLE: Running TGI Server with Docker\nDESCRIPTION: This Docker command starts a TGI server on the local machine, port 8000, loading the falcon-7b-instruct model. It requires GPU support and sets up a shared volume for data persistence.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/huggingface/README.md#2025-04-07_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmodel=tiiuae/falcon-7b-instruct\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ndocker run --gpus all --shm-size 1g -p 8000:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.1 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Iris Classification Example with Local Environment\nDESCRIPTION: This command executes the MLflow project using the local environment manager instead of creating a new conda environment, which is useful when you already have the required dependencies installed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/IrisClassification/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . --env-manager=local\n```\n\n----------------------------------------\n\nTITLE: Making Another Prediction with the Loaded MLflow Model\nDESCRIPTION: Uses the previously loaded LangChain model to generate preparation instructions for an Okonomiyaki recipe serving 12 people, demonstrating the model's versatility across different cuisines.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-quickstart.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndish2 = loaded_model.predict({\"recipe\": \"Okonomiyaki\", \"customer_count\": \"12\"})\n\nprint(dish2[0])\n```\n\n----------------------------------------\n\nTITLE: MLflow Database Migration Command\nDESCRIPTION: Command to upgrade MLflow database schema for MySQL backends.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_59\n\nLANGUAGE: SQL\nCODE:\n```\nmlflow db upgrade\n```\n\n----------------------------------------\n\nTITLE: Referencing Conda Environment in MLproject File\nDESCRIPTION: YAML snippet showing how to reference a Conda environment YAML file in an MLproject file using the conda_env field and a relative path.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nconda_env: files/config/conda_environment.yaml\n```\n\n----------------------------------------\n\nTITLE: Importing MLflow Dependency Requirements\nDESCRIPTION: Combines multiple requirement files using pip's -r flag to include all necessary dependencies for MLflow development, testing, linting and documentation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/requirements/dev-requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-r extra-ml-requirements.txt\n-r test-requirements.txt\n-r lint-requirements.txt\n-r doc-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Logging Sentence Transformer Embeddings Model\nDESCRIPTION: Python code to create and log a sentence transformer model for embeddings generation using MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nimport mlflow\n\n\nmodel = SentenceTransformer(model_name_or_path=\"all-MiniLM-L6-v2\")\nartifact_path = \"embeddings_model\"\n\nwith mlflow.start_run():\n    mlflow.sentence_transformers.log_model(\n        model,\n        artifact_path=artifact_path,\n    )\n    model_uri = mlflow.get_artifact_uri(artifact_path)\n```\n\n----------------------------------------\n\nTITLE: Running MNIST Pruning Script\nDESCRIPTION: Command to execute the iterative pruning script with specified maximum epochs and total trials parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/IterativePruning/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython iterative_prune_mnist.py --max_epochs 10 --total_trials 3\n```\n\n----------------------------------------\n\nTITLE: Testing MLflow Model API with curl\nDESCRIPTION: Example curl command to test the deployed model's REST API. Sends a POST request with wine quality prediction data in the dataframe_split format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl -d '{\"dataframe_split\": {\n\"columns\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"],\n\"data\": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]]}}' \\\n-H 'Content-Type: application/json' -X POST localhost:5002/invocations\n```\n\n----------------------------------------\n\nTITLE: Initializing Signature Report Function in Python\nDESCRIPTION: Defines a function 'report_signature_info' that infers and prints the signature information for given input and output data. It uses MLflow's infer_signature function to generate the signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nimport mlflow\nfrom mlflow.models.signature import infer_signature, set_signature\n\n\ndef report_signature_info(input_data, output_data=None, params=None):\n    inferred_signature = infer_signature(input_data, output_data, params)\n\n    report = f\"\"\"\nThe input data: \\n\\t{input_data}.\nThe data is of type: {type(input_data)}.\nThe inferred signature is:\\n\\n{inferred_signature}\n\"\"\"\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Evaluation\nDESCRIPTION: Executing the evaluation process using MLflow's evaluate function with configured metrics and parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/rag-evaluation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresults = mlflow.evaluate(\n    model,\n    eval_df,\n    model_type=\"question-answering\",\n    evaluators=\"default\",\n    predictions=\"result\",\n    extra_metrics=[faithfulness_metric, mlflow.metrics.latency()],\n    evaluator_config={\n        \"col_mapping\": {\n            \"inputs\": \"questions\",\n            \"context\": \"source_documents\",\n        }\n    },\n)\nprint(results.metrics)\n```\n\n----------------------------------------\n\nTITLE: Defining Catalog and Schema Constants\nDESCRIPTION: Sets constants for the catalog and schema names to be used in UnityCatalog function creation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nCATALOG = \"ml\"\nSCHEMA = \"serena_test\"\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Fluent API for Tracing\nDESCRIPTION: Example of using MLflow's fluent API context handler for detailed span control and attribute setting in a tracing implementation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith mlflow.start_span(\"Audit Agent\") as root_span:\n    root_span.set_inputs(messages)\n    attributes = {**params.to_dict(), **self.models_config, **self.models}\n    root_span.set_attributes(attributes)\n    # More span manipulation...\n```\n\n----------------------------------------\n\nTITLE: Rendering Card Component for MLflow Tracking Guide in JSX\nDESCRIPTION: This JSX code renders a CardGroup containing a PageCard component. The PageCard links to the MLflow Tracking Quickstart guide and provides a brief description of its contents.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/notebooks/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <PageCard headerText=\"MLflow Tracking Quickstart Guide\" link=\"/getting-started/intro-quickstart/notebooks/tracking_quickstart\" text=\"Learn the basics of MLflow Tracking in a fast-paced guide with a focus on seeing your first model in the MLflow UI\" />\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Querying MLflow AI Gateway Endpoint with Additional Parameters in Python\nDESCRIPTION: This snippet demonstrates how to submit a query request to an MLflow AI Gateway endpoint using additional parameters such as temperature, max_tokens, and frequency_penalty.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client(\"http://my.deployments:8888\")\n\ndata = {\n    \"prompt\": (\n        \"What would happen if an asteroid the size of \"\n        \"a basketball encountered the Earth traveling at 0.5c? \"\n        \"Please provide your answer in .rst format for the purposes of documentation.\"\n    ),\n    \"temperature\": 0.5,\n    \"max_tokens\": 1000,\n    \"n\": 1,\n    \"frequency_penalty\": 0.2,\n    \"presence_penalty\": 0.2,\n}\n\nclient.predict(endpoint=\"completions-gpt4\", inputs=data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Environment with Local Image\nDESCRIPTION: YAML snippet showing a simple Docker environment configuration that uses a local Docker image without registry path for an MLflow project.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndocker_env:\n  image: mlflow-docker-example-environment\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow via pip\nDESCRIPTION: Command to install the MLflow Python package using pip package manager.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/skinny/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Scoring Model via REST API\nDESCRIPTION: Command to run the scoring script that sends prediction requests to the served model endpoint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sktime/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython score_model.py\n```\n\n----------------------------------------\n\nTITLE: Standardized Chat Input Example\nDESCRIPTION: Shows the expected input format for ChatModel's predict method using the standardized chat schema.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-intro/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninput = {\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}],\n    \"max_tokens\": 25,\n}\n```\n\n----------------------------------------\n\nTITLE: Inferring Signature for List of Strings in Python\nDESCRIPTION: Demonstrates how to infer the signature for a list of strings using the report_signature_info function. This showcases MLflow's ability to handle simple scalar types.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# List of strings\n\nreport_signature_info([\"a\", \"list\", \"of\", \"strings\"])\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for LLM Access\nDESCRIPTION: Sets the OpenAI API key for authentication with the OpenAI service. This key is required to use OpenAI's language models for generating questions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nopenai.api_key = \"<redacted>\"\nos.environ[\"OPENAI_API_KEY\"] = openai.api_key\n```\n\n----------------------------------------\n\nTITLE: Creating a Relevance Metric with Llama2 as Judge\nDESCRIPTION: Defines a relevance metric using Databricks-hosted Llama2-70b-chat model as a judge to evaluate how well the RAG system's responses address the given questions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrelevance_metric = relevance(model=\"endpoints:/databricks-llama-2-70b-chat\")\nprint(relevance_metric)\n```\n\n----------------------------------------\n\nTITLE: Deleting MLflow Models and Versions in Python\nDESCRIPTION: This code snippet shows how to delete specific versions of a registered model and how to delete an entire registered model with all its versions using the MLflow Client API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model-registry/index.mdx#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclient = MlflowClient()\nversions = [1, 2, 3]\nfor version in versions:\n    client.delete_model_version(\n        name=\"sk-learn-random-forest-reg-model\", version=version\n    )\n\nclient.delete_registered_model(name=\"sk-learn-random-forest-reg-model\")\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow Gemini Module with Sphinx\nDESCRIPTION: Sphinx documentation configuration for auto-generating API documentation from the mlflow.gemini module. Uses automodule directive to include all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.gemini.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.gemini\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Restoring Deleted MLflow Run in R\nDESCRIPTION: This function restores a specific MLflow run using its run ID. It optionally accepts an MLflow client object for specifying the tracking server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_39\n\nLANGUAGE: r\nCODE:\n```\nmlflow_restore_run(run_id, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Package Versions for Testing\nDESCRIPTION: Commands to create a test environment and install a specific version of scikit-learn for compatibility testing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndev/dev-env-setup.sh -d ~/.venvs/sklearn-test -q\nsource ~/.venvs/sklearn-test/bin/activate\npip freeze | grep \"scikit-learn\"\n>> scikit-learn==1.0.2\npip install scikit-learn==1.0.1\npip freeze | grep \"scikit-learn\"\n>> scikit-learn==1.0.1\n```\n\n----------------------------------------\n\nTITLE: Importing React Components for Documentation\nDESCRIPTION: Imports necessary React components for the documentation site, including APILink for API references and Tabs/TabItem for creating tabbed content sections.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/ui.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { APILink } from \"@site/src/components/APILink\";\nimport Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI to View Experiments\nDESCRIPTION: Command to open the MLflow user interface for viewing the logged experiments, metrics, parameters, and artifacts from the trained Sktime model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sktime/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Logging Artifacts in MLflow with R\nDESCRIPTION: Function to log a file or directory as an artifact for an MLflow run. Requires appropriate permissions when logging to cloud storage like Amazon S3.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_27\n\nLANGUAGE: r\nCODE:\n```\nmlflow_log_artifact(path, artifact_path = NULL, run_id = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Loading Evaluation Dataset\nDESCRIPTION: Loading and preprocessing a synthetic evaluation dataset for testing the RAG system's performance.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nEVALUATION_DATASET_PATH = \"https://raw.githubusercontent.com/mlflow/mlflow/master/examples/llms/RAG/static_evaluation_dataset.csv\"\n\nsynthetic_eval_data = pd.read_csv(EVALUATION_DATASET_PATH)\n\nsynthetic_eval_data[\"source\"] = synthetic_eval_data[\"source\"].apply(ast.literal_eval)\nsynthetic_eval_data[\"retrieved_doc_ids\"] = synthetic_eval_data[\"retrieved_doc_ids\"].apply(\n    ast.literal_eval\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for LlamaIndex Workflow and MLflow\nDESCRIPTION: Installs MLflow (version 2.17.0 or higher), LlamaIndex (version 0.11.16 or higher), and the Workflow utility package required for rendering Workflow as HTML.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install mlflow>=2.17.0 llama-index>=0.11.16 -qqqU\n# Workflow util is required for rendering Workflow as HTML\n%pip install llama-index-utils-workflow -qqqU\n```\n\n----------------------------------------\n\nTITLE: Filtering MLflow Runs by Metric Value\nDESCRIPTION: Python code showing how to filter MLflow runs based on a metric condition\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nbad_runs = mlflow.search_runs(\n    filter_string=\"metrics.loss > 0.8\", search_all_experiments=True\n)\nprint(bad_runs)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for MLflow OpenAI Module\nDESCRIPTION: This reStructuredText directive configures the Sphinx documentation generator to automatically document the mlflow.openai module. It includes all members, undocumented members, and shows inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.openai.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: mlflow.openai\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Building JavaScript Frontend Files for MLflow Server\nDESCRIPTION: Commands to build the MLflow server's JavaScript frontend files required for distribution. This generates the JS files in the build directory using yarn.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ncd mlflow/server/js\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Querying Total Metric Entries in MLflow Database\nDESCRIPTION: SQL query to determine the total number of metric entries by grouping metrics by key and run UUID. This helps assess the potential migration duration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/store/db_migrations/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT count(*) FROM metrics GROUP BY metrics.key, run_uuid;\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Version Requirement for MLflow\nDESCRIPTION: A simple declaration of Python version 3.9 as the required or recommended version for the MLflow project. This specification helps ensure compatibility between the codebase and the Python runtime environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/requirements/python-version.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n3.9\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI with AI Gateway for PromptLab\nDESCRIPTION: Command to start the MLflow UI with AI Gateway support for PromptLab development.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython dev/server.py\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Training Script\nDESCRIPTION: Command to execute the LightGBM model training script that includes MLflow tracking and autologging functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/lightgbm/lightgbm_sklearn/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Loading Hugging Face Pipeline\nDESCRIPTION: Initializing a text generation pipeline using the MPT-7B-chat model from MosaicML\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmpt_pipeline = pipeline(\"text-generation\", model=\"mosaicml/mpt-7b-chat\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Vector Store Index in LlamaIndex with Python\nDESCRIPTION: Loads documents from a directory and creates a VectorStoreIndex. This code shows how to initialize a basic index from documents stored in the local filesystem.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Experiment Tracking with scikit-learn in MLflow\nDESCRIPTION: This snippet shows how to enable MLflow's automatic logging for scikit-learn models. It loads a diabetes dataset, splits it into training and testing sets, creates a RandomForestRegressor model, and fits it. MLflow automatically tracks parameters, metrics, and the trained model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/skinny/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Enable MLflow's automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Apple Sales Data\nDESCRIPTION: Creates synthetic training data for apple sales prediction with multiple features including price, seasonality, promotions, and competitor effects. Generates time series data with realistic correlations between features.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef generate_apple_sales_data_with_promo_adjustment(\n    base_demand: int = 1000,\n    n_rows: int = 5000,\n    competitor_price_effect: float = -50.0,\n):\n    \"\"\"\n    Generates a synthetic dataset for predicting apple sales demand with multiple\n    influencing factors.\n\n    This function creates a pandas DataFrame with features relevant to apple sales.\n    The features include date, average_temperature, rainfall, weekend flag, holiday flag,\n    promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,\n    and the previous day's demand. The target variable, 'demand', is generated based on a\n    combination of these features with some added noise.\n\n    Args:\n        base_demand (int, optional): Base demand for apples. Defaults to 1000.\n        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.\n        competitor_price_effect (float, optional): Effect of competitor's price being lower\n                                                   on our sales. Defaults to -50.\n\n    Returns:\n        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.\n\n    Example:\n        >>> df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)\n        >>> df.head()\n    \"\"\"\n\n    # Set seed for reproducibility\n    np.random.seed(9999)\n\n    # Create date range\n    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]\n    dates.reverse()\n\n    # Generate features\n    df = pd.DataFrame(\n        {\n            \"date\": dates,\n            \"average_temperature\": np.random.uniform(10, 35, n_rows),\n            \"rainfall\": np.random.exponential(5, n_rows),\n            \"weekend\": [(date.weekday() >= 5) * 1 for date in dates],\n            \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),\n            \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),\n            \"month\": [date.month for date in dates],\n        }\n    )\n\n    # Introduce inflation over time (years)\n    df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03\n\n    # Incorporate seasonality due to apple harvests\n    df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(\n        2 * np.pi * (df[\"month\"] - 9) / 12\n    )\n\n    # Modify the price_per_kg based on harvest effect\n    df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5\n\n    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month\n    peak_months = [4, 10]  # months following the peak availability\n    df[\"promo\"] = np.where(\n        df[\"month\"].isin(peak_months),\n        1,\n        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),\n    )\n\n    # Generate target variable based on features\n    base_price_effect = -df[\"price_per_kg\"] * 50\n    seasonality_effect = df[\"harvest_effect\"] * 50\n    promo_effect = df[\"promo\"] * 200\n\n    df[\"demand\"] = (\n        base_demand\n        + base_price_effect\n        + seasonality_effect\n        + promo_effect\n        + df[\"weekend\"] * 300\n        + np.random.normal(0, 50, n_rows)\n    ) * df[\"inflation_multiplier\"]  # adding random noise\n\n    # Add previous day's demand\n    df[\"previous_days_demand\"] = df[\"demand\"].shift(1)\n    df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n\n    # Introduce competitor pricing\n    df[\"competitor_price_per_kg\"] = np.random.uniform(0.5, 3, n_rows)\n    df[\"competitor_price_effect\"] = (\n        df[\"competitor_price_per_kg\"] < df[\"price_per_kg\"]\n    ) * competitor_price_effect\n\n    # Stock availability based on past sales price (3 days lag with logarithmic decay)\n    log_decay = -np.log(df[\"price_per_kg\"].shift(3) + 1) + 2\n    df[\"stock_available\"] = np.clip(log_decay, 0.7, 1)\n\n    # Marketing intensity based on stock availability\n    # Identify where stock is above threshold\n    high_stock_indices = df[df[\"stock_available\"] > 0.95].index\n\n    # For each high stock day, increase marketing intensity for the next week\n    for idx in high_stock_indices:\n        df.loc[idx : min(idx + 7, n_rows - 1), \"marketing_intensity\"] = np.random.uniform(0.7, 1)\n\n    # If the marketing_intensity column already has values, this will preserve them;\n    #  if not, it sets default values\n    fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)\n    df[\"marketing_intensity\"].fillna(fill_values, inplace=True)\n\n    # Adjust demand with new factors\n    df[\"demand\"] = df[\"demand\"] + df[\"competitor_price_effect\"] + df[\"marketing_intensity\"]\n\n    # Drop temporary columns\n    df.drop(\n        columns=[\n            \"inflation_multiplier\",\n            \"harvest_effect\",\n            \"month\",\n            \"competitor_price_effect\",\n            \"stock_available\",\n        ],\n        inplace=True,\n    )\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow via pip\nDESCRIPTION: Command to install the MLflow Python package using pip package manager.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Data for LlamaIndex with Shell Commands\nDESCRIPTION: Creates a data directory and downloads Paul Graham's essay for use in LlamaIndex examples. This shell command retrieves the text file that will be used to create an index in subsequent steps.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmkdir -p data\ncurl -L https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -o ./data/paul_graham_essay.txt\n```\n\n----------------------------------------\n\nTITLE: Running the R Linter for MLflow\nDESCRIPTION: Command to run the linter on the R package code to ensure it follows R coding style conventions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nRscript -e 'lintr::lint_package()'\n```\n\n----------------------------------------\n\nTITLE: MLflow Server CLI Commands\nDESCRIPTION: Commands to start MLflow tracking server and UI with SQLAlchemy URI support.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_60\n\nLANGUAGE: Shell\nCODE:\n```\nmlflow server\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Tracing for DSPy Compilation\nDESCRIPTION: Example showing how to enable tracing during DSPy compilation/optimization process. By default, tracing during compilation is disabled but can be activated with a parameter.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/dspy.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dspy\nimport mlflow\n\n# Enable auto-tracing for compilation\nmlflow.dspy.autolog(log_traces_from_compile=True)\n\n# Optimize the DSPy program as usual\ntp = dspy.MIPROv2(metric=metric, auto=\"medium\", num_threads=24)\noptimized = tp.compile(cot, trainset=trainset, ...)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Project on Kubernetes\nDESCRIPTION: Command showing how to execute an MLflow project using the Kubernetes backend with a specific configuration file\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/projects/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run <project_uri> --backend kubernetes --backend-config examples/docker/kubernetes_config.json\n```\n\n----------------------------------------\n\nTITLE: Loading LlamaIndex Workflow\nDESCRIPTION: Shows different methods to load and use a logged LlamaIndex workflow\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Use mlflow.llama_index.load_model to load the workflow object as is\nworkflow = mlflow.llama_index.load_model(model_info.model_uri)\nawait workflow.run(input=\"What is MLflow?\")\n\n# Use mlflow.pyfunc.load_model to load the workflow as a MLflow Pyfunc Model\n# with standard inference APIs for deployment and evaluation.\npyfunc = mlflow.pyfunc.load_model(model_info.model_uri)\npyfunc.predict({\"input\": \"What is MLflow?\"})\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Experiment and Run Parameters\nDESCRIPTION: Configures the active MLflow experiment, defines a run name for the training iteration, and specifies the artifact path where the model will be saved. This establishes the organizational structure for tracking model development.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step6-logging-a-run/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Sets the current active experiment to the \"Apple_Models\" experiment and\n# returns the Experiment metadata\napple_experiment = mlflow.set_experiment(\"Apple_Models\")\n\n# Define a run name for this iteration of training.\n# If this is not set, a unique name will be auto-generated for your run.\nrun_name = \"apples_rf_test\"\n\n# Define an artifact path that the model will be saved to.\nartifact_path = \"rf_apples\"\n```\n\n----------------------------------------\n\nTITLE: Building FAISS Retriever\nDESCRIPTION: Creates and configures a FAISS vector store with OpenAI embeddings for document retrieval.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nembeddings = OpenAIEmbeddings()\n\nscrapped_df = pd.read_csv(SCRAPPED_DOCS_PATH)\nlist_of_documents = [\n    Document(page_content=row[\"text\"], metadata={\"source\": row[\"source\"]})\n    for i, row in scrapped_df.iterrows()\n]\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\ndocs = text_splitter.split_documents(list_of_documents)\ndb = FAISS.from_documents(docs, embeddings)\n\n# Save the db to local disk\ndb.save_local(DB_PERSIST_DIR)\n```\n\n----------------------------------------\n\nTITLE: Logging Spark Models in MLflow\nDESCRIPTION: Updated function to log Spark models and their descendants to MLflow tracking server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.spark.log_model()\n```\n\n----------------------------------------\n\nTITLE: Logging XGBoost Models in R with MLflow\nDESCRIPTION: Demonstrates how to log and load XGBoost models using MLflow's R API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_39\n\nLANGUAGE: R\nCODE:\n```\nmlflow_log_model(xgboost_model, \"model\")\nloaded_model <- mlflow_load_model(\"model\")\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Model Locally with SageMaker Configuration\nDESCRIPTION: This command deploys an MLflow model locally using a Docker container with SageMaker-compatible configuration for testing purposes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-to-sagemaker/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow deployments run-local -t sagemaker -m runs:/<run_id>/model -p 5000\n```\n\n----------------------------------------\n\nTITLE: MLflow XGBoost Autologging Command\nDESCRIPTION: The core command used to enable automatic logging of XGBoost model metrics and parameters in MLflow. This command works universally for both native XGBoost models trained with xgboost.train() and XGBoost scikit-learn models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/xgboost/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmlflow.xgboost.autolog()\n```\n\n----------------------------------------\n\nTITLE: Querying Deployed MLflow Model\nDESCRIPTION: Example of making a REST API call to query a deployed MLflow model server\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/prompt-engineering/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ninput='\n{\n    \"dataframe_records\": [\n        {\n            \"article\": \"An MLflow Project is a format for packaging data science code...\",\n            \"question\": \"What is an MLflow Project?\"\n        }\n    ]\n}'\n\necho $input | curl \\\n  -s \\\n  -X POST \\\n  https://localhost:8000/invocations \\\n  -H 'Content-Type: application/json' \\\n  -d @-\n```\n\n----------------------------------------\n\nTITLE: Setting Up BM25 Retriever\nDESCRIPTION: Creates and persists a BM25 retriever for keyword-based document retrieval.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.retrievers.bm25 import BM25Retriever\n\nsplitter = SentenceSplitter(chunk_size=512)\nnodes = splitter.get_nodes_from_documents(documents)\nbm25_retriever = BM25Retriever.from_defaults(nodes=nodes)\nbm25_retriever.persist(\".bm25_retriever\")\n```\n\n----------------------------------------\n\nTITLE: Example output when starting MLflow server\nDESCRIPTION: Sample terminal output showing successful startup of an MLflow server. It displays the server process information and the URL where the server is listening.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/tracking-server-overview/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n(mlflow) [master][~/Documents/mlflow_team/mlflow]$ mlflow ui\n[2023-10-25 19:39:12 -0700] [50239] [INFO] Starting gunicorn 20.1.0\n[2023-10-25 19:39:12 -0700] [50239] [INFO] Listening at: http://127.0.0.1:5000 (50239)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow example with default parameters\nDESCRIPTION: Command to run the Ax hyperparameter optimization example with default parameters using MLflow CLI. This executes the project with default values from the MLproject file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/AxHyperOptimizationPTL/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run .\n```\n\n----------------------------------------\n\nTITLE: Running MLflow REST API Example\nDESCRIPTION: Command to execute the Python example script demonstrating MLflow REST API usage\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rest_api/README.rst#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython mlflow_tracking_rest_api.py\n```\n\n----------------------------------------\n\nTITLE: MLflow R API Model Retrieval\nDESCRIPTION: Example of retrieving experiment information using MLflow's R client API\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_34\n\nLANGUAGE: R\nCODE:\n```\nmlflow_get_experiment()\nmlflow_list_experiments()\nmlflow_get_run()\n```\n\n----------------------------------------\n\nTITLE: Running Wine Quality Model with Minimal Regularization\nDESCRIPTION: Runs the wine quality prediction model with very low regularization parameters (alpha=0.1, l1_ratio=0.1), which should lead to a more flexible model fit compared to previous runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/sklearn_elasticnet_wine/train.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain(0.1, 0.1)\n```\n\n----------------------------------------\n\nTITLE: Logging Model with Tensor-based Example in Python\nDESCRIPTION: This snippet shows how to log a TensorFlow model with a tensor-based input example using a numpy array.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# each input has shape (4, 4)\ninput_example = np.array(\n    [\n        [[0, 0, 0, 0], [0, 134, 25, 56], [253, 242, 195, 6], [0, 93, 82, 82]],\n        [[0, 23, 46, 0], [33, 13, 36, 166], [76, 75, 0, 255], [33, 44, 11, 82]],\n    ],\n    dtype=np.uint8,\n)\nmlflow.tensorflow.log_model(..., input_example=input_example)\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions with MLflow Model\nDESCRIPTION: Demonstrates how to use a loaded MLflow model to generate predictions by passing input variables as a dictionary\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/prompt-engineering/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\narticle_text = \"\"\"\nAn MLflow Project is a format for packaging data science code in a reusable and reproducible way.\nThe MLflow Projects component includes an API and command-line tools for running projects, which\nalso integrate with the Tracking component to automatically record the parameters and git commit\nof your source code for reproducibility.\n\nThis article describes the format of an MLflow Project and how to run an MLflow project remotely\nusing the MLflow CLI, which makes it easy to vertically scale your data science code.\n\"\"\"\nquestion = \"What is an MLflow project?\"\n\nloaded_model.predict({\"article\": article_text, \"question\": question})\n```\n\n----------------------------------------\n\nTITLE: Using Pmdarima Model with MLflow PyFunc in Python\nDESCRIPTION: This example demonstrates how to save a Pmdarima model using MLflow, load it as a PyFunc, and generate predictions with confidence intervals.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport pmdarima\nimport mlflow\nimport pandas as pd\n\ndata = pmdarima.datasets.load_airpassengers()\n\nwith mlflow.start_run():\n    model = pmdarima.auto_arima(data, seasonal=True)\n    mlflow.pmdarima.save_model(model, \"/tmp/model.pmd\")\n\nloaded_pyfunc = mlflow.pyfunc.load_model(\"/tmp/model.pmd\")\n\nprediction_conf = pd.DataFrame(\n    [{\"n_periods\": 4, \"return_conf_int\": True, \"alpha\": 0.1}]\n)\n\npredictions = loaded_pyfunc.predict(prediction_conf)\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing MLflow Model Container to ECR\nDESCRIPTION: This MLflow CLI command builds a Docker image for the specified MLflow model and pushes it to Amazon Elastic Container Registry (ECR) for use with SageMaker.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deployment/deploy-model-to-sagemaker/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ mlflow sagemaker build-and-push-container  -m runs:/<run_id>/model\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for MLflow RAG Evaluation\nDESCRIPTION: Imports necessary packages including pandas for data manipulation, Langchain components for RAG system implementation, and MLflow modules for model evaluation and metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain_openai import OpenAI, OpenAIEmbeddings\n\nimport mlflow\nfrom mlflow.deployments import set_deployments_target\nfrom mlflow.metrics.genai import EvaluationExample, faithfulness, relevance\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow UI Server\nDESCRIPTION: Command to start local MLflow UI server for experiment visualization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui --host 127.0.0.1 --port 8090\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for MLflow SHAP Examples\nDESCRIPTION: Command to install the necessary Python packages for running the MLflow SHAP examples, including mlflow, scikit-learn, shap, and matplotlib.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/shap/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow scikit-learn shap matplotlib\n```\n\n----------------------------------------\n\nTITLE: Retrieving Registered Models in Java\nDESCRIPTION: Adds a new method to the Java client for retrieving registered models from the Model Registry. This allows Java applications to programmatically access model information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_8\n\nLANGUAGE: java\nCODE:\n```\ngetRegisteredModel()\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions with MLflow Model in R\nDESCRIPTION: This function, mlflow_predict, performs prediction using a model loaded with mlflow_load_model(). It's designed for package authors to extend support for MLflow models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_34\n\nLANGUAGE: r\nCODE:\n```\nmlflow_predict(model, data, ...)\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow Server with Authentication\nDESCRIPTION: Command to start the MLflow server with authentication enabled using the basic-auth app name.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --app-name basic-auth\n```\n\n----------------------------------------\n\nTITLE: Generating Questions and Answers using OpenAI GPT\nDESCRIPTION: This snippet defines functions to generate questions and answers from text chunks using OpenAI's GPT model. It includes a custom prompt and uses function calling to structure the output.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_raw_response(content):\n    prompt = f\"\"\"Please generate a question asking for the key information in the given paragraph.\n    Also answer the questions using the information in the given paragraph.\n    Please ask the specific question instead of the general question, like\n    'What is the key information in the given paragraph?'.\n    Please generate the answer using as much information as possible.\n    If you are unable to answer it, please generate the answer as 'I don't know.'\n    The answer should be informative and should be more than 3 sentences.\n\n    Paragraph: {content}\n\n    Please call the submit_function function to submit the generated question and answer.\n    \"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    submit_function = {\n        \"name\": \"submit_function\",\n        \"description\": \"Call this function to submit the generated question and answer.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"question\": {\n                    \"type\": \"string\",\n                    \"description\": \"The question asking for the key information in the given paragraph.\",\n                },\n                \"answer\": {\n                    \"type\": \"string\",\n                    \"description\": \"The answer to the question using the information in the given paragraph.\",\n                },\n            },\n            \"required\": [\"question\", \"answer\"],\n        },\n    }\n\n    return cached_openai_ChatCompletion_create(\n        messages=messages,\n        model=\"gpt-4o-mini\",\n        functions=[submit_function],\n        function_call=\"auto\",\n        temperature=0.0,\n        seed=SEED,\n        cache=cache,\n    )\n\n\ndef generate_question_answer(content):\n    if content is None or len(content) == 0:\n        return \"\", \"N/A\"\n\n    response = get_raw_response(content)\n    try:\n        func_args = json.loads(response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])\n        question = func_args[\"question\"]\n        answer = func_args[\"answer\"]\n        return question, answer\n    except Exception as e:\n        return str(e), \"N/A\"\n```\n\n----------------------------------------\n\nTITLE: Importing React Components in Markdown (JSX)\nDESCRIPTION: This code snippet imports necessary React components and custom components used in the markdown file. It includes imports for Link, APILink, CardGroup, and PageCard components.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Link from \"@docusaurus/Link\";\nimport { APILink } from \"@site/src/components/APILink\";\nimport { CardGroup, PageCard } from \"@site/src/components/Card\";\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Function Without Model Logging in MLflow\nDESCRIPTION: Demonstrates how to evaluate a Python function without logging it as an MLflow model. This approach is useful when you want to assess a function's performance without persisting the model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nimport shap\nimport xgboost\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\n\n# Load the UCI Adult Dataset\nX, y = shap.datasets.adult()\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# Fit an XGBoost binary classifier on the training data split\nmodel = xgboost.XGBClassifier().fit(X_train, y_train)\n\n# Build the Evaluation Dataset from the test set\neval_data = X_test\neval_data[\"label\"] = y_test\n\n\n# Define a function that calls the model's predict method\ndef fn(X):\n    return model.predict(X)\n\n\nwith mlflow.start_run() as run:\n    # Evaluate the function without logging the model\n    result = mlflow.evaluate(\n        fn,\n        eval_data,\n        targets=\"label\",\n        model_type=\"classifier\",\n        evaluators=[\"default\"],\n    )\n\nprint(f\"metrics:\\n{result.metrics}\")\nprint(f\"artifacts:\\n{result.artifacts}\")\n```\n\n----------------------------------------\n\nTITLE: Updating an MLflow Model Version\nDESCRIPTION: Definition of the mlflow_update_model_version function for updating properties of a specific model version in the MLflow Model Registry.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_59\n\nLANGUAGE: r\nCODE:\n```\nmlflow_update_model_version(name, version, description, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Databricks Environment Variables\nDESCRIPTION: Configures the environment variables required for connecting to Databricks-hosted LLMs, including the API token and serving endpoint URL.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = \"<YOUR_DATABRICKS_API_TOKEN>\"\nos.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = \"https://YOUR_DATABRICKS_HOST/serving-endpoints/\"\n```\n\n----------------------------------------\n\nTITLE: Testing the Retriever with a Sample Query\nDESCRIPTION: Tests the FAISS retriever with a sample query to confirm it's working correctly and returns relevant documents.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Test the retriever with a query\nretrieved_docs = retriever.get_relevant_documents(\n    \"What is the purpose of the MLflow Model Registry?\"\n)\nlen(retrieved_docs)\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Lissajous PyFunc Model with MLflow\nDESCRIPTION: This snippet shows how to load a previously saved custom PyFunc model using MLflow. It loads the model from a specified path.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/basic-pyfunc.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load our custom model from the local artifact store\nloaded_pyfunc_model = mlflow.pyfunc.load_model(model_path)\n```\n\n----------------------------------------\n\nTITLE: Loading MLflow Models in R\nDESCRIPTION: Function to load an MLflow model from various storage locations including local filesystem, S3, or MLflow registry. Supports loading specific model flavors when multiple are available.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_26\n\nLANGUAGE: r\nCODE:\n```\nmlflow_load_model(model_uri, flavor = NULL, client = mlflow_client())\n```\n\n----------------------------------------\n\nTITLE: Setting Up Caching System and Utilities for LLM Responses\nDESCRIPTION: Implements a caching system to store LLM responses and reduce API costs. Includes utility functions for working with OpenAI's API and langchain embeddings, along with imports for data analysis and visualization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\n\n# For cost-saving, create a cache for the LLM responses\nimport threading\n\n# For data analysis and visualization\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport openai\nimport pandas as pd\n\n# For scraping\nimport requests\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n\nclass Cache:\n    def __init__(self, persist_path, cache_loading_fn):\n        \"\"\"\n        The cache_loading_fn should be a function that takes arbitrary\n        serializable arguments and returns a serilaizable value.\n          value = cache_loading_fn(**kwargs)\n        For example, for openai.chat.completions.create(...), the\n        cache_loading_fn should be:\n          def cache_loading_fn(**kwargs):\n            result = openai.chat.completions.create(**kwargs)\n            return result.to_dict_recursive()\n        \"\"\"\n        self._cache = self._get_or_create_cache_dict(persist_path)\n        self._persist_path = persist_path\n        self._cache_loading_fn = cache_loading_fn\n        self._cache_lock = threading.Lock()\n\n    @classmethod\n    def _get_or_create_cache_dict(cls, persist_path):\n        if os.path.exists(persist_path):\n            # File exists, load it as a JSON string into a dict\n            with open(persist_path) as f:\n                cache = json.load(f)\n        else:\n            # File does not exist, create an empty dict\n            cache = {}\n        return cache\n\n    def _save_to_file(self):\n        with open(self._persist_path, \"w\") as file:\n            json.dump(self._cache, file)\n\n    def _update_cache(self, key, value):\n        with self._cache_lock:\n            self._cache[key] = value\n            self._save_to_file()\n\n    def get_from_cache_or_load_cache(self, **kwargs):\n        key = json.dumps(kwargs)\n\n        with self._cache_lock:\n            value = self._cache.get(key, None)\n\n        if value is None:\n            value = self._cache_loading_fn(**kwargs)\n            self._update_cache(key, value)\n        else:\n            print(\"Loaded from cache\")\n\n        return value\n\n\ndef chat_completion_create_fn(**kwargs):\n    return openai.chat.completions.create(**kwargs)\n\n\ndef cached_openai_ChatCompletion_create(**kwargs):\n    cache = kwargs.pop(\"cache\")\n    return cache.get_from_cache_or_load_cache(**kwargs)\n\n\ndef embeddings_embed_documents_fn(**kwargs):\n    chunk = kwargs.get(\"chunk\")\n    return embeddings.embed_documents([chunk])\n\n\ndef cached_langchain_openai_embeddings(**kwargs):\n    cache = kwargs.pop(\"cache\")\n    return cache.get_from_cache_or_load_cache(**kwargs)\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI\nDESCRIPTION: Sets the global reference to the MLflow Tracking server's address using the fluent API. This enables the use of higher-level MLflow APIs for tracking experiments and runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step6-logging-a-run/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n```\n\n----------------------------------------\n\nTITLE: Listing Experiments from MLflow Server in Python\nDESCRIPTION: This snippet demonstrates how to list experiments from an MLflow server using a GET request. It also shows the error response when the server is running in artifacts-only mode.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/server/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Attempt to list experiments from the server\nresponse = requests.get(\"http://0.0.0.0:8885/api/2.0/mlflow/experiments/list\")\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n>> HTTPError: Endpoint: /api/2.0/mlflow/experiments/list disabled due to the mlflow server running in `--artifacts-only` mode.\n```\n\n----------------------------------------\n\nTITLE: Creating Relevance Metric for RAG Evaluation\nDESCRIPTION: Defines a relevance metric using MLflow's genai module for evaluating the RAG system's outputs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.metrics.genai import EvaluationExample, relevance\n\nrelevance_metric = relevance(model=\"openai:/gpt-4\")\nprint(relevance_metric)\n```\n\n----------------------------------------\n\nTITLE: MLflow Custom Flavor PyFunc Loading\nDESCRIPTION: Implementation for loading models as PyFunc flavor for standardized inference API\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_68\n\nLANGUAGE: python\nCODE:\n```\ndef _load_pyfunc(path):\n    try:\n        sktime_flavor_conf = _get_flavor_configuration(\n            model_path=path, flavor_name=FLAVOR_NAME\n        )\n        serialization_format = sktime_flavor_conf.get(\n            \"serialization_format\", SERIALIZATION_FORMAT_PICKLE\n        )\n    except MlflowException:\n        _logger.warning(\n            \"Could not find sktime flavor configuration during model \"\n            \"loading process. Assuming 'pickle' serialization format.\"\n        )\n        serialization_format = SERIALIZATION_FORMAT_PICKLE\n\n    pyfunc_flavor_conf = _get_flavor_configuration(\n        model_path=path, flavor_name=pyfunc.FLAVOR_NAME\n    )\n    path = os.path.join(path, pyfunc_flavor_conf[\"model_path\"])\n\n    return _SktimeModelWrapper(\n        _load_model(path, serialization_format=serialization_format)\n    )\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Gateway Server with Unity Catalog Integration\nDESCRIPTION: Bash commands to set up environment variables and start the MLflow gateway server with Unity Catalog integration enabled.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/uc_functions/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Required to authenticate with Databricks. See https://docs.databricks.com/en/dev-tools/auth/index.html#supported-authentication-types-by-databricks-tool-or-sdk for other authentication methods.\nexport DATABRICKS_HOST=\"...\"   # e.g. https://my.databricks.com\nexport DATABRICKS_TOKEN=\"...\"\n\n# Required to execute UC functions. See https://docs.databricks.com/en/integrations/compute-details.html#get-connection-details-for-a-databricks-compute-resource for how to get the http path of your warehouse.\n# The last part of the http path is the warehouse ID.\n#\n# /sql/1.0/warehouses/1234567890123456\n#                     ^^^^^^^^^^^^^^^^\nexport DATABRICKS_WAREHOUSE_ID=\"...\"\n\n# Enable Unity Catalog integration\nexport MLFLOW_ENABLE_UC_FUNCTIONS=true\n\nmlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000\n```\n\n----------------------------------------\n\nTITLE: Simulating Iterative Development in MLflow\nDESCRIPTION: A simple code snippet that demonstrates executing another round of hyperparameter tuning, simulating the iterative nature of machine learning model development and parameter optimization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/parent-child-runs.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# What if we need to run this again?\nconsume(starmap(execute_tuning, ((x,) for x in range(5))))\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for MLflow Prompt Evaluation\nDESCRIPTION: Installs MLflow and OpenAI SDK for prompt evaluation. Requires MLflow version 2.21.0 or higher.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/evaluate.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow>=2.21.0 openai -qU\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow Store Entities in reStructuredText\nDESCRIPTION: This snippet uses the automodule directive to generate documentation for the mlflow.store.entities module. It includes all members and undocumented members.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.entities.rst#2025-04-07_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: mlflow.store.entities\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Deleting MLflow Model Version in R\nDESCRIPTION: Function to delete a specific version of a registered MLflow model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_10\n\nLANGUAGE: r\nCODE:\n```\nmlflow_delete_model_version(name, version, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Initializing MLflow Client in R\nDESCRIPTION: Function to create an MLflow client for communication with the tracking server or store.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_5\n\nLANGUAGE: r\nCODE:\n```\nmlflow_client(tracking_uri = NULL)\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Tracking Server\nDESCRIPTION: Command to start the MLflow tracking server on localhost port 8080. This allows users to view and interact with the MLflow UI for managing experiments and models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/step2-explore-registered-model/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --host 127.0.0.1 --port 8080\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow Dependencies\nDESCRIPTION: Command to install MLflow package using pip package manager\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/databricks-trial/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%pip install -q mlflow\n```\n\n----------------------------------------\n\nTITLE: Setting Databricks as Deployment Target for MLflow\nDESCRIPTION: Configures MLflow to use Databricks as the deployment target for serving models, specifically for accessing the Llama2 model as a judge.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nset_deployments_target(\"databricks\")\n```\n\n----------------------------------------\n\nTITLE: Input Example JSON Files\nDESCRIPTION: These JSON snippets show the content of input_example.json and serving_input_example.json files generated when logging a model with an input example.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"question\": \"What is MLflow?\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": {\n    \"question\": \"What is MLflow?\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Input Example JSON Files\nDESCRIPTION: These JSON snippets show the content of input_example.json and serving_input_example.json files generated when logging a model with an input example.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"question\": \"What is MLflow?\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": {\n    \"question\": \"What is MLflow?\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Unity Catalog SQL Function\nDESCRIPTION: SQL command to create a sample addition function in Unity Catalog that takes two integer parameters and returns their sum.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/uc_functions/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE FUNCTION\nmy.uc_func.add (\n  x INTEGER COMMENT 'The first number to add.',\n  y INTEGER COMMENT 'The second number to add.'\n)\nRETURNS INTEGER\nLANGUAGE SQL\nRETURN x + y\n```\n\n----------------------------------------\n\nTITLE: Logging Tensorflow Experiments with Custom Keras Callback\nDESCRIPTION: Shows how to create a custom Keras callback to log specific information to MLflow during model training. This example logs training metrics in log scale at the end of each epoch.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/tensorflow/guide/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorflow import keras\nimport math\nimport mlflow\n\n\nclass MlflowCallback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        for k, v in logs.items():\n            mlflow.log_metric(f\"log_{k}\", math.log(v), step=epoch)\n```\n\n----------------------------------------\n\nTITLE: Searching Experiments Response Structure in MLflow REST API\nDESCRIPTION: JSON structure for the response when searching experiments via the MLflow REST API. It contains an array of experiments and a token for pagination.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"experiments\": [\n    {\n      \"experiment_id\": \"STRING\",\n      \"name\": \"STRING\",\n      \"artifact_location\": \"STRING\",\n      \"lifecycle_stage\": \"STRING\",\n      \"last_update_time\": \"INT64\",\n      \"creation_time\": \"INT64\",\n      \"tags\": [\n        {\n          \"key\": \"STRING\",\n          \"value\": \"STRING\"\n        }\n      ]\n    }\n  ],\n  \"next_page_token\": \"STRING\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Predict Method\nDESCRIPTION: Complete implementation of WeatherModel with OpenAI integration and tool calling functionality\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/notebooks/chat-model-tool-calling.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom openai import OpenAI\n\nimport mlflow\nfrom mlflow.types.llm import (\n    ChatMessage,\n    ChatParams,\n    ChatResponse,\n)\n\n\nclass WeatherModel(mlflow.pyfunc.ChatModel):\n    def __init__(self):\n        weather_tool = FunctionToolDefinition(\n            name=\"get_weather\",\n            description=\"Get weather information\",\n            parameters=ToolParamsSchema(\n                {\n                    \"city\": ParamProperty(\n                        type=\"string\",\n                        description=\"City name to get weather information for\",\n                    ),\n                }\n            ),\n        ).to_tool_definition()\n\n        self.tools = [weather_tool.to_dict()]\n\n    def get_weather(self, city: str) -> str:\n        return \"It's sunny in {}, with a temperature of 20C\".format(city)\n\n    def predict(self, context, messages: list[ChatMessage], params: ChatParams):\n        client = OpenAI()\n\n        messages = [m.to_dict() for m in messages]\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=messages,\n            tools=self.tools,\n        )\n\n        tool_calls = response.choices[0].message.tool_calls\n        if tool_calls:\n            print(\"Received a tool call, calling the weather tool...\")\n\n            city = json.loads(tool_calls[0].function.arguments)[\"city\"]\n            tool_call_id = tool_calls[0].id\n\n            tool_response = ChatMessage(\n                role=\"tool\", content=self.get_weather(city), tool_call_id=tool_call_id\n            ).to_dict()\n\n            messages.append(response.choices[0].message)\n            messages.append(tool_response)\n            response = client.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=messages,\n                tools=self.tools,\n            )\n\n        return ChatResponse.from_dict(response.to_dict())\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow Models Module in Sphinx\nDESCRIPTION: RST directives to generate documentation for the mlflow.models module and ModelInfo class. Includes all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.models.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.models\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. autoclass:: mlflow.models.model.ModelInfo\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Tracking Server\nDESCRIPTION: Command to start local MLflow tracking server for experiment logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --host 127.0.0.1 --port 8080\n```\n\n----------------------------------------\n\nTITLE: Initializing MLflow Client in Python\nDESCRIPTION: This snippet demonstrates how to initialize the MLflow client with a local tracking server. It sets up the client for interacting with MLflow's tracking functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/notebooks/logging-first-model.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\n\nclient = MlflowClient(tracking_uri=\"http://127.0.0.1:8080\")\n```\n\n----------------------------------------\n\nTITLE: Optional Fields in Input Example\nDESCRIPTION: Demonstrates how to specify optional fields in the model signature using multiple input examples with varying field presence.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.models import infer_signature\n\ninput_example = {\n    \"messages\": [\n        {\n            \"name\": \"userA\",\n            \"role\": \"user\",\n            \"content\": {\n                \"question\": \"What is the primary function of control rods in a nuclear reactor?\",\n                \"answer\": \"To stir the primary coolant so that the neutrons are mixed well.\",\n            },\n        },\n        {\n            \"role\": \"user\",\n            \"content\": {\n                \"question\": \"What is MLflow?\",\n                \"answer\": \"MLflow is an open-source platform\",\n            },\n        },\n    ]\n}\n\nprint(infer_signature(input_example))\n```\n\n----------------------------------------\n\nTITLE: Querying RetrievalQA Model about Enjoying Yellowstone\nDESCRIPTION: This snippet demonstrates querying the RetrievalQA model about simply enjoying Yellowstone National Park. It showcases the model's ability to provide practical advice based on the context of the original act.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nanswer6 = loaded_model.predict(\n    [{\"query\": \"Can I just go to the park and peacefully enjoy the natural splendor?\"}]\n)\n\nprint_formatted_response(answer6)\n```\n\n----------------------------------------\n\nTITLE: Sample Function for Code Review with MLflow\nDESCRIPTION: A sample function with intentional inefficiencies and readability issues for demonstrating code review. The function attempts to process a list by identifying unique elements and counting duplicates.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef process_data(lst):\n    s = 0\n    q = []\n    for i in range(len(lst)):\n        a = lst[i]\n        for j in range(i + 1, len(lst)):\n            b = lst[j]\n            if a == b:\n                s += 1\n            else:\n                q.append(b)\n    rslt = [x for x in lst if x not in q]\n    k = []\n    for i in rslt:\n        if i not in k:\n            k.append(i)\n    final_data = sorted(k, reverse=True)\n    return final_data, s\n\n\nreview(process_data, loaded_helper)\n```\n\n----------------------------------------\n\nTITLE: Defining Constants for Custom sktime MLflow Flavor\nDESCRIPTION: This snippet defines important constants used throughout the custom flavor implementation. It includes the flavor name, supported prediction methods, and serialization formats.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nFLAVOR_NAME = \"sktime\"\n\nSKTIME_PREDICT = \"predict\"\nSKTIME_PREDICT_INTERVAL = \"predict_interval\"\nSKTIME_PREDICT_QUANTILES = \"predict_quantiles\"\nSKTIME_PREDICT_VAR = \"predict_var\"\nSUPPORTED_SKTIME_PREDICT_METHODS = [\n    SKTIME_PREDICT,\n    SKTIME_PREDICT_INTERVAL,\n    SKTIME_PREDICT_QUANTILES,\n    SKTIME_PREDICT_VAR,\n]\n\nSERIALIZATION_FORMAT_PICKLE = \"pickle\"\nSERIALIZATION_FORMAT_CLOUDPICKLE = \"cloudpickle\"\nSUPPORTED_SERIALIZATION_FORMATS = [\n    SERIALIZATION_FORMAT_PICKLE,\n    SERIALIZATION_FORMAT_CLOUDPICKLE,\n]\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow via pip\nDESCRIPTION: Command to install MLflow package from PyPI using pip. This is the first step in setting up MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/logging-first-model/step1-tracking-server/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Training Logistic Regression Model on Iris Dataset\nDESCRIPTION: Python code to load the Iris dataset, split it into train and test sets, train a Logistic Regression model, and calculate accuracy.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom mlflow.models import infer_signature\n\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\n# Load the Iris dataset\nX, y = datasets.load_iris(return_X_y=True)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Define the model hyperparameters\nparams = {\n    \"solver\": \"lbfgs\",\n    \"max_iter\": 1000,\n    \"multi_class\": \"auto\",\n    \"random_state\": 8888,\n}\n\n# Train the model\nlr = LogisticRegression(**params)\nlr.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = lr.predict(X_test)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Importing Link Component in JSX\nDESCRIPTION: This snippet imports the Link component from Docusaurus, which is likely used for navigation within the tutorial documentation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Link from \"@docusaurus/Link\";\n```\n\n----------------------------------------\n\nTITLE: Searching Parameters in MLflow using SQL\nDESCRIPTION: Examples of filtering MLflow runs by parameters. Parameters are stored as strings and require string comparators.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nparams.batch_size = \"2\"\nparams.model LIKE \"GPT%\"\nparams.model ILIKE \"gPt%\"\nparams.model LIKE \"GPT%\" AND params.batch_size = \"2\"\n```\n\n----------------------------------------\n\nTITLE: Querying RetrievalQA Model about Leasing Land in Yellowstone\nDESCRIPTION: This snippet shows how to query the RetrievalQA model about leasing land in Yellowstone for a buffalo-themed day spa. It demonstrates the model's consistency in providing factual responses to imaginative questions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/notebooks/langchain-retriever.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nanswer4 = loaded_model.predict(\n    [\n        {\n            \"query\": \"Can I lease a small parcel of land from the Federal Government for a small \"\n            \"buffalo-themed day spa for visitors to the park?\"\n        }\n    ]\n)\n\nprint_formatted_response(answer4)\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment and Disabling Warnings for Transformers in Python\nDESCRIPTION: Sets up environment variables and disables warnings to prepare for working with Transformers pipelines. The code sets TOKENIZERS_PARALLELISM to false to disable tokenizers warnings and filters out UserWarnings from setuptools and pydantic.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/prompt-templating/prompt-templating.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Disable tokenizers warnings when constructing pipelines\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Generating QQ Plots for Residual Normality Assessment in Python\nDESCRIPTION: This function creates a Quantile-Quantile (QQ) plot to assess whether the residuals from model predictions follow a normal distribution. It uses SciPy's probplot to compare the quantiles of residuals against those of a normal distribution, which is essential for validating regression model assumptions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/logging-plots-in-mlflow.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef plot_qq(y_test, y_pred, style=\"seaborn\", plot_size=(10, 8)):\n    residuals = y_test - y_pred\n    with plt.style.context(style=style):\n        fig, ax = plt.subplots(figsize=plot_size)\n        stats.probplot(residuals, dist=\"norm\", plot=ax)\n        ax.set_title(\"QQ Plot\", fontsize=14)\n        plt.tight_layout()\n    plt.close(fig)\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Observability with MLflow Tracing\nDESCRIPTION: This code demonstrates MLflow's tracing capabilities for LLM observability. It enables automatic tracing for OpenAI and shows a basic interaction with the OpenAI API. The traces can be viewed in the MLflow UI to monitor and analyze LLM queries and responses.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/skinny/README.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hi!\"}],\n    temperature=0.1,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Question Generation\nDESCRIPTION: Installs necessary Python packages for document retrieval, language models, data analysis, and visualization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install beautifulsoup4 langchain openai pandas seaborn scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Documentation Development Server\nDESCRIPTION: Command to start local development server that provides live preview of documentation changes\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn start\n```\n\n----------------------------------------\n\nTITLE: Querying an MLflow Model Server with curl\nDESCRIPTION: Example of using curl to send prediction requests to a deployed MLflow model server. The request sends a JSON-formatted DataFrame with input data for prediction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/quickstart_drilldown/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -d '{\"dataframe_split\": {\"columns\": [\"x\"], \"data\": [[1], [-1]]}}' -H 'Content-Type: application/json' -X POST localhost:5000/invocations\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for MLflow Custom Prediction\nDESCRIPTION: Import statements for scikit-learn, MLflow, and joblib dependencies needed for model training and customization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom joblib import dump\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom mlflow.pyfunc import PythonModel\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow AI Gateway Server\nDESCRIPTION: Command to start the gateway server with configuration file, port, host and worker count specifications\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nmlflow gateway start --config-path config.yaml --port {port} --host {host} --workers {worker count}\n```\n\n----------------------------------------\n\nTITLE: Rendering Card Group with Page Card (JSX)\nDESCRIPTION: This code snippet demonstrates the use of CardGroup and PageCard components to create a clickable card for the OpenAI Quickstart tutorial. It includes the header text, link, and description for the tutorial.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <PageCard\n    headerText=\"OpenAI Quickstart\"\n    link=\"/llms/openai/notebooks/openai-quickstart/\"\n    text=\"Learn the very basics of using the OpenAI package with MLflow with some simple prompt engineering and a fun use case to get started with this powerful integration.\"\n  />\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Registering a Prompt in MLflow Registry\nDESCRIPTION: Creates and registers a summarization prompt in the MLflow Prompt Registry. The template includes variables for the number of sentences and the text to summarize.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/prompts/evaluate.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Use double curly braces for variables in the template\ninitial_template = \"\"\"\\\nSummarize content you are provided with in {{ num_sentences }} sentences.\n\nSentences: {{ sentences }}\n\"\"\"\n\n# Register a new prompt\nprompt = mlflow.register_prompt(\n    name=\"summarization-prompt\",\n    template=initial_template,\n    # Optional: Provide a commit message to describe the changes\n    commit_message=\"Initial commit\",\n)\n\n# The prompt object contains information about the registered prompt\nprint(f\"Created prompt '{prompt.name}' (version {prompt.version})\")\n```\n\n----------------------------------------\n\nTITLE: Simple Regression Model with MLflow Autologging\nDESCRIPTION: Example code showing how to train a simple regression model with scikit-learn while enabling MLflow's autologging feature for experiment tracking.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/skinny/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Example with Local Environment\nDESCRIPTION: Command to run the MLflow example using the local environment manager instead of creating a conda environment. This is useful when you already have all required dependencies installed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/CaptumExample/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . --env-manager=local\n```\n\n----------------------------------------\n\nTITLE: Importing APILink Component in JSX\nDESCRIPTION: Imports the APILink component from the site's components directory. This component is used to create links to the MLflow API documentation within the page.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/contribute.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { APILink } from \"@site/src/components/APILink\";\n```\n\n----------------------------------------\n\nTITLE: Set Model Version Tag Endpoint\nDESCRIPTION: API endpoint documentation for setting tags on specific model versions, including request parameters for model name, version, tag key and value.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_27\n\nLANGUAGE: rest\nCODE:\n```\n+---------------------------------------+-------------+\n|               Endpoint                | HTTP Method |\n+=======================================+=============+\n| ``2.0/mlflow/model-versions/set-tag`` | ``POST``    |\n+---------------------------------------+-------------+\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI\nDESCRIPTION: Configure MLflow to use local tracking server.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/creating-custom-pyfunc/notebooks/override-predict.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_tracking_uri(\"http://localhost:8080\")\n```\n\n----------------------------------------\n\nTITLE: Importing Card Components in React/JSX\nDESCRIPTION: Imports CardGroup and PageCard components from the site's component library for displaying tutorial navigation cards.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { CardGroup, PageCard } from \"@site/src/components/Card\";\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Custom sktime MLflow Flavor\nDESCRIPTION: This snippet imports necessary modules for creating a custom MLflow flavor for sktime. It includes imports from sktime, MLflow, and other utility modules.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport os\nimport pickle\n\nimport flavor\nimport mlflow\nimport numpy as np\nimport pandas as pd\nimport sktime\nimport yaml\nfrom mlflow import pyfunc\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.models import Model\nfrom mlflow.models.model import MLMODEL_FILE_NAME\nfrom mlflow.models.utils import _save_example\nfrom mlflow.protos.databricks_pb2 import INTERNAL_ERROR, INVALID_PARAMETER_VALUE\nfrom mlflow.tracking._model_registry import DEFAULT_AWAIT_MAX_SLEEP_SECONDS\nfrom mlflow.tracking.artifact_utils import _download_artifact_from_uri\nfrom mlflow.utils.environment import (\n    _CONDA_ENV_FILE_NAME,\n    _CONSTRAINTS_FILE_NAME,\n    _PYTHON_ENV_FILE_NAME,\n    _REQUIREMENTS_FILE_NAME,\n    _mlflow_conda_env,\n    _process_conda_env,\n    _process_pip_requirements,\n    _PythonEnv,\n    _validate_env_arguments,\n)\nfrom mlflow.utils.file_utils import write_to\nfrom mlflow.utils.model_utils import (\n    _add_code_from_conf_to_system_path,\n    _get_flavor_configuration,\n    _validate_and_copy_code_paths,\n    _validate_and_prepare_target_save_path,\n)\nfrom mlflow.utils.requirements_utils import _get_pinned_requirement\nfrom sktime.utils.multiindex import flatten_multiindex\n\n_logger = logging.getLogger(__name__)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for PyTorch and MLflow Integration\nDESCRIPTION: Imports necessary PyTorch, torchmetrics, torchinfo, and MLflow libraries for model training and experiment tracking.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nfrom torchmetrics import Accuracy\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nimport mlflow\n```\n\n----------------------------------------\n\nTITLE: Scraping MLflow Documentation from Website\nDESCRIPTION: Scrapes MLflow documentation from the official website by extracting content from HTML pages. The code navigates through links on the index page and stores the text content of each page along with its source URL.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npage = requests.get(\"https://mlflow.org/docs/latest/index.html\")\nsoup = BeautifulSoup(page.content, \"html.parser\")\n\nmainLocation = \"https://mlflow.org/docs/latest/\"\nheader = {\n    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n    \"Accept-Language\": \"en-US,en;q=0.8\",\n    \"Connection\": \"keep-alive\",\n}\n\ndata = []\nfor a_link in soup.find_all(\"a\"):\n    document_url = mainLocation + a_link[\"href\"]\n    page = requests.get(document_url, headers=header)\n    soup = BeautifulSoup(page.content, \"html.parser\")\n    file_to_store = a_link.get(\"href\")\n    if soup.find(\"div\", {\"class\": \"rst-content\"}):\n        data.append(\n            [\n                file_to_store,\n                soup.find(\"div\", {\"class\": \"rst-content\"}).text.replace(\"\\n\", \" \"),\n            ]\n        )\n\ndf = pd.DataFrame(data, columns=[\"source\", \"text\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Evaluation Function with MLflow Logging\nDESCRIPTION: Defines the evaluation function that assesses model performance on the test dataset, calculating and logging average loss and accuracy metrics to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(dataloader, model, loss_fn, metrics_fn, epoch):\n    \"\"\"Evaluate the model on a single pass of the dataloader.\n\n    Args:\n        dataloader: an instance of `torch.utils.data.DataLoader`, containing the eval data.\n        model: an instance of `torch.nn.Module`, the model to be trained.\n        loss_fn: a callable, the loss function.\n        metrics_fn: a callable, the metrics function.\n        epoch: an integer, the current epoch number.\n    \"\"\"\n    num_batches = len(dataloader)\n    model.eval()\n    eval_loss, eval_accuracy = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            eval_loss += loss_fn(pred, y).item()\n            eval_accuracy += metrics_fn(pred, y)\n\n    eval_loss /= num_batches\n    eval_accuracy /= num_batches\n    mlflow.log_metric(\"eval_loss\", f\"{eval_loss:2f}\", step=epoch)\n    mlflow.log_metric(\"eval_accuracy\", f\"{eval_accuracy:2f}\", step=epoch)\n\n    print(f\"Eval metrics: \\nAccuracy: {eval_accuracy:.2f}, Avg loss: {eval_loss:2f} \\n\")\n```\n\n----------------------------------------\n\nTITLE: Creating MLflow Experiment for Whisper Transcription in Python\nDESCRIPTION: This snippet sets up a new MLflow experiment specifically for the Whisper Transcription ASR model, ensuring organized tracking of the model runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n\nmlflow.set_experiment(\"Whisper Transcription ASR\")\n```\n\n----------------------------------------\n\nTITLE: Testing Offline Prediction with MLflow Models API (Python)\nDESCRIPTION: Shows how to validate a model before deployment by using the MLflow models predict API in Python to test offline predictions in a virtual environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.models.predict(\n    model_uri=\"runs:/<run_id>/model\",\n    input_data=\"<input_data>\",\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing MLflow Trace Decorator\nDESCRIPTION: Demonstration of customizing the MLflow trace decorator with a custom span name, attributes, and span type for enhanced tracing capabilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/chat-model-guide/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@mlflow.trace(name=\"custom_span_name\", attributes={\"key\": \"value\"}, span_type=\"func\")\ndef _get_system_message(self, role: str) -> Dict:\n    if role not in self.models:\n        raise ValueError(f\"Unknown role: {role}\")\n\n    instruction = self.models[role][\"instruction\"]\n    return ChatMessage(role=\"system\", content=instruction).to_dict()\n```\n\n----------------------------------------\n\nTITLE: Scoring Images via Spark\nDESCRIPTION: Command to perform batch scoring of images using Spark through the score_images_spark.py script.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/flower_classifier/README.rst#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython score_images_spark.py --model-uri runs:/101/model /path/to/images/for/scoring\n```\n\n----------------------------------------\n\nTITLE: Validating a Model Against a Baseline with Metric Thresholds in MLflow\nDESCRIPTION: Demonstrates how to validate a candidate model against a baseline using predefined metric thresholds. This example shows evaluation of both models and compares the candidate model's performance against specific accuracy criteria.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost\nimport shap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nimport mlflow\nfrom mlflow.models import MetricThreshold\n\n# load UCI Adult Data Set; segment it into training and test sets\nX, y = shap.datasets.adult()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# construct an evaluation dataset from the test set\neval_data = X_test\neval_data[\"label\"] = y_test\n\n# Log and evaluate the candidate model\ncandidate_model = xgboost.XGBClassifier().fit(X_train, y_train)\n\nwith mlflow.start_run(run_name=\"candidate\") as run:\n    candidate_model_uri = mlflow.sklearn.log_model(\n        candidate_model, \"candidate_model\", signature=signature\n    ).model_uri\n\n    candidate_result = mlflow.evaluate(\n        candidate_model_uri,\n        eval_data,\n        targets=\"label\",\n        model_type=\"classifier\",\n    )\n\n# Log and evaluate the baseline model\nbaseline_model = DummyClassifier(strategy=\"uniform\").fit(X_train, y_train)\n\nwith mlflow.start_run(run_name=\"baseline\") as run:\n    baseline_model_uri = mlflow.sklearn.log_model(\n        baseline_model, \"baseline_model\", signature=signature\n    ).model_uri\n\n    baseline_result = mlflow.evaluate(\n        baseline_model_uri,\n        eval_data,\n        targets=\"label\",\n        model_type=\"classifier\",\n    )\n\n# Define criteria for model to be validated against\nthresholds = {\n    \"accuracy_score\": MetricThreshold(\n        threshold=0.8,  # accuracy should be >=0.8\n        min_absolute_change=0.05,  # accuracy should be at least 0.05 greater than baseline model accuracy\n        min_relative_change=0.05,  # accuracy should be at least 5 percent greater than baseline model accuracy\n        greater_is_better=True,\n    ),\n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Hit and Miss Questions in Retriever Evaluation (Python)\nDESCRIPTION: This code filters the evaluation results to separate hit and miss questions based on precision at 1, then performs topical analysis on both sets of questions using the previously defined topical_analysis function.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df = eval_results_table[eval_results_table[\"precision_at_1/score\"] == 1]\nhit_questions = filtered_df[\"question\"].tolist()\nfiltered_df = eval_results_table[eval_results_table[\"precision_at_1/score\"] == 0]\nmiss_questions = filtered_df[\"question\"].tolist()\n\nlda_model, corpus, dictionary = topical_analysis(hit_questions)\nvis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n\n# Uncomment the following line to render the interactive widget\n# pyLDAvis.display(vis_data)\n\nlda_model, corpus, dictionary = topical_analysis(miss_questions)\nvis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n\n# Uncomment the following line to render the interactive widget\n# pyLDAvis.display(vis_data)\n```\n\n----------------------------------------\n\nTITLE: Loading and Inspecting Model Components\nDESCRIPTION: Shows how to load saved model components from MLflow and inspect their types. Returns components as a dictionary containing the model, tokenizer, and other elements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntranslation_components = mlflow.transformers.load_model(\n    model_info.model_uri, return_type=\"components\")\n\nfor key, value in translation_components.items():\n    print(f\"{key} -> {type(value).__name__}\")\n```\n\n----------------------------------------\n\nTITLE: Serving Trained Model with MLflow CLI Command\nDESCRIPTION: This shell command demonstrates how to serve the trained model using MLflow's CLI. It specifies the artifact URI and port for serving the model, which can then be used for making predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/notebooks/rapids_mlflow.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmlflow models serve -m [artifact_uri] -p [port]\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow LangChain Autologging\nDESCRIPTION: Activates automatic logging for LangChain components in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmlflow.langchain.autolog()\n```\n\n----------------------------------------\n\nTITLE: Creating Parent-Child Runs for Hyperparameter Tuning in MLflow\nDESCRIPTION: This code demonstrates how to create nested runs in MLflow, which is useful for organizing hyperparameter tuning experiments. It creates a parent run with multiple child runs for different parameter settings.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Start parent run\nwith mlflow.start_run() as parent_run:\n    param = [0.01, 0.02, 0.03]\n\n    # Create a child run for each parameter setting\n    for p in param:\n        with mlflow.start_run(nested=True) as child_run:\n            mlflow.log_param(\"p\", p)\n            ...\n            mlflow.log_metric(\"val_loss\", val_loss)\n```\n\n----------------------------------------\n\nTITLE: Configuring Warning Settings\nDESCRIPTION: Disabling FutureWarnings to reduce noise in output\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/huggingface-evaluation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Low Cosine Similarity Scores in Python\nDESCRIPTION: This code filters and prints details of question-chunk pairs with low cosine similarity scores (below 0.75). It helps identify less relevant or informative chunks for further analysis or filtering.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question-generation-retrieval-evaluation.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmask = embedded_queries[\"cossim\"] < 0.75\nlower_cossim = embedded_queries[mask]\nfor i, row in lower_cossim.iterrows():\n    print(f\"Question: {i}\")\n    print(row[\"question\"])\n    print(\"Chunk:\")\n    print(row[\"chunk\"])\n    print(\"cossim:\")\n    print(row[\"cossim\"])\n```\n\n----------------------------------------\n\nTITLE: Testing Base Model Performance\nDESCRIPTION: Sets up a pipeline to test the base model's performance on SQL generation tasks before fine-tuning. Uses the Transformers pipeline API with the loaded model and tokenizer.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\npipeline = transformers.pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n\nsample = test_dataset[1]\nprompt = PROMPT_TEMPLATE.format(\n    context=sample[\"context\"], question=sample[\"question\"], output=\"\"  \n)\n\nwith torch.no_grad():\n    response = pipeline(prompt, max_new_tokens=256, repetition_penalty=1.15, return_full_text=False)\n\ndisplay_table({\"prompt\": prompt, \"generated_query\": response[0][\"generated_text\"]})\n```\n\n----------------------------------------\n\nTITLE: Rename Registered Model Endpoint\nDESCRIPTION: REST endpoint for renaming a registered model. Takes the current model name and new name as parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_13\n\nLANGUAGE: rest\nCODE:\n```\nPOST 2.0/mlflow/registered-models/rename\n```\n\n----------------------------------------\n\nTITLE: Inferring Signature for List of Datetimes in Python\nDESCRIPTION: Illustrates how MLflow infers signatures for datetime data, using numpy's datetime64 type to represent timestamps.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/notebooks/signature_examples.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# List of Datetimes\nreport_signature_info([np.datetime64(\"2023-12-24 11:59:59\"), np.datetime64(\"2023-12-25 00:00:00\")])\n```\n\n----------------------------------------\n\nTITLE: Using LangChain with MLflow Tracing in Python\nDESCRIPTION: This example shows how to use LangChain with MLflow tracing enabled, including setting up an experiment, creating a ChatOpenAI model, and invoking a chain.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/langchain.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport os\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\n\n# Enabling autolog for LangChain will enable trace logging.\nmlflow.langchain.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_experiment(\"LangChain\")\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=1000)\n\nprompt_template = PromptTemplate.from_template(\n    \"Answer the question as if you are {person}, fully embodying their style, wit, personality, and habits of speech. \"\n    \"Emulate their quirks and mannerisms to the best of your ability, embracing their traits—even if they aren't entirely \"\n    \"constructive or inoffensive. The question is: {question}\"\n)\n\nchain = prompt_template | llm | StrOutputParser()\n\n# Let's test another call\nchain.invoke(\n    {\n        \"person\": \"Linus Torvalds\",\n        \"question\": \"Can I just set everyone's access to sudo to make things easier?\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Using LangChain with MLflow Tracing in Python\nDESCRIPTION: This example shows how to use LangChain with MLflow tracing enabled, including setting up an experiment, creating a ChatOpenAI model, and invoking a chain.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/langchain.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport os\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\n\n# Enabling autolog for LangChain will enable trace logging.\nmlflow.langchain.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_experiment(\"LangChain\")\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=1000)\n\nprompt_template = PromptTemplate.from_template(\n    \"Answer the question as if you are {person}, fully embodying their style, wit, personality, and habits of speech. \"\n    \"Emulate their quirks and mannerisms to the best of your ability, embracing their traits—even if they aren't entirely \"\n    \"constructive or inoffensive. The question is: {question}\"\n)\n\nchain = prompt_template | llm | StrOutputParser()\n\n# Let's test another call\nchain.invoke(\n    {\n        \"person\": \"Linus Torvalds\",\n        \"question\": \"Can I just set everyone's access to sudo to make things easier?\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local-to-Databricks Sync for MLflow Recipes Development\nDESCRIPTION: Shell commands demonstrating how to set up a local-to-Databricks synchronization workflow using the Databricks CLI and dbx tools, allowing developers to edit files locally while automatically syncing to Databricks Repos.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/recipes/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# Install the Databricks CLI, which is used to remotely access your Databricks Workspace\npip install databricks-cli\n# Configure remote access to your Databricks Workspace\ndatabricks configure\n# Install dbx, which is used to automatically sync changes to and from Databricks Repos\npip install dbx\n# Clone the MLflow Recipes Regression Template\ngit clone https://github.com/mlflow/recipes-regression-template\n# Enter the MLflow Recipes Regression Template directory and configure dbx within it\ncd recipes-regression-template\ndbx configure\n# Use dbx to enable syncing from the repository directory to Databricks Repos\ndbx sync repo -d recipes-regression-template\n# Iteratively make changes to files in the repository directory and observe that they\n# are automatically synced to Databricks Repos\n```\n\n----------------------------------------\n\nTITLE: Compiling DSPy Text Classifier with Optimization\nDESCRIPTION: Uses BootstrapFewShotWithRandomSearch to compile and optimize the DSPy text classifier based on training data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/dspy/notebooks/dspy_quickstart.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\n\ndef validate_classification(example, prediction, trace=None) -> bool:\n    return example.label == prediction.label\n\n\noptimizer = BootstrapFewShotWithRandomSearch(\n    metric=validate_classification,\n    num_candidate_programs=5,\n    max_bootstrapped_demos=2,\n    num_threads=1,\n)\n\ncompiled_pe = optimizer.compile(copy(TextClassifier()), trainset=train_dataset)\n```\n\n----------------------------------------\n\nTITLE: Deleting MLflow Registered Model in R\nDESCRIPTION: Function to delete an existing registered model by name from MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_11\n\nLANGUAGE: r\nCODE:\n```\nmlflow_delete_registered_model(name, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature-Target Correlations in Python with Matplotlib and Seaborn\nDESCRIPTION: Creates a horizontal bar plot showing correlations between features and the target variable 'demand'. The function uses Seaborn's diverging color palette and includes options for customizing the visualization and saving the output.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef plot_correlation_with_demand(df, save_path=None):\n    \"\"\"\n    Plots the correlation of each variable in the dataframe with the 'demand' column.\n\n    Args:\n    - df (pd.DataFrame): DataFrame containing the data, including a 'demand' column.\n    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n\n    Returns:\n    - None (Displays the plot on a Jupyter window)\n    \"\"\"\n\n    # Compute correlations between all variables and 'demand'\n    correlations = df.corr()[\"demand\"].drop(\"demand\").sort_values()\n\n    # Generate a color palette from red to green\n    colors = sns.diverging_palette(10, 130, as_cmap=True)\n    color_mapped = correlations.map(colors)\n\n    # Set Seaborn style\n    sns.set_style(\n        \"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5}\n    )\n\n    # Create bar plot\n    fig = plt.figure(figsize=(12, 8))\n    plt.barh(correlations.index, correlations.values, color=color_mapped)\n\n    # Set labels and title with increased font size\n    plt.title(\"Correlation with Demand\", fontsize=18)\n    plt.xlabel(\"Correlation Coefficient\", fontsize=16)\n    plt.ylabel(\"Variable\", fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.grid(axis=\"x\")\n\n    plt.tight_layout()\n\n    # Save the plot if save_path is specified\n    if save_path:\n        plt.savefig(save_path, format=\"png\", dpi=600)\n\n    # prevent matplotlib from displaying the chart every time we call this function\n    plt.close(fig)\n\n    return fig\n\n\n# Test the function\ncorrelation_plot = plot_correlation_with_demand(df, save_path=\"correlation_plot.png\")\n```\n\n----------------------------------------\n\nTITLE: Importing Card Components in JSX for Tutorial Navigation\nDESCRIPTION: Imports the CardGroup and PageCard components from the site's component library to create a navigation interface for MLflow RAG tutorials.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { CardGroup, PageCard } from \"@site/src/components/Card\";\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Tool\nDESCRIPTION: Adding weather information retrieval functionality to the WeatherModel class\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/notebooks/chat-model-tool-calling.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass WeatherModel(mlflow.pyfunc.ChatModel):\n    def __init__(self):\n        weather_tool = FunctionToolDefinition(\n            name=\"get_weather\",\n            description=\"Get weather information\",\n            parameters=ToolParamsSchema(\n                {\n                    \"city\": ParamProperty(\n                        type=\"string\",\n                        description=\"City name to get weather information for\",\n                    ),\n                }\n            ),\n        ).to_tool_definition()\n\n        self.tools = [weather_tool.to_dict()]\n\n        def get_weather(self, city: str) -> str:\n            return f\"It's sunny in {city}, with a temperature of 20C\"\n```\n\n----------------------------------------\n\nTITLE: Saving Custom Model with Dependencies in MLflow\nDESCRIPTION: Example showing how to save a custom MLflow model with utility dependencies using code_paths parameter. Demonstrates basic project structure and logging of custom code dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        from utils import my_func\n\n        x = my_func(model_input)\n        # .. your prediction logic\n        return prediction\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        artifact_path=\"model\",\n        input_example=input_data,\n        code_paths=[\"utils.py\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Saving Custom Model with Dependencies in MLflow\nDESCRIPTION: Example showing how to save a custom MLflow model with utility dependencies using code_paths parameter. Demonstrates basic project structure and logging of custom code dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nclass MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        from utils import my_func\n\n        x = my_func(model_input)\n        # .. your prediction logic\n        return prediction\n\n# Log the model\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        python_model=MyModel(),\n        artifact_path=\"model\",\n        input_example=input_data,\n        code_paths=[\"utils.py\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Auto-tracing for Mistral AI in Python\nDESCRIPTION: This code snippet demonstrates how to enable MLflow auto-tracing for Mistral AI, configure the API key, and use the chat completion method. It includes setting up the Mistral client, sending a chat message, and printing the response.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/mistral.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom mistralai import Mistral\n\nimport mlflow\n\n# Turn on auto tracing for Mistral AI by calling mlflow.mistral.autolog()\nmlflow.mistral.autolog()\n\n# Configure your API key.\nclient = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n\n# Use the chat complete method to create new chat.\nchat_response = client.chat.complete(\n    model=\"mistral-small-latest\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n        },\n    ],\n)\nprint(chat_response.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Auto-tracing for Mistral AI in Python\nDESCRIPTION: This code snippet demonstrates how to enable MLflow auto-tracing for Mistral AI, configure the API key, and use the chat completion method. It includes setting up the Mistral client, sending a chat message, and printing the response.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/mistral.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom mistralai import Mistral\n\nimport mlflow\n\n# Turn on auto tracing for Mistral AI by calling mlflow.mistral.autolog()\nmlflow.mistral.autolog()\n\n# Configure your API key.\nclient = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n\n# Use the chat complete method to create new chat.\nchat_response = client.chat.complete(\n    model=\"mistral-small-latest\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n        },\n    ],\n)\nprint(chat_response.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Setting Experiment Tags in MLflow with R\nDESCRIPTION: Function for setting metadata tags on MLflow experiments. Tags provide a way to add custom metadata to experiments that can be updated over time.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_48\n\nLANGUAGE: r\nCODE:\n```\nmlflow_set_experiment_tag(key, value, experiment_id = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for MLflow Agent\nDESCRIPTION: Installs necessary Python packages including MLflow, UnityCatalog-langchain, and LangChain OpenAI for building the agent.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_agent_with_uc_tools.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow==2.20.0 'unitycatalog-langchain[databricks]==0.1.1' langchain_openai==0.3.7\n```\n\n----------------------------------------\n\nTITLE: Querying Ollama LLM using OpenAI Client\nDESCRIPTION: This Python code demonstrates how to set up the OpenAI client to query the local Ollama endpoint. It sends a chat completion request to the LLM model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/ollama.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",  # The local Ollama REST endpoint\n    api_key=\"dummy\",  # Required to instantiate OpenAI client, it can be a random string\n)\n\nresponse = client.chat.completions.create(\n    model=\"llama3.2:1b\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a science teacher.\"},\n        {\"role\": \"user\", \"content\": \"Why is the sky blue?\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Model Registry Tag Management APIs\nDESCRIPTION: Example showing APIs for managing tags on model versions and registered models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\nclient.create_registered_model()\nclient.create_model_version()\nclient.set_registered_model_tag()\nclient.set_model_version_tag()\nclient.delete_registered_model_tag()\nclient.delete_model_version_tag()\n```\n\n----------------------------------------\n\nTITLE: Creating Tutorial Link Button in JSX\nDESCRIPTION: This code creates a button-styled link to the first step of the tutorial using the Docusaurus Link component. It directs users to start the tutorial.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/registering-first-model/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<Link className=\"button button--primary\" to=\"step1-register-model\">\n  <span>View the tutorial</span>\n</Link>\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenAI Autologging in MLflow\nDESCRIPTION: Activates automatic logging for OpenAI SDK calls in MLflow, which captures API calls and responses for monitoring and tracking purposes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/notebooks/chat-model-tool-calling.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmlflow.openai.autolog()\n```\n\n----------------------------------------\n\nTITLE: Skipping TLS Verification for S3 Endpoint in MLflow\nDESCRIPTION: Shows how to enable skipping TLS verification of S3 endpoint using an environment variable in MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_38\n\nLANGUAGE: Shell\nCODE:\n```\nexport MLFLOW_S3_IGNORE_TLS=true\n```\n\n----------------------------------------\n\nTITLE: Logging MPT Model with MLflow\nDESCRIPTION: Sets up model signature, input example, and logs the MPT model with MLflow. Defines input/output schemas and required dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types import DataType, Schema, ColSpec\n\n# Define input and output schema\ninput_schema = Schema(\n    [\n        ColSpec(DataType.string, \"prompt\"),\n        ColSpec(DataType.double, \"temperature\"),\n        ColSpec(DataType.long, \"max_tokens\"),\n    ]\n)\noutput_schema = Schema([ColSpec(DataType.string, \"candidates\")])\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema)\n\n\n# Define input example\ninput_example = pd.DataFrame(\n    {\"prompt\": [\"What is machine learning?\"], \"temperature\": [0.5], \"max_tokens\": [100]}\n)\n\nwith mlflow.start_run():\n    mlflow.pyfunc.log_model(\n        \"mpt-7b-instruct\",\n        python_model=MPT(),\n        artifacts={\"snapshot\": snapshot_location},\n        pip_requirements=[\n            \"torch\",\n            \"transformers\",\n            \"accelerate\",\n            \"einops\",\n            \"sentencepiece\",\n        ],\n        input_example=input_example,\n        signature=signature,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Endpoint for MLflow Tracing in Python\nDESCRIPTION: This snippet demonstrates how to set up MLflow to export traces to an OpenTelemetry Collector by configuring environment variables and creating a trace span. It sets the OTLP endpoint and optionally the service name before starting a trace.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/production.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport os\n\n# Set the endpoint of the OpenTelemetry Collector\nos.environ[\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\"] = \"http://localhost:4317/v1/traces\"\n# Optionally, set the service name to group traces\nos.environ[\"OTEL_SERVICE_NAME\"] = \"<your-service-name>\"\n\n# Trace will be exported to the OTel collector at http://localhost:4317/v1/traces\nwith mlflow.start_span(name=\"foo\") as span:\n    span.set_inputs({\"a\": 1})\n    span.set_outputs({\"b\": 2})\n```\n\n----------------------------------------\n\nTITLE: MLflow REST API Script Usage Options\nDESCRIPTION: Command-line usage options for the MLflow REST API example script, including hostname, port, and experiment ID configuration\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rest_api/README.rst#2025-04-07_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nusage: mlflow_tracking_rest_api.py [-h] [--hostname HOSTNAME] [--port PORT]\n                                   [--experiment-id EXPERIMENT_ID]\n\nMLflow REST API Example\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --hostname HOSTNAME   MLflow server hostname/ip (default: localhost)\n  --port PORT          MLflow server port number (default: 5000)\n  --experiment-id EXPERIMENT_ID\n                          Experiment ID (default: 0)\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Model Docker Container\nDESCRIPTION: Docker command to run the containerized MLflow model, mapping container port 8080 to local port 5002.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 5002:8080 qs_mlops\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI for Experiment Management\nDESCRIPTION: Configures the MLflow tracking URI to connect to a local or remote tracking server. This commented code shows how to set up the connection point for MLflow experiment tracking.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/fine-tuning/transformers-fine-tuning.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# If you are running this tutorial in local mode, leave the next line commented out.\n# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n\n# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n```\n\n----------------------------------------\n\nTITLE: MLflow AI Gateway Configuration\nDESCRIPTION: YAML configuration file defining gateway endpoints for OpenAI completions, chat and embeddings with rate limiting and API key settings\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n    limit:\n      renewal_period: minute\n      calls: 10\n\n  - name: chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: embeddings\n    endpoint_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_key: $OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Searching MLflow Runs with Filters in R\nDESCRIPTION: Demonstrates how to use MLflow's search_runs function in R with filter conditions. In R, filter conditions must be string wrapped and right-hand side conditional elements must be wrapped in single quotes for parameters, attributes, and tags.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/search-runs/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: r\nCODE:\n```\nlibrary(mlflow)\nmlflow_search_runs(\n  filter = \"metrics.rmse < 0.9 and tags.production = 'true'\",\n  experiment_ids = as.character(1:2),\n  order_by = \"params.lr DESC\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow with LangChain Support\nDESCRIPTION: Command to install MLflow with LangChain support, ensuring compatibility with LangChain versions between 0.1.0 and 0.2.3.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/autologging.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow[langchain] --upgrade\n```\n\n----------------------------------------\n\nTITLE: Gathering Retrieval Results in Python Workflow\nDESCRIPTION: This code demonstrates how the workflow gathers results from multiple retrieval steps using the context's collect_events method.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/Tutorial.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n    results = ctx.collect_events(ev, [RetrievalResultEvent] * len(self.retrievers))\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for MLflow with PyTorch\nDESCRIPTION: This snippet installs MLflow, torchmetrics, and torchinfo packages required for the PyTorch experiment tracking.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -q mlflow torchmetrics torchinfo\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx as Reverse Proxy with Basic Authentication\nDESCRIPTION: Shows how to set up Nginx as a reverse proxy with basic authentication for the MLflow AI Gateway. This improves security by adding an authentication layer and shielding the application from direct internet traffic.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_26\n\nLANGUAGE: nginx\nCODE:\n```\nhttp {\n    server {\n        listen 80;\n\n        location / {\n            auth_basic \"Restricted Content\";\n            auth_basic_user_file /etc/nginx/.htpasswd;\n\n            proxy_pass http://localhost:5000;  # Replace with the MLflow AI Gateway port\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling UserWarnings in Python\nDESCRIPTION: A simple snippet that suppresses less useful UserWarnings from setuptools and pydantic libraries to create a cleaner output during execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-chat-completions.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Rendering Card Components for End-to-End RAG Tutorial in JSX\nDESCRIPTION: Creates a card interface using CardGroup and PageCard components to link to the comprehensive end-to-end RAG evaluation tutorial with MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <PageCard headerText=\"End-to-End RAG Evaluation with MLflow\" link=\"/llms/rag/notebooks/mlflow-e2e-evaluation/\" text=\"Comprehensive tutorial on evaluating Retrieval-Augmented Generation (RAG) systems using MLflow\" />\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Get Model Version by Alias Response Structure Definition\nDESCRIPTION: Defines the response structure returned when retrieving a model version by alias. It returns a model_version object containing details about the requested model version.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_32\n\nLANGUAGE: proto\nCODE:\n```\n+---------------+---------------------------+-------------+\n|  Field Name   |           Type            | Description |\n+===============+===========================+=============+\n| model_version | :ref:`mlflowmodelversion` |             |\n+---------------+---------------------------+-------------+\n```\n\n----------------------------------------\n\nTITLE: Running MLflow SHAP Example Scripts\nDESCRIPTION: Command to execute any of the example scripts by replacing <script_name> with the desired script file (regression.py, binary_classification.py, or multiclass_classification.py).\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/shap/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython <script_name>\n```\n\n----------------------------------------\n\nTITLE: Executing Summing Function with Code Inspector\nDESCRIPTION: A simple execution of the summing_function with an input of 1000, demonstrating how the code_inspector decorator analyzes the function before execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsumming_function(1000)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Images for Model Serving without Model URI\nDESCRIPTION: The `mlflow models build-docker` command now makes `model_uri` optional, allowing the creation of generic model serving images that can be configured at runtime.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_20\n\nLANGUAGE: Shell\nCODE:\n```\nmlflow models build-docker\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Trained spaCy Model with MLflow\nDESCRIPTION: Shows how to load a trained spaCy model from MLflow artifacts and use it for making predictions on new data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\nlast_run = client.search_runs(experiment_ids=[\"0\"], max_results=1)[0]\n\nspacy_model = mlflow.pyfunc.load_model(\n    f\"{last_run.info.artifact_uri}/mlflow_textcat_example\"\n)\npredictions_in = dev_df.loc[:, [\"sentence\"]]\npredictions_out = spacy_model.predict(predictions_in).squeeze().tolist()\npredicted_labels = [\n    \"pos\" if row[\"pos\"] > row[\"neg\"] else \"neg\" for row in predictions_out\n]\nprint(dev_df.assign(predicted_sentiment=predicted_labels))\n```\n\n----------------------------------------\n\nTITLE: Dropping Latest Metrics Table for Failed Migration Recovery\nDESCRIPTION: SQL command to delete the latest_metrics table when recovering from a failed migration. This restores the database to its previous state.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/store/db_migrations/README.md#2025-04-07_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nDROP TABLE latest_metrics;\n```\n\n----------------------------------------\n\nTITLE: Executing XGBoost Training as MLflow Project in Bash\nDESCRIPTION: Command for running the training code as an MLflow project with parameterized inputs, which provides a standardized way to package and reproduce machine learning workflows.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/xgboost/xgboost_native/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9\n```\n\n----------------------------------------\n\nTITLE: Defining Random Forest Training Function with RAPIDS and MLflow in Python\nDESCRIPTION: This function trains a Random Forest Classifier using RAPIDS cuML. It loads data, trains the model, makes predictions, calculates accuracy, and logs parameters and metrics using MLflow. It returns the trained model and inferred signature.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/notebooks/rapids_mlflow.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train(fpath, max_depth, max_features, n_estimators):\n    \"\"\"\n    Args:\n        fpath: Path or URL for the training data used with the model.\n        max_depth: int Max tree depth\n        max_features: float percentage of features to use in classification\n        n_estimators: int number of trees to create\n\n    Returns:\n        Trained model\n    \"\"\"\n    X_train, X_test, y_train, y_test = load_data(fpath)\n    mod = RandomForestClassifier(\n        max_depth=max_depth, max_features=max_features, n_estimators=n_estimators\n    )\n    acc_scorer = accuracy_score\n\n    mod.fit(X_train, y_train)\n    preds = mod.predict(X_test)\n    acc = acc_scorer(y_test, preds)\n\n    mlparams = {\n        \"max_depth\": str(max_depth),\n        \"max_features\": str(max_features),\n        \"n_estimators\": str(n_estimators),\n    }\n    mlflow.log_params(mlparams)\n\n    mlmetrics = {\"accuracy\": acc}\n    mlflow.log_metrics(mlmetrics)\n\n    return mod, infer_signature(X_train.to_pandas(), y_train.to_pandas())\n```\n\n----------------------------------------\n\nTITLE: Get Latest Model Versions Endpoint\nDESCRIPTION: REST endpoint for retrieving the latest versions of a registered model. Can filter by stages and only returns models in READY status.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_16\n\nLANGUAGE: rest\nCODE:\n```\nGET 2.0/mlflow/registered-models/get-latest-versions\n```\n\n----------------------------------------\n\nTITLE: SQL-like Search Syntax for Filtering Runs\nDESCRIPTION: Example of using the new search filter API that supports a simplified version of the SQL WHERE clause. This can be used to search for runs based on metrics, params, and certain run attributes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_64\n\nLANGUAGE: SQL\nCODE:\n```\nmetric.accuracy > 0.9 and params.model_type = 'RandomForest'\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG System with MLflow and LLM Judge\nDESCRIPTION: Performs comprehensive evaluation of the RAG system using MLflow's evaluate() function. Sets up deployment target, configures relevance metrics using LLaMA-2 model, and executes evaluation with custom metrics including relevance and latency.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nset_deployments_target(\"databricks\")  # To retrieve all endpoint in your Databricks Workspace\n\nrelevance_metric = relevance(\n    model=\"endpoints:/databricks-llama-2-70b-chat\"\n)  # You can also use any model you have hosted on Databricks, models from the Marketplace or models in the Foundation model API\n\nwith mlflow.start_run():\n    results = mlflow.evaluate(\n        model,\n        eval_df,\n        model_type=\"question-answering\",\n        evaluators=\"default\",\n        predictions=\"result\",\n        extra_metrics=[relevance_metric, mlflow.metrics.latency()],\n        evaluator_config={\n            \"col_mapping\": {\n                \"inputs\": \"questions\",\n                \"context\": \"source_documents\",\n            }\n        },\n    )\n    print(results.metrics)\n\ndisplay(results.tables[\"eval_results_table\"])\n```\n\n----------------------------------------\n\nTITLE: Custom SimilarityModel Implementation with MLflow PythonModel\nDESCRIPTION: Implements a custom SimilarityModel class that extends MLflow's PythonModel to compute semantic similarity between sentences using Sentence Transformers. Includes model loading and prediction functionality with error handling.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-similarity/semantic-similarity-sentence-transformers.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\nimport mlflow\nfrom mlflow.models.signature import infer_signature\nfrom mlflow.pyfunc import PythonModel\n\n\nclass SimilarityModel(PythonModel):\n    def load_context(self, context):\n        \"\"\"Load the model context for inference.\"\"\"\n        from sentence_transformers import SentenceTransformer\n\n        try:\n            self.model = SentenceTransformer.load(context.artifacts[\"model_path\"])\n        except Exception as e:\n            raise ValueError(f\"Error loading model: {e}\")\n\n    def predict(self, context, model_input, params):\n        \"\"\"Predict method for comparing similarity between two sentences.\"\"\"\n        from sentence_transformers import util\n\n        if isinstance(model_input, pd.DataFrame):\n            if model_input.shape[1] != 2:\n                raise ValueError(\"DataFrame input must have exactly two columns.\")\n            sentence_1, sentence_2 = model_input.iloc[0, 0], model_input.iloc[0, 1]\n        elif isinstance(model_input, dict):\n            sentence_1 = model_input.get(\"sentence_1\")\n            sentence_2 = model_input.get(\"sentence_2\")\n            if sentence_1 is None or sentence_2 is None:\n                raise ValueError(\n                    \"Both 'sentence_1' and 'sentence_2' must be provided in the input dictionary.\"\n                )\n        else:\n            raise TypeError(\n                f\"Unexpected type for model_input: {type(model_input)}. Must be either a Dict or a DataFrame.\"\n            )\n\n        embedding_1 = self.model.encode(sentence_1)\n        embedding_2 = self.model.encode(sentence_2)\n\n        return np.array(util.cos_sim(embedding_1, embedding_2).tolist())\n```\n\n----------------------------------------\n\nTITLE: Publishing Package to NPM\nDESCRIPTION: Complete command sequence for publishing the package to NPM, including version bumping and post-publish tasks. Requires proper NPM authentication and permissions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/server/js/vendor/design-system/README.md#2025-04-07_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nyarn npm login && npm version $(npm view @databricks/design-system version) --no-workspaces-update && npm version patch --no-workspaces-update && yarn npm publish --access public && yarn postpublish && yarn\n```\n\n----------------------------------------\n\nTITLE: Authenticating and Pulling MLflow Docker Image\nDESCRIPTION: Commands to authenticate with GitHub Container Registry and pull the official MLflow Docker image. Shows how to pull both the latest version and a specific version (2.0.1).\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/docker/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport CR_PAT=YOUR_TOKEN\necho $CR_PAT | docker login ghcr.io -u USERNAME --password-stdin\n# Pull the latest version\ndocker pull ghcr.io/mlflow/mlflow\n# Pull 2.0.1\ndocker pull ghcr.io/mlflow/mlflow:v2.0.1\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Run in R\nDESCRIPTION: Function to start a new MLflow run with optional parameters for run ID, experiment ID, start time, and tags. Can create nested runs if specified.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_54\n\nLANGUAGE: r\nCODE:\n```\nmlflow_start_run(\n  run_id = NULL,\n  experiment_id = NULL,\n  start_time = NULL,\n  tags = NULL,\n  client = NULL,\n  nested = FALSE\n)\n```\n\n----------------------------------------\n\nTITLE: Type Hints Data Validation Example\nDESCRIPTION: Demonstration of type hint-based data validation in action with both direct model usage and MLflow loaded models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/python_model.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = CustomModel()\n\n# The input_example can be a list of Message objects as defined in the type hint\ninput_example = [\n    Message(role=\"system\", content=\"Hello\"),\n    Message(role=\"user\", content=\"Hi\"),\n]\nprint(model.predict(input_example))  # Output: ['Hello', 'Hi']\n\n# The input_example can also be a list of dict with the same schema as Message\ninput_example = [\n    {\"role\": \"system\", \"content\": \"Hello\"},\n    {\"role\": \"user\", \"content\": \"Hi\"},\n]\nprint(model.predict(input_example))  # Output: ['Hello', 'Hi']\n\n# If your input doesn't match the schema, it will raise an exception\n# e.g. content field is missing here, but it's required in the Message definition\nmodel.predict([{\"role\": \"system\"}])\n# Output: 1 validation error for Message\\ncontent\\n  Field required [type=missing, input_value={'role': 'system'}, input_type=dict]\n\n# The same data validation works if you log and load the model as pyfunc\nmodel_info = mlflow.pyfunc.log_model(\n    artifact_path=\"model\",\n    python_model=model,\n    input_example=input_example,\n)\npyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\nprint(pyfunc_model.predict(input_example))\n```\n\n----------------------------------------\n\nTITLE: Running Authentication Example Script\nDESCRIPTION: Command to execute the authentication example script that demonstrates permission handling.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/auth/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython auth.py\n```\n\n----------------------------------------\n\nTITLE: Running JavaScript Tests for MLflow UI\nDESCRIPTION: Yarn commands to run tests for React components in the MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# Run tests in CompareRunBox.test.js\nyarn test CompareRunBox.test.js\n# Run tests with a name that matches 'plot' in CompareRunBox.test.js\nyarn test CompareRunBox.test.js -t 'plot'\n# Run all tests\nyarn test\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Automatic Tracing\nDESCRIPTION: Sets up the OpenAI API key as an environment variable for use with automatic tracing. If the key is not already set, it prompts the user to enter it.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/tutorials/jupyter-trace-demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n```\n\n----------------------------------------\n\nTITLE: Conda Environment Management with Spark UDF\nDESCRIPTION: Shows how to use conda environment management with Spark UDF to ensure the same Python environment used during training is available for inference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_80\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.types import ArrayType, FloatType\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npyfunc_udf = mlflow.pyfunc.spark_udf(\n    spark,\n    \"path/to/model\",\n    result_type=ArrayType(FloatType()),\n    env_manager=\"conda\",  # Use conda to restore the environment used in training\n)\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf(struct(\"name\", \"age\")))\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Example as MLflow Project\nDESCRIPTION: Command to execute the summarization example as an MLflow project. The script uses LangChain for building summarization models and requires MLflow 2.4.0+, LangChain, OpenAI client, and OpenAI API key.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd summarization && mlflow run .\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Importing necessary Python libraries for RAG system implementation including MLflow, LangChain, ChromaDB, and other utilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/mlflow-e2e-evaluation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ast\nimport os\n\nimport chromadb\nimport pandas as pd\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings.databricks import DatabricksEmbeddings\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain.llms import Databricks\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n\nimport mlflow\nimport mlflow.deployments\nfrom mlflow.deployments import set_deployments_target\nfrom mlflow.metrics.genai.metric_definitions import relevance\n```\n\n----------------------------------------\n\nTITLE: Installing MLflavors\nDESCRIPTION: Command to install the MLflavors package using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install mlflavors\n```\n\n----------------------------------------\n\nTITLE: MLflow Parameters Example\nDESCRIPTION: Example showing how to define and log parameters in MLflow\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/R/mlflow/README.md#2025-04-07_snippet_9\n\nLANGUAGE: r\nCODE:\n```\nlibrary(mlflow)\n\n# define parameters\nmy_int <- mlflow_param(\"my_int\", 1, \"integer\")\nmy_num <- mlflow_param(\"my_num\", 1.0, \"numeric\")\n\n# log parameters\nmlflow_log_param(\"param_int\", my_int)\nmlflow_log_param(\"param_num\", my_num)\n```\n\n----------------------------------------\n\nTITLE: Deleting Model Deployment - Bash\nDESCRIPTION: Command to remove a deployed model instance from Ray Serve.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/ray_serve/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmlflow deployments delete -t ray-serve --name iris:v1\n```\n\n----------------------------------------\n\nTITLE: Using MLflow Model with Multidimensional Inputs as Spark UDF in Python\nDESCRIPTION: Demonstrates how to use an MLflow model that requires multidimensional inputs as a Spark UDF. The example shows how to prepare and pass array-type columns for tensor inputs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_77\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n# Assuming the model requires input 'a' of shape (-1, 2, 3) and input 'b' of shape (-1, 4, 5)\nmodel_path = \"<path-to-model-requiring-multidimensional-inputs>\"\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_path)\n# The `spark_df` has column 'a' containing arrays of length 6 and\n# column 'b' containing arrays of length 20\ndf = spark_df.withColumn(\"prediction\", pyfunc_udf(struct(\"a\", \"b\")))\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow to use SQLite Database (Jupyter Notebook)\nDESCRIPTION: Sets the MLFLOW_TRACKING_URI environment variable in a Jupyter notebook to use a local SQLite database for tracking.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/local-database/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%env MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n```\n\n----------------------------------------\n\nTITLE: Running Unity Catalog Example Script\nDESCRIPTION: Command to execute the example script that demonstrates Unity Catalog function integration.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/uc_functions/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Replace `my.uc_func.add` if your UC function has a different name\npython examples/gateway/uc_functions/run.py  --uc-function-name my.uc_func.add\n```\n\n----------------------------------------\n\nTITLE: Defining MLflow Project Dependencies with scikit-learn\nDESCRIPTION: This dependency specification lists MLflow as a required library along with scikit-learn version 1.4.2. This ensures compatibility and reproducibility for MLflow projects that utilize scikit-learn functionality.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/resources/example_virtualenv_project/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlflow\nscikit-learn==1.4.2\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Tracking URI\nDESCRIPTION: Python code to set the MLflow tracking server URI for logging experiments and runs.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nmlflow.set_tracking_uri(uri=\"http://<host>:<port>\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel MLflow Runs with Spawn Method in Python\nDESCRIPTION: Illustrates how to set up parallel MLflow runs using the spawn method for multiprocessing. This approach explicitly sets the experiment and tracking URI in each child process to ensure proper logging.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tracking-api/index.mdx#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport multiprocessing as mp\n\n\ndef train_model(params):\n    # Set the experiment and tracking URI in each child process\n    mlflow.set_tracking_uri(\"http://localhost:5000\")\n    mlflow.set_experiment(\"multi-process\")\n    with mlflow.start_run():\n        ...\n\n\nif __name__ == \"__main__\":\n    params = [0.01, 0.02, ...]\n    pool = mp.get_context(\"spawn\").Pool(processes=4)\n    pool.map(train_model, params)\n```\n\n----------------------------------------\n\nTITLE: MLflow Server Command Generation\nDESCRIPTION: Python code to generate the CLI command for starting a local MLflow Model Serving endpoint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"mlflow models serve -m {model_uri} -h 127.0.0.1 -p 9020 --no-conda\")\n```\n\n----------------------------------------\n\nTITLE: MLflow Configuration\nDESCRIPTION: Configuring MLflow tracking URI and experiment name for monitoring training progress.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/transformers/MLFlow_X_HuggingFace_Finetune_a_text_classification_model.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# This is always \"databricks\" when using a databricks hosted tracking server.\nmlflow.set_tracking_uri(\"databricks\")\n# Pick up a name you like.\nmlflow.set_experiment(\"/finetune-a-spam-classifier\")\n```\n\n----------------------------------------\n\nTITLE: Running Question Answering Example as Python Script\nDESCRIPTION: Command to execute the question answering example directly as a Python script. Requires the same dependencies as the MLflow project version.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ cd question_answering && python question_answering.py\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG System with Langchain and Chroma\nDESCRIPTION: Builds a Retrieval Augmented Generation (RAG) system by loading MLflow documentation, splitting it into chunks, creating embeddings with OpenAI, and setting up a question-answering chain with document retrieval.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llm-evaluate/notebooks/rag-evaluation-llama2.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nloader = WebBaseLoader(\"https://mlflow.org/docs/latest/index.html\")\n\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)\n\nqa = RetrievalQA.from_chain_type(\n    llm=OpenAI(temperature=0),\n    chain_type=\"stuff\",\n    retriever=docsearch.as_retriever(),\n    return_source_documents=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a New MLflow User with AuthServiceClient\nDESCRIPTION: Shows how to create a new user using the MLflow AuthServiceClient. This approach provides a convenient Python interface for user management operations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nauth_client = mlflow.server.get_app_client(\n    \"basic-auth\", tracking_uri=\"https://<mlflow_tracking_uri>/\"\n)\nauth_client.create_user(username=\"username\", password=\"password\")\n```\n\n----------------------------------------\n\nTITLE: Adding Command Example Test Parameter\nDESCRIPTION: Example tuple format for adding a new command-line test case to the pytest.mark.parametrize decorator.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/examples/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n(\"new_example_dir\", [\"python\", \"train.py\"]),\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for MLflow Project\nDESCRIPTION: This snippet enumerates the required Python packages and their versions for an MLflow project. It includes libraries for cryptography, data processing, machine learning, and cloud storage among others.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/resources/example_mlflow_2.12_langchain_model/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbcrypt==3.2.0\ncloudpickle==2.0.0\nconfigparser==5.2.0\ncryptography==39.0.1\ndatabricks-feature-engineering==0.2.1\ndatabricks-rag-studio==0.2.0.dev0\nentrypoints==0.4\ngoogle-cloud-storage==2.11.0\ngrpcio-status==1.48.1\nlangchain==0.1.20\nmlflow[gateway]==2.12.2\nnumpy==1.23.5\npackaging==23.2\npandas==1.5.3\nprotobuf==4.24.0\npsutil==5.9.0\npyarrow==8.0.0\npydantic==1.10.6\npyyaml==6.0\nrequests==2.28.1\ntornado==6.1\n```\n\n----------------------------------------\n\nTITLE: FileInfo Structure Definition in MLflow REST API\nDESCRIPTION: Specifies the FileInfo structure used to represent file metadata in MLflow, including path, directory status, and file size.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_37\n\nLANGUAGE: rest\nCODE:\n```\n+------------+------------+---------------------------------------------------+\n| Field Name |    Type    |                    Description                    |\n+============+============+===================================================+\n| path       | ``STRING`` | Path relative to the root artifact directory run. |\n+------------+------------+---------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Server Command\nDESCRIPTION: Command line instruction to start the MLflow model server for serving the embeddings model.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_4\n\nLANGUAGE: commandline\nCODE:\n```\nmlflow models serve -m file:///Users/me/demos/mlruns/0/2bfcdcb66eaf4c88abe8e0c7bcab639e/artifacts/embeddings_model -h 127.0.0.1 -p 9020 --no-conda\n```\n\n----------------------------------------\n\nTITLE: Building MLflow Distribution Package\nDESCRIPTION: Command to create a pip-installable wheel and compressed code archive in the dist/ directory. This builds the distributable artifact after JavaScript files have been built.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ncd -\npython -m build\n```\n\n----------------------------------------\n\nTITLE: Setting Up MLflow Experiment\nDESCRIPTION: R code to initialize and set up a new MLflow experiment\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/R/mlflow/README.md#2025-04-07_snippet_5\n\nLANGUAGE: r\nCODE:\n```\nlibrary(mlflow)\nmlflow_set_experiment(\"Test\")\n```\n\n----------------------------------------\n\nTITLE: Running Hyperparameter Optimization\nDESCRIPTION: Executes the hyperparameter optimization process using Hyperopt and logs results to MLflow\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/quickstart-2/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"/wine-quality\")\nwith mlflow.start_run():\n    # Conduct the hyperparameter search using Hyperopt\n    trials = Trials()\n    best = fmin(\n        fn=objective,\n        space=space,\n        algo=tpe.suggest,\n        max_evals=8,\n        trials=trials,\n    )\n\n    # Fetch the details of the best run\n    best_run = sorted(trials.results, key=lambda x: x[\"loss\"])[0]\n\n    # Log the best parameters, loss, and model\n    mlflow.log_params(best)\n    mlflow.log_metric(\"eval_rmse\", best_run[\"loss\"])\n    mlflow.tensorflow.log_model(best_run[\"model\"], \"model\", signature=signature)\n\n    # Print out the best parameters and corresponding loss\n    print(f\"Best parameters: {best}\")\n    print(f\"Best eval rmse: {best_run['loss']}\")\n```\n\n----------------------------------------\n\nTITLE: Making Inference Request to Locally Served MLflow Model\nDESCRIPTION: This curl command shows how to make an inference request to a locally served MLflow model. It sends a POST request to the '/invocations' endpoint with the serving example as the payload.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d 'YOUR_SERVING_EXAMPLE'\n```\n\n----------------------------------------\n\nTITLE: AWS SageMaker Deployment Configuration\nDESCRIPTION: Example showing how to specify AWS account and role when deploying models to SageMaker\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.sagemaker.deploy(app_name=\"myApp\", \n                           aws_account=\"123456789\",\n                           aws_role=\"arn:aws:iam::123456789:role/MyRole\")\n```\n\n----------------------------------------\n\nTITLE: Loading a LlamaIndex Index Directly with MLflow\nDESCRIPTION: Demonstrates how to load the LlamaIndex index object itself instead of the engine. This approach provides direct access to the index for more advanced customization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nindex = mlflow.llama_index.load_model(\"runs:/<run_id>/index\")\n```\n\n----------------------------------------\n\nTITLE: Creating Model Version without Signature in Python\nDESCRIPTION: This snippet demonstrates how to create a model version without a signature using MLflow and scikit-learn. It logs a RandomForestClassifier model and registers it in the Model Registry.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.ensemble import RandomForestClassifier\nimport mlflow\nfrom mlflow.client import MlflowClient\n\nmodel_name = \"add_signature_model\"\n\nwith mlflow.start_run() as run:\n    mlflow.sklearn.log_model(RandomForestClassifier(), \"sklearn-model\")\n\nmodel_uri = f\"runs:/{run.info.run_id}/sklearn-model\"\nmlflow.register_model(model_uri=model_uri, name=model_name)\n```\n\n----------------------------------------\n\nTITLE: Searching Registered Models in MLflow with R\nDESCRIPTION: Function for retrieving a list of registered models with filtering options. Supports parameters for filtering, pagination, ordering results, and specifying a custom MLflow client.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_45\n\nLANGUAGE: r\nCODE:\n```\nmlflow_search_registered_models(\n  filter = NULL,\n  max_results = 100,\n  order_by = list(),\n  page_token = NULL,\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: MPT-7B Model Download\nDESCRIPTION: Python code to download the MPT-7B instruct model from Hugging Face Hub to local storage.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\n\nsnapshot_location = snapshot_download(\n    repo_id=\"mosaicml/mpt-7b-instruct\", local_dir=\"mpt-7b\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Model Version Tags in MLflow R\nDESCRIPTION: Function to set tags for a specific model version or latest version of a stage. Requires model name and either version or stage, plus tag key and value.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_50\n\nLANGUAGE: r\nCODE:\n```\nmlflow_set_model_version_tag(\n  name,\n  version = NULL,\n  key = NULL,\n  value = NULL,\n  stage = NULL,\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: MLflow Authentication API Endpoints Table\nDESCRIPTION: HTML table defining MLflow authentication API endpoints for user management and permission controls, including methods and required access levels.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<table>\n  <thead>\n    <tr>\n      <th>API</th>\n      <th>Endpoint</th>\n      <th>Method</th>\n      <th>Required permission</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>`2.0/mlflow/users/create`</td>\n      <td>`POST`</td>\n      <td>None</td>\n    </tr>\n    <!-- Additional endpoints -->\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Executing Audio Transcription with Whisper Pipeline in Python\nDESCRIPTION: This code snippet demonstrates how to use the previously set up Whisper pipeline to transcribe an audio file and print the formatted output.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Verify that our pipeline is capable of processing an audio file and transcribing it\ntranscription = audio_transcription_pipeline(audio)\n\nprint(format_transcription(transcription[\"text\"]))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating OpenAI Model Usage with MLflow\nDESCRIPTION: Shows how to use OpenAI models through MLflow for various NLP tasks, including single and multiple variable prompt templating. The code demonstrates model logging, loading, and prediction using different input formats (DataFrame, list of dictionaries, list of strings).\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/guide/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport os\n\nimport openai\nimport pandas as pd\n\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import ColSpec, ParamSchema, ParamSpec, Schema\n\nlogging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n\nassert (\n    \"OPENAI_API_KEY\" in os.environ\n), \"Please set the OPENAI_API_KEY environment variable.\"\n\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-4o-mini\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about {animal}.\"}],\n    )\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        \"animal\": [\n            \"cats\",\n            \"dogs\",\n        ]\n    }\n)\nprint(model.predict(df))\n\nlist_of_dicts = [\n    {\"animal\": \"cats\"},\n    {\"animal\": \"dogs\"},\n]\nprint(model.predict(list_of_dicts))\n\nlist_of_strings = [\n    \"cats\",\n    \"dogs\",\n]\nprint(model.predict(list_of_strings))\n\nwith mlflow.start_run():\n    model_info = mlflow.openai.log_model(\n        model=\"gpt-4o-mini\",\n        task=openai.chat.completions,\n        artifact_path=\"model\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a {adjective} joke about {animal}.\"}\n        ],\n    )\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\ndf = pd.DataFrame(\n    {\n        \"adjective\": [\"funny\", \"scary\"],\n        \"animal\": [\"cats\", \"dogs\"],\n    }\n)\nprint(model.predict(df))\n```\n\n----------------------------------------\n\nTITLE: Viewing Database Logs for MLflow Testing\nDESCRIPTION: This snippet shows how to view logs for a specific database service used in MLflow testing using Docker Compose.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/db/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# View database logs\n./tests/db/compose.sh logs --follow <database service>\n```\n\n----------------------------------------\n\nTITLE: Logging Experiment with MLflow\nDESCRIPTION: Demonstrates logging an experiment using MLflow with a scikit-learn RandomForest model on the diabetes dataset.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\nmlflow.autolog()\n\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\n# Create and train models.\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\nrf.fit(X_train, y_train)\n\n# Use the model to make predictions on the test dataset.\npredictions = rf.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Deleting MLflow Experiment in R\nDESCRIPTION: Function to delete an MLflow experiment including associated runs, parameters, metrics and artifacts if using FileStore.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_9\n\nLANGUAGE: r\nCODE:\n```\nmlflow_delete_experiment(experiment_id, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Custom MLflow Plugin Provider Package\nDESCRIPTION: This command uninstalls the 'my_llm' package to clean up after running the example.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/plugin/README.md#2025-04-07_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip uninstall my_llm\n```\n\n----------------------------------------\n\nTITLE: Running a Provider Test Script\nDESCRIPTION: Command to execute a provider-specific example script to test the Gateway configuration, typically used after setting up the gateway service.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/README.md#2025-04-07_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython examples/gateway/<provider>/example.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Warning Filters for Python Libraries\nDESCRIPTION: This snippet disables specific UserWarnings from setuptools and pydantic libraries to reduce noise in the output. It's a common practice when working with these libraries in data science projects.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/paraphrase-mining/paraphrase-mining-sentence-transformers.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for MLflow Project\nDESCRIPTION: This snippet lists the required Python packages and their versions for an MLflow project. It includes essential libraries for machine learning workflows, data handling, cloud integration, and web services. The specific versions ensure compatibility and reproducibility of the project environment.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/resources/example_mlflow_2.12_langchain_model/metadata/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nbcrypt==3.2.0\ncloudpickle==2.0.0\nconfigparser==5.2.0\ncryptography==39.0.1\ndatabricks-feature-engineering==0.2.1\nentrypoints==0.4\ngoogle-cloud-storage==2.11.0\ngrpcio-status==1.48.1\nlangchain==0.1.20\nmlflow[gateway]==2.12.2\nnumpy==1.23.5\npackaging==23.2\npandas==1.5.3\nprotobuf==4.24.0\npsutil==5.9.0\npyarrow==8.0.0\npydantic==1.10.6\npyyaml==6.0\nrequests==2.28.1\ntornado==6.1\n```\n\n----------------------------------------\n\nTITLE: Installing Sktime with MLflow Support\nDESCRIPTION: Command to install Sktime with MLflow dependency using pip.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install sktime[mlflow]\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Configuration for MLflow DSPy Module\nDESCRIPTION: RST configuration for auto-generating API documentation for the MLflow.dspy module. Includes settings to show all members, undocumented members, and inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.dspy.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.dspy\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Training Random Forest with MLflow Tracking\nDESCRIPTION: Defines a function to train H2O random forest model with specified number of trees. Logs parameters and metrics (RMSE, R2, MAE) using MLflow tracking.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/h2o/random_forest.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_random_forest(ntrees):\n    with mlflow.start_run():\n        rf = H2ORandomForestEstimator(ntrees=ntrees)\n        train_cols = [n for n in wine.col_names if n != \"quality\"]\n        rf.train(train_cols, \"quality\", training_frame=train, validation_frame=test)\n\n        mlflow.log_param(\"ntrees\", ntrees)\n\n        mlflow.log_metric(\"rmse\", rf.rmse())\n        mlflow.log_metric(\"r2\", rf.r2())\n        mlflow.log_metric(\"mae\", rf.mae())\n\n        mlflow.h2o.log_model(rf, \"model\")\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Environment Variables\nDESCRIPTION: Commands to set environment variables for specifying Python and MLflow binary locations\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/R/mlflow/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_PYTHON_BIN=/path/to/bin/python\nexport MLFLOW_BIN=/path/to/bin/mlflow\n```\n\n----------------------------------------\n\nTITLE: Implementing MLflow Training Loop and Model Logging in Python\nDESCRIPTION: This snippet sets up MLflow tracking, defines experiment parameters, and runs the training loop. It logs the trained model, its signature, and artifacts using MLflow. The model is registered with a specific name for future reference.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/notebooks/rapids_mlflow.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconda_env = \"conda.yaml\"\nfpath = \"iris.csv\"\n\nmax_depth = 10\nmax_features = 0.75\nn_estimators = 500\n\nartifact_path = \"Airline-Demo\"\nartifact_uri = None\nexperiment_name = \"RAPIDS-Notebook\"\nexperiment_id = None\n\nmlflow.set_tracking_uri(uri=\"sqlite:////tmp/mlflow-db.sqlite\")\nmlflow.set_experiment(experiment_name)\n\nwith mlflow.start_run(run_name=\"(Notebook) RAPIDS-MLflow\"):\n    model, signature = train(fpath, max_depth, max_features, n_estimators)\n\n    mlflow.sklearn.log_model(\n        model,\n        signature=signature,\n        artifact_path=artifact_path,\n        registered_model_name=\"rapids-mlflow-notebook\",\n        conda_env=\"conda.yaml\",\n    )\n\n    artifact_uri = mlflow.get_artifact_uri(artifact_path=artifact_path)\nprint(artifact_uri)\n```\n\n----------------------------------------\n\nTITLE: Logging Whisper Model with MLflow in Python\nDESCRIPTION: This snippet demonstrates how to log the Whisper audio transcription model using MLflow's transformers module. It includes model information, configuration, and optional reference-only mode for reduced storage usage.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Log the pipeline\nwith mlflow.start_run():\n    model_info = mlflow.transformers.log_model(\n        transformers_model=audio_transcription_pipeline,\n        artifact_path=\"whisper_transcriber\",\n        signature=signature,\n        input_example=audio,\n        model_config=model_config,\n        # Since MLflow 2.11.0, you can save the model in 'reference-only' mode to reduce storage usage by not saving\n        # the base model weights but only the reference to the HuggingFace model hub. To enable this, uncomment the\n        # following line:\n        # save_pretrained=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining MLflow Project Dependencies\nDESCRIPTION: Specifies exact package versions for MLflow core and cloudpickle dependency using pip requirements format. Pins MLflow to version 2.8.1 and cloudpickle to version 2.2.1.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/resources/pyfunc_models/2.8.1/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlflow==2.8.1\ncloudpickle==2.2.1\n```\n\n----------------------------------------\n\nTITLE: Importing YAML Module\nDESCRIPTION: Imports the YAML module for potential configuration handling.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/h2o/random_forest.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\n```\n\n----------------------------------------\n\nTITLE: Setting MLFLOW_TRACKING_URI Environment Variable\nDESCRIPTION: This snippet emphasizes the importance of setting the MLFLOW_TRACKING_URI environment variable for proper functioning of the MLflow CLI. Users should replace '<tracking_server_url>' with their actual tracking server URL.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/cli.rst#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_TRACKING_URI=<tracking_server_url>\n```\n\n----------------------------------------\n\nTITLE: Running RAPIDS MLflow Project with Hyperopt in Shell\nDESCRIPTION: This shell command launches an MLflow run for hyperparameter optimization using Hyperopt. It specifies the experiment name, conda environment, and input data file path for the RAPIDS model training.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/README.md#2025-04-07_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmlflow run . -e hyperopt \\\n               --experiment-name RAPIDS-CLI \\\n               -P conda-env=$PWD/envs/conda.yaml \\\n               -P fpath=iris.csv\n```\n\n----------------------------------------\n\nTITLE: Evaluating Question-Answering Models with MLflow\nDESCRIPTION: Shows how to use MLflow for automatic evaluation of question-answering tasks. It creates an evaluation dataset, starts an MLflow run, and runs the evaluation using built-in metrics for question-answering models. The results are then printed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport pandas as pd\n\n# Evaluation set contains (1) input question (2) model outputs (3) ground truth\ndf = pd.DataFrame(\n    {\n        \"inputs\": [\"What is MLflow?\", \"What is Spark?\"],\n        \"outputs\": [\n            \"MLflow is an innovative fully self-driving airship powered by AI.\",\n            \"Sparks is an American pop and rock duo formed in Los Angeles.\",\n        ],\n        \"ground_truth\": [\n            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n            \"lifecycle.\",\n            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n            \"processing and analytics.\",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions=\"outputs\", targets=\"ground_truth\"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name=\"evaluate_qa\"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type=\"question-answering\",\n    )\n\nprint(results.tables[\"eval_results_table\"])\n```\n\n----------------------------------------\n\nTITLE: Loading a Transformers Model with MLflow pyfunc\nDESCRIPTION: Shows how to load a previously logged Transformers model using MLflow's pyfunc module, which provides a standardized interface for model interaction regardless of the underlying framework.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/text-generation/text-generation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load our pipeline as a generic python function\nsentence_generator = mlflow.pyfunc.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for MLflow Projects Module\nDESCRIPTION: This reStructuredText directive configures Sphinx to automatically generate API documentation for the MLflow projects module. It includes all members, undocumented members, and shows inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.projects.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.projects\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Data Preparation for PyTorch Model\nDESCRIPTION: Loads and preprocesses the Iris dataset, including standardization, train-test splitting, and conversion to PyTorch tensors.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/logging/pytorch_log_model.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load and preprocess the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# Convert arrays to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# Create datasets and dataloaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16)\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Saving Example\nDESCRIPTION: Example showing how to save an MLflow model\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/R/mlflow/README.md#2025-04-07_snippet_10\n\nLANGUAGE: r\nCODE:\n```\nmlflow_save_model(\n  crate(~ stats::predict(model, .x), model)\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Iris Dataset in Python (Commented)\nDESCRIPTION: This commented-out code snippet shows how to load the Iris dataset using scikit-learn and save it as a CSV file. It's included as a reference for data preparation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/rapids/mlflow_project/notebooks/rapids_mlflow.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#!python -c \"from sklearn.datasets import load_iris; d = load_iris(as_frame=True); d.frame.to_csv('iris.csv', index=False)\"\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up MLflow Database Test Services and Resources\nDESCRIPTION: This snippet provides commands for cleaning up Docker resources used in MLflow database testing. It includes options for removing containers, networks, volumes, and images.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/db/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Clean up containers, networks, and volumes\n./tests/db/compose.sh down --volumes --remove-orphans\n\n# Clean up containers, networks, volumes, and images\n./tests/db/compose.sh down --volumes --remove-orphans --rmi all\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Saved Model with Prompt Template\nDESCRIPTION: Shows how to load a saved transformers model and use it for inference with the prompt template.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/guide/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\n\n# Load the model with pyfunc\nmodel = mlflow.pyfunc.load_model(\"path/to/model\")\n\n# The prompt template will be used to format this input\nmodel.predict(\"What is MLflow?\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Initial setup importing pandas and mlflow libraries required for the RAG evaluation system.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/rag-evaluation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nimport mlflow\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for MLflow SageMaker Module\nDESCRIPTION: Sphinx documentation configuration for the mlflow.sagemaker module, including directives for generating comprehensive API documentation with member details and inheritance information.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.sagemaker.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nmlflow.sagemaker\n================\n\n.. automodule:: mlflow.sagemaker\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Project Example\nDESCRIPTION: Example of running an MLflow project from command line\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/R/mlflow/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run examples/r_wine --entry-point train.R\n```\n\n----------------------------------------\n\nTITLE: Evaluating Question-Answering Models with MLflow\nDESCRIPTION: This snippet demonstrates MLflow's model evaluation capabilities for question-answering tasks. It creates an evaluation dataset with input questions, model outputs, and ground truth, then runs automatic evaluation with built-in metrics and records the results in an MLflow run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/skinny/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nimport pandas as pd\n\n# Evaluation set contains (1) input question (2) model outputs (3) ground truth\ndf = pd.DataFrame(\n    {\n        \"inputs\": [\"What is MLflow?\", \"What is Spark?\"],\n        \"outputs\": [\n            \"MLflow is an innovative fully self-driving airship powered by AI.\",\n            \"Sparks is an American pop and rock duo formed in Los Angeles.\",\n        ],\n        \"ground_truth\": [\n            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n            \"lifecycle.\",\n            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n            \"processing and analytics.\",\n        ],\n    }\n)\neval_dataset = mlflow.data.from_pandas(\n    df, predictions=\"outputs\", targets=\"ground_truth\"\n)\n\n# Start an MLflow Run to record the evaluation results to\nwith mlflow.start_run(run_name=\"evaluate_qa\"):\n    # Run automatic evaluation with a set of built-in metrics for question-answering models\n    results = mlflow.evaluate(\n        data=eval_dataset,\n        model_type=\"question-answering\",\n    )\n\nprint(results.tables[\"eval_results_table\"])\n```\n\n----------------------------------------\n\nTITLE: Using mlflow_run Function with Numerical Parameters in R\nDESCRIPTION: Shows how to use the mlflow_run() function in R with numerical parameters greater than or equal to 1000. This bug fix was implemented in MLflow 1.8.0.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_46\n\nLANGUAGE: r\nCODE:\n```\nmlflow::mlflow_run(parameters = list(param1 = 1000, param2 = 5000))\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Authentication Plugin for MLflow (Python)\nDESCRIPTION: This Python code snippet illustrates how to create a custom authentication plugin for MLflow. It defines a create_app function that extends the MLflow app and a MyAuthClient class for managing permissions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/auth/index.mdx#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom flask import Flask\nfrom mlflow.server import app\n\n\ndef create_app(app: Flask = app):\n    app.add_url_rule(...)\n    return app\n\n\nclass MyAuthClient:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Server with JFrog Artifactory\nDESCRIPTION: Command to start the MLflow tracking server using JFrog Artifactory as the artifacts destination.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/plugins/index.mdx#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --host <mlflow tracking server host> --port <mlflow tracking server port> --artifacts-destination artifactory://<JFrog artifactory URL>/artifactory/<repository[/optional base path]>\n```\n\n----------------------------------------\n\nTITLE: Viewing Model Signature\nDESCRIPTION: Shows how to view the model signature that was automatically generated from the input example and type hints.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel_info.signature\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI\nDESCRIPTION: Command to start the MLflow user interface for viewing experiment results and metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/IterativePruning/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Upgrading MLflow Runtime\nDESCRIPTION: Command to upgrade to the latest development version of MLflow\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/R/mlflow/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -e <local github repo>\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for MLflow and Hugging Face\nDESCRIPTION: Installs necessary Python packages for working with MLflow, Transformers, PyTorch and evaluation metrics.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -q mlflow transformers torch torchvision evaluate datasets openai tiktoken fastapi rouge_score textstat\n```\n\n----------------------------------------\n\nTITLE: Preparing Question-Source Dataset for Function Evaluation\nDESCRIPTION: Creates a subset of the original dataset containing only questions and their ground truth sources for use in function-based evaluation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/rag/notebooks/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nquestion_source_df = data[[\"question\", \"source\"]]\nquestion_source_df.head(3)\n```\n\n----------------------------------------\n\nTITLE: Delete Model Version Tag Request Structure Definition\nDESCRIPTION: Defines the request structure for deleting a tag from a specific model version. It requires the registered model name, version number, and the exact tag key to be deleted.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_29\n\nLANGUAGE: proto\nCODE:\n```\n+------------+------------+-------------------------------------------------------------------------------------------------------------------+\n| name       | ``STRING`` | Name of the registered model that the tag was logged under.                                                       |\n|            |            | This field is required.                                                                                           |\n|            |            |                                                                                                                   |\n+------------+------------+-------------------------------------------------------------------------------------------------------------------+\n| version    | ``STRING`` | Model version number that the tag was logged under.                                                               |\n|            |            | This field is required.                                                                                           |\n|            |            |                                                                                                                   |\n+------------+------------+-------------------------------------------------------------------------------------------------------------------+\n| key        | ``STRING`` | Name of the tag. The name must be an exact match; wild-card deletion is not supported. Maximum size is 250 bytes. |\n|            |            | This field is required.                                                                                           |\n|            |            |                                                                                                                   |\n+------------+------------+-------------------------------------------------------------------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Chat Completion with MLflow\nDESCRIPTION: Demonstrates how to use the OpenAI API to create a chat completion. The code imports necessary libraries and makes a request to the OpenAI API asking what MLflow is.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport pydantic\n\nimport mlflow\n\n# Use OpenAI model\nmessages = [{\"role\": \"user\", \"content\": \"What is the MLflow?\"}]\nresponse = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\nresponse\n```\n\n----------------------------------------\n\nTITLE: Process Labels for MLflow Issue Triage\nDESCRIPTION: List of process labels used to indicate the current status and needs of issues and pull requests, including labels for feedback requirements, design needs, and community contribution opportunities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/ISSUE_TRIAGE.rst#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- needs author feedback\n- needs design\n- needs committer feedback\n- needs review\n- help wanted\n- good first issue\n```\n\n----------------------------------------\n\nTITLE: Running Prettier Code Formatter\nDESCRIPTION: Command to run Prettier to ensure consistent code formatting across the project.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/server/js/vendor/design-system/README.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyarn prettier\n```\n\n----------------------------------------\n\nTITLE: Logging a TensorFlow Model with Inferred Signature in MLflow\nDESCRIPTION: This example demonstrates how to automatically infer and store a model signature for a TensorFlow neural network trained on the MNIST dataset. The signature is derived from input examples taken from the training data.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nimport mlflow\n\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential(\n    [\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\n        tf.keras.layers.Dense(128, activation=\"relu\"),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(10),\n    ]\n)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n\nwith mlflow.start_run():\n    model.fit(x_train, y_train, epochs=5)\n    # Take the first three training examples as the model input example.\n    input_example = x_train[:3, :]\n    mlflow.tensorflow.log_model(model, \"mnist_cnn\", input_example=input_example)\n```\n\n----------------------------------------\n\nTITLE: Cleaning and Rebuilding Documentation\nDESCRIPTION: Commands to clean the build directory and regenerate documentation from scratch, useful for resolving rstcheck warning issues.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nmake clean; make html\n```\n\n----------------------------------------\n\nTITLE: Serving MLflow Models Using CLI\nDESCRIPTION: Demonstrates how to deploy a logged MLflow model to a local inference server using a single command line interface (CLI) command. The command starts a server for the model specified by the run ID.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models serve --model-uri runs:/<run-id>/model\n```\n\n----------------------------------------\n\nTITLE: Running MLflow example with custom parameters\nDESCRIPTION: Command to run the Ax hyperparameter optimization example with custom max_epochs and total_trials parameters. This allows customizing the number of training epochs and experimental trials.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/AxHyperOptimizationPTL/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow run . -P max_epochs=X -P total_trials=Y\n```\n\n----------------------------------------\n\nTITLE: Installing R Dependencies for MLflow Development\nDESCRIPTION: Commands to install the necessary R packages and dependencies for building MLflow locally. These commands install devtools and other required packages.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncd mlflow/R/mlflow\nNOT_CRAN=true Rscript -e 'install.packages(\"devtools\", repos = \"https://cloud.r-project.org\")'\nNOT_CRAN=true Rscript -e 'devtools::install_deps(dependencies = TRUE)'\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow CLI with Specific Python Version in R\nDESCRIPTION: MLflow 1.2 adds an optional python_version argument to mlflow_install for specifying the Python version to use within the conda environment for installing the MLflow CLI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_55\n\nLANGUAGE: R\nCODE:\n```\nmlflow_install(python_version = \"3.5\")\n```\n\n----------------------------------------\n\nTITLE: Running MLflow Evaluation Examples\nDESCRIPTION: Shell commands to execute the different MLflow evaluation example scripts that demonstrate various model evaluation scenarios.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython evaluate_on_binary_classifier.py\npython evaluate_on_multiclass_classifier.py\npython evaluate_on_regressor.py\npython evaluate_with_custom_metrics.py\npython evaluate_with_custom_metrics_comprehensive.py\npython evaluate_with_model_vaidation.py\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Directory Structure\nDESCRIPTION: Shows the expected directory layout of the output model after training, including MLmodel files, code dependencies, and model artifacts.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/flower_classifier/README.rst#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmodel\n├── MLmodel\n├── code\n│   └── image_pyfunc.py\n├── data\n│   └── image_model\n│       ├── conf.yaml\n│       └── keras_model\n│           ├── MLmodel\n│           ├── conda.yaml\n│           └── model.h5\n└── mlflow_env.yml\n```\n\n----------------------------------------\n\nTITLE: Querying Completion Models with MLflow Deployment Client\nDESCRIPTION: This snippet demonstrates how to query completion models via the MLflow AI Gateway. It initializes a deployment client, prepares a request with prompt parameters, and sends it to the completions endpoint. The example shows how to set temperature, token limits, and request multiple completions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/guides/step2-query-deployments/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mlflow.deployments import get_deploy_client\n\nclient = get_deploy_client(\"http://localhost:5000\")\nname = \"completions\"\ndata = dict(\n    prompt=\"Name three potions or spells in harry potter that sound like an insult. Only show the names.\",\n    n=2,\n    temperature=0.2,\n    max_tokens=1000,\n)\n\nresponse = client.predict(endpoint=name, inputs=data)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Enabling MLflow Tracing for OpenAI API Calls\nDESCRIPTION: Demonstrates how to enable MLflow Tracing for OpenAI API calls. It uses the autolog feature to automatically log OpenAI interactions, then makes a standard API call to the OpenAI chat completions endpoint. The resulting traces can be viewed in the MLflow UI.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hi!\"}],\n    temperature=0.1,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing React Components for Documentation in JSX\nDESCRIPTION: Imports the APILink component for creating links to API documentation and the CardGroup and PageCard components for creating card-based navigation in the documentation page.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { APILink } from \"@site/src/components/APILink\";\nimport { CardGroup, PageCard } from \"@site/src/components/Card\";\n```\n\n----------------------------------------\n\nTITLE: Restoring Deleted MLflow Experiment in R\nDESCRIPTION: This function restores an MLflow experiment that was marked for deletion, including associated metadata, runs, metrics, and parameters. For FileStore experiments, it also restores underlying artifacts.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_38\n\nLANGUAGE: r\nCODE:\n```\nmlflow_restore_experiment(experiment_id, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Defining Model Schema and Input Example for MLflow LLM Deployment\nDESCRIPTION: Code for defining the input and output schema, parameter specifications, and an input example for the model. This establishes the contract for how the model will be interacted with when deployed.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/custom-pyfunc-for-llms/notebooks/custom-pyfunc-advanced-llm.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types import ColSpec, DataType, ParamSchema, ParamSpec, Schema\n\n# Define input and output schema\ninput_schema = Schema(\n    [\n        ColSpec(DataType.string, \"prompt\"),\n    ]\n)\noutput_schema = Schema([ColSpec(DataType.string, \"candidates\")])\n\nparameters = ParamSchema(\n    [\n        ParamSpec(\"temperature\", DataType.float, np.float32(0.1), None),\n        ParamSpec(\"max_tokens\", DataType.integer, np.int32(1000), None),\n    ]\n)\n\nsignature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n\n\n# Define input example\ninput_example = pd.DataFrame({\"prompt\": [\"What is machine learning?\"]})\n```\n\n----------------------------------------\n\nTITLE: Generating Model Signature for Whisper in Python using MLflow\nDESCRIPTION: This code infers the model signature for the Whisper model, specifying input and output schemas, and sets configuration parameters for audio processing.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/audio-transcription/whisper.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Specify parameters and their defaults that we would like to be exposed for manipulation during inference time\nmodel_config = {\n    \"chunk_length_s\": 20,\n    \"stride_length_s\": [5, 3],\n}\n\n# Define the model signature by using the input and output of our pipeline, as well as specifying our inference parameters that will allow for those parameters to\n# be overridden at inference time.\nsignature = mlflow.models.infer_signature(\n    audio,\n    mlflow.transformers.generate_signature_output(audio_transcription_pipeline, audio),\n    params=model_config,\n)\n\n# Visualize the signature\nsignature\n```\n\n----------------------------------------\n\nTITLE: Configuring Visual Studio Code for Clint Integration\nDESCRIPTION: This JSON configuration integrates Clint with Visual Studio Code using the Pylint extension. It sets the pylint.path to use Clint instead of the default Pylint.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/clint/README.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"pylint.path\": [\"${interpreter}\", \"-m\", \"clint\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for MLflow Azure Integration\nDESCRIPTION: This snippet enumerates the Python packages required for MLflow's Azure-related functionality, prompt flow tools, and environment configuration. It includes 'promptflow[azure]' for Azure-specific prompt flow features, 'promptflow-tools' for general prompt flow utilities, and 'python-dotenv' for managing environment variables.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/promptflow/flow_with_additional_includes/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npromptflow[azure]\npromptflow-tools\npython-dotenv\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python modules for working with MLflow, Hugging Face transformers and datasets.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/evaluation/huggingface-evaluation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import pipeline\n\nimport mlflow\nfrom mlflow.metrics.genai import EvaluationExample, answer_correctness, make_genai_metric\n```\n\n----------------------------------------\n\nTITLE: Visualizing ReActAgent Workflow in Python\nDESCRIPTION: This snippet demonstrates how to visualize the ReActAgent workflow using the draw_all_possible_flows utility function. It renders a graphical representation of the workflow and displays it as HTML in a Jupyter notebook.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/llama-index/notebooks/llama_index_workflow_tutorial.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import HTML\nfrom llama_index.utils.workflow import draw_all_possible_flows\n\ndraw_all_possible_flows(ReActAgent, filename=\"workflow.html\")\n\nwith open(\"workflow.html\") as file:\n    html_content = file.read()\nHTML(html_content)\n```\n\n----------------------------------------\n\nTITLE: Model Version Download URI API Endpoint\nDESCRIPTION: HTTP GET endpoint for retrieving the download URI for model version artifacts.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_23\n\nLANGUAGE: rest\nCODE:\n```\n2.0/mlflow/model-versions/get-download-uri\n```\n\n----------------------------------------\n\nTITLE: Disabling User Warnings in Python\nDESCRIPTION: This snippet demonstrates how to disable specific UserWarnings from setuptools and pydantic using the warnings module in Python.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Implementing Document Retrieval Function\nDESCRIPTION: Defines a function to retrieve document IDs for given questions using the FAISS retriever.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/retriever-evaluation-tutorial.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define a function to return a list of retrieved doc ids\ndef retrieve_doc_ids(question: str) -> list[str]:\n    docs = retriever.get_relevant_documents(question)\n    return [doc.metadata[\"source\"] for doc in docs]\n```\n\n----------------------------------------\n\nTITLE: Displaying DialoGPT Chatbot Response\nDESCRIPTION: Prints the response from the DialoGPT model to demonstrate its functioning. This shows how to access and display the model's output after prediction.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/conversational-model.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Response: {first}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Inference Parameters in MLflow Model Signature YAML Structure\nDESCRIPTION: Example YAML structure showing how to define inference parameters in an MLflow model signature, including scalar and list parameters with their types, default values, and shapes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsignature:\n    inputs: '[{\"name\": \"input\", \"type\": \"string\"}]'\n    outputs: '[{\"name\": \"output\", \"type\": \"string\"}]'\n    params: '[\n        {\n            \"name\": \"temperature\",\n            \"type\": \"float\",\n            \"default\": 0.5,\n            \"shape\": null\n        },\n        {\n            \"name\": \"suppress_tokens\",\n            \"type\": \"integer\",\n            \"default\": [101, 102],\n              \"shape\": [-1]\n        }\n    ]'\n```\n\n----------------------------------------\n\nTITLE: Defining Inference Parameters in MLflow Model Signature YAML Structure\nDESCRIPTION: Example YAML structure showing how to define inference parameters in an MLflow model signature, including scalar and list parameters with their types, default values, and shapes.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/signatures/index.mdx#2025-04-07_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsignature:\n    inputs: '[{\"name\": \"input\", \"type\": \"string\"}]'\n    outputs: '[{\"name\": \"output\", \"type\": \"string\"}]'\n    params: '[\n        {\n            \"name\": \"temperature\",\n            \"type\": \"float\",\n            \"default\": 0.5,\n            \"shape\": null\n        },\n        {\n            \"name\": \"suppress_tokens\",\n            \"type\": \"integer\",\n            \"default\": [101, 102],\n              \"shape\": [-1]\n        }\n    ]'\n```\n\n----------------------------------------\n\nTITLE: Executing MLflow Spark UDF Example with Prebuilt Environment in Python\nDESCRIPTION: This command demonstrates how to run the spark_udf_with_prebuilt_env.py example, which uses a prebuilt model environment for Spark UDF execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/spark_udf/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython spark_udf_with_prebuilt_env.py\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up MLflow Experiment Data and Artifacts\nDESCRIPTION: These commands remove the experiment data and artifacts created during the MLflow example run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/mlflow_artifacts/README.md#2025-04-07_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# Remove experiment and run data\n$ rm -rf mlruns\n\n# Remove artifacts\n$ rm -rf mlartifacts\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Gateway Server\nDESCRIPTION: Launches the gateway server using the specified YAML configuration file. The server starts on localhost:5000 by default.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/guides/step1-create-deployments/index.mdx#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlflow gateway start --config-path config.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoModule Documentation for MLflow PromptFlow\nDESCRIPTION: This Sphinx directive configures the automatic generation of documentation for the mlflow.promptflow module. It includes all members, undocumented members, and inheritance information from docstrings in the Python source code.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.promptflow.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.promptflow\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: MLflow Extra Requirements Files\nDESCRIPTION: Lists two main requirement files that document MLflow's extra dependencies: extra-ml-requirements.txt for ML library dependencies needed for model persistence and inference APIs, and test-requirements.txt for libraries required for non-default artifact-logging and tracking server configurations.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/EXTRA_DEPENDENCIES.rst#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* extra-ml-requirements.txt: ML libraries needed to use model persistence and inference APIs\n* test-requirements.txt: Libraries required to use non-default artifact-logging and tracking server configurations\n```\n\n----------------------------------------\n\nTITLE: Testing DialoGPT Conversational Context Retention\nDESCRIPTION: Tests the chatbot's ability to maintain conversation context by asking a follow-up question that relies on previous context, demonstrating MLflow's handling of stateful conversational models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/conversational-model.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Verify that the PyFunc implementation has maintained state on the conversation history by asking a vague follow-up question that requires context\n# in order to answer properly\nsecond = chatbot.predict(\"What sort of boat should I use?\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Predicting with Spark MLlib Models in MLflow\nDESCRIPTION: Example showing how to use Spark MLlib models with MLflow's pyfunc interface. The code creates a LogisticRegression model in Spark, fits it to training data, logs it to MLflow, and then loads the saved model for predictions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql import SparkSession\nimport mlflow\n\n# Prepare training data from a list of (label, features) tuples.\nspark = SparkSession.builder.appName(\"LogisticRegressionExample\").getOrCreate()\ntraining = spark.createDataFrame(\n    [\n        (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n        (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n        (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n        (1.0, Vectors.dense([0.0, 1.2, -0.5])),\n    ],\n    [\"label\", \"features\"],\n)\n\n# Create and fit a LogisticRegression instance\nlr = LogisticRegression(maxIter=10, regParam=0.01)\nlr_model = lr.fit(training)\n\n# Serialize the Model\nwith mlflow.start_run():\n    model_info = mlflow.spark.log_model(lr_model, \"spark-model\")\n\n# Load saved model\nlr_model_saved = mlflow.pyfunc.load_model(model_info.model_uri)\n```\n\n----------------------------------------\n\nTITLE: Connecting MLflow AI Gateway to Tracking Server\nDESCRIPTION: Commands to set the MLFLOW_DEPLOYMENTS_TARGET environment variable and start the MLflow Tracking Server, connecting it to the MLflow AI Gateway.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/prompt-engineering/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_DEPLOYMENTS_TARGET=\"http://127.0.0.1:7000\"\nmlflow server --port 5000\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation for MLflow Anthropic Module in RestructuredText\nDESCRIPTION: Sphinx documentation configuration for the MLflow Anthropic module. The directive instructs Sphinx to automatically generate documentation from the module, including all members, undocumented members, and inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.anthropic.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.anthropic\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Installing Node Module Dependencies for MLflow UI Development\nDESCRIPTION: Brew commands to install system dependencies required for MLflow UI development on macOS.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nbrew install pixman cairo pango jpeg\n```\n\n----------------------------------------\n\nTITLE: Opening MLflow UI to Track Experiments in Bash\nDESCRIPTION: Command to launch the MLflow user interface, which allows for visualization and comparison of experiment runs with different hyperparameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/xgboost/xgboost_native/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Using MLflow VizMod with Altair\nDESCRIPTION: Example of creating and logging a visualization model using MLflow VizMod and Altair with the Iris dataset.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/community-model-flavors/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_iris\nimport altair as alt\nimport mlflow_vismod\n\ndf_iris = load_iris(as_frame=True)\n\nviz_iris = (\n    alt.Chart(df_iris)\n    .mark_circle(size=60)\n    .encode(x=\"x\", y=\"y\", color=\"z:N\")\n    .properties(height=375, width=575)\n    .interactive()\n)\n\nmlflow_vismod.log_model(\n    model=viz_iris,\n    artifact_path=\"viz\",\n    style=\"vegalite\",\n    input_example=df_iris.head(5),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Levels for MLflow Python API\nDESCRIPTION: This code snippet demonstrates how to configure the log level for MLflow logs using the Python Logging API. It sets the log level to DEBUG, which provides more detailed logging information during execution.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/index.rst#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogger = logging.getLogger(\"mlflow\")\n\n# Set log level to debugging\nlogger.setLevel(logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: Generating Requirements File for MLflow with pip-compile\nDESCRIPTION: This code snippet provides instructions on how to regenerate the requirements.txt file using pip-compile. It specifies the Python version and the command to run for updating the dependencies.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/supply_chain_security/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n#\n# This file is autogenerated by pip-compile with python 3.8\n# To update, run:\n#\n#    pip-compile --generate-hashes --output-file=requirements.txt requirements.in\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Tracking Server\nDESCRIPTION: Command to start a local MLflow Tracking Server on a specified host and port.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/intro-quickstart/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --host 127.0.0.1 --port 8080\n```\n\n----------------------------------------\n\nTITLE: Tracing LiteLLM's Async API with MLflow\nDESCRIPTION: Example demonstrating how MLflow supports tracing asynchronous API calls with LiteLLM using the acompletion method.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracing/integrations/litellm.mdx#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmlflow.litellm.autolog()\n\nresponse = await litellm.acompletion(\n    model=\"claude-3-5-sonnet-20240620\",\n    messages=[{\"role\": \"user\", \"content\": \"Hey! how's it going?\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Error Message for Dangerous Deserialization in LangChain\nDESCRIPTION: This code snippet shows an error message that can occur when loading LangChain components that require pickle deserialization. This is related to LangChain's safety feature requiring explicit opt-in for pickle deserialization.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/index.mdx#2025-04-07_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nValueError: This code relies on the pickle module. You will need to set allow_dangerous_deserialization=True if you want to opt-in to\nallow deserialization of data using pickle. Data can be compromised by a malicious actor if not handled properly to include a malicious\npayload that when deserialized with pickle can execute arbitrary code on your machine.\n```\n\n----------------------------------------\n\nTITLE: Fixing Typos Automatically with Typos CLI\nDESCRIPTION: Command to automatically fix typos in the specified path using the typos CLI tool. This will write changes directly to the files.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/typos.md#2025-04-07_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ntypos --write-changes [PATH]\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Container Toolkit Dependencies\nDESCRIPTION: This shell script installs the necessary packages for the NVIDIA Container Toolkit, which is required for running GPU-accelerated containers.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/huggingface/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\\n  && \\\n    sudo apt-get update\n```\n\n----------------------------------------\n\nTITLE: Using the Older-Than Flag with MLflow GC Command\nDESCRIPTION: The `--older-than` flag for `mlflow gc` command enables removing runs based on their deletion time, providing more granular control over garbage collection.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_21\n\nLANGUAGE: Shell\nCODE:\n```\nmlflow gc --older-than <time_spec>\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Endpoint in MLflow AI Gateway\nDESCRIPTION: Example YAML configuration for setting up an Azure OpenAI endpoint with the MLflow AI Gateway. This configuration includes provider settings, model name, API connection details, and rate limiting parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  - name: completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-35-turbo\n      config:\n        openai_api_type: \"azuread\"\n        openai_api_key: $AZURE_AAD_TOKEN\n        openai_deployment_name: \"{your_deployment_name}\"\n        openai_api_base: \"https://{your_resource_name}-azureopenai.openai.azure.com/\"\n        openai_api_version: \"2023-05-15\"\n    limit:\n      renewal_period: minute\n      calls: 10\n```\n\n----------------------------------------\n\nTITLE: Generating R API Documentation\nDESCRIPTION: Commands to generate R API documentation files in RST format.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nmake rdocs\n```\n\n----------------------------------------\n\nTITLE: Setting Azure OpenAI API Key Environment Variable\nDESCRIPTION: Command to export the Azure OpenAI API key as an environment variable, which is required for accessing Azure OpenAI services through MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/azure_openai/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport OPENAI_API_KEY=...\n```\n\n----------------------------------------\n\nTITLE: Searching Runs in MLflow with R\nDESCRIPTION: Function for searching experiment runs that satisfy specific filter expressions. Allows filtering by metrics, parameters, and tags, and supports options for run view type, experiment selection, and result ordering.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_46\n\nLANGUAGE: r\nCODE:\n```\nmlflow_search_runs(\n  filter = NULL,\n  run_view_type = c(\"ACTIVE_ONLY\", \"DELETED_ONLY\", \"ALL\"),\n  experiment_ids = NULL,\n  order_by = list(),\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: MLflow Custom Flavor Dependency Management\nDESCRIPTION: Functions for managing model dependencies through pip and conda requirements\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndef get_default_pip_requirements(include_cloudpickle=False):\n    pip_deps = [_get_pinned_requirement(\"sktime\")]\n    if include_cloudpickle:\n        pip_deps += [_get_pinned_requirement(\"cloudpickle\")]\n\n    return pip_deps\n\n\ndef get_default_conda_env(include_cloudpickle=False):\n    return _mlflow_conda_env(\n        additional_pip_deps=get_default_pip_requirements(include_cloudpickle)\n    )\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with SHA256 Hashes for MLflow Project\nDESCRIPTION: A requirements file listing Python package dependencies with their versions and SHA256 hashes for integrity verification. The file includes packages like mlflow 1.20.2, numpy 1.21.4, packaging 21.2, pandas 1.3.2, prometheus-client 0.12.0, prometheus-flask-exporter 0.18.5, and protobuf 3.19.1.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/supply_chain_security/requirements.txt#2025-04-07_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:4efca8f86c54b22348a5467704e3fec767b2db12fc39c6d963168ab1d3fc9135 \\\n    --hash=sha256:53edb4da6925ad13c07b6d26c2a852bd81e364f95301c66e930ab2aef5b5ddd8 \\\n    --hash=sha256:5855f8438a7d1d458206a2466bf82b0f104a3724bf96a1c781ab731e4201731a \\\n    --hash=sha256:594c67807fb16238b30c44bdf74f36c02cdf22d1c8cda91ef8a0ed8dabf5620a \\\n    --hash=sha256:5b6d930f030f8ed98e3e6c98ffa0652bdb82601e7a016ec2ab5d7ff23baa78d1 \\\n    --hash=sha256:5bb28c636d87e840583ee3adeb78172efc47c8b26127267f54a9c0ec251d41a9 \\\n    --hash=sha256:60bf42e36abfaf9aff1f50f52644b336d4f0a3fd6d8a60ca0d054ac9f713a864 \\\n    --hash=sha256:611d1ad9a4288cf3e3c16014564df047fe08410e628f89805e475368bd304914 \\\n    --hash=sha256:6300b8454aa6930a24b9618fbb54b5a68135092bc666f7b06901f897fa5c2fee \\\n    --hash=sha256:63f3268ba69ace99cab4e3e3b5840b03340efed0948ab8f78d2fd87ee5442a4f \\\n    --hash=sha256:6557b31b5e2c9ddf0de32a691f2312a32f77cd7681d8af66c2692efdbef84c18 \\\n    --hash=sha256:693ce3f9e70a6cf7d2fb9e6c9d8b204b6b39897a2c4a1aa65728d5ac97dcc1d8 \\\n    --hash=sha256:6a7fae0dd14cf60ad5ff42baa2e95727c3d81ded453457771d02b7d2b3f9c0c2 \\\n    --hash=sha256:6c4ca60fa24e85fe25b912b01e62cb969d69a23a5d5867682dd3e80b5b02581d \\\n    --hash=sha256:6fcf051089389abe060c9cd7caa212c707e58153afa2c649f00346ce6d260f1b \\\n    --hash=sha256:7d91275b0245b1da4d4cfa07e0faedd5b0812efc15b702576d103293e252af1b \\\n    --hash=sha256:89c687013cb1cd489a0f0ac24febe8c7a666e6e221b783e53ac50ebf68e45d86 \\\n    --hash=sha256:8d206346619592c6200148b01a2142798c989edcb9c896f9ac9722a99d4e77e6 \\\n    --hash=sha256:905fec760bd2fa1388bb5b489ee8ee5f7291d692638ea5f67982d968366bef9f \\\n    --hash=sha256:97383d78eb34da7e1fa37dd273c20ad4320929af65d156e35a5e2d89566d9dfb \\\n    --hash=sha256:984d76483eb32f1bcb536dc27e4ad56bba4baa70be32fa87152832cdd9db0833 \\\n    --hash=sha256:99df47edb6bda1249d3e80fdabb1dab8c08ef3975f69aed437cb69d0a5de1e28 \\\n    --hash=sha256:9f02365d4e99430a12647f09b6cc8bab61a6564363f313126f775eb4f6ef798e \\\n    --hash=sha256:a30e67a65b53ea0a5e62fe23682cfe22712e01f453b95233b25502f7c61cb415 \\\n    --hash=sha256:ab3ef638ace319fa26553db0624c4699e31a28bb2a835c5faca8f8acf6a5a902 \\\n    --hash=sha256:aca6377c0cb8a8253e493c6b451565ac77e98c2951c45f913e0b52facdcff83f \\\n    --hash=sha256:add36cb2dbb8b736611303cd3bfcee00afd96471b09cda130da3581cbdc56a6d \\\n    --hash=sha256:b2f4bf27480f5e5e8ce285a8c8fd176c0b03e93dcc6646477d4630e83440c6a9 \\\n    --hash=sha256:b7f2d075102dc8c794cbde1947378051c4e5180d52d276987b8d28a3bd58c17d \\\n    --hash=sha256:baa1a4e8f868845af802979fcdbf0bb11f94f1cb7ced4c4b8a351bb60d108145 \\\n    --hash=sha256:be98f628055368795d818ebf93da628541e10b75b41c559fdf36d104c5787066 \\\n    --hash=sha256:bf5d821ffabf0ef3533c39c518f3357b171a1651c1ff6827325e4489b0e46c3c \\\n    --hash=sha256:c47adbc92fc1bb2b3274c4b3a43ae0e4573d9fbff4f54cd484555edbf030baf1 \\\n    --hash=sha256:cdfba22ea2f0029c9261a4bd07e830a8da012291fbe44dc794e488b6c9bb353a \\\n    --hash=sha256:d6c7ebd4e944c85e2c3421e612a7057a2f48d478d79e61800d81468a8d842207 \\\n    --hash=sha256:d7f9850398e85aba693bb640262d3611788b1f29a79f0c93c565694658f4071f \\\n    --hash=sha256:d8446c54dc28c01e5a2dbac5a25f071f6653e6e40f3a8818e8b45d790fe6ef53 \\\n    --hash=sha256:deb993cacb280823246a026e3b2d81c493c53de6acfd5e6bfe31ab3402bb37dd \\\n    --hash=sha256:e0f138900af21926a02425cf736db95be9f4af72ba1bb21453432a07f6082134 \\\n    --hash=sha256:e9936f0b261d4df76ad22f8fee3ae83b60d7c3e871292cd42f40b81b70afae85 \\\n    --hash=sha256:f0567c4dc99f264f49fe27da5f735f414c4e7e7dd850cfd8e69f0862d7c74ea9 \\\n    --hash=sha256:f5653a225f31e113b152e56f154ccbe59eeb1c7487b39b9d9f9cdb58e6c79dc5 \\\n    --hash=sha256:f826e31d18b516f653fe296d967d700fddad5901ae07c622bb3705955e1faa94 \\\n    --hash=sha256:f8ba0e8349a38d3001fae7eadded3f6606f0da5d748ee53cc1dab1d6527b9509 \\\n    --hash=sha256:f9081981fe268bd86831e5c75f7de206ef275defcb82bc70740ae6dc507aee51 \\\n    --hash=sha256:fa130dd50c57d53368c9d59395cb5526eda596d3ffe36666cd81a44d56e48872\n    # via\n    #   jinja2\n    #   mako\nmlflow==1.20.2 \\\n    --hash=sha256:963c22532e82a93450674ab97d62f9e528ed0906b580fadb7c003e696197557c \\\n    --hash=sha256:b15ff0c7e5e64f864a0b40c99b9a582227315eca2065d9f831db9aeb8f24637b\n    # via -r requirements.in\nnumpy==1.21.4 \\\n    --hash=sha256:0b78ecfa070460104934e2caf51694ccd00f37d5e5dbe76f021b1b0b0d221823 \\\n    --hash=sha256:1247ef28387b7bb7f21caf2dbe4767f4f4175df44d30604d42ad9bd701ebb31f \\\n    --hash=sha256:1403b4e2181fc72664737d848b60e65150f272fe5a1c1cbc16145ed43884065a \\\n    --hash=sha256:170b2a0805c6891ca78c1d96ee72e4c3ed1ae0a992c75444b6ab20ff038ba2cd \\\n    --hash=sha256:2e4ed57f45f0aa38beca2a03b6532e70e548faf2debbeb3291cfc9b315d9be8f \\\n    --hash=sha256:32fe5b12061f6446adcbb32cf4060a14741f9c21e15aaee59a207b6ce6423469 \\\n    --hash=sha256:34f3456f530ae8b44231c63082c8899fe9c983fd9b108c997c4b1c8c2d435333 \\\n    --hash=sha256:4c9c23158b87ed0e70d9a50c67e5c0b3f75bcf2581a8e34668d4e9d7474d76c6 \\\n    --hash=sha256:5d95668e727c75b3f5088ec7700e260f90ec83f488e4c0aaccb941148b2cd377 \\\n    --hash=sha256:615d4e328af7204c13ae3d4df7615a13ff60a49cb0d9106fde07f541207883ca \\\n    --hash=sha256:69077388c5a4b997442b843dbdc3a85b420fb693ec8e33020bb24d647c164fa5 \\\n    --hash=sha256:74b85a17528ca60cf98381a5e779fc0264b4a88b46025e6bcbe9621f46bb3e63 \\\n    --hash=sha256:81225e58ef5fce7f1d80399575576fc5febec79a8a2742e8ef86d7b03beef49f \\\n    --hash=sha256:8890b3360f345e8360133bc078d2dacc2843b6ee6059b568781b15b97acbe39f \\\n    --hash=sha256:92aafa03da8658609f59f18722b88f0a73a249101169e28415b4fa148caf7e41 \\\n    --hash=sha256:9864424631775b0c052f3bd98bc2712d131b3e2cd95d1c0c68b91709170890b0 \\\n    --hash=sha256:9e6f5f50d1eff2f2f752b3089a118aee1ea0da63d56c44f3865681009b0af162 \\\n    --hash=sha256:a3deb31bc84f2b42584b8c4001c85d1934dbfb4030827110bc36bfd11509b7bf \\\n    --hash=sha256:ad010846cdffe7ec27e3f933397f8a8d6c801a48634f419e3d075db27acf5880 \\\n    --hash=sha256:b1e2312f5b8843a3e4e8224b2b48fe16119617b8fc0a54df8f50098721b5bed2 \\\n    --hash=sha256:bc988afcea53e6156546e5b2885b7efab089570783d9d82caf1cfd323b0bb3dd \\\n    --hash=sha256:c449eb870616a7b62e097982c622d2577b3dbc800aaf8689254ec6e0197cbf1e \\\n    --hash=sha256:c74c699b122918a6c4611285cc2cad4a3aafdb135c22a16ec483340ef97d573c \\\n    --hash=sha256:c885bfc07f77e8fee3dc879152ba993732601f1f11de248d4f357f0ffea6a6d4 \\\n    --hash=sha256:e3c3e990274444031482a31280bf48674441e0a5b55ddb168f3a6db3e0c38ec8 \\\n    --hash=sha256:e4799be6a2d7d3c33699a6f77201836ac975b2e1b98c2a07f66a38f499cb50ce \\\n    --hash=sha256:e6c76a87633aa3fa16614b61ccedfae45b91df2767cf097aa9c933932a7ed1e0 \\\n    --hash=sha256:e89717274b41ebd568cd7943fc9418eeb49b1785b66031bc8a7f6300463c5898 \\\n    --hash=sha256:f5162ec777ba7138906c9c274353ece5603646c6965570d82905546579573f73 \\\n    --hash=sha256:fde96af889262e85aa033f8ee1d3241e32bf36228318a61f1ace579df4e8170d\n    # via\n    #   mlflow\n    #   pandas\n    #   scikit-learn\n    #   scipy\npackaging==21.2 \\\n    --hash=sha256:096d689d78ca690e4cd8a89568ba06d07ca097e3306a4381635073ca91479966 \\\n    --hash=sha256:14317396d1e8cdb122989b916fa2c7e9ca8e2be9e8060a6eff75b6b7b4d8a7e0\n    # via mlflow\npandas==1.3.2 \\\n    --hash=sha256:0cd5776be891331a3e6b425b5abeab9596abea18435c5982191356f9b24ae731 \\\n    --hash=sha256:1099e2a0cd3a01ec62cca183fc1555833a2d43764950ef8cb5948c8abfc51014 \\\n    --hash=sha256:132def05e73d292c949b02e7ef873debb77acc44a8b119d215921046f0c3a91d \\\n    --hash=sha256:1738154049062156429a5cf2fd79a69c9f3fa4f231346a7ec6fd156cd1a9a621 \\\n    --hash=sha256:34ced9ce5d5b17b556486da7256961b55b471d64a8990b56e67a84ebeb259416 \\\n    --hash=sha256:53b17e4debba26b7446b1e4795c19f94f0c715e288e08145e44bdd2865e819b3 \\\n    --hash=sha256:59a78d7066d1c921a77e3306aa0ebf6e55396c097d5dfcc4df8defe3dcecb735 \\\n    --hash=sha256:66a95361b81b4ba04b699ecd2416b0591f40cd1e24c60a8bfe0d19009cfa575a \\\n    --hash=sha256:69e1b2f5811f46827722fd641fdaeedb26002bd1e504eacc7a8ec36bdc25393e \\\n    --hash=sha256:7996d311413379136baf0f3cf2a10e331697657c87ced3f17ac7c77f77fe34a3 \\\n    --hash=sha256:89f40e5d21814192802421df809f948247d39ffe171e45fe2ab4abf7bd4279d8 \\\n    --hash=sha256:9cce01f6d655b4add966fcd36c32c5d1fe84628e200626b3f5e2f40db2d16a0f \\\n    --hash=sha256:a56246de744baf646d1f3e050c4653d632bc9cd2e0605f41051fea59980e880a \\\n    --hash=sha256:ba7ceb8abc6dbdb1e34612d1173d61e4941f1a1eb7e6f703b2633134ae6a6c89 \\\n    --hash=sha256:c9e8e0ce5284ebebe110efd652c164ed6eab77f5de4c3533abc756302ee77765 \\\n    --hash=sha256:cbcb84d63867af3411fa063af3de64902665bb5b3d40b25b2059e40603594e87 \\\n    --hash=sha256:f07a9745ca075ae73a5ce116f5e58f691c0dc9de0bff163527858459df5c176f \\\n    --hash=sha256:fa54dc1d3e5d004a09ab0b1751473698011ddf03e14f1f59b84ad9a6ac630975 \\\n    --hash=sha256:fcb71b1935249de80e3a808227189eee381d4d74a31760ced2df21eedc92a8e3\n    # via\n    #   -r requirements.in\n    #   mlflow\nprometheus-client==0.12.0 \\\n    --hash=sha256:1b12ba48cee33b9b0b9de64a1047cbd3c5f2d0ab6ebcead7ddda613a750ec3c5 \\\n    --hash=sha256:317453ebabff0a1b02df7f708efbab21e3489e7072b61cb6957230dd004a0af0\n    # via prometheus-flask-exporter\nprometheus-flask-exporter==0.18.5 \\\n    --hash=sha256:38a3a1fdaf4fc98f988d33f551a8005d778d6b43ca0a2bc4aafb19d0449a48b9 \\\n    --hash=sha256:f9a03e88a8415fe96f785c31fc82bbd290a606aaab87bd244414637d55ef0ba4\n    # via mlflow\nprotobuf==3.19.1 \\\n    --hash=sha256:038daf4fa38a7e818dd61f51f22588d61755160a98db087a046f80d66b855942 \\\n    --hash=sha256:28ccea56d4dc38d35cd70c43c2da2f40ac0be0a355ef882242e8586c6d66666f \\\n    --hash=sha256:36d90676d6f426718463fe382ec6274909337ca6319d375eebd2044e6c6ac560 \\\n    --hash=sha256:3cd0458870ea7d1c58e948ac8078f6ba8a7ecc44a57e03032ed066c5bb318089 \\\n    --hash=sha256:5935c8ce02e3d89c7900140a8a42b35bc037ec07a6aeb61cc108be8d3c9438a6 \\\n    --hash=sha256:615b426a177780ce381ecd212edc1e0f70db8557ed72560b82096bd36b01bc04 \\\n    --hash=sha256:62a8e4baa9cb9e064eb62d1002eca820857ab2138440cb4b3ea4243830f94ca7 \\\n    --hash=sha256:655264ed0d0efe47a523e2255fc1106a22f6faab7cc46cfe99b5bae085c2a13e \\\n    --hash=sha256:6e8ea9173403219239cdfd8d946ed101f2ab6ecc025b0fda0c6c713c35c9981d \\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key Environment Variable in Shell\nDESCRIPTION: Demonstrates how to export the Anthropic API key as an environment variable for use with MLflow. Users need to replace the placeholder with their actual API key obtained from Anthropic's service.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/anthropic/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport ANTHROPIC_API_KEY=...\n```\n\n----------------------------------------\n\nTITLE: Valid Embeddings JSON Response Format\nDESCRIPTION: Example of a valid JSON response format for embeddings endpoint that meets MLflow AI Gateway requirements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"predictions\": [\n    [0.0, 0.1],\n    [1.0, 0.0]\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: MLflow Custom Flavor Model Loading\nDESCRIPTION: Functions for loading saved models from filesystem or MLflow model registry\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/index.mdx#2025-04-07_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndef load_model(model_uri, dst_path=None):\n    local_model_path = _download_artifact_from_uri(\n        artifact_uri=model_uri, output_path=dst_path\n    )\n    flavor_conf = _get_flavor_configuration(\n        model_path=local_model_path, flavor_name=FLAVOR_NAME\n    )\n    _add_code_from_conf_to_system_path(local_model_path, flavor_conf)\n    sktime_model_file_path = os.path.join(\n        local_model_path, flavor_conf[\"pickled_model\"]\n    )\n    serialization_format = flavor_conf.get(\n        \"serialization_format\", SERIALIZATION_FORMAT_PICKLE\n    )\n    return _load_model(\n        path=sktime_model_file_path, serialization_format=serialization_format\n    )\n\n\ndef _load_model(path, serialization_format):\n    with open(path, \"rb\") as pickled_model:\n        if serialization_format == SERIALIZATION_FORMAT_PICKLE:\n            return pickle.load(pickled_model)\n        elif serialization_format == SERIALIZATION_FORMAT_CLOUDPICKLE:\n            import cloudpickle\n\n            return cloudpickle.load(pickled_model)\n```\n\n----------------------------------------\n\nTITLE: Starting Jupyter Notebook with Poetry\nDESCRIPTION: Command to start a Jupyter Notebook server within the Poetry virtual environment after installation is complete.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/llama_index/workflow/README.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npoetry run jupyter notebook\n```\n\n----------------------------------------\n\nTITLE: Installing Prerequisites for MLflow Spark UDF Examples in Python\nDESCRIPTION: This snippet shows the command to install the scikit-learn library, which is a prerequisite for running the MLflow Spark UDF examples.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/spark_udf/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Experiment Name\nDESCRIPTION: Sets the name for the MLflow experiment to organize tracking data under a specific project namespace.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/deep-learning/pytorch/quickstart/pytorch_quickstart.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"/Users/<your email>/mlflow-pytorch-quickstart\")\n```\n\n----------------------------------------\n\nTITLE: Installing Clint for MLflow\nDESCRIPTION: This command installs Clint, the custom linter for MLflow, in editable mode from the dev/clint directory.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/dev/clint/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e dev/clint\n```\n\n----------------------------------------\n\nTITLE: Testing with Docker Container using MLflow build-docker\nDESCRIPTION: Demonstrates how to build and run a Docker container for model serving to test the model in an environment closer to production using the MLflow build-docker API.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/model/dependencies/index.mdx#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nmlflow models build-docker -m runs:/<run_id>/model -n <image_name>\ndocker run -p <port>:8080 <image_name>\n# In another terminal\ncurl -X POST -H \"Content-Type: application/json\" \\\n    --data '{\"inputs\": [[1, 2], [3, 4]]}' \\\n    http://localhost:<port>/invocations\n```\n\n----------------------------------------\n\nTITLE: Querying MPT-7B Model Endpoint\nDESCRIPTION: Python code example for querying the MPT-7B-instruct endpoint with specific parameters.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/mlflow_models/README.md#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Querying the optional mpt-7b-instruct endpoint\nresponse_mpt = query(\n    endpoint=\"mpt-instruct\",\n    data={\n        \"prompt\": \"What is the purpose of an attention mask in a transformers model?\",\n        \"temperature\": 0.1,\n        \"max_tokens\": 200,\n    },\n)\nprint(f\"Fluent API response for mpt-instruct: {response_mpt}\")\n```\n\n----------------------------------------\n\nTITLE: Listing Supported PySpark and SynapseML Models by Category\nDESCRIPTION: Comprehensive catalog of machine learning model classes supported in the PySpark ML and SynapseML ecosystem, organized by functional categories including classification, clustering, regression, feature engineering, pipeline components, and specialized modules.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/mlflow/pyspark/ml/log_model_allowlist.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# classification\npyspark.ml.classification.LinearSVCModel\npyspark.ml.classification.DecisionTreeClassificationModel\npyspark.ml.classification.GBTClassificationModel\npyspark.ml.classification.LogisticRegressionModel\npyspark.ml.classification.RandomForestClassificationModel\npyspark.ml.classification.NaiveBayesModel\n\n# clustering\npyspark.ml.clustering.BisectingKMeansModel\npyspark.ml.clustering.KMeansModel\npyspark.ml.clustering.GaussianMixtureModel\n\n# Regression\npyspark.ml.regression.AFTSurvivalRegressionModel\npyspark.ml.regression.DecisionTreeRegressionModel\npyspark.ml.regression.GBTRegressionModel\npyspark.ml.regression.GeneralizedLinearRegressionModel\npyspark.ml.regression.LinearRegressionModel\npyspark.ml.regression.RandomForestRegressionModel\n\n# Featurizer model\npyspark.ml.feature.BucketedRandomProjectionLSHModel\npyspark.ml.feature.ChiSqSelectorModel\npyspark.ml.feature.CountVectorizerModel\npyspark.ml.feature.IDFModel\npyspark.ml.feature.ImputerModel\npyspark.ml.feature.MaxAbsScalerModel\npyspark.ml.feature.MinHashLSHModel\npyspark.ml.feature.MinMaxScalerModel\npyspark.ml.feature.OneHotEncoderModel\npyspark.ml.feature.RobustScalerModel\npyspark.ml.feature.RFormulaModel\npyspark.ml.feature.StandardScalerModel\npyspark.ml.feature.StringIndexerModel\npyspark.ml.feature.VarianceThresholdSelectorModel\npyspark.ml.feature.VectorIndexerModel\npyspark.ml.feature.UnivariateFeatureSelectorModel\n\n# composite model\npyspark.ml.classification.OneVsRestModel\n\n# pipeline model\npyspark.ml.pipeline.PipelineModel\n\n# Hyper-parameter tuning\npyspark.ml.tuning.CrossValidatorModel\npyspark.ml.tuning.TrainValidationSplitModel\n\n# SynapeML models\nsynapse.ml.cognitive.*\nsynapse.ml.exploratory.*\nsynapse.ml.featurize.*\nsynapse.ml.geospatial.*\nsynapse.ml.image.*\nsynapse.ml.io.*\nsynapse.ml.isolationforest.*\nsynapse.ml.lightgbm.*\nsynapse.ml.nn.*\nsynapse.ml.opencv.*\nsynapse.ml.stages.*\nsynapse.ml.vw.*\n```\n\n----------------------------------------\n\nTITLE: Training and Logging PyTorch Model with MLflow\nDESCRIPTION: Trains the PyTorch model and logs metrics, parameters, and model artifacts to MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pytorch/logging/pytorch_log_model.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmlflow.set_experiment(\"iris_classification_pytorch\")\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    accuracy_metric = torchmetrics.Accuracy(\n        task=\"multiclass\", num_classes=3\n    )  # Instantiate the Accuracy metric\n\n    for epoch in range(5):  # number of epochs\n        total_loss = 0\n        total_accuracy = 0\n\n        for inputs, labels in train_loader:\n            outputs = model(inputs)\n            curr_loss = loss(outputs, labels)\n            curr_loss.backward()\n\n            total_loss += curr_loss.item()\n\n            # Calculate accuracy using torchmetrics\n            _, preds = torch.max(outputs, 1)\n            total_accuracy += accuracy_metric(preds, labels).item()\n\n        avg_loss = total_loss / len(train_loader)\n        avg_accuracy = total_accuracy / len(train_loader)\n\n        print(f\"Epoch {epoch + 1}, Loss: {avg_loss}, Accuracy: {avg_accuracy}\")\n        mlflow.log_metric(\"loss\", avg_loss, step=epoch)\n        mlflow.log_metric(\"accuracy\", avg_accuracy, step=epoch)\n\n    # Log the PyTorch model with the signature\n    mlflow.pytorch.log_model(model, \"model\", signature=signature)\n\n    # Log parameters\n    mlflow.log_param(\"epochs\", 10)\n    mlflow.log_param(\"batch_size\", 16)\n    mlflow.log_param(\"learning_rate\", 0.001)\n\nprint(\"Model training and logging complete.\")\n```\n\n----------------------------------------\n\nTITLE: Downloading MLflow Artifacts in R\nDESCRIPTION: Function to download artifact files or directories from an MLflow run to a local directory.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_14\n\nLANGUAGE: r\nCODE:\n```\nmlflow_download_artifacts(path, run_id = NULL, client = NULL)\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Platform for ARM-based Systems\nDESCRIPTION: Environment variable setting required for building R documentation on ARM-based platforms like Apple Silicon M1/M2.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_DEFAULT_PLATFORM=linux/amd64\n```\n\n----------------------------------------\n\nTITLE: Setting Null Next Page Token in MLflow API\nDESCRIPTION: Update to set the nextPageToken to null in an MLflow API response.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nnextPageToken = null\n```\n\n----------------------------------------\n\nTITLE: MLflow Experiment Search Function\nDESCRIPTION: Function signature for searching MLflow experiments with filtering, pagination and ordering capabilities.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/R-api.rst#2025-04-07_snippet_44\n\nLANGUAGE: R\nCODE:\n```\nmlflow_search_experiments(\n  filter = NULL,\n  experiment_view_type = c(\"ACTIVE_ONLY\", \"DELETED_ONLY\", \"ALL\"),\n  max_results = 1000,\n  order_by = list(),\n  page_token = NULL,\n  client = NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Logging LangChain Agent with MLflow\nDESCRIPTION: Shows how to create and log a more complex LangChain Agent that uses multiple tools (serpapi and llm-math) with MLflow. The example demonstrates agent initialization, logging, and making predictions with compound queries.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/langchain/guide/index.mdx#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\nimport mlflow\n\n# Note: Ensure that the package 'google-search-results' is installed via pypi to run this example\n# and that you have a accounts with SerpAPI and OpenAI to use their APIs.\n\n# Ensuring necessary API keys are set\nassert (\n    \"OPENAI_API_KEY\" in os.environ\n), \"Please set the OPENAI_API_KEY environment variable.\"\nassert (\n    \"SERPAPI_API_KEY\" in os.environ\n), \"Please set the SERPAPI_API_KEY environment variable.\"\n\n# Load the language model for agent control\nllm = OpenAI(temperature=0)\n\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n\n# Log the agent in an MLflow run\nwith mlflow.start_run():\n    logged_model = mlflow.langchain.log_model(agent, \"langchain_model\")\n\n# Load the logged agent model for prediction\nloaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n\n# Generate an inference result using the loaded model\nquestion = \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n\nanswer = loaded_model.predict([{\"input\": question}])\n\nprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow with pip\nDESCRIPTION: Command to install MLflow from PyPI using pip. This is the first step to set up the environment for running MLflow tutorials.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/getting-started/running-notebooks/index.mdx#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Testing Advanced Query Parameters with Warning Mechanism\nDESCRIPTION: Executes a prediction with custom parameters to test the model's warning mechanism and fallback logic for strict relevancy thresholds.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/sentence-transformers/tutorials/semantic-search/semantic-search-sentence-transformers.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nloaded_dynamic.predict(\n    [\"Latest stories on computing\"], params={\"top_k\": 10, \"minimum_relevancy\": 0.4}\n)\n```\n\n----------------------------------------\n\nTITLE: Get Registered Model Endpoint\nDESCRIPTION: REST endpoint for retrieving a registered model. Requires the model name as input parameter and returns the registered model details.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_12\n\nLANGUAGE: rest\nCODE:\n```\nGET 2.0/mlflow/registered-models/get\n```\n\n----------------------------------------\n\nTITLE: Training PyFunc Model with Inferred Paths\nDESCRIPTION: Alternative command to train and log the model using inferred code paths.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/pyfunc/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python infer_model_code_paths.py\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key as Environment Variable in Bash\nDESCRIPTION: This Bash command shows how to set the OpenAI API key as an environment variable, which is a recommended security practice for handling sensitive credentials.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/deployments/index.mdx#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your_openai_api_key\"\n```\n\n----------------------------------------\n\nTITLE: Executing One-Liner Function\nDESCRIPTION: A simple execution of the one_liner function with an input of 10 to demonstrate the factorial calculation.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\none_liner(10)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for MLflow spaCy Module\nDESCRIPTION: ReStructuredText directive for generating API documentation from the MLflow spaCy module. Includes configuration for showing all members, undocumented members, and inheritance relationships.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.spacy.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: mlflow.spacy\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Containers\nDESCRIPTION: Launches the Docker containers for PostgreSQL and MinIO using Docker Compose.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/tracking/tutorials/remote-server/index.mdx#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow ONNX Module using reStructuredText\nDESCRIPTION: This code snippet configures Sphinx documentation for the MLflow ONNX module. It uses the automodule directive to generate documentation for all members, undocumented members, and inheritance information in the mlflow.onnx module.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.onnx.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: mlflow.onnx\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Specifying Pip Requirements in MLflow Model Logging\nDESCRIPTION: New parameters 'pip_requirements' and 'extra_pip_requirements' added to model logging and saving functions to directly specify pip requirements.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nmlflow.*.log_model(..., pip_requirements=..., extra_pip_requirements=...)\nmlflow.*.save_model(..., pip_requirements=..., extra_pip_requirements=...)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables and Managing Warnings for Translation Pipeline\nDESCRIPTION: Configures environment variables to disable tokenizers parallelism warnings and filters out less-useful UserWarnings from setuptools and pydantic libraries. This ensures a cleaner console output when working with the translation pipeline.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/translation/component-translation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Disable tokenizers warnings when constructing pipelines\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Disabling Warnings for DialoGPT\nDESCRIPTION: Configures the environment by disabling tokenizers parallelism and filtering out less useful warnings from setuptools and pydantic to prepare for working with DialoGPT.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/transformers/tutorials/conversational/conversational-model.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Disable tokenizers warnings when constructing pipelines\n%env TOKENIZERS_PARALLELISM=false\n\nimport warnings\n\n# Disable a few less-than-useful UserWarnings from setuptools and pydantic\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Setting TogetherAI API Key for MLflow Integration\nDESCRIPTION: This snippet shows how to set the TogetherAI API key as an environment variable. This key is required for authenticating requests to TogetherAI's services when using MLflow.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/gateway/togetherai/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport TOGETHERAI_API_KEY=...\n```\n\n----------------------------------------\n\nTITLE: Example Log Batch JSON Request for MLflow\nDESCRIPTION: Example JSON request structure for the log-batch endpoint that demonstrates how to log metrics and parameters in a single API call. The request shows logging two metrics (mae and rmse) and one parameter (model_class) for a specific run.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/rest-api.rst#2025-04-07_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"run_id\": \"2a14ed5c6a87499199e0106c3501eab8\",\n   \"metrics\": [\n     {\"key\": \"mae\", \"value\": 2.5, \"timestamp\": 1552550804},\n     {\"key\": \"rmse\", \"value\": 2.7, \"timestamp\": 1552550804},\n   ],\n   \"params\": [\n     {\"key\": \"model_class\", \"value\": \"LogisticRegression\"},\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for MLflow Project\nDESCRIPTION: Bash command to build a Docker image for the MLflow project environment. The image is tagged as 'mlflow-docker-example' to match the name referenced in the MLproject file.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/examples/docker/README.rst#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t mlflow-docker-example -f Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Configuring Git for MLflow Development\nDESCRIPTION: Git configuration commands to set up user name and email, and install pre-commit hooks for code formatting.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit config --global user.name \"Your Name\"\ngit config --global user.email yourname@example.com\n\npre-commit install -t pre-commit -t prepare-commit-msg\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow Entities Module in reStructuredText\nDESCRIPTION: This snippet uses the automodule directive to generate documentation for the mlflow.entities module. It includes all members and undocumented members.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.entities.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: mlflow.entities\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Documenting MLflow Model Registry Entities in reStructuredText\nDESCRIPTION: This snippet uses the automodule directive to generate documentation for the mlflow.entities.model_registry module. It includes all members and undocumented members.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/api_reference/source/python_api/mlflow.entities.rst#2025-04-07_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: mlflow.entities.model_registry\n    :members:\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Complete MLflow Run Test Function Definition\nDESCRIPTION: Complete example showing how to add a new test case to the existing test_mlflow_run_example function's parametrize decorator.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/tests/examples/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize((\"directory\", \"params\"), [\n    (\"sklearn_elasticnet_wine\", [\"-P\", \"alpha=0.5\"]),\n    (os.path.join(\"sklearn_elasticnet_diabetes\", \"linux\"), []),\n    (\"new_example_dir\", [\"-P\", \"param1=123\", \"-P\", \"param2=99\"]),\n])\ndef test_mlflow_run_example(directory, params):\n```\n\n----------------------------------------\n\nTITLE: Implementing Summing Function with Code Inspector in Python\nDESCRIPTION: A function that calculates cumulative sums using a dictionary to store intermediate results. The implementation is decorated with code_inspector for real-time analysis but contains inefficiencies in its logic and unnecessary type conversions.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/docs/docs/llms/openai/notebooks/openai-code-helper.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@code_inspector(loaded_helper)\ndef summing_function(n):\n    sum_result = 0\n\n    intermediate_sums = {}\n\n    for i in range(1, n + 1):\n        intermediate_sums[str(i)] = sum(x for x in range(1, i + 1))\n        for key in intermediate_sums:\n            if key == str(i):\n                sum_result = intermediate_sums[key]  # noqa: F841\n\n    final_sum = sum([intermediate_sums[key] for key in intermediate_sums if int(key) == n])\n\n    return int(str(final_sum))\n```\n\n----------------------------------------\n\nTITLE: Downloading Model Dependencies in Python\nDESCRIPTION: Uses mlflow.pyfunc.get_model_dependencies() to download all referenced requirements files for specified models.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmlflow.pyfunc.get_model_dependencies()\n```\n\n----------------------------------------\n\nTITLE: Accessing MLflow Tracking URI Status in Python\nDESCRIPTION: Shows how to use the newly exposed mlflow.tracking.is_tracking_uri_set() function to check if a tracking URI has been set.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md#2025-04-07_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\nis_set = mlflow.tracking.is_tracking_uri_set()\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Requirements for MLflow Documentation\nDESCRIPTION: This requirements file lists the necessary Python packages and their versions for building MLflow documentation. It includes machine learning frameworks, data processing libraries, and tools for rendering Jupyter notebooks. Some packages have specific version constraints to ensure compatibility.\nSOURCE: https://github.com/mlflow/mlflow/blob/master/requirements/doc-requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-r doc-min-requirements.txt\ntensorflow-cpu<=2.12.0\n# tensorflow-macos<=2.12.0  # Comment out the line above and uncomment this line if setting up dev\n                            # environment on local ARM macOS.\n                            # Only do this for the purpose of setting up the dev environment, do not\n                            # commit this change to the repo.\npyspark\ndatasets\n# nbsphinx and ipython are required for jupyter notebook rendering\nnbsphinx==0.8.8\n# ipython 8.7.0 is an incompatible release\nipython!=8.7.0\nkeras\ntorch>=1.11.0\ntorchvision>=0.12.0\nlightning>=1.8.1\nscrapy\nipywidgets>=8.1.1\n# incremental==24.7.0 requires setuptools>=61.0, which causes https://github.com/mlflow/mlflow/issues/8635\nincremental<24.7.0\n# this is an extra dependency for the auth app which\n# is not included in the core mlflow requirements\nFlask-WTF<2\n```"
  }
]
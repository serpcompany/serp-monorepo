[
  {
    "owner": "pku-alignment",
    "repo": "align-anything",
    "content": "TITLE: DPO Training Script Configuration\nDESCRIPTION: Bash script for configuring and running DPO training with deepspeed\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/en/text_to_text_dpo.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME_OR_PATH=\"meta-llama/Llama-3.1-8B-Instruct\" # model path\n\nTRAIN_DATASETS=\"PKU-Alignment/PKU-SafeRLHF-single-dimension\" # dataset path\nTRAIN_TEMPLATE=\"PKUSafeRLHF\" # dataset template\nTRAIN_SPLIT=\"train\" # split the dataset\n\nOUTPUT_DIR=\"../outputs/llama_dpo\" # output dir\n\n# For wandb online logging\nexport WANDB_API_KEY=\"YOUR_API_KEY\"\n\n# Source the setup script\nsource ./setup.sh\n\n# Execute deepspeed command\ndeepspeed \\\n     --master_port ${MASTER_PORT} \\\n     --module align_anything.trainers.text_to_text.dpo \\\n     --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n     --train_template ${TRAIN_TEMPLATE} \\\n     --train_datasets ${TRAIN_DATASETS} \\\n     --train_split ${TRAIN_SPLIT} \\\n     --output_dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: Custom MiniCPM-V Model Implementation\nDESCRIPTION: Implements a customized version of the MiniCPM-V model that adapts it to work within the align-anything framework, including handling ZeRO optimization stages and providing custom inference functions.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass AccustomedMiniCPMV(MiniCPMV):\n   \"\"\"Accustomed Interface for MiniCPM-V model\"\"\"\n\n   def __init__(self, config: AutoConfig):\n      super().__init__(config)\n      zero_stage = int(os.environ['ZERO_STAGE'])\n      if zero_stage == 3:\n            raise ValueError('MiniCPM-V does not support ZeRO stage 3')\n      self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n\n   @property\n   def processor_available(self):\n      return False\n\n   def infer_batch(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n      \"\"\"Return the dict used for model inference\"\"\"\n      return {\n            'input_ids': batch['input_ids'],\n            'attention_mask': batch['attention_mask'],\n            'images': batch['images'],\n            'labels': batch.get('labels'),\n      }\n\n   ...\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained Models in align-anything\nDESCRIPTION: Demonstrates how to load pretrained models in the align-anything framework using the load_pretrained_models function, which handles model initialization with specified configurations.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom align_anything.models.pretrained_model import load_pretrained_models\nself.model, self.tokenizer, self.processor = load_pretrained_models(\n   self.cfgs.model_cfgs.model_name_or_path,\n   model_max_length=self.cfgs.model_cfgs.model_max_length,\n   padding_side='right',\n   trust_remote_code=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Model Mapping Definition in align-anything\nDESCRIPTION: Shows the model mapping configuration in align-anything that connects model types to their respective implementation classes, enabling unified loading of different model architectures.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nMODEL_MAPPING_NAMES: OrderedDict[str, str] = OrderedDict(\n   [\n      # Score model mapping\n      ('llama', 'AccustomedLlamaModel'),\n      ('mllama', 'AccustomedMllamaModel'),\n      ('llava', 'AccustomedLlavaModel'),\n      ('llava_next', 'AccustomedLlavaNextModel'),\n      ('qwen2_audio', 'AccustomedQwen2AudioModel'),\n      ('chameleon', 'AccustomedChameleonModel'),\n      ('qwen2_vl', 'AccustomedQwen2VLModel'),\n      ('modeling_emu3.mllm.modeling_emu3', 'Emu3ForCausalLM'),\n   ],\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing LLaVA Model and Processor\nDESCRIPTION: Loads the LLaVA 1.5-7b model and its processor from Hugging Face. The model is loaded with float16 precision to optimize memory usage and moved to the first GPU device.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Model and Processor initialization\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nmodel = LlavaForConditionalGeneration.from_pretrained(\n   model_id,\n   torch_dtype=torch.float16,\n   low_cpu_mem_usage=True,\n).to(0)\nprocessor = AutoProcessor.from_pretrained(model_id)\n```\n\n----------------------------------------\n\nTITLE: Implementing format_preference_sample Method for AA_TI2T Template in Python\nDESCRIPTION: Shows how to implement the format_preference_sample method to prepare data for preference-based training methods like RM, DPO, and KTO by identifying better and worse responses.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/dataset_custom.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@register_template('AA_TI2T')\nclass AA_TI2T(BaseFormatter):\n    system_prompt: str = \"\"\n\n    def format_preference_sample(self, raw_sample: dict[str, Any]) -> tuple[list[dict[str, Any]], list[dict[str, Any]], dict[str, Any]]:\n        better_id = int(raw_sample['overall_response'])\n        worse_id = 2 if better_id==1 else 1\n\n        if better_id not in [1, 2] or worse_id not in [1, 2]:\n            return [], [], {}\n\n        raw_better_response = raw_sample[f'response_{better_id}']\n        raw_worse_response = raw_sample[f'response_{worse_id}']\n        prompt = raw_sample['question']\n        image = raw_sample['image'].convert('RGBA')\n        better_conversation = [\n            {'role': 'user', 'content': [\n                    {'type': 'image'},\n                    {'type': 'text', 'text': prompt},\n                ]\n            },\n            {'role': 'assistant', 'content': [{'type': 'text', 'text': raw_better_response}]},\n        ]\n        worse_conversation = [\n            {'role': 'user', 'content': [\n                    {'type': 'image'},\n                    {'type': 'text', 'text': prompt},\n                ]\n            },\n            {'role': 'assistant', 'content': [{'type': 'text', 'text': raw_worse_response}]},\n        ]\n\n        meta_info = {\n            'image': image,\n            'better_response': raw_better_response,\n            'worse_response': raw_worse_response,\n        }\n\n        return better_conversation, worse_conversation, meta_info\n```\n\n----------------------------------------\n\nTITLE: Generating Output with LLaVA Model\nDESCRIPTION: Processes the image and text inputs into tensors using the processor, then passes them to the model for inference. The generated output is decoded back to text using the processor.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n# Here, the text input -> ``input_ids`` tensor by the tokenizer.\n# And the ``images`` input -> ``pixel_values`` tensor by the image processor.\n\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Step Function for SFT\nDESCRIPTION: Training step implementation for Supervised Fine-Tuning (SFT) using CrossEntropyLoss from transformers library. Processes a single batch and returns the loss information.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef train_step(self, batch: dict[str, Any]) -> dict[str, Any]:\n    outputs = self.model(**batch)\n    loss = outputs.loss\n    return {'train/loss': loss.item()}\n```\n\n----------------------------------------\n\nTITLE: Loading LLaMA Model with Transformers\nDESCRIPTION: Demonstrates loading a LLaMA model using the LlamaForCausalLM class from the transformers library, showing how different model architectures require different loading classes.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\nmodel = LlamaForCausalLM.from_pretrained(\n   model_id,\n   torch_dtype=torch.float16,\n).to(0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Training Loop in Python\nDESCRIPTION: Main training loop implementation that handles epoch iterations, batch processing, and model checkpointing. Uses tqdm for progress tracking and includes cuda memory management.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef train(self) -> None:\n    \"\"\"Train the model.\"\"\"\n\n    progress_bar = tqdm(\n          total=self.cfgs.train_cfgs.epochs * len(self.train_dataloader),\n          desc=f'Training 1/{self.cfgs.train_cfgs.epochs} epoch',\n          position=0,\n          leave=True,\n          disable=not is_main_process(),\n    )\n\n    for epoch in range(int(self.cfgs.train_cfgs.epochs)):\n          self.model.train()\n\n          for batch in self.train_dataloader:\n             info = self.train_step(batch)\n             torch.cuda.empty_cache()\n\n             self.global_step += 1\n\n             info['train/epoch'] = self.global_step / len(self.train_dataloader)\n\n             if self.global_step % self.cfgs.logger_cfgs.save_interval == 0:\n                self.save(tag=self.global_step)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for LLaVA Model\nDESCRIPTION: Imports necessary Python libraries for working with the LLaVA vision-language model, including requests and PIL for image handling, and transformer libraries for model loading.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom PIL import Image\n\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n```\n\n----------------------------------------\n\nTITLE: Multi-Modal Data Processing Implementation\nDESCRIPTION: Implementation of multi-modal data processing using a processor to convert images and text into tensor format.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nmulti_modal_padding = self.processor(\n   images=images,\n   text=concated_text,\n   return_tensors='pt',\n   padding=True,\n   padding_side=self.padding_side,\n   return_attention_mask=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing format_supervised_sample Method for AA_TI2T Template in Python\nDESCRIPTION: Shows how to implement the format_supervised_sample method to convert dataset items into question-answer format for supervised fine-tuning.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/dataset_custom.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@register_template('AA_TI2T')\nclass AA_TI2T(BaseFormatter):\n    system_prompt: str = \"\"\n\ndef format_supervised_sample(self, raw_sample: dict[str, Any]) -> tuple[list[dict[str, Any]], dict[str, Any]]:\n    prompt = raw_sample['prompt']\n    answer = raw_sample['response']\n    image = raw_sample['image'].convert('RGBA')\n\n    return [\n        {'role': 'user', 'content': [\n                {'type': 'image'},\n                {'type': 'text', 'text': prompt},\n            ]\n        },\n        {'role': 'assistant', 'content': [{'type': 'text', 'text': answer}]},\n    ], {'image': image}\n```\n\n----------------------------------------\n\nTITLE: Implementing Supervised Data Formatting for Alpaca Model\nDESCRIPTION: Implementation of the format_supervised_sample method for the Alpaca data formatter that processes raw samples into conversation format with user and assistant roles.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@register_template('Alpaca')\nclass Alpaca(BaseFormatter):\n\ndef format_supervised_sample(self, raw_sample: dict[str, Any]) -> tuple[list[dict[str, Any]], dict]:\n   prompt = ' '.join((raw_sample['instruction'], raw_sample['input']))\n   response = raw_sample['output']\n   return [\n         {\"role\": \"user\", \"content\": prompt},\n         {\"role\": \"assistant\", \"content\": response},\n   ], {}\n```\n\n----------------------------------------\n\nTITLE: Loading DPO Fine-tuned Llama Model for Evaluation\nDESCRIPTION: Python code for loading the DPO fine-tuned Llama model to evaluate its performance after alignment training. Sets up the model and tokenizer for evaluation.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel_path = \"/PATH/TO/YOUR/TRAINED_MODEL\"  # 请更换为实际的模型路径\nmodel = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n\n# 将模型设置为eval模式\nmodel.eval()\n```\n\n----------------------------------------\n\nTITLE: Implementing Preference Data Formatting for Text-Image Model\nDESCRIPTION: Implementation of the format_preference_sample method for handling text-image paired data with better/worse response comparisons.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@register_template('AA_TI2T')\nclass AA_TI2T(BaseFormatter):\n   system_prompt: str = \"\"\n\n   def format_preference_sample(self, raw_sample: dict[str, Any]) -> tuple[list[dict[str, Any]], list[dict[str, Any]], dict[str, Any]]:\n      better_id = int(raw_sample['overall_response'])\n      worse_id = 2 if better_id==1 else 1\n\n      if better_id not in [1, 2] or worse_id not in [1, 2]:\n            return [], [], {}\n\n      raw_better_response = raw_sample[f'response_{better_id}']\n      raw_worse_response = raw_sample[f'response_{worse_id}']\n      prompt = raw_sample['question']\n      image = raw_sample['image'].convert('RGBA')\n      better_conversation = [\n            {'role': 'user', 'content': [\n                  {'type': 'image'},\n                  {'type': 'text', 'text': prompt},\n               ]\n            },\n            {'role': 'assistant', 'content': [{'type': 'text', 'text': raw_better_response}]},\n      ]\n      worse_conversation = [\n            {'role': 'user', 'content': [\n                  {'type': 'image'},\n                  {'type': 'text', 'text': prompt},\n               ]\n            },\n            {'role': 'assistant', 'content': [{'type': 'text', 'text': raw_worse_response}]},\n      ]\n\n      meta_info = {\n            'image': image,\n            'better_response': raw_better_response,\n            'worse_response': raw_worse_response,\n      }\n\n      return better_conversation, worse_conversation, meta_info\n```\n\n----------------------------------------\n\nTITLE: Custom LLaVA Model Implementation\nDESCRIPTION: Defines a custom class for LLaVA models that inherits from LlavaForConditionalGeneration, serving as an interface for the align-anything framework without modifying the original model behavior.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass AccustomedLlavaModel(LlavaForConditionalGeneration):\n   \"\"\"Accustomed Interface for LlaVA model\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generic Dataset Key-Value Parsing Example\nDESCRIPTION: Demonstrates an alternative parsing approach for datasets with a different structure, handling prompt and response fields and formatting them into the multi-modal conversation format.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprompt = raw_sample['prompt']\nanswer = raw_sample['response']\n\nreturn [\n      {'role': 'user', 'content': [{'type': 'text', 'text': prompt}]},\n      {'role': 'assistant', 'content': [{'type': 'text', 'text': answer}]},\n], {}\n```\n\n----------------------------------------\n\nTITLE: Alpaca Dataset Key-Value Parsing Example\nDESCRIPTION: Shows how to parse data from the Alpaca dataset format into the conversation format required by the align-anything framework, mapping instruction and input fields to user prompts.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprompt = ' '.join((raw_sample['instruction'], raw_sample['input']))\nresponse = raw_sample['output']\nreturn [\n      {'role': 'user', 'content': prompt},\n      {'role': 'assistant', 'content': response},\n], {}\n```\n\n----------------------------------------\n\nTITLE: Example of AA_TI2T Dataset Key-Value Structure in Python\nDESCRIPTION: Shows the original data structure of the Align-Anything-200K (Text+Image -> Text) dataset, including image, question, multiple responses, and preference rating.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/dataset_custom.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n  'image': PIL.Image.Image,\n  'question': str,\n  'response_1': str,\n  'response_2': str,\n  'overall_response': int,\n}\n```\n\n----------------------------------------\n\nTITLE: Using Image-Audio-Text Model\nDESCRIPTION: Example code for loading and using the Image-Audio-Text multimodal language model for processing text, images, and audio.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/any_to_text/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom align_anything.models.llama_vision_audio_model import (\n    LlamaVisionAudioForConditionalGeneration,\n    LlamaVisionAudioProcessor,\n)\nimport torch\nimport torchaudio\nfrom PIL import Image\n\npath = <path_to_model_dir>\nprocessor = LlamaVisionAudioProcessor.from_pretrained(path)\nmodel = LlamaVisionAudioForConditionalGeneration.from_pretrained(path)\n\nprompt = \"<|start_header_id|>user<|end_header_id|>: Where is the capital of China?\\n<|start_header_id|>assistant<|end_header_id|>: \"\n\ninputs = processor(text=prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=1024)\nprint(processor.decode(outputs[0], skip_special_tokens=True))\n\nprompt = \"<|start_header_id|>user<|end_header_id|>: Summarize the audio's contents.<audio>\\n<|start_header_id|>assistant<|end_header_id|>: \"\n\naudio_path = \"align-anything/assets/test_audio.wav\"\naudio, _ = torchaudio.load(audio_path)\nif audio.shape[0] == 2:\n    audio = audio.mean(dim=0, keepdim=True)\naudio = audio.squeeze().tolist()\n\ninputs = processor(text=prompt, raw_speech=audio, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=1024)\nprint(processor.decode(outputs[0], skip_special_tokens=False))\n\nprompt = \"<|start_header_id|>user<|end_header_id|>: <image> Give an overview of what's in the image.\\n<|start_header_id|>assistant<|end_header_id|>: \"\nimage_path = \"align-anything/assets/test_image.webp\"\nimage = Image.open(image_path)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=1024)\nprint(processor.decode(outputs[0], skip_special_tokens=True))\n```\n\n----------------------------------------\n\nTITLE: Custom Chat Template Implementation for MiniCPM Model\nDESCRIPTION: Implementation of a custom chat template for the MiniCPM model with special token handling and formatting.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass AccustomedMiniCPMV(MiniCPMV):\n\n   def apply_chat_template(\n      self, messages: list[dict[str, Any]], add_generation_prompt: bool = False\n   ) -> dict[str, Any]:\n      conversation = ''\n      for message in messages:\n            role = message['role']\n            contents = message['content']\n            for content in contents:\n               if content['type'] == 'text':\n                  if role == 'user':\n                        content = (\n                           '<User>'\n                           + self.tokenizer.im_start\n                           + self.tokenizer.unk_token * self.config.query_num\n                           + self.tokenizer.im_end\n                           + '\\n'\n                           + content['text']\n                        )\n                  else:\n                        content = '<AI>' + '\\n' + content['text']\n                  conversation += content\n      if add_generation_prompt:\n            conversation += '<AI>'\n      return conversation\n```\n\n----------------------------------------\n\nTITLE: Initializing Multimodal Models\nDESCRIPTION: Basic commands to initialize vision-text and vision-audio-text language models using provided scripts.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/any_to_text/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython build_llama_vision.py\npython build_llama_vision_audio.py\n```\n\n----------------------------------------\n\nTITLE: Preparing Conversation Data for LLaVA\nDESCRIPTION: Creates a conversation structure with a user query that includes both text and image placeholders. The processor's apply_chat_template function formats this into a prompt following LLaVA's required chat template.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Data preparation\nconversation = [\n   {\n      \"role\": \"user\",\n      \"content\": [\n         {\"type\": \"text\", \"text\": \"What are these?\"},\n         {\"type\": \"image\"},\n      ],\n   },\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\n```\n\n----------------------------------------\n\nTITLE: Training Stage 2: Joint Model Training\nDESCRIPTION: Deepspeed training script for jointly training the projector and Language Model with multiple datasets.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/any_to_text/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Initialize variables\nTRAIN_DATASETS=\"\\\nliuhaotian/LLaVA-Instruct-150K,\\\nHuggingFaceM4/A-OKVQA,\\\nHuggingFaceM4/VQAv2,\\\n\"\n\nTRAIN_TEMPLATE=\"\\\nLLAVA,\\\nA-OKVQA,\\\nVQAv2,\\\n\"\n\nTRAIN_SPLIT=\"\\\ntrain,\\\ntrain,\\\ntrain,\\\n\"\n\n# Source the setup script\nsource ./setup.sh\n\n# Execute deepspeed command\ndeepspeed \\\n\t--master_port ${MASTER_PORT} \\\n\t--module align_anything.trainers.text_image_to_text.sft \\\n\t--model_name_or_path ${MODEL_NAME_OR_PATH} \\\n\t--train_datasets ${TRAIN_DATASETS} \\\n  \t--train_split ${TRAIN_SPLIT} \\\n\t--train_template ${TRAIN_TEMPLATE} \\\n\t--output_dir ${OUTPUT_DIR} \\\n\t--freeze_vision_tower True \\\n\t--freeze_mm_proj False \\\n\t--freeze_language_model False \\\n```\n\n----------------------------------------\n\nTITLE: Implementing format_prompt_only_sample Method for AA_TI2T Template in Python\nDESCRIPTION: Shows how to implement the format_prompt_only_sample method to extract only prompts from the dataset for reinforcement learning fine-tuning.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/dataset_custom.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@register_template('AA_TI2T')\nclass AA_TI2T(BaseFormatter):\n    system_prompt: str = \"\"\n\n    def format_prompt_only_sample(self, raw_sample: dict[str, Any]) -> tuple[list[dict[str, Any]], dict[str, Any]]:\n        prompt = raw_sample['question']\n        image = raw_sample['image'].convert('RGBA')\n\n        return [\n            {'role': 'user', 'content': [\n                    {'type': 'image'},\n                    {'type': 'text', 'text': prompt},\n                ]\n            },\n        ], {'image': image}\n```\n\n----------------------------------------\n\nTITLE: Training Stage 1: Projector Training\nDESCRIPTION: Deepspeed training script for training the projector between the Encoder and Language Model while keeping other components frozen.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/any_to_text/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Initialize variables\nMODEL_NAME_OR_PATH=\"\"\nTRAIN_DATASETS=\"\"\nOUTPUT_DIR=\"\"\n\n# Source the setup script\nsource ./setup.sh\n\n# Execute deepspeed command\ndeepspeed \\\n\t--master_port ${MASTER_PORT} \\\n\t--module align_anything.trainers.text_image_to_text.sft \\\n\t--model_name_or_path ${MODEL_NAME_OR_PATH} \\\n\t--train_datasets ${TRAIN_DATASETS} \\\n\t--output_dir ${OUTPUT_DIR} \\\n\t--freeze_vision_tower True \\\n\t--freeze_mm_proj False \\\n\t--freeze_language_model True \\\n```\n\n----------------------------------------\n\nTITLE: Example of Conversation Format Structure in Python\nDESCRIPTION: Shows the structure of a conversation format that includes both image and text content, compatible with the apply_chat_template function in transformers.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/dataset_custom.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[\n    {'role': 'user', 'content': [\n            {'type': 'image'},\n            {'type': 'text', 'text': prompt},\n        ]\n    },\n    {'role': 'assistant', 'content': [{'type': 'text', 'text': answer}]},\n]\n```\n\n----------------------------------------\n\nTITLE: Using Image-Text Model\nDESCRIPTION: Example code for loading and using the Image-Text multimodal language model for image analysis.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/any_to_text/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import (\n    LlavaForConditionalGeneration,\n    AutoProcessor,\n)\nfrom PIL import Image\n\npath = <path_to_model_dir>\nprocessor = AutoProcessor.from_pretrained(path)\nmodel = LlavaForConditionalGeneration.from_pretrained(path)\n\nprompt = \"<|start_header_id|>user<|end_header_id|>: <image> Give an overview of what's in the image.\\n<|start_header_id|>assistant<|end_header_id|>: \"\nimage_path = \"align-anything/assets/test_image.webp\"\nimage = Image.open(image_path)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=1024)\nprint(processor.decode(outputs[0], skip_special_tokens=True))\n```\n\n----------------------------------------\n\nTITLE: Example of Multi-Modal Info Dictionary in Python\nDESCRIPTION: Demonstrates the structure for storing multi-modal information like images within the dataset formatter.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/dataset_custom.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n{\n  'image': PIL.Image.Image,\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Configuring Llama Model\nDESCRIPTION: Python code for loading the Llama model and tokenizer, and setting it to evaluation mode\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/en/text_to_text_dpo.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndevice = \"cuda\"  # Set device to \"cuda\" to use GPU\nmodel_path = (\n    \"/PATH/TO/YOUR/Meta-Llama-3.1-8B-Instruct\"  # Please replace with your actual model path\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n\n# Set the model to evaluation mode\nmodel.eval()\n```\n\n----------------------------------------\n\nTITLE: Registering Template Class for AA_TI2T in Python\nDESCRIPTION: Demonstrates how to create and register a new template class for the AA_TI2T dataset using the register_template decorator, initializing with an empty system prompt.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/dataset_custom.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@register_template('AA_TI2T')\nclass AA_TI2T(BaseFormatter):\n    system_prompt: str = \"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt-Only Data Formatting for Text-Audio Model\nDESCRIPTION: Implementation of the format_prompt_only_sample method for processing audio-text paired data with system prompts.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@register_template('AA_TA2T')\nclass AA_TA2T(BaseFormatter):\n   system_prompt: str = 'You are a helpful assistant.'\n\n   def format_prompt_only_sample(self, raw_sample: dict[str, Any]) -> dict[str, Any]:\n      prompt = raw_sample['prompt']\n      audio_path = raw_sample['audio_path']\n\n      conversation = [\n            {'role': 'system', 'content': [{'type': 'text', 'text': self.system_prompt}]},\n            {'role': 'user', 'content': [\n                  {'type': 'audio', 'audio_url': audio_path},\n                  {'type': 'text', 'text': prompt},\n               ]},\n      ]\n\n      return conversation, {'audio_path': audio_path}\n```\n\n----------------------------------------\n\nTITLE: Loading Llama-3.1-8B-Instruct Model for Testing\nDESCRIPTION: Python code for loading the pre-trained Llama-3.1-8B-Instruct model using Hugging Face Transformers. Sets up the model and tokenizer for inference on GPU.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndevice = \"cuda\"  # 将device设置为\"cuda\"以使用GPU\nmodel_path = \"/PATH/TO/YOUR/Llama-3.1-8B-Instruct\"  # 请更换为实际的模型路径\nmodel = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n\n# 将模型设置为eval模式\nmodel.eval()\n```\n\n----------------------------------------\n\nTITLE: AutoModelMapping in align-anything\nDESCRIPTION: Defines the auto-mapping mechanism in align-anything that leverages Hugging Face's AutoModelForCausalLM for compatible models, simplifying model loading across different architectures.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nMODEL_MAPPING: OrderedDict[str, Any] = _LazyAutoMappingInAlignAnything(\n   CONFIG_MAPPING_NAMES,\n   MODEL_FOR_CAUSAL_LM_MAPPING_NAMES | MODEL_MAPPING_NAMES,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for LLM Testing\nDESCRIPTION: Python code for importing necessary libraries to test the Llama model. Sets environment variables for offline operation to avoid dependency on Hugging Face online services.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport torch\n\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\nos.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n```\n\n----------------------------------------\n\nTITLE: DPO Fine-tuning Script for Llama Model\nDESCRIPTION: Bash script for fine-tuning the Llama-3.1-8B-Instruct model using the DPO algorithm. Uses DeepSpeed for distributed training on the PKU-SafeRLHF dataset with wandb logging.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME_OR_PATH=\"meta-llama/Llama-3.1-8B-Instruct\" # model path\n\nTRAIN_DATASETS=\"PKU-Alignment/PKU-SafeRLHF-single-dimension\" # dataset path\nTRAIN_TEMPLATE=\"PKUSafeRLHF\" # dataset template\nTRAIN_SPLIT=\"train\" # split the dataset\n\nOUTPUT_DIR=\"../outputs/llama_dpo\" # output dir\n\n# For wandb online logging\nexport WANDB_API_KEY=\"YOUR_API_KEY\"\n\n# Source the setup script\nsource ./setup.sh\n\n# Execute deepspeed command\ndeepspeed \\\n     --master_port ${MASTER_PORT} \\\n     --module align_anything.trainers.text_to_text.dpo \\\n     --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n     --train_template ${TRAIN_TEMPLATE} \\\n     --train_datasets ${TRAIN_DATASETS} \\\n     --train_split ${TRAIN_SPLIT} \\\n     --output_dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: Running LLaVA DPO Training Script\nDESCRIPTION: Commands to run the LLaVA DPO training script for the Text+Image->Text modality. The script automatically downloads the required model and dataset from Hugging Face.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncd scripts\nbash llava/llava_dpo.sh\n```\n\n----------------------------------------\n\nTITLE: Running PPO Training Script for Text-Image Model\nDESCRIPTION: Bash script for configuring and running PPO training with DeepSpeed. Sets up model paths, datasets, and training parameters for text-image to text-image translation.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nACTOR_MODEL_NAME_OR_PATH=\"PKU-Alignment/AA-chameleon-7b-base\"\nCRITIC_MODEL_NAME_OR_PATH=\"\"\nREWARD_MODEL_NAME_OR_PATH=\"\"\nTRAIN_DATASETS=\"\"\nTRAIN_PT_NAME=\"\"\nPTX_DATASETS=\"\"\nPTX_PT_NAME=\"\"\nOUTPUT_DIR=\"../outputs/ppo_text_image_to_text_image\"\n\nsource ./setup.sh\n\ndeepspeed \\\n--master_port ${MASTER_PORT} \\\n--module align_anything.trainers.text_image_to_text_image.ppo \\\n--actor_model_name_or_path ${ACTOR_MODEL_NAME_OR_PATH} \\\n--reward_model_name_or_path ${REWARD_MODEL_NAME_OR_PATH} \\\n--reward_critic_model_name_or_path ${CRITIC_MODEL_NAME_OR_PATH} \\\n--train_datasets ${TRAIN_DATASETS} \\\n--train_template ANYTHING_TI2TI \\\n--train_data_files ${TRAIN_PT_NAME} \\\n--ptx_datasets ${PTX_DATASETS} \\\n--ptx_data_files ${PTX_PT_NAME} \\\n--ptx_template Llava \\\n--output_dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: Proximal Policy Optimization (PPO) Script for Chameleon Model\nDESCRIPTION: This bash script configures and runs the PPO training process for the Chameleon model, specifying actor, critic, and reward model paths, as well as training and PTX datasets.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nACTOR_MODEL_NAME_OR_PATH=\"\"\nCRITIC_MODEL_NAME_OR_PATH=\"\"\nREWARD_MODEL_NAME_OR_PATH=\"\"\nTRAIN_DATASETS=\"\"\nTRAIN_PT_NAME=\"dataset_file_name\"\nPTX_DATASETS=\"\"\nPTX_PT_NAME=\"dataset_file_name\"\nOUTPUT_DIR=\"../outputs/ppo_text_image_to_text_image\"\n\nsource ./setup.sh\n\ndeepspeed \\\n  --master_port ${MASTER_PORT} \\\n  --module align_anything.trainers.text_image_to_text_image.ppo \\\n  --actor_model_name_or_path ${ACTOR_MODEL_NAME_OR_PATH} \\\n  --reward_model_name_or_path ${REWARD_MODEL_NAME_OR_PATH} \\\n  --reward_critic_model_name_or_path ${CRITIC_MODEL_NAME_OR_PATH} \\\n  --train_datasets ${TRAIN_DATASETS} \\\n  --train_template ANYTHING_TI2TI \\\n  --train_data_files ${TRAIN_PT_NAME} \\\n  --ptx_datasets ${PTX_DATASETS} \\\n  --ptx_data_files ${PTX_PT_NAME} \\\n  --ptx_template LLAVA \\\n  --output_dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: Running Model Evaluation on Benchmarks using Bash Script\nDESCRIPTION: This script, evaluate.sh, runs model evaluation on specified benchmarks. It sets up environment variables and executes a Python script for each benchmark. Users need to configure parameters like benchmarks, output directory, generation backend, model ID, model path, and chat template.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/evaluation/example.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nSCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\ncd \"${SCRIPT_DIR}/../align_anything/evaluation\" || exit 1\n\nBENCHMARKS=(\"\")\nOUTPUT_DIR=\"\"\nGENERATION_BACKEND=\"\"\nMODEL_ID=\"\"\nMODEL_NAME_OR_PATH=\"\"\nCHAT_TEMPLATE=\"\"\n\nfor BENCHMARK in \"${BENCHMARKS[@]}\"; do\n    python __main__.py \\\n        --benchmark ${BENCHMARK} \\\n        --output_dir ${OUTPUT_DIR} \\\n        --generation_backend ${GENERATION_BACKEND} \\\n        --model_id ${MODEL_ID} \\\n        --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n        --chat_template ${CHAT_TEMPLATE}\ndone\n```\n\n----------------------------------------\n\nTITLE: Custom Chat Template Implementation for Idefics2 Model\nDESCRIPTION: Implementation of a chat template property for the Idefics2 model using Jinja-style templating.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass AccustomedIdefics2Model(Idefics2ForConditionalGeneration):\n   \"\"\"Accustomed Interface for Idefics2 model\"\"\"\n\n   @property\n   def chat_template(self):\n      return \"{% for message in messages %}{{message['role'].capitalize()}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\\n{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\"\n```\n\n----------------------------------------\n\nTITLE: Direct Preference Optimization (DPO) Script for Chameleon Model\nDESCRIPTION: This bash script configures and executes the DPO training process for the Chameleon model, setting various hyperparameters and specifying input and output paths.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME_OR_PATH=\"\"\nTRAIN_DATASETS=\"path/to/dataset\"\nOUTPUT_DIR=\"../outputs/dpo_text_image_to_text_image\"\nPT_NAME=\"dataset_file_name\"\nexport WANDB_API_KEY=\"\"\nsource ./setup.sh\n\ndeepspeed \\\n\t--master_port ${MASTER_PORT} \\\n\t--module align_anything.trainers.text_image_to_text_image.dpo \\\n\t--model_name_or_path ${MODEL_NAME_OR_PATH} \\\n\t--train_datasets ${TRAIN_DATASETS} \\\n\t--output_dir ${OUTPUT_DIR} \\\n\t--per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 2 \\\n    --train_template ANYTHING_TI2TI \\\n    --train_split train \\\n\t--train_data_files ${PT_NAME} \\\n\t--learning_rate 5e-7 \\\n\t--epochs 3 \\\n\t--lr_scheduler_type cosine \\\n\t--save_interval 2500\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Comparing Two Models across Benchmarks using Bash Script\nDESCRIPTION: This script, models_pk.sh, evaluates and compares two models across specified benchmarks. It runs the evaluation for each model on each benchmark and then compares the results. Users need to set parameters for benchmarks, output directory, generation backend, model IDs, model paths, and chat templates for both models.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/evaluation/example.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nSCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\ncd \"${SCRIPT_DIR}/../align_anything/evaluation\" || exit 1\n\nBENCHMARKS=(\"\")\nOUTPUT_DIR=\"\"\nGENERATION_BACKEND=\"\"\nMODEL_IDS=(\"\" \"\")\nMODEL_NAME_OR_PATHS=(\"\" \"\")\nCHAT_TEMPLATES=(\"\" \"\")\n\nfor BENCHMARK in \"${BENCHMARKS[@]}\"; do\n    echo \"Processing benchmark: ${BENCHMARK}\"\n\n    for i in \"${!MODEL_IDS[@]}\"; do\n        MODEL_ID=${MODEL_IDS[$i]}\n        MODEL_NAME_OR_PATH=${MODEL_NAME_OR_PATHS[$i]}\n        CHAT_TEMPLATE=${CHAT_TEMPLATES[$i]}\n\n        echo \"Running model ${MODEL_ID} for benchmark ${BENCHMARK}\"\n        python __main__.py \\\n            --benchmark ${BENCHMARK} \\\n            --output_dir ${OUTPUT_DIR} \\\n            --generation_backend ${GENERATION_BACKEND} \\\n            --model_id ${MODEL_ID} \\\n            --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n            --chat_template ${CHAT_TEMPLATE}\n    done\n\n    python models_pk.py --benchmark ${BENCHMARK} \\\n                        --model_1 \"${MODEL_IDS[0]}\" \\\n                        --model_2 \"${MODEL_IDS[1]}\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Reward Model Training Script for Chameleon Model\nDESCRIPTION: This bash script sets up and runs the reward model training process for the Chameleon model, specifying training and evaluation datasets, hyperparameters, and output paths.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME_OR_PATH=\"\"\nTRAIN_DATASETS=\"path/to/dataset\"\nTRAIN_PT_NAME=\"dataset_file_name\"\nEVAL_DATASETS=\"path/to/dataset\"\nEVAL_PT_NAME=\"dataset_file_name\"\nOUTPUT_DIR=\"../outputs/rm_text_image_to_text_image\"\nexport WANDB_API_KEY=\"\"\nsource ./setup.sh\n\ndeepspeed \\\n\t--master_port ${MASTER_PORT} \\\n\t--module align_anything.trainers.text_image_to_text_image.rm \\\n\t--model_name_or_path ${MODEL_NAME_OR_PATH} \\\n\t--train_datasets ${TRAIN_DATASETS} \\\n\t--output_dir ${OUTPUT_DIR} \\\n\t--per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 2 \\\n    --train_template ANYTHING_TI2TI \\\n    --train_split train \\\n\t--train_data_files ${TRAIN_PT_NAME} \\\n\t--eval_datasets ${EVAL_DATASETS} \\\n\t--eval_data_files ${EVAL_PT_NAME} \\\n\t--eval_template ANYTHING_TI2TI \\\n\t--learning_rate 5e-6 \\\n\t--epochs 3 \\\n\t--lr_scheduler_type cosine \\\n\t--save_interval 2500\n```\n\n----------------------------------------\n\nTITLE: Configuring MMLU Benchmark in YAML\nDESCRIPTION: Example YAML configuration for the MMLU benchmark evaluation, showing settings for inference, evaluation parameters, dataset configuration, and model specifications.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/evaluation/evaluation_configurations.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninfer_cfgs:\n  # The deepspeed configuration\n  ds_cfgs: ds_z3_config.json\n  vllm_cfgs: vllm_basic.json\n\ndefault:\n  # Evaluation configurations\n  eval_cfgs:\n    # Output directory name\n    output_dir: null\n    # Unique identifier for cache folder\n    uuid: null\n    # Num shot\n    n_shot: 0\n    # Use Chain of Thought\n    cot: false\n  # Configuration for data\n  data_cfgs:\n    # Task name\n    task: ['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics']\n    # Task directory\n    task_dir: cais/mmlu\n    # Evaluation split\n    split: test\n    # Candidate labels\n    candidate_labels: [\"A\", \"B\", \"C\", \"D\"]\n\n  # Model configurations\n  model_cfgs:\n    model_id: null\n    # Pretrained model name or path\n    model_name_or_path: null\n    # Chat template\n    chat_template: null\n    # Whether to trust remote code\n    trust_remote_code: True\n    # The max token length\n    model_max_length: 2048\n```\n\n----------------------------------------\n\nTITLE: Running VLA SFT Training Script\nDESCRIPTION: Command to run the supervised fine-tuning script for the SPOC (VLA) model. The HOME_PREFIX in the script should be modified to point to your local data path before execution.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/vla/spoc_sft.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring vLLM Inference Parameters in JSON\nDESCRIPTION: JSON configuration for vLLM inference backend, specifying sampling parameters and LLM settings that control the generation process during model evaluation.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/evaluation/evaluation_configurations.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"SamplingParams\":\n    {\n        \"n\": 1,\n        \"top_k\": 10,\n        \"top_p\": 0.95,\n        \"temperature\": 0.05,\n        \"max_tokens\": 512,\n        \"frequency_penalty\": 1.2,\n        \"prompt_logprobs\": 0,\n        \"logprobs\": 20\n    },\n    \"LLM\":\n    {\n        \"tokenizer_mode\": \"auto\",\n        \"trust_remote_code\": true,\n        \"gpu_memory_utilization\": 0.9,\n        \"max_num_seqs\": 16\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Supervised Finetuning Pre-tokenization Script\nDESCRIPTION: Command to execute the supervised tokenization script. This needs to be configured with correct model paths before execution.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/janus/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash supervised_tokenize.sh\n```\n\n----------------------------------------\n\nTITLE: Supervised Fine-Tuning Script for Emu3 Model (Bash)\nDESCRIPTION: This bash script sets up the environment and executes a deepspeed command for supervised fine-tuning of the Emu3 model. It includes variable initialization for model paths, datasets, and output directory, and uses a setup script before running the training module.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/any_to_any.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Initialize variables\nMODEL_NAME_OR_PATH=\"BAAI/Emu3-Chat\"\nPROCESSOR_NAME_OR_PATH=\"BAAI/Emu3-VisionTokenizer\"\nTRAIN_DATASETS=\"\"\nTRAIN_DATA_FILE=\"\"\nOUTPUT_DIR=\"../outputs/any2any\"\n\n# Source the setup script\nsource ./setup.sh\n\n# Execute deepspeed command\ndeepspeed \\\n    --master_port ${MASTER_PORT} \\\n    --module align_anything.trainers.any_to_any.sft \\\n    --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n    --processor_name_or_path ${PROCESSOR_NAME_OR_PATH} \\\n    --train_datasets ${TRAIN_DATASETS} \\\n    --train_data_file ${TRAIN_DATA_FILE} \\\n    --train_template Any2Any \\\n    --train_split train \\\n    --output_dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: Default Template Formatting Implementation\nDESCRIPTION: Implementation of the default_format method for converting conversation data into formatted text strings.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef default_format(\n   self, raw_sample: list[dict[str, Any]], add_generation_prompt: bool = False\n) -> str:\nfinal_text = ''\nfor line in raw_sample:\n      for content in line['content']:\n         if content['type'] == 'text':\n            final_text += line['role'].upper() + ': ' + content['text'] + '\\n'\nif add_generation_prompt:\n      final_text += 'ASSISTANT: '\nreturn final_text\n```\n\n----------------------------------------\n\nTITLE: Preference Pre-tokenization Script Execution\nDESCRIPTION: Command to run pre-tokenization for preference datasets used in DPO or RM training\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython preference_tokenize_example.py --model_path $MODEL_PATH --input_path $INPUT_PATH --output_path $OUTPUT_PATH --cache_dir $CACHE_DIR\n```\n\n----------------------------------------\n\nTITLE: Supervised Fine-Tuning (SFT) Script for Chameleon Model\nDESCRIPTION: This bash script sets up and runs the supervised fine-tuning process for the Chameleon model using DeepSpeed, specifying various training parameters and dataset paths.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME_OR_PATH=\"\"\nTRAIN_DATASETS=\"path/to/dataset\"\nPT_NAME=\"dataset_file_name\"\nOUTPUT_DIR=\"../outputs/sft_text_image_to_text_image\"\nexport WANDB_API_KEY=\"\"\nsource ./setup.sh\n\ndeepspeed \\\n\t--master_port ${MASTER_PORT} \\\n\t--module align_anything.trainers.text_image_to_text_image.sft \\\n\t--model_name_or_path ${MODEL_NAME_OR_PATH} \\\n\t--train_datasets ${TRAIN_DATASETS} \\\n\t--train_data_files ${PT_NAME} \\\n\t--output_dir ${OUTPUT_DIR} \\\n\t--train_template ANYTHING_TI2TI \\\n\t--train_split 'train' \\\n\t--per_device_train_batch_size 2 \\\n\t--per_device_eval_batch_size 2 \\\n\t--gradient_accumulation_steps 2 \\\n\t--save_interval 500 \\\n\t--learning_rate 1e-5 \\\n\t--epochs 12 \\\n    --lr_scheduler_type constant\n```\n\n----------------------------------------\n\nTITLE: Prompt-Only Pre-tokenization Script Execution\nDESCRIPTION: Command to run pre-tokenization for prompt-only datasets used in PPO training\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython prompt_only_tokenize_example.py --model_path $MODEL_PATH --input_path $INPUT_PATH --output_path $OUTPUT_PATH --cache_dir $CACHE_DIR\n```\n\n----------------------------------------\n\nTITLE: Parallel Pre-tokenization Script Execution\nDESCRIPTION: Command to run accelerated parallel pre-tokenization using multiple GPUs\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython pre_tokenize_parallel_example.py --model_path $MODEL_PATH --input_path $INPUT_PATH --output_path $OUTPUT_PATH --cache_dir $CACHE_DIR\n```\n\n----------------------------------------\n\nTITLE: Testing DPO Fine-tuned Llama Model on Safety Query\nDESCRIPTION: Python code for testing the DPO fine-tuned model's response to the same safety-critical query about handling aggressive wild animals. Used to compare results with the original model.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n    {\n        \"role\": \"user\",\n        \"content\": \"Recently, a wild animal in the local area has become aggressive towards humans and caused several injuries. How should I handle this wild animal?\",\n    },\n]\n\ninput_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n\n# the model generate new tokens\nwith torch.no_grad():\n    output = model.generate(**inputs, max_new_tokens=2048)\n# convert the generated tokens to text\ngenerated_text = tokenizer.decode(\n    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n)\nprint(\"\\nGenerated Text:\", generated_text)\n```\n\n----------------------------------------\n\nTITLE: Pre-tokenizing Input Data for Chameleon Model\nDESCRIPTION: These commands run different pre-tokenization scripts depending on the dataset type (standard, preference, or prompt-only) for preparing input data for the Chameleon model.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython pre_tokenize_example.py\n\npython pre_tokenize_parallel_example.py\n\npython preference_tokenize_example.py\n\npython prompt_only_tokenize_example.py\n```\n\n----------------------------------------\n\nTITLE: Testing Llama Model with Safety-Critical Query\nDESCRIPTION: Python code to test the Llama model's response to a safety-critical query about handling an aggressive wild animal. Demonstrates inference with the model before DPO fine-tuning.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n    {\n        \"role\": \"user\",\n        \"content\": \"Recently, a wild animal in the local area has become aggressive towards humans and caused several injuries. How should I handle this wild animal?\",\n    },\n]\n\ninput_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n\n# the model generate new tokens\nwith torch.no_grad():\n    output = model.generate(**inputs, max_new_tokens=2048)\n# convert the generated tokens to text\ngenerated_text = tokenizer.decode(\n    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n)\nprint(\"\\nGenerated Text:\", generated_text)\n```\n\n----------------------------------------\n\nTITLE: Running Training on Slurm Cluster\nDESCRIPTION: Command to execute the example Slurm training script for LLaVA DPO. This script is pre-configured with suitable Slurm parameters that should be adjusted for specific cluster configurations.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncd scripts\nbash slurm/slurm_llava_dpo.sh\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Multimodal Task Notation in Markdown\nDESCRIPTION: Shows the notation format for different types of multimodal tasks supported by the evaluation module, including text-to-text, multimodal-to-text, and text-to-multimodal transformations.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/evaluation/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nText→Text\nText+Image/Video/Audio→Text\nText→Image/Video/Audio\n```\n\n----------------------------------------\n\nTITLE: Testing Model Generation Capabilities\nDESCRIPTION: Python code for testing the model's text generation capabilities with a sample prompt\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/en/text_to_text_dpo.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n    {\n        \"role\": \"user\",\n        \"content\": \"Recently, a wild animal in the local area has become aggressive towards humans and caused several injuries. How should I handle this wild animal?\",\n    },\n]\n\ninput_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n\n# the model generate new tokens\nwith torch.no_grad():\n    output = model.generate(**inputs, max_new_tokens=2048)\n# convert the generated tokens to text\ngenerated_text = tokenizer.decode(\n    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n)\nprint(\"\\nGenerated Text:\", generated_text)\n```\n\n----------------------------------------\n\nTITLE: Example Training Data Format for Emu3 Model (JSON)\nDESCRIPTION: This snippet demonstrates the JSON format for training data, including both generation (TG) and understanding (TU) tasks. It shows how to structure input and output for text and image data.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/training/any_to_any.rst#2025-04-22_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n[\n    {\n        \"mode\": \"TU\",\n        \"input_text\": \"Please tell me what you see in this image.\",\n        \"input_image\": \"example.jpg\",\n        \"output_text\": \"I see an old British train coming towards the station, and a group of people waiting for it.\"\n    },\n    {\n        \"mode\": \"TG\",\n        \"input_text\": \"Generate an image where an old british train is coming towards the station, and a group of people is waiting for it.\",\n        \"output_image\": \"example.jpg\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Installing Align-Anything for Huawei Ascend NPU\nDESCRIPTION: Command to install Align-Anything with Ascend NPU support using pip. This enables the framework to run on Huawei's specialized AI hardware.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -e .[ascend]\n```\n\n----------------------------------------\n\nTITLE: Installing Align-Anything on Nvidia GPU\nDESCRIPTION: Steps to clone the Align-Anything repository, create a virtual environment, and install the package with optional CUDA setup for Nvidia GPUs. This installation process prepares the environment for training and evaluating models using the framework.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# clone the repository\ngit clone git@github.com:PKU-Alignment/align-anything.git\ncd align-anything\n\n# create virtual env\nconda create -n align-anything python==3.11\nconda activate align-anything\n```\n\n----------------------------------------\n\nTITLE: Finalizing Align-Anything Installation\nDESCRIPTION: Final installation steps for the Align-Anything package, including installing it in development mode and adding the vLLM dependency for accelerated PPO training.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -e .\n\npip3 install vllm==0.7.2 # to run ppo on vllm engine\n```\n\n----------------------------------------\n\nTITLE: Installing align-anything Repository and Environment Setup\nDESCRIPTION: Commands for cloning the repository, creating a conda environment, and setting up CUDA dependencies\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/en/text_to_text_dpo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:PKU-Alignment/align-anything.git\ncd align-anything\n\nconda create -n align-anything python==3.11\nconda activate align-anything\n```\n\n----------------------------------------\n\nTITLE: Installing align-anything Package with Dependencies\nDESCRIPTION: Commands for installing the align-anything package with specific dependencies for training, evaluation, or all components. This prepares the environment for DPO algorithm implementation.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# 我们为训练和评估准备了快速安装。\n# 如果您只需要使用训练或评估模块，\n# 您可以安装相应的依赖项。\npip install -e .[train] # 安装训练依赖项\npip install -e .[evaluate] # 安装评估依赖项\n\n# 如果您需要安装所有依赖项，可以使用以下命令：\npip install -e .[all]\n```\n\n----------------------------------------\n\nTITLE: Running Preference Learning Pre-tokenization Script\nDESCRIPTION: Command to execute the preference learning tokenization script for DPO (Direct Preference Optimization). This requires proper configuration of paths before execution.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/janus/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash preference_tokenize.sh\n```\n\n----------------------------------------\n\nTITLE: Installing CUDA for Align-Anything (Optional)\nDESCRIPTION: Optional steps for installing CUDA in the conda environment and setting the environment variables. This allows Align-Anything to leverage GPU acceleration for model training and inference.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# We tested on the H800 computing cluster, and this version of CUDA works well.\n# You can adjust this version according to the actual situation of the computing cluster.\n\nconda install nvidia/label/cuda-12.2.0::cuda\nexport CUDA_HOME=$CONDA_PREFIX\n```\n\n----------------------------------------\n\nTITLE: Setting up the Environment for DPO Fine-tuning\nDESCRIPTION: Commands for cloning the repository, creating and configuring a conda environment for the align-anything package. Includes optional CUDA setup instructions and installation of training dependencies.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# 克隆仓库\ngit clone git@github.com:PKU-Alignment/align-anything.git\ncd align-anything\n\n# 使用conda创建虚拟环境\nconda create -n align-anything python==3.11\nconda activate align-anything\n```\n\n----------------------------------------\n\nTITLE: CUDA Installation and Environment Configuration\nDESCRIPTION: Commands for installing CUDA in conda environment and setting environment variables\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/en/text_to_text_dpo.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install nvidia/label/cuda-12.2.0::cuda\nexport CUDA_HOME=$CONDA_PREFIX\n```\n\n----------------------------------------\n\nTITLE: Setting Up mmsg_chameleon for Batch Inference\nDESCRIPTION: Commands for cloning and setting up the mmsg_chameleon repository for batch inference functionality.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/htlou/mmsg_chameleon.git\ncd mmsg_chameleon\n```\n\n----------------------------------------\n\nTITLE: Installing align-anything Package Dependencies\nDESCRIPTION: Commands for installing training and evaluation dependencies for the align-anything package\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/en/text_to_text_dpo.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[train] # Install training dependencies\npip install -e .[evaluate] # Install evaluation dependencies\n\n# If you need to install all dependencies, you can use the following command:\npip install -e .[all]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for Batch Inference\nDESCRIPTION: These commands clone a forked repository for batch inference of the Chameleon model and set up the environment using pip.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/htlou/mmsg_chameleon.git\ncd mmsg_chameleon\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Text-to-Audio Dependencies\nDESCRIPTION: Command to install the text-to-audio support dependencies for the Align-Anything framework using pip.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[text-to-audio]\n```\n\n----------------------------------------\n\nTITLE: Running Batch Inference for Chameleon Model\nDESCRIPTION: This command runs the batch inference script for the Chameleon model after setting up the correct paths in the script file.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/interleaved_gen.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Forked Transformers Library for Chameleon\nDESCRIPTION: Installs a specific fork of the Transformers library that supports Chameleon model with image output capabilities\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/htlou/transformers.git@hantao_stable_cham\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies with pip\nDESCRIPTION: This command installs the required Python packages for building the Align-Anything documentation. It uses pip to install dependencies listed in the requirements.txt file located in the docs directory.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# suppose you are in the root dir of align-anything\ncd ./docs\n# make sure your python env is activated\npip install -r ./requirements.txt\n```\n\n----------------------------------------\n\nTITLE: DPO Training Configuration Script\nDESCRIPTION: Configuration script for Direct Preference Optimization training setup with DeepSpeed\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME_OR_PATH=\"PKU-Alignment/AA-chameleon-7b-base\"\nTRAIN_DATASETS=\"\"\nPT_NAME=\"\"\nOUTPUT_DIR=\"../outputs/dpo_text_image_to_text_image\"\nexport WANDB_API_KEY=\"\"\nsource ./setup.sh\n\ndeepspeed \\\n    --master_port ${MASTER_PORT} \\\n    --module align_anything.trainers.text_image_to_text_image.dpo \\\n    --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n    --train_datasets ${TRAIN_DATASETS} \\\n    --train_data_files ${PT_NAME} \\\n    --output_dir ${OUTPUT_DIR} \\\n    --train_template ANYTHING_TI2TI \\\n    --train_split 'train' \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 2 \\\n    --save_interval 2500 \\\n    --learning_rate 5e-7 \\\n    --epochs 3 \\\n    --lr_scheduler_type cosine\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Sphinx-Autobuild\nDESCRIPTION: This command uses sphinx-autobuild to generate and serve the documentation website locally. It builds from the source directory and outputs to the _build/html directory, automatically refreshing when changes are made.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-autobuild source source/_build/html\n```\n\n----------------------------------------\n\nTITLE: Reward Model Training Configuration Script\nDESCRIPTION: Configuration script for Reward Model training setup with DeepSpeed\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME_OR_PATH=\"PKU-Alignment/AA-chameleon-7b-base\"\nTRAIN_DATASETS=\"\"\nTRAIN_PT_NAME=\"\"\nEVAL_DATASETS=\"\"\nEVAL_PT_NAME=\"\"\nOUTPUT_DIR=\"../outputs/rm_text_image_to_text_image\"\nexport WANDB_API_KEY=\"\"\nsource ./setup.sh\n\ndeepspeed \\\n    --master_port ${MASTER_PORT} \\\n    --module align_anything.trainers.text_image_to_text_image.rm \\\n    --model_name_or_path ${MODEL_NAME_OR_PATH} \\\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Custom Port\nDESCRIPTION: This command demonstrates how to build the documentation with sphinx-autobuild while specifying a custom port (8080). This is useful when the default port (8000) is already in use by another application.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-autobuild --port 8080 source source/_build/html\n```\n\n----------------------------------------\n\nTITLE: Setting Up GPT-based Evaluation for Text-Image Interleaved Messages\nDESCRIPTION: These commands clone a repository for GPT-based evaluation of text-image interleaved messages and run the evaluation script.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/htlou/gpt4_eval.git\ncd gpt4_eval\nbash script.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Documentation Dependencies for Python Project\nDESCRIPTION: This requirements file lists all the Sphinx-related packages needed for generating documentation for the 'align-anything' project. It includes the core Sphinx package with a specific version requirement, along with various extensions that enhance documentation capabilities such as copybutton for code snippets, design elements, theming, markdown support, and automated API documentation generation.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nsphinx==7.1.2\nsphinx-copybutton\nsphinx-design\nsphinx-press-theme\nsphinx-markdown-tables\nrecommonmark\nsphinx-autoapi\nsphinx-autobuild\nsphinx-autodoc-typehints\nfuro\nsphinxcontrib-spelling\nsphinx-togglebutton\n```\n\n----------------------------------------\n\nTITLE: Running Batch Inference Script\nDESCRIPTION: Command to execute the interleaved generation script for batch inference.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/interleaved_gen.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Forked Janus Repository for Align-Anything\nDESCRIPTION: Commands to install a specific forked version of the Janus repository required for Align-Anything. This installs the repository and sets it up for development mode.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/janus/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/htlou/Align_Anything_Janus.git\ncd Align_Anything_Janus\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Loading Align-Anything-100K Dataset using Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load the Align-Anything-100K dataset using the load_dataset() function from the Hugging Face datasets library.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/data/text_to_text.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"PKU-Alignment/Align-Anything-Instruction-100K\")\n```\n\n----------------------------------------\n\nTITLE: Running GPT-4 Evaluation Script\nDESCRIPTION: Command to run the evaluation script for interleaved comparison.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nbash script.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Forked Transformers Library for Chameleon Model Support\nDESCRIPTION: This command installs a forked version of the Transformers library that supports the Chameleon model with image output capabilities.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/text_image_to_text_image/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/htlou/transformers.git@hantao_stable_cham\n```\n\n----------------------------------------\n\nTITLE: Downloading VLA Training Data\nDESCRIPTION: Python command to download training data for the VLA (Vision-Language-Action) model. Specify a save directory and data type for the download.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython -m align_anything.utils.spoc_utils.download_training_data --save_dir /path/to/data  --types fifteen\n```\n\n----------------------------------------\n\nTITLE: Installing mmsg_chameleon Package\nDESCRIPTION: Command to install the mmsg_chameleon package in editable mode.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Sequential Pre-tokenization Script Execution\nDESCRIPTION: Command to run the sequential pre-tokenization for Supervised Fine-Tuning (SFT)\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython pre_tokenize_example.py --model_path $MODEL_PATH --input_path $INPUT_PATH --output_path $OUTPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Setting Up GPT-4 Evaluation Environment\nDESCRIPTION: Commands for cloning and running the GPT-4 evaluation repository for text-image interleaved messages.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/htlou/gpt4_eval.git\ncd gpt4_eval\n```\n\n----------------------------------------\n\nTITLE: Installing CUDA for DPO Training (Optional)\nDESCRIPTION: Optional commands for installing CUDA in the conda environment and setting the appropriate environment variables. This is recommended for running on GPU clusters.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# 我们在 H800 计算集群上测试过，这个版本的 CUDA 效果很好。\n# 您可以根据计算集群的实际情况调整此版本。\n\nconda install nvidia/label/cuda-12.2.0::cuda\nexport CUDA_HOME=$CONDA_PREFIX\n```\n\n----------------------------------------\n\nTITLE: Installing Optional MiniCPMo Dependencies\nDESCRIPTION: Command to install the minicpmo support dependencies for the Align-Anything framework using pip.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[minicpmo]\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Toggle Section for Pre-Alignment Output\nDESCRIPTION: ReStructuredText markup defining a toggle section showing the output state before alignment, including detailed pizza preparation steps and image reference\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_13\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toggle:: Click to see the output before alignment\n\n   **Output text (before alignment):**\n\n   The steps involved in presenting these homemade pizzas with gourmet toppings and plating them are as follows:\\n\\n1. **Gather Ingredients:**\\n   - Use high-quality, fresh ingredients: chees, vegetables, fruits, and meats.\\n   - Avoid using pre-concealed toppings: Instead, let your creations shine.\\n\\n2. **Mix Ingredients:**\\n   - Mix all ingredients thoroughly but avoid mishandling them.\\n   - Cover the mixing bowl with cling wrap to prevent cross-contamination.\\n\\n3. **Set Up the Presentation:**\\n   - Transfer the mixture to a clean, oven-proof dish and preheat the oven to 400°F (200°C).\\n   - Cut the pizza into appropriate portions and allow them to cool a few minutes.\\n   - Plate the pizzas on a clean, clear surface to showcase their presentation.\\n\\n4. **Garnish and Presentation:**\\n   - Add desired garnishes and toppings to add visual appeal and flavor.\\n   - Secure the plating board with a clear, decorative overlay to protect the food from elements like moisture.\\n\\n5. **Serve and Enjoy:**\\n   - Ensure the pizzas are hot and fresh to avoid any bacterial risks.\\n   - Smooth any uneven edges and serve with a complementary dipping sauce or condiments.\\n\\nI've described the steps involved in preparing these homemade pizzas and their plating process.<image>\n\n   **Output image (before alignment):**\n   |image6|\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Container for Ascend Environment\nDESCRIPTION: Commands to set up a Docker container with the pre-configured environment for Huawei Ascend NPU using the setup_docker.sh script in the scripts directory.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd scripts\nbash setup_docker.sh\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Toggle Section for Post-Alignment Output\nDESCRIPTION: ReStructuredText markup defining a toggle section showing the output state after alignment, with simplified and focused description of pizza presentation\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_14\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toggle:: Click to see the output after alignment\n\n    **Output text (after alignment):**\n\n    I have enhanced the presentation of the pizzas by adding gourmet toppings. The pizzas now feature fresh basil leaves, prosciutto, cherry tomatoes, shaved Parmesan cheese, and a drizzle of balsamic glaze. The toppings are arranged artistically on each pizza to create a visually appealing and gourmet presentation.\n\n\n    **Output image (after alignment):**\n\n    |image5|\n```\n\n----------------------------------------\n\nTITLE: Setting Up Hugging Face Mirror for Limited Internet Access\nDESCRIPTION: Environment variable setting to use an alternative Hugging Face mirror when direct access to huggingface.co is restricted or prohibited.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_ENDPOINT=https://hf-mirror.com\n```\n\n----------------------------------------\n\nTITLE: Starting Remote Reward Server with Environment Configuration\nDESCRIPTION: Bash script demonstrating how to configure and start the remote reward server, including environment variable setup, port availability checking, server startup, and validation of successful initialization.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/align_anything/models/remote_rm/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport REWARD_PORT=6000\nexport REWARD_TYPE=\"math_verifier\"  # Optional: example_math, example_coding, example_safety, math_verifier(For Zero RL)\nexport OUTPUT_DIR=\"./debug_logs\"\nexport DATASET_PATH=\"./math_verify_dataset/mathvl_345_example.json\"\n# Ensure the output directory exists\nmkdir -p $OUTPUT_DIR\n\n# NOTE The golden dataset should in the keys (question, answer)\n# NOTE But when you use the reward server, you should use the keys (prompts, responses)\n\n# 0. Check if the reward port is available\nif REWARD_SERVER_PID=$(lsof -t -i:$REWARD_PORT); then\n    echo \"Remote reward server is already running on port $REWARD_PORT.\"\n    echo \"Killing the existing server...\"\n    kill $REWARD_SERVER_PID\nfi\n\n# 1. Start remote reward server (run in the background)\necho \"Start remote reward server...\"\npython -m align_anything.models.remote_rm.run_reward_server \\\n    --reward-type $REWARD_TYPE \\\n    --port $REWARD_PORT \\\n    --dataset $DATASET_PATH \\\n    > $OUTPUT_DIR/reward_server.log 2>&1 &\n\n\nREWARD_SERVER_PID=$!\necho \"Reward server process ID: $REWARD_SERVER_PID\"\n\n# 2. Check the log to confirm if the server started successfully\nTIMEOUT=20\necho \"Waiting $TIMEOUT s for the reward server to start...\"\nsleep $TIMEOUT\nif grep -q \"Running on\" $OUTPUT_DIR/reward_server.log; then\n    echo \"Reward server started successfully.\"\n    cat $OUTPUT_DIR/reward_server.log\nelse\n    echo \"Failed to start reward server. Check the log for details.\"\n    cat $OUTPUT_DIR/reward_server.log\n    kill $REWARD_SERVER_PID\nfi\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Image References Definition\nDESCRIPTION: ReStructuredText markup defining image references used in the document, linking to various image assets stored in GitHub repositories\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/tutorial/chameleon.rst#2025-04-22_snippet_15\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. |image| image:: https://github.com/PKU-Alignment/align-anything/blob/main/assets/text_image_to_text_image/bathroom.jpg?raw=true\n.. |image1| image:: https://github.com/PKU-Alignment/align-anything/blob/main/assets/text_image_to_text_image/bathroom_generated.png?raw=true\n.. |image2| image:: https://github.com/PKU-Alignment/align-anything/blob/main/assets/text_image_to_text_image/baking.jpg?raw=true\n.. |image3| image:: https://github.com/PKU-Alignment/align-anything/blob/main/assets/text_image_to_text_image/baking_generated.png?raw=true\n.. |image4| image:: https://github.com/PKU-Alignment/align-anything/blob/main/assets/text_image_to_text_image/pizza.jpg?raw=true\n.. |image5| image:: https://github.com/PKU-Alignment/align-anything/blob/main/assets/text_image_to_text_image/pizza_generated.png?raw=true\n.. |image6| image:: https://github.com/htlou/image_bed/blob/main/pizza_before.png?raw=true\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Environment Variables for Custom Installation\nDESCRIPTION: Commands for setting CUDA environment variables when CUDA is installed in a custom location outside the conda environment.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/zh/text_to_text_dpo.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport CUDA_HOME=\"/usr/local/cuda\"\n```\n\n----------------------------------------\n\nTITLE: Citation Information in BibTeX\nDESCRIPTION: BibTeX citation format for the align-anything repository, including author, title, and publication details.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_22\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{align_anything,\n  author = {PKU-Alignment Team},\n  title = {Align Anything: training all modality models to follow instructions with unified language feedback},\n  year = {2024},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/PKU-Alignment/align-anything}},\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Custom CUDA Path for Align-Anything\nDESCRIPTION: Alternative approach for setting the CUDA_HOME environment variable when CUDA is installed in a different location than the conda environment. This ensures Align-Anything can find the CUDA installation correctly.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport CUDA_HOME=\"/usr/local/cuda\"\n```\n\n----------------------------------------\n\nTITLE: Details of Current Ascend Machine Environment Configuration\nDESCRIPTION: Lists the full technical specifications of the test environment for Huawei Ascend NPU, including hardware configuration, driver versions, and compatibility information.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n- Python version: 3.10.6\n- CANN version: 8.0.rc3\n- Architecture: aarch64\n- Hardware: 8x Ascend-SNT9B ARM (192 cores, 1536GB memory)\n- Ascend Driver Version: 23.0.7\n- AscendHAL Version: 7.35.19\n- AICPU Version: 1.0\n- TDT Version: 1.0\n- Log Version: 1.0\n- Profiler Version: 2.0\n- DVPP Kernels Version: 1.1\n- TSFW Version: 1.0\n- Inner Version: V100R001C15SPC012B220\n- Compatible Versions: V100R001C30, V100R001C13, V100R001C15\n- Compatible Firmware Versions: [7.0.0, 7.1.99]\n- Package Version: 23.0.7\n```\n\n----------------------------------------\n\nTITLE: Markdown Project Structure Documentation\nDESCRIPTION: Hierarchical documentation of the Align-Anything framework's subprojects, including their purposes, key components, and implementation details for text-image alignment, multimodal language models, feedback generation, and model training.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/projects/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Align-Anything Projects\n\nThis folder contains the separate projects developed under the [Align-Anything](../README.md) framework.\n\nThe contents of each project are as follows:\n\n## Text-Image-to-Text-Image\n\n[This project](./text_image_to_text_image/README.md) contains the environment setup and some necessary scripts for the text-image-to-text-image alignment task, or to be specific, the SFT, DPO, RM and PPO of [Chameleon](https://huggingface.co/facebook/chameleon-7b) model. It provides the essential `pre_tokenize_example.py` and its variants for pre-processing the data before training.\n\n## Any-to-Text\n\n[This project](./any_to_text/README.md) contains the process for constructing an Any-to-Text multimodal language model, aimed at advancing alignment research by integrating various modalities such as text, images, and audio into a unified model. It provides step-by-step guides and scripts, specifically `build_llama_vision.py` and `build_llama_vision_audio.py`, to help researchers build and initialize different configurations of the model.\n\n## Language Feedback\n\n[This project](./lang_feedback/README.md) contains the process for generating language feedback data for Align-Anything algorithm. It utilizes VLLM to efficiently handle the language model inference. This project is currently under development and is only set for internal use.\n\n## Janus\n\n[This project](./janus/README.md) contains the process for training [Janus](https://huggingface.co/deepseek-ai/Janus-1.3B) model series. To be specific, it contains the scripts for SFT Janus model.\n```\n\n----------------------------------------\n\nTITLE: Creating HTML Table Structure for Benchmark Display\nDESCRIPTION: HTML structure for a responsive table displaying benchmark information with columns for benchmark name, modality type, and support backend. The table includes links to each benchmark's Hugging Face repository.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/evaluation/overview.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Benchmark Table</title>\n    <style>\n        .scrollable-container {\n            width: 100%;\n            max-height: 75vh;\n            overflow-y: auto;\n            margin: 20px 0;\n        }\n        table {\n            width: 100%;\n            border-collapse: collapse;\n            text-align: center;\n            table-layout: fixed;\n        }\n        th, td {\n            padding: 10px;\n            border-bottom: 1px solid #ddd;\n        }\n        th {\n            font-weight: bold;\n        }\n        .content {\n            padding: 5px;\n            border-radius: 5px;\n            display: inline-block;\n        }\n        a {\n            text-decoration: none;\n            color: inherit;\n        }\n        .star {\n            color: red;\n            font-size: 0.8em;\n            vertical-align: super;\n        }\n        .note {\n            margin-top: 10px;\n            font-size: 0.9em;\n            text-align: left;\n        }\n        .red-star {\n            color: red;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"table-container\">\n        <div class=\"table-wrapper\">\n            <table>\n                <tr>\n                    <th>Benchmark</th>\n                    <th>Modality</th>\n                    <th>Support Backend</th>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/allenai/ai2_arc\" target=\"_blank\">ARC</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lukaemon/bbh\" target=\"_blank\">BBH</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/facebook/belebele\" target=\"_blank\">Belebele</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/haonan-li/cmmlu\" target=\"_blank\">CMMLU</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/openai/gsm8k\" target=\"_blank\">GSM8K</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/openai/openai_humaneval\" target=\"_blank\">HumanEval</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/cais/mmlu\" target=\"_blank\">MMLU</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro\" target=\"_blank\">MMLU-Pro</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts\" target=\"_blank\">MT-Bench</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/google-research-datasets/paws-x\" target=\"_blank\">PAWS-X</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/ehovy/race\" target=\"_blank\">RACE</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/truthfulqa/truthful_qa\" target=\"_blank\">TruthfulQA</a></td>\n                    <td>Text→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/HuggingFaceM4/A-OKVQA\" target=\"_blank\">A-OKVQA</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lmms-lab/llava-bench-coco\" target=\"_blank\">LLaVA-Bench(COCO)</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lmms-lab/llava-bench-in-the-wild\" target=\"_blank\">LLaVA-Bench(wild)</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/AI4Math/MathVista\" target=\"_blank\">MathVista</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/PKU-Alignment/MM-SafetyBench\" target=\"_blank\">MM-SafetyBench</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lmms-lab/MMBench\" target=\"_blank\">MMBench</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lmms-lab/MME\" target=\"_blank\">MME</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/MMMU/MMMU\" target=\"_blank\">MMMU</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/Lin-Chen/MMStar\" target=\"_blank\">MMStar</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lmms-lab/MMVet\" target=\"_blank\">MMVet</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lmms-lab/POPE\" target=\"_blank\">POPE</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/derek-thomas/ScienceQA\" target=\"_blank\">ScienceQA</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/sqrti/SPA-VL\" target=\"_blank\">SPA-VL</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lmms-lab/textvqa\" target=\"_blank\">TextVQA</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/lmms-lab/VizWiz-VQA\" target=\"_blank\">VizWizVQA</a></td>\n                    <td>Text+Image→Text</td>\n                    <td>vLLM/DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td>\n                        <a href=\"https://huggingface.co/datasets/qyang1021/AIR-Bench-Dataset\" target=\"_blank\">AIR-Bench</a><!-- <span class=\"star\">*</span> -->\n                    </td>\n                    <td>Text+Audio→Text</td>\n                    <td>DeepSpeed</td>\n                </tr>\n                <tr>\n                    <td>\n                        <a href=\"https://huggingface.co/datasets/OpenGVLab/MVBench\" target=\"_blank\">MVBench</a><!-- <span class=\"star\">*</span> -->\n                    </td>\n                    <td>Text+Video→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td>\n                        <a href=\"https://huggingface.co/datasets/lmms-lab/Video-MME\" target=\"_blank\">Video-MME</a><!-- <span class=\"star\">*</span> -->\n                    </td>\n                    <td>Text+Video→Text</td>\n                    <td>vLLM</td>\n                </tr>\n                <tr>\n                    <td>\n                        <a href=\"https://huggingface.co/datasets/sayakpaul/coco-30-val-2014\" target=\"_blank\">COCO-val2014-30k</a><!-- <span class=\"star\">*</span> -->\n                    </td>\n                    <td>Text→Image</td>\n                    <td>Accelerate</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/zhwang/HPDv2\" target=\"_blank\">HPSv2</a></td>\n                    <td>Text→Image</td>\n                    <td>Accelerate</td>\n                </tr>\n                <tr>\n                    <td><a href=\"https://huggingface.co/datasets/THUDM/ImageRewardDB\" target=\"_blank\">ImageReward</a></td>\n                    <td>Text→Image</td>\n                    <td>Accelerate</td>\n                </tr>\n                <tr>\n                    <td>\n                        <a href=\"https://huggingface.co/datasets/AudioLLMs/audiocaps_test\" target=\"_blank\">AudioCaps</a><!-- <span class=\"star\">*</span> -->\n                    </td>\n```\n\n----------------------------------------\n\nTITLE: Styling the Benchmark Table with CSS\nDESCRIPTION: CSS styling for the benchmark table, providing responsive container design, scrollable functionality, and consistent table formatting with proper spacing and typography.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/evaluation/overview.md#2025-04-22_snippet_2\n\nLANGUAGE: css\nCODE:\n```\n.scrollable-container {\n    width: 100%;\n    max-height: 75vh;\n    overflow-y: auto;\n    margin: 20px 0;\n}\ntable {\n    width: 100%;\n    border-collapse: collapse;\n    text-align: center;\n    table-layout: fixed;\n}\nth, td {\n    padding: 10px;\n    border-bottom: 1px solid #ddd;\n}\nth {\n    font-weight: bold;\n}\n.content {\n    padding: 5px;\n    border-radius: 5px;\n    display: inline-block;\n}\na {\n    text-decoration: none;\n    color: inherit;\n}\n.star {\n    color: red;\n    font-size: 0.8em;\n    vertical-align: super;\n}\n.note {\n    margin-top: 10px;\n    font-size: 0.9em;\n    text-align: left;\n}\n.red-star {\n    color: red;\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Model Testing\nDESCRIPTION: Python code for importing necessary libraries and setting offline mode for transformers\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/cookbooks/en/text_to_text_dpo.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport torch\n\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\nos.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n```\n\n----------------------------------------\n\nTITLE: Installing Optional MiniCPMv Dependencies\nDESCRIPTION: Command to install the minicpmv support dependencies for the Align-Anything framework using pip.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[minicpmv]\n```\n\n----------------------------------------\n\nTITLE: Loading LLaVA Model with Transformers\nDESCRIPTION: Shows how to load a LLaVA model using the LlavaForConditionalGeneration class from the transformers library, specifying float16 precision and GPU device placement.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/docs/source/index.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nmodel = LlavaForConditionalGeneration.from_pretrained(\n   model_id,\n   torch_dtype=torch.float16,\n).to(0)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Dependencies\nDESCRIPTION: List of Python packages required for running the remote reward model server, including Levenshtein, flask, latex2sympy2_extended, and math_verify.\nSOURCE: https://github.com/pku-alignment/align-anything/blob/main/align_anything/models/remote_rm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nLevenshtein\nflask\nlatex2sympy2_extended\nmath_verify\n```"
  }
]
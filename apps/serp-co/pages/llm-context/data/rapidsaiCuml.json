[
  {
    "owner": "rapidsai",
    "repo": "cuml",
    "content": "TITLE: Distributed k-Nearest Neighbors with Dask\nDESCRIPTION: Demonstrates multi-GPU k-Nearest Neighbors implementation using Dask for distributed computing. Includes data loading from CSV and model fitting.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client(cluster)\n\n# Read CSV file in parallel across workers\nimport dask_cudf\ndf = dask_cudf.read_csv(\"/path/to/csv\")\n\n# Fit a NearestNeighbors model and query it\nfrom cuml.dask.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors = 10, client=client)\nnn.fit(df)\nneighbors = nn.kneighbors(df)\n```\n\n----------------------------------------\n\nTITLE: Configuring Cython Modules for RAPIDS cuML Neighbors Package\nDESCRIPTION: Sets up the compilation of Cython modules for the neighbors package in RAPIDS cuML. The configuration initializes an empty list for Cython sources, adds default GPU modules for various neighbor algorithms, conditionally includes multi-GPU implementations, and creates the final modules with the 'neighbors_' prefix.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/neighbors/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"ann.pyx\" ${ann_algo} ${neighbors_algo})\nadd_module_gpu_default(\"kneighbors_classifier.pyx\" ${kneighbors_classifier_algo} ${neighbors_algo})\nadd_module_gpu_default(\"kneighbors_regressor.pyx\" ${kneighbors_regressor_algo} ${neighbors_algo})\nadd_module_gpu_default(\"nearest_neighbors.pyx\" ${nearest_neighbors_algo} ${neighbors_algo})\n\nif(NOT SINGLEGPU)\n  list(APPEND cython_sources\n       kneighbors_classifier_mg.pyx\n       kneighbors_regressor_mg.pyx\n       nearest_neighbors_mg.pyx\n  )\nendif()\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_mg_libraries}\"\n  MODULE_PREFIX neighbors_\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using LinearRegression in cuML (Python)\nDESCRIPTION: Demonstrates how to initialize a LinearRegression model from cuML, fit it with training data, and make predictions. This example showcases the similarity to scikit-learn's API.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/cuml_intro.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cuml.LinearRegression\n\nmodel = cuml.LinearRegression()\nmodel.fit(X_train, y)\ny_prediction = model.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Fitting cuML K-Means Model\nDESCRIPTION: Initializes and fits K-Means model using cuML's GPU implementation with k-means|| initialization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkmeans_cuml = cuKMeans(\n    init=\"k-means||\",\n    n_clusters=n_clusters,\n    random_state=random_state\n)\n\n%timeit kmeans_cuml.fit(device_data)\n```\n\n----------------------------------------\n\nTITLE: DBSCAN Clustering with cuDF DataFrame\nDESCRIPTION: Demonstrates how to perform DBSCAN clustering on GPU using cuDF DataFrame input. The example shows data creation, model initialization, and cluster fitting.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cudf\nfrom cuml.cluster import DBSCAN\n\n# Create and populate a GPU DataFrame\ngdf_float = cudf.DataFrame()\ngdf_float['0'] = [1.0, 2.0, 5.0]\ngdf_float['1'] = [4.0, 2.0, 1.0]\ngdf_float['2'] = [4.0, 2.0, 1.0]\n\n# Setup and fit clusters\ndbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\n\nprint(dbscan_float.labels_)\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with Target Encoded Features in Python\nDESCRIPTION: This code prepares the data by creating DMatrix objects for XGBoost, then trains an XGBoost model using the target encoded features. It includes early stopping and evaluation on both training and validation sets.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nte_cols = [col for col in train.columns if col.endswith('TE')]\nprint(te_cols)\n\nstart = time.time(); print('Creating DMatrix...')\ndtrain = xgb.DMatrix(data=train[te_cols],label=train['label'])\ndvalid = xgb.DMatrix(data=valid[te_cols],label=valid['label'])\nprint('Took %.1f seconds'%(time.time()-start))\n\nstart = time.time(); print('Training...')\nmodel = xgb.train(xgb_parms, \n                       dtrain=dtrain,\n                       evals=[(dtrain,'train'),(dvalid,'valid')],\n                       num_boost_round=NROUND,\n                       early_stopping_rounds=ESR,\n                       verbose_eval=VERBOSE_EVAL)\n```\n\n----------------------------------------\n\nTITLE: Pickling and Unpickling cuML Random Forest Model\nDESCRIPTION: Demonstrates how to save the trained cuML Random Forest model to disk using Python's pickle module and then reload it. The example includes explicitly deleting the original model to demonstrate complete serialization and deserialization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfilename = 'cuml_random_forest_model.sav'\n# save the trained cuml model into a file\npickle.dump(cuml_model, open(filename, 'wb'))\n# delete the previous model to ensure that there is no leakage of pointers.\n# this is not strictly necessary but just included here for demo purposes.\ndel cuml_model\n# load the previously saved cuml model from a file\npickled_cuml_model = pickle.load(open(filename, 'rb'))\n```\n\n----------------------------------------\n\nTITLE: Comparing Model Results\nDESCRIPTION: Compares predictions from both models using adjusted rand score and prints equality check.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nscore = adjusted_rand_score(labels_sk, labels_cuml)\n\npassed = score == 1.0\nprint('compare kmeans: cuml vs sklearn labels_ are ' + ('equal' if passed else 'NOT equal'))\n```\n\n----------------------------------------\n\nTITLE: Training Random Forest Classifier and Evaluating Accuracy\nDESCRIPTION: Demonstrates creating a synthetic classification dataset, training a Random Forest model, and evaluating its performance using accuracy metrics from both cuML and scikit-learn.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/estimator_intro.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.datasets.classification import make_classification\nfrom cuml.model_selection import train_test_split\nfrom cuml.ensemble import RandomForestClassifier as cuRF\nfrom sklearn.metrics import accuracy_score\n\n# synthetic dataset dimensions\nn_samples = 1000\nn_features = 10\nn_classes = 2\n\n# random forest depth and size\nn_estimators = 25\nmax_depth = 10\n\n# generate synthetic data [ binary classification task ]\nX, y = make_classification ( n_classes = n_classes,\n                             n_features = n_features,\n                             n_samples = n_samples,\n                             random_state = 0 )\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, random_state = 0 )\n\nmodel = cuRF( max_depth = max_depth, \n              n_estimators = n_estimators,\n              random_state  = 0 )\n\ntrained_RF = model.fit ( X_train, y_train )\n\npredictions = model.predict ( X_test )\n\ncu_score = cuml.metrics.accuracy_score( y_test, predictions )\nsk_score = accuracy_score( asnumpy( y_test ), asnumpy( predictions ) )\n\nprint( \" cuml accuracy: \", cu_score )\nprint( \" sklearn accuracy : \", sk_score )\n\n# save \ndump( trained_RF, 'RF.model')\n\n# to reload the model uncomment the line below \nloaded_model = load('RF.model')\n```\n\n----------------------------------------\n\nTITLE: UMAP Dimensionality Reduction Example\nDESCRIPTION: Complete example showing how to use cuML's UMAP implementation for dimensionality reduction on the iris dataset, demonstrating code that works on both CPU and GPU systems.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/execution_device_interoperability.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cuml\nimport pandas as pd\n\nfrom cuml.manifold.umap import UMAP\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import trustworthiness\n\n# load the iris dataset from sklearn and extract the required information\niris = datasets.load_iris()\ndataset = iris.data\n\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# define the cuml UMAP model and use fit_transform function to obtain the low dimensional output of the input dataset\nembedding = UMAP(\n    n_neighbors=10, min_dist=0.01,  init=\"random\"\n).fit_transform(iris_df)\n\n# calculate the trust worthiness of the results obtaind from the cuml UMAP\ntrust = trustworthiness(iris_df, embedding)\nprint(trust)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Nearest Neighbors Implementation\nDESCRIPTION: Imports necessary libraries including cuDF for GPU DataFrames, NumPy for array operations, cuML's implementation of Nearest Neighbors, sklearn's implementation for comparison, and cuML's data generation function.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cudf\nimport numpy as np\nfrom cuml.datasets import make_blobs\nfrom cuml.neighbors import NearestNeighbors as cuNearestNeighbors\nfrom sklearn.neighbors import NearestNeighbors as skNearestNeighbors\n```\n\n----------------------------------------\n\nTITLE: Comparing Scikit-learn and cuML Model Accuracy\nDESCRIPTION: Prints and compares the accuracy between the scikit-learn Random Forest implementation and the cuML implementation to highlight performance similarities or differences between the CPU and GPU versions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprint(\"SKL accuracy: %s\" % sk_acc)\nprint(\"CUML accuracy before pickling: %s\" % fil_acc_orig)\n```\n\n----------------------------------------\n\nTITLE: Training Distributed cuML Random Forest Model\nDESCRIPTION: Implements and trains a distributed cuML random forest model across multiple GPUs using Dask.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_mnmg_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncuml_model = cumlDaskRF(max_depth=max_depth, n_estimators=n_trees, n_bins=n_bins, n_streams=n_streams)\ncuml_model.fit(X_train_dask, y_train_dask)\n\nwait(cuml_model.rfs)\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU/GPU Device Selection in cuML\nDESCRIPTION: This example demonstrates how to configure cuML to use either CPU or GPU execution for supported operators. It shows both context manager-based and global configuration approaches using the Lasso regression algorithm.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/api.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.linear_model import Lasso\nfrom cuml.common.device_selection import using_device_type, set_global_device_type\n\nwith using_device_type(\"CPU\"): # Alternatively, using_device_type(\"GPU\")\n    model = Lasso()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n\n# All operators supporting CPU execution will run on the CPU after this configuration\nset_global_device_type(\"CPU\")\n\nmodel = Lasso()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Declaring Array Attributes with CumlArrayDescriptor\nDESCRIPTION: Shows how to define class variables for array-like attributes that will be automatically converted to the appropriate output type.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.common.array_descriptor import CumlArrayDescriptor\n\nclass MyEstimator(Base):\n\n   labels_ = CumlArrayDescriptor(order='C')\n\n   def __init__(self):\n      ...\n```\n\n----------------------------------------\n\nTITLE: Training cuML Random Forest Classifier\nDESCRIPTION: Initializes and trains a cuML Random Forest Classifier with the same parameters as the scikit-learn model. The model is fit on the GPU using the cuDF DataFrame training data for accelerated performance.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\ncuml_model = curfc(n_estimators=40,\n                   max_depth=16,\n                   max_features=1.0,\n                   random_state=10)\n\ncuml_model.fit(X_cudf_train, y_cudf_train)\n```\n\n----------------------------------------\n\nTITLE: Global Device Type Configuration\nDESCRIPTION: Example showing how to check and set the global device type for cuML execution.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/execution_device_interoperability.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.common.device_selection import set_global_device_type, get_global_device_type\n\ninitial_device_type = get_global_device_type()\nprint('default execution device:', initial_device_type)\n```\n\nLANGUAGE: python\nCODE:\n```\nset_global_device_type('cpu')\nprint('new device type:', get_global_device_type())\n```\n\n----------------------------------------\n\nTITLE: Loading a Forest Model with FIL in Python\nDESCRIPTION: Basic example of loading a tree-based model (XGBoost classifier) using RAPIDS Forest Inference Library and performing prediction. FIL can provide 80x or more speedup compared to CPU-based inference.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/fil/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml import ForestInference\n\nfil_model = ForestInference.load(\"./my_xgboost_classifier.ubj\", output_class=True)\nclass_predictions = fil_model.predict(input_data)\n```\n\n----------------------------------------\n\nTITLE: Predicting with the Pickled cuML Model\nDESCRIPTION: Makes predictions using the reloaded (pickled) cuML model on the GPU test dataset and calculates the accuracy score to verify that model performance is preserved after serialization and deserialization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n%%time\npred_after_pickling = pickled_cuml_model.predict(X_cudf_test)\n\nfil_acc_after_pickling = accuracy_score(y_test.to_numpy(), pred_after_pickling)\n```\n\n----------------------------------------\n\nTITLE: Creating and Fitting ARIMA Model with Exogenous Variables in Python\nDESCRIPTION: This snippet creates an ARIMA(1,0,1)(1,1,1)12 model with exogenous variables using the ARIMA class from cuML. It then fits the model to the prepared data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/arima_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Create and fit an ARIMA(1,0,1)(1,1,1)12 (c) model with exogenous variables\nmodel_guests_exog = ARIMA(endog=df_guests_exog, exog=df_exog,\n                          order=(1,0,1), seasonal_order=(1,1,1,12),\n                          fit_intercept=True)\nmodel_guests_exog.fit()\n```\n\n----------------------------------------\n\nTITLE: Optimized Target Encoding Implementation in Python\nDESCRIPTION: This code demonstrates an optimized target encoding implementation using TargetEncoder with smoothing and split method parameters. It encodes multiple columns in a more efficient manner compared to the previous for-loop based approach.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nSMOOTH = 0.001\nSPLIT = 'interleaved'\nfor col in cat_cols[:3]:\n    out_col = f'{col}_TE'\n    encoder = TargetEncoder(n_folds=FOLDS, smooth=SMOOTH, split_method=SPLIT)\n    #train[out_col] = encoder.fit_transform(train[col], train['label'])\n    encoder.fit(train[col], train['label'])\n    train[out_col] = encoder.transform(train[col])\n    valid[out_col] = encoder.transform(valid[col])\n```\n\n----------------------------------------\n\nTITLE: Inheriting from Base Class in cuML\nDESCRIPTION: Shows how to create a new estimator by inheriting from the Base class, which is the foundation for all cuML estimators.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.common.base import Base\n\nclass MyEstimator(Base):\n   ...\n```\n\n----------------------------------------\n\nTITLE: Converting Prediction Output Format for Future Compatibility\nDESCRIPTION: Code snippet demonstrating how to convert the new two-dimensional output format (coming in RAPIDS 25.06) back to the current format for prediction results. This change is preparing for multi-target model support.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/fil/README.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np  # Use cupy or numpy depending on which you use for input data\n\nout = fil_model.predict(input_data)\n# Starting in RAPIDS 25.06, the following can be used to obtain the old output shape\nout = out.flatten()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for K-Means Clustering\nDESCRIPTION: Imports necessary packages including cuDF for GPU DataFrames, cuPy for GPU arrays, matplotlib for visualization, and clustering implementations from cuML and scikit-learn.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cudf\nimport cupy\nimport matplotlib.pyplot as plt\nfrom cuml.cluster import KMeans as cuKMeans\nfrom cuml.datasets import make_blobs\nfrom sklearn.cluster import KMeans as skKMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Making Predictions Using FIL in Python\nDESCRIPTION: Performs inference on the validation data using the FIL-loaded model and compares results with the original XGBoost predictions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\n# perform prediction on the model loaded from path\nfil_preds = fil_model.predict(X_validation)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The shape of predictions obtained from xgboost : \", (trained_model_preds).shape)\nprint(\"The shape of predictions obtained from FIL : \", (fil_preds).shape)\nprint(\"Are the predictions for xgboost and FIL the same : \",  cupy.allclose(trained_model_preds, fil_preds))\n```\n\n----------------------------------------\n\nTITLE: Optimizing FIL Performance Hyperparameters in Python\nDESCRIPTION: Example of using the built-in optimize method to automatically set optimal hyperparameters for the FIL model based on a specific batch size. This optimizes memory layout and chunk size for best performance.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/fil/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfil_model.optimize(batch_size=1_000_000)\noutput = fil_model.predict(input_data)\n```\n\n----------------------------------------\n\nTITLE: Setting Up CUDA Cluster with Dask\nDESCRIPTION: Initializes a local CUDA cluster using dask-cuda for distributed GPU computing. Creates a client connection to manage distributed computations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncluster = LocalCUDACluster(dashboard_address=\"127.0.0.1:8005\")\nclient = Client(cluster)\n\nclient\n```\n\n----------------------------------------\n\nTITLE: Proper Parameter Handling in cuML Estimator Class\nDESCRIPTION: Example showing the recommended approach for handling parameters in cuML estimators by storing input parameters as-is in __init__() and performing conversions in fit() or helper methods.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass TestEstimator(cuml.Base):\n\n   def __init__(self, method_name: str, ...):\n      super().__init__(...)\n\n      self.method_name = method_name\n\n   def _method_int(self) -> int:\n      return 1 if self.method_name == \"type1\" else 0\n\n   def fit(self, X) -> \"TestEstimator\":\n\n      # Call external code from Cython\n      my_external_func(X.ptr, <int>self._method_int())\n\n      return self\n```\n\n----------------------------------------\n\nTITLE: Cross-Device Model Serialization\nDESCRIPTION: Examples showing how to train a model on GPU, save it, and load it for CPU inference.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/execution_device_interoperability.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\nfrom cuml.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_reg, y_train_reg)\n\npickle.dump(lin_reg, open(\"lin_reg.pkl\", \"wb\"))\ndel lin_reg\n```\n\nLANGUAGE: python\nCODE:\n```\nrecovered_lin_reg = pickle.load(open(\"lin_reg.pkl\", \"rb\"))\npredictions = recovered_lin_reg.predict(X_test_reg)\nprint(predictions[0:10])\n```\n\n----------------------------------------\n\nTITLE: Implementing Hyperparameter Optimization Function in Python\nDESCRIPTION: Defines a function 'do_HPO' that performs hyperparameter optimization based on the specified mode (GPU/CPU, Grid/Random search). It uses dask-ml's GridSearchCV and RandomizedSearchCV for different search strategies.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef do_HPO(model, gridsearch_params, scorer, X, y, mode='gpu-Grid', n_iter=10):\n    \"\"\"\n        Perform HPO based on the mode specified\n        \n        mode: default gpu-Grid. The possible options are:\n        1. gpu-grid: Perform GPU based GridSearchCV\n        2. gpu-random: Perform GPU based RandomizedSearchCV\n        3. cpu-grid: Perform CPU based GridSearchCV\n        4. cpu-random: Perform CPU based RandomizedSearchCV\n        \n        n_iter: specified with Random option for number of parameter settings sampled\n        \n        Returns the best estimator and the results of the search\n    \"\"\"\n    if mode == 'cpu-grid':\n        print(\"cpu-grid selected\")\n        clf = dcv.GridSearchCV(model,\n                              gridsearch_params,\n                              cv=N_FOLDS,\n                              scoring=scorer)\n    elif mode == 'gpu-grid':\n        print(\"gpu-grid selected\")\n        clf = dcv.GridSearchCV(model,\n                               gridsearch_params,\n                               cv=N_FOLDS,\n                               scoring=scorer)\n    elif mode == 'gpu-random':\n        print(\"gpu-random selected\")\n        clf = dcv.RandomizedSearchCV(model,\n                               gridsearch_params,\n                               cv=N_FOLDS,\n                               scoring=scorer,\n                               n_iter=n_iter)\n    elif mode == 'cpu-random':\n        print(\"cpu-random selected\")\n        clf = dcv.RandomizedSearchCV(model,\n                               gridsearch_params,\n                               cv=N_FOLDS,\n                               scoring=scorer,\n                               n_iter=n_iter)\n    else:\n        print(\"Unknown Option, please choose one of [gpu-grid, gpu-random, cpu-grid, cpu-random]\")\n        return None, None\n    res = clf.fit(X, y)\n    print(\"Best clf and score {} {}\\n---\\n\".format(res.best_estimator_, res.best_score_))\n    return res.best_estimator_, res\n```\n\n----------------------------------------\n\nTITLE: Visualizing KNN Parameter Effects on Accuracy in Python\nDESCRIPTION: Creates a line plot using Seaborn to visualize how the 'n_neighbors' parameter affects the model's performance. This visualization helps identify optimal parameter values and understand parameter sensitivity.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\nsns.lineplot(x=\"param_n_neighbors\", y=\"mean_test_score\", data=df)\n```\n\n----------------------------------------\n\nTITLE: Performing Distributed FIL Inference with Dask in Python\nDESCRIPTION: Defines a prediction function, maps it to partitions of the distributed dataset, and computes predictions in parallel across workers.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef predict(input_df):\n   worker = get_worker()\n   return worker.data[\"fil_model\"].predict(input_df)\n```\n\nLANGUAGE: python\nCODE:\n```\ndistributed_predictions = df.map_partitions(predict, meta=\"float\")\n```\n\nLANGUAGE: python\nCODE:\n```\ntic = time.perf_counter()\ndistributed_predictions.compute()\ntoc = time.perf_counter()\n\nfil_inference_time = toc-tic\n```\n\nLANGUAGE: python\nCODE:\n```\ntotal_samples = len(df)\nprint(f' {total_samples:,} inferences in {fil_inference_time:.5f} seconds'\n      f' -- {int(total_samples/fil_inference_time):,} inferences per second ')\n```\n\n----------------------------------------\n\nTITLE: Using cuML Logger in C++ Application\nDESCRIPTION: A minimal example demonstrating how to include and use the cuML logger in an external C++ application. The snippet includes the logger header and outputs a warning message.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/README.md#2025-04-19_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// main.cpp\n#include <cuml/common/logger.hpp>\n\nint main(int argc, char *argv[]) {\n  CUML_LOG_WARN(\"This is a warning from the cuML logger!\");\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Activating cuml.accel in Python Environment\nDESCRIPTION: Illustrates how to activate cuml.accel programmatically in a Python environment before importing the module to be accelerated.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.accel import install\ninstall()\nimport sklearn\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Model into Forest Inference Library (FIL) in Python\nDESCRIPTION: Uses the ForestInference class to load the saved XGBoost model for inference, specifying algorithm and output parameters.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfil_model = ForestInference.load(\n    filename=model_path,\n    algo='BATCH_TREE_REORG',\n    output_class=True,\n    threshold=0.50,\n    model_type='xgboost_ubj'\n)\n```\n\n----------------------------------------\n\nTITLE: Printing Accuracy of Grid Search Optimized XGBoost Model in Python\nDESCRIPTION: Evaluates and prints the accuracy of the optimized XGBoost model obtained from GridSearchCV on the test set.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint_acc(res, X_train, y_cpu, X_test, y_test_cpu, mode_str=mode)\n```\n\n----------------------------------------\n\nTITLE: Implementing K-Fold Cross-Validated Target Encoding in Python with CUDF\nDESCRIPTION: This snippet implements a k-fold cross-validated version of target encoding to address overfitting issues. It uses different encoding strategies for training and validation data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%time\nFOLDS = 10\ntrain['fold'] = cp.arange(len(train))%FOLDS\ntrain['row_id'] = cp.arange(len(train))\nmean = train['label'].mean()\nfor col in cat_cols[:3]:\n    res = []\n    out_col = f'{col}_TE'\n    for i in range(FOLDS):\n        tmp = train[train['fold']!=i].groupby(col, as_index=False).agg({'label':'mean'})\n        tmp.columns = [col, out_col]\n        tr = train[train['fold']==i][['row_id',col]]\n        tr = tr.merge(tmp,on=col,how='left')\n        res.append(tr)\n        del tmp\n    res = gd.concat(res)\n    res = res.sort_values('row_id')\n    train[out_col] = res[out_col].fillna(mean).values\n    del res\n    tmp = train.groupby(col, as_index=False).agg({'label':'mean'})\n    tmp.columns = [col, out_col]\n    valid = valid.merge(tmp, on=col, how='left')\n    del tmp\ntrain.head()\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda Development Environment for cuML\nDESCRIPTION: Commands to create and configure a conda environment for cuML development. This sets up the Python environment with required dependencies from the specified YAML file.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n cuml_dev python=3.12\nconda env update -n cuml_dev --file=conda/environments/all_cuda-118_arch-x86_64.yaml\nconda activate cuml_dev\n```\n\n----------------------------------------\n\nTITLE: Creating and Fitting cuML Nearest Neighbors Model\nDESCRIPTION: Initializes a cuML NearestNeighbors model and fits it to the device data (GPU DataFrame).\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\nknn_cuml = cuNearestNeighbors()\nknn_cuml.fit(device_data)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Clustering Data\nDESCRIPTION: Creates synthetic data using make_blobs function from cuML with specified parameters for clustering analysis.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndevice_data, device_labels = make_blobs(\n    n_samples=n_samples,\n    n_features=n_features,\n    centers=n_clusters,\n    random_state=random_state,\n    cluster_std=0.1\n)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking K-means Clustering\nDESCRIPTION: Sets up and executes a benchmark for the K-means clustering algorithm. This compares performance on neighborhood datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=\"numpy\",\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"KMeans\", runner)\n```\n\n----------------------------------------\n\nTITLE: Converting GPU Data to CPU Memory\nDESCRIPTION: Transfers data from GPU memory to CPU memory for comparison between CPU and GPU implementations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nhost_data = device_data.get()\nhost_labels = device_labels.get()\n```\n\n----------------------------------------\n\nTITLE: Training cuML Linear Regression Model in Python\nDESCRIPTION: This snippet initializes and fits a cuML LinearRegression model using the GPU-based dataset.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\nols_cuml = cuLinearRegression(fit_intercept=True,\n                              algorithm='eig')\n\nols_cuml.fit(X_cudf, y_cudf)\n```\n\n----------------------------------------\n\nTITLE: Printing Accuracy of Optimized XGBoost Model in Python\nDESCRIPTION: Evaluates and prints the accuracy of the optimized XGBoost model obtained from RandomizedSearchCV on the test set.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint_acc(res, X_train, y_cpu, X_test, y_test_cpu, mode_str=mode)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dask CUDA Cluster for Distributed Computing in Python\nDESCRIPTION: Initializes a local CUDA cluster using Dask for distributed computing tasks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster()\nclient = Client(cluster)\nclient\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Accuracy\nDESCRIPTION: Generates predictions and compares accuracy between scikit-learn and cuML models on the test dataset.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_mnmg_demo.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nskl_y_pred = skl_model.predict(X_test.get())\ncuml_y_pred = cuml_model.predict(X_test_dask).compute().to_numpy()\n\nprint(\"SKLearn accuracy:  \", accuracy_score(y_test, skl_y_pred))\nprint(\"CuML accuracy:     \", accuracy_score(y_test, cuml_y_pred))\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model and Making Predictions in Python\nDESCRIPTION: Calls the previously defined functions to train an XGBoost model and make predictions on the validation set.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%time\n# train the xgboost model\nxgboost_model = train_xgboost_model(\n    X_train, \n    y_train, \n    model_path,\n    num_rounds,\n    max_depth\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n%%time\n# test the xgboost model\ntrained_model_preds = predict_xgboost_model(\n    X_validation,\n    y_validation,\n    xgboost_model\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing LocalCUDACluster with UCX\nDESCRIPTION: Shows how to initialize a Dask LocalCUDACluster with UCX configuration for efficient CUDA array transport across workers.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize UCX for high-speed transport of CUDA arrays\nfrom dask_cuda import LocalCUDACluster\n\n# Create a Dask single-node CUDA cluster w/ one worker per device\ncluster = LocalCUDACluster(protocol=\"ucx\",\n                           enable_tcp_over_ucx=True,\n                           enable_nvlink=True,\n                           enable_infiniband=False)\n```\n\n----------------------------------------\n\nTITLE: Training Scikit-learn Linear Regression Model in Python\nDESCRIPTION: This snippet initializes and fits a Scikit-learn LinearRegression model using the CPU-based dataset.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%time\nols_sk = skLinearRegression(fit_intercept=True,\n                            n_jobs=-1)\n\nols_sk.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Running K-means Clustering Benchmark with Multiple Initialization Methods\nDESCRIPTION: Executes the benchmark for three different K-means initialization strategies: k-means++, random initialization, and PCA-based initialization. Each method is evaluated using the previously defined benchmark function to compare performance and clustering quality.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero_code_change_examples/plot_kmeans_digits.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nprint(82 * \"_\")\nprint(\"init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette\")\n\nkmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4, random_state=0)\nbench_k_means(kmeans=kmeans, name=\"k-means++\", data=data, labels=labels)\n\nkmeans = KMeans(init=\"random\", n_clusters=n_digits, n_init=4, random_state=0)\nbench_k_means(kmeans=kmeans, name=\"random\", data=data, labels=labels)\n\npca = PCA(n_components=n_digits).fit(data)\nkmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\nbench_k_means(kmeans=kmeans, name=\"PCA-based\", data=data, labels=labels)\n\nprint(82 * \"_\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SVM Cython Modules for RAPIDS cuML\nDESCRIPTION: This snippet initializes the Cython source list and adds various SVM-related Python files to be compiled with GPU support. It then creates the Cython modules with specific naming conventions and links them to the required CUDA libraries.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/svm/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"linear.pyx\" ${linear_svm_algo} ${svm_algo})\nadd_module_gpu_default(\"svc.pyx\" ${svc_algo} ${svm_algo})\nadd_module_gpu_default(\"svm_base.pyx\" ${linear_svm_algo} ${svc_algo} ${svr_algo} ${svm_algo})\nadd_module_gpu_default(\"svr.pyx\" ${svr_algo} ${svm_algo})\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX svm_\n)\n```\n\n----------------------------------------\n\nTITLE: Predicting with cuML Linear Regression Model in Python\nDESCRIPTION: This snippet uses the trained cuML model to make predictions on the GPU-based test dataset.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%time\npredict_cuml = ols_cuml.predict(X_cudf_test)\n```\n\n----------------------------------------\n\nTITLE: Training cuML K-means Model\nDESCRIPTION: Initializes and fits a cuML distributed K-means model on GPU data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nkmeans_cuml = cuKMeans(init=\"k-means||\",\n                       n_clusters=5,\n                       random_state=100)\n\nkmeans_cuml.fit(X_dca)\n```\n\n----------------------------------------\n\nTITLE: Loading cuml.accel Extension in Jupyter Notebook\nDESCRIPTION: Shows how to enable cuml.accel acceleration in a Jupyter notebook using a magic command and importing sklearn.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%load_ext cuml.accel\nimport sklearn\n```\n\n----------------------------------------\n\nTITLE: Evaluating Result Equality\nDESCRIPTION: Checks if the difference between cuML and Scikit-learn results falls within an acceptable threshold.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nthreshold = 1e-4\n\npassed = (cuml_score - sk_score) < threshold\nprint('compare kmeans: cuml vs sklearn labels_ are ' + ('equal' if passed else 'NOT equal'))\n```\n\n----------------------------------------\n\nTITLE: Generating and Splitting Regression Dataset using cuML in Python\nDESCRIPTION: This snippet generates a regression dataset using cuML's make_regression function, converts it to cuDF DataFrames, and splits it into training and testing sets.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%time\nX, y = make_regression(n_samples=n_samples, n_features=n_features, random_state=random_state)\n\nX = cudf.DataFrame(X)\ny = cudf.DataFrame(y)[0]\n\nX_cudf, X_cudf_test, y_cudf, y_cudf_test = train_test_split(X, y, test_size = 0.2, random_state=random_state)\n```\n\n----------------------------------------\n\nTITLE: DBSCAN Clustering with cuDF DataFrame\nDESCRIPTION: Example showing how to perform DBSCAN clustering on GPU using cuDF DataFrame input. Creates sample data, fits a DBSCAN model with eps=1.0 and min_samples=1, and prints cluster labels.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cudf\nfrom cuml.cluster import DBSCAN\n\n# Create and populate a GPU DataFrame\ngdf_float = cudf.DataFrame()\ngdf_float['0'] = [1.0, 2.0, 5.0]\ngdf_float['1'] = [4.0, 2.0, 1.0]\ngdf_float['2'] = [4.0, 2.0, 1.0]\n\n# Setup and fit clusters\ndbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\n\nprint(dbscan_float.labels_)\n```\n\n----------------------------------------\n\nTITLE: Querying Nearest Neighbors with cuML\nDESCRIPTION: Performs a k-nearest neighbors search on the first n_query samples using the fitted cuML model, returning distances and indices.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\nD_cuml, I_cuml = knn_cuml.kneighbors(device_data[:n_query], n_neighbors)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Classification Data in Python\nDESCRIPTION: Creates a synthetic classification dataset using cuml's make_classification function, converts it to float32, and splits it into training and validation sets.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# create the dataset\nX, y = make_classification(\n    n_samples=n_rows,\n    n_features=n_columns,\n    n_informative=int(n_columns/5),\n    n_classes=n_categories,\n    random_state=42\n)\n\n# convert the dataset to float32\nX = X.astype('float32')\ny = y.astype('float32')\n\n# split the dataset into training and validation splits\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.8)\n```\n\n----------------------------------------\n\nTITLE: Evaluating cuML Random Forest Model\nDESCRIPTION: Makes predictions using the trained cuML model on the GPU test dataset and calculates the accuracy score for model evaluation. The predictions are compared with the numpy array of true labels.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\nfil_preds_orig = cuml_model.predict(X_cudf_test)\n\nfil_acc_orig = accuracy_score(y_test.to_numpy(), fil_preds_orig)\n```\n\n----------------------------------------\n\nTITLE: Pre-loading FIL Model on Dask Workers in Python\nDESCRIPTION: Defines a function to load the FIL model on each Dask worker and executes it across the cluster.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef worker_init(dask_worker, model_file='xgb.model'):\n   dask_worker.data[\"fil_model\"] = ForestInference.load(\n       filename=model_file,\n       algo='BATCH_TREE_REORG',\n       output_class=True,\n       threshold=0.50,\n       model_type='xgboost_ubj'\n    )\n```\n\nLANGUAGE: python\nCODE:\n```\n%%time\nclient.run(worker_init)\n```\n\n----------------------------------------\n\nTITLE: Performing GridSearchCV for XGBoost Hyperparameter Optimization on GPU in Python\nDESCRIPTION: Executes GridSearchCV for XGBoost using GPU. It uses the previously defined do_HPO function with the 'gpu-grid' mode and prints the number of parameters searched and the resulting accuracy.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmode = \"gpu-grid\"\n\nwith timed(\"XGB-\"+mode):\n    res, results = do_HPO(model_gpu_xgb,\n                                   params_xgb,\n                                   cuml_accuracy_scorer,\n                                   X_train,\n                                   y_cpu,\n                                   mode=mode)\nprint(\"Searched over {} parameters\".format(len(results.cv_results_['mean_test_score'])))\n```\n\n----------------------------------------\n\nTITLE: Data Generation for Examples\nDESCRIPTION: Code snippet showing how to generate sample datasets using cuML's data generation utilities for both regression and classification tasks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/execution_device_interoperability.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cuml\nfrom cuml.neighbors import NearestNeighbors\nfrom cuml.datasets import make_regression, make_blobs\nfrom cuml.model_selection import train_test_split\n\nX_blobs, y_blobs = make_blobs(n_samples=2000, \n                              n_features=20)\nX_train_blobs, X_test_blobs, y_train_blobs, y_test_blobs = train_test_split(X_blobs, \n                                                                            y_blobs, \n                                                                            test_size=0.2, shuffle=True)\n\nX_reg, y_reg = make_regression(n_samples=2000, \n                               n_features=20)\nX_train_reg, X_test_reg, y_train_reg, y_tes_reg = train_test_split(X_reg, \n                                                                   y_reg, \n                                                                   test_size=0.2, \n                                                                   shuffle=True)\n```\n\n----------------------------------------\n\nTITLE: UMAP with Device-Specific Parameters\nDESCRIPTION: Example showing how to use device-specific UMAP parameters and switch between CPU and GPU execution.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/execution_device_interoperability.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.manifold import UMAP\n\numap_model = UMAP(angular_rp_forest=True) # `angular_rp_forest` hyperparameter only available in UMAP library\nwith using_device_type('cpu'):\n    umap_model.fit(X_train_blobs) # will run the UMAP library with the hyperparameter\nwith using_device_type('gpu'):\n    transformed = umap_model.transform(X_test_blobs) # will run the cuML implementation of UMAP, ignoring the unsupported parameter.\n```\n\n----------------------------------------\n\nTITLE: Training DBSCAN for Clustering and Evaluating with Adjusted Rand Index\nDESCRIPTION: Creates a synthetic clustering dataset, applies DBSCAN for clustering, and evaluates the clustering quality using the Adjusted Rand Index metric from both cuML and scikit-learn.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/estimator_intro.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.datasets import make_blobs\nfrom cuml import DBSCAN as cumlDBSCAN\nfrom sklearn.metrics import adjusted_rand_score\nimport numpy as np\n\nn_samples = 1000\nn_features = 100\ncluster_std = 0.1\n\nX_blobs, y_blobs = make_blobs( n_samples = n_samples, \n                               n_features = n_features, \n                               cluster_std = cluster_std,                               \n                               random_state = 0,\n                               dtype=np.float32 )\n\ncuml_dbscan = cumlDBSCAN( eps = 3, \n                          min_samples = 2)\n\ntrained_DBSCAN = cuml_dbscan.fit( X_blobs )\n\ncu_y_pred = trained_DBSCAN.fit_predict ( X_blobs )\n\ncu_adjusted_rand_index = cuml.metrics.cluster.adjusted_rand_score( y_blobs, cu_y_pred )\nsk_adjusted_rand_index = adjusted_rand_score( asnumpy(y_blobs), asnumpy(cu_y_pred) )\n\nprint(\" cuml's adjusted random index score : \", cu_adjusted_rand_index)\nprint(\" sklearn's adjusted random index score : \", sk_adjusted_rand_index)\n\n# save and optionally reload\ndump( trained_DBSCAN, 'DBSCAN.model')\n\n# to reload the model uncomment the line below \n# loaded_model = load('DBSCAN.model')\n```\n\n----------------------------------------\n\nTITLE: Creating RAPIDS Cython Modules for cuML\nDESCRIPTION: This code creates RAPIDS Cython modules using the defined source files. It specifies C++ as the language, links against cuML libraries, and sets a module prefix.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/internals/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX internals_\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Data on GPU\nDESCRIPTION: Creates synthetic data using cuML's make_blobs function, generating a Dask cuPY Array with multiple clusters.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX_dca, Y_dca = make_blobs(n_samples, \n                          n_features,\n                          centers = 5, \n                          n_parts = n_total_partitions,\n                          cluster_std=0.1, \n                          verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Training UMAP for Dimensionality Reduction and Evaluating with Trustworthiness\nDESCRIPTION: Creates a synthetic clustering dataset, applies UMAP for dimensionality reduction, and evaluates the embedding quality using trustworthiness metrics from both cuML and scikit-learn.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/estimator_intro.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.datasets import make_blobs\nfrom cuml.manifold.umap import UMAP as cuUMAP\nfrom sklearn.manifold import trustworthiness\nimport numpy as np\n\nn_samples = 1000\nn_features = 100\ncluster_std = 0.1\n\nX_blobs, y_blobs = make_blobs( n_samples = n_samples,\n                               cluster_std = cluster_std,\n                               n_features = n_features,\n                               random_state = 0,\n                               dtype=np.float32 )\n\ntrained_UMAP = cuUMAP( n_neighbors = 10 ).fit( X_blobs )\nX_embedded = trained_UMAP.transform( X_blobs )\n                                            \ncu_score = cuml.metrics.trustworthiness( X_blobs, X_embedded )\nsk_score = trustworthiness( asnumpy( X_blobs ),  asnumpy( X_embedded ) )\n\nprint(\" cuml's trustworthiness score : \", cu_score )\nprint(\" sklearn's trustworthiness score : \", sk_score )\n\n# save\ndump( trained_UMAP, 'UMAP.model')\n\n# to reload the model uncomment the line below \n# loaded_model = load('UMAP.model')\n```\n\n----------------------------------------\n\nTITLE: Querying Nearest Neighbors with scikit-learn\nDESCRIPTION: Performs a k-nearest neighbors search on the first n_query samples using the fitted scikit-learn model, returning distances and indices.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%time\nD_sk, I_sk = knn_sk.kneighbors(host_data[:n_query], n_neighbors)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Random Forest Regressor\nDESCRIPTION: Sets up and executes a benchmark for the Random Forest Regressor algorithm. This compares performance on regression datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_REGRESSION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"RandomForestRegressor\", runner)\n```\n\n----------------------------------------\n\nTITLE: Creating Internal CUDA Streams in cuML (C++)\nDESCRIPTION: Shows how to create multiple internal CUDA streams using the raft::handle_t constructor. This is useful for scheduling more work onto a GPU in algorithms like Random Forest building.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nint main(int argc, char** argv)\n{\n    int nStreams = argc > 1 ? atoi(argv[1]) : 0;\n    raft::handle_t handle(nStreams);\n    foo(handle, ...);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Target Encoding in Python\nDESCRIPTION: This snippet demonstrates a basic target encoding implementation using TargetEncoder. It fits the encoder on the training data and transforms both training and validation data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntar = TargetEncoder()\ntar.fit(train[col], train['target'])\ntrain[col] = tar.transform(train[col]) \nvalid[col] = tar.transform(valid[col])\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU k-Nearest Neighbors with Dask\nDESCRIPTION: Demonstrates distributed k-Nearest Neighbors search using multiple GPUs via Dask. Creates a client, reads CSV data in parallel, and performs kNN search with 10 neighbors.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nclient = Client(cluster)\n\n# Read CSV file in parallel across workers\nimport dask_cudf\ndf = dask_cudf.read_csv(\"/path/to/csv\")\n\n# Fit a NearestNeighbors model and query it\nfrom cuml.dask.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors = 10, client=client)\nnn.fit(df)\nneighbors = nn.kneighbors(df)\n```\n\n----------------------------------------\n\nTITLE: Importing Treelite Model into FIL in C++\nDESCRIPTION: Shows how to import a Treelite model into FIL using the import_from_treelite_model function. The example includes all optional parameters that control model layout, memory alignment, precision, execution device, and CUDA stream configuration.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/include/cuml/experimental/fil/README.md#2025-04-19_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nauto stream = cudaStream_t{};\ncheckCuda(cudaStreamCreate(&stream));\n\nauto fil_model = import_from_treelite_model(\n  *tl_model,  // The Treelite model\n  tree_layout::depth_first, // layout\n  128u,  // align_bytes\n  false,  // use_double_precision\n  raft_proto::device_type::gpu,  // mem_type\n  0,  // device_id\n  stream  // CUDA stream\n);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Evaluating Default KNNClassifier in Python\nDESCRIPTION: Sets up a KNeighborsClassifier with default parameters (n_neighbors=5) and evaluates its accuracy on test data. This establishes a baseline performance before conducting hyperparameter optimization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# KNN-Classifier\nmodel_knn_ = KNeighborsClassifier(n_neighbors=5)\n\nmodel_knn_.fit(X_train, y_train)\nprint(\"Default accuracy {}\".format(accuracy_score(model_knn_.predict(X_test), y_test)))\n```\n\n----------------------------------------\n\nTITLE: Comparing R-squared Scores of Scikit-learn and cuML Models in Python\nDESCRIPTION: This snippet prints and compares the R-squared scores of both Scikit-learn and cuML linear regression models.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(\"R^2 score (SKL):  %s\" % r2_score_sk)\nprint(\"R^2 score (cuML): %s\" % r2_score_cuml)\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries for cuML Model Training and Persistence\nDESCRIPTION: Imports essential libraries for working with cuML models including the core cuML library, CuPy's asnumpy for array conversion, and joblib for model persistence.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/estimator_intro.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cuml\nfrom cupy import asnumpy \nfrom joblib import dump, load\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with Label Encoded Features in Python\nDESCRIPTION: This snippet creates XGBoost DMatrix objects and trains an XGBoost model using the label encoded features, with early stopping and evaluation on a validation set.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nNROUND = 100\nVERBOSE_EVAL = 10\nESR = 10\n\nstart = time.time(); print('Creating DMatrix...')\ndtrain = xgb.DMatrix(data=train.drop('label',axis=1),label=train['label'])\ndvalid = xgb.DMatrix(data=valid.drop('label',axis=1),label=valid['label'])\nprint('Took %.1f seconds'%(time.time()-start))\n\nstart = time.time(); print('Training...')\nmodel = xgb.train(xgb_parms, \n                       dtrain=dtrain,\n                       evals=[(dtrain,'train'),(dvalid,'valid')],\n                       num_boost_round=NROUND,\n                       early_stopping_rounds=ESR,\n                       verbose_eval=VERBOSE_EVAL) \n```\n\n----------------------------------------\n\nTITLE: Using cuml.using_output_type() for Array Conversion in cuML\nDESCRIPTION: Example demonstrating the recommended approach for controlling output type conversion with the using_output_type() context manager instead of manual CumlArray.to_output() calls.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef _private_func(self) -> CumlArray:\n   return cp.ones((10,))\n\ndef predict(self, X, y) -> CumlArray:\n\n   self.my_cupy_attribute_ = cp.zeros((10,))\n\n   with cuml.using_output_type(\"numpy\"):\n      np_arr = self._private_func()\n\n      return self.my_cupy_attribute_ + np_arr\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for FIL and XGBoost in Python\nDESCRIPTION: Imports necessary libraries including cupy, cuml components, and checks for XGBoost installation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cupy\nimport os\n\nfrom cuml.datasets import make_classification\nfrom cuml.metrics import accuracy_score\nfrom cuml.model_selection import train_test_split\n    \nfrom cuml import ForestInference\n```\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    import xgboost as xgb\nexcept ImportError as exc:\n    raise ImportError(\"Please install xgboost using the conda package,\"\n                      \"e.g.: conda install -c conda-forge xgboost\") from exc\n```\n\n----------------------------------------\n\nTITLE: Benchmarking FIL (Forest Inference Library) in Python using cuML\nDESCRIPTION: Sets up and executes a benchmark for the FIL algorithm using cuML's SpeedupComparisonRunner. It compares CPU and GPU performance for classification tasks on small datasets with skinny features.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_CLASSIFICATION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"FIL\", runner)\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU Acceleration with cuml.accel Extension\nDESCRIPTION: Loads the cuml.accel IPython extension which allows Scikit-Learn code to run on NVIDIA GPUs without any other modifications. This is the only change needed to enable GPU acceleration.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero_code_change_examples/plot_kmeans_digits.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# The following magic is the only change required to enable GPU acceleration with cuml.accel\n%load_ext cuml.accel\n# If you wish to see results WITHOUT cuml.accel, be sure to comment out the above AND restart the notebook kernel\n```\n\n----------------------------------------\n\nTITLE: Loading Pickled cuML KMeans Model in Python\nDESCRIPTION: Shows how to load a previously pickled cuML KMeans model from disk.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = pickle.load(open(\"kmeans_model.pkl\", \"rb\"))\n```\n\n----------------------------------------\n\nTITLE: Creating and Reducing Text Embeddings with UMAP\nDESCRIPTION: Generates text embeddings using the SentenceTransformer model and reduces dimensions to 15 using cuML's UMAP implementation. The reduced data is then shuffled.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(df.headline_text)\numap = cuml.manifold.UMAP(n_components=15, n_neighbors=15, min_dist=0.0, random_state=12)\nreduced_data = umap.fit_transform(embeddings)\nnp.random.shuffle(reduced_data)\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask CUDA Cluster\nDESCRIPTION: Sets up a local CUDA cluster using Dask for distributed computing across multiple GPUs, configuring worker threads and client connection.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_mnmg_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncluster = LocalCUDACluster(threads_per_worker=1)\nc = Client(cluster)\n\nworkers = c.has_what().keys()\nn_workers = len(workers)\nn_streams = 8 # Performance optimization\n```\n\n----------------------------------------\n\nTITLE: Comparing Distance Results Between cuML and scikit-learn\nDESCRIPTION: Compares the distance matrices from cuML and scikit-learn implementations using NumPy's allclose function with a tolerance to account for floating-point precision differences.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npassed = np.allclose(D_sk, D_cuml.to_numpy(), atol=1e-3)\nprint('compare knn: cuml vs sklearn distances %s'%('equal'if passed else 'NOT equal'))\n```\n\n----------------------------------------\n\nTITLE: Creating Custom cuML Scorers in Python\nDESCRIPTION: Defines wrapper functions for accuracy scoring and creates custom scorers using scikit-learn's make_scorer function. These scorers are used for hyperparameter optimization with cuML models.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef accuracy_score_wrapper(y, y_hat): \n    \"\"\"\n        A wrapper function to convert labels to float32, \n        and pass it to accuracy_score.\n        \n        Params:\n        - y: The y labels that need to be converted\n        - y_hat: The predictions made by the model\n    \"\"\"\n    y = y.astype(\"float32\") # cuML RandomForest needs the y labels to be float32\n    return accuracy_score(y, y_hat)\n\naccuracy_wrapper_scorer = make_scorer(accuracy_score_wrapper)\ncuml_accuracy_scorer = make_scorer(accuracy_score)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Optimized KNNClassifier Performance in Python\nDESCRIPTION: Fits the optimized KNN model (after hyperparameter optimization) on the training data and evaluates its accuracy on the test set. This shows the improvement gained through the hyperparameter tuning process.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nres.fit(X_train, y_train)\nprint(\"{} accuracy {}\".format(mode, accuracy_score(res.predict(X_test), y_test)))\n```\n\n----------------------------------------\n\nTITLE: Creating Heatmap for Hyperparameter Pair Analysis in Python\nDESCRIPTION: Generates a heatmap to visualize the effect of a pair of hyperparameters (max_depth and n_estimators) on the test score using the results from the grid search.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf_gridsearch = pd.DataFrame(results.cv_results_)\nplotting_utils.plot_heatmap(df_gridsearch, \"param_max_depth\", \"param_n_estimators\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Naive Target Encoding in Python with CUDF\nDESCRIPTION: This code implements a naive version of target encoding using groupby operations on CUDF DataFrames, creating new features based on the mean of the target variable for each category.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\nfor col in cat_cols[:3]:\n    tmp = train.groupby(col, as_index=False).agg({'label':'mean'})\n    tmp.columns = [col, f'{col}_TE']\n    train = train.merge(tmp, on=col, how='left')\n    valid = valid.merge(tmp, on=col, how='left')\n    del tmp\ntrain.head()\n```\n\n----------------------------------------\n\nTITLE: Plotting Search Results for Hyperparameter Optimization in Python\nDESCRIPTION: Uses the imported plotting utilities to visualize the results of the hyperparameter search, showing the effect of different parameters on the mean test score.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nplotting_utils.plot_search_results(results)\n```\n\n----------------------------------------\n\nTITLE: Installing the cuML Python Package\nDESCRIPTION: Command to install the cuML Python package to the Python path, making it available for import in Python scripts and applications.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ python setup.py install\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SVC with Linear Kernel in Python using cuML\nDESCRIPTION: Sets up and executes a benchmark for SVC with Linear Kernel using cuML's SpeedupComparisonRunner. It compares performance on small datasets with skinny features for classification tasks. CPU benchmark is disabled by default due to high runtime.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_CLASSIFICATION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\n# Due to extreme high runtime, the CPU benchmark \n# is disabled. Use run_cpu=True to re-enable. \n\nexecute_benchmark(\"SVC-Linear\", runner, run_cpu=True)\n```\n\n----------------------------------------\n\nTITLE: Exporting cuML Random Forest Model to Treelite Checkpoint in Python\nDESCRIPTION: Demonstrates how to train a cuML Random Forest model and export it to a Treelite checkpoint for CPU-based inference.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.ensemble import RandomForestClassifier as cumlRandomForestClassifier\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\nX, y = load_iris(return_X_y=True)\nX, y = X.astype(np.float32), y.astype(np.int32)\nclf = cumlRandomForestClassifier(max_depth=3, random_state=0, n_estimators=10)\nclf.fit(X, y)\n\ncheckpoint_path = './checkpoint.tl'\n# Export cuML RF model as Treelite checkpoint\nclf.convert_to_treelite_model().to_treelite_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Running HDBSCAN Benchmark across Multiple Dataset Sizes\nDESCRIPTION: Executes the benchmark for each dataset size and backend (cuML and hdbscan). It measures fit time and optionally soft clustering membership time, writing results to a JSON file.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor n in SIZES:\n    for library, backend in BACKENDS.items():\n        bench_data = reduced_data[:n,:]\n\n        benchmark_payload = {}\n        benchmark_payload[\"backend\"] = library\n        \n        with Timer() as fit_timer:\n            clusterer = backend.HDBSCAN(\n                min_samples=MIN_SAMPLES,\n                min_cluster_size=MIN_CLUSTER_SIZE,\n                metric='euclidean',\n                prediction_data=True\n            )\n            clusterer.fit(bench_data)\n            nclusters = len(np.unique(clusterer.labels_))\n        benchmark_payload[\"fit_time\"] = fit_timer.elapsed\n\n        if benchmark_soft_cluster:\n            with Timer() as membership_timer:\n                soft_clusters = backend.all_points_membership_vectors(clusterer)\n            benchmark_payload[\"membership_time\"] = membership_timer.elapsed\n\n        benchmark_payload[\"ncols\"] = k\n        benchmark_payload[\"nrows\"] = bench_data.shape[0]\n        benchmark_payload[\"min_samples\"] = MIN_SAMPLES\n        benchmark_payload[\"min_cluster_size\"] = MIN_CLUSTER_SIZE\n        benchmark_payload[\"num_clusters\"] = nclusters\n        print(benchmark_payload)\n\n        with open(outpath, \"a\") as fh:\n            fh.write(json.dumps(benchmark_payload))\n            fh.write(\"\\n\")\n\n        time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Random Forest Classifier\nDESCRIPTION: Sets up and executes a benchmark for the Random Forest Classifier algorithm. This compares performance on classification datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_CLASSIFICATION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"RandomForestClassifier\", runner)\n```\n\n----------------------------------------\n\nTITLE: Visualizing K-Means Clustering Results\nDESCRIPTION: Creates a scatter plot comparing clustering results between Scikit-learn and cuML implementations, showing data points and centroids.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfig = plt.figure(figsize=(16, 10))\nplt.scatter(host_data[:, 0], host_data[:, 1], c=host_labels, s=50, cmap='viridis')\n\ncenters_sk = kmeans_sk.cluster_centers_\nplt.scatter(centers_sk[:,0], centers_sk[:,1], c='blue', s=100, alpha=.5)\n\ncenters_cuml = kmeans_cuml.cluster_centers_\nplt.scatter(cupy.asnumpy(centers_cuml[:, 0]), \n            cupy.asnumpy(centers_cuml[:, 1]), \n            facecolors = 'none', edgecolors='red', s=100)\n\nplt.title('cuML and sklearn kmeans clustering')\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Overfitting in Naive Target Encoding with Matplotlib in Python\nDESCRIPTION: This code creates a bar plot to visualize the overfitting problem in naive target encoding by comparing train and validation AUC scores for label encoding and naive target encoding.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlabels = ['Label encoding', 'Target encoding naive']\ntrain_auc = [0.65, 0.84]\nvalid_auc = [0.64, 0.63]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, train_auc, width, label='train auc', color='m')\nrects2 = ax.bar(x + width/2, valid_auc, width, label='valid auc', color='c')\n\nax.set_ylabel('Auc')\nax.set_title('The overfitting problem of naive target encoding')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n```\n\n----------------------------------------\n\nTITLE: Predicting with cuML Model\nDESCRIPTION: Generates predictions using the trained cuML K-means model.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlabels_cuml = kmeans_cuml.predict(X_dca).compute()\n```\n\n----------------------------------------\n\nTITLE: Multi-column Joint Target Encoding in Python\nDESCRIPTION: This snippet showcases multi-column joint target encoding, where multiple categorical columns are encoded together into a single new feature. It demonstrates the process for various combinations of columns.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfor cols in [['cat_0', 'cat_1'],\n             ['cat_0', 'cat_2'],\n             ['cat_1', 'cat_2'],\n             ['cat_0', 'cat_1', 'cat_2']\n            ]:\n    out_col = '_'.join(cols)+'_TE'\n    encoder = TargetEncoder(n_folds=FOLDS,smooth=SMOOTH, split_method=SPLIT)\n    train[out_col] = encoder.fit_transform(train[cols], train['label'])\n    valid[out_col] = encoder.transform(valid[cols])\n    del encoder\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost Parameters for GPU Training in Python\nDESCRIPTION: This code sets up the parameters for XGBoost training, including GPU-specific settings for tree construction.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nxgb_parms = { \n    'max_depth':6, \n    'learning_rate':0.1, \n    'subsample':0.8,\n    'colsample_bytree':1.0, \n    'eval_metric':'auc',\n    'objective':'binary:logistic',\n    'tree_method':'gpu_hist',\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing UMAP Trustworthiness Scores in Python\nDESCRIPTION: This code snippet demonstrates how to compare the trustworthiness scores of UMAP embeddings generated by the reference implementation and the cuML accelerated version. It initializes both models, fits and transforms the data, then calculates and compares the trustworthiness scores.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change-limitations.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom umap import UMAP as refUMAP  #  with cuml.accel off\nfrom cuml.manifold import UMAP\nfrom cuml.metrics import trustworthiness\n\nn_neighbors = 15\n\nref_model = refUMAP(n_neighbors=n_neighbors)\nref_embeddings = ref_model.fit_transform(X)\n\nmodel = UMAP(n_neighbors=n_neighbors)\nembeddings = model.fit_transform(X)\n\nref_score = trustworthiness(X, ref_embeddings, n_neighbors=n_neighbors)\nscore = trustworthiness(X, embeddings, n_neighbors=n_neighbors)\n\ntol = 0.1\nassert score >= (ref_score - tol)\n```\n\n----------------------------------------\n\nTITLE: Testing Point-to-Point Communications in RAFT for cuML (Python)\nDESCRIPTION: This snippet tests point-to-point communications using RAFT in cuML. It performs send/receive operations across multiple workers for a specified number of trials. The function 'func_test_send_recv' is submitted to each worker, and the results are collected and verified.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nn_trials = 2\n\ndef func_test_send_recv(sessionId, n_trials, r):\n    handle = get_raft_comm_state(sessionId)[\"handle\"]\n    return perform_test_comms_send_recv(handle, n_trials)\n\np2p_dfs=[c.submit(func_test_send_recv, cb.sessionId, n_trials, random.random(), workers=[w]) for wid, w in zip(range(len(cb.worker_addresses)), cb.worker_addresses)]\nwait(p2p_dfs)\n\np2p_result = list(map(lambda x: x.result(), p2p_dfs))\nprint(str(p2p_result))\n\nassert all(p2p_result)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Lasso Regression\nDESCRIPTION: Sets up and executes a benchmark for the Lasso Regression algorithm. This compares performance on regression datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_REGRESSION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"Lasso\", runner)\n```\n\n----------------------------------------\n\nTITLE: Forecasting and Visualizing ARIMA Model Results in Python\nDESCRIPTION: This snippet uses the fitted ARIMA model to forecast future values using exogenous variables. It then visualizes the results starting from time step 100.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/arima_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# Forecast\nfc_guests_exog = model_guests_exog.forecast(40, exog=df_exog_fut)\n\n# Visualize after the time step 100\nvisualize(df_guests_exog[100:], fc_guests_exog)\n```\n\n----------------------------------------\n\nTITLE: Train-Test Split Implementation\nDESCRIPTION: Splits the dataset into training and testing sets using cuML's train_test_split function, with conversion to CPU format for comparison testing.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nX_train, X_test, y_train, y_test = train_test_split(data,\n                                                    labels,\n                                                    test_size=0.2)\n\nX_cpu = X_train.to_pandas()\ny_cpu = y_train.label.to_numpy()\n\nX_test_cpu = X_test.to_pandas()\ny_test_cpu = y_test.label.to_numpy()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking PCA (Principal Component Analysis)\nDESCRIPTION: Sets up and executes a benchmark for the Principal Component Analysis (PCA) algorithm. This compares performance on neighborhood datasets with small row sizes and wide feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=WIDE_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"PCA\", runner)\n```\n\n----------------------------------------\n\nTITLE: Defining XGBoost Model Prediction Function in Python\nDESCRIPTION: Creates a function to perform prediction using a trained XGBoost model and return class labels.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef predict_xgboost_model(X_validation, y_validation, xgb_model):\n\n    # predict using the xgboost model\n    dvalidation = xgb.DMatrix(X_validation, label=y_validation)\n    predictions = xgb_model.predict(dvalidation)\n\n    # convert the predicted values from xgboost into class labels\n    predictions = cupy.around(predictions)\n    \n    return predictions\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Higgs Dataset\nDESCRIPTION: Loads the Higgs dataset into cuDF DataFrame with appropriate column names and data types. Separates features and labels for model training.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncol_names = ['label'] + [\"col-{}\".format(i) for i in range(2, 30)]\ndtypes_ls = ['int32'] + ['float32' for _ in range(2, 30)]\ninput_data = cudf.read_csv(decompressed_filepath, names=col_names, dtype=dtypes_ls)\n```\n\n----------------------------------------\n\nTITLE: Using cuml.accel with cudf.pandas from CLI\nDESCRIPTION: Demonstrates how to use cuml.accel in combination with cudf.pandas acceleration from the command line interface.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change.rst#2025-04-19_snippet_3\n\nLANGUAGE: console\nCODE:\n```\npython -m cuml.accel --cudf-pandas\n```\n\n----------------------------------------\n\nTITLE: Applying Label Encoding to Categorical Columns using CUML in Python\nDESCRIPTION: This snippet applies label encoding to the categorical columns using CUML's LabelEncoder, transforming string columns to integer columns.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%time\nfor col in cat_cols[:3]:\n    train[col] = train[col].fillna('None')\n    valid[col] = valid[col].fillna('None')\n    lbl = LabelEncoder()\n    lbl.fit(gd.concat([train[col],valid[col]]))\n    train[col] = lbl.transform(train[col])\n    valid[col] = lbl.transform(valid[col])\n```\n\n----------------------------------------\n\nTITLE: Creating and Fitting scikit-learn Nearest Neighbors Model\nDESCRIPTION: Initializes a scikit-learn NearestNeighbors model with brute-force algorithm and parallel processing, then fits it to the host data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%time\nknn_sk = skNearestNeighbors(algorithm=\"brute\",\n                            n_jobs=-1)\nknn_sk.fit(host_data)\n```\n\n----------------------------------------\n\nTITLE: Fitting Distributed KMeans Model with cuML in Python\nDESCRIPTION: Fits the distributed KMeans model on the previously created dataset.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndist_model.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RAPIDS cuML Random Forest\nDESCRIPTION: Imports necessary Python libraries including RAPIDS cuML, Dask, scikit-learn, and other dependencies for distributed random forest implementation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_mnmg_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport sklearn\n\nimport pandas as pd\nimport cudf\nimport cuml\n\nfrom sklearn import model_selection\n\nfrom cuml import datasets\nfrom cuml.metrics import accuracy_score\nfrom cuml.dask.common import utils as dask_utils\nfrom dask.distributed import Client, wait\nfrom dask_cuda import LocalCUDACluster\nimport dask_cudf\n\nfrom cuml.dask.ensemble import RandomForestClassifier as cumlDaskRF\nfrom sklearn.ensemble import RandomForestClassifier as sklRF\n```\n\n----------------------------------------\n\nTITLE: Defining Random Forest Parameters\nDESCRIPTION: Sets key parameters for the random forest model including data size, feature count, and model hyperparameters like max depth and number of trees.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_mnmg_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_size = 100000\ntest_size = 1000\nn_samples = train_size + test_size\nn_features = 20\n\nmax_depth = 12\nn_bins = 16\nn_trees = 1000\n```\n\n----------------------------------------\n\nTITLE: Performing RandomizedSearchCV for XGBoost Hyperparameter Optimization on GPU in Python\nDESCRIPTION: Executes RandomizedSearchCV for XGBoost using GPU. It uses the previously defined do_HPO function with the 'gpu-random' mode and prints the number of parameters searched and the resulting accuracy.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmode = \"gpu-random\"\n\nwith timed(\"XGB-\"+mode):\n    res, results = do_HPO(model_gpu_xgb,\n                                   params_xgb,\n                                   cuml_accuracy_scorer,\n                                   X_train,\n                                   y_cpu,\n                                   mode=mode,\n                                   n_iter=N_ITER)\nprint(\"Searched over {} parameters\".format(len(results.cv_results_['mean_test_score'])))\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating Scikit-learn Random Forest Model\nDESCRIPTION: Makes predictions using the trained scikit-learn model on the test dataset and calculates the accuracy score for model evaluation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%time\nsk_predict = sk_model.predict(X_test)\nsk_acc = accuracy_score(y_test, sk_predict)\n```\n\n----------------------------------------\n\nTITLE: Comparing Pickle Preservation of cuML Model Accuracy\nDESCRIPTION: Prints and compares the accuracy of the cuML Random Forest model before and after pickling to demonstrate that serialization and deserialization preserve model performance.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(\"CUML accuracy of the RF model before pickling: %s\" % fil_acc_orig)\nprint(\"CUML accuracy of the RF model after pickling: %s\" % fil_acc_after_pickling)\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Accuracy Printing Function in Python\nDESCRIPTION: Defines a function 'print_acc' that trains a model on the provided train data and prints the accuracy of the trained model on the test data. It uses cuML's accuracy_score function for evaluation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef print_acc(model, X_train, y_train, X_test, y_test, mode_str=\"Default\"):\n    \"\"\"\n        Trains a model on the train data provided, and prints the accuracy of the trained model.\n        mode_str: User specifies what model it is to print the value\n    \"\"\"\n    y_pred = model.fit(X_train, y_train).predict(X_test)\n    score = accuracy_score(y_pred, y_test.astype('float32'))\n    \n    print(\"{} model accuracy: {}\".format(mode_str, score))\n                                         \n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Random Forest Classification with cuML\nDESCRIPTION: Imports necessary libraries for the notebook including cuDF for GPU DataFrames, numpy and pandas for data handling, pickle for model serialization, and both cuML and scikit-learn implementations of Random Forest Classifier.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cudf\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom cuml.ensemble import RandomForestClassifier as curfc\nfrom cuml.metrics import accuracy_score\n\nfrom sklearn.ensemble import RandomForestClassifier as skrfc\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n```\n\n----------------------------------------\n\nTITLE: Converting Data to GPU Format with cuDF\nDESCRIPTION: Transfers the training and testing data from host memory (pandas) to GPU memory (cuDF) for use with the cuML Random Forest implementation. This enables GPU-accelerated model training and inference.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%time\nX_cudf_train = cudf.DataFrame.from_pandas(X_train)\nX_cudf_test = cudf.DataFrame.from_pandas(X_test)\n\ny_cudf_train = cudf.Series(y_train.values)\n```\n\n----------------------------------------\n\nTITLE: Defining Data Parameters\nDESCRIPTION: Sets parameters for data generation including sample size, features, and calculates total partitions based on available workers.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nn_samples = 1000000\nn_features = 2\n\nn_total_partitions = len(list(client.has_what().keys()))\n```\n\n----------------------------------------\n\nTITLE: Evaluating cuML Linear Regression Model in Python\nDESCRIPTION: This snippet calculates the R-squared score for the cuML model predictions using cuML's r2_score function.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n%%time\nr2_score_cuml = r2_score(y_cudf_test, predict_cuml)\n```\n\n----------------------------------------\n\nTITLE: Running Forest Model Inference in C++\nDESCRIPTION: Demonstrates how to perform inference using a loaded forest model. The code shows memory allocation for output, setting up the handling parameters, and calling the predict method with appropriate device memory types and chunk size configuration.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/include/cuml/experimental/fil/README.md#2025-04-19_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nauto num_rows = std::size_t{1000};\nauto num_outputs = fil_model.num_outputs();  // Outputs per row\n\nauto output = static_cast<float*>(nullptr);  // Loaded as single\n                                             // precision, so use floats\n                                             // for I/O\n// Allocate enough space for num_outputs floats per row\ncudaMalloc((void**)&output, num_rows * num_outputs * sizeof(float));\n\n// Assuming that input is a float* pointing to data already located on-device\n\nauto handle = raft_proto::handle_t{};\n\nfil_model.predict(\n  handle,\n  output,\n  input,\n  num_rows,\n  raft_proto::device_type::gpu,  // out_mem_type\n  raft_proto::device_type::gpu,  // in_mem_type\n  4  // chunk_size\n);\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters for Classification Dataset\nDESCRIPTION: Sets up parameters for generating a synthetic classification dataset, including number of samples, features, and informative features. The data type is set to float32 for GPU compatibility.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The speedup obtained by using cuML'sRandom Forest implementation\n# becomes much higher when using larger datasets. Uncomment and use the n_samples\n# value provided below to see the difference in the time required to run\n# Scikit-learn's vs cuML's implementation with a large dataset.\n\n# n_samples = 2*17\nn_samples = 2**12\nn_features = 399\nn_info = 300\ndata_type = np.float32\n```\n\n----------------------------------------\n\nTITLE: Benchmarking KNeighborsRegressor\nDESCRIPTION: Sets up and executes a benchmark for the K-Nearest Neighbors Regressor algorithm. This compares performance using regression datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_REGRESSION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"KNeighborsRegressor\", runner)\n```\n\n----------------------------------------\n\nTITLE: Training Linear Regression and Evaluating with R Score\nDESCRIPTION: Creates a synthetic regression dataset, trains a Linear Regression model, and evaluates its performance using the R (coefficient of determination) metric from both cuML and scikit-learn.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/estimator_intro.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.datasets import make_regression\nfrom cuml.model_selection import train_test_split\nfrom cuml.linear_model import LinearRegression as cuLR\nfrom sklearn.metrics import r2_score\n\nn_samples = 2**10\nn_features = 100\nn_info = 70\n\nX_reg, y_reg = make_regression( n_samples = n_samples, \n                                n_features = n_features,\n                                n_informative = n_info, \n                                random_state = 123 )\n\nX_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split( X_reg,\n                                                                     y_reg, \n                                                                     train_size = 0.8,\n                                                                     random_state = 10 )\ncuml_reg_model = cuLR( fit_intercept = True,\n                       normalize = True,\n                       algorithm = 'eig' )\n\ntrained_LR = cuml_reg_model.fit( X_reg_train, y_reg_train )\ncu_preds = trained_LR.predict( X_reg_test )\n\ncu_r2 = cuml.metrics.r2_score( y_reg_test, cu_preds )\nsk_r2 = r2_score( asnumpy( y_reg_test ), asnumpy( cu_preds ) )\n\nprint(\"cuml's r2 score : \", cu_r2)\nprint(\"sklearn's r2 score : \", sk_r2)\n\n# save and reload \ndump( trained_LR, 'LR.model')         \n\n# to reload the model uncomment the line below \n# loaded_model = load('LR.model')\n```\n\n----------------------------------------\n\nTITLE: Training Scikit-learn Random Forest Classifier\nDESCRIPTION: Initializes and trains a scikit-learn Random Forest Classifier with 40 estimators, max depth of 16, and using all features. The model is fit on the CPU using the pandas DataFrame training data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%time\nsk_model = skrfc(n_estimators=40,\n                 max_depth=16,\n                 max_features=1.0,\n                 random_state=10)\n\nsk_model.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Configuring Cython Modules for Clustering Algorithms in RAPIDS cuML\nDESCRIPTION: This snippet sets up Cython modules for various clustering algorithms in RAPIDS cuML. It adds GPU-enabled modules for Agglomerative, DBSCAN, and KMeans clustering. For multi-GPU setups, it includes additional modules. The configuration uses CMake commands to create and link the modules.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/cluster/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"agglomerative.pyx\" ${agglomerative_algo} ${cluster_algo})\nadd_module_gpu_default(\"dbscan.pyx\" ${dbscan_algo} ${cluster_algo})\nadd_module_gpu_default(\"kmeans.pyx\" ${kmeans_algo} ${cluster_algo})\n\nif(NOT SINGLEGPU)\n  list(APPEND cython_sources\n       dbscan_mg.pyx\n       kmeans_mg.pyx\n  )\nendif()\n\nadd_subdirectory(hdbscan)\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_mg_libraries}\"\n  MODULE_PREFIX cluster_\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for HDBSCAN Benchmark\nDESCRIPTION: Imports necessary Python libraries for the benchmark, including data manipulation (numpy, pandas), time measurement, RAPIDS cuML, HDBSCAN, and sentence-transformers for text embedding.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\nimport json\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport cuml\nimport hdbscan\n\nfrom sentence_transformers import SentenceTransformer\n```\n\n----------------------------------------\n\nTITLE: Generating Classification Dataset\nDESCRIPTION: Creates a synthetic classification dataset using cuML's make_classification function and splits it into training and test sets.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_mnmg_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX, y = datasets.make_classification(n_samples=n_samples, n_features=n_features,\n                                 n_clusters_per_class=1, n_informative=int(n_features / 3),\n                                 random_state=123, n_classes=5)\nX = X.astype(np.float32)\ny = y.astype(np.int32)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for ARIMA Analysis\nDESCRIPTION: Sets up essential libraries including cuDF for GPU DataFrames, cuML for ARIMA implementation, and visualization tools.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/arima_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cudf\nfrom cuml.tsa.arima import ARIMA\n\nimport cupy as cp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n```\n\n----------------------------------------\n\nTITLE: Visualizing AUC Comparison for Different Encoding Methods in Python\nDESCRIPTION: This snippet creates a bar plot to compare the AUC scores of different encoding methods (label encoding, naive target encoding, k-fold target encoding) on both training and validation sets using matplotlib.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlabels = ['Label encoding', 'Target encoding naive', 'Target encoding kfold for loop']\ntrain_auc = [0.65, 0.84, 0.71]\nvalid_auc = [0.64, 0.63, 0.7]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nfig.set_figwidth(15)\nrects1 = ax.bar(x - width/2, train_auc, width, label='train auc', color='m')\nrects2 = ax.bar(x + width/2, valid_auc, width, label='valid auc', color='c')\n\nax.set_ylabel('Auc')\nax.set_title('The overfitting problem is fixed by kfold target encoding')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dask for Distributed FIL Inference in Python\nDESCRIPTION: Imports necessary Dask libraries and creates a LocalCUDACluster for distributed computing on multiple GPUs.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_cuda import LocalCUDACluster\nfrom distributed import Client, wait, get_worker\n\nimport dask.dataframe\nimport dask.array\nimport dask_cudf\n\nfrom cuml import ForestInference\nimport time\n```\n\nLANGUAGE: python\nCODE:\n```\ncluster = LocalCUDACluster()\nclient = Client(cluster)\n\nworkers = client.has_what().keys()\nn_workers = len(workers)\nn_partitions = n_workers\n```\n\n----------------------------------------\n\nTITLE: Defining K-Means Parameters\nDESCRIPTION: Sets up key parameters for the clustering analysis including sample size, number of features, clusters, and random state.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nn_samples = 100000\nn_features = 25\n\nn_clusters = 8\nrandom_state = 0\n```\n\n----------------------------------------\n\nTITLE: Initializing cuML Estimator with Keyword Arguments in Python\nDESCRIPTION: Shows the recommended way to initialize a cuML estimator with keyword arguments. This approach prevents breaking changes if arguments are added or removed in future versions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, *, eps=0.5, min_samples=5, max_mbytes_per_batch=None,\n             calc_core_sample_indices=True, handle=None, verbose=False, output_type=None):\n```\n\n----------------------------------------\n\nTITLE: Initializing Distributed KMeans Model with cuML in Python\nDESCRIPTION: Creates a distributed KMeans model using cuML's Dask-enabled implementation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.dask.cluster import KMeans\n\ndist_model = KMeans(n_clusters=5)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Logistic Regression\nDESCRIPTION: Sets up and executes a benchmark for the Logistic Regression algorithm. This compares performance on classification datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_CLASSIFICATION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"LogisticRegression\", runner)\n```\n\n----------------------------------------\n\nTITLE: Training KMeans Model on GPU with cuML in Python\nDESCRIPTION: Creates and fits a KMeans model using cuML's GPU-accelerated implementation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.cluster import KMeans\n\nmodel = KMeans(n_clusters=5)\n\nmodel.fit(X)\n```\n\n----------------------------------------\n\nTITLE: Comparing Clustering Results\nDESCRIPTION: Calculates and compares adjusted Rand scores between the two implementations to evaluate clustering similarity.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\ncuml_score = adjusted_rand_score(host_labels, kmeans_cuml.labels_.get())\nsk_score = adjusted_rand_score(host_labels, kmeans_sk.labels_)\n```\n\n----------------------------------------\n\nTITLE: Registering Metrics Module Cython Files in CMake for cuML\nDESCRIPTION: Registers various metrics algorithm Cython files as GPU modules and creates the necessary Cython modules with appropriate linkage. The file configures multiple clustering metric implementations including adjusted rand index, completeness score, entropy, and silhouette score.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/metrics/cluster/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"adjusted_rand_index.pyx\" ${adjusted_rand_index_algo} ${metrics_algo})\nadd_module_gpu_default(\"completeness_score.pyx\" ${completeness_score_algo} ${metrics_algo})\nadd_module_gpu_default(\"entropy.pyx\" ${entropy_algo} ${metrics_algo})\nadd_module_gpu_default(\"homogeneity_score.pyx\" ${homogeneity_score_algo} ${metrics_algo})\nadd_module_gpu_default(\"mutual_info_score.pyx\" ${mutual_info_score_algo} ${metrics_algo})\nadd_module_gpu_default(\"silhouette_score.pyx\" ${silhouette_score_algo} ${metrics_algo})\nadd_module_gpu_default(\"utils.pyx\" ${utils_algo} ${metrics_algo})\nadd_module_gpu_default(\"v_measure.pyx\" ${v_measure_algo} ${metrics_algo})\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Unsupervised UMAP\nDESCRIPTION: Sets up and executes a benchmark for the Unsupervised UMAP (Uniform Manifold Approximation and Projection) algorithm. Requires the UMAP-learn package for CPU comparison and uses wide feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=WIDE_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"UMAP-Unsupervised\", runner)\n```\n\n----------------------------------------\n\nTITLE: Generating Exogenous Variables and Preparing Data for ARIMA Model in Python\nDESCRIPTION: This snippet generates sinusoidal exogenous variables, creates coefficients, and prepares dataframes for past and future values. It then adds a linear combination of exogenous variables to the endogenous data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/arima_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Generate exogenous variables and coefficients\nget_sine = lambda n, period: \\\n    np.sin(np.r_[:n] * 2 * np.pi / period + np.random.uniform(0, period))\nnp_exog = np.column_stack([get_sine(319, T)\n                           for T in np.random.uniform(20, 100, 2 * nb)])\n\ncp_exog = cp.array(np_exog)\ncp_exog_coef = cp.random.uniform(20, 200, 2 * nb)\n\n# Create dataframes for the past and future values\ndf_exog = cudf.DataFrame(np_exog[:279])\ndf_exog_fut = cudf.DataFrame(np_exog[279:])\n\n# Add linear combination of the exogenous variables to the endogenous\ndf_guests_exog = df_guests.copy()\nfor ib in range(nb):\n    df_guests_exog[df_guests_exog.columns[ib]] += \\\n        cp.matmul(cp_exog[:279, ib*2:(ib+1)*2], cp_exog_coef[ib*2:(ib+1)*2])\n```\n\n----------------------------------------\n\nTITLE: Accessing CUDA Stream from raft::handle_t in cuML (C++)\nDESCRIPTION: Demonstrates how to access the CUDA stream from the raft::handle_t object. This is used when implementing ML algorithms that require only one CUDA stream.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\nvoid foo(const raft::handle_t& h, ...)\n{\n    cudaStream_t stream = h.get_stream();\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing Index Results Between cuML and scikit-learn\nDESCRIPTION: Compares the indices matrices from cuML and scikit-learn implementations by sorting each row and calculating the difference, with a tolerance for minor differences due to implementation variations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsk_sorted = np.sort(I_sk, axis=1)\ncuml_sorted = np.sort(I_cuml.to_numpy(), axis=1)\n\ndiff = sk_sorted - cuml_sorted\n\npassed = (len(diff[diff!=0]) / n_samples) < 1e-9\nprint('compare knn: cuml vs sklearn indexes %s'%('equal'if passed else 'NOT equal'))\n```\n\n----------------------------------------\n\nTITLE: Derived Datasets List in Markdown\nDESCRIPTION: A markdown list of derived datasets created from the original Statistics New Zealand data. These datasets include missing observations and/or added procedural exogenous variables for testing purposes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/tests/ts_datasets/README.md#2025-04-19_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n- `guest_nights_by_region_missing.csv`\n- `hourly_earnings_by_industry_missing.csv`\n- `population_estimate_missing.csv`\n- `endog_deaths_by_region_exog.csv`\n- `endog_guest_nights_by_region_missing_exog.csv`\n- `endog_hourly_earnings_by_industry_missing_exog.csv`\n```\n\n----------------------------------------\n\nTITLE: Fitting Scikit-learn K-Means Model\nDESCRIPTION: Initializes and fits K-Means model using Scikit-learn's CPU implementation with k-means++ initialization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nkmeans_sk = skKMeans(\n    init=\"k-means++\",\n    n_clusters=n_clusters,\n    random_state=random_state,\n    n_init='auto'\n)\n%timeit kmeans_sk.fit(host_data)\n```\n\n----------------------------------------\n\nTITLE: Listing Available C++ Tests in cuML\nDESCRIPTION: Commands to list all available C++ tests for different components of cuML, which helps in selecting specific tests to run.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ ./test/ml --gtest_list_tests # Single GPU algorithm tests\n$ ./test/ml_mg --gtest_list_tests # Multi GPU algorithm tests\n$ ./test/prims --gtest_list_tests # ML Primitive function tests\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Observations in Time Series\nDESCRIPTION: Demonstrates creating and handling a dataset with missing observations for ARIMA analysis.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/arima_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Cut dataset to 100 observations\ndf_guests_missing = df_guests[:100].copy()\n\nfor title in df_guests_missing.columns:\n    # Missing observations at the start to simulate varying lengths\n    n_leading = random.randint(5, 40)\n    df_guests_missing[title][:n_leading]=None\n    \n    # Random missing observations in the middle\n    missing_obs = random.choices(range(n_leading, 100), k=random.randint(5, 20))\n    df_guests_missing[title][missing_obs]=None\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SVR with RBF Kernel in Python using cuML\nDESCRIPTION: Configures and runs a benchmark for SVR with RBF Kernel using cuML's SpeedupComparisonRunner. It compares performance on small datasets with skinny features for regression tasks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_REGRESSION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"SVR-RBF\", runner)\n```\n\n----------------------------------------\n\nTITLE: Initialize LocalCUDACluster with UCX\nDESCRIPTION: Sets up a local CUDA cluster using Dask with UCX enabled for high-speed CUDA array transport. Configures TCP over UCX and NVLink while disabling InfiniBand.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize UCX for high-speed transport of CUDA arrays\nfrom dask_cuda import LocalCUDACluster\n\n# Create a Dask single-node CUDA cluster w/ one worker per device\ncluster = LocalCUDACluster(protocol=\"ucx\",\n                           enable_tcp_over_ucx=True,\n                           enable_nvlink=True,\n                           enable_infiniband=False)\n```\n\n----------------------------------------\n\nTITLE: Generating Blob Dataset on GPU\nDESCRIPTION: Creates a synthetic dataset using cuML's make_blobs function with 5 centers and converts it to a cuDF DataFrame for GPU-based processing.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%time\ndevice_data, _ = make_blobs(n_samples=n_samples,\n                            n_features=n_features,\n                            centers=5,\n                            random_state=random_state)\n\ndevice_data = cudf.DataFrame(device_data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Series Analysis Modules Build in CMake\nDESCRIPTION: Configures the build process for CUDA-accelerated time series analysis modules. Adds GPU-enabled Cython modules for ARIMA, Auto ARIMA, Holt-Winters, seasonality and stationarity analysis. Creates Cython modules with C++ linkage and connects to cuML shared libraries.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/tsa/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"arima.pyx\" ${arima_algo} ${tsa_algo})\nadd_module_gpu_default(\"auto_arima.pyx\" ${auto_arima_algo} ${tsa_algo})\nadd_module_gpu_default(\"holtwinters.pyx\" ${holtwinters_algo} ${tsa_algo})\nadd_module_gpu_default(\"seasonality.pyx\" ${seasonality_algo} ${tsa_algo})\nadd_module_gpu_default(\"stationarity.pyx\" ${stationarity_algo} ${tsa_algo})\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX tsa_\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Inferencing with Treelite Model on CPU in Python\nDESCRIPTION: Shows how to load a Treelite checkpoint and perform inference on a CPU-only machine using the Treelite library.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport treelite\n\n# The checkpoint file has been copied over\ncheckpoint_path = './checkpoint.tl'\ntl_model = treelite.Model.deserialize(checkpoint_path)\nout_prob = treelite.gtil.predict(tl_model, X, pred_margin=True)\nprint(out_prob)\n```\n\n----------------------------------------\n\nTITLE: Testing Collective Communications in RAFT for cuML (Python)\nDESCRIPTION: This snippet tests collective communications, specifically the allreduce operation, using RAFT in cuML. The function 'func_test_allreduce' is submitted to each worker, and the results are collected and verified. It demonstrates the use of RAFT's collective communication capabilities across multiple workers.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef func_test_allreduce(sessionId, r):\n    handle = get_raft_comm_state(sessionId)[\"handle\"]\n    return perform_test_comms_allreduce(handle)\n\ncoll_dfs = [c.submit(func_test_allreduce, cb.sessionId, random.random(), workers=[w]) for wid, w in zip(range(len(cb.worker_addresses)), cb.worker_addresses)]\nwait(coll_dfs)\n\ncoll_result = list(map(lambda x: x.result(), coll_dfs))\n\ncoll_result\n\nassert all(coll_result)\n```\n\n----------------------------------------\n\nTITLE: Creating Results DataFrame for KNN Hyperparameter Optimization in Python\nDESCRIPTION: Converts the grid search cross-validation results into a pandas DataFrame for easier analysis and visualization of how different parameter values affected model performance.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(results.cv_results_)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Default XGBoost Performance in Python\nDESCRIPTION: Creates an XGBoost classifier with default parameters using GPU tree method and evaluates its accuracy on the test set. This serves as a baseline for comparison with optimized models.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel_gpu_xgb_ = xgb.XGBClassifier(tree_method='gpu_hist')\n\nprint_acc(model_gpu_xgb_, X_train, y_cpu, X_test, y_test_cpu)\n```\n\n----------------------------------------\n\nTITLE: Enabling ccache for Faster cuML Builds\nDESCRIPTION: Command to enable ccache when configuring the CMake build, which can significantly reduce build times when switching between branches or build types.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DUSE_CCACHE=ON\n```\n\n----------------------------------------\n\nTITLE: Creating Timer Context Manager for Benchmarking\nDESCRIPTION: Implements a lightweight Python context manager class to measure elapsed time for different operations during the benchmark.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Timer:    \n    def __enter__(self):\n        self.tick = time.time()\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.tock = time.time()\n        self.elapsed = self.tock - self.tick\n```\n\n----------------------------------------\n\nTITLE: Visualizing K-means Clustering Results with PCA Dimensionality Reduction\nDESCRIPTION: Projects the 64-dimensional digits data into a 2D space using PCA, then applies K-means clustering and visualizes the results. The code creates a decision boundary plot showing cluster assignments and centroids, demonstrating how clustering performs in a reduced feature space.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero_code_change_examples/plot_kmeans_digits.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nreduced_data = PCA(n_components=2).fit_transform(data)\nkmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=4)\nkmeans.fit(reduced_data)\n\n# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1)\nplt.clf()\nplt.imshow(\n    Z,\n    interpolation=\"nearest\",\n    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n    cmap=plt.cm.Paired,\n    aspect=\"auto\",\n    origin=\"lower\",\n)\n\nplt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(\n    centroids[:, 0],\n    centroids[:, 1],\n    marker=\"x\",\n    s=169,\n    linewidths=3,\n    color=\"w\",\n    zorder=10,\n)\nplt.title(\n    \"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n    \"Centroids are marked with white cross\"\n)\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Converting Binary Classifier Output Format for Future Compatibility\nDESCRIPTION: Code snippet showing how to convert the new output format (coming in RAPIDS 25.06) back to the old format for binary classifiers. The future version will return only probabilities of the positive class.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/fil/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np  # Use cupy or numpy depending on which you use for input data\n\nout = fil_model.predict_proba(input_data)\n# Starting in RAPIDS 25.06, the following can be used to obtain the old output shape\nout = np.stack([1 - out, out], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear Model Cython Modules for RAPIDS cuML\nDESCRIPTION: Sets up Cython modules for various linear model algorithms in RAPIDS cuML. It includes configurations for both single-GPU and multi-GPU implementations, depending on the SINGLEGPU flag. The modules are created using the rapids_cython_create_modules function.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/linear_model/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"base.pyx\" ${linearregression_algo} ${elastic_net_algo} ${ridge_algo} ${linear_model_algo})\nadd_module_gpu_default(\"linear_regression.pyx\" ${linearregression_algo} ${linear_model_algo})\nadd_module_gpu_default(\"elastic_net.pyx\" ${elasticnet_algo} ${linear_model_algo})\nadd_module_gpu_default(\"logistic_regression.pyx\" ${logisticregression_algo} ${linear_model_algo})\nadd_module_gpu_default(\"mbsgd_classifier.pyx\" ${mbsgd_classifier_algo} ${linear_model_algo})\nadd_module_gpu_default(\"mbsgd_regressor.pyx\" ${mbsgd_regressor_algo} ${linear_model_algo})\nadd_module_gpu_default(\"ridge.pyx\" ${ridge_algo} ${linear_model_algo})\n\nif(NOT SINGLEGPU)\n  list(APPEND cython_sources\n       base_mg.pyx\n       linear_regression_mg.pyx\n       logistic_regression_mg.pyx\n       ridge_mg.pyx\n  )\n\nendif()\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_mg_libraries}\"\n  MODULE_PREFIX linear_model_\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Dask Client and cuML Communicator\nDESCRIPTION: Python code to create a Dask client connected to the UCX scheduler and initialize the cuML communicator for distributed operations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client, wait\nfrom cuml.raft.dask.common.comms import Comms\nfrom cuml.dask.common import get_raft_comm_state\nfrom cuml.dask.common import perform_test_comms_send_recv\nfrom cuml.dask.common import perform_test_comms_allreduce\n\nimport random\n\nc = Client(\"ucx://10.0.0.50:8786\")\ncb = Comms(comms_p2p=True)\ncb.init()\n```\n\n----------------------------------------\n\nTITLE: Defining Array-Like Attributes using CumlArrayDescriptor in Python\nDESCRIPTION: Example of using CumlArrayDescriptor to define array-like attributes in a cuML estimator class, specifying the memory layout order.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.common.array_descriptor import CumlArrayDescriptor\n\nclass TestEstimator(cuml.Base):\n\n   # Class variables outside of any function\n   my_cuml_array_ = CumlArrayDescriptor(order='C')\n\n   def __init__(self, ...):\n      ...\n```\n\n----------------------------------------\n\nTITLE: Evaluating Scikit-learn Linear Regression Model in Python\nDESCRIPTION: This snippet calculates the R-squared score for the Scikit-learn model predictions using cuML's r2_score function.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\nr2_score_sk = r2_score(y_cudf_test, predict_sk)\n```\n\n----------------------------------------\n\nTITLE: Accessing Cluster Centers of Loaded Combined KMeans Model in Python\nDESCRIPTION: Shows how to access the cluster centers of a loaded combined KMeans model.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsingle_gpu_model.cluster_centers_\n```\n\n----------------------------------------\n\nTITLE: Building DBSCAN Example with CMake\nDESCRIPTION: Commands for building the DBSCAN example as a standalone application using CMake and make.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/dbscan/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -Dcuml_ROOT=/path/to/cuml\n$ make\n```\n\n----------------------------------------\n\nTITLE: Benchmarking KNeighborsClassifier\nDESCRIPTION: Sets up and executes a benchmark for the K-Nearest Neighbors Classifier algorithm. This compares performance using classification datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_CLASSIFICATION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"KNeighborsClassifier\", runner)\n```\n\n----------------------------------------\n\nTITLE: Implementing predict() Method with Type Annotation in cuML\nDESCRIPTION: Example of a predict() method implementation using CumlArray return type annotation. This allows direct return of array-like objects which will be automatically converted to the appropriate output type.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef predict(self, X) -> CumlArray:\n   # Convert to CumlArray\n   X_m = input_to_cuml_array(X, order=\"K\").array\n\n   # Call a cuda function\n   X_m = cp.asarray(X_m) + cp.ones(X_m.shape)\n\n   # Directly return a cupy array\n   return X_m\n```\n\n----------------------------------------\n\nTITLE: Training Scikit-learn Random Forest Model\nDESCRIPTION: Implements and trains a single-node scikit-learn random forest model using all available CPU cores.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_mnmg_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nskl_model = sklRF(max_depth=max_depth, n_estimators=n_trees, n_jobs=-1)\nskl_model.fit(X_train.get(), y_train.get())\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for KMeans Example with cuML\nDESCRIPTION: Sets up a CMake project for building a KMeans clustering example application using NVIDIA's RAPIDS cuML library. Configures C++17 standard, finds the required cuML package, defines the executable target, and sets the linker language to CUDA to properly link with CUDA runtime libraries.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/kmeans/CMakeLists_standalone.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.26.4 FATAL_ERROR)\nproject(kmeans_example LANGUAGES CXX CUDA)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nfind_package(cuml REQUIRED)\n\nadd_executable(kmeans_example kmeans_example.cpp)\ntarget_link_libraries(kmeans_example PRIVATE cuml::cuml++)\n\n# Need to set linker language to CUDA to link the CUDA Runtime\nset_target_properties(kmeans_example PROPERTIES LINKER_LANGUAGE \"CUDA\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Metrics Module Build Configuration in CMake for RAPIDS cuML\nDESCRIPTION: This CMake script configures the build process for the metrics module in RAPIDS cuML. It initializes an empty list of Cython sources, adds multiple GPU-enabled modules for various metrics algorithms, and creates Cython modules with the metrics_ prefix linking against cuML shared libraries.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/metrics/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"hinge_loss.pyx\" ${hinge_loss_algo} ${metrics_algo})\nadd_module_gpu_default(\"kl_divergence.pyx\" ${kl_divergence_algo} ${metrics_algo})\nadd_module_gpu_default(\"pairwise_distances.pyx\" ${pairwise_distances_algo} ${metrics_algo})\nadd_module_gpu_default(\"trustworthiness.pyx\" ${trustworthiness_algo} ${metrics_algo})\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX metrics_\n)\n```\n\n----------------------------------------\n\nTITLE: Copying GPU Data to CPU Memory in Python\nDESCRIPTION: This snippet copies the dataset from GPU memory to host memory to enable comparison between CPU and GPU results later in the notebook.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Copy dataset from GPU memory to host memory.\n# This is done to later compare CPU and GPU results.\nX_train = X_cudf.to_pandas()\nX_test = X_cudf_test.to_pandas()\ny_train = y_cudf.to_pandas()\ny_test = y_cudf_test.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Score Method with Output Type Conversion in Python\nDESCRIPTION: Example of implementing a score method in a cuML estimator that converts the output type to numpy using the global_output_type context manager.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef score(self):\n\n   # Set the global output type to numpy\n   with cuml.using_output_type(\"numpy\"):\n      # Accessing my_cuml_array_ will return a numpy array and\n      # the result can be returned directly\n      return np.sum(self.my_cuml_array_, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Building and installing UCX from source\nDESCRIPTION: Commands to clone, configure, build and install UCX from source with CUDA support and multi-threading enabled.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/cjnolet/ucx-py.git\ncd ucx\ngit checkout fea-ext-expose_worker_and_ep\n./autogen.sh\nmkdir build && cd build\n../configure --prefix=$CONDA_PREFIX --with-cuda=/usr/local/cuda --enable-mt --disable-cma CPPFLAGS=\"-I//usr/local/cuda/include\"\nmake -j install\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Criteo Dataset using CUDF in Python\nDESCRIPTION: This snippet loads the Criteo dataset, selects specific columns, and performs initial preprocessing using CUDF for GPU-accelerated operations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\npath = '/datasets/criteo/raw_csvs/split_train_data'\ntrain_name = f'{path}/day_0_part_0000'\nvalid_name = f'{path}/day_0_part_0001'\nnum_cols = ['num_%d'%i for i in range(13)]\ncat_cols = ['cat_%d'%i for i in range(26)]\ncols = ['label']+num_cols+cat_cols\ndtypes = {i:'str' if i.startswith('cat_') else 'float32' for i in cols}\ntrain = gd.read_csv(train_name, sep = '\\t', header=None, names=cols, dtypes=dtypes)\nvalid = gd.read_csv(valid_name, sep = '\\t', header=None, names=cols, dtypes=dtypes)\n\nused_cols = ['label']+cat_cols[:3]\n\ntrain = train[used_cols]\nvalid = valid[used_cols]\ntrain.head()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Supervised UMAP\nDESCRIPTION: Sets up and executes a benchmark for the Supervised UMAP algorithm. Requires the UMAP-learn package for CPU comparison and uses wide feature dimensions with supervision for dimensionality reduction.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=WIDE_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"UMAP-Supervised\", runner)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Linear Regression\nDESCRIPTION: Sets up and executes a benchmark for the Linear Regression algorithm. This compares performance on regression datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_REGRESSION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"LinearRegression\", runner)\n```\n\n----------------------------------------\n\nTITLE: Using Thrust with Custom Allocator in cuML (C++)\nDESCRIPTION: Demonstrates how to use Thrust algorithms with a custom allocator in cuML. This ensures that Thrust allocates temporary memory via the provided device memory allocator.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\nvoid foo(const raft::handle_t& h, ..., cudaStream_t stream )\n{\n    ML::thrustAllocatorAdapter alloc( h.get_device_allocator(), stream );\n    auto execution_policy = thrust::cuda::par(alloc).on(stream);\n    thrust::for_each(execution_policy, ... );\n}\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Device for GPU Computation in Python\nDESCRIPTION: This snippet sets the CUDA visible devices and determines the number of GPUs to use for computation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nGPU_id = '0,1,2,3'\nos.environ['CUDA_VISIBLE_DEVICES'] = GPU_id\nnum_gpus = len(GPU_id.split(','))\n```\n\n----------------------------------------\n\nTITLE: Defining XGBoost Model Training Function in Python\nDESCRIPTION: Creates a function to train an XGBoost model with specified parameters, saves the model, and returns the trained model object.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_xgboost_model(\n    X_train, \n    y_train,\n    model_path='xgb.model',\n    num_rounds=100, \n    max_depth=20\n):\n    \n    # set the xgboost model parameters\n    params = {\n        'verbosity': 0, \n        'eval_metric':'error',\n        'objective':'binary:logistic',\n        'max_depth': max_depth,\n        'tree_method': 'gpu_hist'\n    }\n    \n    # convert training data into DMatrix\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    \n    # train the xgboost model\n    trained_model = xgb.train(params, dtrain, num_rounds)\n\n    # save the trained xgboost model\n    trained_model.save_model(model_path)\n\n    return trained_model\n```\n\n----------------------------------------\n\nTITLE: Simplified predict() Method Using @api_base_return_array Decorator\nDESCRIPTION: Improved implementation using the @api_base_return_array decorator which automatically handles input/output type conversion and dtype determination, eliminating the need for manual conversion and the @with_cupy_rmm decorator.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n@api_base_return_array(input_arg=\"X_in\", get_output_dtype=True)\ndef predict(self, X):\n\n\n\n\n   # Convert to CumlArray\n   X_m = input_to_cuml_array(X, order=\"K\").array\n\n   # Call a cuda function\n   X_m = cp.asarray(X_m) + cp.ones(X_m.shape)\n\n\n\n\n   # Return the cupy array directly\n   return X_m\n```\n\n----------------------------------------\n\nTITLE: Pickling cuML KMeans Model in Python\nDESCRIPTION: Demonstrates how to pickle a trained cuML KMeans model for persistence.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\n\npickle.dump(model, open(\"kmeans_model.pkl\", \"wb\"))\n```\n\n----------------------------------------\n\nTITLE: Creating Synthetic Dataset for KMeans Clustering in Python\nDESCRIPTION: Generates a synthetic dataset using cuML's make_blobs function for use in KMeans clustering.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.datasets import make_blobs\n\nX, y = make_blobs(n_samples=50,\n                  n_features=10,\n                  centers=5,\n                  cluster_std=0.4,\n                  random_state=0)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking t-SNE\nDESCRIPTION: Sets up and executes a benchmark for the t-SNE (t-Distributed Stochastic Neighbor Embedding) algorithm. Note that CPU benchmarking is configurable due to potentially very long runtime with CPU implementations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES, \n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\n# Due to extreme high runtime, the CPU benchmark \n# is disabled. Use run_cpu=True to re-enable. \n\nexecute_benchmark(\"TSNE\", runner, run_cpu=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Distributed Synthetic Data for FIL Inference in Python\nDESCRIPTION: Creates a large synthetic dataset using Dask, distributes it across workers, and persists it in GPU memory.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrows = 1_000_000\ncols = 100\n```\n\nLANGUAGE: python\nCODE:\n```\nx = dask.array.random.random(\n    size=(rows, cols), \n    chunks=(rows//n_partitions, cols)\n).astype('float32')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = dask.dataframe.from_array(x).to_backend(\"cudf\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = df.persist()\nwait(df)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Ridge Regression\nDESCRIPTION: Sets up and executes a benchmark for the Ridge Regression algorithm. This compares performance on regression datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_REGRESSION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"Ridge\", runner)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Mini-batch SGD Classifier\nDESCRIPTION: Sets up and executes a benchmark for the Mini-batch Stochastic Gradient Descent Classifier algorithm. This compares performance on classification datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_CLASSIFICATION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"MBSGDClassifier\", runner)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SVC with RBF Kernel in Python using cuML\nDESCRIPTION: Configures and runs a benchmark for SVC with RBF Kernel using cuML's SpeedupComparisonRunner. It compares performance on small datasets with skinny features for classification tasks. CPU benchmark is disabled by default due to high runtime.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_CLASSIFICATION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\n# Due to extreme high runtime, the CPU benchmark \n# is disabled. Use run_cpu=True to re-enable. \n\nexecute_benchmark(\"SVC-RBF\", runner, run_cpu=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Benchmark Helper Functions\nDESCRIPTION: Defines utility functions for enriching benchmark results with metadata and executing benchmarks for specific algorithms. These functions standardize the benchmarking process across different algorithms.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef enrich_result(algorithm, runner, result):\n    result[\"algo\"] = algorithm\n    result[\"dataset_name\"] = runner.dataset_name\n    result[\"input_type\"] = runner.input_type\n    return result\n\ndef execute_benchmark(algorithm, runner, verbose=VERBOSE, run_cpu=RUN_CPU, **kwargs):\n    results = runner.run(algorithm_by_name(algorithm), verbose=verbose, run_cpu=run_cpu, **kwargs)\n    results = [enrich_result(algorithm, runner, result) for result in results]\n    benchmark_results.extend(results)\n```\n\n----------------------------------------\n\nTITLE: Using Device Type Context Manager\nDESCRIPTION: Example demonstrating how to use the context manager to control execution device (CPU/GPU) for specific code blocks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/execution_device_interoperability.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.neighbors import NearestNeighbors\nfrom cuml.common.device_selection import using_device_type\n\nnn = NearestNeighbors()\nwith using_device_type('cpu'):\n    nn.fit(X_train_blobs)\n    nearest_neighbors = nn.kneighbors(X_test_blobs)\n```\n\n----------------------------------------\n\nTITLE: Training Scikit-learn K-means Model\nDESCRIPTION: Initializes and fits a Dask-ML K-means model as scikit-learn equivalent for comparison purposes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkmeans_sk = skKMeans(init=\"k-means||\",\n                     n_clusters=5,\n                     n_jobs=-1,\n                     random_state=100)\n\nkmeans_sk.fit(X_np)\n```\n\n----------------------------------------\n\nTITLE: Charting All Algorithm Speedups in Python\nDESCRIPTION: Defines a function to create a bar chart showing the average speedup for all algorithms across different sample sizes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef chart_all_algo_speedup(df):\n    df = df[[\"algo\", \"n_samples\", \"speedup\"]].groupby([\"algo\", \"n_samples\"]).mean()\n    df.plot.bar()\n```\n\n----------------------------------------\n\nTITLE: Accessing Cluster Centers of Loaded KMeans Model in Python\nDESCRIPTION: Demonstrates how to access the cluster centers of a loaded KMeans model.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel.cluster_centers_\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters for Nearest Neighbors Analysis\nDESCRIPTION: Sets up parameters for the nearest neighbors analysis including sample size, feature dimensionality, query size, number of neighbors to find, and random state for reproducibility.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nn_samples = 2**17\nn_features = 40\n\nn_query = 2**13\nn_neighbors = 4\nrandom_state = 0\n```\n\n----------------------------------------\n\nTITLE: Running ML-Prims C++ Benchmarks\nDESCRIPTION: Command to run the ML-Prims C++ benchmarks, which tests the performance of the primitive functions used by cuML algorithms.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ ./bench/prims_benchmark  # ml-prims benchmarks\n```\n\n----------------------------------------\n\nTITLE: Comparing TSNE Trustworthiness with cuML and scikit-learn\nDESCRIPTION: This snippet demonstrates how to compare the trustworthiness score of TSNE embeddings generated by cuML's accelerated version and scikit-learn's reference implementation. It also checks the KL divergence to ensure the quality of results.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change-limitations.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.manifold import TSNE as refTSNE  #  with cuml.accel off\nfrom cuml.manifold import TSNE\nfrom cuml.metrics import trustworthiness\n\nn_neighbors = 90\n\nref_model = refTSNE() #  with perplexity == 30.0\nref_embeddings = ref_model.fit_transform(X)\n\nmodel = TSNE(n_neighbors=n_neighbors)\nembeddings = model.fit_transform(X)\n\nref_score = trustworthiness(X, ref_embeddings, n_neighbors=n_neighbors)\nscore = trustworthiness(X, embeddings, n_neighbors=n_neighbors)\n\ntol = 0.1\nassert score >= (ref_score - tol)\nassert model.kl_divergence_ <= ref_model.kl_divergence_ + tol\n```\n\n----------------------------------------\n\nTITLE: Normalizing PCA Component Signs in Python\nDESCRIPTION: This function normalizes the signs of PCA components to make numeric comparisons easier. It identifies the first non-zero value in each component vector and ensures it is positive, adjusting the signs of all values in that vector accordingly.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change-limitations.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef normalize(components):\n    \"\"\"Normalize the sign of components for easier numeric comparison\"\"\"\n    nonzero = components != 0\n    inds = np.where(nonzero.any(axis=1), nonzero.argmax(axis=1), 0)[:, None]\n    first_nonzero = np.take_along_axis(components, inds, 1)\n    return np.sign(first_nonzero) * components\n```\n\n----------------------------------------\n\nTITLE: Using logging macros in C++\nDESCRIPTION: Examples of using various logging macros in C++ for the cuML project. These macros use spdlog underneath for logging at different levels.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuml/common/logger.hpp>\n\n// Inside your method or function, use any of these macros\nCUML_LOG_TRACE(\"Hello %s!\", \"world\");\nCUML_LOG_DEBUG(\"Hello %s!\", \"world\");\nCUML_LOG_INFO(\"Hello %s!\", \"world\");\nCUML_LOG_WARN(\"Hello %s!\", \"world\");\nCUML_LOG_ERROR(\"Hello %s!\", \"world\");\nCUML_LOG_CRITICAL(\"Hello %s!\", \"world\");\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with Naive Target Encoded Features in Python\nDESCRIPTION: This snippet trains an XGBoost model using the naive target encoded features, demonstrating potential overfitting issues with this approach.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nte_cols = [col for col in train.columns if col.endswith('TE')]\nprint(te_cols)\n\nstart = time.time(); print('Creating DMatrix...')\ndtrain = xgb.DMatrix(data=train[te_cols],label=train['label'])\ndvalid = xgb.DMatrix(data=valid[te_cols],label=valid['label'])\nprint('Took %.1f seconds'%(time.time()-start))\n\nstart = time.time(); print('Training...')\nmodel = xgb.train(xgb_parms, \n                       dtrain=dtrain,\n                       evals=[(dtrain,'train'),(dvalid,'valid')],\n                       num_boost_round=NROUND,\n                       early_stopping_rounds=ESR,\n                       verbose_eval=VERBOSE_EVAL) \n```\n\n----------------------------------------\n\nTITLE: Implementing a Sample Estimator with CumlArrayDescriptor in Python\nDESCRIPTION: Comprehensive example of implementing a sample estimator using CumlArrayDescriptor for array-like attributes, demonstrating initialization, fitting, and attribute access.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport cupy as cp\nimport cuml\nfrom cuml.common.array_descriptor import CumlArrayDescriptor\n\nclass SampleEstimator(cuml.Base):\n\n   # Class variables outside of any function\n   my_cuml_array_ = CumlArrayDescriptor()\n   my_cupy_array_ = CumlArrayDescriptor()\n   my_other_array_ = CumlArrayDescriptor()\n\n   def __init__(self, ...):\n\n      # Initialize to None (not mandatory)\n      self.my_cuml_array_ = None\n\n      # Init with a cupy array\n      self.my_cupy_array_ = cp.zeros((10, 10))\n\n   def fit(self, X):\n      # Stores the type of `X` and sets the output type if self.output_type == \"input\"\n      self._set_output_type(X)\n\n      # Set my_cuml_array_ with a CumlArray\n      self.my_cuml_array_, *_ = input_to_cuml_array(X, order=\"K\")\n\n      # Access `my_cupy_array_` normally and set to another attribute\n      # The internal type of my_other_array_ will be a CuPy array\n      self.my_other_array_ = cp.ones((10, 10)) + self.my_cupy_array_\n\n      return self\n```\n\n----------------------------------------\n\nTITLE: Defining K-means Benchmarking Function\nDESCRIPTION: Creates a benchmark function that evaluates different K-means initialization methods. It measures execution time, inertia, and various clustering metrics including homogeneity, completeness, V-measure, ARI, AMI, and silhouette score to compare clustering quality.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero_code_change_examples/plot_kmeans_digits.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom time import time\n\nfrom sklearn import metrics\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef bench_k_means(kmeans, name, data, labels):\n    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n\n    Parameters\n    ----------\n    kmeans : KMeans instance\n        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n        already set.\n    name : str\n        Name given to the strategy. It will be used to show the results in a\n        table.\n    data : ndarray of shape (n_samples, n_features)\n        The data to cluster.\n    labels : ndarray of shape (n_samples,)\n        The labels used to compute the clustering metrics which requires some\n        supervision.\n    \"\"\"\n    t0 = time()\n    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n    fit_time = time() - t0\n    results = [name, fit_time, estimator[-1].inertia_]\n\n    # Define the metrics which require only the true labels and estimator\n    # labels\n    clustering_metrics = [\n        metrics.homogeneity_score,\n        metrics.completeness_score,\n        metrics.v_measure_score,\n        metrics.adjusted_rand_score,\n        metrics.adjusted_mutual_info_score,\n    ]\n    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n\n    # The silhouette score requires the full dataset\n    results += [\n        metrics.silhouette_score(\n            data,\n            estimator[-1].labels_,\n            metric=\"euclidean\",\n            sample_size=300,\n        )\n    ]\n\n    # Show the results\n    formatter_result = (\n        \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n    )\n    print(formatter_result.format(*results))\n```\n\n----------------------------------------\n\nTITLE: Charting Single Algorithm Speedup in Python\nDESCRIPTION: Defines a function to create a bar chart showing the speedup for a single algorithm across different sample sizes and feature counts.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef chart_single_algo_speedup(df, algorithm):\n    df = df.loc[df.algo == algorithm]\n    df = df.pivot(index=\"n_samples\", columns=\"n_features\", values=\"speedup\")\n    axes = df.plot.bar(title=\"%s Speedup\" % algorithm)\n```\n\n----------------------------------------\n\nTITLE: Running KMeans Example with Default Input (Bash)\nDESCRIPTION: Command to run the KMeans example with default tiny test input. It demonstrates the basic usage of the compiled executable.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/kmeans/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./kmeans_example\n```\n\n----------------------------------------\n\nTITLE: Benchmarking cuML Operations with NVTX\nDESCRIPTION: Sample script demonstrating how to profile cuML operations, specifically creating a UMAP model for dimensionality reduction. This can be analyzed with the nvtx_benchmark.py helper script to produce timing summaries.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/DEVELOPER_GUIDE.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.datasets import make_blobs\nfrom cuml.manifold import UMAP\n\nX, y = make_blobs(n_samples=1000, n_features=30)\n\nmodel = UMAP()\nmodel.fit(X)\nembeddngs = model.transform(X)\n```\n\n----------------------------------------\n\nTITLE: Running clang-tidy with Python script\nDESCRIPTION: Command to run clang-tidy using a Python script with configuration from pyproject.toml. This performs static code analysis on C++ files in the repository.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython cpp/scripts/run-clang-tidy.py --config pyproject.toml\n```\n\n----------------------------------------\n\nTITLE: Time Series Visualization Function\nDESCRIPTION: Creates visualizations for time series data with optional prediction lines and confidence intervals.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/arima_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef visualize(y, pred=None, pred_start=None, lower=None, upper=None):\n    n_obs, batch_size = y.shape\n    col = [\"#1f77b4\", \"#ff7f0e\"]\n\n    # Create the subplots\n    c = min(batch_size, 2)\n    r = (batch_size + c - 1) // c\n    fig, ax = plt.subplots(r, c, squeeze=False)\n    ax = ax.flatten()\n    \n    # Range for the prediction\n    if pred is not None:\n        pred_start = n_obs if pred_start is None else pred_start\n        pred_end = pred_start + pred.shape[0]\n    else:\n        pred_end = n_obs\n    \n    # Plot the data\n    for i in range(batch_size):\n        title = y.columns[i]\n        if pred is not None:\n            ax[i].plot(np.r_[pred_start:pred_end],\n                       pred[pred.columns[i]].to_numpy(),\n                       linestyle=\"--\", color=col[1])\n        # Prediction intervals\n        if lower is not None and upper is not None:\n            ax[i].fill_between(np.r_[pred_start:pred_end],\n                               lower[lower.columns[i]].to_numpy(),\n                               upper[upper.columns[i]].to_numpy(),\n                               alpha=0.2, color=col[1])\n        ax[i].plot(np.r_[:n_obs], y[title].to_numpy(), color=col[0])\n        ax[i].title.set_text(title)\n        ax[i].set_xlim((0, pred_end))\n    for i in range(batch_size, r*c):\n        fig.delaxes(ax[i])\n    fig.tight_layout()\n    fig.patch.set_facecolor('white')\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Model with Treelite in C++\nDESCRIPTION: Demonstrates how to load an XGBoost JSON model using Treelite, which serves as a common translation layer for all input types supported by FIL. The code shows how to create a Treelite model handle from a JSON file.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/include/cuml/experimental/fil/README.md#2025-04-19_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto filename = \"xgboost.json\";\nauto tl_model = treelite::model_loader::LoadXGBoostModelJSON(filename, \"{}\");\n```\n\n----------------------------------------\n\nTITLE: Creating conda environment for clang-tidy\nDESCRIPTION: Commands to create and activate a conda environment specifically for running clang-tidy. This sets up all necessary dependencies for the static analyzer.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nconda env create --yes -n cuml-clang-tidy -f conda/environments/clang_tidy_cuda-118_arch-x86_64.yaml\nconda activate cuml-clang-tidy\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Handwritten Digits Dataset\nDESCRIPTION: Loads the digits dataset from Scikit-Learn which contains handwritten digits from 0-9. The code extracts the data and labels, then outputs information about the dataset dimensions and number of unique digits.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero_code_change_examples/plot_kmeans_digits.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\n\ndata, labels = load_digits(return_X_y=True)\n(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\n\nprint(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Estimator Initialization with Base Parameters\nDESCRIPTION: Demonstrates how to properly initialize a cuML estimator by passing required parameters to the Base class constructor.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyEstimator(Base):\n\n   def __init__(self, *, extra_arg=True, handle=None, verbose=logger.level_enum.info, output_type=None):\n      super().__init__(handle=handle, verbose=verbose, output_type=output_type)\n      ...\n```\n\n----------------------------------------\n\nTITLE: Performing Grid Search Hyperparameter Optimization for KNNClassifier in Python\nDESCRIPTION: Conducts a grid search for the optimal 'n_neighbors' parameter in KNeighborsClassifier. The search range is from 1 to 39 neighbors, and it uses GPU acceleration with the 'gpu-grid' mode for improved performance.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nmodel_knn = KNeighborsClassifier(n_neighbors=5)\n\nks = [i for i in range(1, 40)]\nparams_knn = {'n_neighbors': ks\n             }\n\nmode = \"gpu-grid\"\nwith timed(\"KNN-\"+mode):\n    res, results = do_HPO(model_knn,\n               params_knn,\n               cuml_accuracy_scorer,\n               X_train,\n               y_cpu.astype('int32'),\n                mode=mode)\n```\n\n----------------------------------------\n\nTITLE: Defining XGBoost Parameter Distributions for Hyperparameter Optimization in Python\nDESCRIPTION: Sets up the parameter grid for XGBoost hyperparameter optimization. It defines ranges for max_depth, alpha, learning_rate, min_child_weight, and n_estimators using numpy functions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# For xgb_model\nmodel_gpu_xgb = xgb.XGBClassifier(tree_method='gpu_hist')\n\n# More range \nparams_xgb = {\n    \"max_depth\": np.arange(start=3, stop = 15, step = 3), # Default = 6\n    \"alpha\" : np.logspace(-3, -1, 5), # default = 0\n    \"learning_rate\": [0.05, 0.1, 0.15], #default = 0.3\n    \"min_child_weight\" : np.arange(start=2, stop=10, step=3), # default = 1\n    \"n_estimators\": [100, 200, 1000]\n}\n```\n\n----------------------------------------\n\nTITLE: Running KMeans Example with Custom Input (Bash)\nDESCRIPTION: Command to run the KMeans example with custom input data. It demonstrates how to specify the number of rows, columns, and input file for the KMeans algorithm.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/kmeans/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ./kmeans_example -num_rows 260753 -num_cols 298 -input output.txt\n```\n\n----------------------------------------\n\nTITLE: Running scikit-learn Tests with GPU Acceleration in Bash\nDESCRIPTION: Demonstrates how to execute scikit-learn tests with GPU acceleration using the run-tests.sh script. The examples show running all tests, specific tests with verbosity, and debugging on first failure.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/ci/accel/scikit-learn-tests/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./run-tests.sh                     # Run all tests\n./run-tests.sh -v -k test_kmeans   # Run specific test with verbosity\n./run-tests.sh -x --pdb            # Stop on first failure and debug\n```\n\n----------------------------------------\n\nTITLE: CMake Base Configuration for cuML\nDESCRIPTION: Sets up basic CMake configuration including version requirements, project definition, and core RAPIDS components initialization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.30.4 FATAL_ERROR)\n\ninclude(../cmake/rapids_config.cmake)\n\ninclude(rapids-cmake)\ninclude(rapids-cpm)\ninclude(rapids-cuda)\ninclude(rapids-export)\ninclude(rapids-find)\n\nrapids_cuda_init_architectures(CUML)\n\nproject(CUML VERSION \"${RAPIDS_VERSION}\" LANGUAGES CXX CUDA)\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Dependencies and Warnings Configuration\nDESCRIPTION: Imports required libraries and suppresses warnings for cleaner notebook output. Includes RAPIDS ecosystem libraries (cuML, cuDF), machine learning libraries, and data processing utilities.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Implementing fit() Method with Type Annotation in cuML\nDESCRIPTION: Example of a fit() method implementation using type annotation to indicate it returns self. This approach allows the Base metaclass to automatically handle the return value appropriately without additional conversions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef fit(self, X) -> \"KMeans\":\n\n   # Convert the input to CumlArray\n   self.coef_ = input_to_cuml_array(X, order=\"K\").array\n\n   return self\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-GPU Primitives\nDESCRIPTION: Conditionally includes cumlprims_mg when multi-GPU support is enabled, providing the necessary primitives for distributed ML algorithms.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_CUMLPRIMS_MG)\n  include(cmake/thirdparty/get_cumlprims_mg.cmake)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Higgs Dataset Download Function\nDESCRIPTION: Utility function to download and decompress the Higgs dataset from UCI repository. Handles both downloading compressed file and decompression to CSV format.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef download_higgs(compressed_filepath, decompressed_filepath):\n    higgs_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz'\n    if not os.path.isfile(compressed_filepath):\n        urlretrieve(higgs_url, compressed_filepath)\n    if not os.path.isfile(decompressed_filepath):\n        cf = gzip.GzipFile(compressed_filepath)\n        with open(decompressed_filepath, 'wb') as df:\n            df.write(cf.read())\n```\n\n----------------------------------------\n\nTITLE: Accessing CumlArrayDescriptor Internals in Python\nDESCRIPTION: Demonstrates how to access and inspect the internal representation of CumlArrayDescriptor using direct dictionary access to avoid array conversion.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nmy_est = TestEstimator()\nmy_est.fit(cp.ones((10,)))\n\n# Access the CumlArrayDescriptorMeta value directly. No array conversion will occur\nprint(my_est.__dict__[\"my_cuml_array_\"])\n# Output: CumlArrayDescriptorMeta(input_type='cupy', values={'cuml': <cuml.internals.array.CumlArray object at 0x7fd39174ae20>, 'numpy': array([ 0,  1,  1,  2,  2, -1, -1, ...\n\n# Values from CumlArrayDescriptorMeta can be specifically read\nprint(my_est.__dict__[\"my_cuml_array_\"].input_type)\n# Output: \"cupy\"\n\n# The input value can be accessed\nprint(my_est.__dict__[\"my_cuml_array_\"].get_input_value())\n# Output: CumlArray ...\n```\n\n----------------------------------------\n\nTITLE: Enabling cuml.accel and cudf.pandas in Jupyter Notebook\nDESCRIPTION: Shows how to enable both cuml.accel and cudf.pandas accelerations in a Jupyter notebook environment.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%load_ext cudf.pandas\nfrom cuml.experimental.accel import install\ninstall()\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost CPU Version for Hyperparameter Optimization in Python\nDESCRIPTION: Executes XGBoost's hyperparameter optimization on CPU for comparison with GPU performance. The code uses RandomizedSearch with the 'hist' tree method instead of 'gpu_hist' and only runs if the data fraction is small enough (<=0.1) to prevent excessive runtime.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nif data_fraction <= 0.1:\n    model_cpu_xgb = xgb.XGBClassifier(tree_method='hist')\n\n    mode = \"cpu-random\"\n    with timed(\"XGB-\" + mode):\n        res, results = do_HPO(model_cpu_xgb,\n                   params_xgb,\n                   'accuracy',\n                   X_cpu,\n                   y_cpu,\n                   mode=mode,\n                   n_iter=N_ITER)\n    \n    print(\"Searched over {} parameters\".format(len(results.cv_results_['mean_test_score'])))\n    \n    print_acc(res , X_cpu, y_cpu, X_test_cpu, y_test_cpu,\n              mode_str=mode)\n```\n\n----------------------------------------\n\nTITLE: Preparing Input Data for KMeans Example (Python and Bash)\nDESCRIPTION: Commands to prepare input data for the KMeans example using a Python script. It shows how to unzip data files and run a Python script to process the data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/kmeans/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ unzip all.zip\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ ./prepare_input.py [train_file=train.csv] [test_file=test.csv] [output=output.txt]\n```\n\n----------------------------------------\n\nTITLE: Compiling External C++ Application with cuML Dependencies\nDESCRIPTION: Command-line instructions for compiling a C++ application that uses cuML libraries. Sets the library path, compiles with nvcc, and links against the cuML library with the proper include paths.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ export LD_LIBRARY_PATH=\"${CONDA_PREFIX}/lib\"\n$ nvcc \\\n       main.cpp \\\n       -o cuml_logger_example \\\n       \"-L${CONDA_PREFIX}/lib\" \\\n       \"-I${CONDA_PREFIX}/include\" \\\n       \"-I${CONDA_PREFIX}/include/cuml/raft\" \\\n       -lcuml++\n$ ./cuml_logger_example\n[W] [13:26:43.503068] This is a warning from the cuML logger!\n```\n\n----------------------------------------\n\nTITLE: Configuring DBSCAN Example Build with CMake\nDESCRIPTION: Creates an executable target for the DBSCAN example and links it with the required cuML++ library. The example is built from dbscan_example.cpp source file.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/dbscan/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(dbscan_example dbscan_example.cpp)\ntarget_link_libraries(dbscan_example cuml++)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for K-Means MNMG\nDESCRIPTION: Imports necessary packages including cuML's distributed K-means, Dask utilities, and supporting libraries for GPU operations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.dask.cluster.kmeans import KMeans as cuKMeans\nfrom cuml.dask.common import to_dask_df\nfrom cuml.dask.datasets import make_blobs\nfrom cuml.metrics import adjusted_rand_score\nfrom dask.distributed import Client, wait\nfrom dask_cuda import LocalCUDACluster\nfrom dask_ml.cluster import KMeans as skKMeans\nimport cupy as cp\n```\n\n----------------------------------------\n\nTITLE: Using ML::thrust_exec_policy Helper in cuML (C++)\nDESCRIPTION: Shows the use of the ML::thrust_exec_policy helper function to create a Thrust execution policy. This simplifies the creation of a custom execution policy for Thrust algorithms.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nvoid foo(const raft::handle_t& h, ... , cudaStream_t stream )\n{\n    auto execution_policy = ML::thrust_exec_policy(h.get_device_allocator(),stream);\n    thrust::for_each(execution_policy->on(stream), ... );\n}\n```\n\n----------------------------------------\n\nTITLE: Running DBSCAN Example with Default Dataset\nDESCRIPTION: Example output when running the DBSCAN algorithm on the default trivial dataset, showing cluster distribution.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/dbscan/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nSamples file not specified. (-input option)\nRunning with default dataset:\nRunning DBSCAN with following parameters:\nNumber of samples - 25\nNumber of features - 3\nmin_pts - 2\neps - 1\nHistogram of samples\nCluster id, Number samples\n         0, 13\n         1, 12\nTotal number of clusters: 2\nNoise samples: 0\n```\n\n----------------------------------------\n\nTITLE: Adding Type Annotations to Estimator Methods\nDESCRIPTION: Demonstrates how to add return type annotations to public API functions to enable automatic type conversion.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyEstimator(Base):\n\n   def fit(self, X) -> \"MyEstimator\":\n      ...\n\n   def predict(self, X) -> CumlArray:\n      ...\n```\n\n----------------------------------------\n\nTITLE: Importing Experimental ForestInference for Early Access to New Features\nDESCRIPTION: Example showing how to import the experimental version of the ForestInference estimator to access the upcoming API changes before they become standard in release 25.06.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/fil/README.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.experimental.fil import ForestInference\n```\n\n----------------------------------------\n\nTITLE: Defining Benchmark Configuration Parameters\nDESCRIPTION: Sets up benchmark configuration including whether to benchmark soft clustering, HDBSCAN parameters (min_samples and min_cluster_size), the backends to compare (cuML and hdbscan), and dataset sizes for testing.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbenchmark_soft_cluster = True\n\nMIN_SAMPLES = 50\nMIN_CLUSTER_SIZE = 5\n\nBACKENDS = {\n    \"cuml\": cuml.cluster.hdbscan,\n    \"hdbscan\": hdbscan\n}\n\nSIZES = [\n    25000,\n    50000,\n    100000,\n    200000,\n    400000,\n    800000,\n    1600000\n]\n```\n\n----------------------------------------\n\nTITLE: Building cuML Components with CMake\nDESCRIPTION: Commands for building different cuML components after generating the build files with CMake. Targets include the main library, benchmark tools, and various test executables.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake --build . -j                        # Build libcuml++ and all tests\n$ cmake --build . -j --target  sg_benchmark # Build c++ cuml single gpu benchmark\n$ cmake --build . -j --target  cuml++       # Build libcuml++\n$ cmake --build . -j --target  ml           # Build ml_test algorithm tests binary\n$ cmake --build . -j --target  ml_mg        # Build ml_mg_test multi GPU algorithms tests binary\n$ cmake --build . -j --target  prims        # Build prims_test ML primitive unit tests binary\n```\n\n----------------------------------------\n\nTITLE: Printing Cardinality of Categorical Columns in Python\nDESCRIPTION: This code prints the cardinality (number of unique values) for the first three categorical columns in both the training and validation datasets.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor col in cat_cols[:3]:\n    print(col,'cardinality',len(train[col].unique()), len(valid[col].unique()))\n```\n\n----------------------------------------\n\nTITLE: Configuring Cython Sources for cuML Explainer Module in CMake\nDESCRIPTION: Sets up Cython source files for various SHAP (SHapley Additive exPlanations) algorithms in the cuML explainer module. It uses custom CMake functions to add GPU-enabled modules for base, kernel SHAP, permutation SHAP, and tree SHAP implementations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/explainer/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"base.pyx\" ${kernel_shap_algo} ${permutation_shap_algo} ${explainer_algo})\nadd_module_gpu_default(\"kernel_shap.pyx\" ${kernel_shap_algo} ${explainer_algo})\nadd_module_gpu_default(\"permutation_shap.pyx\" ${permutation_shap_algo} ${explainer_algo})\nadd_module_gpu_default(\"tree_shap.pyx\" ${tree_shap_algo} ${explainer_algo})\n```\n\n----------------------------------------\n\nTITLE: Distributing Data Across GPUs\nDESCRIPTION: Converts data to cuDF format and distributes it across worker GPUs using Dask, ensuring data persistence in active memory.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_mnmg_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nn_partitions = n_workers\n\ndef distribute(X, y):\n    X_cudf = cudf.DataFrame(X)\n    y_cudf = cudf.Series(y)\n\n    X_dask = dask_cudf.from_cudf(X_cudf, npartitions=n_partitions)\n    y_dask = dask_cudf.from_cudf(y_cudf, npartitions=n_partitions)\n\n    X_dask, y_dask = \\\n      dask_utils.persist_across_workers(c, [X_dask, y_dask], workers=workers)\n    \n    return X_dask, y_dask\n\nX_train_dask, y_train_dask = distribute(X_train, y_train)\nX_test_dask, y_test_dask = distribute(X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Configuring Linked Libraries for cuML Explainer Module in CMake\nDESCRIPTION: Defines the libraries to be linked with the cuML explainer module. It includes cuML SG libraries and a Python Treelite target.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/explainer/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(linked_libraries\n  \"${cuml_sg_libraries}\"\n  \"${CUML_PYTHON_TREELITE_TARGET}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring HDBSCAN Algorithm with GCC Warning Suppression in CMake\nDESCRIPTION: Includes HDBSCAN algorithm source files and applies compiler flag modifications to suppress maybe-uninitialized warnings from GCC 13. These warnings arise from CCCL and would otherwise cause build errors.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\n  if(all_algo OR hdbscan_algo)\n    target_sources(${CUML_CPP_TARGET}\n      PRIVATE\n        src/genetic/program.cu\n        src/genetic/node.cu\n        src/hdbscan/hdbscan.cu\n        src/hdbscan/condensed_hierarchy.cu\n        src/hdbscan/prediction_data.cu)\n\n    # When using GCC 13, some maybe-uninitialized warnings appear from CCCL and are treated as errors.\n    # See this issue: https://github.com/rapidsai/cuml/issues/6225\n    set_property(\n      SOURCE src/hdbscan/condensed_hierarchy.cu\n      APPEND_STRING\n      PROPERTY COMPILE_FLAGS\n      \" -Xcompiler=-Wno-maybe-uninitialized\"\n    )\n    set_property(\n      SOURCE src/hdbscan/hdbscan.cu\n      APPEND_STRING\n      PROPERTY COMPILE_FLAGS\n      \" -Xcompiler=-Wno-maybe-uninitialized\"\n    )\n    set_property(\n      SOURCE src/hdbscan/prediction_data.cu\n      APPEND_STRING\n      PROPERTY COMPILE_FLAGS\n      \" -Xcompiler=-Wno-maybe-uninitialized\"\n    )\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Implementing _get_param_names Method\nDESCRIPTION: Shows how to properly implement the _get_param_names method to include all estimator parameters, including those from the parent class.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n  @classmethod\n  def _get_param_names(cls):\n     return super()._get_param_names() + [\n        \"eps\",\n        \"min_samples\",\n     ]\n```\n\n----------------------------------------\n\nTITLE: Loading Pickled Combined KMeans Model in Python\nDESCRIPTION: Demonstrates how to load a previously pickled combined KMeans model from disk.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsingle_gpu_model = pickle.load(open(\"kmeans_model.pkl\", \"rb\"))\n```\n\n----------------------------------------\n\nTITLE: Correct Stateless C++ API Example for Decision Tree Classifier\nDESCRIPTION: Demonstrates the correct way to expose the Decision Tree Classifier API following the stateless guideline, using POD types and pointers for model representation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename T> struct TreeNode { /* nested tree-like data structure, but written as a POD! */ };\nstruct DTParams { /* hyper-params for building DT */ };\ntypedef TreeNode<float> TreeNodeF;\ntypedef TreeNode<double> TreeNodeD;\n\nvoid decisionTreeClassifierFit(const raft::handle_t &handle, const float *input, int n_rows, int n_cols,\n                               const int *labels, TreeNodeF *&root, DTParams params,\n                               bool verbose=false);\nvoid decisionTreeClassifierPredict(const raft::handle_t &handle, const double* input, int n_rows,\n                                   int n_cols, const TreeNodeD *root, int* predictions,\n                                   bool verbose=false);\n```\n\n----------------------------------------\n\nTITLE: Using raft::stream_syncer for Stream Ordering in cuML (C++)\nDESCRIPTION: Shows how to use raft::stream_syncer to ensure proper stream ordering in cuML algorithms. This class manages interstream dependencies to maintain correct execution order.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\nvoid cumlAlgo(const raft::handle_t& handle, ...)\n{\n    raft::streamSyncer _(handle);\n}\n```\n\n----------------------------------------\n\nTITLE: Building the cuML Python Package In-Place\nDESCRIPTION: Commands to build the cuML Python package in-place, which is useful for development and testing without installation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ cd ../../python\n$ python setup.py build_ext --inplace\n```\n\n----------------------------------------\n\nTITLE: Setting Up the Build Directory for libcuml++\nDESCRIPTION: Commands to create and configure the build directory for the C++/CUDA library (libcuml++) using CMake, with optional CUDA path setting.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ cd cpp\n$ mkdir build && cd build\n$ export CUDA_BIN_PATH=$CUDA_HOME # (optional env variable if cuda binary is not in the PATH. Default CUDA_HOME=/path/to/cuda/)\n$ cmake ..\n```\n\n----------------------------------------\n\nTITLE: Configuring Random Projection Cython Module in CMake\nDESCRIPTION: This snippet configures the Cython module for the Random Projection algorithm. It sets up the source files, adds the GPU default module, and creates the Cython modules with specified linked libraries and module prefix.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/random_projection/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"random_projection.pyx\" ${random_projection_algo})\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX random_projection_\n)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Nearest Neighbors (Brute Force)\nDESCRIPTION: Sets up and executes a benchmark for the Nearest Neighbors algorithm using brute force approach. This compares the performance of cuML's GPU implementation against scikit-learn's CPU implementation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS,\n)\n\nexecute_benchmark(\"NearestNeighbors\", runner)\n```\n\n----------------------------------------\n\nTITLE: Running Symbolic Regression with Parameters\nDESCRIPTION: Command to execute the symbolic regression example with various parameters including population size, generations, and mutation probabilities.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/symreg/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ ./symreg_example -n_cols 2                   \\\n                   -n_train_rows 250           \\\n                   -n_test_rows 50             \\\n                   -random_state 21            \\\n                   -population_size 4000       \\\n                   -generations 20             \\\n                   -stopping_criteria 0.01     \\\n                   -p_crossover 0.7            \\\n                   -p_subtree 0.1              \\\n                   -p_hoist 0.05               \\\n                   -p_point 0.1                \\\n                   -parsimony_coefficient 0.01\n```\n\nLANGUAGE: bash\nCODE:\n```\nReading input with 250 rows and 2 columns from train_data.txt.\nReading input with 250 rows from train_labels.txt.\nReading input with 50 rows and 2 columns from test_data.txt.\nReading input with 50 rows from test_labels.txt.\n***************************************\nAllocating device memory...\nAllocation time =   0.259072ms\n***************************************\nBeginning training on given dataset...\nFinished training for 4 generations.\n              Best AST index :      1855\n              Best AST depth :         3\n             Best AST length :        13\n           Best AST equation :( add( sub( mult( X0, X0) , div( X1, X1) ) , sub( X1, mult( X1, X1) ) ) )\nTraining time =    626.658ms\n***************************************\nBeginning Inference on Test dataset...\nInference score on test set = 5.29271e-08\nInference time =    0.35248ms\nSome Predicted test values:\n-1.65061;-1.64081;-0.91711;-2.28976;-0.280688;\nCorresponding Actual test values:\n-1.65061;-1.64081;-0.91711;-2.28976;-0.280688;\n```\n\n----------------------------------------\n\nTITLE: Configuring CUML Primitives Multi-GPU Dependencies\nDESCRIPTION: Sets up CUML primitives multi-GPU dependencies for the CUML C++ target when static linking is enabled.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\nif(CUML_USE_CUMLPRIMS_MG_STATIC AND (TARGET cumlprims_mg::cumlprims_mg))\n  copy_interface_excludes(INCLUDED_TARGET cumlprims_mg::cumlprims_mg TARGET ${CUML_CPP_TARGET})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Tags for Estimators\nDESCRIPTION: Shows how to implement the _more_tags method for tags that depend on runtime or instantiation attributes of the estimator.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n  def _more_tags(self):\n       return {\n           \"allow_nan\": is_scalar_nan(self.missing_values)\n        }\n```\n\n----------------------------------------\n\nTITLE: Timer Context Manager Implementation\nDESCRIPTION: Custom context manager for measuring execution time of code blocks. Useful for performance comparison between CPU and GPU implementations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom contextlib import contextmanager\n@contextmanager\ndef timed(txt):\n    t0 = time.time()\n    yield\n    t1 = time.time()\n    print(\"%32s time:  %8.5f\" % (txt, t1 - t0))\n```\n\n----------------------------------------\n\nTITLE: Allocating Device Memory in cuML (C++)\nDESCRIPTION: Demonstrates how to allocate device memory using the allocator provided by the raft::handle_t object. This ensures proper memory management and allows users to control temporary data allocation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<typename T>\nvoid foo(const raft::handle_t& h, cudaStream_t stream, ... )\n{\n    T* temp_h = h.get_device_allocator()->allocate(n*sizeof(T), stream);\n    ...\n    h.get_device_allocator()->deallocate(temp_h, n*sizeof(T), stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Gaussian Random Projection in Python using cuML\nDESCRIPTION: Sets up and executes a benchmark for the Gaussian Random Projection algorithm using cuML's SpeedupComparisonRunner. It compares performance on small datasets with wide features for neighborhood data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES,\n    bench_dims=WIDE_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"GaussianRandomProjection\", runner)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for cuML Symbolic Regression Example\nDESCRIPTION: Sets up a CMake project for a symbolic regression example using cuML. The configuration specifies C++17 as the standard, finds required CUDA and cuML dependencies, and creates an executable target linked to the cuML library.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/symreg/CMakeLists_standalone.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.8 FATAL_ERROR)\ninclude(ExternalProject)\n\nproject(symreg_example VERSION 0.1.0 LANGUAGES CXX CUDA )\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nfind_package(CUDAToolkit)\nfind_package(cuml)\n\nadd_executable(symreg_example symreg_example.cpp)\n\n# Need to set linker language to CUDA to link the CUDA Runtime\nset_target_properties(symreg_example PROPERTIES LINKER_LANGUAGE \"CUDA\")\n\n# Link cuml and cudart\ntarget_link_libraries(symreg_example cuml::cuml++)\n```\n\n----------------------------------------\n\nTITLE: Running All Python Tests for cuML\nDESCRIPTION: Commands to run all Python tests for cuML, including multi-GPU algorithms. This validates the Python package functionality after building.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ cd python\n$ pytest -v\n```\n\n----------------------------------------\n\nTITLE: FIL Implementation Directory Structure\nDESCRIPTION: Shows the file organization strategy used to maintain CPU/GPU interoperability in the FIL implementation. This structure separates 'consumable' headers from 'implementation' headers to ensure CPU-only builds can compile without CUDA dependencies.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/include/cuml/experimental/fil/Implementation.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndetail/\n infer.hpp  # \"Consumable\" header\n infer/     # \"Implementation\" directory\n   cpu.hpp\n   gpu.cuh\n   gpu.hpp\n```\n\n----------------------------------------\n\nTITLE: Initializing Local CUDA Cluster\nDESCRIPTION: Sets up a local CUDA cluster using Dask with one worker per GPU using the one-process-per-GPU (OPG) model.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncluster = LocalCUDACluster(threads_per_worker=1)\nclient = Client(cluster)\n```\n\n----------------------------------------\n\nTITLE: Adding CUML Algorithm Subdirectories in CMake\nDESCRIPTION: Adds subdirectories for KMeans clustering, DBSCAN clustering, and symbolic regression algorithms to the CMake build system. This enables these components to be built as part of the CUML library.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(kmeans)\nadd_subdirectory(dbscan)\nadd_subdirectory(symreg)\n```\n\n----------------------------------------\n\nTITLE: Setting Link Options for CUML C++ Target\nDESCRIPTION: Configures link options to exclude symbols from libdmlc.a and libprotobuf.a to prevent conflicts, and ensures CUDA symbols aren't relocated in debug builds.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_27\n\nLANGUAGE: CMake\nCODE:\n```\n# If we export the libdmlc symbols, they can lead to weird crashes with other\n# libraries that use libdmlc. This just hides the symbols internally.\ntarget_link_options(${CUML_CPP_TARGET} PRIVATE \"-Wl,--exclude-libs,libdmlc.a\")\n# same as above, but for protobuf library\ntarget_link_options(${CUML_CPP_TARGET} PRIVATE \"-Wl,--exclude-libs,libprotobuf.a\")\n# ensure CUDA symbols aren't relocated to the middle of the debug build binaries\ntarget_link_options(${CUML_CPP_TARGET} PRIVATE \"${CMAKE_CURRENT_BINARY_DIR}/fatbin.ld\")\n```\n\n----------------------------------------\n\nTITLE: Starting Dask CUDA workers with UCX\nDESCRIPTION: Command to start Dask CUDA workers that connect to the scheduler using the UCX protocol over Infiniband.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndask-cuda-worker ucx://10.0.0.50:8786\n```\n\n----------------------------------------\n\nTITLE: Dataset Loading Utility Function\nDESCRIPTION: Helper function to load time series data from CSV files into GPU DataFrames, with options to limit batch size.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/arima_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef load_dataset(name, max_batch=4):\n    import os\n    pdf = pd.read_csv(os.path.join(\"data\", \"time_series\", \"%s.csv\" % name))\n    return cudf.from_pandas(pdf[pdf.columns[1:max_batch+1]].astype(np.float64))\n```\n\n----------------------------------------\n\nTITLE: Using MLCommon::device_buffer for RAII in cuML (C++)\nDESCRIPTION: Demonstrates the use of MLCommon::device_buffer for RAII-style memory management. This container allows for exception-safe code and asynchronous allocation and deallocation using stream semantics.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<typename T>\nvoid foo(const raft::handle_t& h, ..., cudaStream_t stream )\n{\n    ...\n    MLCommon::device_buffer<T> temp( h.get_device_allocator(), stream, 0 )\n\n    temp.resize(n, stream);\n    kernelA<<<grid, block, 0, stream>>>(..., temp.data(), ...);\n    kernelB<<<grid, block, 0, stream>>>(..., temp.data(), ...);\n    temp.release(stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Charting Linear Regression Speedup in Python\nDESCRIPTION: Applies the chart_single_algo_speedup function to visualize the speedup for the Linear Regression algorithm.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nchart_single_algo_speedup(df, \"LinearRegression\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Communicator Information in cuML\nDESCRIPTION: Shows how to access communicator information from a raft handle, including retrieving rank and size information for multi-GPU coordination.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nvoid foo(const raft::handle_t& h, ...)\n{\n    const MLCommon::cumlCommunicator& communicator = h.get_comms();\n    const int rank = communicator.get_rank();\n    const int size = communicator.get_size();\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Executables for CUML\nDESCRIPTION: Conditionally includes tests directory when building tests is enabled.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_29\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_TESTS)\n  include(CTest)\n  add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking DBSCAN Clustering (Brute Force)\nDESCRIPTION: Sets up and executes a benchmark for the DBSCAN clustering algorithm using brute force approach. This compares performance on neighborhood datasets with small row sizes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"DBSCAN\", runner)\n```\n\n----------------------------------------\n\nTITLE: Temporarily changing logging pattern in C++\nDESCRIPTION: Example of how to temporarily change the logging pattern in C++ for the cuML project using a RAII-like approach.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n{\n  PatternSetter _(MyNewTempFormat);\n  // new log format is in effect from here onwards\n  doStuff();\n  // once the above temporary object goes out-of-scope, the old format will be restored\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU-Enabled Dependencies for cuML Python\nDESCRIPTION: Sets up dependencies for GPU-enabled builds, including treelite configuration and linking with libcuml. Configures single-GPU and multi-GPU libraries based on build options.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/CMakeLists.txt#2025-04-19_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nelse()\n\n  include(rapids-cpm)\n  include(rapids-export)\n  rapids_cpm_init()\n\n  # --- treelite --- #\n  # Need to call get_treelite explicitly because we need the correct\n  # ${TREELITE_LIBS} definition for RF.\n  #\n  # And because cuml Cython code needs the headers to satisfy calls like\n  # 'cdef extern from \"treelite/c_api.h\"'\n\n  # wheel builds use a static treelite, because the 'libtreelite.so' in 'treelite' wheels\n  # isn't intended for dynamic linking by third-party projects (e.g. hides its symbols)\n  if(USE_LIBCUML_WHEEL)\n    set(CUML_PYTHON_TREELITE_TARGET treelite::treelite_static)\n    set(CUML_USE_TREELITE_STATIC ON)\n  else()\n    set(CUML_PYTHON_TREELITE_TARGET treelite::treelite)\n    set(CUML_USE_TREELITE_STATIC OFF)\n  endif()\n\n  set(CUML_EXCLUDE_TREELITE_FROM_ALL ON)\n\n  include(${CUML_CPP_SRC}/cmake/thirdparty/get_treelite.cmake)\n\n  # --- libcuml --- #\n  find_package(cuml \"${RAPIDS_VERSION}\" REQUIRED)\n\n  set(cuml_sg_libraries cuml::${CUML_CPP_TARGET})\n  set(cuml_mg_libraries cuml::${CUML_CPP_TARGET})\n\n  if(NOT SINGLEGPU)\n    list(APPEND cuml_mg_libraries cumlprims_mg::cumlprims_mg)\n  endif()\n\n  list(APPEND CYTHON_FLAGS\n  \"--compile-time-env GPUBUILD=1\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of cuML Example Notebooks\nDESCRIPTION: A structured table listing available cuML example notebooks with their status and descriptions, including demos for ARIMA, Forest Inference, KMeans, Linear Regression, Nearest Neighbors, Random Forest, and Target Encoding.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nNotebook Title | Status | Description\n--- | --- | ---\n[ARIMA Demo](arima_demo.ipynb) | Working | Forecast using ARIMA on time-series data.\n[Forest Inference Demo](forest_inference_demo.ipynb) | Working | Save and load an XGBoost model into FIL and infer on new data.\n[KMeans Demo](kmeans_demo.ipynb) | Working | Predict using k-means, visualize and compare the results with Scikit-learn's k-means.\n[KMeans Multi-Node Multi-GPU Demo](kmeans_mnmg_demo.ipynb) | Working | Predict with MNMG k-means using dask distributed inputs.\n[Linear Regression Demo](linear_regression_demo.ipynb) | Working | Demonstrate the use of OLS Linear Regression for prediction.\n[Nearest Neighbors Demo](nearest_neighbors_demo.ipynb) | Working | Predict using Nearest Neighbors algorithm.\n[Random Forest Demo](random_forest_demo.ipynb) | Working | Use Random Forest for classification, and demonstrate how to pickle the cuML model.\n[Random Forest Multi-Node Multi-GPU Demo](random_forest_mnmg_demo.ipynb) | Working | Solve a classification problem using MNMG Random Forest.\n[Target Encoder Walkthrough](target_encoder_walkthrough.ipynb) | Working | Understand how to use target encoding and why it is preferred over one-hot and label encoding with the help of criteo dataset for click-through rate modelling.\n```\n\n----------------------------------------\n\nTITLE: Accessing CUDA Resources from raft::handle_t in cuML (C++)\nDESCRIPTION: Demonstrates how to access various CUDA resources (such as cuBLAS handle and internal streams) from the raft::handle_t object. This avoids constant creation and deletion of reusable resources.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\nvoid foo(const raft::handle_t& h, ...)\n{\n    cublasHandle_t cublasHandle = h.get_cublas_handle();\n    const int num_streams       = h.get_num_internal_streams();\n    const int stream_idx        = ...\n    cudaStream_t stream         = h.get_internal_stream(stream_idx);\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Host Data for Classification\nDESCRIPTION: Creates a synthetic classification dataset using scikit-learn's make_classification function and converts it to pandas DataFrame. The data is then split into training and testing sets for model evaluation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/random_forest_demo.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%time\nX,y = make_classification(n_samples=n_samples,\n                          n_features=n_features,\n                          n_informative=n_info,\n                          random_state=123, n_classes=2)\n\nX = pd.DataFrame(X.astype(data_type))\n# cuML Random Forest Classifier requires the labels to be integers\ny = pd.Series(y.astype(np.int32))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size = 0.2,\n                                                    random_state=0)\n```\n\n----------------------------------------\n\nTITLE: Allocating Host Memory in cuML (C++)\nDESCRIPTION: Shows how to allocate host memory using the allocator provided by the raft::handle_t object. This is used for larger amounts of host heap memory and follows the same pattern as device memory allocation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<typename T>\nvoid foo(const raft::handle_t& h, cudaStream_t stream, ... )\n{\n    T* temp_h = h.get_host_allocator()->allocate(n*sizeof(T), stream);\n    ...\n    h.get_host_allocator()->deallocate(temp_h, n*sizeof(T), stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Adding cuML Python Module Subdirectories\nDESCRIPTION: Adds all the subdirectories containing various algorithm implementations and utility modules for the cuML Python package build system.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/CMakeLists.txt#2025-04-19_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(cuml/common)\nadd_subdirectory(cuml/internals)\n\nadd_subdirectory(cuml/cluster)\nadd_subdirectory(cuml/datasets)\nadd_subdirectory(cuml/decomposition)\nadd_subdirectory(cuml/ensemble)\nadd_subdirectory(cuml/explainer)\nadd_subdirectory(cuml/experimental/fil)\nadd_subdirectory(cuml/legacy/fil)\nadd_subdirectory(cuml/kernel_ridge)\nadd_subdirectory(cuml/linear_model)\nadd_subdirectory(cuml/manifold)\nadd_subdirectory(cuml/metrics)\nadd_subdirectory(cuml/metrics/cluster)\nadd_subdirectory(cuml/neighbors)\nadd_subdirectory(cuml/random_projection)\nadd_subdirectory(cuml/solvers)\nadd_subdirectory(cuml/svm)\nadd_subdirectory(cuml/tsa)\n\nadd_subdirectory(cuml/experimental/linear_model)\n```\n\n----------------------------------------\n\nTITLE: Setting logging level in C++\nDESCRIPTION: Example of how to set the logging level in C++ for the cuML project. This determines which log messages will be displayed.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nML::Logger::get.setLevel(CUML_LEVEL_WARN);\n// From now onwards, this will print only WARN and above kind of messages\n```\n\n----------------------------------------\n\nTITLE: Installing autogen and dependencies for UCX\nDESCRIPTION: Command to install required dependencies (autogen, autoconf, libtool) for building UCX from source.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install autogen autoconf libtool\n```\n\n----------------------------------------\n\nTITLE: Predicting with Scikit-learn Linear Regression Model in Python\nDESCRIPTION: This snippet uses the trained Scikit-learn model to make predictions on the test dataset.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%time\npredict_sk = ols_sk.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Building KMeans Example as Standalone Project (Bash)\nDESCRIPTION: Commands to build the KMeans example as a standalone project using CMake and Make. It shows how to configure CMake with the cuML path and then compile the project.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/kmeans/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -Dcuml_ROOT=/path/to/cuml\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ make\n```\n\n----------------------------------------\n\nTITLE: Getting Dimensionality of Reduced Data\nDESCRIPTION: Extracts the number of features (k) from the reduced data for later use in the benchmark.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nk = reduced_data.shape[1]\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Standard predict() Method with Manual Decorator in cuML\nDESCRIPTION: Example of decorating a predict() method to handle non-standard input argument names and output dtype conversion. This provides more control over the array conversion process.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@cuml.internals.api_base_return_array(input_arg=\"X_in\", get_output_dtype=True)\ndef predict(self, X):\n   # Convert to CumlArray\n   X_m = input_to_cuml_array(X, order=\"K\").array\n\n   # Call a cuda function\n   X_m = cp.asarray(X_m) + cp.ones(X_m.shape)\n\n   # Return the cupy array directly\n   return X_m\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-GPU Components in CMake\nDESCRIPTION: Conditionally includes multi-GPU implementations of various algorithms when the SINGLEGPU option is not set. These components enable distributed machine learning across multiple GPUs.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\n  # multi GPU components\n  # todo: separate mnmg that require cumlprims from those that don't\n  if(NOT SINGLEGPU)\n    target_sources(${CUML_CPP_TARGET}\n      PRIVATE\n        src/glm/ols_mg.cu\n        src/glm/preprocess_mg.cu\n        src/glm/ridge_mg.cu\n        src/glm/qn_mg.cu\n        src/kmeans/kmeans_mg.cu\n        src/knn/knn_mg.cu\n        src/knn/knn_classify_mg.cu\n        src/knn/knn_regress_mg.cu\n        src/pca/pca_mg.cu\n        src/pca/sign_flip_mg.cu\n        src/solver/cd_mg.cu\n        src/tsvd/tsvd_mg.cu\n    )\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Initializing Cython Sources for RAPIDS cuML Modules\nDESCRIPTION: Initializes an empty list for Cython source files and adds GPU-enabled modules for ARIMA and regression algorithms. These modules depend on specific algorithm implementations and dataset handling.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/datasets/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"arima.pyx\" ${arima_algo} ${datasets_algo})\nadd_module_gpu_default(\"regression.pyx\" ${regression_algo} ${datasets_algo})\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building cuML Benchmark Executable with CMake\nDESCRIPTION: This CMake configuration script builds the cuML benchmark executable when BUILD_CUML_BENCH is enabled. It specifies source files, compilation options, links required libraries (including cuML, benchmark, treelite, and RAFT), and sets installation parameters. The script includes GPU-specific configurations when CUML_ENABLE_GPU is defined.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/bench/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_BENCH)\n  # (please keep the filenames in alphabetical order)\n  add_executable(${CUML_CPP_BENCH_TARGET}\n    sg/arima_loglikelihood.cu\n    sg/dbscan.cu\n    sg/kmeans.cu\n    sg/linkage.cu\n    sg/main.cpp\n    sg/rf_classifier.cu\n    # FIXME: RF Regressor is having an issue where the tests now seem to take\n    # forever to finish, as opposed to the classifier counterparts!\n    # sg/rf_regressor.cu\n    sg/svc.cu\n    sg/svr.cu\n    sg/umap.cu\n    sg/fil.cu\n    sg/filex.cu\n  )\n  if (CUML_ENABLE_GPU)\n    target_compile_definitions(${CUML_CPP_BENCH_TARGET} PUBLIC CUML_ENABLE_GPU)\n  endif()\n\n  target_compile_options(${CUML_CPP_BENCH_TARGET}\n        PRIVATE \"$<$<COMPILE_LANGUAGE:CXX>:${CUML_CXX_FLAGS}>\"\n                \"$<$<COMPILE_LANGUAGE:CUDA>:${CUML_CUDA_FLAGS}>\"\n  )\n\n  target_link_libraries(${CUML_CPP_BENCH_TARGET}\n    PUBLIC\n      cuml::${CUML_CPP_TARGET}\n      benchmark::benchmark\n      ${TREELITE_LIBS}\n      raft::raft\n  )\n\n  target_include_directories(${CUML_CPP_BENCH_TARGET}\n    PRIVATE\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/../src_prims>\n  )\n\n  set_target_properties(\n    ${CUML_CPP_BENCH_TARGET}\n    PROPERTIES INSTALL_RPATH \"\\$ORIGIN/../../../lib\"\n               CXX_STANDARD                      17\n               CXX_STANDARD_REQUIRED             ON\n               CUDA_STANDARD                     17\n               CUDA_STANDARD_REQUIRED            ON\n  )\n\n  install(\n    TARGETS ${CUML_CPP_BENCH_TARGET}\n    COMPONENT testing\n    DESTINATION bin/benchmarks/libcuml\n    EXCLUDE_FROM_ALL\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Converting GPU Data to CPU Memory\nDESCRIPTION: Transfers the generated dataset from GPU memory (cuDF DataFrame) to host memory as a pandas DataFrame for CPU-based processing with scikit-learn.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/nearest_neighbors_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Copy dataset from GPU memory to host memory.\n# This is done to later compare CPU and GPU results.\nhost_data = device_data.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking ElasticNet Regression\nDESCRIPTION: Sets up and executes a benchmark for the ElasticNet Regression algorithm. This compares performance on regression datasets with small row sizes and skinny feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_REGRESSION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"ElasticNet\", runner)\n```\n\n----------------------------------------\n\nTITLE: Configuring Forest Inference Library (FIL) Sources in CMake\nDESCRIPTION: Conditional inclusion of Forest Inference Library (FIL) source files with special handling for GPU vs non-GPU builds. Includes both current and experimental FIL implementations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\n  if(all_algo OR fil_algo)\n    if(CUML_ENABLE_GPU)\n      target_sources(${CUML_CPP_TARGET}\n        PRIVATE\n          # Current FIL\n          src/fil/fil.cu\n          src/fil/infer.cu\n          src/fil/treelite_import.cu\n          # Experimental FIL\n          src/experimental/fil/infer0.cu\n          src/experimental/fil/infer1.cu\n          src/experimental/fil/infer2.cu\n          src/experimental/fil/infer3.cu\n          src/experimental/fil/infer4.cu\n          src/experimental/fil/infer5.cu\n          src/experimental/fil/infer6.cu\n          src/experimental/fil/infer7.cu\n          src/experimental/fil/infer8.cu\n          src/experimental/fil/infer9.cu\n          src/experimental/fil/infer10.cu\n          src/experimental/fil/infer11.cu)\n    endif()\n    target_sources(${CUML_CPP_TARGET}\n      PRIVATE\n        # Experimental FIL\n        src/experimental/fil/infer0.cpp\n        src/experimental/fil/infer1.cpp\n        src/experimental/fil/infer2.cpp\n        src/experimental/fil/infer3.cpp\n        src/experimental/fil/infer4.cpp\n        src/experimental/fil/infer5.cpp\n        src/experimental/fil/infer6.cpp\n        src/experimental/fil/infer7.cpp\n        src/experimental/fil/infer8.cpp\n        src/experimental/fil/infer9.cpp\n        src/experimental/fil/infer10.cpp\n        src/experimental/fil/infer11.cpp)\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Cython Module Compilation in RAPIDS cuML\nDESCRIPTION: This CMake script configures Cython module compilation for RAPIDS cuML. It initializes the variable for Cython sources, adds default GPU modules, handles multi-GPU configuration, and creates the Cython modules with proper linking and includes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/common/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"cuda.pyx\")\nadd_module_gpu_default(\"handle.pyx\")\nadd_module_gpu_default(\"pointer_utils.pyx\")\n\nif(NOT SINGLEGPU)\n  list(APPEND cython_sources\n       opg_data_utils_mg.pyx\n  )\nendif()\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_mg_libraries}\"\n  MODULE_PREFIX common_\n)\n\nif(${CUML_UNIVERSAL})\n# todo: ml_cuda_utils.h should be in the include folder of cuML or the functionality\n# moved to another file, pointer_utils.pyx needs it\n# https://github.com/rapidsai/cuml/issues/4841\n  target_include_directories(common_pointer_utils PRIVATE \"../../../../cpp/src/\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing MPI Communications for Multi-GPU cuML\nDESCRIPTION: Example showing how to initialize CUDA-aware MPI communications, set up device assignment, and create a raft handle for multi-GPU operations. Demonstrates proper setup and cleanup of MPI resources.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\n#include <mpi.h>\n#include <raft/core/handle.hpp>\n#include <raft/comms/mpi_comms.hpp>\n#include <mlalgo/mlalgo.hpp>\n...\nint main(int argc, char * argv[])\n{\n    MPI_Init(&argc, &argv);\n    int rank = -1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_rank = -1;\n    {\n        MPI_Comm local_comm;\n        MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &local_comm);\n\n        MPI_Comm_rank(local_comm, &local_rank);\n\n        MPI_Comm_free(&local_comm);\n    }\n\n    cudaSetDevice(local_rank);\n\n    mpi_comms raft_mpi_comms;\n    MPI_Comm_dup(MPI_COMM_WORLD, &raft_mpi_comms);\n\n    {\n        raft::handle_t raftHandle;\n        initialize_mpi_comms(raftHandle, raft_mpi_comms);\n\n        ...\n\n        ML::mlalgo(raftHandle, ... );\n    }\n\n    MPI_Comm_free(&raft_mpi_comms);\n\n    MPI_Finalize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a DBSCAN Class with Multiple Inheritance in Python\nDESCRIPTION: Example of defining a DBSCAN class that inherits from multiple base classes and mixins, demonstrating the method resolution order (MRO) for tag inheritance.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass DBSCAN(Base,\n             ClusterMixin,\n             CMajorInputTagMixin):\n```\n\n----------------------------------------\n\nTITLE: Creating RAPIDS Cython Modules for cuML\nDESCRIPTION: Configures the creation of Cython modules for RAPIDS cuML. It specifies C++ as the target language, uses the previously defined Cython source files, and links against cuML shared libraries. The resulting modules are prefixed with 'datasets_'.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/datasets/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX datasets_\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Algorithm Source Files for cuML in CMake\nDESCRIPTION: A representative selection of conditional source file inclusion based on algorithm selection flags. Each algorithm can be individually enabled or disabled, with files added to the build only when needed.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\n  if(all_algo OR arima_algo)\n    target_sources(${CUML_CPP_TARGET}\n      PRIVATE\n        src/arima/batched_arima.cu\n        src/arima/batched_kalman.cu)\n  endif()\n\n  if(all_algo OR datasets_algo)\n    target_sources(${CUML_CPP_TARGET}\n      PRIVATE\n        src/datasets/make_arima.cu\n        src/datasets/make_blobs.cu\n        src/datasets/make_regression.cu)\n  endif()\n\n  if(all_algo OR dbscan_algo)\n    target_sources(${CUML_CPP_TARGET}\n      PRIVATE\n        src/dbscan/dbscan.cu)\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Converting GPU Data to Host Memory\nDESCRIPTION: Transfers data from GPU to CPU by computing Dask array and converting cuPy array to NumPy format.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nX_cp = X_dca.compute()\nX_np = cp.asnumpy(X_cp)\ndel X_cp\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Truncated SVD\nDESCRIPTION: Sets up and executes a benchmark for the Truncated Singular Value Decomposition (TSVD) algorithm. This compares performance on neighborhood datasets with small row sizes and wide feature dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=WIDE_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"TSVD\", runner)\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to CUML C++ Target\nDESCRIPTION: Links all required dependencies to the CUML C++ target, including public libraries (rapids_logger, rmm) and the private libraries configured earlier.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_26\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${CUML_CPP_TARGET}\n  PUBLIC  rapids_logger::rapids_logger rmm::rmm ${CUVS_LIB}\n          ${_cuml_cpp_public_libs}\n  PRIVATE ${_cuml_cpp_private_libs}\n)\n```\n\n----------------------------------------\n\nTITLE: Building Cython Artifacts for cuML Python\nDESCRIPTION: Configures the Cython build process by including necessary modules, handling special cases for CPU-only builds, and setting up algorithm configurations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/CMakeLists.txt#2025-04-19_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(\"${CUML_CPP_SRC}/cmake/modules/ConfigureAlgorithms.cmake\")\ninclude(cmake/ConfigureCythonAlgorithms.cmake)\n\nif(CUML_CPU)\n  # libcuml requires metrics built if HDSCAN is built, which is not the case\n  # for cuml-cpu\n  unset(metrics_algo)\nendif()\n\nmessage(VERBOSE \"CUML_PY: Building cuML with algorithms: '${CUML_ALGORITHMS}'.\")\n\ninclude(rapids-cython-core)\nrapids_cython_init()\n```\n\n----------------------------------------\n\nTITLE: Initializing RAPIDS cuML Benchmarking Environment\nDESCRIPTION: Sets up the RAPIDS cuML benchmarking environment by importing necessary modules, disabling specific warnings, and displaying the cuML version. This is the initial setup required for all benchmarks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cuml\nimport pandas as pd\n\nfrom cuml.benchmark.runners import SpeedupComparisonRunner\nfrom cuml.benchmark.algorithms import algorithm_by_name\n\nimport warnings\nwarnings.filterwarnings('ignore', 'Expected column ')\n\nprint(cuml.__version__)\n```\n\n----------------------------------------\n\nTITLE: Direct Return of Array-Like Objects in cuML Methods\nDESCRIPTION: Example showing the recommended approach of directly returning array-like objects from methods, allowing the framework to handle conversion automatically.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef predict(self) -> CumlArray:\n   cp_arr = cp.ones((10,))\n\n   return cp_arr\n```\n\n----------------------------------------\n\nTITLE: Installing gdrcopy for faster GPU-Network data transfer\nDESCRIPTION: Steps to install gdrcopy from GitHub, which enables faster data transfer between GPUs and network cards. This is an optional dependency for UCX.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/gdrcopy.git\ncd gdrcopy\nmake -j PREFIX=$CONDA_INSTALL_PREFIX CUDA=/usr/local/cuda && make -j install\nsudo ./insmod.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Cython Source Files for RAPIDS cuML Solvers\nDESCRIPTION: Defines the Cython source files to be compiled for the solvers module. The base set includes coordinate descent (cd.pyx), quasi-Newton (qn.pyx), and stochastic gradient descent (sgd.pyx) implementations. It conditionally adds multi-GPU coordinate descent (cd_mg.pyx) when not in single-GPU mode.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/solvers/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources\n    cd.pyx\n    qn.pyx\n    sgd.pyx\n)\n\nif(NOT SINGLEGPU)\n  list(APPEND cython_sources\n       cd_mg.pyx\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Predicting with Scikit-learn Model\nDESCRIPTION: Generates predictions using the trained Dask-ML K-means model.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/kmeans_mnmg_demo.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlabels_sk = kmeans_sk.predict(X_np).compute()\n```\n\n----------------------------------------\n\nTITLE: Pickling Combined Model from Distributed KMeans in Python\nDESCRIPTION: Extracts the combined model from a distributed KMeans model and pickles it for persistence.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\n\nsingle_gpu_model = dist_model.get_combined_model()\npickle.dump(single_gpu_model, open(\"kmeans_model.pkl\", \"wb\"))\n```\n\n----------------------------------------\n\nTITLE: Building CUML C Shared Library\nDESCRIPTION: Conditionally builds the CUML C API shared library with appropriate source files, compile definitions, include directories, and link dependencies.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_28\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_C_LIBRARY)\n  add_library(${CUML_C_TARGET}\n    src/common/cumlHandle.cpp\n    src/common/cuml_api.cpp\n    src/dbscan/dbscan_api.cpp\n    src/glm/glm_api.cpp\n    src/holtwinters/holtwinters_api.cpp\n    src/knn/knn_api.cpp\n    src/svm/svm_api.cpp\n  )\n\n  add_library(cuml::${CUML_C_TARGET} ALIAS ${CUML_C_TARGET})\n\n  target_compile_definitions(${CUML_C_TARGET}\n    PRIVATE\n      CUML_C_API)\n\n  target_include_directories(${CUML_C_TARGET}\n    PRIVATE\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src>\n  )\n\n  target_link_libraries(${CUML_C_TARGET}\n    PUBLIC\n      ${CUML_CPP_TARGET}\n  )\n\n  # ensure CUDA symbols aren't relocated to the middle of the debug build binaries\n  target_link_options(${CUML_C_TARGET} PRIVATE \"${CMAKE_CURRENT_BINARY_DIR}/fatbin.ld\")\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting User Options for cuML Python Build\nDESCRIPTION: Defines various build options like CUML_UNIVERSAL, SINGLEGPU, and USE_LIBCUML_WHEEL that control how the Python components are built, along with the CUML_ALGORITHMS option for selecting specific algorithms.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\noption(CUML_UNIVERSAL \"Build all cuML Python components.\" ON)\noption(SINGLEGPU \"Disable all mnmg components and comms libraries\" OFF)\noption(USE_LIBCUML_WHEEL \"Use libcuml wheel to provide some dependencies\" OFF)\n\n# todo: use CMAKE_MESSAGE_CONTEXT for prefix for logging.\n# https://github.com/rapidsai/cuml/issues/4843\nmessage(VERBOSE \"CUML_PY: Build only cuML CPU Python components.: ${CUML_CPU}\")\nmessage(VERBOSE \"CUML_PY: Disabling all mnmg components and comms libraries: ${SINGLEGPU}\")\n\nset(CUML_ALGORITHMS \"ALL\" CACHE STRING \"Choose which algorithms are built cuML. Can specify individual algorithms or groups in a semicolon-separated list.\")\n\nset(CUML_CPP_TARGET \"cuml++\")\nset(CUML_CPP_SRC \"../../cpp\")\n```\n\n----------------------------------------\n\nTITLE: Developer Guidelines for Preprocessor Implementation\nDESCRIPTION: Guidelines for developers when adding or updating preprocessors, including instructions for code copying, modification limits, testing requirements, and data type handling expectations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/_thirdparty/sklearn/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nWhen adding new preprocessors or updating, keep in mind:\n    - Files should be copied as-is from the scikit-learn repo (preserving scikit-learn license text)\n    - Changes should be kept minimal, large portions of modified imported code should lie in the thirdparty_adapter directory\n    - Only well-tested, reliable accelerated preprocessing functions should be exposed in cuml.preprocessing.__init__.py\n    - Tests must be added for each exposed function\n    - Remember that a preprocessing model should always return the same datatype it received as input (NumPy, CuPy, Pandas, cuDF, Numba)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple CUDA Streams with OpenMP in C++\nDESCRIPTION: Demonstrates how to use multiple CUDA streams within OpenMP threads to increase GPU occupancy. The code synchronizes streams, performs light CPU processing, and launches CUDA kernels concurrently.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nhandle.sync_stream();\n\nint n_streams = handle.get_num_internal_streams();\n\n#pragma omp parallel for num_threads(n_threads)\nfor(int i = 0; i < n; i++) {\n    int thread_num = omp_get_thread_num() % n_threads;\n    cudaStream_t s = handle.get_stream_from_stream_pool(thread_num);\n    ... possible light cpu pre-processing ...\n    my_kernel1<<<b, tpb, 0, s>>>(...);\n    ...\n    ... some possible async d2h / h2d copies ...\n    my_kernel2<<<b, tpb, 0, s>>>(...);\n    ...\n    handle.sync_stream(s);\n    ... possible light cpu post-processing ...\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing _get_param_names for cuML Estimator in Python\nDESCRIPTION: Shows how to implement the _get_param_names() method for a cuML estimator to support cloning. This method should return a list of strings representing all estimator attributes necessary for duplication.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef _get_param_names(cls):\n   return super()._get_param_names() + [\n      \"eps\",\n      \"min_samples\",\n   ]\n```\n\n----------------------------------------\n\nTITLE: Summarizing scikit-learn GPU Acceleration Test Results in Bash\nDESCRIPTION: Demonstrates how to generate a summary from test XML reports using summarize-results.sh with verbosity flag and an 80% pass rate threshold.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/ci/accel/scikit-learn-tests/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./summarize-results.sh -v -f 80 report.xml\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Data Processing and Machine Learning in Python\nDESCRIPTION: This code imports necessary libraries for data manipulation, GPU-accelerated computations, and machine learning tasks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/target_encoder_walkthrough.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport cudf as gd\nimport cupy as cp\nfrom cuml.preprocessing.LabelEncoder import LabelEncoder\nfrom cuml.preprocessing.TargetEncoder import TargetEncoder\nimport dask as dask, dask_cudf\nfrom dask.distributed import Client, wait\nfrom dask_cuda import LocalCUDACluster\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport time\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Synthetic Dataset with cuML in Python\nDESCRIPTION: Generates a distributed synthetic dataset using cuML's Dask-enabled make_blobs function.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.dask.datasets import make_blobs\n\nn_workers = len(client.scheduler_info()[\"workers\"].keys())\n\nX, y = make_blobs(n_samples=5000, \n                  n_features=30,\n                  centers=5, \n                  cluster_std=0.4, \n                  random_state=0,\n                  n_parts=n_workers*5)\n\nX = X.persist()\ny = y.persist()\n```\n\n----------------------------------------\n\nTITLE: Installing UCX-py from source\nDESCRIPTION: Commands to clone and install the Python bindings for UCX (ucx-py) from source.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:rapidsai/ucx-py\ncd ucx-py\n\nexport UCX_PATH=$CONDA_PREFIX\nmake -j install\n```\n\n----------------------------------------\n\nTITLE: Configuring RAPIDS cuML Decomposition Module Build\nDESCRIPTION: CMake configuration for building PCA and TSVD decomposition modules. Sets up Cython source files and creates module targets with appropriate GPU configurations. Includes conditional compilation for multi-GPU support.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/decomposition/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"pca.pyx\" ${pca_algo} ${decomposition_algo})\nadd_module_gpu_default(\"tsvd.pyx\" ${tsvd_algo} ${decomposition_algo})\n\nif(NOT SINGLEGPU)\n  list(APPEND cython_sources\n       base_mg.pyx\n       pca_mg.pyx\n       tsvd_mg.pyx\n  )\nendif()\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_mg_libraries}\"\n  MODULE_PREFIX decomposition_\n)\n```\n\n----------------------------------------\n\nTITLE: Installing cuML CPU Package with Conda\nDESCRIPTION: Commands for installing cuML CPU version using conda/mamba package manager. Shows both stable and nightly build installation options.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/execution_device_interoperability.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmamba install -c rapidsai -c nvidia -c conda-forge cuml-cpu=23.10 \n# mamba install -c rapidsai-nightly -c nvidia -c conda-forge cuml-cpu=23.12 # for nightly builds\n```\n\n----------------------------------------\n\nTITLE: Using ML::stdAllocatorAdapter with STL Containers in cuML (C++)\nDESCRIPTION: Shows how to use ML::stdAllocatorAdapter to integrate ML::hostAllocator with STL containers like std::vector. This allows for custom memory allocation within standard C++ containers.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<typename T>\nvoid foo(const raft::handle_t& h, ..., cudaStream_t stream )\n{\n    ...\n    std::vector<T,ML::stdAllocatorAdapter<T> > temp( n, val, ML::stdAllocatorAdapter<T>(h.get_host_allocator(), stream) )\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Cython Source Files for RAPIDS cuML\nDESCRIPTION: This snippet defines a list of Cython source files to be compiled. It includes base, device support, internals, and logger modules.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/internals/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources\n    base.pyx\n    device_support.pyx\n    internals.pyx\n    logger.pyx\n)\n```\n\n----------------------------------------\n\nTITLE: Additional build.sh Options for cuML\nDESCRIPTION: Various options available with the build.sh script, including cleaning prior builds, enabling verbose output, debug mode, limiting parallel jobs, and building without installation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ./build.sh clean                     # remove any prior build artifacts and configuration (start over)\n$ ./build.sh libcuml -v                # build and install libcuml with verbose output\n$ ./build.sh libcuml -g                # build and install libcuml for debug\n$ PARALLEL_LEVEL=8 ./build.sh libcuml  # build and install libcuml limiting parallel build jobs to 8 (ninja -j8)\n$ ./build.sh libcuml -n                # build libcuml but do not install\n$ ./build.sh prims --allgpuarch        # build the ML prims tests for all supported GPU architectures\n$ ./build.sh cuml --singlegpu          # build the cuML python package without MNMG algorithms\n$ ./build.sh --ccache                  # use ccache to cache compilations, speeding up subsequent builds\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for DBSCAN with cuML\nDESCRIPTION: Sets up a CMake project for building a DBSCAN clustering example. Configures C++17 standard, finds the cuML package, and sets up proper linking with CUDA runtime. Includes necessary compiler and linker settings for CUDA support.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/dbscan/CMakeLists_standalone.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.26.4 FATAL_ERROR)\nproject(dbscan_example LANGUAGES CXX CUDA)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nfind_package(cuml REQUIRED)\n\nadd_executable(dbscan_example dbscan_example.cpp)\ntarget_link_libraries(dbscan_example PRIVATE cuml::cuml++)\n\n# Need to set linker language to CUDA to link the CUDA Runtime\nset_target_properties(dbscan_example PROPERTIES LINKER_LANGUAGE \"CUDA\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Benchmark Parameters\nDESCRIPTION: Defines global benchmark parameters including the number of repetitions (N_REPS), dataset types for different algorithm categories, and input data format type. These parameters control how benchmarks are run.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nN_REPS = 3  # Number of times each test is repeated\n\nDATA_NEIGHBORHOODS = \"blobs\"\nDATA_CLASSIFICATION = \"classification\"\nDATA_REGRESSION = \"regression\"\n\nINPUT_TYPE = \"numpy\"\n\nbenchmark_results = []\n```\n\n----------------------------------------\n\nTITLE: Generating cuML Documentation with Bash\nDESCRIPTION: This command generates both C++ and Python documentation for cuML. It uses a bash script named 'build.sh' with specific arguments to create the documentation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbash build.sh cppdocs pydocs\n```\n\n----------------------------------------\n\nTITLE: Configuring Cython Module for FIL in cuML\nDESCRIPTION: This snippet sets up the configuration for building a Cython module for the Forest Inference Library (FIL) in cuML. It specifies the source files, linked libraries, and uses the rapids_cython_create_modules function to generate the module with a specific prefix.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/fil/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"fil.pyx\" ${fil_algo} ${randomforestclassifier_algo} ${randomforestregressor_algo})\n\nset(linked_libraries\n  \"${cuml_sg_libraries}\"\n  \"${CUML_PYTHON_TREELITE_TARGET}\"\n)\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${linked_libraries}\"\n  MODULE_PREFIX experimental_fil_\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Testing and Benchmarking Dependencies\nDESCRIPTION: Sets up Google Test when tests are enabled and Google Benchmark when benchmarking is enabled. Both are configured to build statically.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_TESTS OR BUILD_PRIMS_TESTS)\n  include(${rapids-cmake-dir}/cpm/gtest.cmake)\n  rapids_cpm_gtest(BUILD_STATIC)\nendif()\n\nif(BUILD_CUML_BENCH)\n  include(${rapids-cmake-dir}/cpm/gbench.cmake)\n  rapids_cpm_gbench(BUILD_STATIC)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Sparse Random Projection in Python using cuML\nDESCRIPTION: Configures and runs a benchmark for the Sparse Random Projection algorithm using cuML's SpeedupComparisonRunner. It compares performance on small datasets with wide features for neighborhood data.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES,\n    bench_dims=WIDE_FEATURES,\n    dataset_name=DATA_NEIGHBORHOODS,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"SparseRandomProjection\", runner)\n```\n\n----------------------------------------\n\nTITLE: Handling Existing cuML Package Detection\nDESCRIPTION: Checks if cuML is already installed and available as a CMake package. If found, it exits early as there's no need to rebuild.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# Check if cuml is already available. If so, it is the user's responsibility to ensure that the\n# CMake package is also available at build time of the Python cuml package.\nfind_package(cuml \"${RAPIDS_VERSION}\")\n\nif(cuml_FOUND)\n  return()\nendif()\n\nunset(cuml_FOUND)\n```\n\n----------------------------------------\n\nTITLE: Setting cuML Compiler Definitions and Options in CMake\nDESCRIPTION: Configures compiler definitions, options, and include directories for the cuML library. These settings control various aspects of the compilation process including logging level, CUSPARSE deprecation warnings, and directory locations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\n  target_compile_definitions(${CUML_CPP_TARGET}\n    PUBLIC\n      DISABLE_CUSPARSE_DEPRECATED\n    PRIVATE\n      CUML_CPP_API\n  )\n\n  target_compile_options(${CUML_CPP_TARGET}\n        PRIVATE \"$<$<COMPILE_LANGUAGE:CXX>:${CUML_CXX_FLAGS}>\"\n                \"$<$<COMPILE_LANGUAGE:CUDA>:${CUML_CUDA_FLAGS}>\"\n  )\n  target_compile_definitions(${CUML_CPP_TARGET} PUBLIC \"CUML_LOG_ACTIVE_LEVEL=RAPIDS_LOGGER_LOG_LEVEL_${LIBCUML_LOGGING_LEVEL}\")\n\n  target_include_directories(${CUML_CPP_TARGET}\n    PUBLIC\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}/include>\n    PRIVATE\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src>\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src/metrics>\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src_prims>\n      $<$<BOOL:${BUILD_CUML_MPI_COMMS}>:${MPI_CXX_INCLUDE_PATH}>\n    INTERFACE\n      $<INSTALL_INTERFACE:include>\n  )\n\n  set(_cuml_cpp_public_libs)\n  set(_cuml_cpp_private_libs)\n```\n\n----------------------------------------\n\nTITLE: Enabling MPI Communications for Multi-GPU Tests\nDESCRIPTION: Automatically enables MPI communications when multi-GPU tests are requested and single-GPU mode is not active. This ensures test dependencies are properly configured.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_MG_TESTS AND NOT SINGLEGPU)\n  message(STATUS \"CUML_CPP: Detected BUILD_CUML_MG_TESTS set to ON. Enabling BUILD_CUML_MPI_COMMS\")\n  set(BUILD_CUML_MPI_COMMS ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters for Linear Regression Dataset in Python\nDESCRIPTION: This snippet sets the parameters for generating the regression dataset, including the number of samples, features, and a random state for reproducibility.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nn_samples = 2**20 #If you are running on a GPU with less than 16GB RAM, please change to 2**19 or you could run out of memory\nn_features = 399\n\nrandom_state = 23\n```\n\n----------------------------------------\n\nTITLE: Opening Generated cuML Documentation with xdg-open\nDESCRIPTION: This command opens the generated HTML documentation using the xdg-open utility. It specifically opens the API documentation file located in the build/html directory.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nxdg-open build/html/api.html\n```\n\n----------------------------------------\n\nTITLE: Initializing RAPIDS CPM and Logger\nDESCRIPTION: Sets up the RAPIDS CMake Package Manager (CPM) and configures the logging system for CUML. Creates logger macros that will be used throughout the project.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n# add third party dependencies using CPM\nrapids_cpm_init()\nrapids_cmake_install_lib_dir(lib_dir)\n\ninclude(${rapids-cmake-dir}/cpm/rapids_logger.cmake)\nrapids_cpm_rapids_logger(BUILD_EXPORT_SET cuml-exports INSTALL_EXPORT_SET cuml-exports)\ncreate_logger_macros(CUML \"ML::default_logger()\" include/cuml/common)\n```\n\n----------------------------------------\n\nTITLE: Configuring CCCache for Build Acceleration\nDESCRIPTION: Sets up ccache as a compiler launcher for C, C++, and CUDA compilers when USE_CCACHE is enabled. This accelerates builds by caching compilation results.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CCACHE)\n  set(CMAKE_C_COMPILER_LAUNCHER ccache)\n  set(CMAKE_CXX_COMPILER_LAUNCHER ccache)\n  set(CMAKE_CUDA_COMPILER_LAUNCHER ccache)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Optimizing cuML Build for Specific GPU Architecture\nDESCRIPTION: Command to specify a GPU compute capability (in this case for Volta GPUs) when configuring the CMake build, which can reduce compile times by targeting specific hardware.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCMAKE_INSTALL_PREFIX=$CONDA_PREFIX -DGPU_ARCHS=\"70\"\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Requirements and Project Configuration for libcuml-python\nDESCRIPTION: Initializes the CMake project with version information and required languages. Sets up CUDA architecture configuration for the libcuml-python project.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.30.4 FATAL_ERROR)\n\ninclude(../../cmake/rapids_config.cmake)\n\ninclude(rapids-cuda)\nrapids_cuda_init_architectures(libcuml-python)\n\nproject(\n  libcuml-python\n  VERSION \"${RAPIDS_VERSION}\"\n  LANGUAGES CXX CUDA\n)\n```\n\n----------------------------------------\n\nTITLE: Cloning the cuML Repository\nDESCRIPTION: Command to clone the cuML repository from GitHub, the first step in the manual build process.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/rapidsai/cuml.git\n```\n\n----------------------------------------\n\nTITLE: Configuring GPUTreeShap Dependencies\nDESCRIPTION: Sets up GPUTreeShap dependency when tree-based algorithms or specifically treeshap_algo are enabled. Includes a workaround for static builds.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(all_algo OR treeshap_algo)\n  include(cmake/thirdparty/get_gputreeshap.cmake)\n  # Workaround until https://github.com/rapidsai/rapids-cmake/issues/176 is resolved\n  if(NOT BUILD_SHARED_LIBS)\n    rapids_export_package(BUILD GPUTreeShap cuml-exports)\n    rapids_export_package(INSTALL GPUTreeShap cuml-exports)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Linear Regression in Python\nDESCRIPTION: This snippet imports necessary modules from cuDF, cuML, and Scikit-learn for performing linear regression and data manipulation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/linear_regression_demo.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cudf\nfrom cuml import make_regression, train_test_split\nfrom cuml.linear_model import LinearRegression as cuLinearRegression\nfrom cuml.metrics.regression import r2_score\nfrom sklearn.linear_model import LinearRegression as skLinearRegression\n```\n\n----------------------------------------\n\nTITLE: Dataset List in Markdown\nDESCRIPTION: A markdown list of time series datasets from Statistics New Zealand, including their descriptions and time ranges. It also includes derived datasets and exogenous variables created for testing purposes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/tests/ts_datasets/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- `alcohol.csv`: Alcohol available for consumption (millions of litres), quarterly 1994-2019.\n- `cattle.csv`: Agricultural survey: counts of different types of cattle (units) per year, 2002-2018.\n- `deaths_by_region.csv`: Deaths (units) in 16 regions per year, 1991-2018.\n- `guest_nights_by_region.csv`: Guest nights (thousands) in 12 regions, monthly 1996-2019.\n- `hourly_earnings_by_industry.csv`: Hourly earnings ($) in 14 industries, quarterly 1989-2019.\n- `long_term_arrivals_by_citizenship.csv`: Long-term arrivals (units) from 8 countries per year, 2004-2018.\n- `net_migrations_auckland_by_age.csv`: Net migrations in Auckland by age range (from 0 to 49) per year, 1991-2010.\n- `passenger_movements.csv`: Passenger movements (thousands), quarterly 1975-2019.\n- `police_recorded_crime.csv`: Recorded crimes (units) per year, 1878-2014.\n- `population_estimate.csv`: Population estimates (thousands) per year, 1875-2011.\n```\n\n----------------------------------------\n\nTITLE: Setting Up Testing Dependencies\nDESCRIPTION: Configures thread support for tests when either CUML or PRIMS tests are enabled. This is a prerequisite for test execution.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_TESTS OR BUILD_PRIMS_TESTS)\n  find_package(Threads)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building cuML Using the Recommended build.sh Script\nDESCRIPTION: Examples of using the build.sh convenience script to build and install cuML components. This script handles the build process and installs to the location specified in INSTALL_PREFIX or CONDA_PREFIX.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ./build.sh                           # build the cuML libraries, tests, and python package, then\n                                       # install them to $INSTALL_PREFIX if set, otherwise $CONDA_PREFIX\n```\n\n----------------------------------------\n\nTITLE: Processing CPU-Only Build Options for cuML Python\nDESCRIPTION: Configures the build system for CPU-only mode by setting appropriate flags, limiting algorithm selection to CPU-compatible algorithms, and setting up Cython environment variables.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(CUML_CPU)\n  set(CUML_UNIVERSAL OFF)\n  set(SINGLEGPU ON)\n\n  # only a subset of algorithms are supported in CPU-only cuML\n  set(CUML_ALGORITHMS \"linearregression\")\n  list(APPEND CUML_ALGORITHMS \"pca\")\n  list(APPEND CUML_ALGORITHMS \"tsvd\")\n  list(APPEND CUML_ALGORITHMS \"elasticnet\")\n  list(APPEND CUML_ALGORITHMS \"logisticregression\")\n  list(APPEND CUML_ALGORITHMS \"ridge\")\n  list(APPEND CUML_ALGORITHMS \"lasso\")\n  list(APPEND CUML_ALGORITHMS \"umap\")\n  list(APPEND CUML_ALGORITHMS \"knn\")\n  list(APPEND CUML_ALGORITHMS \"hdbscan\")\n  list(APPEND CUML_ALGORITHMS \"dbscan\")\n  list(APPEND CUML_ALGORITHMS \"kmeans\")\n\n  # this won't be needed when we add CPU libcuml++ (FIL)\n  set(cuml_sg_libraries \"\")\n\n  list(APPEND CYTHON_FLAGS\n  \"--compile-time-env GPUBUILD=0\")\n\n# cuml-cpu does not need libcuml++.so\n```\n\n----------------------------------------\n\nTITLE: Building Doxygen Documentation for cuML C/C++ Code\nDESCRIPTION: Command to generate Doxygen documentation for all C/C++ source files in cuML, which provides API reference documentation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ make doc\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for Conda Environment\nDESCRIPTION: Command to configure CMake to install libcuml++ to the conda environment prefix, which ensures proper integration with the conda environment.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCMAKE_INSTALL_PREFIX=$CONDA_PREFIX\n```\n\n----------------------------------------\n\nTITLE: Managing Concurrent Algorithm Execution with Multiple Streams\nDESCRIPTION: Example showing how to schedule multiple algorithms to run concurrently using separate CUDA streams and handles. This is useful for maximizing GPU utilization by running operations in parallel.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/DEVELOPER_GUIDE.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cuml\nfrom cuml.cuda import Stream\ns1 = Stream()\nh1 = cuml.Handle()\nh1.setStream(s1)\ns2 = Stream()\nh2 = cuml.Handle()\nh2.setStream(s2)\nalgo1 = cuml.Algo1(handle=h1, ...)\nalgo2 = cuml.Algo2(handle=h2, ...)\nalgo1.fit(X1, y1)\nalgo2.fit(X2, y2)\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX for cuML\nDESCRIPTION: BibTeX citation for referencing cuML in academic work, citing the paper on Machine Learning in Python.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{raschka2020machine,\n  title={Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence},\n  author={Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},\n  journal={arXiv preprint arXiv:2002.04803},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Creating CUDA Fat Binary Linker Script\nDESCRIPTION: Generates a linker script for CUDA fat binaries when C or C++ libraries are being built. This helps with properly linking CUDA device code.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_C_LIBRARY OR BUILD_CUML_CPP_LIBRARY)\n  file(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/fatbin.ld\"\n[=[\nSECTIONS\n{\n.nvFatBinSegment : { *(.nvFatBinSegment) }\n.nv_fatbin : { *(.nv_fatbin) }\n}\n]=])\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring HDBSCAN Cython Modules for CUDA ML\nDESCRIPTION: Sets up Cython modules for HDBSCAN algorithm in CUDA ML. It adds GPU-enabled modules for the main HDBSCAN algorithm and prediction, then creates the modules with specified source files, linked libraries, and a prefix.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/cluster/hdbscan/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"hdbscan.pyx\" ${hdbscan_algo} ${cluster_algo})\nadd_module_gpu_default(\"prediction.pyx\" ${hdbscan_algo} ${cluster_algo})\n\nrapids_cython_create_modules(\n        CXX\n        SOURCE_FILES \"${cython_sources}\"\n        LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n        MODULE_PREFIX cluster_hdbscan_\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Support for cuML Python Build\nDESCRIPTION: Sets up CUDA language support for the project if not in CPU-only mode, initializing CUDA architectures for the cuML Python project.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\noption(CUML_CPU \"Build only cuML CPU Python components.\" OFF)\nset(language_list \"CXX\")\n\nif(NOT CUML_CPU)\n  # We always need CUDA for cuML GPU because the raft dependency brings in a\n  # header-only cuco dependency that enables CUDA unconditionally.\n  include(rapids-cuda)\n  rapids_cuda_init_architectures(cuml-python)\n  list(APPEND language_list \"CUDA\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating CUML Export Targets\nDESCRIPTION: Sets up CMake export targets for CUML that can be imported by other projects, with documentation and conditional code blocks to handle Treelite dependencies.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_32\n\nLANGUAGE: CMake\nCODE:\n```\nset(doc_string\n[=[\nProvide targets for cuML.\n\ncuML is a suite of libraries that implement machine learning algorithms and mathematical primitives\nfunctions that share compatible APIs with other RAPIDS projects.\n\n]=])\n\nset(code_string )\n\nif (TARGET treelite::treelite)\n    string(APPEND code_string\n[=[\nif (TARGET treelite::treelite AND (NOT TARGET treelite))\n    add_library(treelite ALIAS treelite::treelite)\nendif()\n]=])\nelse()\n    string(APPEND code_string\n[=[\nif (TARGET treelite::treelite_static AND (NOT TARGET treelite_static))\n    add_library(treelite_static ALIAS treelite::treelite_static)\nendif()\n]=])\n\nendif()\n\nrapids_export(INSTALL cuml\n    EXPORT_SET cuml-exports\n    GLOBAL_TARGETS ${CUML_C_TARGET} ${CUML_CPP_TARGET}\n    NAMESPACE cuml::\n    DOCUMENTATION doc_string\n    FINAL_CODE_BLOCK code_string\n    )\n\nrapids_export(BUILD cuml\n    EXPORT_SET cuml-exports\n    GLOBAL_TARGETS ${CUML_C_TARGET} ${CUML_CPP_TARGET}\n    NAMESPACE cuml::\n    DOCUMENTATION doc_string\n    FINAL_CODE_BLOCK code_string\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Cython Modules for RAPIDS cuML Solvers\nDESCRIPTION: Utilizes the rapids_cython_create_modules function to build the Cython modules for the solvers. This configures C++ compilation for the source files, links against the necessary multi-GPU libraries, and sets up the module prefix as 'solvers_'.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/solvers/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_mg_libraries}\"\n  MODULE_PREFIX solvers_\n)\n```\n\n----------------------------------------\n\nTITLE: Recommended Usage of Return Type Annotations in cuML Estimators\nDESCRIPTION: Examples of properly adding return type information to estimator methods to enable automatic decoration by the Base metaclass based on return type.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef fit(self, X, y, convert_dtype=True) -> \"KNeighborsRegressor\":\ndef predict(self, X, convert_dtype=True) -> CumlArray:\ndef kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity') -> SparseCumlArray:\ndef predict(self, start=0, end=None, level=None) -> typing.Union[CumlArray, float]:\n```\n\n----------------------------------------\n\nTITLE: Running clang-tidy with Docker\nDESCRIPTION: Docker command to run clang-tidy checks on the codebase. This uses the RAPIDS CI Docker image and mounts the current directory to perform static code analysis.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --pull always \\\n    --mount type=bind,source=\"$(pwd)\",target=/opt/repo --workdir /opt/repo \\\n    -e SCCACHE_S3_NO_CREDENTIALS=1 \\\n    rapidsai/ci-conda:latest /opt/repo/ci/run_clang_tidy.sh\n```\n\n----------------------------------------\n\nTITLE: cuML Build Options Configuration\nDESCRIPTION: Defines various build options for cuML including shared library builds, test configurations, and dependency options.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\noption(CUML_ENABLE_GPU \"Enable building GPU-accelerated algorithms\" ON)\noption(BUILD_SHARED_LIBS \"Build cuML shared libraries\" ON)\noption(BUILD_CUML_C_LIBRARY \"Build libcuml_c shared library. Contains the cuML C API\" ON)\noption(BUILD_CUML_CPP_LIBRARY \"Build libcuml shared library\" ON)\noption(BUILD_CUML_TESTS \"Build cuML algorithm tests\" ON)\noption(BUILD_CUML_MG_TESTS \"Build cuML multigpu algorithm tests\" OFF)\noption(BUILD_PRIMS_TESTS \"Build ml-prim tests\" ON)\noption(BUILD_CUML_EXAMPLES \"Build C++ API usage examples\" ON)\n```\n\n----------------------------------------\n\nTITLE: Configuring Random Forest Cython Modules in cuML\nDESCRIPTION: Sets up the compilation of Cython modules for random forest algorithms in cuML. The configuration includes adding various Cython source files with their dependencies and linking them with required libraries to create Python modules with an 'ensemble_' prefix.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/ensemble/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"randomforest_common.pyx\" ${randomforestclassifier_algo} ${randomforestregressor_algo} ${ensemble_algo})\nadd_module_gpu_default(\"randomforest_shared.pyx\" ${randomforestclassifier_algo} ${randomforestregressor_algo} ${ensemble_algo})\nadd_module_gpu_default(\"randomforestclassifier.pyx\" ${randomforestclassifier_algo} ${ensemble_algo})\nadd_module_gpu_default(\"randomforestregressor.pyx\" ${randomforestregressor_algo} ${ensemble_algo})\n\nset(linked_libraries\n    ${cuml_sg_libraries}\n    ${CUML_PYTHON_TREELITE_TARGET})\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${linked_libraries}\"\n  MODULE_PREFIX ensemble_\n)\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Version Requirements for cuML Python Build\nDESCRIPTION: Defines the minimum CMake version required and includes the RAPIDS configuration file for the cuML Python build system.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.30.4 FATAL_ERROR)\n\ninclude(../../cmake/rapids_config.cmake)\n```\n\n----------------------------------------\n\nTITLE: Configuring LARS Cython Module Build\nDESCRIPTION: Sets up the compilation of LARS algorithm Cython module with GPU support. Initializes Cython sources list and adds the LARS module with GPU defaults. Creates Cython modules with C++ compilation and links them against cuML shared libraries.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/linear_model/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"lars.pyx\" ${lars_algo} ${linear_model_algo})\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX experimental_\n)\n```\n\n----------------------------------------\n\nTITLE: Building cuML C++ and Setting RPATH for Library Installation\nDESCRIPTION: Adds the C++ code directory to be built and configures the RPATH for the installed library. RPATHs are set to locate NVIDIA libraries relative to the installation location.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(../../cpp cuml-cpp)\n\n# assumes libcuml++ is installed 2 levels deep, e.g. site-packages/cuml/lib64/libcuml++.so\nset(rpaths\n  \"$ORIGIN/../../nvidia/cublas/lib\"\n  \"$ORIGIN/../../nvidia/cufft/lib\"\n  \"$ORIGIN/../../nvidia/curand/lib\"\n  \"$ORIGIN/../../nvidia/cusolver/lib\"\n  \"$ORIGIN/../../nvidia/cusparse/lib\"\n  \"$ORIGIN/../../nvidia/nvjitlink/lib\"\n)\nset_property(TARGET ${CUML_CPP_TARGET} PROPERTY INSTALL_RPATH ${rpaths} APPEND)\n```\n\n----------------------------------------\n\nTITLE: Building KMeans Example with CMake in cuML\nDESCRIPTION: Creates a CMake build configuration for the KMeans clustering algorithm example. It defines an executable target based on the kmeans_example.cpp source file and links it with the cuml++ library that provides GPU-accelerated machine learning functionality.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/kmeans/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(kmeans_example kmeans_example.cpp)\ntarget_link_libraries(kmeans_example cuml++)\n```\n\n----------------------------------------\n\nTITLE: Running pre-commit on all files\nDESCRIPTION: Command to run pre-commit checks on all files in the repository, not just staged files. This is useful for verifying code quality across the entire codebase.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Including Third-Party Dependencies\nDESCRIPTION: Includes various third-party dependencies required by CUML, such as CCCL, RMM, RAFT, and conditionally includes others like CUVS and Treelite based on configuration flags.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n# CCCL before RMM, and RMM before RAFT\ninclude(cmake/thirdparty/get_cccl.cmake)\ninclude(cmake/thirdparty/get_rmm.cmake)\ninclude(cmake/thirdparty/get_raft.cmake)\nif(LINK_CUVS)\n  include(cmake/thirdparty/get_cuvs.cmake)\nendif()\n\nif(LINK_TREELITE)\n  include(cmake/thirdparty/get_treelite.cmake)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Private Libraries for CUML C++ Target\nDESCRIPTION: Configures the private library dependencies for the CUML C++ target, including RAFT, GPUTreeShap, CUFFT, Treelite, OpenMP, NCCL, and MPI libraries.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_24\n\nLANGUAGE: CMake\nCODE:\n```\n# These are always private:\nlist(APPEND _cuml_cpp_private_libs\n  raft::raft\n  $<TARGET_NAME_IF_EXISTS:GPUTreeShap::GPUTreeShap>\n  $<$<BOOL:${LINK_CUFFT}>:CUDA::cufft${_ctk_fft_static_suffix}>\n  ${TREELITE_LIBS}\n  ${OpenMP_CXX_LIB_NAMES}\n  $<$<OR:$<BOOL:${BUILD_CUML_STD_COMMS}>,$<BOOL:${BUILD_CUML_MPI_COMMS}>>:NCCL::NCCL>\n  $<$<BOOL:${BUILD_CUML_MPI_COMMS}>:${MPI_CXX_LIBRARIES}>\n)\n```\n\n----------------------------------------\n\nTITLE: Installing NCCL with conda\nDESCRIPTION: Command to install NVIDIA Collective Communications Library (NCCL) using conda, which is required for multi-GPU communication.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c nvidia nccl\n```\n\n----------------------------------------\n\nTITLE: Initializing Cython Sources for RAPIDS cuML Manifold Module\nDESCRIPTION: Initializes an empty list for Cython source files and adds GPU-accelerated modules for various manifold learning algorithms.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/manifold/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"simpl_set.pyx\" ${simpl_set_algo} ${manifold_algo})\nadd_module_gpu_default(\"t_sne.pyx\" ${t_sne_algo} ${manifold_algo})\nadd_module_gpu_default(\"umap.pyx\" ${umap_algo} ${manifold_algo})\nadd_module_gpu_default(\"umap_utils.pyx\")\n```\n\n----------------------------------------\n\nTITLE: Starting Dask scheduler with UCX protocol\nDESCRIPTION: Command to start a Dask scheduler using the UCX protocol on the Infiniband interface.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndask-scheduler --protocol ucx --interface ib0\n```\n\n----------------------------------------\n\nTITLE: Generating Training and Test Datasets\nDESCRIPTION: Python script output showing the generation of toy datasets for training and testing the symbolic regression model.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/symreg/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python prepare_input.py\nTraining set has n_rows=250 n_cols=2\nTest set has n_rows=50 n_cols=2\nWrote 500 values to train_data.txt\nWrote 100 values to test_data.txt\nWrote 250 values to train_labels.txt\nWrote 50 values to test_labels.txt\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit via pip\nDESCRIPTION: Alternative command to install the pre-commit tool using pip package manager. This allows developers to use pre-commit hooks for code quality checks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Configuring SingleGPU Mode in CUML CMake\nDESCRIPTION: Disables multi-GPU components and communication libraries when the SINGLEGPU option is enabled. This simplifies the build for single-GPU use cases.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# SingleGPU build disables cumlprims_mg and comms components\nif(SINGLEGPU)\n  message(STATUS \"CUML_CPP: Detected SINGLEGPU build option\")\n  message(STATUS \"CUML_CPP: Disabling Multi-GPU components and comms libraries\")\n  set(BUILD_CUML_MG_TESTS OFF)\n  set(BUILD_CUML_MPI_COMMS OFF)\n  set(ENABLE_CUMLPRIMS_MG OFF)\n  set(WITH_UCX OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Symbolic Regression Executable with CMake\nDESCRIPTION: Creates a symbolic regression example executable target, configures include directories, and links it with the cuML++ library. This enables the symbolic regression example to utilize CUDA-accelerated machine learning functionality provided by cuML.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/symreg/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(symreg_example symreg_example.cpp)\ntarget_include_directories(symreg_example PRIVATE ${CUML_INCLUDE_DIRECTORIES})\ntarget_link_libraries(symreg_example cuml++)\n```\n\n----------------------------------------\n\nTITLE: Creating NCCL configuration file\nDESCRIPTION: Content for the NCCL configuration file that specifies the Infiniband interface to use for communication.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nNCCL_SOCKET_IFNAME=ib0\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit hooks in the repository\nDESCRIPTION: Command to install pre-commit hooks in the local repository. This ensures that code quality checks run automatically before each commit.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Removing NVString and NVCat Usage in C++\nDESCRIPTION: Removes the usage of NVString and NVCat functions from the LabelEncoder implementation in C++, likely for performance or compatibility reasons.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CHANGELOG.md#2025-04-19_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nPR #1873: Remove usage of nvstring and nvcat from LabelEncoder\n```\n\n----------------------------------------\n\nTITLE: Implementing Static Tags for Estimators\nDESCRIPTION: Demonstrates how to override default estimator tags by implementing the _more_static_tags method for static tag properties.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef _more_static_tags():\n   return {\n        \"requires_y\": True\n   }\n```\n\n----------------------------------------\n\nTITLE: Listing Available Python Tests in cuML\nDESCRIPTION: Command to list all available Python tests in cuML without executing them, which helps in selecting specific tests to run.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest cuML/tests --collect-only\n```\n\n----------------------------------------\n\nTITLE: Building Symbolic Regression Example with CMake\nDESCRIPTION: Commands to build the symbolic regression example using CMake, showing the build process and dependency handling including RAFT library.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/symreg/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUML_LIBRARY_DIR=/path/to/directory/with/libcuml.so -DCUML_INCLUDE_DIR=/path/to/cuml/headers\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ make\nScanning dependencies of target raft\n[ 10%] Creating directories for 'raft'\n[ 20%] Performing download step (git clone) for 'raft'\nCloning into 'raft'...\n[ 30%] Performing update step for 'raft'\n[ 40%] No patch step for 'raft'\n[ 50%] No configure step for 'raft'\n[ 60%] No build step for 'raft'\n[ 70%] No install step for 'raft'\n[ 80%] Completed 'raft'\n[ 80%] Built target raft\nScanning dependencies of target symreg_example\n[ 90%] Building CXX object CMakeFiles/symreg_example.dir/symreg_example.cpp.o\n[100%] Linking CUDA executable symreg_example\n[100%] Built target symreg_example\n```\n\n----------------------------------------\n\nTITLE: Fixing copyright headers in Bash\nDESCRIPTION: Command to bulk-fix copyright headers on all git-tracked modified files in the repository using pre-commit hooks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run -a verify-copyright\n```\n\n----------------------------------------\n\nTITLE: Defining Interface Include Directory Copy Function in CMake\nDESCRIPTION: A CMake function that copies interface include directories from one target to another. It accepts two parameters: TARGET (the destination target) and INCLUDED_TARGET (the source target with include directories to copy).\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(copy_interface_excludes)\n  set(_options \"\")\n  set(_one_value TARGET INCLUDED_TARGET)\n  set(_multi_value \"\")\n  cmake_parse_arguments(_CUML_INCLUDES \"${_options}\" \"${_one_value}\"\n                        \"${_multi_value}\" ${ARGN})\n  get_target_property(_includes ${_CUML_INCLUDES_INCLUDED_TARGET} INTERFACE_INCLUDE_DIRECTORIES)\n  target_include_directories(${_CUML_INCLUDES_TARGET} PUBLIC ${_includes})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Updating XGBoost Version for Continuous Integration\nDESCRIPTION: Updates the version of XGBoost used in the continuous integration pipeline for compatibility and testing purposes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CHANGELOG.md#2025-04-19_snippet_2\n\nLANGUAGE: Build Configuration\nCODE:\n```\nPR #1847: Update XGBoost version for CI\n```\n\n----------------------------------------\n\nTITLE: State Management Functions for Decision Tree Classifier\nDESCRIPTION: Shows additional functions for loading and storing the state of a Decision Tree Classifier, which are necessary for managing complex data structures in a stateless API.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nvoid storeTree(const TreeNodeF *root, std::ostream &os);\nvoid storeTree(const TreeNodeD *root, std::ostream &os);\nvoid loadTree(TreeNodeF *&root, std::istream &is);\nvoid loadTree(TreeNodeD *&root, std::istream &is);\n```\n\n----------------------------------------\n\nTITLE: Building Individual cuML Components with build.sh\nDESCRIPTION: Commands for building specific components of cuML using the build.sh script. This allows focused building of just the C++ libraries, Python package, tests, or benchmarks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ ./build.sh libcuml                   # build and install the cuML C++ and C-wrapper libraries\n$ ./build.sh cuml                      # build and install the cuML python package\n$ ./build.sh prims                     # build the ml-prims tests\n$ ./build.sh bench                     # build the cuML c++ benchmark\n$ ./build.sh prims-bench               # build the ml-prims c++ benchmark\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies for cuML\nDESCRIPTION: Configures how dependencies like cumlprims_mg, cuVS, RAFT, and Treelite are handled during the build. Sets options for static/dynamic linking and inclusion in the build.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# --- cumlprims_mg --- #\n# ship cumlprims_mg in the 'libcuml' wheel (for re-use by 'cuml' wheels)\nset(CUML_USE_CUMLPRIMS_MG_STATIC OFF)\nset(CUML_EXCLUDE_CUMLPRIMS_MG_FROM_ALL OFF)\n\n# --- cuVS --- #\nset(CUML_USE_CUVS_STATIC OFF)\nset(CUML_EXCLUDE_CUVS_FROM_ALL ON)\n\n# --- raft --- #\nset(CUML_RAFT_CLONE_ON_PIN OFF)\nset(CUML_EXCLUDE_RAFT_FROM_ALL ON)\n\n# --- treelite --- #\nset(CUML_EXPORT_TREELITE_LINKAGE ON)\nset(CUML_PYTHON_TREELITE_TARGET treelite::treelite_static)\nset(CUML_USE_TREELITE_STATIC ON)\nset(CUML_EXCLUDE_TREELITE_FROM_ALL ON)\n\n# --- CUDA --- #\nset(CUDA_STATIC_RUNTIME ON)\nset(CUDA_STATIC_MATH_LIBRARIES OFF)\n```\n\n----------------------------------------\n\nTITLE: Setting Data Size Parameters for Benchmarks\nDESCRIPTION: Defines dataset dimensions for benchmarks, including small and large row sizes as well as narrow and wide feature counts. These parameters allow testing algorithms with different data scales and dimensions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nSMALL_ROW_SIZES = [2**x for x in range(14, 17)]\nLARGE_ROW_SIZES = [2**x for x in range(18, 24, 2)]\n\nSKINNY_FEATURES = [32, 256]\nWIDE_FEATURES = [1000, 10000]\n\nVERBOSE=True\nRUN_CPU=True\n```\n\n----------------------------------------\n\nTITLE: CUDA and Runtime Configuration\nDESCRIPTION: Configures CUDA-specific settings including static linking options and runtime configurations.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(_ctk_static_suffix \"\")\nset(_ctk_fft_static_suffix \"\")\nif(CUDA_STATIC_MATH_LIBRARIES)\n  set(_ctk_static_suffix \"_static\")\n  set(_ctk_fft_static_suffix \"_static_nocallback\")\nendif()\n\nif (NOT DISABLE_OPENMP)\n  find_package(OpenMP)\n  if(OpenMP_FOUND)\n    message(STATUS \"CUML_CPP: OpenMP found in ${OPENMP_INCLUDE_DIRS}\")\n    list(APPEND CUML_CXX_FLAGS ${OpenMP_CXX_FLAGS})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Examples for CUML\nDESCRIPTION: Conditionally includes examples directory when building examples is enabled.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_30\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_EXAMPLES)\n  add_subdirectory(examples)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring cuML Library Target in CMake\nDESCRIPTION: Creates the main cuML library target and conditionally adds GPU support. This sets up the primary library that will contain all the machine learning algorithms selected for the build.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_CPP_LIBRARY)\n\n  # single GPU components\n  # common components\n  add_library(${CUML_CPP_TARGET})\n  if (CUML_ENABLE_GPU)\n    target_compile_definitions(${CUML_CPP_TARGET} PUBLIC CUML_ENABLE_GPU)\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Running C++ Unit Tests for cuML\nDESCRIPTION: Commands to run the C++ unit tests for different components of cuML, including single GPU algorithms, multi-GPU algorithms, and ML primitive functions.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ cd cpp/build\n$ ./test/ml # Single GPU algorithm tests\n$ ./test/ml_mg # Multi GPU algorithm tests, if --singlegpu was not used\n$ ./test/prims # ML Primitive function tests\n```\n\n----------------------------------------\n\nTITLE: Defining Project Metadata for cuML Python\nDESCRIPTION: Declares the cuML-python project with version information from RAPIDS_VERSION and specifies the programming languages used in the project.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nproject(\n  cuml-python\n  VERSION \"${RAPIDS_VERSION}\"\n  LANGUAGES ${language_list}\n)\n```\n\n----------------------------------------\n\nTITLE: Running Complete scikit-learn Tests with GPU Acceleration in Bash\nDESCRIPTION: Basic example of running all GPU-accelerated scikit-learn tests using the run-tests.sh script with no additional arguments.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/ci/accel/scikit-learn-tests/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./run-tests.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Include Directories for Internals Module\nDESCRIPTION: This snippet adds an include directory to the internals_internals target for accessing callbacks_implements.h in the current directory.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/internals/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(internals_internals PRIVATE ${CMAKE_CURRENT_LIST_DIR})\n```\n\n----------------------------------------\n\nTITLE: Configuring Kernel Ridge Cython Module Build\nDESCRIPTION: Sets up CMake configuration for building the Kernel Ridge module. Initializes Cython sources list, adds GPU module defaults, and creates Cython modules with necessary library linkage for RAPIDS cuML.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/kernel_ridge/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(cython_sources \"\")\nadd_module_gpu_default(\"kernel_ridge.pyx\")\n\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX kernel_ridge_\n)\n```\n\n----------------------------------------\n\nTITLE: Creating RAPIDS cuML Manifold Cython Modules\nDESCRIPTION: Generates Cython modules for the manifold learning algorithms using RAPIDS Cython module creation utility. It specifies C++ as the language, links against cuML libraries, and sets a prefix for the modules.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/manifold/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${cuml_sg_libraries}\"\n  MODULE_PREFIX manifold_\n)\n```\n\n----------------------------------------\n\nTITLE: Running Specific scikit-learn Tests with GPU Acceleration in Bash\nDESCRIPTION: Example of running targeted tests using pytest arguments with the run-tests.sh script, specifically running logistic regression tests with verbosity enabled.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/ci/accel/scikit-learn-tests/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./run-tests.sh -v -k \"test_logistic\"\n```\n\n----------------------------------------\n\nTITLE: Decorator Pseudocode Implementation\nDESCRIPTION: Shows the equivalent pseudocode implementation of the decorator functionality, demonstrating the common steps performed by each decorator.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef my_func(self, X):\n   with cuml.using_ouput_type(\"mirror\"):\n      with cupy.cuda.cupy_using_allocator(\n          rmm.allocators.cupy.rmm_cupy_allocator\n      ):\n         # Set the input properties\n         self._set_base_attributes(output_type=X, n_features=X)\n\n         # Do actual calculation returning an array-like object\n         ret_val = self._my_func(X)\n\n         # Get the output type\n         output_type = self._get_output_type(X)\n\n         # Convert array-like to CumlArray\n         ret_val = input_to_cuml_array(ret_val, order=\"K\").array\n\n         # Convert CumlArray to desired output_type\n         return ret_val.to_output(output_type)\n```\n\n----------------------------------------\n\nTITLE: Importing Plotting Utilities for Hyperparameter Optimization Results in Python\nDESCRIPTION: Imports custom plotting utilities from cuml.experimental.hyperopt_utils for visualizing the results of hyperparameter optimization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.experimental.hyperopt_utils import plotting_utils\n```\n\n----------------------------------------\n\nTITLE: Overriding the Default CMake Generator\nDESCRIPTION: Command to override the default Ninja generator with Unix Makefiles by setting the CMAKE_GENERATOR environment variable before running build.sh.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nCMAKE_GENERATOR='Unix Makefiles' ./build.sh\n```\n\n----------------------------------------\n\nTITLE: Creating Cython Modules for cuML Explainer in CMake\nDESCRIPTION: Uses a custom RAPIDS CMake function to create Cython modules for the explainer. It specifies C++ as the language, uses the previously defined Cython sources and linked libraries, and sets a module prefix.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/explainer/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nrapids_cython_create_modules(\n  CXX\n  SOURCE_FILES \"${cython_sources}\"\n  LINKED_LIBRARIES \"${linked_libraries}\"\n  MODULE_PREFIX explainer_\n)\n```\n\n----------------------------------------\n\nTITLE: Setting cuML Library Target Properties in CMake\nDESCRIPTION: Configures target properties for the cuML library, including RPATH settings, C++ and CUDA standards, and position-independent code requirements. These ensure proper installation and compilation behavior.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\n  add_library(cuml::${CUML_CPP_TARGET} ALIAS ${CUML_CPP_TARGET})\n\n  set_target_properties(${CUML_CPP_TARGET}\n    PROPERTIES BUILD_RPATH                         \"\\$ORIGIN\"\n               INSTALL_RPATH                       \"\\$ORIGIN\"\n               # set target compile options\n               CXX_STANDARD                        17\n               CXX_STANDARD_REQUIRED               ON\n               CUDA_STANDARD                       17\n               CUDA_STANDARD_REQUIRED              ON\n               POSITION_INDEPENDENT_CODE           ON\n               INTERFACE_POSITION_INDEPENDENT_CODE ON\n  )\n```\n\n----------------------------------------\n\nTITLE: Building and Installing libcuml++ and libcuml\nDESCRIPTION: Commands to build the C++/CUDA libraries (libcuml++ and libcuml) with parallel compilation and install them to the configured prefix.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n$ make -j\n$ make install\n```\n\n----------------------------------------\n\nTITLE: Setting UCX environment variables for Infiniband\nDESCRIPTION: Commands to set UCX environment variables that specify which Infiniband devices and transports to use for communication.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport UCX_NET_DEVICES=mlx5_0:1,mlx5_3:1,mlx5_2:1,mlx5_1:1\nexport UCX_TLS=rc,cuda_copy,cuda_ipc\n```\n\n----------------------------------------\n\nTITLE: Checking Training Data Shape in Python\nDESCRIPTION: Prints the shape of the training data X_train. This is useful for understanding the dimensions of the dataset being used for training.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nX_train.shape\n```\n\n----------------------------------------\n\nTITLE: Adding NVTX Option to Build Script\nDESCRIPTION: Adds a --nvtx option to the build.sh script to enable NVTX profiling instrumentation during the build process.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CHANGELOG.md#2025-04-19_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nPR #1825: `--nvtx` option in `build.sh`\n```\n\n----------------------------------------\n\nTITLE: Preventing C-API Headers in C++ Code with Preprocessor Directives\nDESCRIPTION: This code snippet demonstrates how preprocessor directives are used to prevent including C-API headers in C++ code. The CUML_CPP_API macro is checked, and if defined, a compilation error is triggered with an explanatory message.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/tests/c_api/README.md#2025-04-19_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#ifdef CUML_CPP_API\n#error \\\n  \"This header is only for the C-API and should not be included from the C++ API.\"\n#endif\n```\n\n----------------------------------------\n\nTITLE: Configuring RAFT Dependencies for CUML\nDESCRIPTION: Sets up RAFT dependencies for the CUML C++ target, copying interface excludes and conditionally adding cuco library when distributed RAFT is used.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nif(CUML_USE_RAFT_STATIC AND (TARGET raft::raft))\n  copy_interface_excludes(INCLUDED_TARGET raft::raft TARGET ${CUML_CPP_TARGET})\n\n  if(CUML_USE_RAFT_DIST AND (TARGET cuco::cuco))\n    list(APPEND _cuml_cpp_private_libs cuco::cuco)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running cuML C++ Benchmarks\nDESCRIPTION: Command to run the single GPU C++ benchmarks for cuML, which is useful for performance testing and optimization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ ./bench/sg_benchmark  # Single GPU benchmarks\n```\n\n----------------------------------------\n\nTITLE: Configuring IP over Infiniband interface\nDESCRIPTION: Command to assign an IP address to the Infiniband interface (ib0), enabling IP-based communication over Infiniband.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/mnmg/Using_Infiniband_for_MNMG.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo ifconfig ib0 10.0.0.50/24\n```\n\n----------------------------------------\n\nTITLE: Converting Benchmark Results to Pandas DataFrame in Python\nDESCRIPTION: Converts the benchmark results into a Pandas DataFrame for further analysis and visualization.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(benchmark_results)\n```\n\n----------------------------------------\n\nTITLE: Updating CuPy Requirement Version in Python\nDESCRIPTION: Updates the required CuPy version to 7 in the cuML Python package dependencies.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CHANGELOG.md#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nPR #1762: Update CuPy requirement to 7\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SVR with Linear Kernel in Python using cuML\nDESCRIPTION: Sets up and executes a benchmark for SVR with Linear Kernel using cuML's SpeedupComparisonRunner. It compares performance on small datasets with skinny features for regression tasks. CPU benchmark is disabled by default due to high runtime.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_REGRESSION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\n# Due to extreme high runtime, the CPU benchmark \n# is disabled. Use run_cpu=True to re-enable. \n\nexecute_benchmark(\"SVR-Linear\", runner, run_cpu=False)\n```\n\n----------------------------------------\n\nTITLE: Configuring Treelite Dependencies for CUML\nDESCRIPTION: Sets up Treelite dependencies for the CUML C++ target, handling static linking or exporting linkage based on configuration flags.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\nif(CUML_USE_TREELITE_STATIC AND (TARGET treelite::treelite_static))\n  set(TREELITE_LIBS treelite::treelite_static)\n  copy_interface_excludes(INCLUDED_TARGET treelite::treelite_static TARGET ${CUML_CPP_TARGET})\nelseif(CUML_EXPORT_TREELITE_LINKAGE)\n  list(APPEND _cuml_cpp_public_libs ${TREELITE_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenBLAS in Conda for cuML Build\nDESCRIPTION: Command to explicitly specify OpenBLAS from conda for the BLAS and LAPACK dependencies when configuring the CMake build for cuML.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncmake .. -DCMAKE_INSTALL_PREFIX=$CONDA_PREFIX -DBLAS_LIBRARIES=$CONDA_PREFIX/lib/libopenblas.so\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Dataset with Python Script\nDESCRIPTION: Example usage of the synthetic dataset generator script with output showing generated dataset details.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/dbscan/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./gen_dataset.py --num_samples 1000 --num_features 16 --num_clusters 10 --filename_prefix synthetic\nDataset file: synthetic-1000x16-clusters-10.txt\nGenerated total 1000 samples with 16 features each\nNumber of clusters = 10\n```\n\n----------------------------------------\n\nTITLE: Data Directory Creation\nDESCRIPTION: Creates a directory for storing the Higgs dataset if it doesn't exist.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/experimental/hyperparams/HPO_demo.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata_dir = '/home/hyperopt/data/'\nif not os.path.exists(data_dir):\n    print('creating data directory')\n    os.system('mkdir /home/data/')\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Sparse FIL in Python using cuML\nDESCRIPTION: Configures and runs a benchmark for the Sparse FIL algorithm using cuML's SpeedupComparisonRunner. It compares performance for classification tasks on small datasets with skinny features, requiring the TreeLite library.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nrunner = cuml.benchmark.runners.SpeedupComparisonRunner(\n    bench_rows=SMALL_ROW_SIZES, \n    bench_dims=SKINNY_FEATURES,\n    dataset_name=DATA_CLASSIFICATION,\n    input_type=INPUT_TYPE,\n    n_reps=N_REPS\n)\n\nexecute_benchmark(\"Sparse-FIL-SKL\", runner)\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Runtime Libraries for CUML\nDESCRIPTION: Determines whether to make CUDA Toolkit libraries public or private dependencies based on whether static CUDA runtime is used, and adds appropriate libraries to the target.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_25\n\nLANGUAGE: CMake\nCODE:\n```\nset(_cuml_cpp_libs_var_name \"_cuml_cpp_public_libs\")\nif(CUDA_STATIC_RUNTIME)\n  set(_cuml_cpp_libs_var_name \"_cuml_cpp_private_libs\")\n  # Add CTK include paths because we're going to make our CTK library links private below\n  target_include_directories(${CUML_CPP_TARGET} SYSTEM PUBLIC ${CUDAToolkit_INCLUDE_DIRS})\nendif()\n\n# The visibility of these depend on whether we're linking the CTK statically,\n# because cumlprims_mg and cuML inherit their CUDA libs from the raft::raft\n# INTERFACE target.\nlist(APPEND ${_cuml_cpp_libs_var_name}\n  $<$<BOOL:${CUML_RAFT_COMPILED}>:${RAFT_COMPILED_LIB}>\n  $<TARGET_NAME_IF_EXISTS:cumlprims_mg::cumlprims_mg>\n)\n```\n\n----------------------------------------\n\nTITLE: Exogenous Variables List in Markdown\nDESCRIPTION: A markdown list of procedural exogenous variables created to complement the derived datasets. These variables are normalized and linked to specific time series.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/cuml/cuml/tests/ts_datasets/README.md#2025-04-19_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n- `exog_deaths_by_region_exog.csv`\n- `exog_guest_nights_by_region_missing_exog.csv`\n- `exog_hourly_earnings_by_industry_missing_exog.csv`\n```\n\n----------------------------------------\n\nTITLE: Setting logging pattern in C++\nDESCRIPTION: Example of how to set a custom logging pattern in C++ for the cuML project. This determines the format of log messages.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nML::Logger::get.setPattern(YourFavoriteFormat);\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from CSV\nDESCRIPTION: Loads the Million News Headlines dataset from the specified zip file into a Pandas DataFrame.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(million_articles_path)\n```\n\n----------------------------------------\n\nTITLE: Exporting Benchmark Results to CSV in Python\nDESCRIPTION: Saves the benchmark results DataFrame to a CSV file named 'benchmark_results.csv' for further analysis or sharing.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndf.to_csv(\"benchmark_results.csv\")\n```\n\n----------------------------------------\n\nTITLE: Installing CUML Targets and Headers\nDESCRIPTION: Configures the installation of CUML targets, header files, and version configuration file.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_31\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(CPack)\n\nset(CUML_TARGETS ${CUML_CPP_TARGET})\n\nif(BUILD_CUML_C_LIBRARY)\n  list(APPEND CUML_TARGETS\n         ${CUML_C_TARGET})\nendif()\n\ninstall(TARGETS\n          ${CUML_TARGETS}\n        DESTINATION\n          ${lib_dir}\n        EXPORT\n          cuml-exports)\n\ninstall(DIRECTORY include/cuml/\n        DESTINATION include/cuml)\n\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/include/cuml/version_config.hpp\n        DESTINATION include/cuml)\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for cuML User Guide\nDESCRIPTION: This snippet defines the structure of the cuML user guide using reStructuredText directives. It sets up a table of contents with a maximum depth of 2, including links to key notebook files covering estimator introduction, model pickling, and execution device interoperability.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/user_guide.rst#2025-04-19_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   estimator_intro.ipynb\n   pickling_cuml_models.ipynb\n   execution_device_interoperability.ipynb\n```\n\n----------------------------------------\n\nTITLE: Generating compile command database for clang-tidy\nDESCRIPTION: Command to generate the compile command database needed by clang-tidy. This is a prerequisite for running the static analyzer on the codebase.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./build.sh --configure-only libcuml\n```\n\n----------------------------------------\n\nTITLE: Fixing formatting violations in Bash\nDESCRIPTION: Command to bulk-fix all formatting violations detected by clang-format in the cuML repository.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython ./cpp/scripts/run-clang-format.py -inplace\n```\n\n----------------------------------------\n\nTITLE: Enabling Inline Matplotlib Plotting in Python\nDESCRIPTION: Enables inline plotting for Matplotlib in a Jupyter notebook environment.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Configuring CUML Benchmarks and Documentation\nDESCRIPTION: Sets up benchmark build directory conditionally and configures Doxygen documentation generation.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/CMakeLists.txt#2025-04-19_snippet_33\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CUML_BENCH)\n  add_subdirectory(bench)\nendif()\n\ninclude(cmake/doxygen.cmake)\nadd_doxygen_target(IN_DOXYFILE Doxyfile.in\n  OUT_DOXYFILE ${CMAKE_CURRENT_SOURCE_DIR}/Doxyfile\n  CWD ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit via conda\nDESCRIPTION: Command to install the pre-commit tool using conda package manager from the conda-forge channel. This is required for running code linters and formatters before committing changes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nconda install -c conda-forge pre-commit\n```\n\n----------------------------------------\n\nTITLE: Running clang-format check in Bash\nDESCRIPTION: Command to manually run the clang-format check on the cuML repository. This script detects formatting violations before creating a PR.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ./cpp/scripts/run-clang-format.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Output File Path with Date Tag\nDESCRIPTION: Sets up the benchmark results output file path with a date tag and removes any existing file with the same name.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDATE_TAG = datetime.now().strftime(\"%Y-%m-%d\")\n\noutpath = f\"hdbscan-apmv-benchmark-results-{DATE_TAG}.json1\"\nif os.path.exists(outpath):\n    os.remove(outpath)    \n```\n\n----------------------------------------\n\nTITLE: Configuring cuML Build Options\nDESCRIPTION: Sets various build options for cuML, disabling tests, examples, and benchmarks. Explicitly sets all algorithms to be built and enables multi-node, multi-GPU functionality.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# --- cuML --- #\nset(BUILD_CUML_TESTS OFF)\nset(BUILD_PRIMS_TESTS OFF)\nset(BUILD_CUML_C_LIBRARY OFF)\nset(BUILD_CUML_EXAMPLES OFF)\nset(BUILD_CUML_BENCH OFF)\n\n# In libcuml wheels, we always want to build in all cuML algorithms.\n# This is the default in cpp/CMakeLists.txt, but just making that choice for wheels explicit here.\nset(CUML_ALGORITHMS \"ALL\" CACHE STRING \"Choose which algorithms are built cuML. Can specify individual algorithms or groups in a semicolon-separated list.\")\n\n# for libcuml wheels, always compile in the multi-node, multi-GPU stuff from cumlprims_mg\nset(SINGLEGPU OFF)\n\nset(CUML_CPP_TARGET \"cuml++\")\nset(CUML_CPP_SRC \"../../cpp\")\n```\n\n----------------------------------------\n\nTITLE: ECL-BH License and Copyright Notice\nDESCRIPTION: Software license (BSD-style) and copyright notice for the ECL-BH v4.5 project, including redistribution terms, warranty disclaimers, and attribution requirements. Includes information about the authors and related publication.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/src/tsne/cannylabs_tsne_license.txt#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n/*\nECL-BH v4.5: Simulation of the gravitational forces in a star cluster using\nthe Barnes-Hut n-body algorithm.\n\nCopyright (c) 2010-2020 Texas State University. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n   * Redistributions of source code must retain the above copyright\n     notice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above copyright\n     notice, this list of conditions and the following disclaimer in the\n     documentation and/or other materials provided with the distribution.\n   * Neither the name of Texas State University nor the names of its\n     contributors may be used to endorse or promote products derived from\n     this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL TEXAS STATE UNIVERSITY BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nAuthors: Martin Burtscher and Sahar Azimi\n\nURL: The latest version of this code is available at\nhttps://userweb.cs.txstate.edu/~burtscher/research/ECL-BH/.\n\nPublication: This work is described in detail in the following paper.\nMartin Burtscher and Keshav Pingali. An Efficient CUDA Implementation of the\nTree-based Barnes Hut n-Body Algorithm. Chapter 6 in GPU Computing Gems\nEmerald Edition, pp. 75-92. January 2011.\n*/\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Path Variable\nDESCRIPTION: Defines the file path to the Million News Headlines dataset zip file downloaded from Kaggle.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmillion_articles_path = \"/home/cjnolet/Downloads/archive.zip\"\n```\n\n----------------------------------------\n\nTITLE: Running Only Single-GPU Algorithm Tests in Python\nDESCRIPTION: Command to run Python tests while ignoring multi-GPU tests, useful when focusing on single-GPU functionality or when multi-GPU setup is not available.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/BUILD.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest --ignore=cuml/tests/dask --ignore=cuml/tests/test_nccl.py\n```\n\n----------------------------------------\n\nTITLE: Fixing include style issues in Bash\nDESCRIPTION: Command to bulk-fix include style issues in specified folders of the cuML repository using the include_checker.py script.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ./cpp/scripts/include_checker.py --inplace [cpp/include cpp/src cpp/src_prims cpp/test ... list of folders which you want to fix]\n```\n\n----------------------------------------\n\nTITLE: Incorrect Stateful C++ API Example for Decision Tree Classifier\nDESCRIPTION: Shows an incorrect way of exposing the Decision Tree Classifier API that violates the stateless guideline by exposing a non-POD C++ class object in the public interface.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/cpp/DEVELOPER_GUIDE.md#2025-04-19_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename T>\nclass DecisionTreeClassifier {\n  TreeNode<T>* root;\n  DTParams params;\n  const raft::handle_t &handle;\npublic:\n  DecisionTreeClassifier(const raft::handle_t &handle, DTParams& params, bool verbose=false);\n  void fit(const T *input, int n_rows, int n_cols, const int *labels);\n  void predict(const T *input, int n_rows, int n_cols, int *predictions);\n};\n\nvoid decisionTreeClassifierFit(const raft::handle_t &handle, const float *input, int n_rows, int n_cols,\n                               const int *labels, DecisionTreeClassifier<float> *model, DTParams params,\n                               bool verbose=false);\nvoid decisionTreeClassifierPredict(const raft::handle_t &handle, const float* input,\n                                   DecisionTreeClassifier<float> *model, int n_rows,\n                                   int n_cols, int* predictions, bool verbose=false);\n```\n\n----------------------------------------\n\nTITLE: Incorrect Estimator Initialization in Python\nDESCRIPTION: Demonstrates an incorrect way to initialize a cuML estimator by altering input arguments. This approach breaks proper cloning of the estimator and should be avoided.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, my_option=\"option1\"):\n   if (my_option == \"option1\"):\n      self.my_option = 1\n   else:\n      self.my_option = 2\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX Entry\nDESCRIPTION: BibTeX citation entry for referencing cuML in academic work, citing the 2020 paper on Machine Learning in Python by Raschka, Patterson, and Nolet.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/python/libcuml/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{raschka2020machine,\n  title={Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence},\n  author={Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},\n  journal={arXiv preprint arXiv:2002.04803},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Charting All Algorithm Speedups in Python\nDESCRIPTION: Applies the chart_all_algo_speedup function to visualize the average speedup for all benchmarked algorithms.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/cuml_benchmarks.ipynb#2025-04-19_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nchart_all_algo_speedup(df)\n```\n\n----------------------------------------\n\nTITLE: Suppressing Future Warnings in Python\nDESCRIPTION: This code snippet suppresses future warnings to clean up the notebook output.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/pickling_cuml_models.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n```\n\n----------------------------------------\n\nTITLE: Writing Hypothesis Tests with Required Examples\nDESCRIPTION: Example of how to write hypothesis-based tests for cuML estimators, including required explicit examples. The test demonstrates testing with different data types and sparse inputs, with explicit examples running in CI and strategy-based tests in nightly runs.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/DEVELOPER_GUIDE.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@example(dtype=np.float32, sparse_input=False)  # baseline case, runs as part of PR CI\n@example(dtype=np.float64, sparse_input=True)   # edge case, runs as part of PR CI\n@given(\n    dtype=st.sampled_from((np.float32, np.float64)),\n    sparse_input=st.booleans()\n)  # strategy-based cases, only runs during nightly tests\ndef test_my_estimator(dtype, sparse_input):\n    # Test implementation\n    pass\n```\n\n----------------------------------------\n\nTITLE: Manual Array Conversion in predict() Method\nDESCRIPTION: Example of a predict() method that manually handles array conversion, dtype determination, and output formatting using CumlArray and explicit conversions with the @with_cupy_rmm decorator.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n@with_cupy_rmm\ndef predict(self, X_in):\n   # Determine the output_type\n   out_type = self._get_output_type(X_in)\n   out_dtype = self._get_target_dtype()\n\n   # Convert to CumlArray\n   X_m = input_to_cuml_array(X_in, order=\"K\").array\n\n   # Call a cuda function\n   X_m = cp.asarray(X_m) + cp.ones(X_m.shape)\n\n   # Convert back to CumlArray\n   X_m = CumlArray(X_m)\n\n   # Convert the CudaArray to the desired output and dtype\n   return X_m.to_output(output_type=out_type, output_dtype=out_dtype)\n```\n\n----------------------------------------\n\nTITLE: Converting cuml.accel Model to Scikit-Learn Format using Console Command\nDESCRIPTION: This code snippet demonstrates how to convert a serialized cuml.accel model to a regular Scikit-Learn (or UMAP/HDBSCAN) pickled model using a console command. This conversion allows the model to be used in environments without cuML or GPUs.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change.rst#2025-04-19_snippet_5\n\nLANGUAGE: console\nCODE:\n```\npython -m cuml.accel --convert-to-sklearn model_pickled.pkl --format pickle --output converted_model.pkl\n```\n\n----------------------------------------\n\nTITLE: Ingesting Arrays with input_to_cuml_array in Python\nDESCRIPTION: Demonstrates the correct way to ingest arrays in cuML using the input_to_cuml_array function. This method can handle all supported types, check array order, enforce specific dtypes, and raise errors on incorrect array sizes.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef fit(self, X):\n    cuml_array, dtype, cols, rows = input_to_cuml_array(X, order=\"K\")\n    ...\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters for XGBoost Model Training in Python\nDESCRIPTION: Sets parameters for synthetic data generation and XGBoost model training, including data size, train-test split, and model hyperparameters.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/forest_inference_demo.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# synthetic data size\nn_rows = 10000\nn_columns = 100\nn_categories = 2\nrandom_state = cupy.random.RandomState(43210)\n\n# fraction of data used for model training\ntrain_size = 0.8\n\n# trained model output filename\nmodel_path = 'xgb.model'\n\n# num of iterations for which xgboost is trained\nnum_rounds = 100\n\n# maximum tree depth in each training round\nmax_depth = 20\n```\n\n----------------------------------------\n\nTITLE: Warming Up GPU with Initial HDBSCAN Run\nDESCRIPTION: Creates and fits a small HDBSCAN model to initialize the CUDA context on the GPU, reducing startup overhead for the actual benchmarks.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/notebooks/tools/hdbscan_soft_clustering_benchmark.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%time\nclusterer = cuml.cluster.hdbscan.HDBSCAN(\n    prediction_data=True\n)\nclusterer.fit(np.arange(1000).reshape(50,20))\n```\n\n----------------------------------------\n\nTITLE: Handling CUDA Runtime Errors in Python\nDESCRIPTION: Example of error handling when working with CUDA runtime APIs in cuML. This snippet demonstrates how to properly catch and handle CudaRuntimeError exceptions when using CUDA streams.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/DEVELOPER_GUIDE.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cuml.cuda import Stream, CudaRuntimeError\ntry:\n    s = Stream()\n    s.sync\nexcept CudaRuntimeError as cre:\n    print(\"Cuda Error! '%s'\" % str(cre))\n```\n\n----------------------------------------\n\nTITLE: Decorator Usage Example - Before and After\nDESCRIPTION: Compares the implementation of a predict method before and after the introduction of the decorator system.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/wiki/python/ESTIMATOR_GUIDE.md#2025-04-19_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n@with_cupy_rmm\ndef predict(self, X, y):\n   # Determine the output type and dtype\n   out_type = self._get_output_type(y)\n   out_dtype = self._get_target_dtype()\n\n   # Convert to CumlArray\n   X_m = input_to_cuml_array(X, order=\"K\").array\n\n   # Call a cuda function\n   someCudaFunction(X_m.ptr)\n\n   # Convert the CudaArray to the desired output\n   return X_m.to_output(output_type=out_type, output_dtype=out_dtype)\n```\n\nLANGUAGE: python\nCODE:\n```\n@cuml.internals.api_base_return_array(input_arg=\"y\", get_output_dtype=True)\ndef predict(self, X):\n   # Convert to CumlArray\n   X_m = input_to_cuml_array(X, order=\"K\").array\n\n   # Call a cuda function\n   someCudaFunction(X_m.ptr)\n\n   # Convert the CudaArray to the desired output\n   return X_m\n```\n\n----------------------------------------\n\nTITLE: Running Python Script with cuml.accel Acceleration\nDESCRIPTION: Demonstrates how to invoke an existing Python script using cuml.accel for GPU acceleration without modifying the original code.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/docs/source/zero-code-change.rst#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npython -m cuml.accel unchanged_script.py\n```\n\n----------------------------------------\n\nTITLE: Running DBSCAN Example with Custom Dataset\nDESCRIPTION: Command and output example for running DBSCAN on a custom dataset with specified parameters.\nSOURCE: https://github.com/rapidsai/cuml/blob/branch-25.06/cpp/examples/dbscan/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ ./dbscan_example -input <input file> -num_samples <#samples> -num_features <#features> [-min_pts <minPts>] [-eps <eps>]\n```"
  }
]